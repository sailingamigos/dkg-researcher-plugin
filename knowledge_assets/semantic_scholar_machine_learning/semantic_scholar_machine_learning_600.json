[
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:39f1cbef12f64dcdb3a7683f9e70f436a7742328",
            "@type": "ScholarlyArticle",
            "paperId": "39f1cbef12f64dcdb3a7683f9e70f436a7742328",
            "corpusId": 9823884,
            "url": "https://www.semanticscholar.org/paper/39f1cbef12f64dcdb3a7683f9e70f436a7742328",
            "title": "Applications of Deep Learning and Reinforcement Learning to Biological Data",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems",
            "publicationVenue": {
                "id": "urn:research:79c5a18d-0295-432c-aaa5-961d73de6d88",
                "name": "IEEE Transactions on Neural Networks and Learning Systems",
                "alternate_names": [
                    "IEEE Trans Neural Netw Learn Syst"
                ],
                "issn": "2162-237X",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5962385"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2768956845",
                "DBLP": "journals/tnn/MahmudKHV18",
                "ArXiv": "1711.03985",
                "DOI": "10.1109/TNNLS.2018.2790388",
                "CorpusId": 9823884,
                "PubMed": "29771663"
            },
            "abstract": "Rapid advances in hardware-based technologies during the past decades have opened up new possibilities for life scientists to gather multimodal data in various application domains, such as omics, bioimaging, medical imaging, and (brain/body)\u2013machine interfaces. These have generated novel opportunities for development of dedicated data-intensive machine learning techniques. In particular, recent research in deep learning (DL), reinforcement learning (RL), and their combination (deep RL) promise to revolutionize the future of artificial intelligence. The growth in computational power accompanied by faster and increased data storage, and declining computing costs have already allowed scientists in various fields to apply these techniques on data sets that were previously intractable owing to their size and complexity. This paper provides a comprehensive survey on the application of DL, RL, and deep RL techniques in mining biological data. In addition, we compare the performances of DL techniques when applied to different data sets across various application domains. Finally, we outline open issues in this challenging research area and discuss future development perspectives.",
            "referenceCount": 213,
            "citationCount": 565,
            "influentialCitationCount": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://dspace.stir.ac.uk/bitstream/1893/26814/1/1711.03985.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2017-11-10",
            "journal": {
                "name": "IEEE Transactions on Neural Networks and Learning Systems",
                "volume": "29"
            },
            "citationStyles": {
                "bibtex": "@Article{Mahmud2017ApplicationsOD,\n author = {M. Mahmud and M. S. Kaiser and A. Hussain and S. Vassanelli},\n booktitle = {IEEE Transactions on Neural Networks and Learning Systems},\n journal = {IEEE Transactions on Neural Networks and Learning Systems},\n pages = {2063-2079},\n title = {Applications of Deep Learning and Reinforcement Learning to Biological Data},\n volume = {29},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1f135e98e867ffcde5b359e7b817bbe21f80cfce",
            "@type": "ScholarlyArticle",
            "paperId": "1f135e98e867ffcde5b359e7b817bbe21f80cfce",
            "corpusId": 118926724,
            "url": "https://www.semanticscholar.org/paper/1f135e98e867ffcde5b359e7b817bbe21f80cfce",
            "title": "Deep Learning and Its Application to LHC Physics",
            "venue": "Annual Review of Nuclear and Particle Science",
            "publicationVenue": {
                "id": "urn:research:9a1eb53e-bbc0-4488-976f-6db6d80789ca",
                "name": "Annual Review of Nuclear and Particle Science",
                "alternate_names": [
                    "Annu Rev Nucl Part Sci"
                ],
                "issn": "0163-8998",
                "url": "https://www.annualreviews.org/journal/nucl"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "3099381338",
                "ArXiv": "1806.11484",
                "DOI": "10.1146/annurev-nucl-101917-021019",
                "CorpusId": 118926724
            },
            "abstract": "Machine learning has played an important role in the analysis of high-energy physics data for decades. The emergence of deep learning in 2012 allowed for machine learning tools which could adeptly handle higher-dimensional and more complex problems than previously feasible. This review is aimed at the reader who is familiar with high-energy physics but not machine learning. The connections between machine learning and high-energy physics data analysis are explored, followed by an introduction to the core concepts of neural networks, examples of the key results demonstrating the power of deep learning for analysis of LHC data, and discussion of future prospects and concerns.",
            "referenceCount": 41,
            "citationCount": 312,
            "influentialCitationCount": 6,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1806.11484",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Physics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Physics",
                    "source": "external"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "2018-06-29",
            "journal": {
                "name": "Annual Review of Nuclear and Particle Science",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Guest2018DeepLA,\n author = {D. Guest and Kyle Cranmer and D. Whiteson},\n booktitle = {Annual Review of Nuclear and Particle Science},\n journal = {Annual Review of Nuclear and Particle Science},\n title = {Deep Learning and Its Application to LHC Physics},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9011405b759b492b1132aea7b6165c9d1b0513e7",
            "@type": "ScholarlyArticle",
            "paperId": "9011405b759b492b1132aea7b6165c9d1b0513e7",
            "corpusId": 8997922,
            "url": "https://www.semanticscholar.org/paper/9011405b759b492b1132aea7b6165c9d1b0513e7",
            "title": "Reconciling schemas of disparate data sources: a machine-learning approach",
            "venue": "ACM SIGMOD Conference",
            "publicationVenue": {
                "id": "urn:research:f68b9e7e-ad3d-46cb-857d-23e49384143c",
                "name": "ACM SIGMOD Conference",
                "alternate_names": [
                    "SIGMOD",
                    "ACM SIGMOD Conf"
                ],
                "issn": null,
                "url": "https://sigmod.org/conferences/"
            },
            "year": 2001,
            "externalIds": {
                "DBLP": "conf/sigmod/DoanDH01",
                "MAG": "2150365753",
                "DOI": "10.1145/375663.375731",
                "CorpusId": 8997922
            },
            "abstract": "A data-integration system provides access to a multitude of data sources through a single mediated schema. A key bottleneck in building such systems has been the laborious manual construction of semantic mappings between the source schemas and the mediated schema. We describe LSD, a system that employs and extends current machine-learning techniques to semi-automatically find such mappings. LSD first asks the user to provide the semantic mappings for a small set of data sources, then uses these mappings together with the sources to train a set of learners. Each learner exploits a different type of information either in the source schemas or in their data. Once the learners have been trained, LSD finds semantic mappings for a new data source by applying the learners, then combining their predictions using a meta-learner. To further improve matching accuracy, we extend machine learning techniques so that LSD can incorporate domain constraints as an additional source of knowledge, and develop a novel learner that utilizes the structural information in XML documents. Our approach thus is distinguished in that it incorporates multiple types of knowledge. Importantly, its architecture is extensible to additional learners that may exploit new kinds of information. We describe a set of experiments on several real-world domains, and show that LSD proposes semantic mappings with a high degree of accuracy.",
            "referenceCount": 23,
            "citationCount": 905,
            "influentialCitationCount": 62,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2001-05-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Doan2001ReconcilingSO,\n author = {A. Doan and Pedro M. Domingos and A. Halevy},\n booktitle = {ACM SIGMOD Conference},\n pages = {509-520},\n title = {Reconciling schemas of disparate data sources: a machine-learning approach},\n year = {2001}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6241276a3074f73c1ce9b9ce4ac69f748732aecd",
            "@type": "ScholarlyArticle",
            "paperId": "6241276a3074f73c1ce9b9ce4ac69f748732aecd",
            "corpusId": 119277652,
            "url": "https://www.semanticscholar.org/paper/6241276a3074f73c1ce9b9ce4ac69f748732aecd",
            "title": "Data Mining and Machine Learning in Astronomy",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2009,
            "externalIds": {
                "ArXiv": "0906.2173",
                "MAG": "2119936234",
                "DOI": "10.1142/S0218271810017160",
                "CorpusId": 119277652
            },
            "abstract": "We review the current state of data mining and machine learning in astronomy. 'Data Mining' can have a somewhat mixed connotation from the point of view of a researcher in this field. If used correctly, it can be a powerful approach, holding the potential to fully exploit the exponentially increasing amount of available data, promising great scientific advance. However, if misused, it can be little more than the black-box application of complex computing algorithms that may give little physical insight, and provide questionable results. Here, we give an overview of the entire data mining process, from data collection through to the interpretation of results. We cover common machine learning algorithms, such as artificial neural networks and support vector machines, applications from a broad range of astronomy, emphasizing those where data mining techniques directly resulted in improved science, and important current and future directions, including probability density functions, parallel algorithms, petascale computing, and the time domain. We conclude that, so long as one carefully selects an appropriate algorithm, and is guided by the astronomical problem at hand, data mining can be very much the powerful tool, and not the questionable black box.",
            "referenceCount": 314,
            "citationCount": 277,
            "influentialCitationCount": 15,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/0906.2173",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Physics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Physics",
                    "source": "external"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "2009-06-11",
            "journal": {
                "name": "arXiv: Instrumentation and Methods for Astrophysics",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Article{Ball2009DataMA,\n author = {N. Ball and Robert J. Brunner Herzberg Institute of Astrophysics and Victoria and Bc and Canada. and D. O. Astronomy and U. I. Urbana-Champaign},\n journal = {arXiv: Instrumentation and Methods for Astrophysics},\n title = {Data Mining and Machine Learning in Astronomy},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:19bb0dce99466077e9bc5a2ad4941607fc28b40c",
            "@type": "ScholarlyArticle",
            "paperId": "19bb0dce99466077e9bc5a2ad4941607fc28b40c",
            "corpusId": 16902615,
            "url": "https://www.semanticscholar.org/paper/19bb0dce99466077e9bc5a2ad4941607fc28b40c",
            "title": "Manifold Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2006,
            "externalIds": {
                "DBLP": "journals/jmlr/BelkinNS06",
                "MAG": "2997701990",
                "CorpusId": 16902615
            },
            "abstract": "We propose a family of learning algorithms based on a new form of regularization that allows us to exploit the geometry of the marginal distribution. We focus on a semi-supervised framework that incorporates labeled and unlabeled data in a general-purpose learner. Some transductive graph learning algorithms and standard methods including support vector machines and regularized least squares can be obtained as special cases. We use properties of reproducing kernel Hilbert spaces to prove new Representer theorems that provide theoretical basis for the algorithms. As a result (in contrast to purely graph-based approaches) we obtain a natural out-of-sample extension to novel examples and so are able to handle both transductive and truly semi-supervised settings. We present experimental evidence suggesting that our semi-supervised algorithms are able to use unlabeled data effectively. Finally we have a brief discussion of unsupervised and fully supervised learning within our general framework.",
            "referenceCount": 52,
            "citationCount": 3962,
            "influentialCitationCount": 496,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2006-12-01",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "7"
            },
            "citationStyles": {
                "bibtex": "@Article{Belkin2006ManifoldRA,\n author = {M. Belkin and P. Niyogi and Vikas Sindhwani},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {2399-2434},\n title = {Manifold Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples},\n volume = {7},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:391b86cf16c2702dcc4beee55a6dd6d3bd7cf27b",
            "@type": "ScholarlyArticle",
            "paperId": "391b86cf16c2702dcc4beee55a6dd6d3bd7cf27b",
            "corpusId": 207217044,
            "url": "https://www.semanticscholar.org/paper/391b86cf16c2702dcc4beee55a6dd6d3bd7cf27b",
            "title": "Deep Learning for Content-Based Image Retrieval: A Comprehensive Study",
            "venue": "ACM Multimedia",
            "publicationVenue": {
                "id": "urn:research:f2c85de5-7cfa-4b92-8714-a0fbdcf0274e",
                "name": "ACM Multimedia",
                "alternate_names": [
                    "MM"
                ],
                "issn": null,
                "url": null
            },
            "year": 2014,
            "externalIds": {
                "MAG": "2123229215",
                "DBLP": "conf/mm/WanWHWZZL14",
                "DOI": "10.1145/2647868.2654948",
                "CorpusId": 207217044
            },
            "abstract": "Learning effective feature representations and similarity measures are crucial to the retrieval performance of a content-based image retrieval (CBIR) system. Despite extensive research efforts for decades, it remains one of the most challenging open problems that considerably hinders the successes of real-world CBIR systems. The key challenge has been attributed to the well-known ``semantic gap'' issue that exists between low-level image pixels captured by machines and high-level semantic concepts perceived by human. Among various techniques, machine learning has been actively investigated as a possible direction to bridge the semantic gap in the long term. Inspired by recent successes of deep learning techniques for computer vision and other applications, in this paper, we attempt to address an open problem: if deep learning is a hope for bridging the semantic gap in CBIR and how much improvements in CBIR tasks can be achieved by exploring the state-of-the-art deep learning techniques for learning feature representations and similarity measures. Specifically, we investigate a framework of deep learning with application to CBIR tasks with an extensive set of empirical studies by examining a state-of-the-art deep learning method (Convolutional Neural Networks) for CBIR tasks under varied settings. From our empirical studies, we find some encouraging results and summarize some important insights for future research.",
            "referenceCount": 61,
            "citationCount": 839,
            "influentialCitationCount": 41,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Book"
            ],
            "publicationDate": "2014-11-03",
            "journal": {
                "name": "Proceedings of the 22nd ACM international conference on Multimedia",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Wan2014DeepLF,\n author = {Ji Wan and Dayong Wang and S. Hoi and Pengcheng Wu and Jianke Zhu and Yongdong Zhang and Jintao Li},\n booktitle = {ACM Multimedia},\n journal = {Proceedings of the 22nd ACM international conference on Multimedia},\n title = {Deep Learning for Content-Based Image Retrieval: A Comprehensive Study},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:298af26244e3ad836c1aa5cf5855d05f5197063d",
            "@type": "ScholarlyArticle",
            "paperId": "298af26244e3ad836c1aa5cf5855d05f5197063d",
            "corpusId": 16919478,
            "url": "https://www.semanticscholar.org/paper/298af26244e3ad836c1aa5cf5855d05f5197063d",
            "title": "Machine Learning Methods Without Tears: A Primer for Ecologists",
            "venue": "The Quarterly review of biology",
            "publicationVenue": {
                "id": "urn:research:aca0cd21-832c-44db-b5dc-a42d77c4d494",
                "name": "The Quarterly review of biology",
                "alternate_names": [
                    "The Quarterly Review of Biology",
                    "Q rev biology",
                    "Q Rev Biology"
                ],
                "issn": "0033-5770",
                "url": "https://www.journals.uchicago.edu/loi/qrb"
            },
            "year": 2008,
            "externalIds": {
                "MAG": "2058731966",
                "DOI": "10.1086/587826",
                "CorpusId": 16919478,
                "PubMed": "18605534"
            },
            "abstract": "Machine learning methods, a family of statistical techniques with origins in the field of artificial intelligence, are recognized as holding great promise for the advancement of understanding and prediction about ecological phenomena. These modeling techniques are flexible enough to handle complex problems with multiple interacting elements and typically outcompete traditional approaches (e.g., generalized linear models), making them ideal for modeling ecological systems. Despite their inherent advantages, a review of the literature reveals only a modest use of these approaches in ecology as compared to other disciplines. One potential explanation for this lack of interest is that machine learning techniques do not fall neatly into the class of statistical modeling approaches with which most ecologists are familiar. In this paper, we provide an introduction to three machine learning approaches that can be broadly used by ecologists: classification and regression trees, artificial neural networks, and evolutionary computation. For each approach, we provide a brief background to the methodology, give examples of its application in ecology, describe model development and implementation, discuss strengths and weaknesses, explore the availability of statistical software, and provide an illustrative example. Although the ecological application of machine learning approaches has increased, there remains considerable skepticism with respect to the role of these techniques in ecology. Our review encourages a greater understanding of machine learning approaches and promotes their future application and utilization, while also providing a basis from which ecologists can make informed decisions about whether to select or avoid these approaches in their future modeling endeavors.",
            "referenceCount": 99,
            "citationCount": 606,
            "influentialCitationCount": 30,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://rydberg.biology.colostate.edu/poff/Public/poffpubs/Olden_etal_2008_Machine_QBR.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Environmental Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2008-06-01",
            "journal": {
                "name": "The Quarterly Review of Biology",
                "volume": "83"
            },
            "citationStyles": {
                "bibtex": "@Article{Olden2008MachineLM,\n author = {J. Olden and J. Lawler and N. L. Poff},\n booktitle = {The Quarterly review of biology},\n journal = {The Quarterly Review of Biology},\n pages = {171 - 193},\n title = {Machine Learning Methods Without Tears: A Primer for Ecologists},\n volume = {83},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9c4da62e9e89e65ac78ee271e424e8b498053e8c",
            "@type": "ScholarlyArticle",
            "paperId": "9c4da62e9e89e65ac78ee271e424e8b498053e8c",
            "corpusId": 60502900,
            "url": "https://www.semanticscholar.org/paper/9c4da62e9e89e65ac78ee271e424e8b498053e8c",
            "title": "Advances in kernel methods: support vector learning",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1999,
            "externalIds": {
                "MAG": "1604938182",
                "DOI": "10.5555/299094",
                "CorpusId": 60502900
            },
            "abstract": "Introduction to support vector learning roadmap. Part 1 Theory: three remarks on the support vector method of function estimation, Vladimir Vapnik generalization performance of support vector machines and other pattern classifiers, Peter Bartlett and John Shawe-Taylor Bayesian voting schemes and large margin classifiers, Nello Cristianini and John Shawe-Taylor support vector machines, reproducing kernel Hilbert spaces, and randomized GACV, Grace Wahba geometry and invariance in kernel based methods, Christopher J.C. Burges on the annealed VC entropy for margin classifiers - a statistical mechanics study, Manfred Opper entropy numbers, operators and support vector kernels, Robert C. Williamson et al. Part 2 Implementations: solving the quadratic programming problem arising in support vector classification, Linda Kaufman making large-scale support vector machine learning practical, Thorsten Joachims fast training of support vector machines using sequential minimal optimization, John C. Platt. Part 3 Applications: support vector machines for dynamic reconstruction of a chaotic system, Davide Mattera and Simon Haykin using support vector machines for time series prediction, Klaus-Robert Muller et al pairwise classification and support vector machines, Ulrich Kressel. Part 4 Extensions of the algorithm: reducing the run-time complexity in support vector machines, Edgar E. Osuna and Federico Girosi support vector regression with ANOVA decomposition kernels, Mark O. Stitson et al support vector density estimation, Jason Weston et al combining support vector and mathematical programming methods for classification, Bernhard Scholkopf et al.",
            "referenceCount": 234,
            "citationCount": 5291,
            "influentialCitationCount": 200,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1999-02-08",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Scholkopf1999AdvancesIK,\n author = {B. Scholkopf and C. Burges and Alex Smola},\n title = {Advances in kernel methods: support vector learning},\n year = {1999}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7550a05bf00f7b24aed9c1ac3ef000575388d21c",
            "@type": "ScholarlyArticle",
            "paperId": "7550a05bf00f7b24aed9c1ac3ef000575388d21c",
            "corpusId": 61116019,
            "url": "https://www.semanticscholar.org/paper/7550a05bf00f7b24aed9c1ac3ef000575388d21c",
            "title": "Making large scale SVM learning practical",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1998,
            "externalIds": {
                "MAG": "2356744885",
                "DOI": "10.17877/DE290R-14262",
                "CorpusId": 61116019
            },
            "abstract": "Training a support vector machine SVM leads to a quadratic optimization problem with bound constraints and one linear equality constraint. Despite the fact that this type of problem is well understood, there are many issues to be considered in designing an SVM learner. In particular, for large learning tasks with many training examples on the shelf optimization techniques for general quadratic programs quickly become intractable in their memory and time requirements. SVM light is an implementation of an SVM learner which addresses the problem of large tasks. This chapter presents algorithmic and computational results developed for SVM light V 2.0, which make large-scale SVM training more practical. The results give guidelines for the application of SVMs to large domains.",
            "referenceCount": 14,
            "citationCount": 5535,
            "influentialCitationCount": 468,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "Technical reports",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Article{Joachims1998MakingLS,\n author = {T. Joachims},\n journal = {Technical reports},\n title = {Making large scale SVM learning practical},\n year = {1998}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:bcce96a2a074448953fc61a29a84afbdfc8db55a",
            "@type": "ScholarlyArticle",
            "paperId": "bcce96a2a074448953fc61a29a84afbdfc8db55a",
            "corpusId": 51730029,
            "url": "https://www.semanticscholar.org/paper/bcce96a2a074448953fc61a29a84afbdfc8db55a",
            "title": "Online Learning and Online Convex Optimization",
            "venue": "Found. Trends Mach. Learn.",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2012,
            "externalIds": {
                "MAG": "2077723394",
                "DBLP": "journals/ftml/Shalev-Shwartz12",
                "DOI": "10.1561/2200000018",
                "CorpusId": 51730029
            },
            "abstract": "Online learning is a well established learning paradigm which has both theoretical and practical appeals. The goal of online learning is to make a sequence of accurate predictions given knowledge of the correct answer to previous prediction tasks and possibly additional available information. Online learning has been studied in several research fields including game theory, information theory, and machine learning. It also became of great interest to practitioners due the recent emergence of large scale applications such as online advertisement placement and online web ranking. In this survey we provide a modern overview of online learning. Our goal is to give the reader a sense of some of the interesting ideas and in particular to underscore the centrality of convexity in deriving efficient online learning algorithms. We do not mean to be comprehensive but rather to give a high-level, rigorous yet easy to follow, survey.",
            "referenceCount": 48,
            "citationCount": 1977,
            "influentialCitationCount": 253,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2012-03-25",
            "journal": {
                "name": "Found. Trends Mach. Learn.",
                "volume": "4"
            },
            "citationStyles": {
                "bibtex": "@Article{Shalev-Shwartz2012OnlineLA,\n author = {S. Shalev-Shwartz},\n booktitle = {Found. Trends Mach. Learn.},\n journal = {Found. Trends Mach. Learn.},\n pages = {107-194},\n title = {Online Learning and Online Convex Optimization},\n volume = {4},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b51a3a7d676df7947a0b28fb902a5a9f0bdf54ee",
            "@type": "ScholarlyArticle",
            "paperId": "b51a3a7d676df7947a0b28fb902a5a9f0bdf54ee",
            "corpusId": 38663643,
            "url": "https://www.semanticscholar.org/paper/b51a3a7d676df7947a0b28fb902a5a9f0bdf54ee",
            "title": "IAM Graph Database Repository for Graph Based Pattern Recognition and Machine Learning",
            "venue": "SSPR/SPR",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2008,
            "externalIds": {
                "DBLP": "conf/sspr/RiesenB08",
                "MAG": "1492230849",
                "DOI": "10.1007/978-3-540-89689-0_33",
                "CorpusId": 38663643
            },
            "abstract": null,
            "referenceCount": 33,
            "citationCount": 495,
            "influentialCitationCount": 78,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/978-3-540-89689-0_33.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2008-12-04",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Riesen2008IAMGD,\n author = {Kaspar Riesen and H. Bunke},\n booktitle = {SSPR/SPR},\n pages = {287-297},\n title = {IAM Graph Database Repository for Graph Based Pattern Recognition and Machine Learning},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:302d64cebed714f453c6a1e63effb6fe884a7e80",
            "@type": "ScholarlyArticle",
            "paperId": "302d64cebed714f453c6a1e63effb6fe884a7e80",
            "corpusId": 14580078,
            "url": "https://www.semanticscholar.org/paper/302d64cebed714f453c6a1e63effb6fe884a7e80",
            "title": "Java-ML: A Machine Learning Library",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2009,
            "externalIds": {
                "MAG": "1760882240",
                "DBLP": "journals/jmlr/AbeelPS09",
                "DOI": "10.5555/1577069.1577103",
                "CorpusId": 14580078
            },
            "abstract": "Java-ML is a collection of machine learning and data mining algorithms, which aims to be a readily usable and easily extensible API for both software developers and research scientists. The interfaces for each type of algorithm are kept simple and algorithms strictly follow their respective interface. Comparing different classifiers or clustering algorithms is therefore straightforward, and implementing new algorithms is also easy. The implementations of the algorithms are clearly written, properly documented and can thus be used as a reference. The library is written in Java and is available from http://java-ml.sourceforge.net/ under the GNU GPL license.",
            "referenceCount": 4,
            "citationCount": 134,
            "influentialCitationCount": 5,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2009-12-01",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "10"
            },
            "citationStyles": {
                "bibtex": "@Article{Abeel2009JavaMLAM,\n author = {T. Abeel and Y. Peer and Y. Saeys},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {931-934},\n title = {Java-ML: A Machine Learning Library},\n volume = {10},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:22feb6532228392457664becc48b3096d9858505",
            "@type": "ScholarlyArticle",
            "paperId": "22feb6532228392457664becc48b3096d9858505",
            "corpusId": 15093314,
            "url": "https://www.semanticscholar.org/paper/22feb6532228392457664becc48b3096d9858505",
            "title": "Asymptotic Equivalence of Bayes Cross Validation and Widely Applicable Information Criterion in Singular Learning Theory",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2010,
            "externalIds": {
                "MAG": "2123222757",
                "ArXiv": "1004.2316",
                "DBLP": "journals/corr/abs-1004-2316",
                "DOI": "10.5555/1756006.1953045",
                "CorpusId": 15093314
            },
            "abstract": "In regular statistical models, the leave-one-out cross-validation is asymptotically equivalent to the Akaike information criterion. However, since many learning machines are singular statistical models, the asymptotic behavior of the cross-validation remains unknown. In previous studies, we established the singular learning theory and proposed a widely applicable information criterion, the expectation value of which is asymptotically equal to the average Bayes generalization loss. In the present paper, we theoretically compare the Bayes cross-validation loss and the widely applicable information criterion and prove two theorems. First, the Bayes cross-validation loss is asymptotically equivalent to the widely applicable information criterion as a random variable. Therefore, model selection and hyperparameter optimization using these two values are asymptotically equivalent. Second, the sum of the Bayes generalization error and the Bayes cross-validation error is asymptotically equal to 2\u03bb/n, where \u03bb is the real log canonical threshold and n is the number of training samples. Therefore the relation between the cross-validation error and the generalization error is determined by the algebraic geometrical structure of a learning machine. We also clarify that the deviance information criteria are different from the Bayes cross-validation and the widely applicable information criterion.",
            "referenceCount": 61,
            "citationCount": 2078,
            "influentialCitationCount": 395,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2010-03-01",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1004.2316"
            },
            "citationStyles": {
                "bibtex": "@Article{Watanabe2010AsymptoticEO,\n author = {Sumio Watanabe},\n booktitle = {Journal of machine learning research},\n journal = {ArXiv},\n title = {Asymptotic Equivalence of Bayes Cross Validation and Widely Applicable Information Criterion in Singular Learning Theory},\n volume = {abs/1004.2316},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:eb0c1e3d880e361b7ff61e5ac1d489cb75c55ece",
            "@type": "ScholarlyArticle",
            "paperId": "eb0c1e3d880e361b7ff61e5ac1d489cb75c55ece",
            "corpusId": 5928200,
            "url": "https://www.semanticscholar.org/paper/eb0c1e3d880e361b7ff61e5ac1d489cb75c55ece",
            "title": "Adaptive computation and machine learning",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1998,
            "externalIds": {
                "MAG": "391985582",
                "CorpusId": 5928200
            },
            "abstract": "A complete list of books published in The Adaptive Computation and Machine Learning series appears at the back of this book.",
            "referenceCount": 985,
            "citationCount": 938,
            "influentialCitationCount": 23,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Dietterich1998AdaptiveCA,\n author = {Thomas G. Dietterich},\n title = {Adaptive computation and machine learning},\n year = {1998}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ccd1282aea3cc7c3d40300d82472fc5f9f54cb8e",
            "@type": "ScholarlyArticle",
            "paperId": "ccd1282aea3cc7c3d40300d82472fc5f9f54cb8e",
            "corpusId": 556331,
            "url": "https://www.semanticscholar.org/paper/ccd1282aea3cc7c3d40300d82472fc5f9f54cb8e",
            "title": "Online Learning for Matrix Factorization and Sparse Coding",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2009,
            "externalIds": {
                "MAG": "2949811050",
                "DBLP": "journals/corr/abs-0908-0050",
                "ArXiv": "0908.0050",
                "DOI": "10.5555/1756006.1756008",
                "CorpusId": 556331
            },
            "abstract": "Sparse coding--that is, modelling data vectors as sparse linear combinations of basis elements--is widely used in machine learning, neuroscience, signal processing, and statistics. This paper focuses on the large-scale matrix factorization problem that consists of learning the basis set in order to adapt it to specific data. Variations of this problem include dictionary learning in signal processing, non-negative matrix factorization and sparse principal component analysis. In this paper, we propose to address these tasks with a new online optimization algorithm, based on stochastic approximations, which scales up gracefully to large data sets with millions of training samples, and extends naturally to various matrix factorization formulations, making it suitable for a wide range of learning problems. A proof of convergence is presented, along with experiments with natural images and genomic data demonstrating that it leads to state-of-the-art performance in terms of speed and optimization for both small and large data sets.",
            "referenceCount": 95,
            "citationCount": 2573,
            "influentialCitationCount": 300,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2009-08-01",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "11"
            },
            "citationStyles": {
                "bibtex": "@Article{Mairal2009OnlineLF,\n author = {J. Mairal and F. Bach and J. Ponce and G. Sapiro},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {19-60},\n title = {Online Learning for Matrix Factorization and Sparse Coding},\n volume = {11},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9512facd37bba2ff1ece31900c08901bb011f1ce",
            "@type": "ScholarlyArticle",
            "paperId": "9512facd37bba2ff1ece31900c08901bb011f1ce",
            "corpusId": 9746839,
            "url": "https://www.semanticscholar.org/paper/9512facd37bba2ff1ece31900c08901bb011f1ce",
            "title": "Using Machine Teaching to Identify Optimal Training-Set Attacks on Machine Learners",
            "venue": "AAAI Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:bdc2e585-4e48-4e36-8af1-6d859763d405",
                "name": "AAAI Conference on Artificial Intelligence",
                "alternate_names": [
                    "National Conference on Artificial Intelligence",
                    "National Conf Artif Intell",
                    "AAAI Conf Artif Intell",
                    "AAAI"
                ],
                "issn": null,
                "url": "http://www.aaai.org/"
            },
            "year": 2015,
            "externalIds": {
                "DBLP": "conf/aaai/MeiZ15",
                "MAG": "2293844262",
                "DOI": "10.1609/aaai.v29i1.9569",
                "CorpusId": 9746839
            },
            "abstract": "\n \n We investigate a problem at the intersection of machine learning and security: training-set attacks on machine learners. In such attacks an attacker contaminates the training data so that a specific learning algorithm would produce a model profitable to the attacker. Understanding training-set attacks is important as more intelligent agents (e.g. spam filters and robots) are equipped with learning capability and can potentially be hacked via data they receive from the environment. This paper identifies the optimal training-set attack on a broad family of machine learners. First we show that optimal training-set attack can be formulated as a bilevel optimization problem. Then we show that for machine learners with certain Karush-Kuhn-Tucker conditions we can solve the bilevel problem efficiently using gradient methods on an implicit function. As examples, we demonstrate optimal training-set attacks on Support VectorMachines, logistic regression, and linear regression with extensive experiments. Finally, we discuss potential defenses against such attacks.\n \n",
            "referenceCount": 29,
            "citationCount": 370,
            "influentialCitationCount": 33,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/9569/9428",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-01-25",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Mei2015UsingMT,\n author = {Shike Mei and Xiaojin Zhu},\n booktitle = {AAAI Conference on Artificial Intelligence},\n pages = {2871-2877},\n title = {Using Machine Teaching to Identify Optimal Training-Set Attacks on Machine Learners},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:664b701a39371c5356754dc72cea1349233c8506",
            "@type": "ScholarlyArticle",
            "paperId": "664b701a39371c5356754dc72cea1349233c8506",
            "corpusId": 12484204,
            "url": "https://www.semanticscholar.org/paper/664b701a39371c5356754dc72cea1349233c8506",
            "title": "Computer Systems That Learn: Classification and Prediction Methods from Statistics, Neural Nets, Machine Learning and Expert Systems",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1990,
            "externalIds": {
                "MAG": "1568619188",
                "DBLP": "books/mk/WeissK90",
                "CorpusId": 12484204
            },
            "abstract": "Preface 1 Overview of Learning Systems 1.1 What is a Learning System? 1.2 Motivation for Building Learning Systems 1.3 Types of Practical Empirical Learning Systems 1.3.1 Common Theme: The Classification Model 1.3.2 Let the Data Speak 1.4 What's New in Learning Methods 1.4.1 The Impact of New Technology 1.5 Outline of the Book 1.6 Bibliographical and Historical Remarks 2 How to Estimate the True Performance of a Learning System 2.1 The Importance of Unbiased Error Rate Estimation 2.2. What is an Error? 2.2.1 Costs and Risks 2.3 Apparent Error Rate Estimates 2.4 Too Good to Be True: Overspecialization 2.5 True Error Rate Estimation 2.5.1 The Idealized Model for Unlimited Samples 2.5.2 Train-and Test Error Rate Estimation 2.5.3 Resampling Techniques 2.5.4 Finding the Right Complexity Fit 2.6 Getting the Most Out of the Data 2.7 Classifier Complexity and Feature Dimensionality 2.7.1 Expected Patterns of Classifier Behavior 2.8 What Can Go Wrong? 2.8.1 Poor Features, Data Errors, and Mislabeled Classes 2.8.2 Unrepresentative Samples 2.9 How Close to the Truth? 2.10 Common Mistakes in Performance Analysis 2.11 Bibliographical and Historical Remarks 3 Statistical Pattern Recognition 3.1 Introduction and Overview 3.2 A Few Sample Applications 3.3 Bayesian Classifiers 3.3.1 Direct Application of the Bayes Rule 3.4 Linear Discriminants 3.4.1 The Normality Assumption and Discriminant Functions 3.4.2 Logistic Regression 3.5 Nearest Neighbor Methods 3.6 Feature Selection 3.7 Error Rate Analysis 3.8 Bibliographical and Historical Remarks 4 Neural Nets 4.1 Introduction and Overview 4.2 Perceptrons 4.2.1 Least Mean Square Learning Systems 4.2.2 How Good Is a Linear Separation Network? 4.3 Multilayer Neural Networks 4.3.1 Back-Propagation 4.3.2 The Practical Application of Back-Propagation 4.4 Error Rate and Complexity Fit Estimation 4.5 Improving on Standard Back-Propagation 4.6 Bibliographical and Historical Remarks 5 Machine Learning: Easily Understood Decision Rules 5.1 Introduction and Overview 5.2 Decision Trees 5.2.1 Finding the Perfect Tree 5.2.2 The Incredible Shrinking Tree 5.2.3 Limitations of Tree Induction Methods 5.3 Rule Induction 5.3.1 Predictive Value Maximization 5.4 Bibliographical and Historical Remarks 6 Which Technique is Best? 6.1 What's Important in Choosing a Classifier? 6.1.1 Prediction Accuracy 6.1.2 Speed of Learning and Classification 6.1.3 Explanation and Insight 6.2 So, How Do I Choose a Learning System? 6.3 Variations on the Standard Problem 6.3.1 Missing Data 6.3.2 Incremental Learning 6.4 Future Prospects for Improved Learning Methods 6.5 Bibliographical and Historical Remarks 7 Expert Systems 7.1 Introduction and Overview 7.1.1 Why Build Expert Systems? New vs. Old Knowledge 7.2 Estimating Error Rates for Expert Systems 7.3 Complexity of Knowledge Bases 7.3.1 How Many Rules Are Too Many? 7.4 Knowledge Base Example 7.5 Empirical Analysis of Knowledge Bases 7.6 Future: Combined Learning and Expert Systems 7.7 Bibliographical and Historical Remarks References Author Index Subject Index",
            "referenceCount": 0,
            "citationCount": 1057,
            "influentialCitationCount": 39,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Weiss1990ComputerST,\n author = {S. Weiss and C. Kulikowski},\n title = {Computer Systems That Learn: Classification and Prediction Methods from Statistics, Neural Nets, Machine Learning and Expert Systems},\n year = {1990}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c0927ffb79810318daab8821068629975cab67ad",
            "@type": "ScholarlyArticle",
            "paperId": "c0927ffb79810318daab8821068629975cab67ad",
            "corpusId": 9413935,
            "url": "https://www.semanticscholar.org/paper/c0927ffb79810318daab8821068629975cab67ad",
            "title": "Deep Learning for Classification of Malware System Call Sequences",
            "venue": "Australasian Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "conf/ausai/KolosnjajiZWE16",
                "MAG": "2557513839",
                "DOI": "10.1007/978-3-319-50127-7_11",
                "CorpusId": 9413935
            },
            "abstract": null,
            "referenceCount": 37,
            "citationCount": 425,
            "influentialCitationCount": 18,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-12-05",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Kolosnjaji2016DeepLF,\n author = {Bojan Kolosnjaji and Apostolis Zarras and George D. Webster and C. Eckert},\n booktitle = {Australasian Conference on Artificial Intelligence},\n pages = {137-149},\n title = {Deep Learning for Classification of Malware System Call Sequences},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4417f78b31546227784941bbd6f6532a177e60b8",
            "@type": "ScholarlyArticle",
            "paperId": "4417f78b31546227784941bbd6f6532a177e60b8",
            "corpusId": 6928185,
            "url": "https://www.semanticscholar.org/paper/4417f78b31546227784941bbd6f6532a177e60b8",
            "title": "Deep Learning using Linear Support Vector Machines",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2013,
            "externalIds": {
                "MAG": "1546411676",
                "ArXiv": "1306.0239",
                "CorpusId": 6928185
            },
            "abstract": "Recently, fully-connected and convolutional neural networks have been trained to achieve state-of-the-art performance on a wide variety of tasks such as speech recognition, image classification, natural language processing, and bioinformatics. For classification tasks, most of these \"deep learning\" models employ the softmax activation function for prediction and minimize cross-entropy loss. In this paper, we demonstrate a small but consistent advantage of replacing the softmax layer with a linear support vector machine. Learning minimizes a margin-based loss instead of the cross-entropy loss. While there have been various combinations of neural nets and SVMs in prior art, our results using L2-SVMs show that by simply replacing softmax with linear SVMs gives significant gains on popular deep learning datasets MNIST, CIFAR-10, and the ICML 2013 Representation Learning Workshop's face expression recognition challenge.",
            "referenceCount": 25,
            "citationCount": 822,
            "influentialCitationCount": 82,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2013-06-02",
            "journal": {
                "name": "arXiv: Learning",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Article{Tang2013DeepLU,\n author = {Yichuan Tang},\n journal = {arXiv: Learning},\n title = {Deep Learning using Linear Support Vector Machines},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:464ebd4c86542e1610d67a9bc8810b8a7c109efe",
            "@type": "ScholarlyArticle",
            "paperId": "464ebd4c86542e1610d67a9bc8810b8a7c109efe",
            "corpusId": 2891159,
            "url": "https://www.semanticscholar.org/paper/464ebd4c86542e1610d67a9bc8810b8a7c109efe",
            "title": "Convex Incremental Extreme Learning Machine",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2007,
            "externalIds": {
                "CorpusId": 2891159
            },
            "abstract": "Unlike the conventional neural network theories and implementations, Huang et al. [Universal approximation using incremental constructive feedforward networks with random hidden nodes, IEEE Transactions on Neural Networks 17(4) (2006) 879\u2013892] have recently proposed a new theory to show that single-hidden-layer feedforward networks (SLFNs) with randomly generated additive or radial basis function (RBF) hidden nodes (according to any continuous sampling distribution) can work as universal approximators and the resulting incremental extreme learning machine (I-ELM) outperforms many popular learning algorithms. I-ELM randomly generates the hidden nodes and analytically calculates the output weights of SLFNs, however, I-ELM does not recalculate the output weights of all the existing nodes when a new node is added. This paper shows that while retaining the same simplicity, the convergence rate of I-ELM can be further improved by recalculating the output weights of the existing nodes based on a convex optimization method when a new hidden node is randomly added. Furthermore, we show that given a type of piecewise continuous computational hidden nodes (possibly not neural alike nodes), if SLFNs f n \u00f0x\u00de \u00bc P n i\u00bc1 b i G\u00f0x; a i ; b i \u00de can work as universal approximators with adjustable hidden node parameters, from a function approximation point of view the hidden node parameters of such ''generalized'' SLFNs (including sigmoid networks, RBF networks, trigonometric networks, threshold networks, fuzzy inference systems, fully complex neural networks, high-order networks, ridge polynomial networks, wavelet networks, etc.) can actually be randomly generated according to any continuous sampling distribution. In theory, the parameters of these SLFNs can be analytically determined by ELM instead of being tuned.",
            "referenceCount": 27,
            "citationCount": 1043,
            "influentialCitationCount": 30,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Huang2007ConvexIE,\n author = {G. Huang and Lei Chen},\n title = {Convex Incremental Extreme Learning Machine},\n year = {2007}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c9d60858f1cbe6b7eb36473b7d37ff4b73c31af8",
            "@type": "ScholarlyArticle",
            "paperId": "c9d60858f1cbe6b7eb36473b7d37ff4b73c31af8",
            "corpusId": 60807209,
            "url": "https://www.semanticscholar.org/paper/c9d60858f1cbe6b7eb36473b7d37ff4b73c31af8",
            "title": "Machine Learning from Imbalanced Data Sets 101",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2008,
            "externalIds": {
                "MAG": "1533970595",
                "CorpusId": 60807209
            },
            "abstract": "For research to progress most effectively, we first should establish common ground regarding just what is the problem that imbalanced data sets present to machine learning systems. Why and when should imbalanced data sets be problematic? When is the problem simply an artifact of easily rectified design choices? I will try to pick the low-hanging fruit and share them with the rest of the workshop participants. Specifically, I would like to discuss what the problem is not. I hope this will lead to a profitable discussion of what the problem indeed is, and how it might be addressed most effectively.",
            "referenceCount": 7,
            "citationCount": 456,
            "influentialCitationCount": 8,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2008-11-17",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Provost2008MachineLF,\n author = {F. Provost},\n title = {Machine Learning from Imbalanced Data Sets 101},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:83cf4b2f39bcc802b09fd59b69e23068447b26b7",
            "@type": "ScholarlyArticle",
            "paperId": "83cf4b2f39bcc802b09fd59b69e23068447b26b7",
            "corpusId": 3666937,
            "url": "https://www.semanticscholar.org/paper/83cf4b2f39bcc802b09fd59b69e23068447b26b7",
            "title": "Multi-Task Learning for Multiple Language Translation",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                "name": "Annual Meeting of the Association for Computational Linguistics",
                "alternate_names": [
                    "Annu Meet Assoc Comput Linguistics",
                    "Meeting of the Association for Computational Linguistics",
                    "ACL",
                    "Meet Assoc Comput Linguistics"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/anthology/venues/acl/"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "2251743902",
                "DBLP": "conf/acl/DongWHYW15",
                "ACL": "P15-1166",
                "DOI": "10.3115/v1/P15-1166",
                "CorpusId": 3666937
            },
            "abstract": "In this paper, we investigate the problem of learning a machine translation model that can simultaneously translate sentences from one source language to multiple target languages. Our solution is inspired by the recently proposed neural machine translation model which generalizes machine translation as a sequence learning problem. We extend the neural machine translation to a multi-task learning framework which shares source language representation and separates the modeling of different target language translation. Our framework can be applied to situations where either large amounts of parallel data or limited parallel data is available. Experiments show that our multi-task learning model is able to achieve significantly higher translation quality over individually learned model in both situations on the data sets publicly available.",
            "referenceCount": 21,
            "citationCount": 573,
            "influentialCitationCount": 50,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/P15-1166.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-07-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Dong2015MultiTaskLF,\n author = {Daxiang Dong and Hua Wu and W. He and Dianhai Yu and Haifeng Wang},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {1723-1732},\n title = {Multi-Task Learning for Multiple Language Translation},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:50feafd2cdafdfb1eead14388f6210f6b467eaa0",
            "@type": "ScholarlyArticle",
            "paperId": "50feafd2cdafdfb1eead14388f6210f6b467eaa0",
            "corpusId": 1386147,
            "url": "https://www.semanticscholar.org/paper/50feafd2cdafdfb1eead14388f6210f6b467eaa0",
            "title": "Pareto-Based Multiobjective Machine Learning: An Overview and Case Studies",
            "venue": "IEEE Transactions on Systems Man and Cybernetics Part C (Applications and Reviews)",
            "publicationVenue": {
                "id": "urn:research:ecb11fdd-9e59-482f-a3b6-0cb14372306c",
                "name": "IEEE Transactions on Systems Man and Cybernetics Part C (Applications and Reviews)",
                "alternate_names": [
                    "IEEE Trans Syst Man Cybern Part C (applications Rev"
                ],
                "issn": "1094-6977",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5326"
            },
            "year": 2008,
            "externalIds": {
                "DBLP": "journals/tsmc/JinS08",
                "MAG": "2112299196",
                "DOI": "10.1109/TSMCC.2008.919172",
                "CorpusId": 1386147
            },
            "abstract": "Machine learning is inherently a multiobjective task. Traditionally, however, either only one of the objectives is adopted as the cost function or multiple objectives are aggregated to a scalar cost function. This can be mainly attributed to the fact that most conventional learning algorithms can only deal with a scalar cost function. Over the last decade, efforts on solving machine learning problems using the Pareto-based multiobjective optimization methodology have gained increasing impetus, particularly due to the great success of multiobjective optimization using evolutionary algorithms and other population-based stochastic search methods. It has been shown that Pareto-based multiobjective learning approaches are more powerful compared to learning algorithms with a scalar cost function in addressing various topics of machine learning, such as clustering, feature selection, improvement of generalization ability, knowledge extraction, and ensemble generation. One common benefit of the different multiobjective learning approaches is that a deeper insight into the learning problem can be gained by analyzing the Pareto front composed of multiple Pareto-optimal solutions. This paper provides an overview of the existing research on multiobjective machine learning, focusing on supervised learning. In addition, a number of case studies are provided to illustrate the major benefits of the Pareto-based approach to machine learning, e.g., how to identify interpretable models and models that can generalize on unseen data from the obtained Pareto-optimal solutions. Three approaches to Pareto-based multiobjective ensemble generation are compared and discussed in detail. Finally, potentially interesting topics in multiobjective machine learning are suggested.",
            "referenceCount": 94,
            "citationCount": 387,
            "influentialCitationCount": 17,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.soft-computing.de/SMC0805.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2008-05-01",
            "journal": {
                "name": "IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)",
                "volume": "38"
            },
            "citationStyles": {
                "bibtex": "@Article{Jin2008ParetoBasedMM,\n author = {Yaochu Jin and B. Sendhoff},\n booktitle = {IEEE Transactions on Systems Man and Cybernetics Part C (Applications and Reviews)},\n journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},\n pages = {397-415},\n title = {Pareto-Based Multiobjective Machine Learning: An Overview and Case Studies},\n volume = {38},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:cf80cc34528273d8fbe17783efe802a6509e1562",
            "@type": "ScholarlyArticle",
            "paperId": "cf80cc34528273d8fbe17783efe802a6509e1562",
            "corpusId": 7027533,
            "url": "https://www.semanticscholar.org/paper/cf80cc34528273d8fbe17783efe802a6509e1562",
            "title": "Online dictionary learning for sparse coding",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2009,
            "externalIds": {
                "MAG": "2005876975",
                "DBLP": "conf/icml/MairalBPS09",
                "DOI": "10.1145/1553374.1553463",
                "CorpusId": 7027533
            },
            "abstract": "Sparse coding---that is, modelling data vectors as sparse linear combinations of basis elements---is widely used in machine learning, neuroscience, signal processing, and statistics. This paper focuses on learning the basis set, also called dictionary, to adapt it to specific data, an approach that has recently proven to be very effective for signal reconstruction and classification in the audio and image processing domains. This paper proposes a new online optimization algorithm for dictionary learning, based on stochastic approximations, which scales up gracefully to large datasets with millions of training samples. A proof of convergence is presented, along with experiments with natural images demonstrating that it leads to faster performance and better dictionaries than classical batch algorithms for both small and large datasets.",
            "referenceCount": 29,
            "citationCount": 2278,
            "influentialCitationCount": 246,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://conservancy.umn.edu/bitstream/11299/180118/1/2249.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2009-06-14",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Mairal2009OnlineDL,\n author = {J. Mairal and F. Bach and J. Ponce and G. Sapiro},\n booktitle = {International Conference on Machine Learning},\n pages = {689-696},\n title = {Online dictionary learning for sparse coding},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a84f4fe31fcfb4ad92c995dba0fc09ed8fe6a4f4",
            "@type": "ScholarlyArticle",
            "paperId": "a84f4fe31fcfb4ad92c995dba0fc09ed8fe6a4f4",
            "corpusId": 15667091,
            "url": "https://www.semanticscholar.org/paper/a84f4fe31fcfb4ad92c995dba0fc09ed8fe6a4f4",
            "title": "Exploiting Machine Learning to Subvert Your Spam Filter",
            "venue": "USENIX Workshop on Large-Scale Exploits and Emergent Threats",
            "publicationVenue": {
                "id": "urn:research:be204d98-ec84-49d8-ae70-bf48866e72a2",
                "name": "USENIX Workshop on Large-Scale Exploits and Emergent Threats",
                "alternate_names": [
                    "USENIX conference on Large-scale exploits and emergent threats",
                    "USENIX Workshop Large-scale Exploit Emergent Threat",
                    "USENIX conf Large-scale exploit emergent threat",
                    "LEET"
                ],
                "issn": null,
                "url": null
            },
            "year": 2008,
            "externalIds": {
                "DBLP": "conf/nsdi/NelsonBCJRSSTX08",
                "MAG": "2162552722",
                "CorpusId": 15667091
            },
            "abstract": "Using statistical machine learning for making security decisions introduces new vulnerabilities in large scale systems. This paper shows how an adversary can exploit statistical machine learning, as used in the SpamBayes spam filter, to render it useless--even if the adversary's access is limited to only 1% of the training messages. We further demonstrate a new class of focused attacks that successfully prevent victims from receiving specific email messages. Finally, we introduce two new types of defenses against these attacks.",
            "referenceCount": 20,
            "citationCount": 341,
            "influentialCitationCount": 17,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2008-04-15",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Article{Nelson2008ExploitingML,\n author = {B. Nelson and M. Barreno and F. J. Chi and A. Joseph and Benjamin I. P. Rubinstein and Udam Saini and Charles Sutton and J. D. Tygar and Kai Xia},\n booktitle = {USENIX Workshop on Large-Scale Exploits and Emergent Threats},\n pages = {7},\n title = {Exploiting Machine Learning to Subvert Your Spam Filter},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:08c81389b3ac4b8253d718a7cebe04a5536efa78",
            "@type": "ScholarlyArticle",
            "paperId": "08c81389b3ac4b8253d718a7cebe04a5536efa78",
            "corpusId": 1189640,
            "url": "https://www.semanticscholar.org/paper/08c81389b3ac4b8253d718a7cebe04a5536efa78",
            "title": "Improving Machine Learning Approaches to Coreference Resolution",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                "name": "Annual Meeting of the Association for Computational Linguistics",
                "alternate_names": [
                    "Annu Meet Assoc Comput Linguistics",
                    "Meeting of the Association for Computational Linguistics",
                    "ACL",
                    "Meet Assoc Comput Linguistics"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/anthology/venues/acl/"
            },
            "year": 2002,
            "externalIds": {
                "DBLP": "conf/acl/NgG02",
                "ACL": "P02-1014",
                "MAG": "2149956050",
                "DOI": "10.3115/1073083.1073102",
                "CorpusId": 1189640
            },
            "abstract": "We present a noun phrase coreference system that extends the work of Soon et al. (2001) and, to our knowledge, produces the best results to date on the MUC-6 and MUC-7 coreference resolution data sets --- F-measures of 70.4 and 63.4, respectively. Improvements arise from two sources: extra-linguistic changes to the learning framework and a large-scale expansion of the feature set to include more sophisticated linguistic knowledge.",
            "referenceCount": 13,
            "citationCount": 776,
            "influentialCitationCount": 97,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://dl.acm.org/ft_gateway.cfm?id=1073102&type=pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2002-07-06",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Ng2002ImprovingML,\n author = {Vincent Ng and Claire Gardent},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {104-111},\n title = {Improving Machine Learning Approaches to Coreference Resolution},\n year = {2002}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0628fdf728d0aba31be803a7d834c7f4b569408d",
            "@type": "ScholarlyArticle",
            "paperId": "0628fdf728d0aba31be803a7d834c7f4b569408d",
            "corpusId": 60266817,
            "url": "https://www.semanticscholar.org/paper/0628fdf728d0aba31be803a7d834c7f4b569408d",
            "title": "Imbalanced Learning: Foundations, Algorithms, and Applications",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2013,
            "externalIds": {
                "MAG": "571200655",
                "DOI": "10.1002/9781118646106",
                "CorpusId": 60266817
            },
            "abstract": "The first book of its kind to review the current status and future direction of the exciting new branch of machine learning/data mining called imbalanced learningImbalanced learning focuses on how an intelligent system can learn when it is provided with imbalanced data. Solving imbalanced learning problems is critical in numerous data-intensive networked systems, including surveillance, security, Internet, finance, biomedical, defense, and more. Due to the inherent complex characteristics of imbalanced data sets, learning from such data requires new understandings, principles, algorithms, and tools to transform vast amounts of raw data efficiently into information and knowledge representation. The first comprehensive look at this new branch of machine learning, this book offers a critical review of the problem of imbalanced learning, covering the state of the art in techniques, principles, and real-world applications. Featuring contributions from experts in both academia and industry, Imbalanced Learning: Foundations, Algorithms, and Applications provides chapter coverage on:Foundations of Imbalanced LearningImbalanced Datasets: From Sampling to ClassifiersEnsemble Methods for Class Imbalance LearningClass Imbalance Learning Methods for Support Vector MachinesClass Imbalance and Active LearningNonstationary Stream Data Learning with Imbalanced Class DistributionAssessment Metrics for Imbalanced LearningImbalanced Learning: Foundations, Algorithms, and Applications will help scientists and engineers learn how to tackle the problem of learning from imbalanced datasets, and gain insight into current developments in the field as well as future research directions.",
            "referenceCount": 0,
            "citationCount": 737,
            "influentialCitationCount": 51,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "2013-07-01",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{He2013ImbalancedLF,\n author = {Haibo He and Yunqian Ma},\n title = {Imbalanced Learning: Foundations, Algorithms, and Applications},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f4055c2f7198443038ec3a5ff44c3bae4f3f5fea",
            "@type": "ScholarlyArticle",
            "paperId": "f4055c2f7198443038ec3a5ff44c3bae4f3f5fea",
            "corpusId": 648945,
            "url": "https://www.semanticscholar.org/paper/f4055c2f7198443038ec3a5ff44c3bae4f3f5fea",
            "title": "Weka: Practical machine learning tools and techniques with Java implementations",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1999,
            "externalIds": {
                "MAG": "1543320899",
                "CorpusId": 648945
            },
            "abstract": "The Waikato Environment for Knowledge Analysis (Weka) is a comprehensive suite of Java class libraries that implement many state-of-the-art machine learning and data mining algorithms. Weka is freely available on the World-Wide Web and accompanies a new text on data mining [1] which documents and fully explains all the algorithms it contains. Applications written using the Weka class libraries can be run on any computer with a Web browsing capability; this allows users to apply machine learning techniques to their own data regardless of computer platform.",
            "referenceCount": 11,
            "citationCount": 805,
            "influentialCitationCount": 82,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1999-08-01",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Witten1999WekaPM,\n author = {I. Witten and E. Frank and Leonard E. Trigg and M. Hall and G. Holmes and S. Cunningham},\n title = {Weka: Practical machine learning tools and techniques with Java implementations},\n year = {1999}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d1208ac421cf8ff67b27d93cd19ae42b8d596f95",
            "@type": "ScholarlyArticle",
            "paperId": "d1208ac421cf8ff67b27d93cd19ae42b8d596f95",
            "corpusId": 8604637,
            "url": "https://www.semanticscholar.org/paper/d1208ac421cf8ff67b27d93cd19ae42b8d596f95",
            "title": "Deep learning with COTS HPC systems",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2013,
            "externalIds": {
                "DBLP": "conf/icml/CoatesHWWCN13",
                "MAG": "2162390675",
                "CorpusId": 8604637
            },
            "abstract": "Scaling up deep learning algorithms has been shown to lead to increased performance in benchmark tasks and to enable discovery of complex high-level features. Recent efforts to train extremely large networks (with over 1 billion parameters) have relied on cloudlike computing infrastructure and thousands of CPU cores. In this paper, we present technical details and results from our own system based on Commodity Off-The-Shelf High Performance Computing (COTS HPC) technology: a cluster of GPU servers with Infiniband interconnects and MPI. Our system is able to train 1 billion parameter networks on just 3 machines in a couple of days, and we show that it can scale to networks with over 11 billion parameters using just 16 machines. As this infrastructure is much more easily marshaled by others, the approach enables much wider-spread research with extremely large neural networks.",
            "referenceCount": 32,
            "citationCount": 719,
            "influentialCitationCount": 48,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2013-06-16",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Coates2013DeepLW,\n author = {Adam Coates and Brody Huval and Tao Wang and David J. Wu and Bryan Catanzaro and A. Ng},\n booktitle = {International Conference on Machine Learning},\n pages = {1337-1345},\n title = {Deep learning with COTS HPC systems},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:08dc94471605308669c8d3d8284ba94fcc93e345",
            "@type": "ScholarlyArticle",
            "paperId": "08dc94471605308669c8d3d8284ba94fcc93e345",
            "corpusId": 51612859,
            "url": "https://www.semanticscholar.org/paper/08dc94471605308669c8d3d8284ba94fcc93e345",
            "title": "Deep Learning in Microscopy Image Analysis: A Survey",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems",
            "publicationVenue": {
                "id": "urn:research:79c5a18d-0295-432c-aaa5-961d73de6d88",
                "name": "IEEE Transactions on Neural Networks and Learning Systems",
                "alternate_names": [
                    "IEEE Trans Neural Netw Learn Syst"
                ],
                "issn": "2162-237X",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5962385"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2768673271",
                "DBLP": "journals/tnn/XingXSLY18",
                "DOI": "10.1109/TNNLS.2017.2766168",
                "CorpusId": 51612859,
                "PubMed": "29989994"
            },
            "abstract": "Computerized microscopy image analysis plays an important role in computer aided diagnosis and prognosis. Machine learning techniques have powered many aspects of medical investigation and clinical practice. Recently, deep learning is emerging as a leading machine learning tool in computer vision and has attracted considerable attention in biomedical image analysis. In this paper, we provide a snapshot of this fast-growing field, specifically for microscopy image analysis. We briefly introduce the popular deep neural networks and summarize current deep learning achievements in various tasks, such as detection, segmentation, and classification in microscopy image analysis. In particular, we explain the architectures and the principles of convolutional neural networks, fully convolutional networks, recurrent neural networks, stacked autoencoders, and deep belief networks, and interpret their formulations or modelings for specific tasks on various microscopy images. In addition, we discuss the open challenges and the potential trends of future research in microscopy image analysis using deep learning.",
            "referenceCount": 0,
            "citationCount": 271,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2018-10-01",
            "journal": {
                "name": "IEEE Transactions on Neural Networks and Learning Systems",
                "volume": "29"
            },
            "citationStyles": {
                "bibtex": "@Article{Xing2018DeepLI,\n author = {F. Xing and Yuanpu Xie and H. Su and Fujun Liu and Lin Yang},\n booktitle = {IEEE Transactions on Neural Networks and Learning Systems},\n journal = {IEEE Transactions on Neural Networks and Learning Systems},\n pages = {4550-4568},\n title = {Deep Learning in Microscopy Image Analysis: A Survey},\n volume = {29},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e6dd83b2aa34c806596fc619ff3fbccf5f9830ab",
            "@type": "ScholarlyArticle",
            "paperId": "e6dd83b2aa34c806596fc619ff3fbccf5f9830ab",
            "corpusId": 7605995,
            "url": "https://www.semanticscholar.org/paper/e6dd83b2aa34c806596fc619ff3fbccf5f9830ab",
            "title": "Unsupervised Learning by Probabilistic Latent Semantic Analysis",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 2004,
            "externalIds": {
                "DBLP": "journals/ml/Hofmann01",
                "MAG": "2134731454",
                "DOI": "10.1023/A:1007617005950",
                "CorpusId": 7605995
            },
            "abstract": null,
            "referenceCount": 25,
            "citationCount": 2585,
            "influentialCitationCount": 315,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1023/A:1007617005950.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": "Machine Learning",
                "volume": "42"
            },
            "citationStyles": {
                "bibtex": "@Article{Hofmann2004UnsupervisedLB,\n author = {Thomas Hofmann},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {177-196},\n title = {Unsupervised Learning by Probabilistic Latent Semantic Analysis},\n volume = {42},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0948365ef39ef153e61e9569ade541cf881c7c2a",
            "@type": "ScholarlyArticle",
            "paperId": "0948365ef39ef153e61e9569ade541cf881c7c2a",
            "corpusId": 1113875,
            "url": "https://www.semanticscholar.org/paper/0948365ef39ef153e61e9569ade541cf881c7c2a",
            "title": "Learning the Kernel Matrix with Semidefinite Programming",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2002,
            "externalIds": {
                "MAG": "2763543653",
                "DBLP": "conf/icml/LanckrietCBGJ02",
                "CorpusId": 1113875
            },
            "abstract": "Kernel-based learning algorithms work by embedding the data into a Euclidean space, and then searching for linear relations among the embedded data points. The embedding is performed implicitly, by specifying the inner products between each pair of points in the embedding space. This information is contained in the so-called kernel matrix, a symmetric and positive semidefinite matrix that encodes the relative positions of all points. Specifying this matrix amounts to specifying the geometry of the embedding space and inducing a notion of similarity in the input space---classical model selection problems in machine learning. In this paper we show how the kernel matrix can be learned from data via semidefinite programming (SDP) techniques. When applied to a kernel matrix associated with both training and test data this gives a powerful transductive algorithm---using the labeled part of the data one can learn an embedding also for the unlabeled part. The similarity between test points is inferred from training points and their labels. Importantly, these learning problems are convex, so we obtain a method for learning both the model class and the function without local minima. Furthermore, this approach leads directly to a convex method for learning the 2-norm soft margin parameter in support vector machines, solving an important open problem.",
            "referenceCount": 30,
            "citationCount": 2532,
            "influentialCitationCount": 276,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2002-07-08",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Lanckriet2002LearningTK,\n author = {Gert R. G. Lanckriet and N. Cristianini and P. Bartlett and L. Ghaoui and Michael I. Jordan},\n booktitle = {Journal of machine learning research},\n pages = {323-330},\n title = {Learning the Kernel Matrix with Semidefinite Programming},\n year = {2002}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:bad620c25920edbaba8836032459b135669171c3",
            "@type": "ScholarlyArticle",
            "paperId": "bad620c25920edbaba8836032459b135669171c3",
            "corpusId": 1152351,
            "url": "https://www.semanticscholar.org/paper/bad620c25920edbaba8836032459b135669171c3",
            "title": "Machine Learning and Its Applications to Biology",
            "venue": "PLoS Comput. Biol.",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2007,
            "externalIds": {
                "PubMedCentral": "1904382",
                "MAG": "2137219016",
                "DBLP": "journals/ploscb/TarcaCCRD07",
                "DOI": "10.1371/journal.pcbi.0030116",
                "CorpusId": 1152351,
                "PubMed": "17604446"
            },
            "abstract": "The term machine learning refers to a set of topics dealing with the creation and evaluation of algorithms that facilitate pattern recognition, classification, and prediction, based on models derived from existing data. Two facets of mechanization should be acknowledged when considering machine learning in broad terms. Firstly, it is intended that the classification and prediction tasks can be accomplished by a suitably programmed computing machine. That is, the product of machine learning is a classifier that can be feasibly used on available hardware. Secondly, it is intended that the creation of the classifier should itself be highly mechanized, and should not involve too much human input. This second facet is inevitably vague, but the basic objective is that the use of automatic algorithm construction methods can minimize the possibility that human biases could affect the selection and performance of the algorithm. Both the creation of the algorithm and its operation to classify objects or predict events are to be based on concrete, observable data. \n \nThe history of relations between biology and the field of machine learning is long and complex. An early technique [1] for machine learning called the perceptron constituted an attempt to model actual neuronal behavior, and the field of artificial neural network (ANN) design emerged from this attempt. Early work on the analysis of translation initiation sequences [2] employed the perceptron to define criteria for start sites in Escherichia coli. Further artificial neural network architectures such as the adaptive resonance theory (ART) [3] and neocognitron [4] were inspired from the organization of the visual nervous system. In the intervening years, the flexibility of machine learning techniques has grown along with mathematical frameworks for measuring their reliability, and it is natural to hope that machine learning methods will improve the efficiency of discovery and understanding in the mounting volume and complexity of biological data. \n \nThis tutorial is structured in four main components. Firstly, a brief section reviews definitions and mathematical prerequisites. Secondly, the field of supervised learning is described. Thirdly, methods of unsupervised learning are reviewed. Finally, a section reviews methods and examples as implemented in the open source data analysis and visualization language R (http://www.r-project.org).",
            "referenceCount": 42,
            "citationCount": 532,
            "influentialCitationCount": 17,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.0030116&type=printable",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review",
                "Editorial"
            ],
            "publicationDate": "2007-06-01",
            "journal": {
                "name": "PLoS Computational Biology",
                "volume": "3"
            },
            "citationStyles": {
                "bibtex": "@Article{Tarca2007MachineLA,\n author = {A. Tarca and V. Carey and Xue-wen Chen and R. Romero and S. Dr\u0103ghici},\n booktitle = {PLoS Comput. Biol.},\n journal = {PLoS Computational Biology},\n title = {Machine Learning and Its Applications to Biology},\n volume = {3},\n year = {2007}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:07295d6836d8fadd4150328d33659e5022fccc2f",
            "@type": "ScholarlyArticle",
            "paperId": "07295d6836d8fadd4150328d33659e5022fccc2f",
            "corpusId": 14922439,
            "url": "https://www.semanticscholar.org/paper/07295d6836d8fadd4150328d33659e5022fccc2f",
            "title": "Structured machine learning: the next ten years",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 2008,
            "externalIds": {
                "DBLP": "journals/ml/DietterichDGMT08",
                "MAG": "2107261164",
                "DOI": "10.1007/s10994-008-5079-1",
                "CorpusId": 14922439
            },
            "abstract": null,
            "referenceCount": 108,
            "citationCount": 136,
            "influentialCitationCount": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/s10994-008-5079-1.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2008-10-01",
            "journal": {
                "name": "Machine Learning",
                "volume": "73"
            },
            "citationStyles": {
                "bibtex": "@Article{Dietterich2008StructuredML,\n author = {Thomas G. Dietterich and Pedro M. Domingos and L. Getoor and S. Muggleton and Prasad Tadepalli},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {3-23},\n title = {Structured machine learning: the next ten years},\n volume = {73},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:32b23a8d79c45d31e8a175ac62d436c37c6dcbe1",
            "@type": "ScholarlyArticle",
            "paperId": "32b23a8d79c45d31e8a175ac62d436c37c6dcbe1",
            "corpusId": 7721910,
            "url": "https://www.semanticscholar.org/paper/32b23a8d79c45d31e8a175ac62d436c37c6dcbe1",
            "title": "Statistical machine translation",
            "venue": "Conference of the Association for Machine Translation in the Americas",
            "publicationVenue": {
                "id": "urn:research:cd648e4a-f86b-4c14-8c82-0fc488d998ff",
                "name": "Conference of the Association for Machine Translation in the Americas",
                "alternate_names": [
                    "AMTA",
                    "Conf Assoc Mach Transl Am"
                ],
                "issn": null,
                "url": "http://www.amtaweb.org/"
            },
            "year": 2008,
            "externalIds": {
                "MAG": "2356129879",
                "DBLP": "journals/csur/Lopez08",
                "ACL": "2000.amta-tutorials.4",
                "DOI": "10.1145/1380584.1380586",
                "CorpusId": 7721910
            },
            "abstract": "Statistical machine translation (SMT) treats the translation of natural language as a machine learning problem. By examining many samples of human-produced translation, SMT algorithms automatically learn how to translate. SMT has made tremendous strides in less than two decades, and new ideas are constantly introduced. This survey presents a tutorial overview of the state of the art. We describe the context of the current research and then move to a formal problem description and an overview of the main subproblems: translation modeling, parameter estimation, and decoding. Along the way, we present a taxonomy of some different approaches within these areas. We conclude with an overview of evaluation and a discussion of future directions.",
            "referenceCount": 114,
            "citationCount": 1524,
            "influentialCitationCount": 178,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.ims.uni-stuttgart.de/institut/mitarbeiter/maletti/pub/slides/2014-08-02.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2008-08-01",
            "journal": {
                "name": "ACM Comput. Surv.",
                "volume": "40"
            },
            "citationStyles": {
                "bibtex": "@Article{Hassan2008StatisticalMT,\n author = {Hany Hassan and Kareem Darwish},\n booktitle = {Conference of the Association for Machine Translation in the Americas},\n journal = {ACM Comput. Surv.},\n pages = {8:1-8:49},\n title = {Statistical machine translation},\n volume = {40},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6f0cde3fcab0044f386b5b8a4244c371507bec15",
            "@type": "ScholarlyArticle",
            "paperId": "6f0cde3fcab0044f386b5b8a4244c371507bec15",
            "corpusId": 168956,
            "url": "https://www.semanticscholar.org/paper/6f0cde3fcab0044f386b5b8a4244c371507bec15",
            "title": "A Survey on Metric Learning for Feature Vectors and Structured Data",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2013,
            "externalIds": {
                "MAG": "1898424075",
                "DBLP": "journals/corr/BelletHS13",
                "ArXiv": "1306.6709",
                "CorpusId": 168956
            },
            "abstract": "The need for appropriate ways to measure the distance or similarity between data is ubiquitous in machine learning, pattern recognition and data mining, but handcrafting such good metrics for specific problems is generally difficult. This has led to the emergence of metric learning, which aims at automatically learning a metric from data and has attracted a lot of interest in machine learning and related fields for the past ten years. This survey paper proposes a systematic review of the metric learning literature, highlighting the pros and cons of each approach. We pay particular attention to Mahalanobis distance metric learning, a well-studied and successful framework, but additionally present a wide range of methods that have recently emerged as powerful alternatives, including nonlinear metric learning, similarity learning and local metric learning. Recent trends and extensions, such as semi-supervised metric learning, metric learning for histogram data and the derivation of generalization guarantees, are also covered. Finally, this survey addresses metric learning for structured data, in particular edit distance learning, and attempts to give an overview of the remaining challenges in metric learning for the years to come.",
            "referenceCount": 214,
            "citationCount": 648,
            "influentialCitationCount": 36,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2013-06-28",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1306.6709"
            },
            "citationStyles": {
                "bibtex": "@Article{Bellet2013ASO,\n author = {A. Bellet and Amaury Habrard and M. Sebban},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {A Survey on Metric Learning for Feature Vectors and Structured Data},\n volume = {abs/1306.6709},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:fcb926027ba5001f8f69dc0f1de5ded7d003b6af",
            "@type": "ScholarlyArticle",
            "paperId": "fcb926027ba5001f8f69dc0f1de5ded7d003b6af",
            "corpusId": 13236759,
            "url": "https://www.semanticscholar.org/paper/fcb926027ba5001f8f69dc0f1de5ded7d003b6af",
            "title": "A comparison of machine learning techniques for phishing detection",
            "venue": "APWG Symposium on Electronic Crime Research",
            "publicationVenue": {
                "id": "urn:research:bd032139-b763-4921-b5b8-7e0d9176b13d",
                "name": "APWG Symposium on Electronic Crime Research",
                "alternate_names": [
                    "eCrime",
                    "APWG Symp Electron Crime Res"
                ],
                "issn": null,
                "url": null
            },
            "year": 2007,
            "externalIds": {
                "DBLP": "conf/ecrime/Abu-NimehNWN07",
                "MAG": "2002964284",
                "DOI": "10.1145/1299015.1299021",
                "CorpusId": 13236759
            },
            "abstract": "There are many applications available for phishing detection. However, unlike predicting spam, there are only few studies that compare machine learning techniques in predicting phishing. The present study compares the predictive accuracy of several machine learning methods including Logistic Regression (LR), Classification and Regression Trees (CART), Bayesian Additive Regression Trees (BART), Support Vector Machines (SVM), Random Forests (RF), and Neural Networks (NNet) for predicting phishing emails. A data set of 2889 phishing and legitimate emails is used in the comparative study. In addition, 43 features are used to train and test the classifiers.",
            "referenceCount": 23,
            "citationCount": 425,
            "influentialCitationCount": 27,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2007-10-04",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Abu-Nimeh2007ACO,\n author = {Saeed Abu-Nimeh and D. Nappa and Xinlei Wang and S. Nair},\n booktitle = {APWG Symposium on Electronic Crime Research},\n pages = {60-69},\n title = {A comparison of machine learning techniques for phishing detection},\n year = {2007}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:23c3953fb45536c9129e86ac7a23098bd9f1381d",
            "@type": "ScholarlyArticle",
            "paperId": "23c3953fb45536c9129e86ac7a23098bd9f1381d",
            "corpusId": 38476,
            "url": "https://www.semanticscholar.org/paper/23c3953fb45536c9129e86ac7a23098bd9f1381d",
            "title": "Machine Learning for Sequential Data: A Review",
            "venue": "SSPR/SPR",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2002,
            "externalIds": {
                "MAG": "1529355025",
                "DBLP": "conf/sspr/Dietterich02",
                "DOI": "10.1007/3-540-70659-3_2",
                "CorpusId": 38476
            },
            "abstract": null,
            "referenceCount": 26,
            "citationCount": 729,
            "influentialCitationCount": 36,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/3-540-70659-3_2.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2002-08-06",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Dietterich2002MachineLF,\n author = {Thomas G. Dietterich},\n booktitle = {SSPR/SPR},\n pages = {15-30},\n title = {Machine Learning for Sequential Data: A Review},\n year = {2002}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:028165965fdf066821e1b65ac1de3ae6c503c30d",
            "@type": "ScholarlyArticle",
            "paperId": "028165965fdf066821e1b65ac1de3ae6c503c30d",
            "corpusId": 6655909,
            "url": "https://www.semanticscholar.org/paper/028165965fdf066821e1b65ac1de3ae6c503c30d",
            "title": "Editorial: On Machine Learning",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 1986,
            "externalIds": {
                "MAG": "2089933214",
                "DBLP": "journals/ml/Langley86",
                "DOI": "10.1023/A:1022687019898",
                "CorpusId": 6655909
            },
            "abstract": null,
            "referenceCount": 19,
            "citationCount": 859,
            "influentialCitationCount": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1023%2FA%3A1022687019898.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1986-03-25",
            "journal": {
                "name": "Machine Learning",
                "volume": "1"
            },
            "citationStyles": {
                "bibtex": "@Article{Langley1986EditorialOM,\n author = {P. Langley},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {5-10},\n title = {Editorial: On Machine Learning},\n volume = {1},\n year = {1986}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:56b37f88b109fd455a389642e1747f766f1be471",
            "@type": "ScholarlyArticle",
            "paperId": "56b37f88b109fd455a389642e1747f766f1be471",
            "corpusId": 7891873,
            "url": "https://www.semanticscholar.org/paper/56b37f88b109fd455a389642e1747f766f1be471",
            "title": "Using machine learning to focus iterative optimization",
            "venue": "IEEE/ACM International Symposium on Code Generation and Optimization",
            "publicationVenue": {
                "id": "urn:research:49699a88-4f79-403d-9250-6b3f361e5d7b",
                "name": "IEEE/ACM International Symposium on Code Generation and Optimization",
                "alternate_names": [
                    "CGO",
                    "Symposium on Code Generation and Optimization",
                    "IEEE/ACM Int Symp Code Gener Optim",
                    "Symp Code Gener Optim"
                ],
                "issn": null,
                "url": "http://www.cgo.org/"
            },
            "year": 2006,
            "externalIds": {
                "MAG": "2168519934",
                "DBLP": "conf/cgo/AgakovBCFFOTTW06",
                "DOI": "10.1109/CGO.2006.37",
                "CorpusId": 7891873
            },
            "abstract": "Iterative compiler optimization has been shown to outperform static approaches. This, however, is at the cost of large numbers of evaluations of the program. This paper develops a new methodology to reduce this number and hence speed up iterative optimization. It uses predictive modelling from the domain of machine learning to automatically focus search on those areas likely to give greatest performance. This approach is independent of search algorithm, search space or compiler infrastructure and scales gracefully with the compiler optimization space size. Off-line, a training set of programs is iteratively evaluated and the shape of the spaces and program features are modelled. These models are learnt and used to focus the iterative optimization of a new program. We evaluate two learnt models, an independent and Markov model, and evaluate their worth on two embedded platforms, the Texas Instrument C67I3 and the AMD Au1500. We show that such learnt models can speed up iterative search on large spaces by an order of magnitude. This translates into an average speedup of 1.22 on the TI C6713 and 1.27 on the AMD Au1500 in just 2 evaluations.",
            "referenceCount": 19,
            "citationCount": 458,
            "influentialCitationCount": 31,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2006-03-26",
            "journal": {
                "name": "International Symposium on Code Generation and Optimization (CGO'06)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Agakov2006UsingML,\n author = {F. Agakov and Edwin V. Bonilla and John Cavazos and Bj\u00f6rn Franke and G. Fursin and M. O\u2019Boyle and John Thomson and M. Toussaint and Christopher K. I. Williams},\n booktitle = {IEEE/ACM International Symposium on Code Generation and Optimization},\n journal = {International Symposium on Code Generation and Optimization (CGO'06)},\n pages = {11 pp.-305},\n title = {Using machine learning to focus iterative optimization},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5726c7b40fcc454b77d989656c085520bf6c15fa",
            "@type": "ScholarlyArticle",
            "paperId": "5726c7b40fcc454b77d989656c085520bf6c15fa",
            "corpusId": 710430,
            "url": "https://www.semanticscholar.org/paper/5726c7b40fcc454b77d989656c085520bf6c15fa",
            "title": "Multimodal learning with deep Boltzmann machines",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2012,
            "externalIds": {
                "MAG": "2164587673",
                "DBLP": "conf/nips/SrivastavaS12",
                "DOI": "10.5555/2627435.2697059",
                "CorpusId": 710430
            },
            "abstract": "Data often consists of multiple diverse modalities. For example, images are tagged with textual information and videos are accompanied by audio. Each modality is characterized by having distinct statistical properties. We propose a Deep Boltzmann Machine for learning a generative model of such multimodal data. We show that the model can be used to create fused representations by combining features across modalities. These learned representations are useful for classification and information retrieval. By sampling from the conditional distributions over each data modality, it is possible to create these representations even when some data modalities are missing. We conduct experiments on bimodal image-text and audio-video data. The fused representation achieves good classification results on the MIR-Flickr data set matching or outperforming other deep models as well as SVM based models that use Multiple Kernel Learning. We further demonstrate that this multimodal model helps classification and retrieval even when only unimodal data is available at test time.",
            "referenceCount": 44,
            "citationCount": 1638,
            "influentialCitationCount": 119,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2012-12-03",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "15"
            },
            "citationStyles": {
                "bibtex": "@Article{Srivastava2012MultimodalLW,\n author = {Nitish Srivastava and R. Salakhutdinov},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {2949-2980},\n title = {Multimodal learning with deep Boltzmann machines},\n volume = {15},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3493e0285fe329a710f54be2ef82350fdaafc991",
            "@type": "ScholarlyArticle",
            "paperId": "3493e0285fe329a710f54be2ef82350fdaafc991",
            "corpusId": 12448514,
            "url": "https://www.semanticscholar.org/paper/3493e0285fe329a710f54be2ef82350fdaafc991",
            "title": "Applications of Machine Learning to Cognitive Radio Networks",
            "venue": "IEEE wireless communications",
            "publicationVenue": {
                "id": "urn:research:2e8c30d0-78f6-4b13-aecb-9a65bff00635",
                "name": "IEEE wireless communications",
                "alternate_names": [
                    "IEEE Wirel Commun",
                    "IEEE Wireless Communications",
                    "IEEE wirel commun"
                ],
                "issn": "1536-1284",
                "url": "http://www.comsoc.org/wirelessmag/"
            },
            "year": 2007,
            "externalIds": {
                "DBLP": "journals/wc/ClancyHSO07",
                "MAG": "2149256602",
                "DOI": "10.1109/MWC.2007.4300983",
                "CorpusId": 12448514
            },
            "abstract": "Cognitive radio offers the promise of intelligent radios that can learn from and adapt to their environment. To date, most cognitive radio research has focused on policy-based radios that are hard-coded with a list of rules on how the radio should behave in certain scenarios. Some work has been done on radios with learning engines tailored for very specific applications. This article describes a concrete model for a generic cognitive radio to utilize a learning engine. The goal is to incorporate the results of the learning engine into a predicate calculus-based reasoning engine so that radios can remember lessons learned in the past and act quickly in the future. We also investigate the differences between reasoning and learning, and the fundamentals of when a particular application requires learning, and when simple reasoning is sufficient. The basic architecture is consistent with cognitive engines seen in AI research. The focus of this article is not to propose new machine learning algorithms, but rather to formalize their application to cognitive radio and develop a framework from within which they can be useful. We describe how our generic cognitive engine can tackle problems such as capacity maximization and dynamic spectrum access.",
            "referenceCount": 15,
            "citationCount": 323,
            "influentialCitationCount": 11,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2007-08-01",
            "journal": {
                "name": "IEEE Wireless Communications",
                "volume": "14"
            },
            "citationStyles": {
                "bibtex": "@Article{Clancy2007ApplicationsOM,\n author = {T. Clancy and Joe Hecker and E. P. Stuntebeck and Tim O'Shea},\n booktitle = {IEEE wireless communications},\n journal = {IEEE Wireless Communications},\n title = {Applications of Machine Learning to Cognitive Radio Networks},\n volume = {14},\n year = {2007}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1409df3208e6e05f4f788037355bb5bf7c151c5e",
            "@type": "ScholarlyArticle",
            "paperId": "1409df3208e6e05f4f788037355bb5bf7c151c5e",
            "corpusId": 59644696,
            "url": "https://www.semanticscholar.org/paper/1409df3208e6e05f4f788037355bb5bf7c151c5e",
            "title": "Machine Learning and Data Mining: Introduction to Principles and Algorithms",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2007,
            "externalIds": {
                "MAG": "2356131502",
                "DOI": "10.1533/9780857099440",
                "CorpusId": 59644696
            },
            "abstract": "Introduction Learning and intelligence Machine learning basics Knowledge representation Learning as search Attribute quality measures Data pre-processing Constructive induction Symbolic learning Statistical learning Artificial neural networks Cluster analysis Learning theory Computational learning theory Definitions References and index.",
            "referenceCount": 4,
            "citationCount": 320,
            "influentialCitationCount": 30,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2007-06-01",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Kononenko2007MachineLA,\n author = {I. Kononenko and M. Kukar},\n title = {Machine Learning and Data Mining: Introduction to Principles and Algorithms},\n year = {2007}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:75e56ef7924972fde2ffc32d7071cd182d0f0f21",
            "@type": "ScholarlyArticle",
            "paperId": "75e56ef7924972fde2ffc32d7071cd182d0f0f21",
            "corpusId": 15719583,
            "url": "https://www.semanticscholar.org/paper/75e56ef7924972fde2ffc32d7071cd182d0f0f21",
            "title": "Selection of Relevant Features in Machine Learning",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1994,
            "externalIds": {
                "MAG": "1608549042",
                "DOI": "10.21236/ada292575",
                "CorpusId": 15719583
            },
            "abstract": "In this paper, we review the problem of selecting rele- vant features for use in machine learning. We describe this problem in terms of heuristic search through a space of feature sets, and we identify four dimensions along which approaches to the problem can vary. We consider recent work on feature selection in terms of this framework, then close with some challenges for future work in the area. 1. The Problem of Irrelevant Features accuracy) to grow slowly with the number of irrele- vant attributes. Theoretical results for algorithms that search restricted hypothesis spaces are encouraging. For instance, the worst-case number of errors made by Littlestone's (1987) WINNOW method grows only logarithmically with the number of irrelevant features. Pazzani and Sarrett's (1992) average-case analysis for WHOLIST, a simple conjunctive algorithm, and Lang- ley and Iba's (1993) treatment of the naive Bayesian classifier, suggest that their sample complexities grow at most linearly with the number of irrelevant features. However, the theoretical results are less optimistic for induction methods that search a larger space of concept descriptions. For example, Langley and Iba's (1993) average-case analysis of simple nearest neighbor indicates that its sample complexity grows exponen- tially with the number of irrelevant attributes, even for conjunctive target concepts. Experimental stud- ies of nearest neighbor are consistent with this conclu- sion, and other experiments suggest that similar results hold even for induction algorithms that explicitly se- lect features. For example, the sample complexity for decision-tree methods appears to grow linearly with the number of irrelevants for conjunctive concepts, but exponentially for parity concepts, since the evaluation metric cannot distinguish relevant from irrelevant fea- tures in the latter situation (Langley & Sage, in press). Results of this sort have encouraged machine learn- ing researchers to explore more sophisticated methods for selecting relevant features. In the sections that fol- low, we present a general framework for this task, and then consider some recent examples of work on this important problem.",
            "referenceCount": 21,
            "citationCount": 797,
            "influentialCitationCount": 26,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.aaai.org/Papers/Symposia/Fall/1994/FS-94-02/FS94-02-034.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "1994-11-01",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Langley1994SelectionOR,\n author = {P. Langley},\n title = {Selection of Relevant Features in Machine Learning},\n year = {1994}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9ded923a192ffbf13e4466c6b7d2ede55724b716",
            "@type": "ScholarlyArticle",
            "paperId": "9ded923a192ffbf13e4466c6b7d2ede55724b716",
            "corpusId": 41680909,
            "url": "https://www.semanticscholar.org/paper/9ded923a192ffbf13e4466c6b7d2ede55724b716",
            "title": "Sparse Greedy Matrix Approximation for Machine Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2000,
            "externalIds": {
                "MAG": "1551209770",
                "DBLP": "conf/icml/SmolaS00",
                "DOI": "10.5555/645529.657980",
                "CorpusId": 41680909
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 746,
            "influentialCitationCount": 50,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2000-06-29",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Smola2000SparseGM,\n author = {Alex Smola and B. Scholkopf},\n booktitle = {International Conference on Machine Learning},\n pages = {911-918},\n title = {Sparse Greedy Matrix Approximation for Machine Learning},\n year = {2000}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:47c862bf4542415702ff1084f2a3aac33d0e13ea",
            "@type": "ScholarlyArticle",
            "paperId": "47c862bf4542415702ff1084f2a3aac33d0e13ea",
            "corpusId": 61503997,
            "url": "https://www.semanticscholar.org/paper/47c862bf4542415702ff1084f2a3aac33d0e13ea",
            "title": "Proceedings of the 24th international conference on Machine learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2007,
            "externalIds": {
                "MAG": "2911322539",
                "CorpusId": 61503997
            },
            "abstract": "This volume contains the papers accepted to the 24th International Conference on Machine Learning (ICML 2007), which was held at Oregon State University in Corvalis, Oregon, from June 20th to 24th, 2007. ICML is the annual conference of the International Machine Learning Society (IMLS), and provides a venue for the presentation and discussion of current research in the field of machine learning. These proceedings can also be found online at: http://www.machinelearning.org. \n \nThis year there were 522 submissions to ICML. There was a very thorough review process, in which each paper was reviewed by three program committee (PC) members. Authors were able to respond to the initial reviews, and the PC members could then modify their reviews based on online discussions and the content of this author response. For the first time this year there were two discussion periods led by the senior program committee (SPC), one just before and one after the submission of author responses. At the end of the second discussion period, the SPC members gave their recommendations and provided a summary review for each of their papers. Also for the first time, authors were asked to submit a list of changes with their final accepted papers, which was checked by the SPCs to ensure that reviewer comments had been addressed. Apart from the length restrictions on papers and the compressed time frame, the review process for ICML resembles that of many journal publications. In total, 150 papers were accepted to ICML this year, including a very small number of papers which were initially conditionally accepted, yielding an overall acceptance rate of 29%. \n \nICML attracts submissions from machine learning researchers around the globe. The 150 accepted papers this year were geographically distributed as follows: 66 papers had a first author from the US, 32 from Europe, 19 from China or Hong Kong, 11 from Canada, 6 from India, 5 each from Australia and Japan, 3 from Israel, and 1 each from Korea, Russia and Taiwan. \n \nIn addition to the main program of accepted papers, which includes both a talk and poster presentation for each paper, the ICML program included 3 workshops and 8 tutorials on machine learning topics which are currently of broad interest. We were also extremely pleased to have David Heckerman (Microsoft Research), Joshua Tenenbaum (Massachussetts Institute of Technology), and Bernhard Scholkopf (Max Planck Institute for Biological Cybernetics) as the invited speakers this year. Thanks to sponsorship by the Machine Learning Journal, we were able to award a number of outstanding student paper prizes. \n \nWe were fortunate this year that ICML was co-located with the International Conference on Inductive Logic Programming (ILP 2007). ICML and ILP held joint sessions on the first day of ICML 2007.",
            "referenceCount": 0,
            "citationCount": 247,
            "influentialCitationCount": 36,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Sociology",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Sociology",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "2007-06-20",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Ghahramani2007ProceedingsOT,\n author = {Zoubin Ghahramani},\n booktitle = {International Conference on Machine Learning},\n title = {Proceedings of the 24th international conference on Machine learning},\n year = {2007}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:fb829c5e6b406bb325fa5a02e05073df12b1772b",
            "@type": "ScholarlyArticle",
            "paperId": "fb829c5e6b406bb325fa5a02e05073df12b1772b",
            "corpusId": 16552240,
            "url": "https://www.semanticscholar.org/paper/fb829c5e6b406bb325fa5a02e05073df12b1772b",
            "title": "Classes of Kernels for Machine Learning: A Statistics Perspective",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2002,
            "externalIds": {
                "DBLP": "journals/jmlr/Genton01",
                "MAG": "1567012231",
                "CorpusId": 16552240
            },
            "abstract": "In this paper, we present classes of kernels for machine learning from a statistics perspective. Indeed, kernels are positive definite functions and thus also covariances. After discussing key properties of kernels, as well as a new formula to construct kernels, we present several important classes of kernels: anisotropic stationary kernels, isotropic stationary kernels, compactly supported kernels, locally stationary kernels, nonstationary kernels, and separable nonstationary kernels. Compactly supported kernels and separable nonstationary kernels are of prime interest because they provide a computational reduction for kernel-based methods. We describe the spectral representation of the various classes of kernels and conclude with a discussion on the characterization of nonlinear maps that reduce nonstationary kernels to either stationarity or local stationarity.",
            "referenceCount": 38,
            "citationCount": 701,
            "influentialCitationCount": 32,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2002-03-01",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "2"
            },
            "citationStyles": {
                "bibtex": "@Article{Genton2002ClassesOK,\n author = {M. Genton},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {299-312},\n title = {Classes of Kernels for Machine Learning: A Statistics Perspective},\n volume = {2},\n year = {2002}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:28d2503b0f86dd3947bf745efdd609dee7975cd8",
            "@type": "ScholarlyArticle",
            "paperId": "28d2503b0f86dd3947bf745efdd609dee7975cd8",
            "corpusId": 1104773,
            "url": "https://www.semanticscholar.org/paper/28d2503b0f86dd3947bf745efdd609dee7975cd8",
            "title": "TensorLy: Tensor Learning in Python",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "journals/corr/KossaifiPP16",
                "MAG": "2952198983",
                "ArXiv": "1610.09555",
                "CorpusId": 1104773
            },
            "abstract": "Tensors are higher-order extensions of matrices. While matrix methods form the cornerstone of traditional machine learning and data analysis, tensor methods have been gaining increasing traction. However, software support for tensor operations is not on the same footing. In order to bridge this gap, we have developed TensorLy, a Python library that provides a high-level API for tensor methods and deep tensorized neural networks. TensorLy aims to follow the same standards adopted by the main projects of the Python scientific community, and to seamlessly integrate with them. Its BSD license makes it suitable for both academic and commercial applications. TensorLy's backend system allows users to perform computations with several libraries such as NumPy or PyTorch to name but a few. They can be scaled on multiple CPU or GPU machines. In addition, using the deep-learning frameworks as backend allows to easily design and train deep tensorized neural networks. TensorLy is available at https://github.com/tensorly/tensorly",
            "referenceCount": 45,
            "citationCount": 282,
            "influentialCitationCount": 20,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-10-29",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1610.09555"
            },
            "citationStyles": {
                "bibtex": "@Article{Kossaifi2016TensorLyTL,\n author = {Jean Kossaifi and Yannis Panagakis and M. Pantic},\n booktitle = {Journal of machine learning research},\n journal = {ArXiv},\n title = {TensorLy: Tensor Learning in Python},\n volume = {abs/1610.09555},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:571929abb1f8dbb244f416ee470c4a30d255cde7",
            "@type": "ScholarlyArticle",
            "paperId": "571929abb1f8dbb244f416ee470c4a30d255cde7",
            "corpusId": 62963320,
            "url": "https://www.semanticscholar.org/paper/571929abb1f8dbb244f416ee470c4a30d255cde7",
            "title": "Map-Reduce for Machine Learning on Multicore",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2007,
            "externalIds": {
                "MAG": "2547440996",
                "DOI": "10.7551/mitpress/7503.003.0040",
                "CorpusId": 62963320
            },
            "abstract": "We are at the beginning of the multicore era. Computers will have increasingly many cores (processors), but there is still no good programming framework for these architectures, and thus no simple and unified way for machine learning to take advantage of the potential speed up. In this paper, we develop a broadly applicable parallel programming method, one that is easily applied to manydifferent learning algorithms. Our work is in distinct contrast to the tradition in machine learning of designing (often ingenious) ways to speed up a singlealgorithm at a time. Specifically, we show that algorithms that fit the Statistical Query model [15] can be written in a certain \u201csummation form,\u201d which allows them to be easily parallelized on multicore computers. We adapt Google\u2019s map-reduce [7] paradigm to demonstrate this parallel speed up technique on a variety of learning algorithms including locally weighted linear regression (LWLR), k-means, logistic regression (LR), naive Bayes (NB), SVM, ICA, PCA, gaussian discriminant analysis (GDA), EM, and backpropagation (NN). Our experimental results show basically linear speedup with an increasing number of processors.",
            "referenceCount": 27,
            "citationCount": 270,
            "influentialCitationCount": 13,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Scholkopf2007MapReduceFM,\n author = {B. Scholkopf and J. Platt and T. Hofmann},\n pages = {281-288},\n title = {Map-Reduce for Machine Learning on Multicore},\n year = {2007}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ab08f2a0b98fe7938d08875eb6125fa518620222",
            "@type": "ScholarlyArticle",
            "paperId": "ab08f2a0b98fe7938d08875eb6125fa518620222",
            "corpusId": 10085004,
            "url": "https://www.semanticscholar.org/paper/ab08f2a0b98fe7938d08875eb6125fa518620222",
            "title": "The Need for Open Source Software in Machine Learning",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2007,
            "externalIds": {
                "MAG": "2114690085",
                "DBLP": "journals/jmlr/SonnenburgBOBBHLMPRRSSVWW07",
                "DOI": "10.5555/1314498.1314577",
                "CorpusId": 10085004
            },
            "abstract": "Open source tools have recently reached a level of maturity which makes them suitable for building large-scale real-world systems. At the same time, the field of machine learning has developed a large body of powerful learning algorithms for diverse applications. However, the true potential of these methods is not used, since existing implementations are not openly shared, resulting in software with low usability, and weak interoperability. We argue that this situation can be significantly improved by increasing incentives for researchers to publish their software under an open source model. Additionally, we outline the problems authors are faced with when trying to publish algorithmic implementations of machine learning methods. We believe that a resource of peer reviewed software accompanied by short articles would be highly valuable to both the machine learning and the general scientific community.",
            "referenceCount": 44,
            "citationCount": 219,
            "influentialCitationCount": 15,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2007-12-01",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "8"
            },
            "citationStyles": {
                "bibtex": "@Article{Sonnenburg2007TheNF,\n author = {S. Sonnenburg and M. Braun and Cheng Soon Ong and Samy Bengio and L. Bottou and G. Holmes and Yann LeCun and K. M\u00fcller and Fernando C Pereira and C. Rasmussen and Gunnar R\u00e4tsch and B. Scholkopf and Alex Smola and Pascal Vincent and J. Weston and R. C. Williamson},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {2443-2466},\n title = {The Need for Open Source Software in Machine Learning},\n volume = {8},\n year = {2007}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9874b4cfd9e8ef89fd0b753af18c14cbc7c42744",
            "@type": "ScholarlyArticle",
            "paperId": "9874b4cfd9e8ef89fd0b753af18c14cbc7c42744",
            "corpusId": 54180662,
            "url": "https://www.semanticscholar.org/paper/9874b4cfd9e8ef89fd0b753af18c14cbc7c42744",
            "title": "What do you mean by collaborative learning",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1999,
            "externalIds": {
                "MAG": "2122575992",
                "CorpusId": 54180662
            },
            "abstract": "This book arises from a series of workshops on collaborative learning, that gathered together 20 scholars from the disciplines of psychology, education and computer science. The series was part of a research program entitled 'Learning in Humans and Machines' (LHM), launched by Peter Reimann and Hans Spada, and funded by the European Science Foundation. This program aimed to develop a multidisciplinary dialogue on learning, involving mainly scholars from cognitive psychology, educational science, and artificial intelligence (including machine learning). During the preparation of the program, Agnes Blaye, Claire O'Malley, Michael Baker and I developed a theme on collaborative learning. When the program officially began, 12 members were selected to work on this theme and formed the so-called 'task force 5'. I became the coordinator of the group. This group organised two workshops, in Sitges (Spain, 1994) and Aix-en-Provence (France, 1995). In 1996, the group was enriched with new members to reach its final size. Around 20 members met in the subsequent workshops, at Samoens (France, 1996), Houthalen (Belgium, 1996) and Mannheim (Germany, 1997). Several individuals joined the group for some time but have not written a chapter. I would nevertheless like to acknowledge their contributions to our activities: George Bilchev, Stevan Harnad, Calle Jansson and Claire O'Malley.",
            "referenceCount": 47,
            "citationCount": 2469,
            "influentialCitationCount": 204,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Education",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Dillenbourg1999WhatDY,\n author = {P. Dillenbourg},\n pages = {1-19},\n title = {What do you mean by collaborative learning},\n year = {1999}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:00ec8123dd2ba03afab7c1fa02f774062f769181",
            "@type": "ScholarlyArticle",
            "paperId": "00ec8123dd2ba03afab7c1fa02f774062f769181",
            "corpusId": 51868784,
            "url": "https://www.semanticscholar.org/paper/00ec8123dd2ba03afab7c1fa02f774062f769181",
            "title": "Using Reward Machines for High-Level Task Specification and Decomposition in Reinforcement Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2804948070",
                "DBLP": "conf/icml/IcarteKVM18",
                "CorpusId": 51868784
            },
            "abstract": "In this paper we propose Reward Machines \u2013 a type of finite state machine that supports the specification of reward functions while exposing reward function structure to the learner and supporting decomposition. We then present Q-Learning for Reward Machines (QRM), an algorithm which appropriately decomposes the reward machine and uses off-policy q-learning to simultaneously learn subpolicies for the different components. QRM is guaranteed to converge to an optimal policy in the tabular case, in contrast to Hierarchical Reinforcement Learning methods which might converge to suboptimal policies. We demonstrate this behavior experimentally in two discrete domains. We also show how function approximation methods like neural networks can be incorporated into QRM, and that doing so can find better policies more quickly than hierarchical methods in a domain with a continuous state space.",
            "referenceCount": 25,
            "citationCount": 192,
            "influentialCitationCount": 35,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-07-03",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Icarte2018UsingRM,\n author = {Rodrigo Toro Icarte and Toryn Q. Klassen and R. Valenzano and Sheila A. McIlraith},\n booktitle = {International Conference on Machine Learning},\n pages = {2112-2121},\n title = {Using Reward Machines for High-Level Task Specification and Decomposition in Reinforcement Learning},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8df9c71f09eb0dabf5adf17bee0f6b36190b52b2",
            "@type": "ScholarlyArticle",
            "paperId": "8df9c71f09eb0dabf5adf17bee0f6b36190b52b2",
            "corpusId": 12996116,
            "url": "https://www.semanticscholar.org/paper/8df9c71f09eb0dabf5adf17bee0f6b36190b52b2",
            "title": "Representational Learning with Extreme Learning Machine for Big Data Liyanaarachchi",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": null,
            "externalIds": {
                "CorpusId": 12996116
            },
            "abstract": "Restricted Boltzmann Machines (RBM) and auto encoders, learns to represent features in a dataset meaningfully and used as the basic building blocks to create deep networks. This paper introduces Extreme Learning Machine based Auto Encoder (ELM-AE), which learns feature representations using singular values and is used as the basic building block for Multi Layer Extreme Learning Machine (ML-ELM). ML-ELM performance is better than auto encoders based deep networks and Deep Belief Networks (DBN), while in par with Deep Boltzmann Machines (DBM) for MNIST dataset. However MLELM is significantly faster than any state\u2212of\u2212the\u2212art deep networks.",
            "referenceCount": 9,
            "citationCount": 290,
            "influentialCitationCount": 39,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Misc{None,\n author = {L. C. Kasun and Hongming Zhou and G. Huang and C. Vong},\n title = {Representational Learning with Extreme Learning Machine for Big Data Liyanaarachchi}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:403a730841c1e8e9e8062df22ff8f43537afd6ee",
            "@type": "ScholarlyArticle",
            "paperId": "403a730841c1e8e9e8062df22ff8f43537afd6ee",
            "corpusId": 839773,
            "url": "https://www.semanticscholar.org/paper/403a730841c1e8e9e8062df22ff8f43537afd6ee",
            "title": "Feature Selection for Machine Learning: Comparing a Correlation-Based Filter Approach to the Wrapper",
            "venue": "The Florida AI Research Society",
            "publicationVenue": {
                "id": "urn:research:546d164a-fc31-4aee-99a5-879e03ff7d36",
                "name": "The Florida AI Research Society",
                "alternate_names": [
                    "Fla AI Res Soc",
                    "FLAIRS"
                ],
                "issn": null,
                "url": "http://www.flairs.com/"
            },
            "year": 1999,
            "externalIds": {
                "DBLP": "conf/flairs/HallS99",
                "MAG": "2155925556",
                "CorpusId": 839773
            },
            "abstract": "Feature selection is often an essential data processing step prior to applying a learning algorithm. The removal of irrelevant and redundant information often improves the performance of machine learning algorithms. There are two common approaches: a wrapper uses the intended learning algorithm itself to evaluate the usefulness of features, while a fllter evaluates features according to heuristics based on general characteristics of the data. The wrapper approach is generally considered to produce better feature subsets but runs much more slowly than a fllter. This paper describes a new fllter approach to feature selection that uses a correlation based heuristic to evaluate the worth of feature subsets When applied as a data preprocessing step for two common machine learning algorithms, the new method compares favourably with the wrapper but requires much less computation.",
            "referenceCount": 16,
            "citationCount": 616,
            "influentialCitationCount": 59,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1999-05-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Hall1999FeatureSF,\n author = {M. Hall and L. A. Smith},\n booktitle = {The Florida AI Research Society},\n pages = {235-239},\n title = {Feature Selection for Machine Learning: Comparing a Correlation-Based Filter Approach to the Wrapper},\n year = {1999}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:db64f424710d57025c5fb42a564551f093a4d111",
            "@type": "ScholarlyArticle",
            "paperId": "db64f424710d57025c5fb42a564551f093a4d111",
            "corpusId": 3439578,
            "url": "https://www.semanticscholar.org/paper/db64f424710d57025c5fb42a564551f093a4d111",
            "title": "The Extreme Value Machine",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "publicationVenue": {
                "id": "urn:research:25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "alternate_names": [
                    "IEEE Trans Pattern Anal Mach Intell"
                ],
                "issn": "0162-8828",
                "url": "http://www.computer.org/tpami/"
            },
            "year": 2015,
            "externalIds": {
                "DBLP": "journals/pami/RuddJSB18",
                "MAG": "572355794",
                "ArXiv": "1506.06112",
                "DOI": "10.1109/TPAMI.2017.2707495",
                "CorpusId": 3439578,
                "PubMed": "28541894"
            },
            "abstract": "It is often desirable to be able to recognize when inputs to a recognition function learned in a supervised manner correspond to classes unseen at training time. With this ability, new class labels could be assigned to these inputs by a human operator, allowing them to be incorporated into the recognition function\u2014ideally under an efficient incremental update mechanism. While good algorithms that assume inputs from a fixed set of classes exist, e.g. , artificial neural networks and kernel machines, it is not immediately obvious how to extend them to perform incremental learning in the presence of unknown query classes. Existing algorithms take little to no distributional information into account when learning recognition functions and lack a strong theoretical foundation. We address this gap by formulating a novel, theoretically sound classifier\u2014the Extreme Value Machine (EVM). The EVM has a well-grounded interpretation derived from statistical Extreme Value Theory (EVT), and is the first classifier to be able to perform nonlinear kernel-free variable bandwidth incremental learning. Compared to other classifiers in the same deep network derived feature space, the EVM is accurate and efficient on an established benchmark partition of the ImageNet dataset.",
            "referenceCount": 66,
            "citationCount": 238,
            "influentialCitationCount": 43,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-06-19",
            "journal": {
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "volume": "40"
            },
            "citationStyles": {
                "bibtex": "@Article{Rudd2015TheEV,\n author = {Ethan M. Rudd and Lalit P. Jain and W. Scheirer and T. Boult},\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n pages = {762-768},\n title = {The Extreme Value Machine},\n volume = {40},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:cfc04be0eaf30ad81dcaaada8d6e6171aa058432",
            "@type": "ScholarlyArticle",
            "paperId": "cfc04be0eaf30ad81dcaaada8d6e6171aa058432",
            "corpusId": 14667458,
            "url": "https://www.semanticscholar.org/paper/cfc04be0eaf30ad81dcaaada8d6e6171aa058432",
            "title": "Flow Clustering Using Machine Learning Techniques",
            "venue": "Passive and Active Network Measurement Conference",
            "publicationVenue": {
                "id": "urn:research:251bb249-7a60-480f-beba-598ca8b11fad",
                "name": "Passive and Active Network Measurement Conference",
                "alternate_names": [
                    "Passive and Active Network Measurement",
                    "Passiv Act Netw Meas",
                    "PAM",
                    "Passiv Act Netw Meas Conf"
                ],
                "issn": null,
                "url": null
            },
            "year": 2004,
            "externalIds": {
                "MAG": "1481277647",
                "DBLP": "conf/pam/McGregorHLB04",
                "DOI": "10.1007/978-3-540-24668-8_21",
                "CorpusId": 14667458
            },
            "abstract": null,
            "referenceCount": 10,
            "citationCount": 586,
            "influentialCitationCount": 24,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://researchcommons.waikato.ac.nz/bitstream/10289/10848/2/Flow_Clustering_Using_Machine_Learning_Techniques.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2004-04-19",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{McGregor2004FlowCU,\n author = {A. McGregor and M. Hall and Perry Lorier and James Brunskill},\n booktitle = {Passive and Active Network Measurement Conference},\n pages = {205-214},\n title = {Flow Clustering Using Machine Learning Techniques},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:016fb4a9312b8b7fcf4fe1d96b85054cc52cc5cb",
            "@type": "ScholarlyArticle",
            "paperId": "016fb4a9312b8b7fcf4fe1d96b85054cc52cc5cb",
            "corpusId": 1339639,
            "url": "https://www.semanticscholar.org/paper/016fb4a9312b8b7fcf4fe1d96b85054cc52cc5cb",
            "title": "Weka-A Machine Learning Workbench for Data Mining",
            "venue": "Data Mining and Knowledge Discovery Handbook",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2005,
            "externalIds": {
                "MAG": "1555243480",
                "DBLP": "books/sp/datamining2005/FrankHHKP05",
                "DOI": "10.1007/978-0-387-09823-4_66",
                "CorpusId": 1339639
            },
            "abstract": null,
            "referenceCount": 13,
            "citationCount": 501,
            "influentialCitationCount": 44,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://researchcommons.waikato.ac.nz/bitstream/10289/1497/1/Weka%20A%20machine%20learning%20workbench%20for%20data%20mining.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Frank2005WekaAML,\n author = {E. Frank and M. Hall and G. Holmes and Richard Kirkby and B. Pfahringer and I. Witten and Leonard E. Trigg},\n booktitle = {Data Mining and Knowledge Discovery Handbook},\n pages = {1305-1314},\n title = {Weka-A Machine Learning Workbench for Data Mining},\n year = {2005}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c2c4db02486ea139e8142295888ff7b075575fba",
            "@type": "ScholarlyArticle",
            "paperId": "c2c4db02486ea139e8142295888ff7b075575fba",
            "corpusId": 58506268,
            "url": "https://www.semanticscholar.org/paper/c2c4db02486ea139e8142295888ff7b075575fba",
            "title": "Text Classification Using Machine Learning Techniques",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2005,
            "externalIds": {
                "MAG": "1211014924",
                "CorpusId": 58506268
            },
            "abstract": "Automated text classification has been considered as a vital method to manage and process a vast amount of documents in digital forms that are widespread and continuously increasing. In general, text classification plays an important role in information extraction and summarization, text retrieval, and question- answering. This paper illustrates the text classification process using machine learning techniques. The references cited cover the major theoretical issues and guide the researcher to interesting research directions.",
            "referenceCount": 36,
            "citationCount": 502,
            "influentialCitationCount": 26,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Ikonomakis2005TextCU,\n author = {M. Ikonomakis and S. Kotsiantis and V. Tampakas},\n title = {Text Classification Using Machine Learning Techniques},\n year = {2005}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d237de6e4974e6a34d2b35d7a3a223f6fb611219",
            "@type": "ScholarlyArticle",
            "paperId": "d237de6e4974e6a34d2b35d7a3a223f6fb611219",
            "corpusId": 12874183,
            "url": "https://www.semanticscholar.org/paper/d237de6e4974e6a34d2b35d7a3a223f6fb611219",
            "title": "Learning using privileged information: similarity control and knowledge transfer",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "2173379916",
                "DBLP": "journals/jmlr/VapnikI15a",
                "DOI": "10.5555/2789272.2886814",
                "CorpusId": 12874183
            },
            "abstract": "This paper describes a new paradigm of machine learning, in which Intelligent Teacher is involved. During training stage, Intelligent Teacher provides Student with information that contains, along with classification of each example, additional privileged information (for example, explanation) of this example. The paper describes two mechanisms that can be used for significantly accelerating the speed of Student's learning using privileged information: (1) correction of Student's concepts of similarity between examples, and (2) direct Teacher-Student knowledge transfer.",
            "referenceCount": 23,
            "citationCount": 341,
            "influentialCitationCount": 37,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "16"
            },
            "citationStyles": {
                "bibtex": "@Article{Vapnik2015LearningUP,\n author = {V. Vapnik and R. Izmailov},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {2023-2049},\n title = {Learning using privileged information: similarity control and knowledge transfer},\n volume = {16},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:160a4786dd643d9f758b9cc0758bdd2581524941",
            "@type": "ScholarlyArticle",
            "paperId": "160a4786dd643d9f758b9cc0758bdd2581524941",
            "corpusId": 8937162,
            "url": "https://www.semanticscholar.org/paper/160a4786dd643d9f758b9cc0758bdd2581524941",
            "title": "Machine learning for detection and diagnosis of disease.",
            "venue": "Annual Review of Biomedical Engineering",
            "publicationVenue": {
                "id": "urn:research:b1aef0b1-58ef-4dd9-aad5-2f40c42e19c4",
                "name": "Annual Review of Biomedical Engineering",
                "alternate_names": [
                    "Annu Rev Biomed Eng"
                ],
                "issn": "1523-9829",
                "url": "https://www.annualreviews.org/journal/bioeng"
            },
            "year": 2006,
            "externalIds": {
                "MAG": "2104787607",
                "DOI": "10.1146/ANNUREV.BIOENG.8.061505.095802",
                "CorpusId": 8937162,
                "PubMed": "16834566"
            },
            "abstract": "Machine learning offers a principled approach for developing sophisticated, automatic, and objective algorithms for analysis of high-dimensional and multimodal biomedical data. This review focuses on several advances in the state of the art that have shown promise in improving detection, diagnosis, and therapeutic monitoring of disease. Key in the advancement has been the development of a more in-depth understanding and theoretical analysis of critical issues related to algorithmic construction and learning theory. These include trade-offs for maximizing generalization performance, use of physically realistic constraints, and incorporation of prior knowledge and uncertainty. The review describes recent developments in machine learning, focusing on supervised and unsupervised linear methods and Bayesian inference, which have made significant impacts in the detection and diagnosis of disease in biomedicine. We describe the different methodologies and, for each, provide examples of their application to specific domains in biomedical diagnostics.",
            "referenceCount": 135,
            "citationCount": 328,
            "influentialCitationCount": 8,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2006-07-11",
            "journal": {
                "name": "Annual review of biomedical engineering",
                "volume": "8"
            },
            "citationStyles": {
                "bibtex": "@Article{Sajda2006MachineLF,\n author = {P. Sajda},\n booktitle = {Annual Review of Biomedical Engineering},\n journal = {Annual review of biomedical engineering},\n pages = {\n          537-65\n        },\n title = {Machine learning for detection and diagnosis of disease.},\n volume = {8},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:78a907839eadb15730ed51b574009067a5ac394a",
            "@type": "ScholarlyArticle",
            "paperId": "78a907839eadb15730ed51b574009067a5ac394a",
            "corpusId": 6995153,
            "url": "https://www.semanticscholar.org/paper/78a907839eadb15730ed51b574009067a5ac394a",
            "title": "Orange: From Experimental Machine Learning to Interactive Data Mining",
            "venue": "European Conference on Principles of Data Mining and Knowledge Discovery",
            "publicationVenue": {
                "id": "urn:research:df3a6b9b-fe26-4e65-965f-b1762a59eb79",
                "name": "European Conference on Principles of Data Mining and Knowledge Discovery",
                "alternate_names": [
                    "Eur Conf Princ Data Min Knowl Discov",
                    "PKDD"
                ],
                "issn": null,
                "url": "https://link.springer.com/conference/ecml"
            },
            "year": 2004,
            "externalIds": {
                "DBLP": "conf/pkdd/DemsarZLC04",
                "MAG": "172019652",
                "DOI": "10.1007/978-3-540-30116-5_58",
                "CorpusId": 6995153
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 494,
            "influentialCitationCount": 37,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007%2F978-3-540-30116-5_58.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2004-09-20",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Dem\u0161ar2004OrangeFE,\n author = {J. Dem\u0161ar and B. Zupan and Gregor Leban and T. Curk},\n booktitle = {European Conference on Principles of Data Mining and Knowledge Discovery},\n pages = {537-539},\n title = {Orange: From Experimental Machine Learning to Interactive Data Mining},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:fc82788021963ff8e318ffe955829bc68e48943a",
            "@type": "ScholarlyArticle",
            "paperId": "fc82788021963ff8e318ffe955829bc68e48943a",
            "corpusId": 18281724,
            "url": "https://www.semanticscholar.org/paper/fc82788021963ff8e318ffe955829bc68e48943a",
            "title": "Machine Learning of Temporal Relations",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                "name": "Annual Meeting of the Association for Computational Linguistics",
                "alternate_names": [
                    "Annu Meet Assoc Comput Linguistics",
                    "Meeting of the Association for Computational Linguistics",
                    "ACL",
                    "Meet Assoc Comput Linguistics"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/anthology/venues/acl/"
            },
            "year": 2006,
            "externalIds": {
                "ACL": "P06-1095",
                "MAG": "2127194753",
                "DBLP": "conf/acl/ManiVWLP06",
                "DOI": "10.3115/1220175.1220270",
                "CorpusId": 18281724
            },
            "abstract": "This paper investigates a machine learning approach for temporally ordering and anchoring events in natural language texts. To address data sparseness, we used temporal reasoning as an over-sampling method to dramatically expand the amount of training data, resulting in predictive accuracy on link labeling as high as 93% using a Maximum Entropy classifier on human annotated data. This method compared favorably against a series of increasingly sophisticated baselines involving expansion of rules derived from human intuitions.",
            "referenceCount": 25,
            "citationCount": 287,
            "influentialCitationCount": 29,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://dl.acm.org/ft_gateway.cfm?id=1220270&type=pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2006-07-17",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Article{Mani2006MachineLO,\n author = {I. Mani and M. Verhagen and Ben Wellner and Chong Min Lee and J. Pustejovsky},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {753-760},\n title = {Machine Learning of Temporal Relations},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:463565c30b7a9c12c2ef0558a51cfc7b05055737",
            "@type": "ScholarlyArticle",
            "paperId": "463565c30b7a9c12c2ef0558a51cfc7b05055737",
            "corpusId": 59913655,
            "url": "https://www.semanticscholar.org/paper/463565c30b7a9c12c2ef0558a51cfc7b05055737",
            "title": "Semi-Supervised Learning (Adaptive Computation and Machine Learning)",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2006,
            "externalIds": {
                "MAG": "155855048",
                "CorpusId": 59913655
            },
            "abstract": "If searched for a ebook Semi-Supervised Learning (Adaptive Computation and Machine Learning series) in pdf format, then you have come on to right website. We presented utter variation of this ebook in DjVu, PDF, txt, doc, ePub forms. You may read Semi-Supervised Learning (Adaptive Computation and Machine Learning series) online or downloading. Further, on our site you can read the instructions and diverse artistic eBooks online, or downloading them. We like draw on your regard what our website does not store the eBook itself, but we give ref to the site wherever you can download or read online. If have necessity to downloading Semi-Supervised Learning (Adaptive Computation and Machine Learning series) pdf, in that case you come on to loyal website. We own Semi-Supervised Learning (Adaptive Computation and Machine Learning series) ePub, txt, PDF, DjVu, doc forms. We will be glad if you revert to us over.",
            "referenceCount": 34,
            "citationCount": 247,
            "influentialCitationCount": 20,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2006-09-01",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Chapelle2006SemiSupervisedL,\n author = {O. Chapelle and B. Scholkopf and A. Zien},\n title = {Semi-Supervised Learning (Adaptive Computation and Machine Learning)},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d91ea30f4f9de817b29bb4ece00f43cb971822b4",
            "@type": "ScholarlyArticle",
            "paperId": "d91ea30f4f9de817b29bb4ece00f43cb971822b4",
            "corpusId": 61014716,
            "url": "https://www.semanticscholar.org/paper/d91ea30f4f9de817b29bb4ece00f43cb971822b4",
            "title": "Machine Learning Benchmarks and Random Forest Regression",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2004,
            "externalIds": {
                "MAG": "1599871777",
                "CorpusId": 61014716
            },
            "abstract": "Breiman (2001a,b) has recently developed an ensemble classification and regression approach that displayed outstanding performance with regard prediction error on a suite of benchmark datasets. As the base constituents of the ensemble are tree-structured predictors, and since each of these is constructed using an injection of randomness, the method is called \u2018random forests\u2019. That the exceptional performance is attained with seemingly only a single tuning parameter, to which sensitivity is minimal, makes the methodology all the more remarkable. The individual trees comprising the forest are all grown to maximal depth. While this helps with regard bias, there is the familiar tradeoff with variance. However, these variability concerns were potentially obscured because of an interesting feature of those benchmarking datasets extracted from the UCI machine learning repository for testing: all these datasets are hard to overfit using tree-structured methods. This raises issues about the scope of the repository. With this as motivation, and coupled with experience from boosting methods, we revisit the formulation of random forests and investigate prediction performance on real-world and simulated datasets for which maximally sized trees do overfit. These explorations reveal that gains can be realized by additional tuning to regulate tree size via limiting the number of splits and/or the size of nodes for which splitting is allowed. Nonetheless, even in these settings, good performance for random forests can be attained by using larger (than default) primary tuning parameter values.",
            "referenceCount": 13,
            "citationCount": 546,
            "influentialCitationCount": 22,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2004-04-14",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Segal2004MachineLB,\n author = {M. Segal},\n title = {Machine Learning Benchmarks and Random Forest Regression},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:dc418bccc06fdf3873b0241c97af4219dfca794a",
            "@type": "ScholarlyArticle",
            "paperId": "dc418bccc06fdf3873b0241c97af4219dfca794a",
            "corpusId": 206598347,
            "url": "https://www.semanticscholar.org/paper/dc418bccc06fdf3873b0241c97af4219dfca794a",
            "title": "Machine Learning in a Quantum World",
            "venue": "Canadian AI",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2006,
            "externalIds": {
                "DBLP": "conf/ai/AimeurBG06",
                "MAG": "2124269824",
                "DOI": "10.1007/11766247_37",
                "CorpusId": 206598347
            },
            "abstract": null,
            "referenceCount": 25,
            "citationCount": 160,
            "influentialCitationCount": 5,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2006-06-07",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{A\u00efmeur2006MachineLI,\n author = {Esma A\u00efmeur and G. Brassard and S. Gambs},\n booktitle = {Canadian AI},\n pages = {431-442},\n title = {Machine Learning in a Quantum World},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:06757c457ec442eb35af6ea45d8d0e2339415178",
            "@type": "ScholarlyArticle",
            "paperId": "06757c457ec442eb35af6ea45d8d0e2339415178",
            "corpusId": 18608327,
            "url": "https://www.semanticscholar.org/paper/06757c457ec442eb35af6ea45d8d0e2339415178",
            "title": "The Interplay of Optimization and Machine Learning Research",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2006,
            "externalIds": {
                "DBLP": "journals/jmlr/BennettP06",
                "MAG": "2170905826",
                "CorpusId": 18608327
            },
            "abstract": "The fields of machine learning and mathematical programming are increasingly intertwined. Optimization problems lie at the heart of most machine learning approaches. The Special Topic on Machine Learning and Large Scale Optimization examines this interplay. Machine learning researchers have embraced the advances in mathematical programming allowing new types of models to be pursued. The special topic includes models using quadratic, linear, second-order cone, semi-definite, and semi-infinite programs. We observe that the qualities of good optimization algorithms from the machine learning and optimization perspectives can be quite different. Mathematical programming puts a premium on accuracy, speed, and robustness. Since generalization is the bottom line in machine learning and training is normally done off-line, accuracy and small speed improvements are of little concern in machine learning. Machine learning prefers simpler algorithms that work in reasonable computational time for specific classes of problems. Reducing machine learning problems to well-explored mathematical programming classes with robust general purpose optimization codes allows machine learning researchers to rapidly develop new techniques. In turn, machine learning presents new challenges to mathematical programming. The special issue include papers from two primary themes: novel machine learning models and novel optimization approaches for existing models. Many papers blend both themes, making small changes in the underlying core mathematical program that enable the develop of effective new algorithms.",
            "referenceCount": 50,
            "citationCount": 155,
            "influentialCitationCount": 4,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2006-12-01",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "7"
            },
            "citationStyles": {
                "bibtex": "@Article{Bennett2006TheIO,\n author = {Kristin P. Bennett and E. Parrado-Hern\u00e1ndez},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {1265-1281},\n title = {The Interplay of Optimization and Machine Learning Research},\n volume = {7},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4fe855079443b097587fe42bcff972fc825b8c26",
            "@type": "ScholarlyArticle",
            "paperId": "4fe855079443b097587fe42bcff972fc825b8c26",
            "corpusId": 44277890,
            "url": "https://www.semanticscholar.org/paper/4fe855079443b097587fe42bcff972fc825b8c26",
            "title": "Machine Learning: ECML 2007, 18th European Conference on Machine Learning, Warsaw, Poland, September 17-21, 2007, Proceedings",
            "venue": "European Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:f9a81beb-9bcb-43bc-96a7-67cf23dba2d6",
                "name": "European Conference on Machine Learning",
                "alternate_names": [
                    "Eur Conf Mach Learn",
                    "European conference on Machine Learning",
                    "Eur conf Mach Learn",
                    "ECML"
                ],
                "issn": null,
                "url": "https://link.springer.com/conference/ecml"
            },
            "year": 2007,
            "externalIds": {
                "MAG": "2492778706",
                "DBLP": "conf/ecml/2007",
                "DOI": "10.1007/978-3-540-74958-5",
                "CorpusId": 44277890
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 165,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Conference"
            ],
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": "4701"
            },
            "citationStyles": {
                "bibtex": "@Conference{Kok2007MachineLE,\n author = {J. Kok},\n booktitle = {European Conference on Machine Learning},\n title = {Machine Learning: ECML 2007, 18th European Conference on Machine Learning, Warsaw, Poland, September 17-21, 2007, Proceedings},\n volume = {4701},\n year = {2007}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f2642db17084b14068d56f332de2f2d5a1622c5a",
            "@type": "ScholarlyArticle",
            "paperId": "f2642db17084b14068d56f332de2f2d5a1622c5a",
            "corpusId": 17292559,
            "url": "https://www.semanticscholar.org/paper/f2642db17084b14068d56f332de2f2d5a1622c5a",
            "title": "Error Minimized Extreme Learning Machine With Growth of Hidden Nodes and Incremental Learning",
            "venue": "IEEE Transactions on Neural Networks",
            "publicationVenue": {
                "id": "urn:research:2ac50919-507e-41c7-93a8-721c4b804757",
                "name": "IEEE Transactions on Neural Networks",
                "alternate_names": [
                    "IEEE Trans Neural Netw"
                ],
                "issn": "1045-9227",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=72"
            },
            "year": 2009,
            "externalIds": {
                "MAG": "2096987757",
                "DBLP": "journals/tnn/FengHLG09",
                "DOI": "10.1109/TNN.2009.2024147",
                "CorpusId": 17292559,
                "PubMed": "19596632"
            },
            "abstract": "One of the open problems in neural network research is how to automatically determine network architectures for given applications. In this brief, we propose a simple and efficient approach to automatically determine the number of hidden nodes in generalized single-hidden-layer feedforward networks (SLFNs) which need not be neural alike. This approach referred to as error minimized extreme learning machine (EM-ELM) can add random hidden nodes to SLFNs one by one or group by group (with varying group size). During the growth of the networks, the output weights are updated incrementally. The convergence of this approach is proved in this brief as well. Simulation results demonstrate and verify that our new approach is much faster than other sequential/incremental/growing algorithms with good generalization performance.",
            "referenceCount": 26,
            "citationCount": 615,
            "influentialCitationCount": 30,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2009-08-01",
            "journal": {
                "name": "IEEE Transactions on Neural Networks",
                "volume": "20"
            },
            "citationStyles": {
                "bibtex": "@Article{Feng2009ErrorME,\n author = {Guorui Feng and G. Huang and Qingping Lin and R. Gay},\n booktitle = {IEEE Transactions on Neural Networks},\n journal = {IEEE Transactions on Neural Networks},\n pages = {1352-1357},\n title = {Error Minimized Extreme Learning Machine With Growth of Hidden Nodes and Incremental Learning},\n volume = {20},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:32d12b621e75cdb5942896c16421496bdfd4a6fa",
            "@type": "ScholarlyArticle",
            "paperId": "32d12b621e75cdb5942896c16421496bdfd4a6fa",
            "corpusId": 14213246,
            "url": "https://www.semanticscholar.org/paper/32d12b621e75cdb5942896c16421496bdfd4a6fa",
            "title": "Ontology Matching: A Machine Learning Approach",
            "venue": "Handbook on Ontologies",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2004,
            "externalIds": {
                "DBLP": "books/sp/staabS2004/DoanMDH04",
                "MAG": "1781737156",
                "DOI": "10.1007/978-3-540-24750-0_19",
                "CorpusId": 14213246
            },
            "abstract": null,
            "referenceCount": 36,
            "citationCount": 566,
            "influentialCitationCount": 33,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.cs.washington.edu/homes/pedrod/papers/hois.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Doan2004OntologyMA,\n author = {A. Doan and J. Madhavan and Pedro M. Domingos and A. Halevy},\n booktitle = {Handbook on Ontologies},\n pages = {385-404},\n title = {Ontology Matching: A Machine Learning Approach},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:cf171d57f8232ba90a0696f8cb46144b39380d0b",
            "@type": "ScholarlyArticle",
            "paperId": "cf171d57f8232ba90a0696f8cb46144b39380d0b",
            "corpusId": 22799724,
            "url": "https://www.semanticscholar.org/paper/cf171d57f8232ba90a0696f8cb46144b39380d0b",
            "title": "Bioinformatics - The Machine Learning Approach",
            "venue": "Computers and Chemistry",
            "publicationVenue": {
                "id": "urn:research:527a7e6f-7ca5-472c-8059-8f0dcacbeda5",
                "name": "Computers and Chemistry",
                "alternate_names": [
                    "Computational Chemistry",
                    "Comput Chem"
                ],
                "issn": "0097-8485",
                "url": "http://www.sciencedirect.com/science/journal/00978485"
            },
            "year": 2000,
            "externalIds": {
                "MAG": "2090388864",
                "DBLP": "journals/candc/Grant00",
                "DOI": "10.1016/S0097-8485(00)80015-7",
                "CorpusId": 22799724
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 677,
            "influentialCitationCount": 36,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": "Comput. Chem.",
                "volume": "24"
            },
            "citationStyles": {
                "bibtex": "@Article{Grant2000BioinformaticsT,\n author = {G. Grant},\n booktitle = {Computers and Chemistry},\n journal = {Comput. Chem.},\n pages = {139-141},\n title = {Bioinformatics - The Machine Learning Approach},\n volume = {24},\n year = {2000}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8d1b6888aa08b0a590c9e5d1c563d5e646f0505e",
            "@type": "ScholarlyArticle",
            "paperId": "8d1b6888aa08b0a590c9e5d1c563d5e646f0505e",
            "corpusId": 37896067,
            "url": "https://www.semanticscholar.org/paper/8d1b6888aa08b0a590c9e5d1c563d5e646f0505e",
            "title": "Multi-Objective Machine Learning",
            "venue": "Studies in Computational Intelligence",
            "publicationVenue": {
                "id": "urn:research:bf2f9688-933d-44b9-9cd5-c738ddf60c56",
                "name": "Studies in Computational Intelligence",
                "alternate_names": [
                    "Stud Comput Intell",
                    "Studies in computational intelligence",
                    "Stud comput intell"
                ],
                "issn": "1860-949X",
                "url": "https://www.springer.com/series/7092"
            },
            "year": 2006,
            "externalIds": {
                "MAG": "1547172306",
                "DBLP": "series/sci/2006-16",
                "DOI": "10.1007/3-540-33019-4",
                "CorpusId": 37896067
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 214,
            "influentialCitationCount": 8,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": "16"
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Jin2006MultiObjectiveML,\n author = {Yaochu Jin},\n booktitle = {Studies in Computational Intelligence},\n title = {Multi-Objective Machine Learning},\n volume = {16},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:464d94b3dc9a109dd64008a41a00181830f285aa",
            "@type": "ScholarlyArticle",
            "paperId": "464d94b3dc9a109dd64008a41a00181830f285aa",
            "corpusId": 15187647,
            "url": "https://www.semanticscholar.org/paper/464d94b3dc9a109dd64008a41a00181830f285aa",
            "title": "Torch: a modular machine learning software library",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2002,
            "externalIds": {
                "MAG": "1548328233",
                "CorpusId": 15187647
            },
            "abstract": "Keywords: learning Reference EPFL-REPORT-82802 URL: http://publications.idiap.ch/downloads/reports/2002/rr02-46.pdf Record created on 2006-03-10, modified on 2017-05-10",
            "referenceCount": 1,
            "citationCount": 543,
            "influentialCitationCount": 39,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Collobert2002TorchAM,\n author = {Ronan Collobert and Samy Bengio and J. Mari\u00e9thoz},\n title = {Torch: a modular machine learning software library},\n year = {2002}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2ff6fcebe5561433a2d2abeb8b30b2fbf3c0e303",
            "@type": "ScholarlyArticle",
            "paperId": "2ff6fcebe5561433a2d2abeb8b30b2fbf3c0e303",
            "corpusId": 40097546,
            "url": "https://www.semanticscholar.org/paper/2ff6fcebe5561433a2d2abeb8b30b2fbf3c0e303",
            "title": "Introduction to Semi-Supervised Learning",
            "venue": "Synthesis Lectures on Artificial Intelligence and Machine Learning",
            "publicationVenue": {
                "id": "urn:research:84e95d47-8c6e-4f56-b8c8-2fc3088cfb6b",
                "name": "Synthesis Lectures on Artificial Intelligence and Machine Learning",
                "alternate_names": [
                    "Synth Lect Artif Intell Mach Learn"
                ],
                "issn": "1939-4608",
                "url": "https://www.morganclaypool.com/toc/aim/1/1"
            },
            "year": 2009,
            "externalIds": {
                "MAG": "1990334093",
                "DBLP": "series/synthesis/2009Zhu",
                "DOI": "10.1007/978-3-031-01548-9",
                "CorpusId": 40097546
            },
            "abstract": null,
            "referenceCount": 192,
            "citationCount": 2040,
            "influentialCitationCount": 104,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://cs.wisc.edu/~jerryzhu/pub/mm.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2009-06-29",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Zhu2009IntroductionTS,\n author = {Xiaojin Zhu and A. Goldberg},\n booktitle = {Synthesis Lectures on Artificial Intelligence and Machine Learning},\n title = {Introduction to Semi-Supervised Learning},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f455013a3e35fd660eab52f7f4dcb78d1faac266",
            "@type": "ScholarlyArticle",
            "paperId": "f455013a3e35fd660eab52f7f4dcb78d1faac266",
            "corpusId": 263223901,
            "url": "https://www.semanticscholar.org/paper/f455013a3e35fd660eab52f7f4dcb78d1faac266",
            "title": "Elements of Machine Learning",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1995,
            "externalIds": {
                "MAG": "1555244713",
                "DBLP": "books/mk/Langley94",
                "CorpusId": 263223901
            },
            "abstract": "Elements of Machine Learning by Pat Langley Preface 1. An overview of machine learning 1.1 The science of machine learning 1.2 Nature of the environment 1.3 Nature of representation and performance 1.4 Nature of the learning component 1.5 Five paradigms for machine learning 1.6 Summary of the chapter 2. The induction of logical conjunctions 2.1 General issues in logical induction 2.2 Nonincremental induction of logical conjunctions 2.3 Heuristic induction of logical conjunctions 2.4 Incremental induction of logical conjunctions 2.5 Incremental hill climbing for logical conjunctions 2.6 Genetic algorithms for logical concept induction 2.7 Summary of the chapter 3. The induction of threshold concepts 3.1 General issues for threshold concepts 3.2 Induction of criteria tables 3.3 Induction of linear threshold units 3.4 Induction of spherical threshold units 3.5 Summary of the chapter 4. The induction of competitive concepts 4.1 Instance-based learning 4.2 Learning probabilistic concept descriptions 4.3 Summary of the chapter 5. The construction of decision lists 5.1 General issues in disjunctive concept induction 5.2 Nonincremental learning using separate and conquer 5.3 Incremental induction using separate and conquer 5.4 Induction of decision lists through exceptions 5.5 Induction of competitive disjunctions 5.6 Instance-storing algorithms 5.7 Complementary beam search for disjunctive concepts 5.8 Summary of the chapter 6. Revision and extension of inference networks 6.1 General issues surrounding inference network 6.2 Extending an incomplete inference network 6.3 Inducing specialized concepts with inference networks 6.4 Revising an incorrect inference network 6.5 Network construction and term generation 6.6 Summary of the chapter 7. The formation of concept hierarchies 7.1 General issues concerning concept hierarchies 7.2 Nonincremental divisive formation of hierarchies 7.3 Incremental formation of concept hierarchies 7.4 Agglomerative formation of concept hierarchies 7.5 Variations on hierarchies into other structures 7.7 Summary of the chapter 8. Other issues in concept induction 8.1 Overfitting and pruning 8.2 Selecting useful features 8.3 Induction for numeric prediction 8.4 Unsupervised concept induction 8.5 Inducing relational concepts 8.6 Handling missing features 8.7 Summary of the chapter 9. The formation of transition networks 9.1 General issues for state-transition networks 9.2 Constructing finite-state transition networks 9.3 Forming recursive transition networks 9.4 Learning rules and networks for prediction 9.5 Summary of the chapter 10. The acquisition of search-control knowledge 10.1 General issues in search control 10.2 Reinforcement learning 10.3 Learning state-space heuristics from solution traces 10.4 Learning control knowledge for problem reduction 10.5 Learning control knowledge for means-ends analysis 10.6 The utility of search-control knowledge 10.7 Summary of the chapter 11. The formation of macro-operators 11.1 General issues related to macro-operators 11.2 The creation of simple macro-operators 11.3 The formation of flexible macro-operators 11.4 Problem solving by analogy 11.5 The utility of macro-operators 11.6 Summary of the chapter 12. Prospects for machine learning 12.1 Additional areas of machine learning 12.2 Methodological trends in machine learning 12.3 The future of machine learning References Index",
            "referenceCount": 0,
            "citationCount": 620,
            "influentialCitationCount": 19,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "1995-09-15",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Langley1995ElementsOM,\n author = {Pat Langley},\n title = {Elements of Machine Learning},\n year = {1995}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7975195638f3574ac02975df6f4048558ec1bc96",
            "@type": "ScholarlyArticle",
            "paperId": "7975195638f3574ac02975df6f4048558ec1bc96",
            "corpusId": 1100052,
            "url": "https://www.semanticscholar.org/paper/7975195638f3574ac02975df6f4048558ec1bc96",
            "title": "Extreme learning machines: a survey",
            "venue": "International Journal of Machine Learning and Cybernetics",
            "publicationVenue": {
                "id": "urn:research:a0c45882-7c78-4f0c-8886-d3481ba02586",
                "name": "International Journal of Machine Learning and Cybernetics",
                "alternate_names": [
                    "Int J Mach Learn Cybern"
                ],
                "issn": "1868-8071",
                "url": "http://www.springer.com/engineering/mathematical/journal/13042"
            },
            "year": 2011,
            "externalIds": {
                "DBLP": "journals/mlc/HuangWL11",
                "MAG": "1993717606",
                "DOI": "10.1007/s13042-011-0019-y",
                "CorpusId": 1100052
            },
            "abstract": null,
            "referenceCount": 126,
            "citationCount": 1815,
            "influentialCitationCount": 112,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2011-05-25",
            "journal": {
                "name": "International Journal of Machine Learning and Cybernetics",
                "volume": "2"
            },
            "citationStyles": {
                "bibtex": "@Article{Huang2011ExtremeLM,\n author = {G. Huang and Dianhui Wang and Yuan Lan},\n booktitle = {International Journal of Machine Learning and Cybernetics},\n journal = {International Journal of Machine Learning and Cybernetics},\n pages = {107-122},\n title = {Extreme learning machines: a survey},\n volume = {2},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ae73fa99d777efd07ed5a73cdc695191862f9d9e",
            "@type": "ScholarlyArticle",
            "paperId": "ae73fa99d777efd07ed5a73cdc695191862f9d9e",
            "corpusId": 15181105,
            "url": "https://www.semanticscholar.org/paper/ae73fa99d777efd07ed5a73cdc695191862f9d9e",
            "title": "Drug Design by Machine Learning: Support Vector Machines for Pharmaceutical Data Analysis",
            "venue": "Computers and Chemistry",
            "publicationVenue": {
                "id": "urn:research:527a7e6f-7ca5-472c-8059-8f0dcacbeda5",
                "name": "Computers and Chemistry",
                "alternate_names": [
                    "Computational Chemistry",
                    "Comput Chem"
                ],
                "issn": "0097-8485",
                "url": "http://www.sciencedirect.com/science/journal/00978485"
            },
            "year": 2001,
            "externalIds": {
                "MAG": "2169211980",
                "DBLP": "journals/candc/BurbidgeTBH02",
                "DOI": "10.1016/S0097-8485(01)00094-8",
                "CorpusId": 15181105,
                "PubMed": "11765851"
            },
            "abstract": null,
            "referenceCount": 33,
            "citationCount": 617,
            "influentialCitationCount": 17,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Chemistry",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2001-12-01",
            "journal": {
                "name": "Computers & chemistry",
                "volume": "26 1"
            },
            "citationStyles": {
                "bibtex": "@Article{Burbidge2001DrugDB,\n author = {R. Burbidge and M. Trotter and B. Buxton and S. Holden},\n booktitle = {Computers and Chemistry},\n journal = {Computers & chemistry},\n pages = {\n          5-14\n        },\n title = {Drug Design by Machine Learning: Support Vector Machines for Pharmaceutical Data Analysis},\n volume = {26 1},\n year = {2001}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ae6fdc00ec8c2299f101ddd428bfd82a0b55bac6",
            "@type": "ScholarlyArticle",
            "paperId": "ae6fdc00ec8c2299f101ddd428bfd82a0b55bac6",
            "corpusId": 6130737,
            "url": "https://www.semanticscholar.org/paper/ae6fdc00ec8c2299f101ddd428bfd82a0b55bac6",
            "title": "Practical feature subset selection for machine learning",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1998,
            "externalIds": {
                "MAG": "1485955713",
                "CorpusId": 6130737
            },
            "abstract": "Machine learning algorithms automatically extract knowledge from machine readable information. Unfortunately, their success is usually dependant on the quality of the data that they operate on. If the data is inadequate, or contains extraneous and irrelevant information, machine learning algorithms may produce less accurate and less understandable results, or may fail to discover anything of use at all. Feature subset selectors are algorithms that attempt to identify and remove as much irrelevant and redundant information as possible prior to learning. Feature subset selection can result in enhanced performance, a reduced hypothesis search space, and, in some cases, reduced storage requirement. This paper describes a new feature selection algorithm that uses a correlation based heuristic to determine the \u201cgoodness\u201d of feature subsets, and evaluates its effectiveness with three common machine learning algorithms. Experiments using a number of standard machine learning data sets are presented. Feature subset selection gave significant improvement for all three algorithms.",
            "referenceCount": 13,
            "citationCount": 553,
            "influentialCitationCount": 44,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Hall1998PracticalFS,\n author = {M. Hall and L. A. Smith},\n pages = {181-191},\n title = {Practical feature subset selection for machine learning},\n year = {1998}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:629cc74dcaf655feea40f64cd74617ac884ed0f8",
            "@type": "ScholarlyArticle",
            "paperId": "629cc74dcaf655feea40f64cd74617ac884ed0f8",
            "corpusId": 62488180,
            "url": "https://www.semanticscholar.org/paper/629cc74dcaf655feea40f64cd74617ac884ed0f8",
            "title": "Graphical Models for Machine Learning and Digital Communication",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1998,
            "externalIds": {
                "MAG": "2150218618",
                "DOI": "10.1016/s0898-1221(99)90389-9",
                "CorpusId": 62488180
            },
            "abstract": null,
            "referenceCount": 6,
            "citationCount": 633,
            "influentialCitationCount": 35,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1998-06-26",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Frey1998GraphicalMF,\n author = {B. Frey},\n title = {Graphical Models for Machine Learning and Digital Communication},\n year = {1998}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:50a869bcd6d45ec7fdb317877c3d2a047c2cfc38",
            "@type": "ScholarlyArticle",
            "paperId": "50a869bcd6d45ec7fdb317877c3d2a047c2cfc38",
            "corpusId": 5933862,
            "url": "https://www.semanticscholar.org/paper/50a869bcd6d45ec7fdb317877c3d2a047c2cfc38",
            "title": "Overfitting and undercomputing in machine learning",
            "venue": "CSUR",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1995,
            "externalIds": {
                "DBLP": "journals/csur/Dietterich95",
                "MAG": "2013640190",
                "DOI": "10.1145/212094.212114",
                "CorpusId": 5933862
            },
            "abstract": "A central problem in machine learning is supervised learning\u2014that is, learning from labeled training data. For example, a learning system for medical diagnosis might be trained with examples of patients whose case records (medical tests, clinical observations) and diagnoses were known. The task of the learning system is to infer a function that predicts the diagnosis of a patient from his or her case records. The function to be learned might be represented as a set of rules, a decision tree, a Bayes network, or a neural network. Learning algorithms essentially operate by searching some space of functions (usually called the hypothesis class) for a function that fits the given data. Because there are usually exponentially many functions, this search cannot actually examine individual hypothesis functions but instead must use some more direct method of constructing the hypothesis functions from the data. This search can usually be formalized by defining an objective function (e.g., number of data points predicted incorrectly) and applying various algorithms to find a function that minimizes this objective function is NP-hard. For example, fitting the weights of a neural network or finding the smallest decision tree are both NP-complete problems [Blum and Rivest, 1989; Quinlan and Rivest 1989]. Hence, heuristic algorithms such as gradient descent (for neural networks) and greedy search (for decision trees) have been applied with great success. Of course, the suboptimality of such heuristic algorithms ~mmediately suggests a reas&able line of research: find ~lgorithms that can search the hypothesis class better. Hence, there has been extensive research in applying secondorder methods to fit neural networks and in conducting much more thorough searches in learning decision trees and rule sets. Ironically, when these algorithms were tested on real datasets, it was found that their performance was often worse than simrde szradient descent or greedy sear~h [&inlan and Cameron-Jones 1995; Weigend 1994]. In short: it appears to be bet~er not to optimize! One of the other important trends in machine-learning research has been the establishment and nurturing of connections between various previously disparate fields, including computational learning theory, connectionist learning, symbolic learning. and statistics. The . connection to statistics was crucial in resolvins$ this naradox. The-key p~oblem arises from the structure of the machine-learning task, A learning algorithm is trained on a set of training data, but then it is applied to make predictions on new data points. The goal is to maximize its predictive accuracy on the new data points\u2014not necessarily its accuracy on the trammg data. Indeed, if we work too hard to find the very best fit to the training data, there is a risk that we will fit the noise in the data by memorizing various peculiarities",
            "referenceCount": 7,
            "citationCount": 568,
            "influentialCitationCount": 12,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/212094.212114",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1995-09-01",
            "journal": {
                "name": "ACM Comput. Surv.",
                "volume": "27"
            },
            "citationStyles": {
                "bibtex": "@Article{Dietterich1995OverfittingAU,\n author = {Thomas G. Dietterich},\n booktitle = {CSUR},\n journal = {ACM Comput. Surv.},\n pages = {326-327},\n title = {Overfitting and undercomputing in machine learning},\n volume = {27},\n year = {1995}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:13ba53b0a88648a05d55ebb7e5eb1640820c67bc",
            "@type": "ScholarlyArticle",
            "paperId": "13ba53b0a88648a05d55ebb7e5eb1640820c67bc",
            "corpusId": 10424399,
            "url": "https://www.semanticscholar.org/paper/13ba53b0a88648a05d55ebb7e5eb1640820c67bc",
            "title": "Machine learning and data mining",
            "venue": "Communications of the ACM",
            "publicationVenue": {
                "id": "urn:research:4d9ce1c4-dc84-46b9-903e-e3751c00c7dd",
                "name": "Communications of the ACM",
                "alternate_names": [
                    "Commun ACM",
                    "Communications of The ACM"
                ],
                "issn": "0001-0782",
                "url": "http://www.acm.org/pubs/cacm/"
            },
            "year": 1999,
            "externalIds": {
                "MAG": "1969733464",
                "DBLP": "journals/cacm/Mitchell99",
                "DOI": "10.1145/319382.319388",
                "CorpusId": 10424399
            },
            "abstract": "Over the past decade , many organizations have begun to routinely capture huge volumes of historical data describing their operations, products, and customers. At the same time, scientists and engineers in many fields have been capturing increasingly complex experimental data sets, such as gigabytes of functional magnetic resonance imaging (MRI) data describing brain activity in humans. The field of data mining addresses the question of how best to use this historical data to discover general regularities and improve the process of making decisions. Machine Learning and Data Mining",
            "referenceCount": 61,
            "citationCount": 464,
            "influentialCitationCount": 11,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/319382.319388",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1999-11-01",
            "journal": {
                "name": "Communications of the ACM",
                "volume": "42"
            },
            "citationStyles": {
                "bibtex": "@Article{Mitchell1999MachineLA,\n author = {Tom Michael Mitchell},\n booktitle = {Communications of the ACM},\n journal = {Communications of the ACM},\n pages = {30 - 36},\n title = {Machine learning and data mining},\n volume = {42},\n year = {1999}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:bbfad4e88fd8bfbbd77ba53a56fe2886ecc147da",
            "@type": "ScholarlyArticle",
            "paperId": "bbfad4e88fd8bfbbd77ba53a56fe2886ecc147da",
            "corpusId": 5542141,
            "url": "https://www.semanticscholar.org/paper/bbfad4e88fd8bfbbd77ba53a56fe2886ecc147da",
            "title": "Applications of machine learning and rule induction",
            "venue": "CACM",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1995,
            "externalIds": {
                "MAG": "2042385018",
                "DBLP": "journals/cacm/LangleyS95",
                "DOI": "10.1145/219717.219768",
                "CorpusId": 5542141
            },
            "abstract": "Machine learning is the study of computational methods for improving performance by mechanizing the acquisition of knowledge from experience. Expert performance requires much domain-specific knowledge, and knowledge engineering has produced hundreds of AI expert systems that are now used regularly in industry. Machine learning aims to provide increasing levels of automation in the knowledge engineering process, replacing much time-consuming human activity with automatic techniques that improve accuracy or efficiency by discovering and exploiting regularities in training data. The ultimate test of machine learning is its ability to produce systems that are used regularly in industry, education, and elsewhere.",
            "referenceCount": 23,
            "citationCount": 565,
            "influentialCitationCount": 16,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1995-11-01",
            "journal": {
                "name": "Commun. ACM",
                "volume": "38"
            },
            "citationStyles": {
                "bibtex": "@Article{Langley1995ApplicationsOM,\n author = {P. Langley and H. Simon},\n booktitle = {CACM},\n journal = {Commun. ACM},\n pages = {54-64},\n title = {Applications of machine learning and rule induction},\n volume = {38},\n year = {1995}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7d8b40eb7f3eb0e03c35f066c97a2040f2f8b724",
            "@type": "ScholarlyArticle",
            "paperId": "7d8b40eb7f3eb0e03c35f066c97a2040f2f8b724",
            "corpusId": 32895471,
            "url": "https://www.semanticscholar.org/paper/7d8b40eb7f3eb0e03c35f066c97a2040f2f8b724",
            "title": "Machine Learning Approaches to Estimating Software Development Effort",
            "venue": "IEEE Trans. Software Eng.",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1995,
            "externalIds": {
                "DBLP": "journals/tse/SrinivasanF95",
                "MAG": "2133442842",
                "DOI": "10.1109/32.345828",
                "CorpusId": 32895471
            },
            "abstract": "Accurate estimation of software development effort is critical in software engineering. Underestimates lead to time pressures that may compromise full functional development and thorough testing of software. In contrast, overestimates can result in noncompetitive contract bids and/or over allocation of development resources and personnel. As a result, many models for estimating software development effort have been proposed. This article describes two methods of machine learning, which we use to build estimators of software development effort from historical data. Our experiments indicate that these techniques are competitive with traditional estimators on one dataset, but also illustrate that these methods are sensitive to the data on which they are trained. This cautionary note applies to any model-construction strategy that relies on historical data. All such models for software effort estimation should be evaluated by exploring model sensitivity on a variety of historical data. >",
            "referenceCount": 19,
            "citationCount": 544,
            "influentialCitationCount": 24,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1995-02-01",
            "journal": {
                "name": "IEEE Trans. Software Eng.",
                "volume": "21"
            },
            "citationStyles": {
                "bibtex": "@Article{Srinivasan1995MachineLA,\n author = {K. Srinivasan and D. Fisher},\n booktitle = {IEEE Trans. Software Eng.},\n journal = {IEEE Trans. Software Eng.},\n pages = {126-137},\n title = {Machine Learning Approaches to Estimating Software Development Effort},\n volume = {21},\n year = {1995}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e45ead3629cdda9ab60a28b6040623ea84fc1a31",
            "@type": "ScholarlyArticle",
            "paperId": "e45ead3629cdda9ab60a28b6040623ea84fc1a31",
            "corpusId": 2940589,
            "url": "https://www.semanticscholar.org/paper/e45ead3629cdda9ab60a28b6040623ea84fc1a31",
            "title": "Machine Learning for User Modeling",
            "venue": "User modeling and user-adapted interaction",
            "publicationVenue": {
                "id": "urn:research:cf7b0aac-26b9-4d6a-ba15-e72f6755e11c",
                "name": "User modeling and user-adapted interaction",
                "alternate_names": [
                    "User model user-adapted interact",
                    "User Modeling and User-adapted Interaction",
                    "User Model User-adapted Interact"
                ],
                "issn": "0924-1868",
                "url": "https://link.springer.com/journal/11257"
            },
            "year": 2001,
            "externalIds": {
                "MAG": "2124756613",
                "DBLP": "journals/umuai/WebbPB01",
                "DOI": "10.1023/A:1011117102175",
                "CorpusId": 2940589
            },
            "abstract": null,
            "referenceCount": 35,
            "citationCount": 443,
            "influentialCitationCount": 11,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1023/A:1011117102175.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2001-03-27",
            "journal": {
                "name": "User Modeling and User-Adapted Interaction",
                "volume": "11"
            },
            "citationStyles": {
                "bibtex": "@Article{Webb2001MachineLF,\n author = {Geoffrey I. Webb and M. Pazzani and Daniel Billsus},\n booktitle = {User modeling and user-adapted interaction},\n journal = {User Modeling and User-Adapted Interaction},\n pages = {19-29},\n title = {Machine Learning for User Modeling},\n volume = {11},\n year = {2001}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9a24a993288119c9fd94cf2b43e394ad72dd59ee",
            "@type": "ScholarlyArticle",
            "paperId": "9a24a993288119c9fd94cf2b43e394ad72dd59ee",
            "corpusId": 26471402,
            "url": "https://www.semanticscholar.org/paper/9a24a993288119c9fd94cf2b43e394ad72dd59ee",
            "title": "Efficient Learning Machines",
            "venue": "Apress",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2015,
            "externalIds": {
                "MAG": "2473394283",
                "DOI": "10.1007/978-1-4302-5990-9",
                "CorpusId": 26471402
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 318,
            "influentialCitationCount": 9,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/978-1-4302-5990-9.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2015-04-27",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Awad2015EfficientLM,\n author = {M. Awad and R. Khanna},\n booktitle = {Apress},\n title = {Efficient Learning Machines},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b8eb7da56dae58f77788c33a57b5b810ca930527",
            "@type": "ScholarlyArticle",
            "paperId": "b8eb7da56dae58f77788c33a57b5b810ca930527",
            "corpusId": 1998068,
            "url": "https://www.semanticscholar.org/paper/b8eb7da56dae58f77788c33a57b5b810ca930527",
            "title": "The Geometry of ROC Space: Understanding Machine Learning Metrics through ROC Isometrics",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2003,
            "externalIds": {
                "DBLP": "conf/icml/Flach03",
                "MAG": "2143017915",
                "CorpusId": 1998068
            },
            "abstract": "Many different metrics are used in machine learning and data mining to build and evaluate models. However, there is no general theory of machine learning metrics, that could answer questions such as: When we simultaneously want to optimise two criteria, how can or should they be traded off? Some metrics are inherently independent of class and misclassification cost distributions, while other are not -- can this be made more precise? This paper provides a derivation of ROC space from first principles through 3D ROC space and the skew ratio, and redefines metrics in these dimensions. The paper demonstrates that the graphical depiction of machine learning metrics by means of ROC isometrics gives many useful insights into the characteristics of these metrics, and provides a foundation on which a theory of machine learning metrics can be built.",
            "referenceCount": 15,
            "citationCount": 306,
            "influentialCitationCount": 48,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2003-08-21",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Flach2003TheGO,\n author = {Peter A. Flach},\n booktitle = {International Conference on Machine Learning},\n pages = {194-201},\n title = {The Geometry of ROC Space: Understanding Machine Learning Metrics through ROC Isometrics},\n year = {2003}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:562a596836f42714d81e1f861671959ba12e0246",
            "@type": "ScholarlyArticle",
            "paperId": "562a596836f42714d81e1f861671959ba12e0246",
            "corpusId": 5621370,
            "url": "https://www.semanticscholar.org/paper/562a596836f42714d81e1f861671959ba12e0246",
            "title": "Machine Learning Methods for Predicting Failures in Hard Drives: A Multiple-Instance Application",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2005,
            "externalIds": {
                "DBLP": "journals/jmlr/MurrayHK05",
                "MAG": "2119381450",
                "CorpusId": 5621370
            },
            "abstract": "We compare machine learning methods applied to a difficult real-world problem: predicting computer hard-drive failure using attributes monitored internally by individual drives. The problem is one of detecting rare events in a time series of noisy and nonparametrically-distributed data. We develop a new algorithm based on the multiple-instance learning framework and the naive Bayesian classifier (mi-NB) which is specifically designed for the low false-alarm case, and is shown to have promising performance. Other methods compared are support vector machines (SVMs), unsupervised clustering, and non-parametric statistical tests (rank-sum and reverse arrangements). The failure-prediction performance of the SVM, rank-sum and mi-NB algorithm is considerably better than the threshold method currently implemented in drives, while maintaining low false alarm rates. Our results suggest that nonparametric statistical tests should be considered for learning problems involving detecting rare events in time series data. An appendix details the calculation of rank-sum significance probabilities in the case of discrete, tied observations, and we give new recommendations about when the exact calculation should be used instead of the commonly-used normal approximation. These normal approximations may be particularly inaccurate for rare event problems like hard drive failures.",
            "referenceCount": 42,
            "citationCount": 278,
            "influentialCitationCount": 28,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2005-12-01",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "6"
            },
            "citationStyles": {
                "bibtex": "@Article{Murray2005MachineLM,\n author = {Joseph F. Murray and G. Hughes and K. Kreutz-Delgado},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {783-816},\n title = {Machine Learning Methods for Predicting Failures in Hard Drives: A Multiple-Instance Application},\n volume = {6},\n year = {2005}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:158edd3fe6212306487e7173a5de9383a55b59bb",
            "@type": "ScholarlyArticle",
            "paperId": "158edd3fe6212306487e7173a5de9383a55b59bb",
            "corpusId": 431437,
            "url": "https://www.semanticscholar.org/paper/158edd3fe6212306487e7173a5de9383a55b59bb",
            "title": "Advanced Lectures on Machine Learning",
            "venue": "Lecture Notes in Computer Science",
            "publicationVenue": {
                "id": "urn:research:2f5d0e8a-faad-4f10-b323-2b2e3c439a78",
                "name": "Lecture Notes in Computer Science",
                "alternate_names": [
                    "LNCS",
                    "Transactions on Computational Systems Biology",
                    "Trans Comput Syst Biology",
                    "Lect Note Comput Sci"
                ],
                "issn": "0302-9743",
                "url": "http://www.springer.com/lncs"
            },
            "year": 2004,
            "externalIds": {
                "DBLP": "conf/ac/2003ml",
                "MAG": "1500474910",
                "DOI": "10.1007/b100712",
                "CorpusId": 431437
            },
            "abstract": null,
            "referenceCount": 79,
            "citationCount": 278,
            "influentialCitationCount": 6,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2004-09-01",
            "journal": {
                "name": null,
                "volume": "3176"
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Bousquet2004AdvancedLO,\n author = {O. Bousquet and U. V. Luxburg and G. R\u00e4tsch},\n booktitle = {Lecture Notes in Computer Science},\n title = {Advanced Lectures on Machine Learning},\n volume = {3176},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:675b93244f917d5dc2c79b89d2936d81d077e663",
            "@type": "ScholarlyArticle",
            "paperId": "675b93244f917d5dc2c79b89d2936d81d077e663",
            "corpusId": 1303612,
            "url": "https://www.semanticscholar.org/paper/675b93244f917d5dc2c79b89d2936d81d077e663",
            "title": "Using Machine Learning to Break Visual Human Interaction Proofs (HIPs)",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2004,
            "externalIds": {
                "MAG": "2147019420",
                "DBLP": "conf/nips/ChellapillaS04",
                "CorpusId": 1303612
            },
            "abstract": "Machine learning is often used to automatically solve human tasks. In this paper, we look for tasks where machine learning algorithms are not as good as humans with the hope of gaining insight into their current limitations. We studied various Human Interactive Proofs (HIPs) on the market, because they are systems designed to tell computers and humans apart by posing challenges presumably too hard for computers. We found that most HIPs are pure recognition tasks which can easily be broken using machine learning. The harder HIPs use a combination of segmentation and recognition tasks. From this observation, we found that building segmentation tasks is the most effective way to confuse machine learning algorithms. This has enabled us to build effective HIPs (which we deployed in MSN Passport), as well as design challenging segmentation tasks for machine learning algorithms.",
            "referenceCount": 8,
            "citationCount": 293,
            "influentialCitationCount": 23,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2004-12-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Chellapilla2004UsingML,\n author = {K. Chellapilla and P. Simard},\n booktitle = {Neural Information Processing Systems},\n pages = {265-272},\n title = {Using Machine Learning to Break Visual Human Interaction Proofs (HIPs)},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ea2c5c6e84ce5eaea52ecb8bc01a738005cf2092",
            "@type": "ScholarlyArticle",
            "paperId": "ea2c5c6e84ce5eaea52ecb8bc01a738005cf2092",
            "corpusId": 11661272,
            "url": "https://www.semanticscholar.org/paper/ea2c5c6e84ce5eaea52ecb8bc01a738005cf2092",
            "title": "LEARNABLE EVOLUTION MODEL: Evolutionary Processes Guided by Machine Learning",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 2004,
            "externalIds": {
                "MAG": "1587104749",
                "DBLP": "journals/ml/Michalski00",
                "DOI": "10.1023/A:1007677805582",
                "CorpusId": 11661272
            },
            "abstract": null,
            "referenceCount": 69,
            "citationCount": 248,
            "influentialCitationCount": 27,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1023%2FA%3A1007677805582.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": "Machine Learning",
                "volume": "38"
            },
            "citationStyles": {
                "bibtex": "@Article{Michalski2004LEARNABLEEM,\n author = {R. Michalski},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {9-40},\n title = {LEARNABLE EVOLUTION MODEL: Evolutionary Processes Guided by Machine Learning},\n volume = {38},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c21ce2fd906ef6b5c669d55458bd956155eda14f",
            "@type": "ScholarlyArticle",
            "paperId": "c21ce2fd906ef6b5c669d55458bd956155eda14f",
            "corpusId": 58809019,
            "url": "https://www.semanticscholar.org/paper/c21ce2fd906ef6b5c669d55458bd956155eda14f",
            "title": "Machine Learning and Data Mining; Methods and Applications",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1998,
            "externalIds": {
                "MAG": "1531110464",
                "CorpusId": 58809019
            },
            "abstract": "From the Publisher: \nMaster the new computational tools to get the most out of your information system. \nThis practical guide, the first to clearly outline the situation for the benefit of engineers and scientists, provides a straightforward introduction to basic machine learning and data mining methods, covering the analysis of numerical, text, and sound data.",
            "referenceCount": 5,
            "citationCount": 392,
            "influentialCitationCount": 10,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1998-07-01",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Michalski1998MachineLA,\n author = {R. Michalski and I. Bratko and Avan Bratko},\n title = {Machine Learning and Data Mining; Methods and Applications},\n year = {1998}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:58de638505046e7de5fe7cc0660b4c6d79247488",
            "@type": "ScholarlyArticle",
            "paperId": "58de638505046e7de5fe7cc0660b4c6d79247488",
            "corpusId": 8618254,
            "url": "https://www.semanticscholar.org/paper/58de638505046e7de5fe7cc0660b4c6d79247488",
            "title": "Machine Learning for Information Extraction in Informal Domains",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 2000,
            "externalIds": {
                "MAG": "2163915185",
                "DBLP": "journals/ml/Freitag00",
                "DOI": "10.1023/A:1007601113994",
                "CorpusId": 8618254
            },
            "abstract": null,
            "referenceCount": 88,
            "citationCount": 394,
            "influentialCitationCount": 31,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1023/A:1007601113994.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2000-05-01",
            "journal": {
                "name": "Machine Learning",
                "volume": "39"
            },
            "citationStyles": {
                "bibtex": "@Article{Freitag2000MachineLF,\n author = {Dayne Freitag},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {169-202},\n title = {Machine Learning for Information Extraction in Informal Domains},\n volume = {39},\n year = {2000}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7b7222ac076d211d7fcae7d012bebcc4ea71e952",
            "@type": "ScholarlyArticle",
            "paperId": "7b7222ac076d211d7fcae7d012bebcc4ea71e952",
            "corpusId": 1059822,
            "url": "https://www.semanticscholar.org/paper/7b7222ac076d211d7fcae7d012bebcc4ea71e952",
            "title": "An Empirical Comparison of Pattern Recognition, Neural Nets, and Machine Learning Classification Methods",
            "venue": "International Joint Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:67f7f831-711a-43c8-8785-1e09005359b5",
                "name": "International Joint Conference on Artificial Intelligence",
                "alternate_names": [
                    "Int Jt Conf Artif Intell",
                    "IJCAI"
                ],
                "issn": null,
                "url": "http://www.ijcai.org/"
            },
            "year": 1989,
            "externalIds": {
                "MAG": "1559570474",
                "DBLP": "conf/ijcai/WeissK89",
                "CorpusId": 1059822
            },
            "abstract": "Classification methods from statistical pattern recognition, neural nets, and machine learning were applied to four real-world data sets. Each of these data sets has been previously analyzed and reported in the statistical, medical, or machine learning literature. The data sets are characterized by statisucal uncertainty; there is no completely accurate solution to these problems. Training and testing or resampling techniques are used to estimate the true error rates of the classification methods. Detailed attention is given to the analysis of performance of the neural nets using back propagation. For these problems, which have relatively few hypotheses and features, the machine learning procedures for rule induction or tree induction clearly performed best.",
            "referenceCount": 37,
            "citationCount": 534,
            "influentialCitationCount": 21,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1989-08-20",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Weiss1989AnEC,\n author = {S. Weiss and I. Kapouleas},\n booktitle = {International Joint Conference on Artificial Intelligence},\n pages = {781-787},\n title = {An Empirical Comparison of Pattern Recognition, Neural Nets, and Machine Learning Classification Methods},\n year = {1989}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5dcb588150d84ef6d1b1ed6ca96e2fd62399de2c",
            "@type": "ScholarlyArticle",
            "paperId": "5dcb588150d84ef6d1b1ed6ca96e2fd62399de2c",
            "corpusId": 57043060,
            "url": "https://www.semanticscholar.org/paper/5dcb588150d84ef6d1b1ed6ca96e2fd62399de2c",
            "title": "Readings in Machine Learning",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1991,
            "externalIds": {
                "MAG": "1514477737",
                "CorpusId": 57043060
            },
            "abstract": "From the Publisher: \nThe ability to learn is a fundamental characteristic of intelligent behavior. Consequently, machine learning has been a focus of artificial intelligence since the beginnings of AI in the 1950s. The 1980s saw tremendous growth in the field, and this growth promises to continue with valuable contributions to science, engineering, and business. \n \nReadings in Machine Learning collects the best of the published machine learning literature, including papers that address a wide range of learning tasks, and that introduce a variety of techniques for giving machines the ability to learn. The editors, in cooperation with a group of expert referees, have chosen important papers that empirically study, theoretically analyze, or psychologically justify machine learning algorithms. The papers are grouped into a dozen categories, each of which is introduced by the editors.",
            "referenceCount": 0,
            "citationCount": 402,
            "influentialCitationCount": 5,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1991-03-01",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Shavlik1991ReadingsIM,\n author = {J. Shavlik and T. Deitterich and Thomas G. Dietterich},\n title = {Readings in Machine Learning},\n year = {1991}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b379c5eb2f8cc501e855d295fa5712294ca2b3ed",
            "@type": "ScholarlyArticle",
            "paperId": "b379c5eb2f8cc501e855d295fa5712294ca2b3ed",
            "corpusId": 1137942,
            "url": "https://www.semanticscholar.org/paper/b379c5eb2f8cc501e855d295fa5712294ca2b3ed",
            "title": "Application of Machine Learning Algorithms to KDD Intrusion Detection Dataset within Misuse Detection Context",
            "venue": "International Conference on Machine Learning; Models, Technologies and Applications",
            "publicationVenue": {
                "id": "urn:research:0adb706b-c511-4397-8cb5-2e697c984a7f",
                "name": "International Conference on Machine Learning; Models, Technologies and Applications",
                "alternate_names": [
                    "MLMTA",
                    "Int Conf Mach Learn Model Technol Appl"
                ],
                "issn": null,
                "url": null
            },
            "year": 2003,
            "externalIds": {
                "MAG": "135902891",
                "DBLP": "conf/mlmta/SabhnaniS03",
                "CorpusId": 1137942
            },
            "abstract": "A small subset of machine learning algorithms, mostly inductive learning based, applied to the KDD 1999 Cup intrusion detection dataset resulted in dismal performance for user-to-root and remote-to-local attack categories as reported in the recent literature. The uncertainty to explore if other machine learning algorithms can demonstrate better performance compared to the ones already employed constitutes the motivation for the study reported herein. Specifically, exploration of if certain algorithms perform better for certain attack classes and consequently, if a multi-expert classifier design can deliver desired performance measure is of high interest. This paper evaluates performance of a comprehensive set of pattern recognition and machine learning algorithms on four attack categories as found in the KDD 1999 Cup intrusion detection dataset. Results of simulation study implemented to that effect indicated that certain classification algorithms perform better for certain attack categories: a specific algorithm specialized for a given attack category . Consequently, a multi-classifier model, where a specific detection algorithm is associated with an attack category for which it is the most promising, was built. Empirical results obtained through simulation indicate that noticeable performance improvement was achieved for probing, denial of service, and user-to-root",
            "referenceCount": 17,
            "citationCount": 318,
            "influentialCitationCount": 16,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Sabhnani2003ApplicationOM,\n author = {Maheshkumar Sabhnani and G. Serpen},\n booktitle = {International Conference on Machine Learning; Models, Technologies and Applications},\n pages = {209-215},\n title = {Application of Machine Learning Algorithms to KDD Intrusion Detection Dataset within Misuse Detection Context},\n year = {2003}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:43456f4e5f2a7c56b781afe2e2d4e4aed297ceb0",
            "@type": "ScholarlyArticle",
            "paperId": "43456f4e5f2a7c56b781afe2e2d4e4aed297ceb0",
            "corpusId": 18224413,
            "url": "https://www.semanticscholar.org/paper/43456f4e5f2a7c56b781afe2e2d4e4aed297ceb0",
            "title": "Bayesian Inference: An Introduction to Principles and Practice in Machine Learning",
            "venue": "Advanced Lectures on Machine Learning",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2003,
            "externalIds": {
                "DBLP": "conf/ac/Tipping03",
                "MAG": "1485550703",
                "DOI": "10.1007/978-3-540-28650-9_3",
                "CorpusId": 18224413
            },
            "abstract": null,
            "referenceCount": 14,
            "citationCount": 282,
            "influentialCitationCount": 23,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2003-02-02",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Tipping2003BayesianIA,\n author = {Michael E. Tipping},\n booktitle = {Advanced Lectures on Machine Learning},\n pages = {41-62},\n title = {Bayesian Inference: An Introduction to Principles and Practice in Machine Learning},\n year = {2003}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3efcb97c1de1c87832a7a1d99e91801992a938ec",
            "@type": "ScholarlyArticle",
            "paperId": "3efcb97c1de1c87832a7a1d99e91801992a938ec",
            "corpusId": 11738364,
            "url": "https://www.semanticscholar.org/paper/3efcb97c1de1c87832a7a1d99e91801992a938ec",
            "title": "Crafting Papers on Machine Learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2000,
            "externalIds": {
                "MAG": "1532854728",
                "DBLP": "conf/icml/Langley00",
                "CorpusId": 11738364
            },
            "abstract": "This essay gives advice to authors of papers on machine learning, although much of it car-ries over to other computational disciplines. The issues covered include the material that should appear in a well-balanced paper, factors that arise in di(cid:11)erent approaches to evaluation, and ways to improve a submission's ability to communicate ideas to its readers.",
            "referenceCount": 8,
            "citationCount": 341,
            "influentialCitationCount": 4,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2000-06-29",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Langley2000CraftingPO,\n author = {P. Langley},\n booktitle = {International Conference on Machine Learning},\n pages = {1207-1216},\n title = {Crafting Papers on Machine Learning},\n year = {2000}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3ecdaaa55313520b50ae17de9f4f6650403754a3",
            "@type": "ScholarlyArticle",
            "paperId": "3ecdaaa55313520b50ae17de9f4f6650403754a3",
            "corpusId": 17414427,
            "url": "https://www.semanticscholar.org/paper/3ecdaaa55313520b50ae17de9f4f6650403754a3",
            "title": "Book Review: C4.5: Programs for Machine Learning by J. Ross Quinlan. Morgan Kaufmann Publishers, Inc., 1993",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 1994,
            "externalIds": {
                "MAG": "1486487250",
                "DOI": "10.1023/A:1022645310020",
                "CorpusId": 17414427
            },
            "abstract": null,
            "referenceCount": 6,
            "citationCount": 316,
            "influentialCitationCount": 6,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1023/A:1022645310020.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "1994-09-01",
            "journal": {
                "name": "Machine Learning",
                "volume": "16"
            },
            "citationStyles": {
                "bibtex": "@Article{Salzberg1994BookRC,\n author = {S. Salzberg},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {235-240},\n title = {Book Review: C4.5: Programs for Machine Learning by J. Ross Quinlan. Morgan Kaufmann Publishers, Inc., 1993},\n volume = {16},\n year = {1994}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1bf05a4ad4b6dd4b6d14d6d2dc7a9354dd1f4425",
            "@type": "ScholarlyArticle",
            "paperId": "1bf05a4ad4b6dd4b6d14d6d2dc7a9354dd1f4425",
            "corpusId": 34383491,
            "url": "https://www.semanticscholar.org/paper/1bf05a4ad4b6dd4b6d14d6d2dc7a9354dd1f4425",
            "title": "A Machine Learning Approach to Workflow Management",
            "venue": "European Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:f9a81beb-9bcb-43bc-96a7-67cf23dba2d6",
                "name": "European Conference on Machine Learning",
                "alternate_names": [
                    "Eur Conf Mach Learn",
                    "European conference on Machine Learning",
                    "Eur conf Mach Learn",
                    "ECML"
                ],
                "issn": null,
                "url": "https://link.springer.com/conference/ecml"
            },
            "year": 2000,
            "externalIds": {
                "DBLP": "conf/ecml/Herbst00",
                "MAG": "189242244",
                "DOI": "10.1007/3-540-45164-1_19",
                "CorpusId": 34383491
            },
            "abstract": null,
            "referenceCount": 15,
            "citationCount": 238,
            "influentialCitationCount": 7,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/3-540-45164-1_19.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2000-05-31",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Herbst2000AML,\n author = {J. Herbst},\n booktitle = {European Conference on Machine Learning},\n pages = {183-194},\n title = {A Machine Learning Approach to Workflow Management},\n year = {2000}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b9af24fa3faf6dbdc5e952857697588708fad8f5",
            "@type": "ScholarlyArticle",
            "paperId": "b9af24fa3faf6dbdc5e952857697588708fad8f5",
            "corpusId": 11905291,
            "url": "https://www.semanticscholar.org/paper/b9af24fa3faf6dbdc5e952857697588708fad8f5",
            "title": "Guest Editors' Introduction: On Applied Research in Machine Learning",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 1998,
            "externalIds": {
                "MAG": "1494846899",
                "DBLP": "journals/ml/ProvostK98",
                "DOI": "10.1023/A:1007442505281",
                "CorpusId": 11905291
            },
            "abstract": null,
            "referenceCount": 20,
            "citationCount": 298,
            "influentialCitationCount": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1023/A:1007442505281.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1998-02-01",
            "journal": {
                "name": "Machine Learning",
                "volume": "30"
            },
            "citationStyles": {
                "bibtex": "@Article{Provost1998GuestEI,\n author = {F. Provost and Ron Kohavi},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {127-132},\n title = {Guest Editors' Introduction: On Applied Research in Machine Learning},\n volume = {30},\n year = {1998}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1f7ed2ebc6b641e3804cf177fd42a1b8de95003b",
            "@type": "ScholarlyArticle",
            "paperId": "1f7ed2ebc6b641e3804cf177fd42a1b8de95003b",
            "corpusId": 110389766,
            "url": "https://www.semanticscholar.org/paper/1f7ed2ebc6b641e3804cf177fd42a1b8de95003b",
            "title": "Machine-learning techniques and their applications in manufacturing",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2005,
            "externalIds": {
                "MAG": "2035115911",
                "DOI": "10.1243/095440505X32274",
                "CorpusId": 110389766
            },
            "abstract": "Abstract Machine learning is concerned with enabling computer programs automatically to improve their performance at some tasks through experience. Manufacturing is an area where the application of machine learning can be very fruitful. However, little has been published about the use of machine-learning techniques in the manufacturing domain. This paper evaluates several machine-learning techniques and examines applications in which they have been successfully deployed. Special attention is given to inductive learning, which is among the most mature of the machine-learning approaches currently available. Current trends and recent developments in machine-learning research are also discussed. The paper concludes with a summary of some of the key research issues in machine learning.",
            "referenceCount": 157,
            "citationCount": 120,
            "influentialCitationCount": 3,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2005-05-01",
            "journal": {
                "name": "Proceedings of the Institution of Mechanical Engineers, Part B: Journal of Engineering Manufacture",
                "volume": "219"
            },
            "citationStyles": {
                "bibtex": "@Article{Pham2005MachinelearningTA,\n author = {D. T. Pham and A. Afify},\n journal = {Proceedings of the Institution of Mechanical Engineers, Part B: Journal of Engineering Manufacture},\n pages = {395 - 412},\n title = {Machine-learning techniques and their applications in manufacturing},\n volume = {219},\n year = {2005}\n}\n"
            }
        }
    }
]