[
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1bf05a4ad4b6dd4b6d14d6d2dc7a9354dd1f4425",
            "@type": "ScholarlyArticle",
            "paperId": "1bf05a4ad4b6dd4b6d14d6d2dc7a9354dd1f4425",
            "corpusId": 34383491,
            "url": "https://www.semanticscholar.org/paper/1bf05a4ad4b6dd4b6d14d6d2dc7a9354dd1f4425",
            "title": "A Machine Learning Approach to Workflow Management",
            "venue": "European Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:f9a81beb-9bcb-43bc-96a7-67cf23dba2d6",
                "name": "European Conference on Machine Learning",
                "alternate_names": [
                    "Eur Conf Mach Learn",
                    "European conference on Machine Learning",
                    "Eur conf Mach Learn",
                    "ECML"
                ],
                "issn": null,
                "url": "https://link.springer.com/conference/ecml"
            },
            "year": 2000,
            "externalIds": {
                "DBLP": "conf/ecml/Herbst00",
                "MAG": "189242244",
                "DOI": "10.1007/3-540-45164-1_19",
                "CorpusId": 34383491
            },
            "abstract": null,
            "referenceCount": 15,
            "citationCount": 238,
            "influentialCitationCount": 7,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/3-540-45164-1_19.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2000-05-31",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Herbst2000AML,\n author = {J. Herbst},\n booktitle = {European Conference on Machine Learning},\n pages = {183-194},\n title = {A Machine Learning Approach to Workflow Management},\n year = {2000}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b9af24fa3faf6dbdc5e952857697588708fad8f5",
            "@type": "ScholarlyArticle",
            "paperId": "b9af24fa3faf6dbdc5e952857697588708fad8f5",
            "corpusId": 11905291,
            "url": "https://www.semanticscholar.org/paper/b9af24fa3faf6dbdc5e952857697588708fad8f5",
            "title": "Guest Editors' Introduction: On Applied Research in Machine Learning",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 1998,
            "externalIds": {
                "MAG": "1494846899",
                "DBLP": "journals/ml/ProvostK98",
                "DOI": "10.1023/A:1007442505281",
                "CorpusId": 11905291
            },
            "abstract": null,
            "referenceCount": 20,
            "citationCount": 298,
            "influentialCitationCount": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1023/A:1007442505281.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1998-02-01",
            "journal": {
                "name": "Machine Learning",
                "volume": "30"
            },
            "citationStyles": {
                "bibtex": "@Article{Provost1998GuestEI,\n author = {F. Provost and Ron Kohavi},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {127-132},\n title = {Guest Editors' Introduction: On Applied Research in Machine Learning},\n volume = {30},\n year = {1998}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3ecdaaa55313520b50ae17de9f4f6650403754a3",
            "@type": "ScholarlyArticle",
            "paperId": "3ecdaaa55313520b50ae17de9f4f6650403754a3",
            "corpusId": 17414427,
            "url": "https://www.semanticscholar.org/paper/3ecdaaa55313520b50ae17de9f4f6650403754a3",
            "title": "Book Review: C4.5: Programs for Machine Learning by J. Ross Quinlan. Morgan Kaufmann Publishers, Inc., 1993",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 1994,
            "externalIds": {
                "MAG": "1486487250",
                "DOI": "10.1023/A:1022645310020",
                "CorpusId": 17414427
            },
            "abstract": null,
            "referenceCount": 6,
            "citationCount": 316,
            "influentialCitationCount": 6,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1023/A:1022645310020.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "1994-09-01",
            "journal": {
                "name": "Machine Learning",
                "volume": "16"
            },
            "citationStyles": {
                "bibtex": "@Article{Salzberg1994BookRC,\n author = {S. Salzberg},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {235-240},\n title = {Book Review: C4.5: Programs for Machine Learning by J. Ross Quinlan. Morgan Kaufmann Publishers, Inc., 1993},\n volume = {16},\n year = {1994}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e9415d9a6e0065b46acd99ba4ff8b89bd1435fc8",
            "@type": "ScholarlyArticle",
            "paperId": "e9415d9a6e0065b46acd99ba4ff8b89bd1435fc8",
            "corpusId": 845125,
            "url": "https://www.semanticscholar.org/paper/e9415d9a6e0065b46acd99ba4ff8b89bd1435fc8",
            "title": "Machine Learning and Software Engineering",
            "venue": "14th IEEE International Conference on Tools with Artificial Intelligence, 2002. (ICTAI 2002). Proceedings.",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2002,
            "externalIds": {
                "MAG": "3017201198",
                "DBLP": "conf/ictai/ZhangT02",
                "DOI": "10.1023/A:1023760326768",
                "CorpusId": 845125
            },
            "abstract": null,
            "referenceCount": 104,
            "citationCount": 190,
            "influentialCitationCount": 11,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2002-11-04",
            "journal": {
                "name": "Software Quality Journal",
                "volume": "11"
            },
            "citationStyles": {
                "bibtex": "@Article{Zhang2002MachineLA,\n author = {Du Zhang and J. Tsai},\n booktitle = {14th IEEE International Conference on Tools with Artificial Intelligence, 2002. (ICTAI 2002). Proceedings.},\n journal = {Software Quality Journal},\n pages = {87-119},\n title = {Machine Learning and Software Engineering},\n volume = {11},\n year = {2002}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f4971ff0f6ae626e78131bafa012eadfe8e238e2",
            "@type": "ScholarlyArticle",
            "paperId": "f4971ff0f6ae626e78131bafa012eadfe8e238e2",
            "corpusId": 60974791,
            "url": "https://www.semanticscholar.org/paper/f4971ff0f6ae626e78131bafa012eadfe8e238e2",
            "title": "Machine learning: an artificial intelligence approach volume III",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1990,
            "externalIds": {
                "MAG": "1718004567",
                "CorpusId": 60974791
            },
            "abstract": "This book reflects the expansion of machine learning research through presentation of recent advances in the field. The book provides an account of current research directions. Major topics covered include the following: learning concepts and rules from examples; cognitive aspects of learning; learning by analogy; learning by observation and discovery; and an exploration of general aspects of learning.",
            "referenceCount": 0,
            "citationCount": 307,
            "influentialCitationCount": 2,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Art",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1990-06-01",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Kodratoff1990MachineLA,\n author = {Y. Kodratoff and R. Michalski},\n title = {Machine learning: an artificial intelligence approach volume III},\n year = {1990}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c5ca3b6ad0e74ab8a83d77ebef0c5a498ba5d781",
            "@type": "ScholarlyArticle",
            "paperId": "c5ca3b6ad0e74ab8a83d77ebef0c5a498ba5d781",
            "corpusId": 3057396,
            "url": "https://www.semanticscholar.org/paper/c5ca3b6ad0e74ab8a83d77ebef0c5a498ba5d781",
            "title": "On-line Algorithms in Machine Learning",
            "venue": "Online Algorithms",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1996,
            "externalIds": {
                "MAG": "2397425272",
                "DBLP": "conf/dagstuhl/Blum96",
                "DOI": "10.1007/BFb0029575",
                "CorpusId": 3057396
            },
            "abstract": null,
            "referenceCount": 38,
            "citationCount": 255,
            "influentialCitationCount": 13,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www-stat.wharton.upenn.edu/~steele/Courses/9xx/Resources2/BlumSurveyOLA.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Blum1996OnlineAI,\n author = {Avrim Blum},\n booktitle = {Online Algorithms},\n pages = {306-325},\n title = {On-line Algorithms in Machine Learning},\n year = {1996}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1a924613bd171ab9af71e82a2316e10ce4965a7d",
            "@type": "ScholarlyArticle",
            "paperId": "1a924613bd171ab9af71e82a2316e10ce4965a7d",
            "corpusId": 11430990,
            "url": "https://www.semanticscholar.org/paper/1a924613bd171ab9af71e82a2316e10ce4965a7d",
            "title": "Interactive Machine Learning Mustafa Bilgic",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2000,
            "externalIds": {
                "CorpusId": 11430990
            },
            "abstract": null,
            "referenceCount": 6,
            "citationCount": 465,
            "influentialCitationCount": 32,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Balcan2000InteractiveML,\n author = {Maria-Florina Balcan},\n title = {Interactive Machine Learning Mustafa Bilgic},\n year = {2000}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:180882b3f2ea5dfe0a554deead2c0ceb837ee933",
            "@type": "ScholarlyArticle",
            "paperId": "180882b3f2ea5dfe0a554deead2c0ceb837ee933",
            "corpusId": 7364455,
            "url": "https://www.semanticscholar.org/paper/180882b3f2ea5dfe0a554deead2c0ceb837ee933",
            "title": "Machine learning as an experimental science",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 2004,
            "externalIds": {
                "MAG": "2400138769",
                "DBLP": "conf/ecml/KiblerL88",
                "DOI": "10.1007/BF00115008",
                "CorpusId": 7364455
            },
            "abstract": null,
            "referenceCount": 7,
            "citationCount": 130,
            "influentialCitationCount": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/BF00115008.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Sociology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": "Machine Learning",
                "volume": "3"
            },
            "citationStyles": {
                "bibtex": "@Article{Langley2004MachineLA,\n author = {P. Langley},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {5-8},\n title = {Machine learning as an experimental science},\n volume = {3},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9a4da6802d1eca17a07ded017a87bef7001189c4",
            "@type": "ScholarlyArticle",
            "paperId": "9a4da6802d1eca17a07ded017a87bef7001189c4",
            "corpusId": 59718087,
            "url": "https://www.semanticscholar.org/paper/9a4da6802d1eca17a07ded017a87bef7001189c4",
            "title": "Introduction to Machine Learning (Adaptive Computation and Machine Learning)",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2004,
            "externalIds": {
                "MAG": "33990384",
                "CorpusId": 59718087
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 372,
            "influentialCitationCount": 29,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2004-10-01",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Alpaydin2004IntroductionTM,\n author = {Ethem Alpaydin},\n title = {Introduction to Machine Learning (Adaptive Computation and Machine Learning)},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a54657b8de38a18f30fd154d713f9522f705166c",
            "@type": "ScholarlyArticle",
            "paperId": "a54657b8de38a18f30fd154d713f9522f705166c",
            "corpusId": 5437238,
            "url": "https://www.semanticscholar.org/paper/a54657b8de38a18f30fd154d713f9522f705166c",
            "title": "Computational complexity of machine learning",
            "venue": "ACM distinguished dissertations",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1990,
            "externalIds": {
                "DBLP": "books/daglib/0066852",
                "MAG": "1506934790",
                "CorpusId": 5437238
            },
            "abstract": "This thesis is a study of the computational complexity of machine learning from examples in the distribution-free model introduced by L. G. Valiant (V84). In the distribution-free model, a learning algorithm receives positive and negative examples of an unknown target set (or concept) that is chosen from some known class of sets (or concept class). These examples are generated randomly according to a fixed but unknown probability distribution representing Nature, and the goal of the learning algorithm is to infer an hypothesis concept that closely approximates the target concept with respect to the unknown distribution. This thesis is concerned with proving theorems about learning in this formal mathematical model. \nWe are interested in the phenomenon of efficient learning in the distribution-free model, in the standard polynomial-time sense. Our results include general tools for determining the polynomial-time learnability of a concept class, an extensive study of efficient learning when errors are present in the examples, and lower bounds on the number of examples required for learning in our model. A centerpiece of the thesis is a series of results demonstrating the computational difficulty of learning a number of well-studied concept classes. These results are obtained by reducing some apparently hard number-theoretic problems from cryptography to the learning problems. The hard-to-learn concept classes include the sets represented by Boolean formulae, deterministic finite automata and a simplified form of neural networks. We also give algorithms for learning powerful concept classes under the uniform distribution, and give equivalences between natural models of efficient learnability. \nThis thesis also includes detailed definitions and motivation for the distribution-free model, a chapter discussing past research in this model and related models, and a short list of important open problems.",
            "referenceCount": 31,
            "citationCount": 260,
            "influentialCitationCount": 13,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1990-10-22",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Kearns1990ComputationalCO,\n author = {M. Kearns},\n booktitle = {ACM distinguished dissertations},\n pages = {I-IX, 1-165},\n title = {Computational complexity of machine learning},\n year = {1990}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:14f2b886678251cdd80dc9701c889bc55de7940d",
            "@type": "ScholarlyArticle",
            "paperId": "14f2b886678251cdd80dc9701c889bc55de7940d",
            "corpusId": 14509726,
            "url": "https://www.semanticscholar.org/paper/14f2b886678251cdd80dc9701c889bc55de7940d",
            "title": "Student Modeling and Machine Learning",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1998,
            "externalIds": {
                "MAG": "2275238446",
                "CorpusId": 14509726
            },
            "abstract": "After identifying essential student modeling issues and machine learning approaches, this paper examines how machine learning techniques have been used to automate the construction of student models as well as the background knowledge necessary for student modeling. In the process, the paper sheds light on the difficulty, suitability and potential of using machine learning for student modeling processes, and, to a lesser extent, the potential of using student modeling techniques in machine learning. (http://aied.inf.ed.ac.uk/members98/archive/vol_9/sison/full.html)",
            "referenceCount": 93,
            "citationCount": 172,
            "influentialCitationCount": 5,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Education",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": "9"
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Sison1998StudentMA,\n author = {R. Sison and M. Shimura},\n pages = {128-158},\n title = {Student Modeling and Machine Learning},\n volume = {9},\n year = {1998}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5052871d430b803b4c59f459bb26b1f76e56736e",
            "@type": "ScholarlyArticle",
            "paperId": "5052871d430b803b4c59f459bb26b1f76e56736e",
            "corpusId": 380052,
            "url": "https://www.semanticscholar.org/paper/5052871d430b803b4c59f459bb26b1f76e56736e",
            "title": "Evaluation and selection of biases in machine learning",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 1995,
            "externalIds": {
                "MAG": "2035174176",
                "DBLP": "journals/ml/GordonD95",
                "DOI": "10.1007/BF00993472",
                "CorpusId": 380052
            },
            "abstract": null,
            "referenceCount": 46,
            "citationCount": 119,
            "influentialCitationCount": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/BF00993472.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1995-07-01",
            "journal": {
                "name": "Machine Learning",
                "volume": "20"
            },
            "citationStyles": {
                "bibtex": "@Article{Spears1995EvaluationAS,\n author = {D. Spears and Marie desJardins},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {5-22},\n title = {Evaluation and selection of biases in machine learning},\n volume = {20},\n year = {1995}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:bd71831c69191c26a2a091f35509b1a80a4b1b64",
            "@type": "ScholarlyArticle",
            "paperId": "bd71831c69191c26a2a091f35509b1a80a4b1b64",
            "corpusId": 5379749,
            "url": "https://www.semanticscholar.org/paper/bd71831c69191c26a2a091f35509b1a80a4b1b64",
            "title": "Reliable Classifications with Machine Learning",
            "venue": "European Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:f9a81beb-9bcb-43bc-96a7-67cf23dba2d6",
                "name": "European Conference on Machine Learning",
                "alternate_names": [
                    "Eur Conf Mach Learn",
                    "European conference on Machine Learning",
                    "Eur conf Mach Learn",
                    "ECML"
                ],
                "issn": null,
                "url": "https://link.springer.com/conference/ecml"
            },
            "year": 2002,
            "externalIds": {
                "MAG": "2743041518",
                "DBLP": "conf/ecml/KukarK02",
                "DOI": "10.1007/3-540-36755-1_19",
                "CorpusId": 5379749
            },
            "abstract": null,
            "referenceCount": 24,
            "citationCount": 85,
            "influentialCitationCount": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007%2F3-540-36755-1_19.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2002-08-19",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Kukar2002ReliableCW,\n author = {M. Kukar and I. Kononenko},\n booktitle = {European Conference on Machine Learning},\n pages = {219-231},\n title = {Reliable Classifications with Machine Learning},\n year = {2002}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d1e5263787d888da20a28254fd7bd581618e3a06",
            "@type": "ScholarlyArticle",
            "paperId": "d1e5263787d888da20a28254fd7bd581618e3a06",
            "corpusId": 208922511,
            "url": "https://www.semanticscholar.org/paper/d1e5263787d888da20a28254fd7bd581618e3a06",
            "title": "An Overview of Machine Learning",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1983,
            "externalIds": {
                "MAG": "2026319679",
                "DOI": "10.1007/978-3-662-12405-5_1",
                "CorpusId": 208922511
            },
            "abstract": null,
            "referenceCount": 33,
            "citationCount": 264,
            "influentialCitationCount": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.mli.gmu.edu/michalski/../papers/81-85/83-02.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Carbonell1983AnOO,\n author = {J. Carbonell and R. Michalski and Tom Michael Mitchell},\n pages = {3-23},\n title = {An Overview of Machine Learning},\n year = {1983}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:65df2d9b3c656ca85e4d66c327cfd8c8d1182df3",
            "@type": "ScholarlyArticle",
            "paperId": "65df2d9b3c656ca85e4d66c327cfd8c8d1182df3",
            "corpusId": 62496462,
            "url": "https://www.semanticscholar.org/paper/65df2d9b3c656ca85e4d66c327cfd8c8d1182df3",
            "title": "Machine Learning: Neural Networks, Genetic Algorithms, and Fuzzy Systems",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1994,
            "externalIds": {
                "MAG": "2290652772",
                "DOI": "10.1108/k.1999.28.3.317.5",
                "CorpusId": 62496462
            },
            "abstract": "Perceptron Learning with a Hidden Layer An Object-Oriented Backpropagation Learning Model Concurrent Backpropagation Learning Algorithms An Adaptive Conjugate Gradient Learning Algorithm for Efficient Training of Neural Networks A Concurrent Adaptive Conjugate Gradient Learning Algorithm on MIMD Shared Memory Machines A Concurrent Genetic/Neural Network Learning Algorithm for MIMD Shared Memory Machines A Hybrid Learning Algorithm for Distributed Memory Multicomputers A Fuzzy Neural Network Learning Model Appendices References Index.",
            "referenceCount": 0,
            "citationCount": 516,
            "influentialCitationCount": 16,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1994-09-08",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Adeli1994MachineLN,\n author = {H. Adeli and S. Hung},\n title = {Machine Learning: Neural Networks, Genetic Algorithms, and Fuzzy Systems},\n year = {1994}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:22faafeba7d7443da14c1e23e549b94e40d7d6ee",
            "@type": "ScholarlyArticle",
            "paperId": "22faafeba7d7443da14c1e23e549b94e40d7d6ee",
            "corpusId": 4265138,
            "url": "https://www.semanticscholar.org/paper/22faafeba7d7443da14c1e23e549b94e40d7d6ee",
            "title": "\u201cMemo\u201d Functions and Machine Learning",
            "venue": "Nature",
            "publicationVenue": {
                "id": "urn:research:6c24a0a0-b07d-4d7b-a19b-fd09a3ed453a",
                "name": "Nature",
                "alternate_names": null,
                "issn": "0028-0836",
                "url": "https://www.nature.com/"
            },
            "year": 1968,
            "externalIds": {
                "MAG": "1965784613",
                "DOI": "10.1038/218019A0",
                "CorpusId": 4265138
            },
            "abstract": null,
            "referenceCount": 3,
            "citationCount": 541,
            "influentialCitationCount": 18,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1968-04-01",
            "journal": {
                "name": "Nature",
                "volume": "218"
            },
            "citationStyles": {
                "bibtex": "@Article{Michie1968MemoFA,\n author = {D. Michie},\n booktitle = {Nature},\n journal = {Nature},\n pages = {19-22},\n title = {\u201cMemo\u201d Functions and Machine Learning},\n volume = {218},\n year = {1968}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7649af7bf6e9d277ed045930fc08d79247e02375",
            "@type": "ScholarlyArticle",
            "paperId": "7649af7bf6e9d277ed045930fc08d79247e02375",
            "corpusId": 262910792,
            "url": "https://www.semanticscholar.org/paper/7649af7bf6e9d277ed045930fc08d79247e02375",
            "title": "Gaussian Processes in Machine Learning",
            "venue": "Advanced Lectures on Machine Learning",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2003,
            "externalIds": {
                "DBLP": "conf/ac/Rasmussen03",
                "MAG": "1502922572",
                "DOI": "10.1007/978-3-540-28650-9_4",
                "CorpusId": 262910792
            },
            "abstract": null,
            "referenceCount": 5,
            "citationCount": 216,
            "influentialCitationCount": 13,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://mlg.eng.cam.ac.uk/pub/pdf/Ras04.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2003-02-02",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Rasmussen2003GaussianPI,\n author = {Carl E. Rasmussen},\n booktitle = {Advanced Lectures on Machine Learning},\n pages = {63-71},\n title = {Gaussian Processes in Machine Learning},\n year = {2003}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:72282cfbb493c487f9f43a6270b5bd9d94b57d94",
            "@type": "ScholarlyArticle",
            "paperId": "72282cfbb493c487f9f43a6270b5bd9d94b57d94",
            "corpusId": 44522507,
            "url": "https://www.semanticscholar.org/paper/72282cfbb493c487f9f43a6270b5bd9d94b57d94",
            "title": "Machine Learning: ECML-98",
            "venue": "Lecture Notes in Computer Science",
            "publicationVenue": {
                "id": "urn:research:2f5d0e8a-faad-4f10-b323-2b2e3c439a78",
                "name": "Lecture Notes in Computer Science",
                "alternate_names": [
                    "LNCS",
                    "Transactions on Computational Systems Biology",
                    "Trans Comput Syst Biology",
                    "Lect Note Comput Sci"
                ],
                "issn": "0302-9743",
                "url": "http://www.springer.com/lncs"
            },
            "year": 1998,
            "externalIds": {
                "MAG": "2505312721",
                "DBLP": "conf/ecml/1998",
                "DOI": "10.1007/BFb0026664",
                "CorpusId": 44522507
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 387,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/bfm:978-3-540-69781-7/1?pdf=chapter%20toc",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": "1398"
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{N\u00e9dellec1998MachineLE,\n author = {C. N\u00e9dellec and C. Rouveirol},\n booktitle = {Lecture Notes in Computer Science},\n title = {Machine Learning: ECML-98},\n volume = {1398},\n year = {1998}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:298a09325dce98155779f9640ccae8fa5ddca62d",
            "@type": "ScholarlyArticle",
            "paperId": "298a09325dce98155779f9640ccae8fa5ddca62d",
            "corpusId": 1636783,
            "url": "https://www.semanticscholar.org/paper/298a09325dce98155779f9640ccae8fa5ddca62d",
            "title": "Machine-Learning Applications of Algorithmic Randomness",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 1999,
            "externalIds": {
                "MAG": "1488642921",
                "DBLP": "conf/icml/VovkGS99",
                "CorpusId": 1636783
            },
            "abstract": "Machine-LearningApplicationsofAlgorithmicRandomnessVolodyaovk,AlexGammerman,CraigSaundersComputerLearningResearchCentreandDepartmentofScienceRoyalHollowa,UniversitofLondon,Egham,SurreyTW200EX,Englandfvovk,alex,craigg@dcs.rhbnc.ac.ukAbstractMostmachinelearningalgorithmssharethefollowingdrawback:theyonlyoutputbarepredictionsbutnotthecon denceinthosepredictions.Inthe1960salgorithmicinfor-mationtheorysupplieduniversalmeasuresofcon dencebuttheseare,unfortunately,non-computable.Inthispap erwecombinetheideasofalgorithmicinformationtheorywiththetheoryofSupp ortVectormachinestoobtainpracticableapproximationsuni-versalmeasuresofcon dence.Weshowthatinsomestandardproblemsofpatternrecog-nitionourapproximationsworkell.1INTRODUCTIONTwoimp ortantdi erencesofmostmo dernmetho dsmachinelearning(suchasstatisticaltheory,seeVapnik[21],1998,orPACtheory)fromclassicalstatisticalmetho dsarethat:\u000fmachinelearningmetho dspro ducebarepredic-tions,withoutestimatingcon denceinthosepre-dictions(unlike,eg,predictionoffutureobser-vationsintraditionalstatistics(Guttman[5],1970));\u000fmanymachinelearningmetho dsaredesignedtowork(andtheirp erformanceisanalysed)un-derthegeneraliidassumption(unlikeclas-sicalparametricstatistics)andtheyareabletodealwithextremelyhigh-dimensionalhyp othesisspaces;cfVapnik[21](1998).Inthispap erwewillfurtherdeveloptheapproachofGammermanetal[4](1998)andSaunders[17Figure1:Ifthetrainingsetonlycontainsclear2sand7s,weouldliktoattachmucloercon dencethemiddleimagethantorightandleftones(1999),wherethegoalistoobtaincon dencesforpredictionsunderthegeneraliidassumptioninhigh-dimensionalsituations.Figure1demonstratesthede-sirabilityofcon dences.Themaincontributionthispap erisemb eddingtheapproachesofGammermanetal[4](1998)andSaunderset[17(1999)intoagen-eralschemebasedonthenotionofalgorithmicran-domness.Aswillb ecomeclearlater,theproblemofassigningcon dencestopredictionsiscloselyconnectedtheproblemofde ningrandomsequences.ThelatterproblemwassolvedbyKolmogorov[8](1965),whobasedhisde nitionontheexistenceUniver-salTuringMachine(thoughitb ecameclearthatKol-mogorov'sde nitiondo essolvetheproblemofde ningrandomsequencesonlyafterMartin-L\u007fof 'spap er[15],1966);Kolmogorov'sde nitionmovedthenotionofrandomnessfromthegreyareasurroundingprobabil-itytheoryandstatisticstomathematicalcomputersci-ence.Kolmogorovb elievedhisnotionofrandomnesstob easuitablebasisforapplicationsofprobability.Unfor-tunately,fateideaasdi erentfromKol-mogorov's1933axioms(Kolmogorov[7],1933),which",
            "referenceCount": 28,
            "citationCount": 190,
            "influentialCitationCount": 8,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1999-06-27",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Vovk1999MachineLearningAO,\n author = {V. Vovk and A. Gammerman and C. Saunders},\n booktitle = {International Conference on Machine Learning},\n pages = {444-453},\n title = {Machine-Learning Applications of Algorithmic Randomness},\n year = {1999}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:467568f1777bc51a15a5100516cd4fe8de62b9ab",
            "@type": "ScholarlyArticle",
            "paperId": "467568f1777bc51a15a5100516cd4fe8de62b9ab",
            "corpusId": 17216004,
            "url": "https://www.semanticscholar.org/paper/467568f1777bc51a15a5100516cd4fe8de62b9ab",
            "title": "Transfer Learning for Reinforcement Learning Domains: A Survey",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2009,
            "externalIds": {
                "DBLP": "journals/jmlr/TaylorS09",
                "MAG": "2097381042",
                "DOI": "10.5555/1577069.1755839",
                "CorpusId": 17216004
            },
            "abstract": "The reinforcement learning paradigm is a popular way to address problems that have only limited environmental feedback, rather than correctly labeled examples, as is common in other machine learning contexts. While significant progress has been made to improve learning in a single task, the idea of transfer learning has only recently been applied to reinforcement learning tasks. The core idea of transfer is that experience gained in learning to perform one task can help improve learning performance in a related, but different, task. In this article we present a framework that classifies transfer learning methods in terms of their capabilities and goals, and then use it to survey the existing literature, as well as to suggest future directions for transfer learning work.",
            "referenceCount": 131,
            "citationCount": 1728,
            "influentialCitationCount": 110,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2009-12-01",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "10"
            },
            "citationStyles": {
                "bibtex": "@Article{Taylor2009TransferLF,\n author = {Matthew E. Taylor and P. Stone},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {1633-1685},\n title = {Transfer Learning for Reinforcement Learning Domains: A Survey},\n volume = {10},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:fc09717ba476ae2408c454e5557276a9fc4d093d",
            "@type": "ScholarlyArticle",
            "paperId": "fc09717ba476ae2408c454e5557276a9fc4d093d",
            "corpusId": 5258169,
            "url": "https://www.semanticscholar.org/paper/fc09717ba476ae2408c454e5557276a9fc4d093d",
            "title": "Machine Learning: A Theoretical Approach",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1992,
            "externalIds": {
                "DBLP": "books/mk/Natarajan91",
                "MAG": "1530657611",
                "CorpusId": 5258169
            },
            "abstract": "Chapter 1 Introduction Chapter 2 Learning Concept on Countable Domains Chapter 3 Time Complexity of Concept Learning Chapter 4 Learning Concepts on Uncoutable Domains Chapter 5 Learning Functions Chapter 6 Finite Automata Chapter 7 Neural Networks Chapter 8 Generalizing the Learning Model Chapter 9 Conclusion",
            "referenceCount": 0,
            "citationCount": 264,
            "influentialCitationCount": 22,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1992-01-14",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Natarajan1992MachineLA,\n author = {B. Natarajan},\n title = {Machine Learning: A Theoretical Approach},\n year = {1992}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e2d4321c99b74859b8aa57daac7df4f1c11291cb",
            "@type": "ScholarlyArticle",
            "paperId": "e2d4321c99b74859b8aa57daac7df4f1c11291cb",
            "corpusId": 142776104,
            "url": "https://www.semanticscholar.org/paper/e2d4321c99b74859b8aa57daac7df4f1c11291cb",
            "title": "The children's machine: rethinking school in the age of the computer",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1993,
            "externalIds": {
                "MAG": "1599334645",
                "DOI": "10.5860/choice.31-1648",
                "CorpusId": 142776104
            },
            "abstract": "Yearners and Schoolers Personal Thinking School: Change and Resistance to Change Teachers A World for Learning An Anthology of Learning Stories Instructionism versus Constructionism Computerists Yearners and Schoolers Cybernetics What can be done?",
            "referenceCount": 0,
            "citationCount": 1952,
            "influentialCitationCount": 99,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Education",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1993-06-16",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Papert1993TheCM,\n author = {S. Papert},\n title = {The children's machine: rethinking school in the age of the computer},\n year = {1993}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4a6a88fcd374e4fdf2120f65b82c1382bdccfa2d",
            "@type": "ScholarlyArticle",
            "paperId": "4a6a88fcd374e4fdf2120f65b82c1382bdccfa2d",
            "corpusId": 37871036,
            "url": "https://www.semanticscholar.org/paper/4a6a88fcd374e4fdf2120f65b82c1382bdccfa2d",
            "title": "Genetic Algorithms in Machine Learning",
            "venue": "Machine Learning and Its Applications",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2001,
            "externalIds": {
                "MAG": "1847063015",
                "DBLP": "conf/ac/Shapiro01",
                "DOI": "10.1007/3-540-44673-7_7",
                "CorpusId": 37871036
            },
            "abstract": null,
            "referenceCount": 32,
            "citationCount": 106,
            "influentialCitationCount": 5,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Shapiro2001GeneticAI,\n author = {J. Shapiro},\n booktitle = {Machine Learning and Its Applications},\n pages = {146-168},\n title = {Genetic Algorithms in Machine Learning},\n year = {2001}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:884895a86fe15cb9601df4a15a1475c07f28da3c",
            "@type": "ScholarlyArticle",
            "paperId": "884895a86fe15cb9601df4a15a1475c07f28da3c",
            "corpusId": 8153773,
            "url": "https://www.semanticscholar.org/paper/884895a86fe15cb9601df4a15a1475c07f28da3c",
            "title": "Boosting for transfer learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2007,
            "externalIds": {
                "MAG": "2122838776",
                "DBLP": "conf/icml/DaiYXY07",
                "DOI": "10.1145/1273496.1273521",
                "CorpusId": 8153773
            },
            "abstract": "Traditional machine learning makes a basic assumption: the training and test data should be under the same distribution. However, in many cases, this identical-distribution assumption does not hold. The assumption might be violated when a task from one new domain comes, while there are only labeled data from a similar old domain. Labeling the new data can be costly and it would also be a waste to throw away all the old data. In this paper, we present a novel transfer learning framework called TrAdaBoost, which extends boosting-based learning algorithms (Freund & Schapire, 1997). TrAdaBoost allows users to utilize a small amount of newly labeled data to leverage the old data to construct a high-quality classification model for the new data. We show that this method can allow us to learn an accurate model using only a tiny amount of new data and a large amount of old data, even when the new data are not sufficient to train a model alone. We show that TrAdaBoost allows knowledge to be effectively transferred from the old data to the new. The effectiveness of our algorithm is analyzed theoretically and empirically to show that our iterative algorithm can converge well to an accurate model.",
            "referenceCount": 25,
            "citationCount": 1662,
            "influentialCitationCount": 188,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2007-06-20",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Dai2007BoostingFT,\n author = {Wenyuan Dai and Qiang Yang and Gui-Rong Xue and Yong Yu},\n booktitle = {International Conference on Machine Learning},\n pages = {193-200},\n title = {Boosting for transfer learning},\n year = {2007}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:22d6d9c1b7ac2738b51d93be45ac8f753f81867c",
            "@type": "ScholarlyArticle",
            "paperId": "22d6d9c1b7ac2738b51d93be45ac8f753f81867c",
            "corpusId": 8772285,
            "url": "https://www.semanticscholar.org/paper/22d6d9c1b7ac2738b51d93be45ac8f753f81867c",
            "title": "Stacked Autoencoders for Unsupervised Feature Learning and Multiple Organ Detection in a Pilot Study Using 4D Patient Data",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "publicationVenue": {
                "id": "urn:research:25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "alternate_names": [
                    "IEEE Trans Pattern Anal Mach Intell"
                ],
                "issn": "0162-8828",
                "url": "http://www.computer.org/tpami/"
            },
            "year": 2013,
            "externalIds": {
                "MAG": "2036109700",
                "DBLP": "journals/pami/ShinOCDL13",
                "DOI": "10.1109/TPAMI.2012.277",
                "CorpusId": 8772285,
                "PubMed": "23787345"
            },
            "abstract": "Medical image analysis remains a challenging application area for artificial intelligence. When applying machine learning, obtaining ground-truth labels for supervised learning is more difficult than in many more common applications of machine learning. This is especially so for datasets with abnormalities, as tissue types and the shapes of the organs in these datasets differ widely. However, organ detection in such an abnormal dataset may have many promising potential real-world applications, such as automatic diagnosis, automated radiotherapy planning, and medical image retrieval, where new multimodal medical images provide more information about the imaged tissues for diagnosis. Here, we test the application of deep learning methods to organ identification in magnetic resonance medical images, with visual and temporal hierarchical features learned to categorize object classes from an unlabeled multimodal DCE-MRI dataset so that only a weakly supervised training is required for a classifier. A probabilistic patch-based method was employed for multiple organ detection, with the features learned from the deep learning model. This shows the potential of the deep learning model for application to medical images, despite the difficulty of obtaining libraries of correctly labeled training datasets and despite the intrinsic abnormalities present in patient datasets.",
            "referenceCount": 63,
            "citationCount": 476,
            "influentialCitationCount": 20,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2013-08-01",
            "journal": {
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "volume": "35"
            },
            "citationStyles": {
                "bibtex": "@Article{Shin2013StackedAF,\n author = {Hoo-Chang Shin and M. Orton and D. Collins and S. Doran and M. Leach},\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n pages = {1930-1943},\n title = {Stacked Autoencoders for Unsupervised Feature Learning and Multiple Organ Detection in a Pilot Study Using 4D Patient Data},\n volume = {35},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:48e752c719d33ff55b3b3bec3538727f8ce69399",
            "@type": "ScholarlyArticle",
            "paperId": "48e752c719d33ff55b3b3bec3538727f8ce69399",
            "corpusId": 1411149,
            "url": "https://www.semanticscholar.org/paper/48e752c719d33ff55b3b3bec3538727f8ce69399",
            "title": "Ontology Learning for the Semantic Web",
            "venue": "IEEE Intelligent Systems",
            "publicationVenue": {
                "id": "urn:research:7404efea-88b2-4c7c-8cb1-b3a8ced6363f",
                "name": "IEEE Intelligent Systems",
                "alternate_names": [
                    "IEEE Intell Syst"
                ],
                "issn": "1541-1672",
                "url": "https://ieeexplore.ieee.org/servlet/opac?punumber=9670"
            },
            "year": 2002,
            "externalIds": {
                "DBLP": "journals/expert/MaedcheS01",
                "MAG": "2132232128",
                "DOI": "10.1109/5254.920602",
                "CorpusId": 1411149
            },
            "abstract": "The Semantic Web relies heavily on formal ontologies to structure data for comprehensive and transportable machine understanding. Thus, the proliferation of ontologies factors largely in the Semantic Web's success. The authors present an ontology learning framework that extends typical ontology engineering environments by using semiautomatic ontology construction tools. The framework encompasses ontology import, extraction, pruning, refinement and evaluation.",
            "referenceCount": 30,
            "citationCount": 2238,
            "influentialCitationCount": 96,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/bfm%3A978-1-4615-0925-7%2F1",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2002-02-28",
            "journal": {
                "name": "IEEE Intell. Syst.",
                "volume": "16"
            },
            "citationStyles": {
                "bibtex": "@Article{Maedche2002OntologyLF,\n author = {A. Maedche and Steffen Staab},\n booktitle = {IEEE Intelligent Systems},\n journal = {IEEE Intell. Syst.},\n pages = {72-79},\n title = {Ontology Learning for the Semantic Web},\n volume = {16},\n year = {2002}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e4de0f69cd867dbcae88211ac05318be17615a66",
            "@type": "ScholarlyArticle",
            "paperId": "e4de0f69cd867dbcae88211ac05318be17615a66",
            "corpusId": 17128241,
            "url": "https://www.semanticscholar.org/paper/e4de0f69cd867dbcae88211ac05318be17615a66",
            "title": "Regularized Extreme Learning Machine",
            "venue": "IEEE Symposium on Computational Intelligence and Data Mining",
            "publicationVenue": {
                "id": "urn:research:9ed28ae9-1240-4565-a2e3-3c7ee6cf679f",
                "name": "IEEE Symposium on Computational Intelligence and Data Mining",
                "alternate_names": [
                    "Comput Intell Data Min",
                    "Computational Intelligence and Data Mining",
                    "CIDM",
                    "IEEE Symp Comput Intell Data Min"
                ],
                "issn": null,
                "url": "http://www.ieee-ssci.org/"
            },
            "year": 2009,
            "externalIds": {
                "DBLP": "conf/cidm/DengZC09",
                "MAG": "2154852616",
                "DOI": "10.1109/CIDM.2009.4938676",
                "CorpusId": 17128241
            },
            "abstract": "Extreme Learning Machine proposed by Huang G-B has attracted many attentions for its extremely fast training speed and good generalization performance. But it still can be considered as empirical risk minimization theme and tends to generate over-fitting model. Additionally, since ELM doesn't considering heteroskedasticity in real applications, its performance will be affected seriously when outliers exist in the dataset. In order to address these drawbacks, we propose a novel algorithm called Regularized Extreme Learning Machine based on structural risk minimization principle and weighted least square. The generalization performance of the proposed algorithm was improved significantly in most cases without increasing training time.",
            "referenceCount": 15,
            "citationCount": 402,
            "influentialCitationCount": 44,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2009-05-15",
            "journal": {
                "name": "2009 IEEE Symposium on Computational Intelligence and Data Mining",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Deng2009RegularizedEL,\n author = {W. Deng and Qinghua Zheng and Lin Chen},\n booktitle = {IEEE Symposium on Computational Intelligence and Data Mining},\n journal = {2009 IEEE Symposium on Computational Intelligence and Data Mining},\n pages = {389-395},\n title = {Regularized Extreme Learning Machine},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b3852f0113fcf8a3913c55ae92393ae6ccde347e",
            "@type": "ScholarlyArticle",
            "paperId": "b3852f0113fcf8a3913c55ae92393ae6ccde347e",
            "corpusId": 6692382,
            "url": "https://www.semanticscholar.org/paper/b3852f0113fcf8a3913c55ae92393ae6ccde347e",
            "title": "Self-taught learning: transfer learning from unlabeled data",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2007,
            "externalIds": {
                "DBLP": "conf/icml/RainaBLPN07",
                "MAG": "2122922389",
                "DOI": "10.1145/1273496.1273592",
                "CorpusId": 6692382
            },
            "abstract": "We present a new machine learning framework called \"self-taught learning\" for using unlabeled data in supervised classification tasks. We do not assume that the unlabeled data follows the same class labels or generative distribution as the labeled data. Thus, we would like to use a large number of unlabeled images (or audio samples, or text documents) randomly downloaded from the Internet to improve performance on a given image (or audio, or text) classification task. Such unlabeled data is significantly easier to obtain than in typical semi-supervised or transfer learning settings, making self-taught learning widely applicable to many practical learning problems. We describe an approach to self-taught learning that uses sparse coding to construct higher-level features using the unlabeled data. These features form a succinct input representation and significantly improve classification performance. When using an SVM for classification, we further show how a Fisher kernel can be learned for this representation.",
            "referenceCount": 26,
            "citationCount": 1720,
            "influentialCitationCount": 119,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2007-06-20",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Raina2007SelftaughtLT,\n author = {Rajat Raina and Alexis Battle and Honglak Lee and Ben Packer and A. Ng},\n booktitle = {International Conference on Machine Learning},\n pages = {759-766},\n title = {Self-taught learning: transfer learning from unlabeled data},\n year = {2007}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:02e68b069d9cf13c082049429ffed18a5ca5f6d0",
            "@type": "ScholarlyArticle",
            "paperId": "02e68b069d9cf13c082049429ffed18a5ca5f6d0",
            "corpusId": 14177182,
            "url": "https://www.semanticscholar.org/paper/02e68b069d9cf13c082049429ffed18a5ca5f6d0",
            "title": "Support Vector Machines for Multiple-Instance Learning",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2002,
            "externalIds": {
                "DBLP": "conf/nips/AndrewsTH02",
                "MAG": "2108745803",
                "CorpusId": 14177182
            },
            "abstract": "This paper presents two new formulations of multiple-instance learning as a maximum margin problem. The proposed extensions of the Support Vector Machine (SVM) learning approach lead to mixed integer quadratic programs that can be solved heuristic ally. Our generalization of SVMs makes a state-of-the-art classification technique, including non-linear classification via kernels, available to an area that up to now has been largely dominated by special purpose methods. We present experimental results on a pharmaceutical data set and on applications in automated image indexing and document categorization.",
            "referenceCount": 14,
            "citationCount": 1554,
            "influentialCitationCount": 350,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Andrews2002SupportVM,\n author = {Stuart Andrews and Ioannis Tsochantaridis and Thomas Hofmann},\n booktitle = {Neural Information Processing Systems},\n pages = {561-568},\n title = {Support Vector Machines for Multiple-Instance Learning},\n year = {2002}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:85d727b119304dde458bcd8cf5cb87a906fb41ba",
            "@type": "ScholarlyArticle",
            "paperId": "85d727b119304dde458bcd8cf5cb87a906fb41ba",
            "corpusId": 2884606,
            "url": "https://www.semanticscholar.org/paper/85d727b119304dde458bcd8cf5cb87a906fb41ba",
            "title": "Using AUC and accuracy in evaluating learning algorithms",
            "venue": "IEEE Transactions on Knowledge and Data Engineering",
            "publicationVenue": {
                "id": "urn:research:c6840156-ee10-4d78-8832-7f8909811576",
                "name": "IEEE Transactions on Knowledge and Data Engineering",
                "alternate_names": [
                    "IEEE Trans Knowl Data Eng"
                ],
                "issn": "1041-4347",
                "url": "https://www.computer.org/web/tkde"
            },
            "year": 2005,
            "externalIds": {
                "DBLP": "journals/tkde/HuangL05",
                "MAG": "2096451472",
                "DOI": "10.1109/TKDE.2005.50",
                "CorpusId": 2884606
            },
            "abstract": "The area under the ROC (receiver operating characteristics) curve, or simply AUC, has been traditionally used in medical diagnosis since the 1970s. It has recently been proposed as an alternative single-number measure for evaluating the predictive ability of learning algorithms. However, no formal arguments were given as to why AUC should be preferred over accuracy. We establish formal criteria for comparing two different measures for learning algorithms and we show theoretically and empirically that AUC is a better measure (defined precisely) than accuracy. We then reevaluate well-established claims in machine learning based on accuracy using AUC and obtain interesting and surprising new results. For example, it has been well-established and accepted that Naive Bayes and decision trees are very similar in predictive accuracy. We show, however, that Naive Bayes is significantly better than decision trees in AUC. The conclusions drawn in this paper may make a significant impact on machine learning and data mining applications.",
            "referenceCount": 44,
            "citationCount": 1632,
            "influentialCitationCount": 148,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.cs.ust.hk/~qyang/537/Papers/AUC-evaluation.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2005-03-01",
            "journal": {
                "name": "IEEE Transactions on Knowledge and Data Engineering",
                "volume": "17"
            },
            "citationStyles": {
                "bibtex": "@Article{Huang2005UsingAA,\n author = {Jin Huang and C. Ling},\n booktitle = {IEEE Transactions on Knowledge and Data Engineering},\n journal = {IEEE Transactions on Knowledge and Data Engineering},\n pages = {299-310},\n title = {Using AUC and accuracy in evaluating learning algorithms},\n volume = {17},\n year = {2005}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8db95dbd08e4ee64fb258e5380e78cfa507ed94d",
            "@type": "ScholarlyArticle",
            "paperId": "8db95dbd08e4ee64fb258e5380e78cfa507ed94d",
            "corpusId": 2071866,
            "url": "https://www.semanticscholar.org/paper/8db95dbd08e4ee64fb258e5380e78cfa507ed94d",
            "title": "Think Globally, Fit Locally: Unsupervised Learning of Low Dimensional Manifold",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2003,
            "externalIds": {
                "DBLP": "journals/jmlr/SaulR03",
                "MAG": "2063532964",
                "DOI": "10.1162/153244304322972667",
                "CorpusId": 2071866
            },
            "abstract": "The problem of dimensionality reduction arises in many fields of information processing, including machine learning, data compression, scientific visualization, pattern recognition, and neural computation. Here we describe locally linear embedding (LLE), an unsupervised learning algorithm that computes low dimensional, neighborhood preserving embeddings of high dimensional data. The data, assumed to be sampled from an underlying manifold, are mapped into a single global coordinate system of lower dimensionality. The mapping is derived from the symmetries of locally linear reconstructions, and the actual computation of the embedding reduces to a sparse eigenvalue problem. Notably, the optimizations in LLE---though capable of generating highly nonlinear embeddings---are simple to implement, and they do not involve local minima. In this paper, we describe the implementation of the algorithm in detail and discuss several extensions that enhance its performance. We present results of the algorithm applied to data sampled from known manifolds, as well as to collections of images of faces, lips, and handwritten digits. These examples are used to provide extensive illustrations of the algorithm's performance---both successes and failures---and to relate the algorithm to previous and ongoing work in nonlinear dimensionality reduction.",
            "referenceCount": 74,
            "citationCount": 1680,
            "influentialCitationCount": 161,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2003-12-01",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "4"
            },
            "citationStyles": {
                "bibtex": "@Article{Saul2003ThinkGF,\n author = {L. Saul and S. Roweis},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {119-155},\n title = {Think Globally, Fit Locally: Unsupervised Learning of Low Dimensional Manifold},\n volume = {4},\n year = {2003}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5ed59f49c1bb7de06cfa2a9467d5efb535103277",
            "@type": "ScholarlyArticle",
            "paperId": "5ed59f49c1bb7de06cfa2a9467d5efb535103277",
            "corpusId": 6023746,
            "url": "https://www.semanticscholar.org/paper/5ed59f49c1bb7de06cfa2a9467d5efb535103277",
            "title": "Temporal difference learning and TD-Gammon",
            "venue": "CACM",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1995,
            "externalIds": {
                "DBLP": "journals/cacm/Tesauro95",
                "MAG": "2787259794",
                "DOI": "10.1145/203330.203343",
                "CorpusId": 6023746
            },
            "abstract": "Ever since the days of Shannon's proposal for a chess-playing algorithm [12] and Samuel's checkers-learning program [10] the domain of complex board games such as Go, chess, checkers, Othello, and backgammon has been widely regarded as an ideal testing ground for exploring a variety of concepts and approaches in artificial intelligence and machine learning. Such board games offer the challenge of tremendous complexity and sophistication required to play at expert level. At the same time, the problem inputs and performance measures are clear-cut and well defined, and the game environment is readily automated in that it is easy to simulate the board, the rules of legal play, and the rules regarding when the game is over and determining the outcome.",
            "referenceCount": 10,
            "citationCount": 1965,
            "influentialCitationCount": 98,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1995-03-01",
            "journal": {
                "name": "Commun. ACM",
                "volume": "38"
            },
            "citationStyles": {
                "bibtex": "@Article{Tesauro1995TemporalDL,\n author = {G. Tesauro},\n booktitle = {CACM},\n journal = {Commun. ACM},\n pages = {58-68},\n title = {Temporal difference learning and TD-Gammon},\n volume = {38},\n year = {1995}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:66f44806cd46a27f02ceb74bdfd9ad6e77e044ca",
            "@type": "ScholarlyArticle",
            "paperId": "66f44806cd46a27f02ceb74bdfd9ad6e77e044ca",
            "corpusId": 18950272,
            "url": "https://www.semanticscholar.org/paper/66f44806cd46a27f02ceb74bdfd9ad6e77e044ca",
            "title": "Toward an Online Anomaly Intrusion Detection System Based on Deep Learning",
            "venue": "International Conference on Machine Learning and Applications",
            "publicationVenue": {
                "id": "urn:research:f6752838-f268-4a1b-87e7-c5f30a36713c",
                "name": "International Conference on Machine Learning and Applications",
                "alternate_names": [
                    "Int Conf Mach Learn Appl",
                    "ICMLA"
                ],
                "issn": null,
                "url": "http://www.icmla-conference.org/"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "conf/icmla/AlrawashdehP16",
                "MAG": "2584408238",
                "DOI": "10.1109/ICMLA.2016.0040",
                "CorpusId": 18950272
            },
            "abstract": "In the past twenty years, progress in intrusion detection has been steady but slow. The biggest challenge is to detect new attacks in real time. In this work, a deep learning approach for anomaly detection using a Restricted Boltzmann Machine (RBM) and a deep belief network are implemented. Our method uses a one-hidden layer RBM to perform unsupervised feature reduction. The resultant weights from this RBM are passed to another RBM producing a deep belief network. The pre-trained weights are passed into a fine tuning layer consisting of a Logistic Regression (LR) classifier with multi-class soft-max. We have implemented the deep learning architecture in C++ in Microsoft Visual Studio 2013 and we use the DARPA KDDCUP'99 dataset to evaluate its performance. Our architecture outperforms previous deep learning methods implemented by Li and Salama in both detection speed and accuracy. We achieve a detection rate of 97.9% on the total 10% KDDCUP'99 test dataset. By improving the training process of the simulation, we are also able to produce a low false negative rate of 2.47%. Although the deficiencies in the KDDCUP'99 dataset are well understood, it still presents machine learning approaches for predicting attacks with a reasonable challenge. Our future work will include applying our machine learning strategy to larger and more challenging datasets, which include larger classes of attacks.",
            "referenceCount": 21,
            "citationCount": 197,
            "influentialCitationCount": 11,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-12-01",
            "journal": {
                "name": "2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Alrawashdeh2016TowardAO,\n author = {Khaled Alrawashdeh and C. Purdy},\n booktitle = {International Conference on Machine Learning and Applications},\n journal = {2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA)},\n pages = {195-200},\n title = {Toward an Online Anomaly Intrusion Detection System Based on Deep Learning},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:141e6c1dd532504611266d08458dbe2a0dbb4e98",
            "@type": "ScholarlyArticle",
            "paperId": "141e6c1dd532504611266d08458dbe2a0dbb4e98",
            "corpusId": 623918,
            "url": "https://www.semanticscholar.org/paper/141e6c1dd532504611266d08458dbe2a0dbb4e98",
            "title": "Multiple kernel learning, conic duality, and the SMO algorithm",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2004,
            "externalIds": {
                "MAG": "2031823405",
                "DBLP": "conf/icml/BachLJ04",
                "DOI": "10.1145/1015330.1015424",
                "CorpusId": 623918
            },
            "abstract": "While classical kernel-based classifiers are based on a single kernel, in practice it is often desirable to base classifiers on combinations of multiple kernels. Lanckriet et al. (2004) considered conic combinations of kernel matrices for the support vector machine (SVM), and showed that the optimization of the coefficients of such a combination reduces to a convex optimization problem known as a quadratically-constrained quadratic program (QCQP). Unfortunately, current convex optimization toolboxes can solve this problem only for a small number of kernels and a small number of data points; moreover, the sequential minimal optimization (SMO) techniques that are essential in large-scale implementations of the SVM cannot be applied because the cost function is non-differentiable. We propose a novel dual formulation of the QCQP as a second-order cone programming problem, and show how to exploit the technique of Moreau-Yosida regularization to yield a formulation to which SMO techniques can be applied. We present experimental results that show that our SMO-based algorithm is significantly more efficient than the general-purpose interior point methods available in current optimization toolboxes.",
            "referenceCount": 12,
            "citationCount": 1644,
            "influentialCitationCount": 176,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.cs.berkeley.edu/~jordan/papers/skm_icml.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Book",
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2004-07-04",
            "journal": {
                "name": "Proceedings of the twenty-first international conference on Machine learning",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Book{Bach2004MultipleKL,\n author = {F. Bach and Gert R. G. Lanckriet and Michael I. Jordan},\n booktitle = {International Conference on Machine Learning},\n journal = {Proceedings of the twenty-first international conference on Machine learning},\n title = {Multiple kernel learning, conic duality, and the SMO algorithm},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:58740a751f399d634c0542ccecefba7c2dea15be",
            "@type": "ScholarlyArticle",
            "paperId": "58740a751f399d634c0542ccecefba7c2dea15be",
            "corpusId": 57089882,
            "url": "https://www.semanticscholar.org/paper/58740a751f399d634c0542ccecefba7c2dea15be",
            "title": "Machine Learning",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1986,
            "externalIds": {
                "MAG": "2798641625",
                "DOI": "10.1036/1097-8542.395250",
                "CorpusId": 57089882
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Forsyth1986MachineL,\n author = {R. Forsyth},\n title = {Machine Learning},\n year = {1986}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0e032db22bfd6a607eda2185715e8ab0772cbd67",
            "@type": "ScholarlyArticle",
            "paperId": "0e032db22bfd6a607eda2185715e8ab0772cbd67",
            "corpusId": 31015609,
            "url": "https://www.semanticscholar.org/paper/0e032db22bfd6a607eda2185715e8ab0772cbd67",
            "title": "Evaluating Learning Algorithms: A Classification Perspective",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2011,
            "externalIds": {
                "DBLP": "books/cu/Japkowicz2011",
                "MAG": "608314793",
                "CorpusId": 31015609
            },
            "abstract": "The field of machine learning has matured to the point where many sophisticated learning approaches can be applied to practical applications. Thus it is of critical importance that researchers have the proper tools to evaluate learning approaches and understand the underlying issues. This book examines various aspects of the evaluation process with an emphasis on classification algorithms. The authors describe several techniques for classifier performance assessment, error estimation and resampling, obtaining statistical significance as well as selecting appropriate domains for evaluation. They also present a unified evaluation framework and highlight how different components of evaluation are both significantly interrelated and interdependent. The techniques presented in the book are illustrated using R and WEKA facilitating better practical insight as well as implementation.Aimed at researchers in the theory and applications of machine learning, this book offers a solid basis for conducting performance evaluations of algorithms in practical settings.",
            "referenceCount": 167,
            "citationCount": 894,
            "influentialCitationCount": 71,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2011-01-17",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Japkowicz2011EvaluatingLA,\n author = {N. Japkowicz and Mohak Shah},\n title = {Evaluating Learning Algorithms: A Classification Perspective},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8d49d34fff05285cb9a148261caff57775eb4453",
            "@type": "ScholarlyArticle",
            "paperId": "8d49d34fff05285cb9a148261caff57775eb4453",
            "corpusId": 12577326,
            "url": "https://www.semanticscholar.org/paper/8d49d34fff05285cb9a148261caff57775eb4453",
            "title": "ELLA: An Efficient Lifelong Learning Algorithm",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2013,
            "externalIds": {
                "DBLP": "conf/icml/RuvoloE13",
                "MAG": "1519626139",
                "CorpusId": 12577326
            },
            "abstract": "The problem of learning multiple consecutive tasks, known as lifelong learning, is of great importance to the creation of intelligent, general-purpose, and flexible machines. In this paper, we develop a method for online multi-task learning in the lifelong learning setting. The proposed Efficient Lifelong Learning Algorithm (ELLA) maintains a sparsely shared basis for all task models, transfers knowledge from the basis to learn each new task, and refines the basis over time to maximize performance across all tasks. We show that ELLA has strong connections to both online dictionary learning for sparse coding and state-of-the-art batch multitask learning methods, and provide robust theoretical performance guarantees. We show empirically that ELLA yields nearly identical performance to batch multi-task learning while learning tasks sequentially in three orders of magnitude (over 1,000x) less time.",
            "referenceCount": 19,
            "citationCount": 349,
            "influentialCitationCount": 46,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2013-06-16",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Ruvolo2013ELLAAE,\n author = {P. Ruvolo and Eric Eaton},\n booktitle = {International Conference on Machine Learning},\n pages = {507-515},\n title = {ELLA: An Efficient Lifelong Learning Algorithm},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e541c475457a731d7d434c4302867fc45af5876f",
            "@type": "ScholarlyArticle",
            "paperId": "e541c475457a731d7d434c4302867fc45af5876f",
            "corpusId": 207319017,
            "url": "https://www.semanticscholar.org/paper/e541c475457a731d7d434c4302867fc45af5876f",
            "title": "Active Learning",
            "venue": "Synthesis Lectures on Artificial Intelligence and Machine Learning",
            "publicationVenue": {
                "id": "urn:research:84e95d47-8c6e-4f56-b8c8-2fc3088cfb6b",
                "name": "Synthesis Lectures on Artificial Intelligence and Machine Learning",
                "alternate_names": [
                    "Synth Lect Artif Intell Mach Learn"
                ],
                "issn": "1939-4608",
                "url": "https://www.morganclaypool.com/toc/aim/1/1"
            },
            "year": 2012,
            "externalIds": {
                "MAG": "2914331073",
                "DBLP": "series/synthesis/2012Settles",
                "DOI": "10.1007/978-1-4419-1428-6_489",
                "CorpusId": 207319017
            },
            "abstract": null,
            "referenceCount": 91,
            "citationCount": 731,
            "influentialCitationCount": 54,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "2012-07-02",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Settles2012ActiveL,\n author = {Burr Settles},\n booktitle = {Synthesis Lectures on Artificial Intelligence and Machine Learning},\n title = {Active Learning},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:944e1a7b2c5c62e952418d7684e3cade89c76f87",
            "@type": "ScholarlyArticle",
            "paperId": "944e1a7b2c5c62e952418d7684e3cade89c76f87",
            "corpusId": 13650160,
            "url": "https://www.semanticscholar.org/paper/944e1a7b2c5c62e952418d7684e3cade89c76f87",
            "title": "A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2005,
            "externalIds": {
                "DBLP": "journals/jmlr/AndoZ05",
                "MAG": "2130903752",
                "CorpusId": 13650160
            },
            "abstract": "One of the most important issues in machine learning is whether one can improve the performance of a supervised learning algorithm by including unlabeled data. Methods that use both labeled and unlabeled data are generally referred to as semi-supervised learning. Although a number of such methods are proposed, at the current stage, we still don't have a complete understanding of their effectiveness. This paper investigates a closely related problem, which leads to a novel approach to semi-supervised learning. Specifically we consider learning predictive structures on hypothesis spaces (that is, what kind of classifiers have good predictive power) from multiple learning tasks. We present a general framework in which the structural learning problem can be formulated and analyzed theoretically, and relate it to learning with unlabeled data. Under this framework, algorithms for structural learning will be proposed, and computational issues will be investigated. Experiments will be given to demonstrate the effectiveness of the proposed algorithms in the semi-supervised learning setting.",
            "referenceCount": 28,
            "citationCount": 1513,
            "influentialCitationCount": 118,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2005-12-01",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "6"
            },
            "citationStyles": {
                "bibtex": "@Article{Ando2005AFF,\n author = {R. Ando and Tong Zhang},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {1817-1853},\n title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},\n volume = {6},\n year = {2005}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:27609ed3fda10c0f35eaac014f42640ebff3df8d",
            "@type": "ScholarlyArticle",
            "paperId": "27609ed3fda10c0f35eaac014f42640ebff3df8d",
            "corpusId": 14911319,
            "url": "https://www.semanticscholar.org/paper/27609ed3fda10c0f35eaac014f42640ebff3df8d",
            "title": "Evolutionary extreme learning machine",
            "venue": "Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:266f640f-003e-453b-ab76-57e4053252f8",
                "name": "Pattern Recognition",
                "alternate_names": [
                    "Pattern Recognit"
                ],
                "issn": "0031-3203",
                "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/328/description#description"
            },
            "year": 2005,
            "externalIds": {
                "DBLP": "journals/pr/ZhuQSH05",
                "MAG": "2040604977",
                "DOI": "10.1016/j.patcog.2005.03.028",
                "CorpusId": 14911319
            },
            "abstract": null,
            "referenceCount": 5,
            "citationCount": 773,
            "influentialCitationCount": 37,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2005-10-01",
            "journal": {
                "name": "Pattern Recognit.",
                "volume": "38"
            },
            "citationStyles": {
                "bibtex": "@Article{Zhu2005EvolutionaryEL,\n author = {Q. Zhu and A. K. Qin and P. Suganthan and G. Huang},\n booktitle = {Pattern Recognition},\n journal = {Pattern Recognit.},\n pages = {1759-1763},\n title = {Evolutionary extreme learning machine},\n volume = {38},\n year = {2005}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a8582979cc4337bd6349d55ac6a7112a07add3a5",
            "@type": "ScholarlyArticle",
            "paperId": "a8582979cc4337bd6349d55ac6a7112a07add3a5",
            "corpusId": 20881694,
            "url": "https://www.semanticscholar.org/paper/a8582979cc4337bd6349d55ac6a7112a07add3a5",
            "title": "Single-machine scheduling with learning considerations",
            "venue": "European Journal of Operational Research",
            "publicationVenue": {
                "id": "urn:research:0acd87e7-c1b4-434a-955a-c26983a4b9f4",
                "name": "European Journal of Operational Research",
                "alternate_names": [
                    "Eur J Oper Res"
                ],
                "issn": "0377-2217",
                "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/505543/description#description"
            },
            "year": 1999,
            "externalIds": {
                "DBLP": "journals/eor/Biskup99",
                "MAG": "1983974888",
                "DOI": "10.1016/S0377-2217(98)00246-X",
                "CorpusId": 20881694
            },
            "abstract": null,
            "referenceCount": 22,
            "citationCount": 727,
            "influentialCitationCount": 163,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1999-05-16",
            "journal": {
                "name": "Eur. J. Oper. Res.",
                "volume": "115"
            },
            "citationStyles": {
                "bibtex": "@Article{Biskup1999SinglemachineSW,\n author = {D. Biskup},\n booktitle = {European Journal of Operational Research},\n journal = {Eur. J. Oper. Res.},\n pages = {173-178},\n title = {Single-machine scheduling with learning considerations},\n volume = {115},\n year = {1999}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d58e4ea0d39c8b6717436dac496b152d8da3419e",
            "@type": "ScholarlyArticle",
            "paperId": "d58e4ea0d39c8b6717436dac496b152d8da3419e",
            "corpusId": 23624728,
            "url": "https://www.semanticscholar.org/paper/d58e4ea0d39c8b6717436dac496b152d8da3419e",
            "title": "LIBOL: a library for online learning algorithms",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2014,
            "externalIds": {
                "DBLP": "journals/jmlr/HoiWZ14",
                "MAG": "2119393863",
                "DOI": "10.5555/2627435.2627450",
                "CorpusId": 23624728
            },
            "abstract": "LIBOL is an open-source library for large-scale online learning, which consists of a large family of efficient and scalable state-of-the-art online learning algorithms for large-scale online classification tasks. We have offered easy-to-use command-line tools and examples for users and developers, and also have made comprehensive documents available for both beginners and advanced users. LIBOL is not only a machine learning toolbox, but also a comprehensive experimental platform for conducting online learning research.",
            "referenceCount": 20,
            "citationCount": 224,
            "influentialCitationCount": 14,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "15"
            },
            "citationStyles": {
                "bibtex": "@Article{Hoi2014LIBOLAL,\n author = {S. Hoi and Jialei Wang and P. Zhao},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {495-499},\n title = {LIBOL: a library for online learning algorithms},\n volume = {15},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2c8ac3e1f0edeed1fbd76813e61efdc384c319c7",
            "@type": "ScholarlyArticle",
            "paperId": "2c8ac3e1f0edeed1fbd76813e61efdc384c319c7",
            "corpusId": 11039301,
            "url": "https://www.semanticscholar.org/paper/2c8ac3e1f0edeed1fbd76813e61efdc384c319c7",
            "title": "Learning Question Classifiers",
            "venue": "International Conference on Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:f51ff783-cdff-4e22-94fb-28e6336d17b3",
                "name": "International Conference on Computational Linguistics",
                "alternate_names": [
                    "Int Conf Comput Linguistics",
                    "COLING"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/anthology/venues/coling/"
            },
            "year": 2002,
            "externalIds": {
                "ACL": "C02-1150",
                "MAG": "2070246124",
                "DBLP": "conf/coling/LiR02",
                "DOI": "10.3115/1072228.1072378",
                "CorpusId": 11039301
            },
            "abstract": "In order to respond correctly to a free form factual question given a large collection of texts, one needs to understand the question to a level that allows determining some of the constraints the question imposes on a possible answer. These constraints may include a semantic classification of the sought after answer and may even suggest using different strategies when looking for and verifying a candidate answer.This paper presents a machine learning approach to question classification. We learn a hierarchical classifier that is guided by a layered semantic hierarchy of answer types, and eventually classifies questions into fine-grained classes. We show accurate results on a large collection of free-form questions used in TREC 10.",
            "referenceCount": 20,
            "citationCount": 1384,
            "influentialCitationCount": 224,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://acl.ldc.upenn.edu/C/C02/C02-1150.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2002-08-24",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Article{Li2002LearningQC,\n author = {Xin Li and D. Roth},\n booktitle = {International Conference on Computational Linguistics},\n pages = {1-7},\n title = {Learning Question Classifiers},\n year = {2002}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7b2dd79083a74699e4e0509ac3f0a8a302b4eabe",
            "@type": "ScholarlyArticle",
            "paperId": "7b2dd79083a74699e4e0509ac3f0a8a302b4eabe",
            "corpusId": 8188805,
            "url": "https://www.semanticscholar.org/paper/7b2dd79083a74699e4e0509ac3f0a8a302b4eabe",
            "title": "On the mathematical foundations of learning",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2001,
            "externalIds": {
                "MAG": "2047278710",
                "DOI": "10.1090/S0273-0979-01-00923-5",
                "CorpusId": 8188805
            },
            "abstract": "(1) A main theme of this report is the relationship of approximation to learning and the primary role of sampling (inductive inference). We try to emphasize relations of the theory of learning to the mainstream of mathematics. In particular, there are large roles for probability theory, for algorithms such as least squares, and for tools and ideas from linear algebra and linear analysis. An advantage of doing this is that communication is facilitated and the power of core mathematics is more easily brought to bear. We illustrate what we mean by learning theory by giving some instances. (a) The understanding of language acquisition by children or the emergence of languages in early human cultures. (b) In Manufacturing Engineering, the design of a new wave of machines is anticipated which uses sensors to sample properties of objects before, during, and after treatment. The information gathered from these samples is to be analyzed by the machine to decide how to better deal with new input objects (see [43]). (c) Pattern recognition of objects ranging from handwritten letters of the alphabet to pictures of animals, to the human voice. Understanding the laws of learning plays a large role in disciplines such as (Cognitive) Psychology, Animal Behavior, Economic Decision Making, all branches of Engineering, Computer Science, and especially the study of human thought processes (how the brain works). Mathematics has already played a big role towards the goal of giving a universal foundation of studies in these disciplines. We mention as examples the theory of Neural Networks going back to McCulloch and Pitts [25] and Minsky and Papert [27], the PAC learning of Valiant [40], Statistical Learning Theory as developed by Vapnik [42], and the use of reproducing kernels as in [17] among many other mathematical developments. We are heavily indebted to these developments. Recent discussions with a number of mathematicians have also been helpful. In",
            "referenceCount": 50,
            "citationCount": 1597,
            "influentialCitationCount": 182,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.ams.org/bull/2002-39-01/S0273-0979-01-00923-5/S0273-0979-01-00923-5.pdf",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Education",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2001-10-05",
            "journal": {
                "name": "Bulletin of the American Mathematical Society",
                "volume": "39"
            },
            "citationStyles": {
                "bibtex": "@Article{Cucker2001OnTM,\n author = {F. Cucker and S. Smale},\n journal = {Bulletin of the American Mathematical Society},\n pages = {1-49},\n title = {On the mathematical foundations of learning},\n volume = {39},\n year = {2001}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ede30b1b265e62b12410bbf796a23437a64619a6",
            "@type": "ScholarlyArticle",
            "paperId": "ede30b1b265e62b12410bbf796a23437a64619a6",
            "corpusId": 8773092,
            "url": "https://www.semanticscholar.org/paper/ede30b1b265e62b12410bbf796a23437a64619a6",
            "title": "Task-Driven Dictionary Learning",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "publicationVenue": {
                "id": "urn:research:25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "alternate_names": [
                    "IEEE Trans Pattern Anal Mach Intell"
                ],
                "issn": "0162-8828",
                "url": "http://www.computer.org/tpami/"
            },
            "year": 2010,
            "externalIds": {
                "DBLP": "journals/pami/MairalBP12",
                "ArXiv": "1009.5358",
                "MAG": "2163112044",
                "DOI": "10.1109/TPAMI.2011.156",
                "CorpusId": 8773092,
                "PubMed": "21808090"
            },
            "abstract": "Modeling data with linear combinations of a few elements from a learned dictionary has been the focus of much recent research in machine learning, neuroscience, and signal processing. For signals such as natural images that admit such sparse representations, it is now well established that these models are well suited to restoration tasks. In this context, learning the dictionary amounts to solving a large-scale matrix factorization problem, which can be done efficiently with classical optimization tools. The same approach has also been used for learning features from data for other purposes, e.g., image classification, but tuning the dictionary in a supervised way for these tasks has proven to be more difficult. In this paper, we present a general formulation for supervised dictionary learning adapted to a wide variety of tasks, and present an efficient algorithm for solving the corresponding optimization problem. Experiments on handwritten digit classification, digital art identification, nonlinear inverse image problems, and compressed sensing demonstrate that our approach is effective in large-scale settings, and is well suited to supervised and semi-supervised classification, as well as regression tasks for data that admit sparse representations.",
            "referenceCount": 63,
            "citationCount": 882,
            "influentialCitationCount": 96,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1009.5358",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2010-09-27",
            "journal": {
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "volume": "34"
            },
            "citationStyles": {
                "bibtex": "@Article{Mairal2010TaskDrivenDL,\n author = {J. Mairal and F. Bach and J. Ponce},\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n pages = {791-804},\n title = {Task-Driven Dictionary Learning},\n volume = {34},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d26a48aff2abc3460c1018d5b410766f698d696c",
            "@type": "ScholarlyArticle",
            "paperId": "d26a48aff2abc3460c1018d5b410766f698d696c",
            "corpusId": 1513614,
            "url": "https://www.semanticscholar.org/paper/d26a48aff2abc3460c1018d5b410766f698d696c",
            "title": "Large Scale Multiple Kernel Learning",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2006,
            "externalIds": {
                "MAG": "2154462399",
                "DBLP": "journals/jmlr/SonnenburgRSS06",
                "CorpusId": 1513614
            },
            "abstract": "While classical kernel-based learning algorithms are based on a single kernel, in practice it is often desirable to use multiple kernels. Lanckriet et al. (2004) considered conic combinations of kernel matrices for classification, leading to a convex quadratically constrained quadratic program. We show that it can be rewritten as a semi-infinite linear program that can be efficiently solved by recycling the standard SVM implementations. Moreover, we generalize the formulation and our method to a larger class of problems, including regression and one-class classification. Experimental results show that the proposed algorithm works for hundred thousands of examples or hundreds of kernels to be combined, and helps for automatic model selection, improving the interpretability of the learning result. In a second part we discuss general speed up mechanism for SVMs, especially when used with sparse feature maps as appear for string kernels, allowing us to train a string kernel SVM on a 10 million real-world splice data set from computational biology. We integrated multiple kernel learning in our machine learning toolbox SHOGUN for which the source code is publicly available at http://www.fml.tuebingen.mpg.de/raetsch/projects/shogun .",
            "referenceCount": 29,
            "citationCount": 1403,
            "influentialCitationCount": 141,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2006-12-01",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "7"
            },
            "citationStyles": {
                "bibtex": "@Article{Sonnenburg2006LargeSM,\n author = {S. Sonnenburg and Gunnar R\u00e4tsch and C. Sch\u00e4fer and B. Scholkopf},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {1531-1565},\n title = {Large Scale Multiple Kernel Learning},\n volume = {7},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0ca26f9a98dda0abb737692f72ffa682df14cb2f",
            "@type": "ScholarlyArticle",
            "paperId": "0ca26f9a98dda0abb737692f72ffa682df14cb2f",
            "corpusId": 18958836,
            "url": "https://www.semanticscholar.org/paper/0ca26f9a98dda0abb737692f72ffa682df14cb2f",
            "title": "Sparse Bayesian learning for basis selection",
            "venue": "IEEE Transactions on Signal Processing",
            "publicationVenue": {
                "id": "urn:research:1f6f3f05-6a23-42f0-8d31-98ab8089c1f2",
                "name": "IEEE Transactions on Signal Processing",
                "alternate_names": [
                    "IEEE Trans Signal Process"
                ],
                "issn": "1053-587X",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=78"
            },
            "year": 2004,
            "externalIds": {
                "DBLP": "journals/tsp/WipfR04",
                "MAG": "2148154358",
                "DOI": "10.1109/TSP.2004.831016",
                "CorpusId": 18958836
            },
            "abstract": "Sparse Bayesian learning (SBL) and specifically relevance vector machines have received much attention in the machine learning literature as a means of achieving parsimonious representations in the context of regression and classification. The methodology relies on a parameterized prior that encourages models with few nonzero weights. In this paper, we adapt SBL to the signal processing problem of basis selection from overcomplete dictionaries, proving several results about the SBL cost function that elucidate its general behavior and provide solid theoretical justification for this application. Specifically, we have shown that SBL retains a desirable property of the /spl lscr//sub 0/-norm diversity measure (i.e., the global minimum is achieved at the maximally sparse solution) while often possessing a more limited constellation of local minima. We have also demonstrated that the local minima that do exist are achieved at sparse solutions. Later, we provide a novel interpretation of SBL that gives us valuable insight into why it is successful in producing sparse representations. Finally, we include simulation studies comparing sparse Bayesian learning with basis pursuit and the more recent FOCal Underdetermined System Solver (FOCUSS) class of basis selection algorithms. These results indicate that our theoretical insights translate directly into improved performance.",
            "referenceCount": 37,
            "citationCount": 1272,
            "influentialCitationCount": 169,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2004-08-01",
            "journal": {
                "name": "IEEE Transactions on Signal Processing",
                "volume": "52"
            },
            "citationStyles": {
                "bibtex": "@Article{Wipf2004SparseBL,\n author = {D. Wipf and B. Rao},\n booktitle = {IEEE Transactions on Signal Processing},\n journal = {IEEE Transactions on Signal Processing},\n pages = {2153-2164},\n title = {Sparse Bayesian learning for basis selection},\n volume = {52},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:177944150565195ee9c3e28dc6b032200cfda059",
            "@type": "ScholarlyArticle",
            "paperId": "177944150565195ee9c3e28dc6b032200cfda059",
            "corpusId": 60541136,
            "url": "https://www.semanticscholar.org/paper/177944150565195ee9c3e28dc6b032200cfda059",
            "title": "Collaborative Learning: Cognitive and Computational Approaches",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1999,
            "externalIds": {
                "MAG": "1524402640",
                "CorpusId": 60541136
            },
            "abstract": "Acknowledgement. Contributors. Introduction: what do you mean by 'collaborative learning'? (P. Dillenbourg). Learning together: understanding the processes of computer-based collaborative learning (K. Littleton, P. Hakkinen). The role of grounding in collaborative learning tasks (M. Baker et al.). What is \"multi\" in multi-agent learning? (G. Weiss, P. Dillenbourg). Comparing human-human and robot-robot interactions (R. Joiner et al.). Learning by explaining to oneself and to others (R. Ploetzner et al.). Knowledge transformations in agents and interactions: a comparison of machine learning and dialogue operators (E. Mephu Nguifo et al.). Can analytic models support learning in groups? (H.U. Hoppe, R. Ploetzner). Using telematics for collaborative knowledge construction (T. Hansen et al.). The productive agency that drives collaborative learning (D. Schwatrtz). References. Index.",
            "referenceCount": 0,
            "citationCount": 1549,
            "influentialCitationCount": 80,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Hansen1999CollaborativeLC,\n author = {Tia G. B. Hansen and L. Dirckinck-Holmfeld and R. Lewis and J. Rugelj},\n title = {Collaborative Learning: Cognitive and Computational Approaches},\n year = {1999}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6fdb77260fc83dff91c44fea0f31a2cb8ed13d04",
            "@type": "ScholarlyArticle",
            "paperId": "6fdb77260fc83dff91c44fea0f31a2cb8ed13d04",
            "corpusId": 15559637,
            "url": "https://www.semanticscholar.org/paper/6fdb77260fc83dff91c44fea0f31a2cb8ed13d04",
            "title": "Scaling learning algorithms towards AI",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2007,
            "externalIds": {
                "MAG": "2613634265",
                "CorpusId": 15559637
            },
            "abstract": "One long-term goal of machine learning research is to produce methods that are applicable to highly complex tasks, such as perception (vision, audition), reasoning, intelligent control, and other artificially intelligent behaviors. We argue that in order to progress toward this goal, the Machine Learning community must endeavor to discover algorithms that can learn highly complex functions, with minimal need for prior knowledge, and with minimal human intervention. We present mathematical and empirical evidence suggesting that many popular approaches to non-parametric learning, particularly kernel methods, are fundamentally limited in their ability to learn complex high-dimensional functions. Our analysis focuses on two problems. First, kernel machines are shallow architectures, in which one large layer of simple template matchers is followed by a single layer of trainable coefficients. We argue that shallow architectures can be very inefficient in terms of required number of computational elements and examples. Second, we analyze a limitation of kernel machines with a local kernel, linked to the curse of dimensionality, that applies to supervised, unsupervised (manifold learning) and semi-supervised kernel machines. Using empirical results on invariant image recognition tasks, kernel methods are compared with deep architectures, in which lower-level features or concepts are progressively combined into more abstract and higher-level representations. We argue that deep architectures have the potential to generalize in non-local ways, i.e., beyond immediate neighbors, and that this is crucial in order to make progress on the kind of complex tasks required for artificial intelligence.",
            "referenceCount": 53,
            "citationCount": 1243,
            "influentialCitationCount": 51,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Bengio2007ScalingLA,\n author = {Yoshua Bengio and Yann LeCun},\n title = {Scaling learning algorithms towards AI},\n year = {2007}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:fae8bbf868681b83d91b2fec6c840d4d2b32005b",
            "@type": "ScholarlyArticle",
            "paperId": "fae8bbf868681b83d91b2fec6c840d4d2b32005b",
            "corpusId": 2326055,
            "url": "https://www.semanticscholar.org/paper/fae8bbf868681b83d91b2fec6c840d4d2b32005b",
            "title": "Intrinsic Motivation and Reinforcement Learning",
            "venue": "Intrinsically Motivated Learning in Natural and Artificial Systems",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2013,
            "externalIds": {
                "MAG": "2188721763",
                "DBLP": "books/sp/13/Barto13",
                "DOI": "10.1007/978-3-642-32375-1_2",
                "CorpusId": 2326055
            },
            "abstract": null,
            "referenceCount": 118,
            "citationCount": 207,
            "influentialCitationCount": 16,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Barto2013IntrinsicMA,\n author = {A. Barto},\n booktitle = {Intrinsically Motivated Learning in Natural and Artificial Systems},\n pages = {17-47},\n title = {Intrinsic Motivation and Reinforcement Learning},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a13efb90f0b56417bf5dd5b6219681c4259ff355",
            "@type": "ScholarlyArticle",
            "paperId": "a13efb90f0b56417bf5dd5b6219681c4259ff355",
            "corpusId": 41155462,
            "url": "https://www.semanticscholar.org/paper/a13efb90f0b56417bf5dd5b6219681c4259ff355",
            "title": "The Cerebellum: A Neuronal Learning Machine?",
            "venue": "Science",
            "publicationVenue": {
                "id": "urn:research:f59506a8-d8bb-4101-b3d4-c4ac3ed03dad",
                "name": "Science",
                "alternate_names": null,
                "issn": "0193-4511",
                "url": "https://www.jstor.org/journal/science"
            },
            "year": 1996,
            "externalIds": {
                "MAG": "2072802414",
                "DOI": "10.1126/science.272.5265.1126",
                "CorpusId": 41155462,
                "PubMed": "8638157"
            },
            "abstract": "Comparison of two seemingly quite different behaviors yields a surprisingly consistent picture of the role of the cerebellum in motor learning. Behavioral and physiological data about classical conditioning of the eyelid response and motor learning in the vestibulo-ocular reflex suggest that (i) plasticity is distributed between the cerebellar cortex and the deep cerebellar nuclei; (ii) the cerebellar cortex plays a special role in learning the timing of movement; and (iii) the cerebellar cortex guides learning in the deep nuclei, which may allow learning to be transferred from the cortex to the deep nuclei. Because many of the similarities in the data from the two systems typify general features of cerebellar organization, the cerebellar mechanisms of learning in these two systems may represent principles that apply to many motor systems.",
            "referenceCount": 67,
            "citationCount": 618,
            "influentialCitationCount": 25,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Medicine",
                "Biology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "1996-05-24",
            "journal": {
                "name": "Science",
                "volume": "272"
            },
            "citationStyles": {
                "bibtex": "@Article{Raymond1996TheCA,\n author = {J. Raymond and S. Lisberger and M. Mauk},\n booktitle = {Science},\n journal = {Science},\n pages = {1126 - 1131},\n title = {The Cerebellum: A Neuronal Learning Machine?},\n volume = {272},\n year = {1996}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:73e1c4a1152a75ec7310adfb4b8daea16d627bc7",
            "@type": "ScholarlyArticle",
            "paperId": "73e1c4a1152a75ec7310adfb4b8daea16d627bc7",
            "corpusId": 15764546,
            "url": "https://www.semanticscholar.org/paper/73e1c4a1152a75ec7310adfb4b8daea16d627bc7",
            "title": "Learning to learn with the informative vector machine",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2004,
            "externalIds": {
                "MAG": "2030290736",
                "DBLP": "conf/icml/LawrenceP04",
                "DOI": "10.1145/1015330.1015382",
                "CorpusId": 15764546
            },
            "abstract": "This paper describes an efficient method for learning the parameters of a Gaussian process (GP). The parameters are learned from multiple tasks which are assumed to have been drawn independently from the same GP prior. An efficient algorithm is obtained by extending the informative vector machine (IVM) algorithm to handle the multi-task learning case. The multi-task IVM (MTIVM) saves computation by greedily selecting the most informative examples from the separate tasks. The MT-IVM is also shown to be more efficient than random sub-sampling on an artificial data-set and more effective than the traditional IVM in a speaker dependent phoneme recognition task.",
            "referenceCount": 11,
            "citationCount": 373,
            "influentialCitationCount": 12,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Book",
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2004-07-04",
            "journal": {
                "name": "Proceedings of the twenty-first international conference on Machine learning",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Book{Lawrence2004LearningTL,\n author = {Neil D. Lawrence and John C. Platt},\n booktitle = {International Conference on Machine Learning},\n journal = {Proceedings of the twenty-first international conference on Machine learning},\n title = {Learning to learn with the informative vector machine},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:605402e235bd62437baf3c9ebefe77fb4d92ee95",
            "@type": "ScholarlyArticle",
            "paperId": "605402e235bd62437baf3c9ebefe77fb4d92ee95",
            "corpusId": 1890561,
            "url": "https://www.semanticscholar.org/paper/605402e235bd62437baf3c9ebefe77fb4d92ee95",
            "title": "The Helmholtz Machine",
            "venue": "Neural Computation",
            "publicationVenue": {
                "id": "urn:research:69b9bcdd-8229-4a00-a6e0-00f0e99a2bf3",
                "name": "Neural Computation",
                "alternate_names": [
                    "Neural Comput"
                ],
                "issn": "0899-7667",
                "url": "http://cognet.mit.edu/library/journals/journal?issn=08997667"
            },
            "year": 1995,
            "externalIds": {
                "MAG": "2026799324",
                "DBLP": "journals/neco/DayanHNZ95",
                "DOI": "10.1162/neco.1995.7.5.889",
                "CorpusId": 1890561,
                "PubMed": "7584891"
            },
            "abstract": "Discovering the structure inherent in a set of patterns is a fundamental aim of statistical inference or learning. One fruitful approach is to build a parameterized stochastic generative model, independent draws from which are likely to produce the patterns. For all but the simplest generative models, each pattern can be generated in exponentially many ways. It is thus intractable to adjust the parameters to maximize the probability of the observed patterns. We describe a way of finessing this combinatorial explosion by maximizing an easily computed lower bound on the probability of the observations. Our method can be viewed as a form of hierarchical self-supervised learning that may relate to the function of bottom-up and top-down cortical processing pathways.",
            "referenceCount": 38,
            "citationCount": 1269,
            "influentialCitationCount": 68,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1995-09-01",
            "journal": {
                "name": "Neural Computation",
                "volume": "7"
            },
            "citationStyles": {
                "bibtex": "@Article{Dayan1995TheHM,\n author = {P. Dayan and Geoffrey E. Hinton and Radford M. Neal and R. Zemel},\n booktitle = {Neural Computation},\n journal = {Neural Computation},\n pages = {889-904},\n title = {The Helmholtz Machine},\n volume = {7},\n year = {1995}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1695dbabf8e905db0b391ff522c323db5fc8b958",
            "@type": "ScholarlyArticle",
            "paperId": "1695dbabf8e905db0b391ff522c323db5fc8b958",
            "corpusId": 10852076,
            "url": "https://www.semanticscholar.org/paper/1695dbabf8e905db0b391ff522c323db5fc8b958",
            "title": "Learning to select and generalize striking movements in robot table tennis",
            "venue": "AAAI Fall Symposium: Robots Learning Interactively from Human Teachers",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2012,
            "externalIds": {
                "DBLP": "journals/ijrr/MullingKKP13",
                "MAG": "2399574992",
                "DOI": "10.1177/0278364912472380",
                "CorpusId": 10852076
            },
            "abstract": "Learning new motor tasks from physical interactions is an important goal for both robotics and machine learning. However, when moving beyond basic skills, most monolithic machine learning approaches fail to scale. For more complex skills, methods that are tailored for the domain of skill learning are needed. In this paper, we take the task of learning table tennis as an example and present a new framework that allows a robot to learn cooperative table tennis from physical interaction with a human. The robot first learns a set of elementary table tennis hitting movements from a human table tennis teacher by kinesthetic teach-in, which is compiled into a set of motor primitives represented by dynamical systems. The robot subsequently generalizes these movements to a wider range of situations using our mixture of motor primitives approach. The resulting policy enables the robot to select appropriate motor primitives as well as to generalize between them. Finally, the robot plays with a human table tennis partner and learns online to improve its behavior. We show that the resulting setup is capable of playing table tennis using an anthropomorphic robot arm.",
            "referenceCount": 60,
            "citationCount": 427,
            "influentialCitationCount": 20,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2012-10-19",
            "journal": {
                "name": "The International Journal of Robotics Research",
                "volume": "32"
            },
            "citationStyles": {
                "bibtex": "@Article{Muelling2012LearningTS,\n author = {Katharina Muelling and J. Kober and Oliver Kroemer and Jan Peters},\n booktitle = {AAAI Fall Symposium: Robots Learning Interactively from Human Teachers},\n journal = {The International Journal of Robotics Research},\n pages = {263 - 279},\n title = {Learning to select and generalize striking movements in robot table tennis},\n volume = {32},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:efbeedfbf13db70878618553f0c4a0fec6f493fe",
            "@type": "ScholarlyArticle",
            "paperId": "efbeedfbf13db70878618553f0c4a0fec6f493fe",
            "corpusId": 3036907,
            "url": "https://www.semanticscholar.org/paper/efbeedfbf13db70878618553f0c4a0fec6f493fe",
            "title": "Learning Collaborative Information Filters",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 1998,
            "externalIds": {
                "MAG": "1673941785",
                "DBLP": "conf/icml/BillsusP98",
                "CorpusId": 3036907
            },
            "abstract": "Predicting items a user would like on the basis of other users\u2019 ratings for these items has become a well-established strategy adopted by many recommendation services on the Internet. Although this can be seen as a classification problem, algorithms proposed thus far do not draw on results from the machine learning literature. We propose a representation for collaborative filtering tasks that allows the application of virtually any machine learning algorithm. We identify the shortcomings of current collaborative filtering techniques and propose the use of learning algorithms paired with feature extraction techniques that specifically address the limitations of previous approaches. Our best-performing algorithm is based on the singular value decomposition of an initial matrix of user ratings, exploiting latent structure that essentially eliminates the need for users to rate common items in order to become predictors for one another's preferences. We evaluate the proposed algorithm on a large database of user ratings for motion pictures and find that our approach significantly outperforms current collaborative filtering algorithms.",
            "referenceCount": 12,
            "citationCount": 1249,
            "influentialCitationCount": 96,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1998-07-24",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Billsus1998LearningCI,\n author = {Daniel Billsus and M. Pazzani},\n booktitle = {International Conference on Machine Learning},\n pages = {46-54},\n title = {Learning Collaborative Information Filters},\n year = {1998}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8603193192a64f0c9943989d209e7492689045c1",
            "@type": "ScholarlyArticle",
            "paperId": "8603193192a64f0c9943989d209e7492689045c1",
            "corpusId": 260443812,
            "url": "https://www.semanticscholar.org/paper/8603193192a64f0c9943989d209e7492689045c1",
            "title": "Artificial Intelligence A Modern Approach 3rd Edition",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2020,
            "externalIds": {
                "CorpusId": 260443812
            },
            "abstract": "Artificial IntelligenceArtificial Intelligence: A Modern Approach 2Nd Ed.Introduction to Machine LearningArtificial IntelligenceArtificial Intelligence: A Modern Approach, eBook, Global EditionIntroduction to Artificial IntelligenceModern Approaches in Machine Learning and Cognitive Science: A WalkthroughArtificial Intelligence: Pearson New International EditionArtificial IntelligenceArtificial IntelligenceArtificial IntelligenceArtificial Intelligence a Modern ApproachFundamentals of the New Artificial IntelligenceMultiagent SystemsArtificial IntelligenceArtificial IntelligenceThe Hundred-page Machine Learning BookArtificial IntelligenceArtificial IntelligenceArtificial IntelligenceDistributed Artificial IntelligenceArtificial Intelligence For BeginnersParadigms of Artificial Intelligence ProgrammingHuman CompatibleHuman CompatibleARTIFICIAL INTELLIGENCEArtificial IntelligenceArtificial IntelligenceArtificial Intelligence a Modern ApproachDo the Right ThingArtificial IntelligenceArtificial Intelligence : a Modern ApproachArtificial IntelligenceIntelligent Help Systems for UNIXArtificial IntelligenceArtificial IntelligenceArtificial Intelligence a Modern ApproachArtificial IntelligenceArtificial IntelligenceArtificial Intelligence for Human Computer Interaction: A Modern Approach",
            "referenceCount": 0,
            "citationCount": 861,
            "influentialCitationCount": 89,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Art",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Philosophy",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{None,\n title = {Artificial Intelligence A Modern Approach 3rd Edition},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3a288c63576fc385910cb5bc44eaea75b442e62e",
            "@type": "ScholarlyArticle",
            "paperId": "3a288c63576fc385910cb5bc44eaea75b442e62e",
            "corpusId": 3641284,
            "url": "https://www.semanticscholar.org/paper/3a288c63576fc385910cb5bc44eaea75b442e62e",
            "title": "UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2786672974",
                "DBLP": "journals/corr/abs-1802-03426",
                "ArXiv": "1802.03426",
                "CorpusId": 3641284
            },
            "abstract": "UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.",
            "referenceCount": 62,
            "citationCount": 6577,
            "influentialCitationCount": 1126,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-02-09",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1802.03426"
            },
            "citationStyles": {
                "bibtex": "@Article{McInnes2018UMAPUM,\n author = {Leland McInnes and John Healy},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction},\n volume = {abs/1802.03426},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c0883f5930a232a9c1ad601c978caede29155979",
            "@type": "ScholarlyArticle",
            "paperId": "c0883f5930a232a9c1ad601c978caede29155979",
            "corpusId": 13029170,
            "url": "https://www.semanticscholar.org/paper/c0883f5930a232a9c1ad601c978caede29155979",
            "title": "\u201cWhy Should I Trust You?\u201d: Explaining the Predictions of Any Classifier",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:01103732-3808-4930-b8e4-7e9e68d5c68d",
                "name": "North American Chapter of the Association for Computational Linguistics",
                "alternate_names": [
                    "North Am Chapter Assoc Comput Linguistics",
                    "NAACL"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/naacl"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2516809705",
                "ArXiv": "1602.04938",
                "DBLP": "journals/corr/RibeiroSG16",
                "ACL": "N16-3020",
                "DOI": "10.1145/2939672.2939778",
                "CorpusId": 13029170
            },
            "abstract": "Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.",
            "referenceCount": 41,
            "citationCount": 11824,
            "influentialCitationCount": 1587,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/N16-3020.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Book",
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-02-16",
            "journal": {
                "name": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Book{Ribeiro2016WhySI,\n author = {Marco Tulio Ribeiro and Sameer Singh and Carlos Guestrin},\n booktitle = {North American Chapter of the Association for Computational Linguistics},\n journal = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},\n title = {\u201cWhy Should I Trust You?\u201d: Explaining the Predictions of Any Classifier},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:26bc9195c6343e4d7f434dd65b4ad67efe2be27a",
            "@type": "ScholarlyArticle",
            "paperId": "26bc9195c6343e4d7f434dd65b4ad67efe2be27a",
            "corpusId": 4650265,
            "url": "https://www.semanticscholar.org/paper/26bc9195c6343e4d7f434dd65b4ad67efe2be27a",
            "title": "XGBoost: A Scalable Tree Boosting System",
            "venue": "Knowledge Discovery and Data Mining",
            "publicationVenue": {
                "id": "urn:research:a0edb93b-1e95-4128-a295-6b1659149cef",
                "name": "Knowledge Discovery and Data Mining",
                "alternate_names": [
                    "KDD",
                    "Knowl Discov Data Min"
                ],
                "issn": null,
                "url": "http://www.acm.org/sigkdd/"
            },
            "year": 2016,
            "externalIds": {
                "ArXiv": "1603.02754",
                "DBLP": "conf/kdd/ChenG16",
                "MAG": "3102476541",
                "DOI": "10.1145/2939672.2939785",
                "CorpusId": 4650265
            },
            "abstract": "Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.",
            "referenceCount": 27,
            "citationCount": 23755,
            "influentialCitationCount": 2875,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://dl.acm.org/ft_gateway.cfm?id=2939785&type=pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Book",
                "Conference"
            ],
            "publicationDate": "2016-03-09",
            "journal": {
                "name": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Chen2016XGBoostAS,\n author = {Tianqi Chen and Carlos Guestrin},\n booktitle = {Knowledge Discovery and Data Mining},\n journal = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},\n title = {XGBoost: A Scalable Tree Boosting System},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:34f25a8704614163c4095b3ee2fc969b60de4698",
            "@type": "ScholarlyArticle",
            "paperId": "34f25a8704614163c4095b3ee2fc969b60de4698",
            "corpusId": 6844431,
            "url": "https://www.semanticscholar.org/paper/34f25a8704614163c4095b3ee2fc969b60de4698",
            "title": "Dropout: a simple way to prevent neural networks from overfitting",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2014,
            "externalIds": {
                "DBLP": "journals/jmlr/SrivastavaHKSS14",
                "MAG": "2095705004",
                "DOI": "10.5555/2627435.2670313",
                "CorpusId": 6844431
            },
            "abstract": "Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different \"thinned\" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.",
            "referenceCount": 38,
            "citationCount": 35198,
            "influentialCitationCount": 2748,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "15"
            },
            "citationStyles": {
                "bibtex": "@Article{Srivastava2014DropoutAS,\n author = {Nitish Srivastava and Geoffrey E. Hinton and A. Krizhevsky and Ilya Sutskever and R. Salakhutdinov},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {1929-1958},\n title = {Dropout: a simple way to prevent neural networks from overfitting},\n volume = {15},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:bee044c8e8903fb67523c1f8c105ab4718600cdb",
            "@type": "ScholarlyArticle",
            "paperId": "bee044c8e8903fb67523c1f8c105ab4718600cdb",
            "corpusId": 6706414,
            "url": "https://www.semanticscholar.org/paper/bee044c8e8903fb67523c1f8c105ab4718600cdb",
            "title": "Explaining and Harnessing Adversarial Examples",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2014,
            "externalIds": {
                "MAG": "1945616565",
                "DBLP": "journals/corr/GoodfellowSS14",
                "ArXiv": "1412.6572",
                "CorpusId": 6706414
            },
            "abstract": "Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.",
            "referenceCount": 19,
            "citationCount": 14823,
            "influentialCitationCount": 3717,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2014-12-19",
            "journal": {
                "name": "CoRR",
                "volume": "abs/1412.6572"
            },
            "citationStyles": {
                "bibtex": "@Article{Goodfellow2014ExplainingAH,\n author = {I. Goodfellow and Jonathon Shlens and Christian Szegedy},\n booktitle = {International Conference on Learning Representations},\n journal = {CoRR},\n title = {Explaining and Harnessing Adversarial Examples},\n volume = {abs/1412.6572},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f3de86aeb442216a8391befcacb49e58b478f512",
            "@type": "ScholarlyArticle",
            "paperId": "f3de86aeb442216a8391befcacb49e58b478f512",
            "corpusId": 2407601,
            "url": "https://www.semanticscholar.org/paper/f3de86aeb442216a8391befcacb49e58b478f512",
            "title": "Distributed Representations of Sentences and Documents",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2014,
            "externalIds": {
                "DBLP": "conf/icml/LeM14",
                "ArXiv": "1405.4053",
                "MAG": "2949547296",
                "CorpusId": 2407601
            },
            "abstract": "Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, \"powerful,\" \"strong\" and \"Paris\" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperforms bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.",
            "referenceCount": 42,
            "citationCount": 8463,
            "influentialCitationCount": 1169,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2014-05-16",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Le2014DistributedRO,\n author = {Quoc V. Le and Tomas Mikolov},\n booktitle = {International Conference on Machine Learning},\n pages = {1188-1196},\n title = {Distributed Representations of Sentences and Documents},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:899defb6a100af509547b8d74bb626533ee87da4",
            "@type": "ScholarlyArticle",
            "paperId": "899defb6a100af509547b8d74bb626533ee87da4",
            "corpusId": 207597853,
            "url": "https://www.semanticscholar.org/paper/899defb6a100af509547b8d74bb626533ee87da4",
            "title": "Measuring the VC-Dimension of a Learning Machine",
            "venue": "Neural Computation",
            "publicationVenue": {
                "id": "urn:research:69b9bcdd-8229-4a00-a6e0-00f0e99a2bf3",
                "name": "Neural Computation",
                "alternate_names": [
                    "Neural Comput"
                ],
                "issn": "0899-7667",
                "url": "http://cognet.mit.edu/library/journals/journal?issn=08997667"
            },
            "year": 1994,
            "externalIds": {
                "DBLP": "journals/neco/VapnikLL94",
                "MAG": "1998255116",
                "DOI": "10.1162/neco.1994.6.5.851",
                "CorpusId": 207597853
            },
            "abstract": "A method for measuring the capacity of learning machines is described. The method is based on fitting a theoretically derived function to empirical measurements of the maximal difference between the error rates on two separate data sets of varying sizes. Experimental measurements of the capacity of various types of linear classifiers are presented.",
            "referenceCount": 10,
            "citationCount": 393,
            "influentialCitationCount": 35,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1994-09-01",
            "journal": {
                "name": "Neural Computation",
                "volume": "6"
            },
            "citationStyles": {
                "bibtex": "@Article{Vapnik1994MeasuringTV,\n author = {V. Vapnik and E. Levin and Yann LeCun},\n booktitle = {Neural Computation},\n journal = {Neural Computation},\n pages = {851-876},\n title = {Measuring the VC-Dimension of a Learning Machine},\n volume = {6},\n year = {1994}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b36a5bb1707bb9c70025294b3a310138aae8327a",
            "@type": "ScholarlyArticle",
            "paperId": "b36a5bb1707bb9c70025294b3a310138aae8327a",
            "corpusId": 40027675,
            "url": "https://www.semanticscholar.org/paper/b36a5bb1707bb9c70025294b3a310138aae8327a",
            "title": "Automatic differentiation in PyTorch",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2899771611",
                "CorpusId": 40027675
            },
            "abstract": "In this article, we describe an automatic differentiation module of PyTorch \u2014 a library designed to enable rapid research on machine learning models. It builds upon a few projects, most notably Lua Torch, Chainer, and HIPS Autograd [4], and provides a high performance environment with easy access to automatic differentiation of models executed on different devices (CPU and GPU). To make prototyping easier, PyTorch does not follow the symbolic approach used in many other deep learning frameworks, but focuses on differentiation of purely imperative programs, with a focus on extensibility and low overhead. Note that this preprint is a draft of certain sections from an upcoming paper covering all PyTorch features.",
            "referenceCount": 6,
            "citationCount": 13155,
            "influentialCitationCount": 1467,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2017-10-28",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Paszke2017AutomaticDI,\n author = {Adam Paszke and Sam Gross and Soumith Chintala and Gregory Chanan and E. Yang and Zach DeVito and Zeming Lin and Alban Desmaison and L. Antiga and Adam Lerer},\n title = {Automatic differentiation in PyTorch},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4a6e74d4bf4fd0106891e5518692a77c7aa8811d",
            "@type": "ScholarlyArticle",
            "paperId": "4a6e74d4bf4fd0106891e5518692a77c7aa8811d",
            "corpusId": 1929844,
            "url": "https://www.semanticscholar.org/paper/4a6e74d4bf4fd0106891e5518692a77c7aa8811d",
            "title": "Outlier Detection in High Dimensional Data",
            "venue": "Regular Issue",
            "publicationVenue": {
                "id": "urn:research:b3c3187d-18c6-4d52-8596-9c4f882f647a",
                "name": "Regular Issue",
                "alternate_names": [
                    "Regular issue",
                    "Regul Issue",
                    "Regul issue"
                ],
                "issn": "2319-6378",
                "url": "https://www.ijese.org/"
            },
            "year": 2021,
            "externalIds": {
                "MAG": "3176919784",
                "DOI": "10.35940/ijeat.e2675.0610521",
                "CorpusId": 1929844
            },
            "abstract": "Artificial intelligence (AI) is the science that allows\ncomputers to replicate human intelligence in areas such as\ndecision-making, text processing, visual perception. Artificial\nIntelligence is the broader field that contains several subfields\nsuch as machine learning, robotics, and computer vision.\nMachine Learning is a branch of Artificial Intelligence that\nallows a machine to learn and improve at a task over time. Deep\nLearning is a subset of machine learning that makes use of deep\nartificial neural networks for training. The paper proposed on\noutlier detection for multivariate high dimensional data for\nAutoencoder unsupervised model.",
            "referenceCount": 6,
            "citationCount": 1057,
            "influentialCitationCount": 64,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2021-06-30",
            "journal": {
                "name": "Regular issue",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Aggarwal2021OutlierDI,\n author = {C. Aggarwal and Philip S. Yu},\n booktitle = {Regular Issue},\n journal = {Regular issue},\n title = {Outlier Detection in High Dimensional Data},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b544ca32b66b4c9c69bcfa00d63ee4b799d8ab6b",
            "@type": "ScholarlyArticle",
            "paperId": "b544ca32b66b4c9c69bcfa00d63ee4b799d8ab6b",
            "corpusId": 1257772,
            "url": "https://www.semanticscholar.org/paper/b544ca32b66b4c9c69bcfa00d63ee4b799d8ab6b",
            "title": "Adversarial examples in the physical world",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2460937040",
                "DBLP": "conf/iclr/KurakinGB17a",
                "ArXiv": "1607.02533",
                "DOI": "10.1201/9781351251389-8",
                "CorpusId": 1257772
            },
            "abstract": "Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake. Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model. Up to now, all previous work have assumed a threat model in which the adversary can feed data directly into the machine learning classifier. This is not always the case for systems operating in the physical world, for example those which are using signals from cameras and other sensors as an input. This paper shows that even in such physical world scenarios, machine learning systems are vulnerable to adversarial examples. We demonstrate this by feeding adversarial images obtained from cell-phone camera to an ImageNet Inception classifier and measuring the classification accuracy of the system. We find that a large fraction of adversarial examples are classified incorrectly even when perceived through the camera.",
            "referenceCount": 16,
            "citationCount": 4739,
            "influentialCitationCount": 864,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1607.02533",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-07-08",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1607.02533"
            },
            "citationStyles": {
                "bibtex": "@Article{Kurakin2016AdversarialEI,\n author = {Alexey Kurakin and I. Goodfellow and Samy Bengio},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Adversarial examples in the physical world},\n volume = {abs/1607.02533},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2e55ba6c97ce5eb55abd959909403fe8da7e9fe9",
            "@type": "ScholarlyArticle",
            "paperId": "2e55ba6c97ce5eb55abd959909403fe8da7e9fe9",
            "corpusId": 4704285,
            "url": "https://www.semanticscholar.org/paper/2e55ba6c97ce5eb55abd959909403fe8da7e9fe9",
            "title": "Overcoming catastrophic forgetting in neural networks",
            "venue": "Proceedings of the National Academy of Sciences of the United States of America",
            "publicationVenue": {
                "id": "urn:research:bb95bf2e-8383-4748-bf9d-d6906d091085",
                "name": "Proceedings of the National Academy of Sciences of the United States of America",
                "alternate_names": [
                    "PNAS",
                    "PNAS online",
                    "Proceedings of the National Academy of Sciences of the United States of America.",
                    "Proc National Acad Sci",
                    "Proceedings of the National Academy of Sciences",
                    "Proc National Acad Sci u s Am"
                ],
                "issn": "0027-8424",
                "url": "https://www.jstor.org/journal/procnatiacadscie"
            },
            "year": 2016,
            "externalIds": {
                "ArXiv": "1612.00796",
                "MAG": "2560647685",
                "DBLP": "journals/corr/KirkpatrickPRVD16",
                "DOI": "10.1073/pnas.1611835114",
                "CorpusId": 4704285,
                "PubMed": "28292907"
            },
            "abstract": "Significance Deep neural networks are currently the most successful machine-learning technique for solving a variety of tasks, including language translation, image classification, and image generation. One weakness of such models is that, unlike humans, they are unable to learn multiple tasks sequentially. In this work we propose a practical solution to train such models sequentially by protecting the weights important for previous tasks. This approach, inspired by synaptic consolidation in neuroscience, enables state of the art results on multiple reinforcement learning problems experienced sequentially. The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Until now neural networks have not been capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks that they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on a hand-written digit dataset and by learning several Atari 2600 games sequentially.",
            "referenceCount": 48,
            "citationCount": 4745,
            "influentialCitationCount": 945,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.pnas.org/content/pnas/114/13/3521.full.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-12-02",
            "journal": {
                "name": "Proceedings of the National Academy of Sciences",
                "volume": "114"
            },
            "citationStyles": {
                "bibtex": "@Article{Kirkpatrick2016OvercomingCF,\n author = {J. Kirkpatrick and Razvan Pascanu and Neil C. Rabinowitz and J. Veness and Guillaume Desjardins and Andrei A. Rusu and Kieran Milan and John Quan and Tiago Ramalho and A. Grabska-Barwinska and D. Hassabis and C. Clopath and D. Kumaran and R. Hadsell},\n booktitle = {Proceedings of the National Academy of Sciences of the United States of America},\n journal = {Proceedings of the National Academy of Sciences},\n pages = {3521 - 3526},\n title = {Overcoming catastrophic forgetting in neural networks},\n volume = {114},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:81600fd653a828d69f6160705be6814dd101beb7",
            "@type": "ScholarlyArticle",
            "paperId": "81600fd653a828d69f6160705be6814dd101beb7",
            "corpusId": 214265509,
            "url": "https://www.semanticscholar.org/paper/81600fd653a828d69f6160705be6814dd101beb7",
            "title": "From local explanations to global understanding with explainable AI for trees",
            "venue": "Nature Machine Intelligence",
            "publicationVenue": {
                "id": "urn:research:6457124b-39bf-4d02-bff4-73752ff21562",
                "name": "Nature Machine Intelligence",
                "alternate_names": [
                    "Nat Mach Intell"
                ],
                "issn": "2522-5839",
                "url": "https://www.nature.com/natmachintell/"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "2999615587",
                "DBLP": "journals/natmi/LundbergECDPNKH20",
                "DOI": "10.1038/s42256-019-0138-9",
                "CorpusId": 214265509,
                "PubMed": "32607472"
            },
            "abstract": null,
            "referenceCount": 50,
            "citationCount": 2666,
            "influentialCitationCount": 235,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://europepmc.org/articles/pmc7326367?pdf=render",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-01-01",
            "journal": {
                "name": "Nature Machine Intelligence",
                "volume": "2"
            },
            "citationStyles": {
                "bibtex": "@Article{Lundberg2020FromLE,\n author = {Scott M. Lundberg and G. Erion and Hugh Chen and A. DeGrave and J. Prutkin and B. Nair and R. Katz and J. Himmelfarb and N. Bansal and Su-In Lee},\n booktitle = {Nature Machine Intelligence},\n journal = {Nature Machine Intelligence},\n pages = {56 - 67},\n title = {From local explanations to global understanding with explainable AI for trees},\n volume = {2},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:46bed4c578e96e05fa3e5704620c4ffa0746d78f",
            "@type": "ScholarlyArticle",
            "paperId": "46bed4c578e96e05fa3e5704620c4ffa0746d78f",
            "corpusId": 16951291,
            "url": "https://www.semanticscholar.org/paper/46bed4c578e96e05fa3e5704620c4ffa0746d78f",
            "title": "A Learning Machine: Part I",
            "venue": "IBM Journal of Research and Development",
            "publicationVenue": {
                "id": "urn:research:555db9fd-8025-4984-8082-971e1e6bdb24",
                "name": "IBM Journal of Research and Development",
                "alternate_names": [
                    "IBM J Res Dev",
                    "Ibm J Res Dev",
                    "Ibm Journal of Research and Development"
                ],
                "issn": "0018-8646",
                "url": "http://www.research.ibm.com/journal/index.html"
            },
            "year": 1958,
            "externalIds": {
                "MAG": "1970001627",
                "DBLP": "journals/ibmrd/Friedberg58",
                "DOI": "10.1147/rd.21.0002",
                "CorpusId": 16951291
            },
            "abstract": "Machines would be more useful if they could learn to perform tasks for which they were not given precise methods. Difficulties that attend giving a machine this ability are discussed. It is proposed that the program of a stored-program computer be gradually improved by a learning procedure which tries many programs and chooses, from the instructions that may occupy a given location, the one most often associated with a successful result. An experimental test of this principle is described in detail. Preliminary results, which show limited success, are reported and interpreted. Further results and conclusions will appear in the second part of the paper.",
            "referenceCount": 0,
            "citationCount": 413,
            "influentialCitationCount": 6,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": "IBM J. Res. Dev.",
                "volume": "2"
            },
            "citationStyles": {
                "bibtex": "@Article{Friedberg1958ALM,\n author = {R. Friedberg},\n booktitle = {IBM Journal of Research and Development},\n journal = {IBM J. Res. Dev.},\n pages = {2-13},\n title = {A Learning Machine: Part I},\n volume = {2},\n year = {1958}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:81a4fd3004df0eb05d6c1cef96ad33d5407820df",
            "@type": "ScholarlyArticle",
            "paperId": "81a4fd3004df0eb05d6c1cef96ad33d5407820df",
            "corpusId": 57375753,
            "url": "https://www.semanticscholar.org/paper/81a4fd3004df0eb05d6c1cef96ad33d5407820df",
            "title": "A Comprehensive Survey on Graph Neural Networks",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems",
            "publicationVenue": {
                "id": "urn:research:79c5a18d-0295-432c-aaa5-961d73de6d88",
                "name": "IEEE Transactions on Neural Networks and Learning Systems",
                "alternate_names": [
                    "IEEE Trans Neural Netw Learn Syst"
                ],
                "issn": "2162-237X",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5962385"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/corr/abs-1901-00596",
                "ArXiv": "1901.00596",
                "MAG": "2907492528",
                "DOI": "10.1109/TNNLS.2020.2978386",
                "CorpusId": 57375753,
                "PubMed": "32217482"
            },
            "abstract": "Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications, where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on the existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this article, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art GNNs into four categories, namely, recurrent GNNs, convolutional GNNs, graph autoencoders, and spatial\u2013temporal GNNs. We further discuss the applications of GNNs across various domains and summarize the open-source codes, benchmark data sets, and model evaluation of GNNs. Finally, we propose potential research directions in this rapidly growing field.",
            "referenceCount": 193,
            "citationCount": 5535,
            "influentialCitationCount": 376,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": "IEEE Transactions on Neural Networks and Learning Systems",
                "volume": "32"
            },
            "citationStyles": {
                "bibtex": "@Article{Wu2019ACS,\n author = {Zonghan Wu and Shirui Pan and Fengwen Chen and Guodong Long and Chengqi Zhang and Philip S. Yu},\n booktitle = {IEEE Transactions on Neural Networks and Learning Systems},\n journal = {IEEE Transactions on Neural Networks and Learning Systems},\n pages = {4-24},\n title = {A Comprehensive Survey on Graph Neural Networks},\n volume = {32},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2",
            "@type": "ScholarlyArticle",
            "paperId": "af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2",
            "corpusId": 208117506,
            "url": "https://www.semanticscholar.org/paper/af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2",
            "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2980282514",
                "ArXiv": "1910.03771",
                "DBLP": "journals/corr/abs-1910-03771",
                "CorpusId": 208117506
            },
            "abstract": "Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. \\textit{Transformers} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. \\textit{Transformers} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \\url{this https URL}.",
            "referenceCount": 81,
            "citationCount": 6499,
            "influentialCitationCount": 280,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-10-09",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1910.03771"
            },
            "citationStyles": {
                "bibtex": "@Article{Wolf2019HuggingFacesTS,\n author = {Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R\u00e9mi Louf and Morgan Funtowicz and Jamie Brew},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {HuggingFace's Transformers: State-of-the-art Natural Language Processing},\n volume = {abs/1910.03771},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3a9b175324ba11bc0e16c0633912d897b2fac4e2",
            "@type": "ScholarlyArticle",
            "paperId": "3a9b175324ba11bc0e16c0633912d897b2fac4e2",
            "corpusId": 4246903,
            "url": "https://www.semanticscholar.org/paper/3a9b175324ba11bc0e16c0633912d897b2fac4e2",
            "title": "The Pascal Visual Object Classes (VOC) Challenge",
            "venue": "International Journal of Computer Vision",
            "publicationVenue": {
                "id": "urn:research:939ee07c-6009-43f8-b884-69238b40659e",
                "name": "International Journal of Computer Vision",
                "alternate_names": [
                    "Int J Comput Vis"
                ],
                "issn": "0920-5691",
                "url": "https://www.springer.com/computer/image+processing/journal/11263"
            },
            "year": 2010,
            "externalIds": {
                "DBLP": "journals/ijcv/EveringhamGWWZ10",
                "MAG": "2031489346",
                "DOI": "10.1007/s11263-009-0275-4",
                "CorpusId": 4246903
            },
            "abstract": null,
            "referenceCount": 70,
            "citationCount": 14080,
            "influentialCitationCount": 2113,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.pure.ed.ac.uk/ws/files/7879113/ijcv_voc09.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2010-06-01",
            "journal": {
                "name": "International Journal of Computer Vision",
                "volume": "88"
            },
            "citationStyles": {
                "bibtex": "@Article{Everingham2010ThePV,\n author = {M. Everingham and L. V. Gool and Christopher K. I. Williams and J. Winn and A. Zisserman},\n booktitle = {International Journal of Computer Vision},\n journal = {International Journal of Computer Vision},\n pages = {303-338},\n title = {The Pascal Visual Object Classes (VOC) Challenge},\n volume = {88},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0af75728bec67f698a8c619645165de13780c2fa",
            "@type": "ScholarlyArticle",
            "paperId": "0af75728bec67f698a8c619645165de13780c2fa",
            "corpusId": 16193644,
            "url": "https://www.semanticscholar.org/paper/0af75728bec67f698a8c619645165de13780c2fa",
            "title": "Learning Multiple Tasks with Kernel Methods",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2005,
            "externalIds": {
                "DBLP": "journals/jmlr/EvgeniouMP05",
                "MAG": "2144752499",
                "CorpusId": 16193644
            },
            "abstract": "We study the problem of learning many related tasks simultaneously using kernel methods and regularization. The standard single-task kernel methods, such as support vector machines and regularization networks, are extended to the case of multi-task learning. Our analysis shows that the problem of estimating many task functions with regularization can be cast as a single task learning problem if a family of multi-task kernel functions we define is used. These kernels model relations among the tasks and are derived from a novel form of regularizers. Specific kernels that can be used for multi-task learning are provided and experimentally tested on two real data sets. In agreement with past empirical work on multi-task learning, the experiments show that learning multiple related tasks simultaneously using the proposed approach can significantly outperform standard single-task learning particularly when there are many related tasks but few data per task.",
            "referenceCount": 33,
            "citationCount": 972,
            "influentialCitationCount": 98,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2005-12-01",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "6"
            },
            "citationStyles": {
                "bibtex": "@Article{Evgeniou2005LearningMT,\n author = {T. Evgeniou and C. Micchelli and M. Pontil},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {615-637},\n title = {Learning Multiple Tasks with Kernel Methods},\n volume = {6},\n year = {2005}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:273dfbcb68080251f5e9ff38b4413d7bd84b10a1",
            "@type": "ScholarlyArticle",
            "paperId": "273dfbcb68080251f5e9ff38b4413d7bd84b10a1",
            "corpusId": 961425,
            "url": "https://www.semanticscholar.org/paper/273dfbcb68080251f5e9ff38b4413d7bd84b10a1",
            "title": "LIBSVM: A library for support vector machines",
            "venue": "TIST",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2011,
            "externalIds": {
                "MAG": "332049209",
                "DBLP": "journals/tist/ChangL11",
                "DOI": "10.1145/1961189.1961199",
                "CorpusId": 961425
            },
            "abstract": "LIBSVM is a library for Support Vector Machines (SVMs). We have been actively developing this package since the year 2000. The goal is to help users to easily apply SVM to their applications. LIBSVM has gained wide popularity in machine learning and many other areas. In this article, we present all implementation details of LIBSVM. Issues such as solving SVM optimization problems theoretical convergence multiclass classification probability estimates and parameter selection are discussed in detail.",
            "referenceCount": 59,
            "citationCount": 42765,
            "influentialCitationCount": 3916,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2011-04-01",
            "journal": {
                "name": "ACM Trans. Intell. Syst. Technol.",
                "volume": "2"
            },
            "citationStyles": {
                "bibtex": "@Article{Chang2011LIBSVMAL,\n author = {Chih-Chung Chang and Chih-Jen Lin},\n booktitle = {TIST},\n journal = {ACM Trans. Intell. Syst. Technol.},\n pages = {27:1-27:27},\n title = {LIBSVM: A library for support vector machines},\n volume = {2},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8592e46a5435d18bba70557846f47290b34c1aa5",
            "@type": "ScholarlyArticle",
            "paperId": "8592e46a5435d18bba70557846f47290b34c1aa5",
            "corpusId": 58779360,
            "url": "https://www.semanticscholar.org/paper/8592e46a5435d18bba70557846f47290b34c1aa5",
            "title": "Learning and relearning in Boltzmann machines",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1986,
            "externalIds": {
                "MAG": "1547224907",
                "DOI": "10.7551/mitpress/3349.003.0005",
                "CorpusId": 58779360
            },
            "abstract": "This chapter contains sections titled: Relaxation Searches, Easy and Hard Learning, The Boltzmann Machine Learning Algorithm, An Example of Hard Learning, Achieving Reliable Computation with Unreliable Hardware, An Example of the Effects of Damage, Conclusion, Acknowledgments, Appendix: Derivation of the Learning Algorithm, References",
            "referenceCount": 0,
            "citationCount": 1355,
            "influentialCitationCount": 45,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1986-01-03",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Hinton1986LearningAR,\n author = {Geoffrey E. Hinton and T. Sejnowski},\n pages = {282-317},\n title = {Learning and relearning in Boltzmann machines},\n year = {1986}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:41fef1a197fab9684a4608b725d3ae72e1ab4b39",
            "@type": "ScholarlyArticle",
            "paperId": "41fef1a197fab9684a4608b725d3ae72e1ab4b39",
            "corpusId": 5867279,
            "url": "https://www.semanticscholar.org/paper/41fef1a197fab9684a4608b725d3ae72e1ab4b39",
            "title": "Sparse Feature Learning for Deep Belief Networks",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2007,
            "externalIds": {
                "DBLP": "conf/nips/RanzatoBL07",
                "MAG": "2108665656",
                "CorpusId": 5867279
            },
            "abstract": "Unsupervised learning algorithms aim to discover the structure hidden in the data, and to learn representations that are more suitable as input to a supervised machine than the raw input. Many unsupervised methods are based on reconstructing the input from the representation, while constraining the representation to have certain desirable properties (e.g. low dimension, sparsity, etc). Others are based on approximating density by stochastically reconstructing the input from the representation. We describe a novel and efficient algorithm to learn sparse representations, and compare it theoretically and experimentally with a similar machine trained probabilistically, namely a Restricted Boltzmann Machine. We propose a simple criterion to compare and select different unsupervised machines based on the trade-off between the reconstruction error and the information content of the representation. We demonstrate this method by extracting features from a dataset of handwritten numerals, and from a dataset of natural image patches. We show that by stacking multiple levels of such machines and by training sequentially, high-order dependencies between the input observed variables can be captured.",
            "referenceCount": 19,
            "citationCount": 881,
            "influentialCitationCount": 37,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2007-12-03",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Ranzato2007SparseFL,\n author = {Marc'Aurelio Ranzato and Y-Lan Boureau and Yann LeCun},\n booktitle = {Neural Information Processing Systems},\n pages = {1185-1192},\n title = {Sparse Feature Learning for Deep Belief Networks},\n year = {2007}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b5028fb7d55d3045cda0c290ded57c4c0b3cfdc9",
            "@type": "ScholarlyArticle",
            "paperId": "b5028fb7d55d3045cda0c290ded57c4c0b3cfdc9",
            "corpusId": 12596492,
            "url": "https://www.semanticscholar.org/paper/b5028fb7d55d3045cda0c290ded57c4c0b3cfdc9",
            "title": "Foundations of Rule Learning",
            "venue": "Cognitive Technologies",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2012,
            "externalIds": {
                "MAG": "1494581921",
                "DBLP": "series/cogtech/FurnkranzGL12",
                "DOI": "10.1007/978-3-540-75197-7",
                "CorpusId": 12596492
            },
            "abstract": null,
            "referenceCount": 150,
            "citationCount": 319,
            "influentialCitationCount": 17,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2012-11-07",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{F\u00fcrnkranz2012FoundationsOR,\n author = {Johannes F\u00fcrnkranz and D. Gamberger and N. Lavra\u010d},\n booktitle = {Cognitive Technologies},\n pages = {1-298},\n title = {Foundations of Rule Learning},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5ded2b8c64491b4a67f6d39ce473d4b9347a672e",
            "@type": "ScholarlyArticle",
            "paperId": "5ded2b8c64491b4a67f6d39ce473d4b9347a672e",
            "corpusId": 3432876,
            "url": "https://www.semanticscholar.org/paper/5ded2b8c64491b4a67f6d39ce473d4b9347a672e",
            "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:01103732-3808-4930-b8e4-7e9e68d5c68d",
                "name": "North American Chapter of the Association for Computational Linguistics",
                "alternate_names": [
                    "North Am Chapter Assoc Comput Linguistics",
                    "NAACL"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/naacl"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "journals/corr/WilliamsNB17",
                "MAG": "2963846996",
                "ArXiv": "1704.05426",
                "ACL": "N18-1101",
                "DOI": "10.18653/v1/N18-1101",
                "CorpusId": 3432876
            },
            "abstract": "This paper introduces the Multi-Genre Natural Language Inference (MultiNLI) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding. At 433k examples, this resource is one of the largest corpora available for natural language inference (a.k.a. recognizing textual entailment), improving upon available resources in both its coverage and difficulty. MultiNLI accomplishes this by offering data from ten distinct genres of written and spoken English, making it possible to evaluate systems on nearly the full complexity of the language, while supplying an explicit setting for evaluating cross-genre domain adaptation. In addition, an evaluation using existing machine learning models designed for the Stanford NLI corpus shows that it represents a substantially more difficult task than does that corpus, despite the two showing similar levels of inter-annotator agreement.",
            "referenceCount": 55,
            "citationCount": 3358,
            "influentialCitationCount": 680,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/N18-1101.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-04-18",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Williams2017ABC,\n author = {Adina Williams and Nikita Nangia and Samuel R. Bowman},\n booktitle = {North American Chapter of the Association for Computational Linguistics},\n pages = {1112-1122},\n title = {A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f216444d4f2959b4520c61d20003fa30a199670a",
            "@type": "ScholarlyArticle",
            "paperId": "f216444d4f2959b4520c61d20003fa30a199670a",
            "corpusId": 13874643,
            "url": "https://www.semanticscholar.org/paper/f216444d4f2959b4520c61d20003fa30a199670a",
            "title": "Siamese Neural Networks for One-Shot Image Recognition",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2015,
            "externalIds": {
                "MAG": "3091905774",
                "CorpusId": 13874643
            },
            "abstract": "The process of learning good features for machine learning applications can be very computationally expensive and may prove difficult in cases where little data is available. A prototypical example of this is the one-shot learning setting, in which we must correctly make predictions given only a single example of each new class. In this paper, we explore a method for learning siamese neural networks which employ a unique structure to naturally rank similarity between inputs. Once a network has been tuned, we can then capitalize on powerful discriminative features to generalize the predictive power of the network not just to new data, but to entirely new classes from unknown distributions. Using a convolutional architecture, we are able to achieve strong results which exceed those of other deep learning models with near state-of-the-art performance on one-shot classification tasks.",
            "referenceCount": 31,
            "citationCount": 3497,
            "influentialCitationCount": 304,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Koch2015SiameseNN,\n author = {Gregory R. Koch},\n title = {Siamese Neural Networks for One-Shot Image Recognition},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8e0be569ea77b8cb29bb0e8b031887630fe7a96c",
            "@type": "ScholarlyArticle",
            "paperId": "8e0be569ea77b8cb29bb0e8b031887630fe7a96c",
            "corpusId": 89141,
            "url": "https://www.semanticscholar.org/paper/8e0be569ea77b8cb29bb0e8b031887630fe7a96c",
            "title": "Random Forests",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 2001,
            "externalIds": {
                "MAG": "2342146255",
                "DBLP": "journals/ml/Breiman01",
                "DOI": "10.1023/A:1010933404324",
                "CorpusId": 89141
            },
            "abstract": null,
            "referenceCount": 24,
            "citationCount": 81463,
            "influentialCitationCount": 5549,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1023/A:1010933404324.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": "Machine Learning",
                "volume": "45"
            },
            "citationStyles": {
                "bibtex": "@Article{Breiman2001RandomF,\n author = {L. Breiman},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {5-32},\n title = {Random Forests},\n volume = {45},\n year = {2001}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:98f8a0055bb28133efcff359a92937324d0e6f51",
            "@type": "ScholarlyArticle",
            "paperId": "98f8a0055bb28133efcff359a92937324d0e6f51",
            "corpusId": 12156084,
            "url": "https://www.semanticscholar.org/paper/98f8a0055bb28133efcff359a92937324d0e6f51",
            "title": "A Perspective View and Survey of Meta-Learning",
            "venue": "Artificial Intelligence Review",
            "publicationVenue": {
                "id": "urn:research:ea8553fe-2467-4367-afee-c4deb3754820",
                "name": "Artificial Intelligence Review",
                "alternate_names": [
                    "Artif Intell Rev"
                ],
                "issn": "0269-2821",
                "url": "https://link.springer.com/journal/10462"
            },
            "year": 2002,
            "externalIds": {
                "MAG": "2145680191",
                "DBLP": "journals/air/VilaltaD02",
                "DOI": "10.1023/A:1019956318069",
                "CorpusId": 12156084
            },
            "abstract": null,
            "referenceCount": 81,
            "citationCount": 1081,
            "influentialCitationCount": 36,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2002-10-01",
            "journal": {
                "name": "Artificial Intelligence Review",
                "volume": "18"
            },
            "citationStyles": {
                "bibtex": "@Article{Vilalta2002APV,\n author = {R. Vilalta and Youssef Drissi},\n booktitle = {Artificial Intelligence Review},\n journal = {Artificial Intelligence Review},\n pages = {77-95},\n title = {A Perspective View and Survey of Meta-Learning},\n volume = {18},\n year = {2002}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:de4b9bc12ddb6b9f6090c032ef5c6290bd64ef36",
            "@type": "ScholarlyArticle",
            "paperId": "de4b9bc12ddb6b9f6090c032ef5c6290bd64ef36",
            "corpusId": 3538256,
            "url": "https://www.semanticscholar.org/paper/de4b9bc12ddb6b9f6090c032ef5c6290bd64ef36",
            "title": "Artificial Intelligence",
            "venue": "Communications in Computer and Information Science",
            "publicationVenue": {
                "id": "urn:research:82cd90f0-386f-4446-b237-6404c27f6a15",
                "name": "Communications in Computer and Information Science",
                "alternate_names": [
                    "Communications in computer and information science",
                    "Commun Comput Inf Sci",
                    "Commun comput inf sci"
                ],
                "issn": "1865-0929",
                "url": "http://www.springer.com/east/home/business/business+information+systems?SGWID=5-170-69-173753703-0"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "conf/bnaic/2017",
                "MAG": "2895834617",
                "DOI": "10.1007/978-3-319-76892-2",
                "CorpusId": 3538256
            },
            "abstract": null,
            "referenceCount": 27,
            "citationCount": 3314,
            "influentialCitationCount": 249,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Education",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": "823"
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Verheij2017ArtificialI,\n author = {Bart Verheij and M. Wiering},\n booktitle = {Communications in Computer and Information Science},\n title = {Artificial Intelligence},\n volume = {823},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:41f762a455ff8c26aa99be58fe6398722864f06e",
            "@type": "ScholarlyArticle",
            "paperId": "41f762a455ff8c26aa99be58fe6398722864f06e",
            "corpusId": 57630472,
            "url": "https://www.semanticscholar.org/paper/41f762a455ff8c26aa99be58fe6398722864f06e",
            "title": "Learning from Data",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2011,
            "externalIds": {
                "MAG": "1554343978",
                "DOI": "10.1002/9781118029145.CH4",
                "CorpusId": 57630472
            },
            "abstract": "This chapter contains sections titled: Learning Machine Statistical Learning Theory Types of Learning Methods Common Learning Tasks Model Estimation Review Questions and Problems References for further study",
            "referenceCount": 0,
            "citationCount": 438,
            "influentialCitationCount": 25,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "2011-10-05",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Kantardzic2011LearningFD,\n author = {M. Kantardzic},\n pages = {87-139},\n title = {Learning from Data},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:04b23f577c20d1a0e2a67aadda555f58e6d23d6e",
            "@type": "ScholarlyArticle",
            "paperId": "04b23f577c20d1a0e2a67aadda555f58e6d23d6e",
            "corpusId": 661123,
            "url": "https://www.semanticscholar.org/paper/04b23f577c20d1a0e2a67aadda555f58e6d23d6e",
            "title": "Support vector machines",
            "venue": "Data Mining and Knowledge Discovery Handbook",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2008,
            "externalIds": {
                "MAG": "1516735418",
                "DBLP": "books/sp/datamining2005/Shmilovici05",
                "DOI": "10.1002/wics.49",
                "CorpusId": 661123
            },
            "abstract": "Support vector machines (SVMs) are a family of machine learning methods, originally introduced for the problem of classification and later generalized to various other situations. They are based on principles of statistical learning theory and convex optimization, and are currently used in various domains of application, including bioinformatics, text categorization, and computer vision. Copyright \u00a9 2009 John Wiley & Sons, Inc.",
            "referenceCount": 240,
            "citationCount": 6181,
            "influentialCitationCount": 881,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2008-08-12",
            "journal": {
                "name": "Wiley Interdisciplinary Reviews: Computational Statistics",
                "volume": "1"
            },
            "citationStyles": {
                "bibtex": "@Article{Steinwart2008SupportVM,\n author = {Ingo Steinwart and A. Christmann},\n booktitle = {Data Mining and Knowledge Discovery Handbook},\n journal = {Wiley Interdisciplinary Reviews: Computational Statistics},\n title = {Support vector machines},\n volume = {1},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:47c528344fedb6cb67a38e43d095b41c34715330",
            "@type": "ScholarlyArticle",
            "paperId": "47c528344fedb6cb67a38e43d095b41c34715330",
            "corpusId": 211678094,
            "url": "https://www.semanticscholar.org/paper/47c528344fedb6cb67a38e43d095b41c34715330",
            "title": "Adaptive Federated Optimization",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "conf/iclr/ReddiCZGRKKM21",
                "ArXiv": "2003.00295",
                "MAG": "3008187686",
                "CorpusId": 211678094
            },
            "abstract": "Federated learning is a distributed machine learning paradigm in which a large number of clients coordinate with a central server to learn a model without sharing their own training data. Due to the heterogeneity of the client datasets, standard federated optimization methods such as Federated Averaging (FedAvg) are often difficult to tune and exhibit unfavorable convergence behavior. In non-federated settings, adaptive optimization methods have had notable success in combating such issues. In this work, we propose federated versions of adaptive optimizers, including Adagrad, Adam, and Yogi, and analyze their convergence in the presence of heterogeneous data for general nonconvex settings. Our results highlight the interplay between client heterogeneity and communication efficiency. We also perform extensive experiments on these methods and show that the use of adaptive optimizers can significantly improve the performance of federated learning.",
            "referenceCount": 54,
            "citationCount": 851,
            "influentialCitationCount": 206,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-02-29",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2003.00295"
            },
            "citationStyles": {
                "bibtex": "@Article{Reddi2020AdaptiveFO,\n author = {Sashank J. Reddi and Zachary B. Charles and M. Zaheer and Zachary Garrett and Keith Rush and Jakub Konecn\u00fd and Sanjiv Kumar and H. B. McMahan},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Adaptive Federated Optimization},\n volume = {abs/2003.00295},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:09879f7956dddc2a9328f5c1472feeb8402bcbcf",
            "@type": "ScholarlyArticle",
            "paperId": "09879f7956dddc2a9328f5c1472feeb8402bcbcf",
            "corpusId": 8768364,
            "url": "https://www.semanticscholar.org/paper/09879f7956dddc2a9328f5c1472feeb8402bcbcf",
            "title": "Density estimation using Real NVP",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2950173439",
                "DBLP": "journals/corr/DinhSB16",
                "ArXiv": "1605.08803",
                "CorpusId": 8768364
            },
            "abstract": "Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.",
            "referenceCount": 78,
            "citationCount": 2816,
            "influentialCitationCount": 611,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-05-27",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1605.08803"
            },
            "citationStyles": {
                "bibtex": "@Article{Dinh2016DensityEU,\n author = {Laurent Dinh and Jascha Narain Sohl-Dickstein and Samy Bengio},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Density estimation using Real NVP},\n volume = {abs/1605.08803},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7e640729cca5a82c205bf95f346867097446838c",
            "@type": "ScholarlyArticle",
            "paperId": "7e640729cca5a82c205bf95f346867097446838c",
            "corpusId": 7553535,
            "url": "https://www.semanticscholar.org/paper/7e640729cca5a82c205bf95f346867097446838c",
            "title": "Statistical Comparisons of Classifiers over Multiple Data Sets",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2006,
            "externalIds": {
                "DBLP": "journals/jmlr/Demsar06",
                "MAG": "1565746575",
                "CorpusId": 7553535
            },
            "abstract": "While methods for comparing two learning algorithms on a single data set have been scrutinized for quite some time already, the issue of statistical tests for comparisons of more algorithms on multiple data sets, which is even more essential to typical machine learning studies, has been all but ignored. This article reviews the current practice and then theoretically and empirically examines several suitable tests. Based on that, we recommend a set of simple, yet safe and robust non-parametric tests for statistical comparisons of classifiers: the Wilcoxon signed ranks test for comparison of two classifiers and the Friedman test with the corresponding post-hoc tests for comparison of more classifiers over multiple data sets. Results of the latter can also be neatly presented with the newly introduced CD (critical difference) diagrams.",
            "referenceCount": 46,
            "citationCount": 10944,
            "influentialCitationCount": 1066,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2006-12-01",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "7"
            },
            "citationStyles": {
                "bibtex": "@Article{Dem\u0161ar2006StatisticalCO,\n author = {J. Dem\u0161ar},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {1-30},\n title = {Statistical Comparisons of Classifiers over Multiple Data Sets},\n volume = {7},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:68c1bfe375dde46777fe1ac8f3636fb651e3f0f8",
            "@type": "ScholarlyArticle",
            "paperId": "68c1bfe375dde46777fe1ac8f3636fb651e3f0f8",
            "corpusId": 1836349,
            "url": "https://www.semanticscholar.org/paper/68c1bfe375dde46777fe1ac8f3636fb651e3f0f8",
            "title": "Experiments with a New Boosting Algorithm",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 1996,
            "externalIds": {
                "MAG": "2112076978",
                "DBLP": "conf/icml/FreundS96",
                "CorpusId": 1836349
            },
            "abstract": "In an earlier paper, we introduced a new \"boosting\" algorithm called AdaBoost which, theoretically, can be used to significantly reduce the error of any learning algorithm that con- sistently generates classifiers whose performance is a little better than random guessing. We also introduced the related notion of a \"pseudo-loss\" which is a method for forcing a learning algorithm of multi-label concepts to concentrate on the labels that are hardest to discriminate. In this paper, we describe experiments we carried out to assess how well AdaBoost with and without pseudo-loss, performs on real learning problems. We performed two sets of experiments. The first set compared boosting to Breiman's \"bagging\" method when used to aggregate various classifiers (including decision trees and single attribute- value tests). We compared the performance of the two methods on a collection of machine-learning benchmarks. In the second set of experiments, we studied in more detail the performance of boosting using a nearest-neighbor classifier on an OCR problem.",
            "referenceCount": 21,
            "citationCount": 9267,
            "influentialCitationCount": 801,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1996-07-03",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Freund1996ExperimentsWA,\n author = {Y. Freund and R. Schapire},\n booktitle = {International Conference on Machine Learning},\n pages = {148-156},\n title = {Experiments with a New Boosting Algorithm},\n year = {1996}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ccf6a69a7f33bcf052aa7def176d3b9de495beb7",
            "@type": "ScholarlyArticle",
            "paperId": "ccf6a69a7f33bcf052aa7def176d3b9de495beb7",
            "corpusId": 1704893,
            "url": "https://www.semanticscholar.org/paper/ccf6a69a7f33bcf052aa7def176d3b9de495beb7",
            "title": "Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2483215953",
                "DBLP": "conf/nips/BolukbasiCZSK16",
                "ArXiv": "1607.06520",
                "CorpusId": 1704893
            },
            "abstract": "The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.",
            "referenceCount": 48,
            "citationCount": 2378,
            "influentialCitationCount": 329,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-07-21",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Bolukbasi2016ManIT,\n author = {Tolga Bolukbasi and Kai-Wei Chang and James Y. Zou and Venkatesh Saligrama and A. Kalai},\n booktitle = {Neural Information Processing Systems},\n pages = {4349-4357},\n title = {Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6b570069f14c7588e066f7138e1f21af59d62e61",
            "@type": "ScholarlyArticle",
            "paperId": "6b570069f14c7588e066f7138e1f21af59d62e61",
            "corpusId": 8993325,
            "url": "https://www.semanticscholar.org/paper/6b570069f14c7588e066f7138e1f21af59d62e61",
            "title": "Theano: A Python framework for fast computation of mathematical expressions",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "journals/corr/Al-RfouAAa16",
                "MAG": "2384495648",
                "ArXiv": "1605.02688",
                "CorpusId": 8993325
            },
            "abstract": "Theano is a Python library that allows to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. Since its introduction, it has been one of the most used CPU and GPU mathematical compilers - especially in the machine learning community - and has shown steady performance improvements. Theano is being actively and continuously developed since 2008, multiple frameworks have been built on top of it and it has been used to produce many state-of-the-art machine learning models. \nThe present article is structured as follows. Section I provides an overview of the Theano software and its community. Section II presents the principal features of Theano and how to use them, and compares them with other similar projects. Section III focuses on recently-introduced functionalities and improvements. Section IV compares the performance of Theano against Torch7 and TensorFlow on several machine learning models. Section V discusses current limitations of Theano and potential ways of improving it.",
            "referenceCount": 34,
            "citationCount": 2251,
            "influentialCitationCount": 147,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2016-05-09",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1605.02688"
            },
            "citationStyles": {
                "bibtex": "@Article{Al-Rfou2016TheanoAP,\n author = {Rami Al-Rfou and Guillaume Alain and Amjad Almahairi and Christof Angerm\u00fcller and Dzmitry Bahdanau and Nicolas Ballas and Fr\u00e9d\u00e9ric Bastien and Justin Bayer and A. Belikov and A. Belopolsky and Yoshua Bengio and Arnaud Bergeron and J. Bergstra and Valentin Bisson and Josh Bleecher Snyder and Nicolas Bouchard and Nicolas Boulanger-Lewandowski and Xavier Bouthillier and A. D. Br\u00e9bisson and Olivier Breuleux and P. Carrier and Kyunghyun Cho and J. Chorowski and P. Christiano and Tim Cooijmans and Marc-Alexandre C\u00f4t\u00e9 and Myriam C\u00f4t\u00e9 and Aaron C. Courville and Yann Dauphin and Olivier Delalleau and Julien Demouth and Guillaume Desjardins and S. Dieleman and Laurent Dinh and M\u00e9lanie Ducoffe and Vincent Dumoulin and S. Kahou and D. Erhan and Ziye Fan and Orhan Firat and M. Germain and Xavier Glorot and I. Goodfellow and M. Graham and \u00c7aglar G\u00fcl\u00e7ehre and P. Hamel and Iban Harlouchet and J. Heng and Bal\u00e1zs Hidasi and S. Honari and Arjun Jain and S\u00e9bastien Jean and Kai Jia and Mikhail Korobov and Vivek Kulkarni and Alex Lamb and Pascal Lamblin and Eric Larsen and C\u00e9sar Laurent and S. Lee and S. Lefran\u00e7ois and S. Lemieux and Nicholas L\u00e9onard and Zhouhan Lin and J. Livezey and C. Lorenz and J. Lowin and Qianli Ma and Pierre-Antoine Manzagol and Olivier Mastropietro and R. McGibbon and R. Memisevic and Bart van Merrienboer and Vincent Michalski and Mehdi Mirza and A. Orlandi and C. Pal and Razvan Pascanu and M. Pezeshki and Colin Raffel and D. Renshaw and M. Rocklin and Adriana Romero and Markus Roth and Peter Sadowski and J. Salvatier and F. Savard and Jan Schl\u00fcter and J. Schulman and Gabriel Schwartz and Iulian Serban and Dmitriy Serdyuk and S. Shabanian and \u00c9tienne Simon and Sigurd Spieckermann and S. Subramanyam and Jakub Sygnowski and J\u00e9r\u00e9mie Tanguay and Gijs van Tulder and Joseph P. Turian and S. Urban and Pascal Vincent and Francesco Visin and Harm de Vries and David Warde-Farley and Dustin J. Webb and M. Willson and Kelvin Xu and Lijun Xue and Li Yao and Saizheng Zhang and Ying Zhang},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Theano: A Python framework for fast computation of mathematical expressions},\n volume = {abs/1605.02688},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6adf016e7531c91100d3cf4a74f5d4c87b26b528",
            "@type": "ScholarlyArticle",
            "paperId": "6adf016e7531c91100d3cf4a74f5d4c87b26b528",
            "corpusId": 2672720,
            "url": "https://www.semanticscholar.org/paper/6adf016e7531c91100d3cf4a74f5d4c87b26b528",
            "title": "Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks",
            "venue": "IEEE Symposium on Security and Privacy",
            "publicationVenue": {
                "id": "urn:research:29b9c461-963e-4d11-b2ab-92c182243942",
                "name": "IEEE Symposium on Security and Privacy",
                "alternate_names": [
                    "S&P",
                    "IEEE Symp Secur Priv"
                ],
                "issn": null,
                "url": "http://www.ieee-security.org/"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "2174868984",
                "DBLP": "journals/corr/PapernotMWJS15",
                "ArXiv": "1511.04508",
                "DOI": "10.1109/SP.2016.41",
                "CorpusId": 2672720
            },
            "abstract": "Deep learning algorithms have been shown to perform extremely well on many classical machine learning problems. However, recent studies have shown that deep learning, like other machine learning techniques, is vulnerable to adversarial samples: inputs crafted to force a deep neural network (DNN) to provide adversary-selected outputs. Such attacks can seriously undermine the security of the system supported by the DNN, sometimes with devastating consequences. For example, autonomous vehicles can be crashed, illicit or illegal content can bypass content filters, or biometric authentication systems can be manipulated to allow improper access. In this work, we introduce a defensive mechanism called defensive distillation to reduce the effectiveness of adversarial samples on DNNs. We analytically investigate the generalizability and robustness properties granted by the use of defensive distillation when training DNNs. We also empirically study the effectiveness of our defense mechanisms on two DNNs placed in adversarial settings. The study shows that defensive distillation can reduce effectiveness of sample creation from 95% to less than 0.5% on a studied DNN. Such dramatic gains can be explained by the fact that distillation leads gradients used in adversarial sample creation to be reduced by a factor of 1030. We also find that distillation increases the average minimum number of features that need to be modified to create adversarial samples by about 800% on one of the DNNs we tested.",
            "referenceCount": 53,
            "citationCount": 2688,
            "influentialCitationCount": 199,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1511.04508",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-11-14",
            "journal": {
                "name": "2016 IEEE Symposium on Security and Privacy (SP)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Papernot2015DistillationAA,\n author = {Nicolas Papernot and P. Mcdaniel and Xi Wu and S. Jha and A. Swami},\n booktitle = {IEEE Symposium on Security and Privacy},\n journal = {2016 IEEE Symposium on Security and Privacy (SP)},\n pages = {582-597},\n title = {Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d516daff247f7157fccde6649ace91d969cd1973",
            "@type": "ScholarlyArticle",
            "paperId": "d516daff247f7157fccde6649ace91d969cd1973",
            "corpusId": 5981909,
            "url": "https://www.semanticscholar.org/paper/d516daff247f7157fccde6649ace91d969cd1973",
            "title": "The mythos of model interpretability",
            "venue": "Queue",
            "publicationVenue": {
                "id": "urn:research:4bd27ad1-f6fa-4340-9d9d-c62ac5be6fc0",
                "name": "Queue",
                "alternate_names": [
                    "ACM Queue"
                ],
                "issn": "1542-7730",
                "url": "http://portal.acm.org/queue"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "3083430015",
                "DBLP": "journals/queue/Lipton18",
                "ArXiv": "1606.03490",
                "DOI": "10.1145/3233231",
                "CorpusId": 5981909
            },
            "abstract": "In machine learning, the concept of interpretability is both important and slippery.",
            "referenceCount": 40,
            "citationCount": 3029,
            "influentialCitationCount": 227,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1606.03490",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Philosophy",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-06-10",
            "journal": {
                "name": "Communications of the ACM",
                "volume": "61"
            },
            "citationStyles": {
                "bibtex": "@Article{Lipton2016TheMO,\n author = {Zachary Chase Lipton},\n booktitle = {Queue},\n journal = {Communications of the ACM},\n pages = {36 - 43},\n title = {The mythos of model interpretability},\n volume = {61},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0b2a2ed870d9e947aca41daf751d987ab3163d74",
            "@type": "ScholarlyArticle",
            "paperId": "0b2a2ed870d9e947aca41daf751d987ab3163d74",
            "corpusId": 16665343,
            "url": "https://www.semanticscholar.org/paper/0b2a2ed870d9e947aca41daf751d987ab3163d74",
            "title": "Introduction to Statistical Pattern Recognition",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2006,
            "externalIds": {
                "MAG": "2971535665",
                "DOI": "10.1002/0470854774.ch1",
                "CorpusId": 16665343
            },
            "abstract": "Annotation : Pattern recognition problem is briefly characterized as a process of machine learning. Its main stages (dimensionality reduction and classifier design) are stated. Statistical approach is given priority here. Two approaches to dimensionality reduction, namely feature selection (FS) and feature extraction (FE) are specified. Though FS is a special case of FE, they are very different from a practical viewpoint and thus must be considered separately.",
            "referenceCount": 6,
            "citationCount": 4365,
            "influentialCitationCount": 499,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Pudil2006IntroductionTS,\n author = {P. Pudil and P. Somol and M. Haindl},\n title = {Introduction to Statistical Pattern Recognition},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7a59fde27461a3ef4a21a249cc403d0d96e4a0d7",
            "@type": "ScholarlyArticle",
            "paperId": "7a59fde27461a3ef4a21a249cc403d0d96e4a0d7",
            "corpusId": 877929,
            "url": "https://www.semanticscholar.org/paper/7a59fde27461a3ef4a21a249cc403d0d96e4a0d7",
            "title": "Random Features for Large-Scale Kernel Machines",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2007,
            "externalIds": {
                "DBLP": "conf/nips/RahimiR07",
                "MAG": "2144902422",
                "CorpusId": 877929
            },
            "abstract": "To accelerate the training of kernel machines, we propose to map the input data to a randomized low-dimensional feature space and then apply existing fast linear methods. The features are designed so that the inner products of the transformed data are approximately equal to those in the feature space of a user specified shift-invariant kernel. We explore two sets of random features, provide convergence bounds on their ability to approximate various radial basis kernels, and show that in large-scale classification and regression tasks linear machine learning algorithms applied to these features outperform state-of-the-art large-scale kernel machines.",
            "referenceCount": 17,
            "citationCount": 3539,
            "influentialCitationCount": 633,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2007-12-03",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Rahimi2007RandomFF,\n author = {A. Rahimi and B. Recht},\n booktitle = {Neural Information Processing Systems},\n pages = {1177-1184},\n title = {Random Features for Large-Scale Kernel Machines},\n year = {2007}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:88816ae492956f3004daa41357166f1181c0c1bf",
            "@type": "ScholarlyArticle",
            "paperId": "88816ae492956f3004daa41357166f1181c0c1bf",
            "corpusId": 14879317,
            "url": "https://www.semanticscholar.org/paper/88816ae492956f3004daa41357166f1181c0c1bf",
            "title": "Laplacian Eigenmaps for Dimensionality Reduction and Data Representation",
            "venue": "Neural Computation",
            "publicationVenue": {
                "id": "urn:research:69b9bcdd-8229-4a00-a6e0-00f0e99a2bf3",
                "name": "Neural Computation",
                "alternate_names": [
                    "Neural Comput"
                ],
                "issn": "0899-7667",
                "url": "http://cognet.mit.edu/library/journals/journal?issn=08997667"
            },
            "year": 2003,
            "externalIds": {
                "DBLP": "journals/neco/BelkinN03",
                "MAG": "2097308346",
                "DOI": "10.1162/089976603321780317",
                "CorpusId": 14879317
            },
            "abstract": "One of the central problems in machine learning and pattern recognition is to develop appropriate representations for complex data. We consider the problem of constructing a representation for data lying on a low-dimensional manifold embedded in a high-dimensional space. Drawing on the correspondence between the graph Laplacian, the Laplace Beltrami operator on the manifold, and the connections to the heat equation, we propose a geometrically motivated algorithm for representing the high-dimensional data. The algorithm provides a computationally efficient approach to nonlinear dimensionality reduction that has locality-preserving properties and a natural connection to clustering. Some potential applications and illustrative examples are discussed.",
            "referenceCount": 64,
            "citationCount": 7681,
            "influentialCitationCount": 698,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://jupiter.math.nctu.edu.tw/%7Eweng/courses/2010_topic_discrete/Spectrum/Laplacian.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2003-06-01",
            "journal": {
                "name": "Neural Computation",
                "volume": "15"
            },
            "citationStyles": {
                "bibtex": "@Article{Belkin2003LaplacianEF,\n author = {M. Belkin and P. Niyogi},\n booktitle = {Neural Computation},\n journal = {Neural Computation},\n pages = {1373-1396},\n title = {Laplacian Eigenmaps for Dimensionality Reduction and Data Representation},\n volume = {15},\n year = {2003}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5a05b9d5de7d1d11d1aa03826ded577d4acf4f49",
            "@type": "ScholarlyArticle",
            "paperId": "5a05b9d5de7d1d11d1aa03826ded577d4acf4f49",
            "corpusId": 13373306,
            "url": "https://www.semanticscholar.org/paper/5a05b9d5de7d1d11d1aa03826ded577d4acf4f49",
            "title": "Genetic Algorithms + Data Structures = Evolution Programs",
            "venue": "Springer Berlin Heidelberg",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1996,
            "externalIds": {
                "DBLP": "books/lib/Michalewicz92",
                "MAG": "2142183404",
                "DOI": "10.1007/978-3-662-03315-9",
                "CorpusId": 13373306
            },
            "abstract": null,
            "referenceCount": 116,
            "citationCount": 11758,
            "influentialCitationCount": 556,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Michalewicz1996GeneticA,\n author = {Z. Michalewicz},\n booktitle = {Springer Berlin Heidelberg},\n pages = {I-XIV, 1-250},\n title = {Genetic Algorithms + Data Structures = Evolution Programs},\n year = {1996}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:10e51ffcf16a00d9f0e28189a6a9c625e8114321",
            "@type": "ScholarlyArticle",
            "paperId": "10e51ffcf16a00d9f0e28189a6a9c625e8114321",
            "corpusId": 5163286,
            "url": "https://www.semanticscholar.org/paper/10e51ffcf16a00d9f0e28189a6a9c625e8114321",
            "title": "An Introduction to Genetic Algorithms.",
            "venue": "Artificial Life",
            "publicationVenue": {
                "id": "urn:research:5ed9f470-65a3-4077-8dab-163b0c5cb9e6",
                "name": "Artificial Life",
                "alternate_names": [
                    "Artif Life"
                ],
                "issn": "1064-5462",
                "url": "http://cognet.mit.edu/library/journals/journal?issn=10645462"
            },
            "year": 1997,
            "externalIds": {
                "MAG": "2014574528",
                "DOI": "10.1162/artl.1997.3.1.63",
                "CorpusId": 5163286
            },
            "abstract": "An Introduction to Genetic Algorithms is one of the rare examples of a book in which every single page is worth reading. The author, Melanie Mitchell, manages to describe in depth many fascinating examples as well as important theoretical issues, yet the book is concise (200 pages) and readable. Although Mitchell explicitly states that her aim is not a complete survey, the essentials of genetic algorithms (GAs) are contained: theory and practice, problem solving and scientific models, a \"Brief History\" and \"Future Directions.\" Her book is both an introduction for novices interested in GAs and a collection of recent research, including hot topics such as coevolution (interspecies and intraspecies), diploidy and dominance, encapsulation, hierarchical regulation, adaptive encoding, interactions of learning and evolution, self-adapting GAs, and more. Nevertheless, the book focused more on machine learning, artificial life, and modeling evolution than on optimization and engineering.",
            "referenceCount": 0,
            "citationCount": 9403,
            "influentialCitationCount": 668,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": "Artificial Life",
                "volume": "3"
            },
            "citationStyles": {
                "bibtex": "@Article{Heiss-Czedik1997AnIT,\n author = {D. Heiss-Czedik},\n booktitle = {Artificial Life},\n journal = {Artificial Life},\n pages = {63-65},\n title = {An Introduction to Genetic Algorithms.},\n volume = {3},\n year = {1997}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:08ad8fad21f6ec4cda4d56be1ca5e146b7c913a1",
            "@type": "ScholarlyArticle",
            "paperId": "08ad8fad21f6ec4cda4d56be1ca5e146b7c913a1",
            "corpusId": 13193974,
            "url": "https://www.semanticscholar.org/paper/08ad8fad21f6ec4cda4d56be1ca5e146b7c913a1",
            "title": "Understanding Black-box Predictions via Influence Functions",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2017,
            "externalIds": {
                "ArXiv": "1703.04730",
                "DBLP": "journals/corr/KohL17",
                "MAG": "2597603852",
                "CorpusId": 13193974
            },
            "abstract": "How can we explain the predictions of a black-box model? In this paper, we use influence functions \u2014 a classic technique from robust statistics \u2014 to trace a model's prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up influence functions to modern machine learning settings, we develop a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to influence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visually-indistinguishable training-set attacks.",
            "referenceCount": 52,
            "citationCount": 2140,
            "influentialCitationCount": 284,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-03-14",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Koh2017UnderstandingBP,\n author = {Pang Wei Koh and Percy Liang},\n booktitle = {International Conference on Machine Learning},\n pages = {1885-1894},\n title = {Understanding Black-box Predictions via Influence Functions},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:43d2ed5c3c55c1100450cd74dc1031afa24d37b2",
            "@type": "ScholarlyArticle",
            "paperId": "43d2ed5c3c55c1100450cd74dc1031afa24d37b2",
            "corpusId": 62016134,
            "url": "https://www.semanticscholar.org/paper/43d2ed5c3c55c1100450cd74dc1031afa24d37b2",
            "title": "Collective Classification in Network Data",
            "venue": "The AI Magazine",
            "publicationVenue": {
                "id": "urn:research:6fedff74-7525-4b7f-bbb4-4df4e23948e4",
                "name": "The AI Magazine",
                "alternate_names": [
                    "AI Mag",
                    "Ai Mag",
                    "Ai Magazine"
                ],
                "issn": "0738-4602",
                "url": "https://www.aaai.org/Library/Magazine/magazine-library.php"
            },
            "year": 2008,
            "externalIds": {
                "MAG": "2403788960",
                "DBLP": "journals/aim/SenNBGGE08",
                "DOI": "10.1201/b17320-16",
                "CorpusId": 62016134
            },
            "abstract": "Many real-world applications produce networked data such as the world-wide web (hypertext documents connected via hyperlinks), social networks (for example, people connected by friendship links), communication networks (computers connected via communication links) and biological networks (for example, protein interaction networks). A recent focus in machine learning research has been to extend traditional machine learning classification techniques to classify nodes in such networks. In this article, we provide a brief introduction to this area of research and how it has progressed during the past decade. We introduce four of the most widely used inference algorithms for classifying networked data and empirically compare them on both synthetic and real-world data.",
            "referenceCount": 94,
            "citationCount": 3138,
            "influentialCitationCount": 686,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/aimagazine/article/download/2157/2022",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2008-09-06",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Sen2008CollectiveCI,\n author = {P. Sen and Galileo Namata and M. Bilgic and L. Getoor and Brian Gallagher and Tina Eliassi-Rad},\n booktitle = {The AI Magazine},\n pages = {399-416},\n title = {Collective Classification in Network Data},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3a84214cb69ea0b34352285029f368b75718c32b",
            "@type": "ScholarlyArticle",
            "paperId": "3a84214cb69ea0b34352285029f368b75718c32b",
            "corpusId": 3819513,
            "url": "https://www.semanticscholar.org/paper/3a84214cb69ea0b34352285029f368b75718c32b",
            "title": "Understanding of a convolutional neural network",
            "venue": "International Conference on Emerging Technologies",
            "publicationVenue": {
                "id": "urn:research:12b499eb-56c9-4d88-b792-6d90ceff3485",
                "name": "International Conference on Emerging Technologies",
                "alternate_names": [
                    "ICET",
                    "Int Conf Emerg Technol"
                ],
                "issn": null,
                "url": null
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2789876780",
                "DOI": "10.1109/ICENGTECHNOL.2017.8308186",
                "CorpusId": 3819513
            },
            "abstract": "The term Deep Learning or Deep Neural Network refers to Artificial Neural Networks (ANN) with multi layers. Over the last few decades, it has been considered to be one of the most powerful tools, and has become very popular in the literature as it is able to handle a huge amount of data. The interest in having deeper hidden layers has recently begun to surpass classical methods performance in different fields; especially in pattern recognition. One of the most popular deep neural networks is the Convolutional Neural Network (CNN). It take this name from mathematical linear operation between matrixes called convolution. CNN have multiple layers; including convolutional layer, non-linearity layer, pooling layer and fully-connected layer. The convolutional and fully-connected layers have parameters but pooling and non-linearity layers don't have parameters. The CNN has an excellent performance in machine learning problems. Specially the applications that deal with image data, such as largest image classification data set (Image Net), computer vision, and in natural language processing (NLP) and the results achieved were very amazing. In this paper we will explain and define all the elements and important issues related to CNN, and how these elements work. In addition, we will also state the parameters that effect CNN efficiency. This paper assumes that the readers have adequate knowledge about both machine learning and artificial neural network.",
            "referenceCount": 13,
            "citationCount": 2137,
            "influentialCitationCount": 150,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Conference"
            ],
            "publicationDate": "2017-08-01",
            "journal": {
                "name": "2017 International Conference on Engineering and Technology (ICET)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Conference{Albawi2017UnderstandingOA,\n author = {Saad Albawi and T. Mohammed and Saad Al-Zawi},\n booktitle = {International Conference on Emerging Technologies},\n journal = {2017 International Conference on Engineering and Technology (ICET)},\n pages = {1-6},\n title = {Understanding of a convolutional neural network},\n year = {2017}\n}\n"
            }
        }
    }
]