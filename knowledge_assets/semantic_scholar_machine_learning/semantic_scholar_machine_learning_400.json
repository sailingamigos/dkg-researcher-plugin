[
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:84bb60b83f82ad847e19d96403ad0011abfc888f",
            "@type": "ScholarlyArticle",
            "paperId": "84bb60b83f82ad847e19d96403ad0011abfc888f",
            "corpusId": 221284382,
            "url": "https://www.semanticscholar.org/paper/84bb60b83f82ad847e19d96403ad0011abfc888f",
            "title": "The Boosting Approach to Machine Learning An Overview",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2003,
            "externalIds": {
                "MAG": "2168020168",
                "DOI": "10.1007/978-0-387-21579-2_9",
                "CorpusId": 221284382
            },
            "abstract": null,
            "referenceCount": 88,
            "citationCount": 2076,
            "influentialCitationCount": 116,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Schapire2003TheBA,\n author = {R. Schapire},\n pages = {149-171},\n title = {The Boosting Approach to Machine Learning An Overview},\n year = {2003}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:23b559b5ab27f2fca6f56c0a7b6478bcf69db509",
            "@type": "ScholarlyArticle",
            "paperId": "23b559b5ab27f2fca6f56c0a7b6478bcf69db509",
            "corpusId": 5758868,
            "url": "https://www.semanticscholar.org/paper/23b559b5ab27f2fca6f56c0a7b6478bcf69db509",
            "title": "Dual Learning for Machine Translation",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2016,
            "externalIds": {
                "ArXiv": "1611.00179",
                "DBLP": "journals/corr/XiaHQWYLM16",
                "MAG": "2950359962",
                "CorpusId": 5758868
            },
            "abstract": "While neural machine translation (NMT) is making good progress in the past two years, tens of millions of bilingual sentence pairs are needed for its training. However, human labeling is very costly. To tackle this training data bottleneck, we develop a dual-learning mechanism, which can enable an NMT system to automatically learn from unlabeled data through a dual-learning game. This mechanism is inspired by the following observation: any machine translation task has a dual task, e.g., English-to-French translation (primal) versus French-to-English translation (dual); the primal and dual tasks can form a closed loop, and generate informative feedback signals to train the translation models, even if without the involvement of a human labeler. In the dual-learning mechanism, we use one agent to represent the model for the primal task and the other agent to represent the model for the dual task, then ask them to teach each other through a reinforcement learning process. Based on the feedback signals generated during this process (e.g., the language-model likelihood of the output of a model, and the reconstruction error of the original sentence after the primal and dual translations), we can iteratively update the two models until convergence (e.g., using the policy gradient methods). We call the corresponding approach to neural machine translation dual-NMT. Experiments show that dual-NMT works very well on English \u2194 French translation; especially, by learning from monolingual data (with 10% bilingual data for warm start), it achieves a comparable accuracy to NMT trained from the full bilingual data for the French-to-English translation task.",
            "referenceCount": 19,
            "citationCount": 770,
            "influentialCitationCount": 62,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-11-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{He2016DualLF,\n author = {Di He and Yingce Xia and Tao Qin and Liwei Wang and Nenghai Yu and Tie-Yan Liu and Wei-Ying Ma},\n booktitle = {Neural Information Processing Systems},\n pages = {820-828},\n title = {Dual Learning for Machine Translation},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8346b9a8e156d6e7a7012bcd47bc4f5d4be59e92",
            "@type": "ScholarlyArticle",
            "paperId": "8346b9a8e156d6e7a7012bcd47bc4f5d4be59e92",
            "corpusId": 206578669,
            "url": "https://www.semanticscholar.org/paper/8346b9a8e156d6e7a7012bcd47bc4f5d4be59e92",
            "title": "Outside the Closed World: On Using Machine Learning for Network Intrusion Detection",
            "venue": "IEEE Symposium on Security and Privacy",
            "publicationVenue": {
                "id": "urn:research:29b9c461-963e-4d11-b2ab-92c182243942",
                "name": "IEEE Symposium on Security and Privacy",
                "alternate_names": [
                    "S&P",
                    "IEEE Symp Secur Priv"
                ],
                "issn": null,
                "url": "http://www.ieee-security.org/"
            },
            "year": 2010,
            "externalIds": {
                "MAG": "1985987493",
                "DBLP": "conf/sp/SommerP10",
                "DOI": "10.1109/SP.2010.25",
                "CorpusId": 206578669
            },
            "abstract": "In network intrusion detection research, one popular strategy for finding attacks is monitoring a network's activity for anomalies: deviations from profiles of normality previously learned from benign traffic, typically identified using tools borrowed from the machine learning community. However, despite extensive academic research one finds a striking gap in terms of actual deployments of such systems: compared with other intrusion detection approaches, machine learning is rarely employed in operational \"real world\" settings. We examine the differences between the network intrusion detection problem and other areas where machine learning regularly finds much more success. Our main claim is that the task of finding attacks is fundamentally different from these other applications, making it significantly harder for the intrusion detection community to employ machine learning effectively. We support this claim by identifying challenges particular to network intrusion detection, and provide a set of guidelines meant to strengthen future research on anomaly detection.",
            "referenceCount": 68,
            "citationCount": 1381,
            "influentialCitationCount": 134,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.icir.org/robin/papers/oakland10-ml.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2010-05-16",
            "journal": {
                "name": "2010 IEEE Symposium on Security and Privacy",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Sommer2010OutsideTC,\n author = {Robin Sommer and V. Paxson},\n booktitle = {IEEE Symposium on Security and Privacy},\n journal = {2010 IEEE Symposium on Security and Privacy},\n pages = {305-316},\n title = {Outside the Closed World: On Using Machine Learning for Network Intrusion Detection},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c62043a7d2537bbf40a84b9913957452a47fdb83",
            "@type": "ScholarlyArticle",
            "paperId": "c62043a7d2537bbf40a84b9913957452a47fdb83",
            "corpusId": 61294087,
            "url": "https://www.semanticscholar.org/paper/c62043a7d2537bbf40a84b9913957452a47fdb83",
            "title": "Dataset Shift in Machine Learning",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2009,
            "externalIds": {
                "MAG": "2162651021",
                "DOI": "10.7551/MITPRESS/9780262170055.001.0001",
                "CorpusId": 61294087
            },
            "abstract": "Dataset shift is a common problem in predictive modeling that occurs when the joint distribution of inputs and outputs differs between training and test stages. Covariate shift, a particular case of dataset shift, occurs when only the input distribution changes. Dataset shift is present in most practical applications, for reasons ranging from the bias introduced by experimental design to the irreproducibility of the testing conditions at training time. (An example is -email spam filtering, which may fail to recognize spam that differs in form from the spam the automatic filter has been built on.) Despite this, and despite the attention given to the apparently similar problems of semi-supervised learning and active learning, dataset shift has received relatively little attention in the machine learning community until recently. This volume offers an overview of current efforts to deal with dataset and covariate shift. The chapters offer a mathematical and philosophical introduction to the problem, place dataset shift in relationship to transfer learning, transduction, local learning, active learning, and semi-supervised learning, provide theoretical views of dataset and covariate shift (including decision theoretic and Bayesian perspectives), and present algorithms for covariate shift. Contributors: Shai Ben-David, Steffen Bickel, Karsten Borgwardt, Michael Brckner, David Corfield, Amir Globerson, Arthur Gretton, Lars Kai Hansen, Matthias Hein, Jiayuan Huang, Takafumi Kanamori, Klaus-Robert Mller, Sam Roweis, Neil Rubens, Tobias Scheffer, Marcel Schmittfull, Bernhard Schlkopf, Hidetoshi Shimodaira, Alex Smola, Amos Storkey, Masashi Sugiyama, Choon Hui Teo Neural Information Processing series",
            "referenceCount": 162,
            "citationCount": 1679,
            "influentialCitationCount": 51,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.acad.bg/ebook/ml/The.MIT.Press.Dataset.Shift.in.Machine.Learning.Feb.2009.eBook-DDU.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "2009-02-27",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Quionero-Candela2009DatasetSI,\n author = {Joaquin Quionero-Candela and Masashi Sugiyama and A. Schwaighofer and Neil D. Lawrence},\n pages = {29-38},\n title = {Dataset Shift in Machine Learning},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4efe53a9653d1481e50382a2c95bce6eb4f6de9d",
            "@type": "ScholarlyArticle",
            "paperId": "4efe53a9653d1481e50382a2c95bce6eb4f6de9d",
            "corpusId": 122747468,
            "url": "https://www.semanticscholar.org/paper/4efe53a9653d1481e50382a2c95bce6eb4f6de9d",
            "title": "Kernel Methods and Machine Learning",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2014,
            "externalIds": {
                "MAG": "576428146",
                "DOI": "10.1017/CBO9781139176224.013",
                "CorpusId": 122747468
            },
            "abstract": "Part I. Machine Learning and Kernel Vector Spaces: 1. Fundamentals of machine learning 2. Kernel-induced vector spaces Part II. Dimension-Reduction: Feature Selection and PCA/KPCA: 3. Feature selection 4. PCA and Kernel-PCA Part III. Unsupervised Learning Models for Cluster Analysis: 5. Unsupervised learning for cluster discovery 6. Kernel methods for cluster discovery Part IV. Kernel Ridge Regressors and Variants: 7. Kernel-based regression and regularization analysis 8. Linear regression and discriminant analysis for supervised classification 9. Kernel ridge regression for supervised classification Part V. Support Vector Machines and Variants: 10. Support vector machines 11. Support vector learning models for outlier detection 12. Ridge-SVM learning models Part VI. Kernel Methods for Green Machine Learning Technologies: 13. Efficient kernel methods for learning and classifcation Part VII. Kernel Methods and Statistical Estimation Theory: 14. Statistical regression analysis and errors-in-variables models 15: Kernel methods for estimation, prediction, and system identification Part VIII. Appendices: Appendix A. Validation and test of learning models Appendix B. kNN, PNN, and Bayes classifiers References Index.",
            "referenceCount": 285,
            "citationCount": 209,
            "influentialCitationCount": 24,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2014-06-23",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Kung2014KernelMA,\n author = {S. Kung},\n title = {Kernel Methods and Machine Learning},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d82f27f4a8dcee6cbab41ff954cc6c2b7709a693",
            "@type": "ScholarlyArticle",
            "paperId": "d82f27f4a8dcee6cbab41ff954cc6c2b7709a693",
            "corpusId": 62977648,
            "url": "https://www.semanticscholar.org/paper/d82f27f4a8dcee6cbab41ff954cc6c2b7709a693",
            "title": "Mastering Machine Learning With scikit-learn",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2014,
            "externalIds": {
                "MAG": "2565637284",
                "CorpusId": 62977648
            },
            "abstract": "Apply effective learning algorithms to real-world problems using scikit-learn About This BookDesign and troubleshoot machine learning systems for common tasks including regression, classification, and clusteringAcquaint yourself with popular machine learning algorithms, including decision trees, logistic regression, and support vector machinesA practical example-based guide to help you gain expertise in implementing and evaluating machine learning systems using scikit-learnWho This Book Is ForIf you are a software developer who wants to learn how machine learning models work and how to apply them effectively, this book is for you. Familiarity with machine learning fundamentals and Python will be helpful, but is not essential. In Detail This book examines machine learning models including logistic regression, decision trees, and support vector machines, and applies them to common problems such as categorizing documents and classifying images. It begins with the fundamentals of machine learning, introducing you to the supervised-unsupervised spectrum, the uses of training and test data, and evaluating models. You will learn how to use generalized linear models in regression problems, as well as solve problems with text and categorical features.You will be acquainted with the use of logistic regression, regularization, and the various loss functions that are used by generalized linear models. The book will also walk you through an example project that prompts you to label the most uncertain training examples. You will also use an unsupervised Hidden Markov Model to predict stock prices.By the end of the book, you will be an expert in scikit-learn and will be well versed in machine learning",
            "referenceCount": 0,
            "citationCount": 199,
            "influentialCitationCount": 16,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2014-11-10",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Hackeling2014MasteringML,\n author = {Gavin Hackeling},\n title = {Mastering Machine Learning With scikit-learn},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:61ce67533d2dd6605c907146658ccdbc4778a5d8",
            "@type": "ScholarlyArticle",
            "paperId": "61ce67533d2dd6605c907146658ccdbc4778a5d8",
            "corpusId": 19285959,
            "url": "https://www.semanticscholar.org/paper/61ce67533d2dd6605c907146658ccdbc4778a5d8",
            "title": "Learning a Multi-View Stereo Machine",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "conf/nips/KarHM17",
                "MAG": "2748099314",
                "ArXiv": "1708.05375",
                "CorpusId": 19285959
            },
            "abstract": "We present a learnt system for multi-view stereopsis. In contrast to recent learning based methods for 3D reconstruction, we leverage the underlying 3D geometry of the problem through feature projection and unprojection along viewing rays. By formulating these operations in a differentiable manner, we are able to learn the system end-to-end for the task of metric 3D reconstruction. End-to-end learning allows us to jointly reason about shape priors while conforming geometric constraints, enabling reconstruction from much fewer images (even a single image) than required by classical approaches as well as completion of unseen surfaces. We thoroughly evaluate our approach on the ShapeNet dataset and demonstrate the benefits over classical approaches as well as recent learning based methods.",
            "referenceCount": 62,
            "citationCount": 441,
            "influentialCitationCount": 36,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-08-01",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1708.05375"
            },
            "citationStyles": {
                "bibtex": "@Article{Kar2017LearningAM,\n author = {Abhishek Kar and Christian H\u00e4ne and Jitendra Malik},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Learning a Multi-View Stereo Machine},\n volume = {abs/1708.05375},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:df2a7756382540e92895f10703cec32d50c4f316",
            "@type": "ScholarlyArticle",
            "paperId": "df2a7756382540e92895f10703cec32d50c4f316",
            "corpusId": 321566,
            "url": "https://www.semanticscholar.org/paper/df2a7756382540e92895f10703cec32d50c4f316",
            "title": "Fast and accurate modeling of molecular atomization energies with machine learning.",
            "venue": "Physical Review Letters",
            "publicationVenue": {
                "id": "urn:research:16c9f9d4-bee1-435d-8c85-22a3deba109d",
                "name": "Physical Review Letters",
                "alternate_names": [
                    "Phys Rev Lett"
                ],
                "issn": "0031-9007",
                "url": "https://journals.aps.org/prl/"
            },
            "year": 2011,
            "externalIds": {
                "MAG": "2104489082",
                "ArXiv": "1109.2618",
                "DOI": "10.1103/PhysRevLett.108.058301",
                "CorpusId": 321566,
                "PubMed": "22400967"
            },
            "abstract": "We introduce a machine learning model to predict atomization energies of a diverse set of organic molecules, based on nuclear charges and atomic positions only. The problem of solving the molecular Schr\u00f6dinger equation is mapped onto a nonlinear statistical regression problem of reduced complexity. Regression models are trained on and compared to atomization energies computed with hybrid density-functional theory. Cross validation over more than seven thousand organic molecules yields a mean absolute error of \u223c10\u2009\u2009kcal/mol. Applicability is demonstrated for the prediction of molecular atomization potential energy curves.",
            "referenceCount": 21,
            "citationCount": 1359,
            "influentialCitationCount": 32,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.aps.org/accepted/10.1103/PhysRevLett.108.058301",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Physics",
                "Mathematics",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Physics",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Chemistry",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2011-09-12",
            "journal": {
                "name": "Physical review letters",
                "volume": "108 5"
            },
            "citationStyles": {
                "bibtex": "@Article{Rupp2011FastAA,\n author = {M. Rupp and A. Tkatchenko and K. M\u00fcller and O. A. von Lilienfeld},\n booktitle = {Physical Review Letters},\n journal = {Physical review letters},\n pages = {\n          058301\n        },\n title = {Fast and accurate modeling of molecular atomization energies with machine learning.},\n volume = {108 5},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:93aa298b40bb3ec23c25239089284fdf61ded917",
            "@type": "ScholarlyArticle",
            "paperId": "93aa298b40bb3ec23c25239089284fdf61ded917",
            "corpusId": 564746,
            "url": "https://www.semanticscholar.org/paper/93aa298b40bb3ec23c25239089284fdf61ded917",
            "title": "Support vector machine learning for interdependent and structured output spaces",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2004,
            "externalIds": {
                "MAG": "2429914308",
                "DBLP": "conf/icml/TsochantaridisHJA04",
                "DOI": "10.1145/1015330.1015341",
                "CorpusId": 564746
            },
            "abstract": "Learning general functional dependencies is one of the main goals in machine learning. Recent progress in kernel-based methods has focused on designing flexible and powerful input representations. This paper addresses the complementary issue of problems involving complex outputs such as multiple dependent output variables and structured output spaces. We propose to generalize multiclass Support Vector Machine learning in a formulation that involves features extracted jointly from inputs and outputs. The resulting optimization problem is solved efficiently by a cutting plane algorithm that exploits the sparseness and structural decomposition of the problem. We demonstrate the versatility and effectiveness of our method on problems ranging from supervised grammar learning and named-entity recognition, to taxonomic text classification and sequence alignment.",
            "referenceCount": 15,
            "citationCount": 1472,
            "influentialCitationCount": 300,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://luthuli.cs.uiuc.edu/~daf/courses/Optimization/Papers/p76-tsochantaridis.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Book",
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2004-07-04",
            "journal": {
                "name": "Proceedings of the twenty-first international conference on Machine learning",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Book{Tsochantaridis2004SupportVM,\n author = {Ioannis Tsochantaridis and Thomas Hofmann and T. Joachims and Y. Altun},\n booktitle = {International Conference on Machine Learning},\n journal = {Proceedings of the twenty-first international conference on Machine learning},\n title = {Support vector machine learning for interdependent and structured output spaces},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b430afae080f51c925da240aef5c2ec65c9ab2ae",
            "@type": "ScholarlyArticle",
            "paperId": "b430afae080f51c925da240aef5c2ec65c9ab2ae",
            "corpusId": 44769024,
            "url": "https://www.semanticscholar.org/paper/b430afae080f51c925da240aef5c2ec65c9ab2ae",
            "title": "Entanglement-based machine learning on a quantum computer.",
            "venue": "Physical Review Letters",
            "publicationVenue": {
                "id": "urn:research:16c9f9d4-bee1-435d-8c85-22a3deba109d",
                "name": "Physical Review Letters",
                "alternate_names": [
                    "Phys Rev Lett"
                ],
                "issn": "0031-9007",
                "url": "https://journals.aps.org/prl/"
            },
            "year": 2014,
            "externalIds": {
                "ArXiv": "1409.7770",
                "MAG": "2069563009",
                "DOI": "10.1103/PhysRevLett.114.110504",
                "CorpusId": 44769024,
                "PubMed": "25839250"
            },
            "abstract": "Machine learning, a branch of artificial intelligence, learns from previous experience to optimize performance, which is ubiquitous in various fields such as computer sciences, financial analysis, robotics, and bioinformatics. A challenge is that machine learning with the rapidly growing \"big data\" could become intractable for classical computers. Recently, quantum machine learning algorithms [Lloyd, Mohseni, and Rebentrost, arXiv.1307.0411] were proposed which could offer an exponential speedup over classical algorithms. Here, we report the first experimental entanglement-based classification of two-, four-, and eight-dimensional vectors to different clusters using a small-scale photonic quantum computer, which are then used to implement supervised and unsupervised machine learning. The results demonstrate the working principle of using quantum computers to manipulate and classify high-dimensional vectors, the core mathematical routine in machine learning. The method can, in principle, be scaled to larger numbers of qubits, and may provide a new route to accelerate machine learning.",
            "referenceCount": 34,
            "citationCount": 163,
            "influentialCitationCount": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1409.7770",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Physics",
                "Medicine",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Physics",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2014-09-27",
            "journal": {
                "name": "Physical review letters",
                "volume": "114 11"
            },
            "citationStyles": {
                "bibtex": "@Article{Cai2014EntanglementbasedML,\n author = {Xin-Dong Cai and Dian Wu and Z. Su and Ming-Cheng Chen and Xi-Lin Wang and Li Li and Nai-Le Liu and Chaoyang Lu and J. Pan},\n booktitle = {Physical Review Letters},\n journal = {Physical review letters},\n pages = {\n          110504\n        },\n title = {Entanglement-based machine learning on a quantum computer.},\n volume = {114 11},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ed875931b55fd413bff6e04ab58d594647065eac",
            "@type": "ScholarlyArticle",
            "paperId": "ed875931b55fd413bff6e04ab58d594647065eac",
            "corpusId": 24512455,
            "url": "https://www.semanticscholar.org/paper/ed875931b55fd413bff6e04ab58d594647065eac",
            "title": "Encyclopedia of Machine Learning",
            "venue": "Encyclopedia of Machine Learning",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2011,
            "externalIds": {
                "DBLP": "reference/ml/2010",
                "MAG": "1680797894",
                "DOI": "10.1007/978-0-387-30164-8",
                "CorpusId": 24512455
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 1262,
            "influentialCitationCount": 47,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/bfm:978-0-387-30164-8/1?pdf=chapter%20toc",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "2011-03-28",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Sammut2011EncyclopediaOM,\n author = {C. Sammut and Geoffrey I. Webb},\n booktitle = {Encyclopedia of Machine Learning},\n title = {Encyclopedia of Machine Learning},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:92a314bf9ae817836389cc97af5a9f42c561a772",
            "@type": "ScholarlyArticle",
            "paperId": "92a314bf9ae817836389cc97af5a9f42c561a772",
            "corpusId": 6409443,
            "url": "https://www.semanticscholar.org/paper/92a314bf9ae817836389cc97af5a9f42c561a772",
            "title": "Accelerating materials property predictions using machine learning",
            "venue": "Scientific Reports",
            "publicationVenue": {
                "id": "urn:research:f99f77b7-b1b6-44d3-984a-f288e9884b9b",
                "name": "Scientific Reports",
                "alternate_names": [
                    "Sci Rep"
                ],
                "issn": "2045-2322",
                "url": "http://www.nature.com/srep/"
            },
            "year": 2013,
            "externalIds": {
                "MAG": "2074616700",
                "PubMedCentral": "3786293",
                "DOI": "10.1038/srep02810",
                "CorpusId": 6409443,
                "PubMed": "24077117"
            },
            "abstract": null,
            "referenceCount": 46,
            "citationCount": 568,
            "influentialCitationCount": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.nature.com/articles/srep02810.pdf",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Materials Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2013-09-30",
            "journal": {
                "name": "Scientific Reports",
                "volume": "3"
            },
            "citationStyles": {
                "bibtex": "@Article{Pilania2013AcceleratingMP,\n author = {G. Pilania and Chenchen Wang and Xun Jiang and S. Rajasekaran and R. Ramprasad},\n booktitle = {Scientific Reports},\n journal = {Scientific Reports},\n title = {Accelerating materials property predictions using machine learning},\n volume = {3},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a43e61f7900c64e7f3bf18ef7ff5770295149b37",
            "@type": "ScholarlyArticle",
            "paperId": "a43e61f7900c64e7f3bf18ef7ff5770295149b37",
            "corpusId": 61053364,
            "url": "https://www.semanticscholar.org/paper/a43e61f7900c64e7f3bf18ef7ff5770295149b37",
            "title": "Machine Learning, a Probabilistic Perspective",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2014,
            "externalIds": {
                "MAG": "1887369574",
                "DOI": "10.1080/09332480.2014.914768",
                "CorpusId": 61053364
            },
            "abstract": "ISBN-13: 978-0262018029 The book also introduces the notion of a Bayesian likelihood function (p.228), which \u201cdiffers slightly from that in classical statistics.\u201d The only difference I can spot is in the interpretation: Both functions of (\u03b8, x) are numerically the same. Overall, the chapter on Bayesian inference does not spend much time on prior specification. There is a section on conjugate priors that does not mention picking the hyperparameters. While improper priors are introduced as limits of proper priors and as conveying \u201cthe least amount of information about [the parameters]\u201d (p.236), the difficulty in using improper priors for hypothesis testing is not mentioned. Both Chib\u2019s method and the Savage-Dickey density ratio are suggested for the approximation of marginal likelihoods.",
            "referenceCount": 0,
            "citationCount": 248,
            "influentialCitationCount": 12,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2014-04-03",
            "journal": {
                "name": "CHANCE",
                "volume": "27"
            },
            "citationStyles": {
                "bibtex": "@Article{Robert2014MachineLA,\n author = {C. Robert},\n journal = {CHANCE},\n pages = {62 - 63},\n title = {Machine Learning, a Probabilistic Perspective},\n volume = {27},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a749fccbcfc0382f26789ce0b9a03fa98b9e608c",
            "@type": "ScholarlyArticle",
            "paperId": "a749fccbcfc0382f26789ce0b9a03fa98b9e608c",
            "corpusId": 43003852,
            "url": "https://www.semanticscholar.org/paper/a749fccbcfc0382f26789ce0b9a03fa98b9e608c",
            "title": "Programs for Machine Learning. Part I",
            "venue": "Information and Control",
            "publicationVenue": {
                "id": "urn:research:feadd011-44f6-4649-bc71-8c9a604eee42",
                "name": "Information and Control",
                "alternate_names": [
                    "Inf Control"
                ],
                "issn": "0019-9958",
                "url": "https://www.sciencedirect.com/journal/information-and-control"
            },
            "year": 1962,
            "externalIds": {
                "MAG": "2055815280",
                "DBLP": "journals/iandc/Hormann62",
                "DOI": "10.1016/S0019-9958(62)90649-6",
                "CorpusId": 43003852
            },
            "abstract": null,
            "referenceCount": 7,
            "citationCount": 2474,
            "influentialCitationCount": 201,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1962-12-01",
            "journal": {
                "name": "Inf. Control.",
                "volume": "5"
            },
            "citationStyles": {
                "bibtex": "@Article{Hormann1962ProgramsFM,\n author = {A. Hormann},\n booktitle = {Information and Control},\n journal = {Inf. Control.},\n pages = {347-367},\n title = {Programs for Machine Learning. Part I},\n volume = {5},\n year = {1962}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:61017ac0c108f68c987a1fe3e4c2b6223ddd6f31",
            "@type": "ScholarlyArticle",
            "paperId": "61017ac0c108f68c987a1fe3e4c2b6223ddd6f31",
            "corpusId": 9431428,
            "url": "https://www.semanticscholar.org/paper/61017ac0c108f68c987a1fe3e4c2b6223ddd6f31",
            "title": "Machine learning classifiers and fMRI: A tutorial overview",
            "venue": "NeuroImage",
            "publicationVenue": {
                "id": "urn:research:fd4c7628-c16e-4b50-8555-3ac3ad6da2d7",
                "name": "NeuroImage",
                "alternate_names": null,
                "issn": "1053-8119",
                "url": "http://www.elsevier.com/locate/ynimg"
            },
            "year": 2009,
            "externalIds": {
                "MAG": "2158485497",
                "DBLP": "journals/neuroimage/PereiraMB09",
                "DOI": "10.1016/j.neuroimage.2008.11.007",
                "CorpusId": 9431428,
                "PubMed": "19070668"
            },
            "abstract": null,
            "referenceCount": 49,
            "citationCount": 1598,
            "influentialCitationCount": 89,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://europepmc.org/articles/pmc2892746?pdf=render",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2009-03-01",
            "journal": {
                "name": "NeuroImage",
                "volume": "45"
            },
            "citationStyles": {
                "bibtex": "@Article{Pereira2009MachineLC,\n author = {Francisco Pereira and Tom Michael Mitchell and M. Botvinick},\n booktitle = {NeuroImage},\n journal = {NeuroImage},\n pages = {S199-S209},\n title = {Machine learning classifiers and fMRI: A tutorial overview},\n volume = {45},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:07912741c6c96e6ad5b2c2d6c6c3b2de5c8a271b",
            "@type": "ScholarlyArticle",
            "paperId": "07912741c6c96e6ad5b2c2d6c6c3b2de5c8a271b",
            "corpusId": 209202606,
            "url": "https://www.semanticscholar.org/paper/07912741c6c96e6ad5b2c2d6c6c3b2de5c8a271b",
            "title": "Advances and Open Problems in Federated Learning",
            "venue": "Found. Trends Mach. Learn.",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2019,
            "externalIds": {
                "MAG": "3111681398",
                "DBLP": "journals/corr/abs-1912-04977",
                "ArXiv": "1912.04977",
                "DOI": "10.1561/2200000083",
                "CorpusId": 209202606
            },
            "abstract": "Federated learning (FL) is a machine learning setting where many clients (e.g. mobile devices or whole organizations) collaboratively train a model under the orchestration of a central server (e.g. service provider), while keeping the training data decentralized. FL embodies the principles of focused data collection and minimization, and can mitigate many of the systemic privacy risks and costs resulting from traditional, centralized machine learning and data science approaches. Motivated by the explosive growth in FL research, this paper discusses recent advances and presents an extensive collection of open problems and challenges.",
            "referenceCount": 517,
            "citationCount": 3793,
            "influentialCitationCount": 321,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://wrap.warwick.ac.uk/170168/1/WRAP-Advances-and-open-problems-in-federated-learning-Cormode-2022.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-12-10",
            "journal": {
                "name": "Found. Trends Mach. Learn.",
                "volume": "14"
            },
            "citationStyles": {
                "bibtex": "@Article{Kairouz2019AdvancesAO,\n author = {P. Kairouz and H. B. McMahan and Brendan Avent and A. Bellet and M. Bennis and A. Bhagoji and Keith Bonawitz and Zachary B. Charles and Graham Cormode and Rachel Cummings and Rafael G. L. D'Oliveira and S. Rouayheb and David Evans and Josh Gardner and Zachary Garrett and Adri\u00e0 Gasc\u00f3n and Badih Ghazi and Phillip B. Gibbons and M. Gruteser and Za\u00efd Harchaoui and Chaoyang He and Lie He and Zhouyuan Huo and Ben Hutchinson and Justin Hsu and Martin Jaggi and T. Javidi and Gauri Joshi and M. Khodak and Jakub Konecn\u00fd and A. Korolova and F. Koushanfar and Oluwasanmi Koyejo and Tancr\u00e8de Lepoint and Yang Liu and Prateek Mittal and M. Mohri and R. Nock and A. \u00d6zg\u00fcr and R. Pagh and Mariana Raykova and Hang Qi and Daniel Ramage and R. Raskar and D. Song and Weikang Song and S. Stich and Ziteng Sun and A. Suresh and Florian Tram\u00e8r and Praneeth Vepakomma and Jianyu Wang and Li Xiong and Zheng Xu and Qiang Yang and Felix X. Yu and Han Yu and Sen Zhao},\n booktitle = {Found. Trends Mach. Learn.},\n journal = {Found. Trends Mach. Learn.},\n pages = {1-210},\n title = {Advances and Open Problems in Federated Learning},\n volume = {14},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7cda32aeefdd3cabd76871b8ee06bd1a1ea2ba10",
            "@type": "ScholarlyArticle",
            "paperId": "7cda32aeefdd3cabd76871b8ee06bd1a1ea2ba10",
            "corpusId": 13109232,
            "url": "https://www.semanticscholar.org/paper/7cda32aeefdd3cabd76871b8ee06bd1a1ea2ba10",
            "title": "Revisiting the Nystrom Method for Improved Large-scale Machine Learning",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2013,
            "externalIds": {
                "DBLP": "journals/jmlr/GittensM16",
                "MAG": "2949526110",
                "ArXiv": "1303.1849",
                "CorpusId": 13109232
            },
            "abstract": "We reconsider randomized algorithms for the low-rank approximation of SPSD matrices such as Laplacian and kernel matrices that arise in data analysis and machine learning applications. Our main results consist of an empirical evaluation of the performance quality and running time of sampling and projection methods on a diverse suite of SPSD matrices. Our results highlight complementary aspects of sampling versus projection methods, and they point to differences between uniform and nonuniform sampling methods based on leverage scores. We complement our empirical results with a suite of worst-case theoretical bounds for both random sampling and random projection methods. These bounds are qualitatively superior to existing bounds--e.g., improved additive-error bounds for spectral and Frobenius norm error and relative-error bounds for trace norm error.",
            "referenceCount": 75,
            "citationCount": 374,
            "influentialCitationCount": 48,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2013-03-07",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "17"
            },
            "citationStyles": {
                "bibtex": "@Article{Gittens2013RevisitingTN,\n author = {Alex Gittens and Michael W. Mahoney},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {117:1-117:65},\n title = {Revisiting the Nystrom Method for Improved Large-scale Machine Learning},\n volume = {17},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7da323e7103245eeaed32367c46abe3f4913df86",
            "@type": "ScholarlyArticle",
            "paperId": "7da323e7103245eeaed32367c46abe3f4913df86",
            "corpusId": 13172512,
            "url": "https://www.semanticscholar.org/paper/7da323e7103245eeaed32367c46abe3f4913df86",
            "title": "A survey of techniques for internet traffic classification using machine learning",
            "venue": "IEEE Communications Surveys and Tutorials",
            "publicationVenue": {
                "id": "urn:research:95d0dda7-5d58-4afd-b59f-315447b81992",
                "name": "IEEE Communications Surveys and Tutorials",
                "alternate_names": [
                    "IEEE Commun Surv Tutor"
                ],
                "issn": "1553-877X",
                "url": "http://www.comsoc.org/cst"
            },
            "year": 2008,
            "externalIds": {
                "MAG": "2096118443",
                "DBLP": "journals/comsur/NguyenA08",
                "DOI": "10.1109/SURV.2008.080406",
                "CorpusId": 13172512
            },
            "abstract": "The research community has begun looking for IP traffic classification techniques that do not rely on `well known\u00bf TCP or UDP port numbers, or interpreting the contents of packet payloads. New work is emerging on the use of statistical traffic characteristics to assist in the identification and classification process. This survey paper looks at emerging research into the application of Machine Learning (ML) techniques to IP traffic classification - an inter-disciplinary blend of IP networking and data mining techniques. We provide context and motivation for the application of ML techniques to IP traffic classification, and review 18 significant works that cover the dominant period from 2004 to early 2007. These works are categorized and reviewed according to their choice of ML strategies and primary contributions to the literature. We also discuss a number of key requirements for the employment of ML-based traffic classifiers in operational IP networks, and qualitatively critique the extent to which the reviewed works meet these requirements. Open issues and challenges in the field are also discussed.",
            "referenceCount": 60,
            "citationCount": 1561,
            "influentialCitationCount": 82,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://researchbank.swinburne.edu.au/file/4dd80a73-aaee-4a78-929d-b42124b30192/1/PDF (Published version).pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2008-10-01",
            "journal": {
                "name": "IEEE Communications Surveys & Tutorials",
                "volume": "10"
            },
            "citationStyles": {
                "bibtex": "@Article{Nguyen2008ASO,\n author = {Thuy T. T. Nguyen and G. Armitage},\n booktitle = {IEEE Communications Surveys and Tutorials},\n journal = {IEEE Communications Surveys & Tutorials},\n pages = {56-76},\n title = {A survey of techniques for internet traffic classification using machine learning},\n volume = {10},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:06a81f63fc4ccfcf02934647a7c17454b91853b0",
            "@type": "ScholarlyArticle",
            "paperId": "06a81f63fc4ccfcf02934647a7c17454b91853b0",
            "corpusId": 53935745,
            "url": "https://www.semanticscholar.org/paper/06a81f63fc4ccfcf02934647a7c17454b91853b0",
            "title": "Machine Learning - The Art and Science of Algorithms that Make Sense of Data",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2012,
            "externalIds": {
                "MAG": "1526441817",
                "DBLP": "books/daglib/0045948",
                "CorpusId": 53935745
            },
            "abstract": "As one of the most comprehensive machine learning texts around, this book does justice to the field's incredible richness, but without losing sight of the unifying principles. Peter Flach's clear, example-based approach begins by discussing how a spam filter works, which gives an immediate introduction to machine learning in action, with a minimum of technical fuss. Flach provides case studies of increasing complexity and variety with well-chosen examples and illustrations throughout. He covers a wide range of logical, geometric and statistical models and state-of-the-art topics such as matrix factorisation and ROC analysis. Particular attention is paid to the central role played by features. The use of established terminology is balanced with the introduction of new and useful concepts, and summaries of relevant background material are provided with pointers for revision if necessary. These features ensure Machine Learning will set a new standard as an introductory textbook.",
            "referenceCount": 178,
            "citationCount": 855,
            "influentialCitationCount": 80,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2012-09-20",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Flach2012MachineL,\n author = {P. Flach},\n pages = {I-XVII, 1-396},\n title = {Machine Learning - The Art and Science of Algorithms that Make Sense of Data},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d37fc9e9c4fedc32865b08661e7fb950df1f8fbe",
            "@type": "ScholarlyArticle",
            "paperId": "d37fc9e9c4fedc32865b08661e7fb950df1f8fbe",
            "corpusId": 9989562,
            "url": "https://www.semanticscholar.org/paper/d37fc9e9c4fedc32865b08661e7fb950df1f8fbe",
            "title": "Kernel methods in machine learning",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2007,
            "externalIds": {
                "MAG": "2168029744",
                "ArXiv": "math/0701907",
                "DOI": "10.1214/009053607000000677",
                "CorpusId": 9989562
            },
            "abstract": "We review machine learning methods employing positive definite kernels. These methods formulate learning and estimation problems in a reproducing kernel Hilbert space (RKHS) of functions defined on the data domain, expanded in terms of a kernel. Working in linear spaces of function has the benefit of facilitating the construction and analysis of learning algorithms while at the same time allowing large classes of functions. The latter include nonlinear functions as well as functions defined on nonvectorial data. We cover a wide range of methods, ranging from binary classifiers to sophisticated methods for estimation with structured data.",
            "referenceCount": 160,
            "citationCount": 1677,
            "influentialCitationCount": 72,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://projecteuclid.org/journals/annals-of-statistics/volume-36/issue-3/Kernel-methods-in-machine-learning/10.1214/009053607000000677.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "2007-01-31",
            "journal": {
                "name": "Annals of Statistics",
                "volume": "36"
            },
            "citationStyles": {
                "bibtex": "@Article{Hofmann2007KernelMI,\n author = {Thomas Hofmann and B. Scholkopf and Alex Smola},\n journal = {Annals of Statistics},\n pages = {1171-1220},\n title = {Kernel methods in machine learning},\n volume = {36},\n year = {2007}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0d39cc42f2186dac99898121bbc8bb2b965ba93d",
            "@type": "ScholarlyArticle",
            "paperId": "0d39cc42f2186dac99898121bbc8bb2b965ba93d",
            "corpusId": 11201620,
            "url": "https://www.semanticscholar.org/paper/0d39cc42f2186dac99898121bbc8bb2b965ba93d",
            "title": "Assessment and Validation of Machine Learning Methods for Predicting Molecular Atomization Energies.",
            "venue": "Journal of Chemical Theory and Computation",
            "publicationVenue": {
                "id": "urn:research:05f4640f-1261-4186-8153-0cb50b1169b3",
                "name": "Journal of Chemical Theory and Computation",
                "alternate_names": [
                    "J Chem Theory Comput"
                ],
                "issn": "1549-9618",
                "url": "http://pubs.acs.org/jctc"
            },
            "year": 2013,
            "externalIds": {
                "MAG": "1998260904",
                "DOI": "10.1021/ct400195d",
                "CorpusId": 11201620,
                "PubMed": "26584096"
            },
            "abstract": "The accurate and reliable prediction of properties of molecules typically requires computationally intensive quantum-chemical calculations. Recently, machine learning techniques applied to ab initio calculations have been proposed as an efficient approach for describing the energies of molecules in their given ground-state structure throughout chemical compound space (Rupp et al. Phys. Rev. Lett. 2012, 108, 058301). In this paper we outline a number of established machine learning techniques and investigate the influence of the molecular representation on the methods performance. The best methods achieve prediction errors of 3 kcal/mol for the atomization energies of a wide variety of molecules. Rationales for this performance improvement are given together with pitfalls and challenges when applying machine learning approaches to the prediction of quantum-mechanical observables.",
            "referenceCount": 79,
            "citationCount": 496,
            "influentialCitationCount": 4,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Chemistry",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2013-07-30",
            "journal": {
                "name": "Journal of chemical theory and computation",
                "volume": "9 8"
            },
            "citationStyles": {
                "bibtex": "@Article{Hansen2013AssessmentAV,\n author = {K. Hansen and G. Montavon and Franziska Biegler and S. Fazli and M. Rupp and M. Scheffler and O. A. von Lilienfeld and A. Tkatchenko and K. M\u00fcller},\n booktitle = {Journal of Chemical Theory and Computation},\n journal = {Journal of chemical theory and computation},\n pages = {\n          3404-19\n        },\n title = {Assessment and Validation of Machine Learning Methods for Predicting Molecular Atomization Energies.},\n volume = {9 8},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:bb99668d4df98a3f6ff0b9fa3402e09008f22e2c",
            "@type": "ScholarlyArticle",
            "paperId": "bb99668d4df98a3f6ff0b9fa3402e09008f22e2c",
            "corpusId": 60502770,
            "url": "https://www.semanticscholar.org/paper/bb99668d4df98a3f6ff0b9fa3402e09008f22e2c",
            "title": "Making large-scale support vector machine learning practical",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1999,
            "externalIds": {
                "MAG": "1574862351",
                "CorpusId": 60502770
            },
            "abstract": "Training a support vector machine (SVM) leads to a quadratic optimization problem with bound constraints and one linear equality constraint. Despite the fact that this type of problem is well understood, there are many issues to be considered in designing an SVM learner. In particular, for large learning tasks with many training examples, oo-the-shelf optimization techniques for general quadratic programs quickly become intractable in their memory and time requirements. SV M light1 is an implementation of an SVM learner which addresses the problem of large tasks. This chapter presents algorithmic and computational results developed for SV M light V2.0, which make large-scale SVM training more practical. The results give guidelines for the application of SVMs to large domains.",
            "referenceCount": 12,
            "citationCount": 1869,
            "influentialCitationCount": 182,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1999-02-08",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Joachims1999MakingLS,\n author = {T. Joachims},\n pages = {169-184},\n title = {Making large-scale support vector machine learning practical},\n year = {1999}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5776d0fea69d826519ee3649f620e8755a490efe",
            "@type": "ScholarlyArticle",
            "paperId": "5776d0fea69d826519ee3649f620e8755a490efe",
            "corpusId": 18548239,
            "url": "https://www.semanticscholar.org/paper/5776d0fea69d826519ee3649f620e8755a490efe",
            "title": "Lifelong Machine Learning Systems: Beyond Learning Algorithms",
            "venue": "AAAI Spring Symposium: Lifelong Machine Learning",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2013,
            "externalIds": {
                "MAG": "2120501001",
                "DBLP": "conf/aaaiss/SilverYL13",
                "CorpusId": 18548239
            },
            "abstract": "Lifelong Machine Learning, or LML, considers systems that can learn many tasks from one or more domains over its lifetime. The goal is to sequentially retain learned knowledge and to selectively transfer that knowledge when learning a new task so as to develop more accurate hypotheses or policies. Following a review of prior work on LML, we propose that it is now appropriate for the AI community to move beyond learning algorithms to more seriously consider the nature of systems that are capable of learning over a lifetime. Reasons for our position are presented and potential counter-arguments are discussed. The remainder of the paper contributes by defining LML, presenting a reference framework that considers all forms of machine learning, and listing several key challenges for and benefits from LML research. We conclude with ideas for next steps to advance the field.",
            "referenceCount": 36,
            "citationCount": 354,
            "influentialCitationCount": 19,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Philosophy",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2013-03-15",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Article{Silver2013LifelongML,\n author = {D. Silver and Qiang Yang and Lianghao Li},\n booktitle = {AAAI Spring Symposium: Lifelong Machine Learning},\n pages = {49},\n title = {Lifelong Machine Learning Systems: Beyond Learning Algorithms},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:01e1fa7924b3eb76b73f1828c93805f3ba028bae",
            "@type": "ScholarlyArticle",
            "paperId": "01e1fa7924b3eb76b73f1828c93805f3ba028bae",
            "corpusId": 9300748,
            "url": "https://www.semanticscholar.org/paper/01e1fa7924b3eb76b73f1828c93805f3ba028bae",
            "title": "MLbase: A Distributed Machine-learning System",
            "venue": "Conference on Innovative Data Systems Research",
            "publicationVenue": {
                "id": "urn:research:528ced1f-e949-4e1a-8fee-2ffbf0be551d",
                "name": "Conference on Innovative Data Systems Research",
                "alternate_names": [
                    "CIDR",
                    "Conf Innov Data Syst Res"
                ],
                "issn": null,
                "url": "http://cidrdb.org/"
            },
            "year": 2013,
            "externalIds": {
                "MAG": "2184623761",
                "DBLP": "conf/cidr/KraskaTDGFJ13",
                "CorpusId": 9300748
            },
            "abstract": "Machine learning (ML) and statistical techniques are key to transforming big data into actionable knowledge. In spite of the modern primacy of data, the complexity of existing ML algorithms is often overwhelming|many users do not understand the trade-os and challenges of parameterizing and choosing between dierent learning techniques. Furthermore, existing scalable systems that support machine learning are typically not accessible to ML researchers without a strong background in distributed systems and low-level primitives. In this work, we present our vision for MLbase, a novel system harnessing the power of machine learning for both end-users and ML researchers. MLbase provides (1) a simple declarative way to specify ML tasks, (2) a novel optimizer to select and dynamically adapt the choice of learning algorithm, (3) a set of high-level operators to enable ML researchers to scalably implement a wide range of ML methods without deep systems knowledge, and (4) a new run-time optimized for the data-access patterns of these high-level operators.",
            "referenceCount": 26,
            "citationCount": 354,
            "influentialCitationCount": 18,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Article{Kraska2013MLbaseAD,\n author = {Tim Kraska and Ameet Talwalkar and John C. Duchi and Rean Griffith and M. Franklin and Michael I. Jordan},\n booktitle = {Conference on Innovative Data Systems Research},\n title = {MLbase: A Distributed Machine-learning System},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:bc745811e231d1b4e37d2c56cbd2d67e37ba9032",
            "@type": "ScholarlyArticle",
            "paperId": "bc745811e231d1b4e37d2c56cbd2d67e37ba9032",
            "corpusId": 16585863,
            "url": "https://www.semanticscholar.org/paper/bc745811e231d1b4e37d2c56cbd2d67e37ba9032",
            "title": "Machine Learning Paradigms for Speech Recognition: An Overview",
            "venue": "IEEE Transactions on Audio, Speech, and Language Processing",
            "publicationVenue": {
                "id": "urn:research:96b92082-eb93-4682-be66-0a8fa5f2511c",
                "name": "IEEE Transactions on Audio, Speech, and Language Processing",
                "alternate_names": [
                    "IEEE Trans Audio Speech Lang Process"
                ],
                "issn": "1558-7916",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=10376"
            },
            "year": 2013,
            "externalIds": {
                "MAG": "2161742217",
                "DBLP": "journals/taslp/DengL13",
                "DOI": "10.1109/TASL.2013.2244083",
                "CorpusId": 16585863
            },
            "abstract": "Automatic Speech Recognition (ASR) has historically been a driving force behind many machine learning (ML) techniques, including the ubiquitously used hidden Markov model, discriminative learning, structured sequence learning, Bayesian learning, and adaptive learning. Moreover, ML can and occasionally does use ASR as a large-scale, realistic application to rigorously test the effectiveness of a given technique, and to inspire new problems arising from the inherently sequential and dynamic nature of speech. On the other hand, even though ASR is available commercially for some applications, it is largely an unsolved problem - for almost all applications, the performance of ASR is not on par with human performance. New insight from modern ML methodology shows great promise to advance the state-of-the-art in ASR technology. This overview article provides readers with an overview of modern ML techniques as utilized in the current and as relevant to future ASR research and systems. The intent is to foster further cross-pollination between the ML and ASR communities than has occurred in the past. The article is organized according to the major ML paradigms that are either popular already or have potential for making significant contributions to ASR technology. The paradigms presented and elaborated in this overview include: generative and discriminative learning; supervised, unsupervised, semi-supervised, and active learning; adaptive and multi-task learning; and Bayesian learning. These learning paradigms are motivated and discussed in the context of ASR technology and applications. We finally present and analyze recent developments of deep learning and learning with sparse representations, focusing on their direct relevance to advancing ASR technology.",
            "referenceCount": 280,
            "citationCount": 349,
            "influentialCitationCount": 15,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2013-05-01",
            "journal": {
                "name": "IEEE Transactions on Audio, Speech, and Language Processing",
                "volume": "21"
            },
            "citationStyles": {
                "bibtex": "@Article{Deng2013MachineLP,\n author = {L. Deng and Xiao Li},\n booktitle = {IEEE Transactions on Audio, Speech, and Language Processing},\n journal = {IEEE Transactions on Audio, Speech, and Language Processing},\n pages = {1060-1089},\n title = {Machine Learning Paradigms for Speech Recognition: An Overview},\n volume = {21},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:73b51b02a061e2eae2eebe2ceae45872ea7d509d",
            "@type": "ScholarlyArticle",
            "paperId": "73b51b02a061e2eae2eebe2ceae45872ea7d509d",
            "corpusId": 16279144,
            "url": "https://www.semanticscholar.org/paper/73b51b02a061e2eae2eebe2ceae45872ea7d509d",
            "title": "A Survey on Machine-Learning Techniques in Cognitive Radios",
            "venue": "IEEE Communications Surveys and Tutorials",
            "publicationVenue": {
                "id": "urn:research:95d0dda7-5d58-4afd-b59f-315447b81992",
                "name": "IEEE Communications Surveys and Tutorials",
                "alternate_names": [
                    "IEEE Commun Surv Tutor"
                ],
                "issn": "1553-877X",
                "url": "http://www.comsoc.org/cst"
            },
            "year": 2013,
            "externalIds": {
                "MAG": "2091005538",
                "DBLP": "journals/comsur/BkassinyLJ13",
                "DOI": "10.1109/SURV.2012.100412.00017",
                "CorpusId": 16279144
            },
            "abstract": "In this survey paper, we characterize the learning problem in cognitive radios (CRs) and state the importance of artificial intelligence in achieving real cognitive communications systems. We review various learning problems that have been studied in the context of CRs classifying them under two main categories: Decision-making and feature classification. Decision-making is responsible for determining policies and decision rules for CRs while feature classification permits identifying and classifying different observation models. The learning algorithms encountered are categorized as either supervised or unsupervised algorithms. We describe in detail several challenging learning issues that arise in cognitive radio networks (CRNs), in particular in non-Markovian environments and decentralized networks, and present possible solution methods to address them. We discuss similarities and differences among the presented algorithms and identify the conditions under which each of the techniques may be applied.",
            "referenceCount": 203,
            "citationCount": 489,
            "influentialCitationCount": 14,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2013-01-23",
            "journal": {
                "name": "IEEE Communications Surveys & Tutorials",
                "volume": "15"
            },
            "citationStyles": {
                "bibtex": "@Article{Bkassiny2013ASO,\n author = {Mario Bkassiny and Yang Li and S. Jayaweera},\n booktitle = {IEEE Communications Surveys and Tutorials},\n journal = {IEEE Communications Surveys & Tutorials},\n pages = {1136-1159},\n title = {A Survey on Machine-Learning Techniques in Cognitive Radios},\n volume = {15},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:260077710ef86047c582bbe505feca36962ca406",
            "@type": "ScholarlyArticle",
            "paperId": "260077710ef86047c582bbe505feca36962ca406",
            "corpusId": 3188444,
            "url": "https://www.semanticscholar.org/paper/260077710ef86047c582bbe505feca36962ca406",
            "title": "Distributed GraphLab: A Framework for Machine Learning in the Cloud",
            "venue": "Very Large Data Bases Conference",
            "publicationVenue": {
                "id": "urn:research:a5c58053-0673-4cdb-b2b8-b6b0ad6911d1",
                "name": "Very Large Data Bases Conference",
                "alternate_names": [
                    "Very Large Data Bases",
                    "Very Large Data Base",
                    "VLDB",
                    "Very Large Data Base Conf"
                ],
                "issn": null,
                "url": "https://www.vldb.org/conference.html"
            },
            "year": 2012,
            "externalIds": {
                "DBLP": "journals/corr/abs-1204-6078",
                "MAG": "2950570846",
                "CorpusId": 3188444
            },
            "abstract": "While high-level data parallel frameworks, like MapReduce, simplify the design and implementation of large-scale data processing systems, they do not naturally or efficiently support many important data mining and machine learning algorithms and can lead to inefficient learning systems. To help fill this critical void, we introduced the GraphLab abstraction which naturally expresses asynchronous, dynamic, graph-parallel computation while ensuring data consistency and achieving a high degree of parallel performance in the shared-memory setting. In this paper, we extend the GraphLab framework to the substantially more challenging distributed setting while preserving strong data consistency guarantees. We develop graph based extensions to pipelined locking and data versioning to reduce network congestion and mitigate the effect of network latency. We also introduce fault tolerance to the GraphLab abstraction using the classic Chandy-Lamport snapshot algorithm and demonstrate how it can be easily implemented by exploiting the GraphLab abstraction itself. Finally, we evaluate our distributed implementation of the GraphLab abstraction on a large Amazon EC2 deployment and show 1-2 orders of magnitude performance gains over Hadoop-based implementations.",
            "referenceCount": 33,
            "citationCount": 790,
            "influentialCitationCount": 52,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2012-04-26",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1204.6078"
            },
            "citationStyles": {
                "bibtex": "@Article{Low2012DistributedGA,\n author = {Yucheng Low and Joseph E. Gonzalez and Aapo Kyrola and Danny Bickson and Carlos Guestrin and J. Hellerstein},\n booktitle = {Very Large Data Bases Conference},\n journal = {ArXiv},\n title = {Distributed GraphLab: A Framework for Machine Learning in the Cloud},\n volume = {abs/1204.6078},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f04df4e20a18358ea2f689b4c129781628ef7fc1",
            "@type": "ScholarlyArticle",
            "paperId": "f04df4e20a18358ea2f689b4c129781628ef7fc1",
            "corpusId": 14604520,
            "url": "https://www.semanticscholar.org/paper/f04df4e20a18358ea2f689b4c129781628ef7fc1",
            "title": "A large annotated corpus for learning natural language inference",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "publicationVenue": {
                "id": "urn:research:41bf9ed3-85b3-4c90-b015-150e31690253",
                "name": "Conference on Empirical Methods in Natural Language Processing",
                "alternate_names": [
                    "Empir Method Nat Lang Process",
                    "Empirical Methods in Natural Language Processing",
                    "Conf Empir Method Nat Lang Process",
                    "EMNLP"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/emnlp"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "2953084091",
                "DBLP": "conf/emnlp/BowmanAPM15",
                "ACL": "D15-1075",
                "ArXiv": "1508.05326",
                "DOI": "10.18653/v1/D15-1075",
                "CorpusId": 14604520
            },
            "abstract": "Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by the lack of large-scale resources. To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning. At 570K pairs, it is two orders of magnitude larger than all other resources of its type. This increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time.",
            "referenceCount": 40,
            "citationCount": 3531,
            "influentialCitationCount": 912,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.18653/v1/d15-1075",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-08-21",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Bowman2015ALA,\n author = {Samuel R. Bowman and Gabor Angeli and Christopher Potts and Christopher D. Manning},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {632-642},\n title = {A large annotated corpus for learning natural language inference},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b90d44f59fcb74c71d3e31f67a3f09efab187a4e",
            "@type": "ScholarlyArticle",
            "paperId": "b90d44f59fcb74c71d3e31f67a3f09efab187a4e",
            "corpusId": 24085261,
            "url": "https://www.semanticscholar.org/paper/b90d44f59fcb74c71d3e31f67a3f09efab187a4e",
            "title": "Machine learning in cell biology \u2013 teaching computers to recognize phenotypes",
            "venue": "Journal of Cell Science",
            "publicationVenue": {
                "id": "urn:research:398b46b8-6014-4478-b7af-f477167f7bde",
                "name": "Journal of Cell Science",
                "alternate_names": [
                    "J Cell Sci"
                ],
                "issn": "0021-9533",
                "url": "https://jcs.biologists.org/"
            },
            "year": 2013,
            "externalIds": {
                "MAG": "2156472837",
                "DOI": "10.1242/jcs.123604",
                "CorpusId": 24085261,
                "PubMed": "24259662"
            },
            "abstract": "Summary Recent advances in microscope automation provide new opportunities for high-throughput cell biology, such as image-based screening. High-complex image analysis tasks often make the implementation of static and predefined processing rules a cumbersome effort. Machine-learning methods, instead, seek to use intrinsic data structure, as well as the expert annotations of biologists to infer models that can be used to solve versatile data analysis tasks. Here, we explain how machine-learning methods work and what needs to be considered for their successful application in cell biology. We outline how microscopy images can be converted into a data representation suitable for machine learning, and then introduce various state-of-the-art machine-learning algorithms, highlighting recent applications in image-based screening. Our Commentary aims to provide the biologist with a guide to the application of machine learning to microscopy assays and we therefore include extensive discussion on how to optimize experimental workflow as well as the data analysis pipeline.",
            "referenceCount": 102,
            "citationCount": 289,
            "influentialCitationCount": 14,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Biology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Biology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2013-12-15",
            "journal": {
                "name": "Journal of Cell Science",
                "volume": "126"
            },
            "citationStyles": {
                "bibtex": "@Article{Sommer2013MachineLI,\n author = {Christoph Sommer and D. Gerlich},\n booktitle = {Journal of Cell Science},\n journal = {Journal of Cell Science},\n pages = {5529 - 5539},\n title = {Machine learning in cell biology \u2013 teaching computers to recognize phenotypes},\n volume = {126},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2f7ad26514bce4df6c8ebc42c90383ef3a974df4",
            "@type": "ScholarlyArticle",
            "paperId": "2f7ad26514bce4df6c8ebc42c90383ef3a974df4",
            "corpusId": 2172854,
            "url": "https://www.semanticscholar.org/paper/2f7ad26514bce4df6c8ebc42c90383ef3a974df4",
            "title": "Pylearn2: a machine learning research library",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2013,
            "externalIds": {
                "DBLP": "journals/corr/GoodfellowWLDMPBBB13",
                "ArXiv": "1308.4214",
                "MAG": "1872489089",
                "CorpusId": 2172854
            },
            "abstract": "Pylearn2 is a machine learning research library. This does not just mean that it is a collection of machine learning algorithms that share a common API; it means that it has been designed for flexibility and extensibility in order to facilitate research projects that involve new or unusual use cases. In this paper we give a brief history of the library, an overview of its basic philosophy, a summary of the library's architecture, and a description of how the Pylearn2 community functions socially.",
            "referenceCount": 55,
            "citationCount": 301,
            "influentialCitationCount": 15,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2013-08-19",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1308.4214"
            },
            "citationStyles": {
                "bibtex": "@Article{Goodfellow2013Pylearn2AM,\n author = {I. Goodfellow and David Warde-Farley and Pascal Lamblin and Vincent Dumoulin and Mehdi Mirza and Razvan Pascanu and J. Bergstra and Fr\u00e9d\u00e9ric Bastien and Yoshua Bengio},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Pylearn2: a machine learning research library},\n volume = {abs/1308.4214},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a19a69cdb137e83ba4b8d5c99d187b9f44bbc2d3",
            "@type": "ScholarlyArticle",
            "paperId": "a19a69cdb137e83ba4b8d5c99d187b9f44bbc2d3",
            "corpusId": 63247680,
            "url": "https://www.semanticscholar.org/paper/a19a69cdb137e83ba4b8d5c99d187b9f44bbc2d3",
            "title": "Learning scikit-learn: Machine Learning in Python",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2013,
            "externalIds": {
                "MAG": "2568556896",
                "CorpusId": 63247680
            },
            "abstract": "Experience the benefits of machine learning techniques by applying them to real-world problems using Python and the open source scikit-learn library Overview Use Python and scikit-learn to create intelligent applications Apply regression techniques to predict future behaviour and learn to cluster items in groups by their similarities Make use of classification techniques to perform image recognition and document classification In Detail Machine learning, the art of creating applications that learn from experience and data, has been around for many years. However, in the era of big data, huge amounts of information is being generated. This makes machine learning an unavoidable source of new data-based approximations for problem solving. With Learning scikit-learn: Machine Learning in Python, you will learn to incorporate machine learning in your applications. The book combines an introduction to some of the main concepts and methods in machine learning with practical, hands-on examples of real-world problems. Ranging from handwritten digit recognition to document classification, examples are solved step by step using Scikit-learn and Python. The book starts with a brief introduction to the core concepts of machine learning with a simple example. Then, using real-world applications and advanced features, it takes a deep dive into the various machine learning techniques. You will learn to evaluate your results and apply advanced techniques for preprocessing data. You will also be able to select the best set of features and the best methods for each problem. With Learning scikit-learn: Machine Learning in Python you will learn how to use the Python programming language and the scikit-learn library to build applications that learn from experience, applying the main concepts and techniques of machine learning. What you will learn from this book Set up scikit-learn inside your Python environment Classify objects (from documents to human faces and flower species) based on some of their features, using a variety of methods from Support Vector Machines to Nave Bayes Use Decision Trees to explain the main causes of certain phenomenon such as the Titanic passengers survival Predict house prices using regression techniques Display and analyse groups in your data using dimensionality reduction Make use of different tools to preprocess, extract, and select the learning features Select the best parameters for your models using model selection Improve the way you build your models using parallelization techniques Approach The book adopts a tutorial-based approach to introduce the user to Scikit-learn. Who this book is written for If you are a programmer who wants to explore machine learning and data-based methods to build intelligent applications and enhance your programming skills, this the book for you. No previous experience with machine-learning algorithms is required.",
            "referenceCount": 0,
            "citationCount": 203,
            "influentialCitationCount": 14,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "2013-11-25",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Garreta2013LearningSM,\n author = {Ral Garreta and Guillermo Moncecchi},\n title = {Learning scikit-learn: Machine Learning in Python},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5e934c4d76b3c664149f7970ec11c3c8cabab91e",
            "@type": "ScholarlyArticle",
            "paperId": "5e934c4d76b3c664149f7970ec11c3c8cabab91e",
            "corpusId": 62385526,
            "url": "https://www.semanticscholar.org/paper/5e934c4d76b3c664149f7970ec11c3c8cabab91e",
            "title": "Machine learning with R : learn how to use R to apply powerful machine learning methods and gain an insight into real-world applications",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2013,
            "externalIds": {
                "MAG": "2239420864",
                "CorpusId": 62385526
            },
            "abstract": "Written as a tutorial to explore and understand the power of R for machine learning. This practical guide that covers all of the need to know topics in a very systematic way. For each machine learning approach, each step in the process is detailed, from preparing the data for analysis to evaluating the results. These steps will build the knowledge you need to apply them to your own data science tasks. Intended for those who want to learn how to use R's machine learning capabilities and gain insight from your data. Perhaps you already know a bit about machine learning, but have never used R; or perhaps you know a little R but are new to machine learning. In either case, this book will get you up and running quickly. It would be helpful to have a bit of familiarity with basic programming concepts, but no prior experience is required.",
            "referenceCount": 0,
            "citationCount": 208,
            "influentialCitationCount": 15,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "2013-10-25",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Lantz2013MachineLW,\n author = {Brett Lantz},\n title = {Machine learning with R : learn how to use R to apply powerful machine learning methods and gain an insight into real-world applications},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:49bdeb07b045dd77f0bfe2b44436608770235a23",
            "@type": "ScholarlyArticle",
            "paperId": "49bdeb07b045dd77f0bfe2b44436608770235a23",
            "corpusId": 201126242,
            "url": "https://www.semanticscholar.org/paper/49bdeb07b045dd77f0bfe2b44436608770235a23",
            "title": "Federated Learning: Challenges, Methods, and Future Directions",
            "venue": "IEEE Signal Processing Magazine",
            "publicationVenue": {
                "id": "urn:research:f62e5eab-173a-4e0a-a963-ed8de9835d22",
                "name": "IEEE Signal Processing Magazine",
                "alternate_names": [
                    "IEEE Signal Process Mag"
                ],
                "issn": "1053-5888",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=79"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2969736367",
                "DBLP": "journals/spm/LiSTS20",
                "ArXiv": "1908.07873",
                "DOI": "10.1109/MSP.2020.2975749",
                "CorpusId": 201126242
            },
            "abstract": "Federated learning involves training statistical models over remote devices or siloed data centers, such as mobile phones or hospitals, while keeping data localized. Training in heterogeneous and potentially massive networks introduces novel challenges that require a fundamental departure from standard approaches for large-scale machine learning, distributed optimization, and privacy-preserving data analysis. In this article, we discuss the unique characteristics and challenges of federated learning, provide a broad overview of current approaches, and outline several directions of future work that are relevant to a wide range of research communities.",
            "referenceCount": 156,
            "citationCount": 2809,
            "influentialCitationCount": 194,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1908.07873",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2019-08-21",
            "journal": {
                "name": "IEEE Signal Processing Magazine",
                "volume": "37"
            },
            "citationStyles": {
                "bibtex": "@Article{Li2019FederatedLC,\n author = {Tian Li and Anit Kumar Sahu and Ameet Talwalkar and Virginia Smith},\n booktitle = {IEEE Signal Processing Magazine},\n journal = {IEEE Signal Processing Magazine},\n pages = {50-60},\n title = {Federated Learning: Challenges, Methods, and Future Directions},\n volume = {37},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5fb8ba2e3967e079c57aa703bf216c168ea8104f",
            "@type": "ScholarlyArticle",
            "paperId": "5fb8ba2e3967e079c57aa703bf216c168ea8104f",
            "corpusId": 2841283,
            "url": "https://www.semanticscholar.org/paper/5fb8ba2e3967e079c57aa703bf216c168ea8104f",
            "title": "Model-based machine learning",
            "venue": "Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2013,
            "externalIds": {
                "PubMedCentral": "3538442",
                "MAG": "2137077888",
                "DOI": "10.1098/rsta.2012.0222",
                "CorpusId": 2841283,
                "PubMed": "23277612"
            },
            "abstract": "Several decades of research in the field of machine learning have resulted in a multitude of different algorithms for solving a broad range of problems. To tackle a new application, a researcher typically tries to map their problem onto one of these existing methods, often influenced by their familiarity with specific algorithms and by the availability of corresponding software implementations. In this study, we describe an alternative methodology for applying machine learning, in which a bespoke solution is formulated for each new application. The solution is expressed through a compact modelling language, and the corresponding custom machine learning code is then generated automatically. This model-based approach offers several major advantages, including the opportunity to create highly tailored models for specific scenarios, as well as rapid prototyping and comparison of a range of alternative models. Furthermore, newcomers to the field of machine learning do not have to learn about the huge range of traditional methods, but instead can focus their attention on understanding a single modelling environment. In this study, we show how probabilistic graphical models, coupled with efficient inference algorithms, provide a very flexible foundation for model-based machine learning, and we outline a large-scale commercial application of this framework involving tens of millions of users. We also describe the concept of probabilistic programming as a powerful software environment for model-based machine learning, and we discuss a specific probabilistic programming language called Infer.NET, which has been widely used in practical applications.",
            "referenceCount": 34,
            "citationCount": 184,
            "influentialCitationCount": 11,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://royalsocietypublishing.org/doi/pdf/10.1098/rsta.2012.0222",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2013-02-13",
            "journal": {
                "name": "Philosophical transactions. Series A, Mathematical, physical, and engineering sciences",
                "volume": "371"
            },
            "citationStyles": {
                "bibtex": "@Article{Bishop2013ModelbasedML,\n author = {Charles M. Bishop},\n booktitle = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},\n journal = {Philosophical transactions. Series A, Mathematical, physical, and engineering sciences},\n title = {Model-based machine learning},\n volume = {371},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:43dcda631f8a0d39949ba3f9e3e22101db4daba0",
            "@type": "ScholarlyArticle",
            "paperId": "43dcda631f8a0d39949ba3f9e3e22101db4daba0",
            "corpusId": 1230924,
            "url": "https://www.semanticscholar.org/paper/43dcda631f8a0d39949ba3f9e3e22101db4daba0",
            "title": "A Machine Learning Framework for Programming by Example",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2013,
            "externalIds": {
                "MAG": "2560790486",
                "DBLP": "conf/icml/MenonTGLK13",
                "CorpusId": 1230924
            },
            "abstract": "Learning programs is a timely and interesting challenge. In Programming by Example (PBE), a system attempts to infer a program from input and output examples alone, by searching for a composition of some set of base functions. We show how machine learning can be used to speed up this seemingly hopeless search problem, by learning weights that relate textual features describing the provided input-output examples to plausible sub-components of a program. This generic learning framework lets us address problems beyond the scope of earlier PBE systems. Experiments on a prototype implementation show that learning improves search and ranking on a variety of text processing tasks found on help forums.",
            "referenceCount": 16,
            "citationCount": 152,
            "influentialCitationCount": 7,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2013-06-16",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Menon2013AML,\n author = {A. Menon and O. Tamuz and Sumit Gulwani and B. Lampson and A. Kalai},\n booktitle = {International Conference on Machine Learning},\n pages = {187-195},\n title = {A Machine Learning Framework for Programming by Example},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b6cf9167aeb2782651156de5e22cad82ee69a225",
            "@type": "ScholarlyArticle",
            "paperId": "b6cf9167aeb2782651156de5e22cad82ee69a225",
            "corpusId": 209099422,
            "url": "https://www.semanticscholar.org/paper/b6cf9167aeb2782651156de5e22cad82ee69a225",
            "title": "UCI Repository of Machine Learning Databases",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1996,
            "externalIds": {
                "MAG": "2982720039",
                "CorpusId": 209099422
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 2034,
            "influentialCitationCount": 42,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Merz1996UCIRO,\n author = {C. Merz},\n title = {UCI Repository of Machine Learning Databases},\n year = {1996}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c0f7fcc2e03be4395625b6757b17b8834632b952",
            "@type": "ScholarlyArticle",
            "paperId": "c0f7fcc2e03be4395625b6757b17b8834632b952",
            "corpusId": 60235124,
            "url": "https://www.semanticscholar.org/paper/c0f7fcc2e03be4395625b6757b17b8834632b952",
            "title": "Ensemble Machine Learning: Methods and Applications",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2012,
            "externalIds": {
                "MAG": "613690151",
                "CorpusId": 60235124
            },
            "abstract": "It is common wisdom that gathering a variety of views and inputs improves the process of decision making, and, indeed, underpins a democratic society. Dubbed ensemble learning by researchers in computational intelligence and machine learning, it is known to improve a decision systems robustness and accuracy. Now, fresh developments are allowing researchers to unleash the power of ensemble learning in an increasing range of real-world applications. Ensemble learning algorithms such as boosting and random forest facilitate solutions to key computational issues such as face recognition and are now being applied in areas as diverse as object tracking and bioinformatics. Responding to a shortage of literature dedicated to the topic, this volume offers comprehensive coverage of state-of-the-art ensemble learning techniques, including the random forest skeleton tracking algorithm in the Xbox Kinect sensor, which bypasses the need for game controllers. At once a solid theoretical study and a practical guide, the volume is a windfall for researchers and practitioners alike.",
            "referenceCount": 0,
            "citationCount": 709,
            "influentialCitationCount": 34,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2012-02-17",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Zhang2012EnsembleML,\n author = {Cha Zhang and Yunqian Ma},\n title = {Ensemble Machine Learning: Methods and Applications},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ea11efe27e029e391ea52609468353f98d9f946b",
            "@type": "ScholarlyArticle",
            "paperId": "ea11efe27e029e391ea52609468353f98d9f946b",
            "corpusId": 29188161,
            "url": "https://www.semanticscholar.org/paper/ea11efe27e029e391ea52609468353f98d9f946b",
            "title": "Machine learning on Big Data",
            "venue": "IEEE International Conference on Data Engineering",
            "publicationVenue": {
                "id": "urn:research:764e3630-ddac-4c21-af4b-9d32ffef082e",
                "name": "IEEE International Conference on Data Engineering",
                "alternate_names": [
                    "ICDE",
                    "Int Conf Data Eng",
                    "IEEE Int Conf Data Eng",
                    "International Conference on Data Engineering"
                ],
                "issn": null,
                "url": "http://www.wikicfp.com/cfp/program?id=1331"
            },
            "year": 2013,
            "externalIds": {
                "DBLP": "conf/icde/CondieMPW13",
                "MAG": "2035488079",
                "DOI": "10.1145/2463676.2465338",
                "CorpusId": 29188161
            },
            "abstract": "Statistical Machine Learning has undergone a phase transition from a pure academic endeavor to being one of the main drivers of modern commerce and science. Even more so, recent results such as those on tera-scale learning [1] and on very large neural networks [2] suggest that scale is an important ingredient in quality modeling. This tutorial introduces current applications, techniques and systems with the aim of cross-fertilizing research between the database and machine learning communities. The tutorial covers current large scale applications of Machine Learning, their computational model and the workflow behind building those. Based on this foundation, we present the current state-of-the-art in systems support in the bulk of the tutorial. We also identify critical gaps in the state-of-the-art. This leads to the closing of the seminar, where we introduce two sets of open research questions: Better systems support for the already established use cases of Machine Learning and support for recent advances in Machine Learning research.",
            "referenceCount": 21,
            "citationCount": 191,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference",
                "Review"
            ],
            "publicationDate": "2013-04-08",
            "journal": {
                "name": "2013 IEEE 29th International Conference on Data Engineering (ICDE)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Condie2013MachineLO,\n author = {Tyson Condie and Paul Mineiro and N. Polyzotis and Markus Weimer},\n booktitle = {IEEE International Conference on Data Engineering},\n journal = {2013 IEEE 29th International Conference on Data Engineering (ICDE)},\n pages = {1242-1244},\n title = {Machine learning on Big Data},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6aa8f8ae07a57a2025d6adc27a0bc53f6a7ee385",
            "@type": "ScholarlyArticle",
            "paperId": "6aa8f8ae07a57a2025d6adc27a0bc53f6a7ee385",
            "corpusId": 15668505,
            "url": "https://www.semanticscholar.org/paper/6aa8f8ae07a57a2025d6adc27a0bc53f6a7ee385",
            "title": "Machine learning for targeted display advertising: transfer learning in action",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 2013,
            "externalIds": {
                "MAG": "1984363873",
                "DBLP": "journals/ml/PerlichDRSP14",
                "DOI": "10.1007/s10994-013-5375-2",
                "CorpusId": 15668505
            },
            "abstract": null,
            "referenceCount": 30,
            "citationCount": 166,
            "influentialCitationCount": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://archive.nyu.edu/bitstream/2451/31708/2/Provost%201_2013.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Business",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2013-06-01",
            "journal": {
                "name": "Machine Learning",
                "volume": "95"
            },
            "citationStyles": {
                "bibtex": "@Article{Perlich2013MachineLF,\n author = {C. Perlich and B. Dalessandro and T. Raeder and O. Stitelman and F. Provost},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {103-127},\n title = {Machine learning for targeted display advertising: transfer learning in action},\n volume = {95},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ea0425270d0dc496b31108572e706df7e40bce85",
            "@type": "ScholarlyArticle",
            "paperId": "ea0425270d0dc496b31108572e706df7e40bce85",
            "corpusId": 1735983,
            "url": "https://www.semanticscholar.org/paper/ea0425270d0dc496b31108572e706df7e40bce85",
            "title": "Machine Learning in Medical Imaging",
            "venue": "International Journal of Biomedical Imaging",
            "publicationVenue": {
                "id": "urn:research:dc8e1288-1799-4a90-b7f8-cb339c226547",
                "name": "International Journal of Biomedical Imaging",
                "alternate_names": [
                    "Int J Biomed Imaging"
                ],
                "issn": "1687-4188",
                "url": "https://www.hindawi.com/journals/ijbi/"
            },
            "year": 2012,
            "externalIds": {
                "DBLP": "journals/ijbi/SuzukiY0S12",
                "MAG": "2046740986",
                "PubMedCentral": "3303553",
                "DOI": "10.1155/2012/123727",
                "CorpusId": 1735983,
                "PubMed": "22481902"
            },
            "abstract": "Medical imaging is becoming indispensable for patients' healthcare. Machine learning plays an essential role in the medical imaging field, including computer-aided diagnosis, image segmentation, image registration, image fusion, image-guided therapy, image annotation, and image database retrieval. With advances in medical imaging, new imaging modalities and methodologies such as cone-beam/multislice CT, 3D ultrasound imaging, tomosynthesis, diffusion-weighted magnetic resonance imaging (MRI), positron-emission tomography (PET)/CT, electrical impedance tomography, and diffuse optical tomography, new machine-learning algorithms/applications are demanded in the medical imaging field. Because of large variations and complexityit is generally difficult to derive analytic solutions or simple equations to represent objects such as lesions and anatomy in medical images. Therefore, tasks in medical imaging require \u201clearning from examples\u201d for accurate representation of data and prior knowledge. Because of its essential needs, machine learning in medical imaging is one of the most promising, growing fields. \n \nThe main aim of this special issue is to help advance the scientific research within the broad field of machine learning in medical imaging. The special issue was planned in conjunction with the International Workshop on Machine Learning in Medical Imaging (MLMI 2010) [1], which was the first workshop on this topic, held at the 13th International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI 2010) in September, 2010, in Beijing, China. This special issue is one in a series of special issues of journals on this topic [2]; it focuses on major trends and challenges in this area, and it presents work aimed at identifying new cutting-edge techniques and their use in medical imaging. \n \nThe quality level of the submissions for this special issue was very high. A total of 17 papers were submitted to this issue in response to the call for papers. Based on a rigorous review process, 10 papers (59%) were accepted for publication in the special issue. The special issue starts by a review of studies on a class of machine-learning techniques, called pixel/voxel-based machine learning, in medical imaging by K. Suzuki. A series of medical imaging applications of machine-learning techniques are presented. A large variety of applications are well represented here, including organ modeling by D. Wang et al. and X. Qiao and Y.-W. Chen, brain function estimation by V. Michel et al., image reconstruction by H. Shouno et al., lesion classification by P. Wighton et al., modality classification by X.-H. Han and Y.-W. Chen, lesion segmentation by M. Zortea et al., organ segmentation by S. Alzubi et al., and visualization of molecular signals by F. Mattoli et al. Also, the issue covers various biomedical imaging modalities, including MRI by D. Wang et al., CT by X. Qiao and Y.-W. Chen and H. Shouno et al., functional MRI by V. Michel et al., dermoscopy by P. Wighton et al. and M. Zortea et al., scintigraphy by X.-H. Han and Y.-W. Chen, ultrasound imaging by X.-H. Han and Y.-W. Chen, radiography by X.-H. Han and Y.-W. Chen, MR angiography by S. Alzubi et al., and microscopy by F. Mattoli et al. as well as a variety of organs, including the kidneys by D. Wang et al., liver by X. Qiao and Y.-W. Chen, brain by V. Michel et al. and F. Mattoli et al., chest by S. Alzubi et al., skin by P. Wighton et al. and M. Zortea et al., and heart by F. Mattoli et al. Various machine-learning techniques were developed/used to solve the respective problems, including structured dictionary learning by D. Wang et al., generalized N-dimensional principal component analysis by X. Qiao et al., multiclass sparse Bayesian regression by V. Michel et al., Bayesian hyperparameter inference by H. Shouno et al., supervised learning of probabilistic models based on maximum aposteriori estimation and conditional random fields by P. Wighton et al., joint kernel equal contribution in support vector classification by X.-H. Han and Y.-W. Chen, and iterative hybrid classification by M. Zortea et al. \n \nWe are grateful to all authors for their excellent contributions to this special issue and to all reviewers for their reviews and constructive suggestions. We hope that this special issue will inspire further ideas for creative research, advance the field of machine learning in medical imaging, and facilitate the translation of the research from bench to bedside. \n \n \nKenji Suzuki \n \nPingkun Yan \n \nFei Wang \n \nDinggang Shen",
            "referenceCount": 2,
            "citationCount": 679,
            "influentialCitationCount": 16,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2012-03-05",
            "journal": {
                "name": "International Journal of Biomedical Imaging",
                "volume": "2012"
            },
            "citationStyles": {
                "bibtex": "@Article{Suzuki2012MachineLI,\n author = {Kenji Suzuki and Pingkun Yan and Fei Wang and D. Shen},\n booktitle = {International Journal of Biomedical Imaging},\n journal = {International Journal of Biomedical Imaging},\n title = {Machine Learning in Medical Imaging},\n volume = {2012},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7fcb90f68529cbfab49f471b54719ded7528d0ef",
            "@type": "ScholarlyArticle",
            "paperId": "7fcb90f68529cbfab49f471b54719ded7528d0ef",
            "corpusId": 14999259,
            "url": "https://www.semanticscholar.org/paper/7fcb90f68529cbfab49f471b54719ded7528d0ef",
            "title": "Federated Learning: Strategies for Improving Communication Efficiency",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "journals/corr/KonecnyMYRSB16",
                "ArXiv": "1610.05492",
                "MAG": "2535838896",
                "CorpusId": 14999259
            },
            "abstract": "Federated Learning is a machine learning setting where the goal is to train a high-quality centralized model while training data remains distributed over a large number of clients each with unreliable and relatively slow network connections. We consider learning algorithms for this setting where on each round, each client independently computes an update to the current model based on its local data, and communicates this update to a central server, where the client-side updates are aggregated to compute a new global model. The typical clients in this setting are mobile phones, and communication efficiency is of the utmost importance. In this paper, we propose two ways to reduce the uplink communication costs: structured updates, where we directly learn an update from a restricted space parametrized using a smaller number of variables, e.g. either low-rank or a random mask; and sketched updates, where we learn a full model update and then compress it using a combination of quantization, random rotations, and subsampling before sending it to the server. Experiments on both convolutional and recurrent networks show that the proposed methods can reduce the communication cost by two orders of magnitude.",
            "referenceCount": 26,
            "citationCount": 3461,
            "influentialCitationCount": 246,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-10-18",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1610.05492"
            },
            "citationStyles": {
                "bibtex": "@Article{Konecn\u00fd2016FederatedLS,\n author = {Jakub Konecn\u00fd and H. B. McMahan and Felix X. Yu and Peter Richt\u00e1rik and A. Suresh and D. Bacon},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Federated Learning: Strategies for Improving Communication Efficiency},\n volume = {abs/1610.05492},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8ed390c98912ab8d27a398ab583d358c5a41fee9",
            "@type": "ScholarlyArticle",
            "paperId": "8ed390c98912ab8d27a398ab583d358c5a41fee9",
            "corpusId": 6432124,
            "url": "https://www.semanticscholar.org/paper/8ed390c98912ab8d27a398ab583d358c5a41fee9",
            "title": "Machine learning for science and society",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 2013,
            "externalIds": {
                "DBLP": "journals/ml/RudinW14",
                "MAG": "2516435602",
                "DOI": "10.1007/s10994-013-5425-9",
                "CorpusId": 6432124
            },
            "abstract": null,
            "referenceCount": 33,
            "citationCount": 94,
            "influentialCitationCount": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007%2Fs10994-013-5425-9.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Political Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Sociology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2013-11-01",
            "journal": {
                "name": "Machine Learning",
                "volume": "95"
            },
            "citationStyles": {
                "bibtex": "@Article{Rudin2013MachineLF,\n author = {C. Rudin and K. Wagstaff},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {1-9},\n title = {Machine learning for science and society},\n volume = {95},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:184ac0766262312ba76bbdece4e7ffad0aa8180b",
            "@type": "ScholarlyArticle",
            "paperId": "184ac0766262312ba76bbdece4e7ffad0aa8180b",
            "corpusId": 393948,
            "url": "https://www.semanticscholar.org/paper/184ac0766262312ba76bbdece4e7ffad0aa8180b",
            "title": "Representation Learning: A Review and New Perspectives",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "publicationVenue": {
                "id": "urn:research:25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "alternate_names": [
                    "IEEE Trans Pattern Anal Mach Intell"
                ],
                "issn": "0162-8828",
                "url": "http://www.computer.org/tpami/"
            },
            "year": 2012,
            "externalIds": {
                "ArXiv": "1206.5538",
                "MAG": "2952111767",
                "DBLP": "journals/pami/BengioCV13",
                "DOI": "10.1109/TPAMI.2013.50",
                "CorpusId": 393948,
                "PubMed": "23787338"
            },
            "abstract": "The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.",
            "referenceCount": 267,
            "citationCount": 10782,
            "influentialCitationCount": 551,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.cs.princeton.edu/courses/archive/spring13/cos598C/Representation Learning - A Review and New Perspectives.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2012-06-24",
            "journal": {
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "volume": "35"
            },
            "citationStyles": {
                "bibtex": "@Article{Bengio2012RepresentationLA,\n author = {Yoshua Bengio and Aaron C. Courville and Pascal Vincent},\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n pages = {1798-1828},\n title = {Representation Learning: A Review and New Perspectives},\n volume = {35},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8f566001453bc6be0a935bf69ffd90d9db3af32b",
            "@type": "ScholarlyArticle",
            "paperId": "8f566001453bc6be0a935bf69ffd90d9db3af32b",
            "corpusId": 231986372,
            "url": "https://www.semanticscholar.org/paper/8f566001453bc6be0a935bf69ffd90d9db3af32b",
            "title": "Towards Causal Representation Learning",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2021,
            "externalIds": {
                "DBLP": "journals/corr/abs-2102-11107",
                "ArXiv": "2102.11107",
                "CorpusId": 231986372
            },
            "abstract": "The two fields of machine learning and graphical causality arose and developed separately. However, there is now cross-pollination and increasing interest in both fields to benefit from the advances of the other. In the present paper, we review fundamental concepts of causal inference and relate them to crucial open problems of machine learning, including transfer and generalization, thereby assaying how causality can contribute to modern machine learning research. This also applies in the opposite direction: we note that most work in causality starts from the premise that the causal variables are given. A central problem for AI and causality is, thus, causal representation learning, the discovery of high-level causal variables from low-level observations. Finally, we delineate some implications of causality for machine learning and propose key research areas at the intersection of both communities.",
            "referenceCount": 279,
            "citationCount": 492,
            "influentialCitationCount": 58,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Philosophy",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2021-02-22",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2102.11107"
            },
            "citationStyles": {
                "bibtex": "@Article{Scholkopf2021TowardsCR,\n author = {B. Scholkopf and Francesco Locatello and Stefan Bauer and Nan Rosemary Ke and Nal Kalchbrenner and Anirudh Goyal and Yoshua Bengio},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Towards Causal Representation Learning},\n volume = {abs/2102.11107},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9eca724e2b8e7a20fa1b05b8a9398f86a24b86d6",
            "@type": "ScholarlyArticle",
            "paperId": "9eca724e2b8e7a20fa1b05b8a9398f86a24b86d6",
            "corpusId": 61477975,
            "url": "https://www.semanticscholar.org/paper/9eca724e2b8e7a20fa1b05b8a9398f86a24b86d6",
            "title": "The master algorithm: how the quest for the ultimate learning machine will remake our world",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2015,
            "externalIds": {
                "MAG": "2260201820",
                "CorpusId": 61477975
            },
            "abstract": "Nowadays, \u201cmachine learning\u201d is present in several aspects of the current world, internet advisors, advertisements and \u201csmart\u201d devices that seem to know what we need in a given moment. These are some examples of the problems solved by machine learning. This book presents the past, the present and the future of the different types of machine learning algorithms. At the beginning of the book, the author takes us to the first years of the computing science, where a programmer had to do absolutely everything by himself to make an algorithm do a certain task. As time passes, there appeared the first algorithms that were capable of programming themselves learning from the available data. The author presents what he himself calls the five \u201ctribes\u201d of machine learning, the essence that defends each one and the kind of problems that are able to solve without problems. With a great amount of simple examples, the author depicts which advantages and disadvantages of the \u201cmaster\u201d algorithms of each \u201ctribes\u201d are, saying that the problem that a tribe solves perfectly well, another one cannot do it, and the other way about. The author suggests to get the best out of each \u201ctribe\u201d and make a unique learning algorithm able to learn without caring about the problem: the master algorithm.",
            "referenceCount": 0,
            "citationCount": 408,
            "influentialCitationCount": 35,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2015-11-01",
            "journal": {
                "name": "Journal of Computer Science and Technology",
                "volume": "15"
            },
            "citationStyles": {
                "bibtex": "@Article{Hasperu\u00e92015TheMA,\n author = {W. Hasperu\u00e9},\n journal = {Journal of Computer Science and Technology},\n pages = {157-158},\n title = {The master algorithm: how the quest for the ultimate learning machine will remake our world},\n volume = {15},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:18d026ec5d0eebd17ee2c762da89540c0b3d7bde",
            "@type": "ScholarlyArticle",
            "paperId": "18d026ec5d0eebd17ee2c762da89540c0b3d7bde",
            "corpusId": 207847753,
            "url": "https://www.semanticscholar.org/paper/18d026ec5d0eebd17ee2c762da89540c0b3d7bde",
            "title": "A Comprehensive Survey on Transfer Learning",
            "venue": "Proceedings of the IEEE",
            "publicationVenue": {
                "id": "urn:research:6faaccca-1cc4-45a9-aeb6-96a4901d2606",
                "name": "Proceedings of the IEEE",
                "alternate_names": [
                    "Proc IEEE"
                ],
                "issn": "0018-9219",
                "url": "http://www.ieee.org/portal/pages/pubs/proceedings/"
            },
            "year": 2019,
            "externalIds": {
                "ArXiv": "1911.02685",
                "MAG": "3041133507",
                "DBLP": "journals/pieee/ZhuangQDXZZXH21",
                "DOI": "10.1109/JPROC.2020.3004555",
                "CorpusId": 207847753
            },
            "abstract": "Transfer learning aims at improving the performance of target learners on target domains by transferring the knowledge contained in different but related source domains. In this way, the dependence on a large number of target-domain data can be reduced for constructing target learners. Due to the wide application prospects, transfer learning has become a popular and promising area in machine learning. Although there are already some valuable and impressive surveys on transfer learning, these surveys introduce approaches in a relatively isolated way and lack the recent advances in transfer learning. Due to the rapid expansion of the transfer learning area, it is both necessary and challenging to comprehensively review the relevant studies. This survey attempts to connect and systematize the existing transfer learning research studies, as well as to summarize and interpret the mechanisms and the strategies of transfer learning in a comprehensive way, which may help readers have a better understanding of the current research status and ideas. Unlike previous surveys, this survey article reviews more than 40 representative transfer learning approaches, especially homogeneous transfer learning approaches, from the perspectives of data and model. The applications of transfer learning are also briefly introduced. In order to show the performance of different transfer learning models, over 20 representative transfer learning models are used for experiments. The models are performed on three different data sets, that is, Amazon Reviews, Reuters-21578, and Office-31, and the experimental results demonstrate the importance of selecting appropriate transfer learning models for different applications in practice.",
            "referenceCount": 228,
            "citationCount": 2470,
            "influentialCitationCount": 53,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1911.02685",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2019-11-07",
            "journal": {
                "name": "Proceedings of the IEEE",
                "volume": "109"
            },
            "citationStyles": {
                "bibtex": "@Article{Zhuang2019ACS,\n author = {Fuzhen Zhuang and Zhiyuan Qi and Keyu Duan and Dongbo Xi and Yongchun Zhu and Hengshu Zhu and Hui Xiong and Qing He},\n booktitle = {Proceedings of the IEEE},\n journal = {Proceedings of the IEEE},\n pages = {43-76},\n title = {A Comprehensive Survey on Transfer Learning},\n volume = {109},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:819167ace2f0caae7745d2f25a803979be5fbfae",
            "@type": "ScholarlyArticle",
            "paperId": "819167ace2f0caae7745d2f25a803979be5fbfae",
            "corpusId": 7004303,
            "url": "https://www.semanticscholar.org/paper/819167ace2f0caae7745d2f25a803979be5fbfae",
            "title": "The Limitations of Deep Learning in Adversarial Settings",
            "venue": "European Symposium on Security and Privacy",
            "publicationVenue": {
                "id": "urn:research:4c2b8cb8-e51c-4ece-9122-89595989b56f",
                "name": "European Symposium on Security and Privacy",
                "alternate_names": [
                    "EuroS&P",
                    "IEEE European Symposium on Security and Privacy",
                    "Eur Symp Secur Priv",
                    "IEEE Eur Symp Secur Priv",
                    "EUROS&P"
                ],
                "issn": null,
                "url": null
            },
            "year": 2015,
            "externalIds": {
                "ArXiv": "1511.07528",
                "DBLP": "conf/eurosp/PapernotMJFCS16",
                "MAG": "2949152835",
                "DOI": "10.1109/EuroSP.2016.36",
                "CorpusId": 7004303
            },
            "abstract": "Deep learning takes advantage of large datasets and computationally efficient training algorithms to outperform other approaches at various machine learning tasks. However, imperfections in the training phase of deep neural networks make them vulnerable to adversarial samples: inputs crafted by adversaries with the intent of causing deep neural networks to misclassify. In this work, we formalize the space of adversaries against deep neural networks (DNNs) and introduce a novel class of algorithms to craft adversarial samples based on a precise understanding of the mapping between inputs and outputs of DNNs. In an application to computer vision, we show that our algorithms can reliably produce samples correctly classified by human subjects but misclassified in specific targets by a DNN with a 97% adversarial success rate while only modifying on average 4.02% of the input features per sample. We then evaluate the vulnerability of different sample classes to adversarial perturbations by defining a hardness measure. Finally, we describe preliminary work outlining defenses against adversarial samples by defining a predictive measure of distance between a benign input and a target classification.",
            "referenceCount": 45,
            "citationCount": 3408,
            "influentialCitationCount": 432,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1511.07528",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-11-24",
            "journal": {
                "name": "2016 IEEE European Symposium on Security and Privacy (EuroS&P)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Papernot2015TheLO,\n author = {Nicolas Papernot and P. Mcdaniel and S. Jha and Matt Fredrikson and Z. B. Celik and A. Swami},\n booktitle = {European Symposium on Security and Privacy},\n journal = {2016 IEEE European Symposium on Security and Privacy (EuroS&P)},\n pages = {372-387},\n title = {The Limitations of Deep Learning in Adversarial Settings},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:91a613ed06c4654f38f5c2e7fe6ebffeec53d887",
            "@type": "ScholarlyArticle",
            "paperId": "91a613ed06c4654f38f5c2e7fe6ebffeec53d887",
            "corpusId": 26893209,
            "url": "https://www.semanticscholar.org/paper/91a613ed06c4654f38f5c2e7fe6ebffeec53d887",
            "title": "Weighted extreme learning machine for imbalance learning",
            "venue": "Neurocomputing",
            "publicationVenue": {
                "id": "urn:research:df12d289-f447-47d3-8846-75e39de3ab57",
                "name": "Neurocomputing",
                "alternate_names": null,
                "issn": "0925-2312",
                "url": "http://www.elsevier.com/locate/neucom"
            },
            "year": 2013,
            "externalIds": {
                "DBLP": "journals/ijon/ZongHC13",
                "MAG": "2078622091",
                "DOI": "10.1016/j.neucom.2012.08.010",
                "CorpusId": 26893209
            },
            "abstract": null,
            "referenceCount": 26,
            "citationCount": 664,
            "influentialCitationCount": 92,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2013-02-01",
            "journal": {
                "name": "Neurocomputing",
                "volume": "101"
            },
            "citationStyles": {
                "bibtex": "@Article{Zong2013WeightedEL,\n author = {Weiwei Zong and G. Huang and Yiqiang Chen},\n booktitle = {Neurocomputing},\n journal = {Neurocomputing},\n pages = {229-242},\n title = {Weighted extreme learning machine for imbalance learning},\n volume = {101},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b2114228411d367cfa6ca091008291f250a2c490",
            "@type": "ScholarlyArticle",
            "paperId": "b2114228411d367cfa6ca091008291f250a2c490",
            "corpusId": 61156451,
            "url": "https://www.semanticscholar.org/paper/b2114228411d367cfa6ca091008291f250a2c490",
            "title": "Deep learning and process understanding for data-driven Earth system science",
            "venue": "Nature",
            "publicationVenue": {
                "id": "urn:research:6c24a0a0-b07d-4d7b-a19b-fd09a3ed453a",
                "name": "Nature",
                "alternate_names": null,
                "issn": "0028-0836",
                "url": "https://www.nature.com/"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2913323966",
                "DBLP": "journals/nature/ReichsteinCSJDC19",
                "DOI": "10.1038/s41586-019-0912-1",
                "CorpusId": 61156451,
                "PubMed": "30760912"
            },
            "abstract": null,
            "referenceCount": 101,
            "citationCount": 2034,
            "influentialCitationCount": 46,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://pure.mpg.de/pubman/item/item_3029184_9/component/file_3282959/BGC3001P.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Environmental Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Geology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2019-02-01",
            "journal": {
                "name": "Nature",
                "volume": "566"
            },
            "citationStyles": {
                "bibtex": "@Article{Reichstein2019DeepLA,\n author = {M. Reichstein and Gustau Camps-Valls and B. Stevens and M. Jung and Joachim Denzler and N. Carvalhais and Prabhat},\n booktitle = {Nature},\n journal = {Nature},\n pages = {195 - 204},\n title = {Deep learning and process understanding for data-driven Earth system science},\n volume = {566},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6c16cf47f2b872e7b2ad06facb5d491857650514",
            "@type": "ScholarlyArticle",
            "paperId": "6c16cf47f2b872e7b2ad06facb5d491857650514",
            "corpusId": 59994655,
            "url": "https://www.semanticscholar.org/paper/6c16cf47f2b872e7b2ad06facb5d491857650514",
            "title": "C4.5: Programs for Machine Learning (\u66f8\u8a55)",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1995,
            "externalIds": {
                "MAG": "188932407",
                "CorpusId": 59994655
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 1668,
            "influentialCitationCount": 104,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1995-05-01",
            "journal": {
                "name": "",
                "volume": "10"
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{\u91cd\u90ce1995C45PF,\n author = {\u91d1\u7530 \u91cd\u90ce},\n pages = {475-476},\n title = {C4.5: Programs for Machine Learning (\u66f8\u8a55)},\n volume = {10},\n year = {1995}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6d9f55b445f36578802e7eef4393cfa914b11620",
            "@type": "ScholarlyArticle",
            "paperId": "6d9f55b445f36578802e7eef4393cfa914b11620",
            "corpusId": 12561212,
            "url": "https://www.semanticscholar.org/paper/6d9f55b445f36578802e7eef4393cfa914b11620",
            "title": "Object Recognition as Machine Translation: Learning a Lexicon for a Fixed Image Vocabulary",
            "venue": "European Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                "name": "European Conference on Computer Vision",
                "alternate_names": [
                    "ECCV",
                    "Eur Conf Comput Vis"
                ],
                "issn": null,
                "url": "https://link.springer.com/conference/eccv"
            },
            "year": 2002,
            "externalIds": {
                "MAG": "1666447063",
                "DBLP": "conf/eccv/DuyguluBFF02",
                "DOI": "10.1007/3-540-47979-1_7",
                "CorpusId": 12561212
            },
            "abstract": null,
            "referenceCount": 13,
            "citationCount": 1810,
            "influentialCitationCount": 218,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/3-540-47979-1_7.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2002-05-28",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Sahin2002ObjectRA,\n author = {P. D. Sahin and Kobus Barnard and Jo\u00e3o Freitas and D. Forsyth},\n booktitle = {European Conference on Computer Vision},\n pages = {97-112},\n title = {Object Recognition as Machine Translation: Learning a Lexicon for a Fixed Image Vocabulary},\n year = {2002}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a35d565e8f70cb6ddc09a65904a72622e16b9485",
            "@type": "ScholarlyArticle",
            "paperId": "a35d565e8f70cb6ddc09a65904a72622e16b9485",
            "corpusId": 51789432,
            "url": "https://www.semanticscholar.org/paper/a35d565e8f70cb6ddc09a65904a72622e16b9485",
            "title": "Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers",
            "venue": "Found. Trends Mach. Learn.",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2011,
            "externalIds": {
                "MAG": "2164278908",
                "DBLP": "journals/ftml/BoydPCPE11",
                "DOI": "10.1561/2200000016",
                "CorpusId": 51789432
            },
            "abstract": "Many problems of recent interest in statistics and machine learning can be posed in the framework of convex optimization. Due to the explosion in size and complexity of modern datasets, it is increasingly important to be able to solve problems with a very large number of features or training examples. As a result, both the decentralized collection or storage of these datasets as well as accompanying distributed solution methods are either necessary or at least highly desirable. In this review, we argue that the alternating direction method of multipliers is well suited to distributed convex optimization, and in particular to large-scale problems arising in statistics, machine learning, and related areas. The method was developed in the 1970s, with roots in the 1950s, and is equivalent or closely related to many other algorithms, such as dual decomposition, the method of multipliers, Douglas\u2013Rachford splitting, Spingarn's method of partial inverses, Dykstra's alternating projections, Bregman iterative algorithms for l1 problems, proximal methods, and others. After briefly surveying the theory and history of the algorithm, we discuss applications to a wide variety of statistical and machine learning problems of recent interest, including the lasso, sparse logistic regression, basis pursuit, covariance selection, support vector machines, and many others. We also discuss general distributed optimization, extensions to the nonconvex setting, and efficient implementation, including some details on distributed MPI and Hadoop MapReduce implementations.",
            "referenceCount": 185,
            "citationCount": 12805,
            "influentialCitationCount": 2684,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.stanford.edu/~boyd/papers/pdf/admm_distr_stats.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2011-05-23",
            "journal": {
                "name": "Found. Trends Mach. Learn.",
                "volume": "3"
            },
            "citationStyles": {
                "bibtex": "@Article{Boyd2011DistributedOA,\n author = {Stephen P. Boyd and Neal Parikh and Eric Chu and Borja Peleato and Jonathan Eckstein},\n booktitle = {Found. Trends Mach. Learn.},\n journal = {Found. Trends Mach. Learn.},\n pages = {1-122},\n title = {Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers},\n volume = {3},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f466157848d1a7772fb6d02cdac9a7a5e7ef982e",
            "@type": "ScholarlyArticle",
            "paperId": "f466157848d1a7772fb6d02cdac9a7a5e7ef982e",
            "corpusId": 20282961,
            "url": "https://www.semanticscholar.org/paper/f466157848d1a7772fb6d02cdac9a7a5e7ef982e",
            "title": "Neural Discrete Representation Learning",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2752796333",
                "ArXiv": "1711.00937",
                "DBLP": "conf/nips/OordVK17",
                "CorpusId": 20282961
            },
            "abstract": "Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of \"posterior collapse\" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.",
            "referenceCount": 43,
            "citationCount": 2618,
            "influentialCitationCount": 470,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-11-02",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1711.00937"
            },
            "citationStyles": {
                "bibtex": "@Article{Oord2017NeuralDR,\n author = {A\u00e4ron van den Oord and Oriol Vinyals and K. Kavukcuoglu},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Neural Discrete Representation Learning},\n volume = {abs/1711.00937},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1b04936c2599e59b120f743fbb30df2eed3fd782",
            "@type": "ScholarlyArticle",
            "paperId": "1b04936c2599e59b120f743fbb30df2eed3fd782",
            "corpusId": 215786368,
            "url": "https://www.semanticscholar.org/paper/1b04936c2599e59b120f743fbb30df2eed3fd782",
            "title": "Shortcut learning in deep neural networks",
            "venue": "Nature Machine Intelligence",
            "publicationVenue": {
                "id": "urn:research:6457124b-39bf-4d02-bff4-73752ff21562",
                "name": "Nature Machine Intelligence",
                "alternate_names": [
                    "Nat Mach Intell"
                ],
                "issn": "2522-5839",
                "url": "https://www.nature.com/natmachintell/"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "journals/natmi/GeirhosJMZBBW20",
                "ArXiv": "2004.07780",
                "MAG": "3016970897",
                "DOI": "10.1038/s42256-020-00257-z",
                "CorpusId": 215786368
            },
            "abstract": null,
            "referenceCount": 166,
            "citationCount": 1242,
            "influentialCitationCount": 90,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2004.07780",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Education",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-04-16",
            "journal": {
                "name": "Nature Machine Intelligence",
                "volume": "2"
            },
            "citationStyles": {
                "bibtex": "@Article{Geirhos2020ShortcutLI,\n author = {Robert Geirhos and J. Jacobsen and Claudio Michaelis and R. Zemel and Wieland Brendel and M. Bethge and Felix Wichmann},\n booktitle = {Nature Machine Intelligence},\n journal = {Nature Machine Intelligence},\n pages = {665 - 673},\n title = {Shortcut learning in deep neural networks},\n volume = {2},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:79cf9462a583e1889781868cbf8c31e43b36dd2f",
            "@type": "ScholarlyArticle",
            "paperId": "79cf9462a583e1889781868cbf8c31e43b36dd2f",
            "corpusId": 59599820,
            "url": "https://www.semanticscholar.org/paper/79cf9462a583e1889781868cbf8c31e43b36dd2f",
            "title": "Towards Federated Learning at Scale: System Design",
            "venue": "Conference on Machine Learning and Systems",
            "publicationVenue": {
                "id": "urn:research:3bcf77b3-860b-4dd7-84ae-9fe9414c6c6a",
                "name": "Conference on Machine Learning and Systems",
                "alternate_names": [
                    "MLSys",
                    "Conf Mach Learn Syst"
                ],
                "issn": null,
                "url": "https://mlsys.org/"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "conf/mlsys/BonawitzEGHIIKK19",
                "MAG": "3038028469",
                "ArXiv": "1902.01046",
                "CorpusId": 59599820
            },
            "abstract": "Federated Learning is a distributed machine learning approach which enables model training on a large corpus of decentralized data. We have built a scalable production system for Federated Learning in the domain of mobile devices, based on TensorFlow. In this paper, we describe the resulting high-level design, sketch some of the challenges and their solutions, and touch upon the open problems and future directions.",
            "referenceCount": 26,
            "citationCount": 1906,
            "influentialCitationCount": 167,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-02-04",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1902.01046"
            },
            "citationStyles": {
                "bibtex": "@Article{Bonawitz2019TowardsFL,\n author = {Keith Bonawitz and Hubert Eichner and W. Grieskamp and Dzmitry Huba and A. Ingerman and Vladimir Ivanov and Chlo\u00e9 Kiddon and Jakub Konecn\u00fd and S. Mazzocchi and H. B. McMahan and Timon Van Overveldt and David Petrou and Daniel Ramage and Jason Roselander},\n booktitle = {Conference on Machine Learning and Systems},\n journal = {ArXiv},\n title = {Towards Federated Learning at Scale: System Design},\n volume = {abs/1902.01046},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c088a41395a9f08d719be8479dc11ddecf047530",
            "@type": "ScholarlyArticle",
            "paperId": "c088a41395a9f08d719be8479dc11ddecf047530",
            "corpusId": 60858035,
            "url": "https://www.semanticscholar.org/paper/c088a41395a9f08d719be8479dc11ddecf047530",
            "title": "Density Ratio Estimation in Machine Learning",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2012,
            "externalIds": {
                "MAG": "2198076771",
                "DBLP": "books/daglib/0034862",
                "DOI": "10.1017/CBO9781139035613",
                "CorpusId": 60858035
            },
            "abstract": "Machine learning is an interdisciplinary field of science and engineering that studies mathematical theories and practical applications of systems that learn. This book introduces theories, methods, and applications of density ratio estimation, which is a newly emerging paradigm in the machine learning community. Various machine learning problems such as non-stationarity adaptation, outlier detection, dimensionality reduction, independent component analysis, clustering, classification, and conditional density estimation can be systematically solved via the estimation of probability density ratios. The authors offer a comprehensive introduction of various density ratio estimators including methods via density estimation, moment matching, probabilistic classification, density fitting, and density ratio fitting as well as describing how these can be applied to machine learning. The book also provides mathematical theories for density ratio estimation including parametric and non-parametric convergence analysis and numerical stability analysis to complete the first and definitive treatment of the entire framework of density ratio estimation in machine learning.",
            "referenceCount": 53,
            "citationCount": 506,
            "influentialCitationCount": 35,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2012-02-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Sugiyama2012DensityRE,\n author = {Masashi Sugiyama and Taiji Suzuki and T. Kanamori},\n pages = {I-XII, 1-329},\n title = {Density Ratio Estimation in Machine Learning},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1ee7be29ce297de6d79a438d26037595967f5b17",
            "@type": "ScholarlyArticle",
            "paperId": "1ee7be29ce297de6d79a438d26037595967f5b17",
            "corpusId": 57501944,
            "url": "https://www.semanticscholar.org/paper/1ee7be29ce297de6d79a438d26037595967f5b17",
            "title": "Ensemble Machine Learning",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2012,
            "externalIds": {
                "MAG": "2498672755",
                "DOI": "10.1007/978-1-4419-9326-7",
                "CorpusId": 57501944
            },
            "abstract": null,
            "referenceCount": 21,
            "citationCount": 599,
            "influentialCitationCount": 12,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Zhang2012EnsembleML,\n author = {Cha Zhang and Yunqian Ma},\n title = {Ensemble Machine Learning},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:be9811f7e6019d5cd59ff97829a44bb5577bab00",
            "@type": "ScholarlyArticle",
            "paperId": "be9811f7e6019d5cd59ff97829a44bb5577bab00",
            "corpusId": 261329242,
            "url": "https://www.semanticscholar.org/paper/be9811f7e6019d5cd59ff97829a44bb5577bab00",
            "title": "Machine Learning in Action",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2012,
            "externalIds": {
                "MAG": "328766226",
                "CorpusId": 261329242
            },
            "abstract": "SummaryMachine Learning in Action is unique book that blends the foundational theories of machine learning with the practical realities of building tools for everyday data analysis. You'll use the flexible Python programming language to build programs that implement algorithms for data classification, forecasting, recommendations, and higher-level features like summarization and simplification. About the BookA machine is said to learn when its performance improves with experience. Learning requires algorithms and programs that capture data and ferret out the interesting or useful patterns. Once the specialized domain of analysts and mathematicians, machine learning is becoming a skill needed by many.Machine Learning in Action is a clearly written tutorial for developers. It avoids academic language and takes you straight to the techniques you'll use in your day-to-day work. Many (Python) examples present the core algorithms of statistical data processing, data analysis, and data visualization in code you can reuse. You'll understand the concepts and how they fit in with tactical tasks like classification, forecasting, recommendations, and higher-level features like summarization and simplification.Readers need no prior experience with machine learning or statistical processing. Familiarity with Python is helpful.Purchase includes free PDF, ePub, and Kindle eBooks downloadable at manning.com. What's InsideA no-nonsense introduction Examples showing common ML tasks Everyday data analysis Implementing classic algorithms like Apriori and Adaboos=================================== Table of ContentsPART 1 CLASSIFICATION Machine learning basics Classifying with k-Nearest Neighbors Splitting datasets one feature at a time: decision trees Classifying with probability theory: nave Bayes Logistic regression Support vector machines Improving classification with the AdaBoost meta algorithm PART 2 FORECASTING NUMERIC VALUES WITH REGRESSION Predicting numeric values: regression Tree-based regression PART 3 UNSUPERVISED LEARNING Grouping unlabeled items using k-means clustering Association analysis with the Apriori algorithm Efficiently finding frequent itemsets with FP-growth PART 4 ADDITIONAL TOOLS Using principal component analysis to simplify data Simplifying data with the singular value decomposition Big data and MapReduce",
            "referenceCount": 0,
            "citationCount": 488,
            "influentialCitationCount": 47,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Harrington2012MachineLI,\n author = {P. B. Harrington},\n title = {Machine Learning in Action},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:498ca0a1f8c980586408addf7ab2919ecdb7dd3d",
            "@type": "ScholarlyArticle",
            "paperId": "498ca0a1f8c980586408addf7ab2919ecdb7dd3d",
            "corpusId": 6348464,
            "url": "https://www.semanticscholar.org/paper/498ca0a1f8c980586408addf7ab2919ecdb7dd3d",
            "title": "Factorizing YAGO: scalable machine learning for linked data",
            "venue": "The Web Conference",
            "publicationVenue": {
                "id": "urn:research:e07422f9-c065-40c3-a37b-75e98dce79fe",
                "name": "The Web Conference",
                "alternate_names": [
                    "Web Conf",
                    "WWW"
                ],
                "issn": null,
                "url": "http://www.iw3c2.org/"
            },
            "year": 2012,
            "externalIds": {
                "MAG": "2099752825",
                "DBLP": "conf/www/NickelTK12",
                "DOI": "10.1145/2187836.2187874",
                "CorpusId": 6348464
            },
            "abstract": "Vast amounts of structured information have been published in the Semantic Web's Linked Open Data (LOD) cloud and their size is still growing rapidly. Yet, access to this information via reasoning and querying is sometimes difficult, due to LOD's size, partial data inconsistencies and inherent noisiness. Machine Learning offers an alternative approach to exploiting LOD's data with the advantages that Machine Learning algorithms are typically robust to both noise and data inconsistencies and are able to efficiently utilize non-deterministic dependencies in the data. From a Machine Learning point of view, LOD is challenging due to its relational nature and its scale. Here, we present an efficient approach to relational learning on LOD data, based on the factorization of a sparse tensor that scales to data consisting of millions of entities, hundreds of relations and billions of known facts. Furthermore, we show how ontological knowledge can be incorporated in the factorization to improve learning results and how computation can be distributed across multiple nodes. We demonstrate that our approach is able to factorize the YAGO~2 core ontology and globally predict statements for this large knowledge base using a single dual-core desktop computer. Furthermore, we show experimentally that our approach achieves good results in several relational learning tasks that are relevant to Linked Data. Once a factorization has been computed, our model is able to predict efficiently, and without any additional training, the likelihood of any of the 4.3 \u22c5 1014 possible triples in the YAGO~2 core ontology.",
            "referenceCount": 38,
            "citationCount": 411,
            "influentialCitationCount": 48,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Book",
                "Conference"
            ],
            "publicationDate": "2012-04-16",
            "journal": {
                "name": "Proceedings of the 21st international conference on World Wide Web",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Nickel2012FactorizingYS,\n author = {Maximilian Nickel and Volker Tresp and H. Kriegel},\n booktitle = {The Web Conference},\n journal = {Proceedings of the 21st international conference on World Wide Web},\n title = {Factorizing YAGO: scalable machine learning for linked data},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6d431f835c06afdea45dff6b24486bf301ebdef0",
            "@type": "ScholarlyArticle",
            "paperId": "6d431f835c06afdea45dff6b24486bf301ebdef0",
            "corpusId": 10175374,
            "url": "https://www.semanticscholar.org/paper/6d431f835c06afdea45dff6b24486bf301ebdef0",
            "title": "An Overview of Multi-Task Learning in Deep Neural Networks",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "journals/corr/Ruder17a",
                "ArXiv": "1706.05098",
                "MAG": "2624871570",
                "CorpusId": 10175374
            },
            "abstract": "Multi-task learning (MTL) has led to successes in many applications of machine learning, from natural language processing and speech recognition to computer vision and drug discovery. This article aims to give a general overview of MTL, particularly in deep neural networks. It introduces the two most common methods for MTL in Deep Learning, gives an overview of the literature, and discusses recent advances. In particular, it seeks to help ML practitioners apply MTL by shedding light on how MTL works and providing guidelines for choosing appropriate auxiliary tasks.",
            "referenceCount": 58,
            "citationCount": 2312,
            "influentialCitationCount": 152,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2017-06-15",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1706.05098"
            },
            "citationStyles": {
                "bibtex": "@Article{Ruder2017AnOO,\n author = {Sebastian Ruder},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {An Overview of Multi-Task Learning in Deep Neural Networks},\n volume = {abs/1706.05098},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ca011427853d34ce4ec9ccafde8a70c9eacc3e21",
            "@type": "ScholarlyArticle",
            "paperId": "ca011427853d34ce4ec9ccafde8a70c9eacc3e21",
            "corpusId": 3557281,
            "url": "https://www.semanticscholar.org/paper/ca011427853d34ce4ec9ccafde8a70c9eacc3e21",
            "title": "Deep Learning for Computer Vision: A Brief Review",
            "venue": "Computational Intelligence and Neuroscience",
            "publicationVenue": {
                "id": "urn:research:f32b7322-b69c-4e63-801d-8f50784ef778",
                "name": "Computational Intelligence and Neuroscience",
                "alternate_names": [
                    "Comput Intell Neurosci"
                ],
                "issn": "1687-5265",
                "url": "https://www.hindawi.com/journals/cin/"
            },
            "year": 2018,
            "externalIds": {
                "PubMedCentral": "5816885",
                "DBLP": "journals/cin/VoulodimosDDP18",
                "MAG": "2794284562",
                "DOI": "10.1155/2018/7068349",
                "CorpusId": 3557281,
                "PubMed": "29487619"
            },
            "abstract": "Over the last years deep learning methods have been shown to outperform previous state-of-the-art machine learning techniques in several fields, with computer vision being one of the most prominent cases. This review paper provides a brief overview of some of the most significant deep learning schemes used in computer vision problems, that is, Convolutional Neural Networks, Deep Boltzmann Machines and Deep Belief Networks, and Stacked Denoising Autoencoders. A brief account of their history, structure, advantages, and limitations is given, followed by a description of their applications in various computer vision tasks, such as object detection, face recognition, action and activity recognition, and human pose estimation. Finally, a brief overview is given of future directions in designing deep learning schemes for computer vision problems and the challenges involved therein.",
            "referenceCount": 115,
            "citationCount": 2019,
            "influentialCitationCount": 29,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://downloads.hindawi.com/journals/cin/2018/7068349.pdf",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2018-02-01",
            "journal": {
                "name": "Computational Intelligence and Neuroscience",
                "volume": "2018"
            },
            "citationStyles": {
                "bibtex": "@Article{Voulodimos2018DeepLF,\n author = {A. Voulodimos and N. Doulamis and A. Doulamis and Eftychios E. Protopapadakis},\n booktitle = {Computational Intelligence and Neuroscience},\n journal = {Computational Intelligence and Neuroscience},\n title = {Deep Learning for Computer Vision: A Brief Review},\n volume = {2018},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:fc2057499fcc5dfe61316f1722db7837971a9c94",
            "@type": "ScholarlyArticle",
            "paperId": "fc2057499fcc5dfe61316f1722db7837971a9c94",
            "corpusId": 6429534,
            "url": "https://www.semanticscholar.org/paper/fc2057499fcc5dfe61316f1722db7837971a9c94",
            "title": "ML Confidential: Machine Learning on Encrypted Data",
            "venue": "International Conference on Information Security and Cryptology",
            "publicationVenue": {
                "id": "urn:research:7bf51cb8-4a54-4bc9-919c-8f67d9f7d0ae",
                "name": "International Conference on Information Security and Cryptology",
                "alternate_names": [
                    "IEEE Int Conf Intell Syst Control",
                    "ICISC",
                    "IEEE International Conference on Intelligent Systems and Control",
                    "Int Conf Inf Secur Cryptol"
                ],
                "issn": null,
                "url": "http://www.wikicfp.com/cfp/program?id=1397"
            },
            "year": 2012,
            "externalIds": {
                "MAG": "133884053",
                "DBLP": "journals/iacr/GraepelLN12",
                "DOI": "10.1007/978-3-642-37682-5_1",
                "CorpusId": 6429534
            },
            "abstract": null,
            "referenceCount": 27,
            "citationCount": 446,
            "influentialCitationCount": 27,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://eprint.iacr.org/2012/323.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2012-11-28",
            "journal": {
                "name": "IACR Cryptol. ePrint Arch.",
                "volume": "2012"
            },
            "citationStyles": {
                "bibtex": "@Article{Graepel2012MLCM,\n author = {T. Graepel and K. Lauter and M. Naehrig},\n booktitle = {International Conference on Information Security and Cryptology},\n journal = {IACR Cryptol. ePrint Arch.},\n pages = {323},\n title = {ML Confidential: Machine Learning on Encrypted Data},\n volume = {2012},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e3948c28d605e0d90e88e160556cfc14fbba57c8",
            "@type": "ScholarlyArticle",
            "paperId": "e3948c28d605e0d90e88e160556cfc14fbba57c8",
            "corpusId": 2235233,
            "url": "https://www.semanticscholar.org/paper/e3948c28d605e0d90e88e160556cfc14fbba57c8",
            "title": "Incremental and Decremental Support Vector Machine Learning",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2000,
            "externalIds": {
                "MAG": "2108807072",
                "DBLP": "conf/nips/CauwenberghsP00",
                "CorpusId": 2235233
            },
            "abstract": "An on-line recursive algorithm for training support vector machines, one vector at a time, is presented. Adiabatic increments retain the Kuhn-Tucker conditions on all previously seen training data, in a number of steps each computed analytically. The incremental procedure is reversible, and decremental \"unlearning\" offers an efficient method to exactly evaluate leave-one-out generalization performance. Interpretation of decremental unlearning in feature space sheds light on the relationship between generalization and geometry of the data.",
            "referenceCount": 17,
            "citationCount": 1385,
            "influentialCitationCount": 137,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Cauwenberghs2000IncrementalAD,\n author = {G. Cauwenberghs and T. Poggio},\n booktitle = {Neural Information Processing Systems},\n pages = {409-415},\n title = {Incremental and Decremental Support Vector Machine Learning},\n year = {2000}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d2972fa779c91162f447d1e15540fba0df4cb547",
            "@type": "ScholarlyArticle",
            "paperId": "d2972fa779c91162f447d1e15540fba0df4cb547",
            "corpusId": 4474084,
            "url": "https://www.semanticscholar.org/paper/d2972fa779c91162f447d1e15540fba0df4cb547",
            "title": "Deploying an interactive machine learning system in an evidence-based practice center: abstrackr",
            "venue": "International Health Informatics Symposium",
            "publicationVenue": {
                "id": "urn:research:83ad896c-534f-4ee5-b0a9-310a08e91b3a",
                "name": "International Health Informatics Symposium",
                "alternate_names": [
                    "IHI",
                    "Int Health Informatics Symp"
                ],
                "issn": null,
                "url": "http://www.wikicfp.com/cfp/program?id=1551"
            },
            "year": 2012,
            "externalIds": {
                "MAG": "2100053037",
                "DBLP": "conf/ihi/WallaceSBLT12",
                "DOI": "10.1145/2110363.2110464",
                "CorpusId": 4474084
            },
            "abstract": "Medical researchers looking for evidence pertinent to a specific clinical question must navigate an increasingly voluminous corpus of published literature. This data deluge has motivated the development of machine learning and data mining technologies to facilitate efficient biomedical research. Despite the obvious labor-saving potential of these technologies and the concomitant academic interest therein, however, adoption of machine learning techniques by medical researchers has been relatively sluggish. One explanation for this is that while many machine learning methods have been proposed and retrospectively evaluated, they are rarely (if ever) actually made accessible to the practitioners whom they would benefit. In this work, we describe the ongoing development of an end-to-end interactive machine learning system at the Tufts Evidence-based Practice Center. More specifically, we have developed abstrackr, an online tool for the task of citation screening for systematic reviews. This tool provides an interface to our machine learning methods. The main aim of this work is to provide a case study in deploying cutting-edge machine learning methods that will actually be used by experts in a clinical research setting.",
            "referenceCount": 15,
            "citationCount": 442,
            "influentialCitationCount": 19,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2012-01-28",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Wallace2012DeployingAI,\n author = {Byron C. Wallace and Kevin Small and C. Brodley and J. Lau and T. Trikalinos},\n booktitle = {International Health Informatics Symposium},\n pages = {819-824},\n title = {Deploying an interactive machine learning system in an evidence-based practice center: abstrackr},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:99a39ea1b9bf4e3afc422329c1d4d77446f060b8",
            "@type": "ScholarlyArticle",
            "paperId": "99a39ea1b9bf4e3afc422329c1d4d77446f060b8",
            "corpusId": 14024661,
            "url": "https://www.semanticscholar.org/paper/99a39ea1b9bf4e3afc422329c1d4d77446f060b8",
            "title": "Machine Learning Strategies for Time Series Forecasting",
            "venue": "European Business Intelligence Summer School",
            "publicationVenue": {
                "id": "urn:research:0306a9d0-6dc5-446d-8606-7b85fd27634a",
                "name": "European Business Intelligence Summer School",
                "alternate_names": [
                    "eBISS",
                    "Eur Bus Intell Summer Sch"
                ],
                "issn": null,
                "url": null
            },
            "year": 2012,
            "externalIds": {
                "DBLP": "conf/ebiss/BontempiTB12",
                "MAG": "2284097410",
                "DOI": "10.1007/978-3-642-36318-4_3",
                "CorpusId": 14024661
            },
            "abstract": null,
            "referenceCount": 58,
            "citationCount": 415,
            "influentialCitationCount": 11,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Bontempi2012MachineLS,\n author = {Gianluca Bontempi and S. B. Taieb and Y. Borgne},\n booktitle = {European Business Intelligence Summer School},\n pages = {62-77},\n title = {Machine Learning Strategies for Time Series Forecasting},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:02227c94dd41fe0b439e050d377b0beb5d427cda",
            "@type": "ScholarlyArticle",
            "paperId": "02227c94dd41fe0b439e050d377b0beb5d427cda",
            "corpusId": 16852518,
            "url": "https://www.semanticscholar.org/paper/02227c94dd41fe0b439e050d377b0beb5d427cda",
            "title": "Reading Digits in Natural Images with Unsupervised Feature Learning",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2011,
            "externalIds": {
                "MAG": "2335728318",
                "CorpusId": 16852518
            },
            "abstract": "Detecting and reading text from natural images is a hard computer vision task that is central to a variety of emerging applications. Related problems like document character recognition have been widely studied by computer vision and machine learning researchers and are virtually solved for practical applications like reading handwritten digits. Reliably recognizing characters in more complex scenes like photographs, however, is far more difficult: the best existing methods lag well behind human performance on the same tasks. In this paper we attack the problem of recognizing digits in a real application using unsupervised feature learning methods: reading house numbers from street level photos. To this end, we introduce a new benchmark dataset for research use containing over 600,000 labeled digits cropped from Street View images. We then demonstrate the difficulty of recognizing these digits when the problem is approached with hand-designed features. Finally, we employ variants of two recently proposed unsupervised feature learning methods and find that they are convincingly superior on our benchmarks.",
            "referenceCount": 28,
            "citationCount": 5525,
            "influentialCitationCount": 1917,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Netzer2011ReadingDI,\n author = {Yuval Netzer and Tao Wang and Adam Coates and A. Bissacco and Bo Wu and A. Ng},\n title = {Reading Digits in Natural Images with Unsupervised Feature Learning},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d837267b364b4dc97bb35facda235a19be5ed374",
            "@type": "ScholarlyArticle",
            "paperId": "d837267b364b4dc97bb35facda235a19be5ed374",
            "corpusId": 36790775,
            "url": "https://www.semanticscholar.org/paper/d837267b364b4dc97bb35facda235a19be5ed374",
            "title": "Machine Learning in Non-Stationary Environments - Introduction to Covariate Shift Adaptation",
            "venue": "Adaptive computation and machine learning",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2012,
            "externalIds": {
                "MAG": "1493730910",
                "DBLP": "books/daglib/0034863",
                "CorpusId": 36790775
            },
            "abstract": "As the power of computing has grown over the past few decades, the field of machine learning has advanced rapidly in both theory and practice. Machine learning methods are usually based on the assumption that the data generation mechanism does not change over time. Yet real-world applications of machine learning, including image recognition, natural language processing, speech recognition, robot control, and bioinformatics, often violate this common assumption. Dealing with non-stationarity is one of modern machine learning's greatest challenges. This book focuses on a specific non-stationary environment known as covariate shift, in which the distributions of inputs (queries) change but the conditional distribution of outputs (answers) is unchanged, and presents machine learning theory, algorithms, and applications to overcome this variety of non-stationarity. After reviewing the state-of-the-art research in the field, the authors discuss topics that include learning under covariate shift, model selection, importance estimation, and active learning. They describe such real world applications of covariate shift adaption as brain-computer interface, speaker identification, and age prediction from facial images. With this book, they aim to encourage future research in machine learning, statistics, and engineering that strives to create truly autonomous learning machines able to learn under non-stationarity.",
            "referenceCount": 0,
            "citationCount": 377,
            "influentialCitationCount": 25,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "2012-03-30",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Sugiyama2012MachineLI,\n author = {Masashi Sugiyama and M. Kawanabe},\n booktitle = {Adaptive computation and machine learning},\n pages = {I-XIV, 1-261},\n title = {Machine Learning in Non-Stationary Environments - Introduction to Covariate Shift Adaptation},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1d122a074c936fcfd95faf44608e377a9d1799c8",
            "@type": "ScholarlyArticle",
            "paperId": "1d122a074c936fcfd95faf44608e377a9d1799c8",
            "corpusId": 970388,
            "url": "https://www.semanticscholar.org/paper/1d122a074c936fcfd95faf44608e377a9d1799c8",
            "title": "DeepFM: A Factorization-Machine based Neural Network for CTR Prediction",
            "venue": "International Joint Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:67f7f831-711a-43c8-8785-1e09005359b5",
                "name": "International Joint Conference on Artificial Intelligence",
                "alternate_names": [
                    "Int Jt Conf Artif Intell",
                    "IJCAI"
                ],
                "issn": null,
                "url": "http://www.ijcai.org/"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "conf/ijcai/GuoTYLH17",
                "ArXiv": "1703.04247",
                "MAG": "2951001079",
                "DOI": "10.24963/ijcai.2017/239",
                "CorpusId": 970388
            },
            "abstract": "Learning sophisticated feature interactions behind user behaviors is critical in maximizing CTR for recommender systems. Despite great progress, existing methods seem to have a strong bias towards low- or high-order interactions, or require expertise feature engineering. In this paper, we show that it is possible to derive an end-to-end learning model that emphasizes both low- and high-order feature interactions. The proposed model, DeepFM, combines the power of factorization machines for recommendation and deep learning for feature learning in a new neural network architecture. Compared to the latest Wide & Deep model from Google, DeepFM has a shared input to its \"wide\" and \"deep\" parts, with no need of feature engineering besides raw features. Comprehensive experiments are conducted to demonstrate the effectiveness and efficiency of DeepFM over the existing models for CTR prediction, on both benchmark data and commercial data.",
            "referenceCount": 33,
            "citationCount": 2024,
            "influentialCitationCount": 385,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.ijcai.org/proceedings/2017/0239.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-03-13",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1703.04247"
            },
            "citationStyles": {
                "bibtex": "@Article{Guo2017DeepFMAF,\n author = {Huifeng Guo and Ruiming Tang and Yunming Ye and Zhenguo Li and Xiuqiang He},\n booktitle = {International Joint Conference on Artificial Intelligence},\n journal = {ArXiv},\n title = {DeepFM: A Factorization-Machine based Neural Network for CTR Prediction},\n volume = {abs/1703.04247},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3c8bf504ddc7db1829466b6e9da5251025dd48f1",
            "@type": "ScholarlyArticle",
            "paperId": "3c8bf504ddc7db1829466b6e9da5251025dd48f1",
            "corpusId": 14442835,
            "url": "https://www.semanticscholar.org/paper/3c8bf504ddc7db1829466b6e9da5251025dd48f1",
            "title": "Automatic analysis of malware behavior using machine learning",
            "venue": "Journal of computing and security",
            "publicationVenue": {
                "id": "urn:research:2ef3d692-67d8-4150-8e21-55b389e72376",
                "name": "Journal of computing and security",
                "alternate_names": [
                    "J comput secur",
                    "Journal of Computer Security",
                    "J Comput Secur"
                ],
                "issn": "2322-4460",
                "url": "http://jcomsec.org/"
            },
            "year": 2011,
            "externalIds": {
                "DBLP": "journals/jcs/RieckTWH11",
                "MAG": "1851403712",
                "DOI": "10.3233/JCS-2010-0410",
                "CorpusId": 14442835
            },
            "abstract": "Malicious software - so called malware - poses a major threat to the security of computer systems. The amount and diversity of its variants render classic security defenses ineffective, such that millions of hosts in the Internet are infected with malware in the form of computer viruses, Internet worms and Trojan horses. While obfuscation and polymorphism employed by malware largely impede detection at file level, the dynamic analysis of malware binaries during run-time provides an instrument for characterizing and defending against the threat of malicious software. \n \nIn this article, we propose a framework for the automatic analysis of malware behavior using machine learning. The framework allows for automatically identifying novel classes of malware with similar behavior (clustering) and assigning unknown malware to these discovered classes (classification). Based on both, clustering and classification, we propose an incremental approach for behavior-based analysis, capable of processing the behavior of thousands of malware binaries on a daily basis. The incremental analysis significantly reduces the run-time overhead of current analysis methods, while providing accurate discovery and discrimination of novel malware variants.",
            "referenceCount": 70,
            "citationCount": 717,
            "influentialCitationCount": 64,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.eecs.tu-berlin.de/fileadmin/f4/TechReports/2009/tr-2009-18.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2011-12-01",
            "journal": {
                "name": "J. Comput. Secur.",
                "volume": "19"
            },
            "citationStyles": {
                "bibtex": "@Article{Rieck2011AutomaticAO,\n author = {Konrad Rieck and Philipp Trinius and Carsten Willems and Thorsten Holz},\n booktitle = {Journal of computing and security},\n journal = {J. Comput. Secur.},\n pages = {639-668},\n title = {Automatic analysis of malware behavior using machine learning},\n volume = {19},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0e779fd59353a7f1f5b559b9d65fa4bfe367890c",
            "@type": "ScholarlyArticle",
            "paperId": "0e779fd59353a7f1f5b559b9d65fa4bfe367890c",
            "corpusId": 15195762,
            "url": "https://www.semanticscholar.org/paper/0e779fd59353a7f1f5b559b9d65fa4bfe367890c",
            "title": "Geometric Deep Learning: Going beyond Euclidean data",
            "venue": "IEEE Signal Processing Magazine",
            "publicationVenue": {
                "id": "urn:research:f62e5eab-173a-4e0a-a963-ed8de9835d22",
                "name": "IEEE Signal Processing Magazine",
                "alternate_names": [
                    "IEEE Signal Process Mag"
                ],
                "issn": "1053-5888",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=79"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "journals/spm/BronsteinBLSV17",
                "MAG": "3102013575",
                "ArXiv": "1611.08097",
                "DOI": "10.1109/MSP.2017.2693418",
                "CorpusId": 15195762
            },
            "abstract": "Many scientific fields study data with an underlying structure that is non-Euclidean. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions) and are natural targets for machine-learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural-language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure and in cases where the invariances of these structures are built into networks used to model them.",
            "referenceCount": 125,
            "citationCount": 2753,
            "influentialCitationCount": 155,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1611.08097",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-11-24",
            "journal": {
                "name": "IEEE Signal Process. Mag.",
                "volume": "34"
            },
            "citationStyles": {
                "bibtex": "@Article{Bronstein2016GeometricDL,\n author = {M. Bronstein and Joan Bruna and Yann LeCun and Arthur Szlam and P. Vandergheynst},\n booktitle = {IEEE Signal Processing Magazine},\n journal = {IEEE Signal Process. Mag.},\n pages = {18-42},\n title = {Geometric Deep Learning: Going beyond Euclidean data},\n volume = {34},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:222c68fc80b05064680ddc467243b620604fd11e",
            "@type": "ScholarlyArticle",
            "paperId": "222c68fc80b05064680ddc467243b620604fd11e",
            "corpusId": 206869445,
            "url": "https://www.semanticscholar.org/paper/222c68fc80b05064680ddc467243b620604fd11e",
            "title": "Machine learning and radiology",
            "venue": "Medical Image Anal.",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2012,
            "externalIds": {
                "DBLP": "journals/mia/WangS12",
                "MAG": "2069816479",
                "DOI": "10.1016/j.media.2012.02.005",
                "CorpusId": 206869445,
                "PubMed": "22465077"
            },
            "abstract": null,
            "referenceCount": 261,
            "citationCount": 529,
            "influentialCitationCount": 14,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://europepmc.org/articles/pmc3372692?pdf=render",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2012-07-01",
            "journal": {
                "name": "Medical image analysis",
                "volume": "16 5"
            },
            "citationStyles": {
                "bibtex": "@Article{Wang2012MachineLA,\n author = {Shijun Wang and R. Summers},\n booktitle = {Medical Image Anal.},\n journal = {Medical image analysis},\n pages = {\n          933-51\n        },\n title = {Machine learning and radiology},\n volume = {16 5},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8729441d734782c3ed532a7d2d9611b438c0a09a",
            "@type": "ScholarlyArticle",
            "paperId": "8729441d734782c3ed532a7d2d9611b438c0a09a",
            "corpusId": 7365802,
            "url": "https://www.semanticscholar.org/paper/8729441d734782c3ed532a7d2d9611b438c0a09a",
            "title": "ADADELTA: An Adaptive Learning Rate Method",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2012,
            "externalIds": {
                "ArXiv": "1212.5701",
                "MAG": "6908809",
                "DBLP": "journals/corr/abs-1212-5701",
                "CorpusId": 7365802
            },
            "abstract": "We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.",
            "referenceCount": 8,
            "citationCount": 6272,
            "influentialCitationCount": 917,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2012-12-22",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1212.5701"
            },
            "citationStyles": {
                "bibtex": "@Article{Zeiler2012ADADELTAAA,\n author = {Matthew D. Zeiler},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {ADADELTA: An Adaptive Learning Rate Method},\n volume = {abs/1212.5701},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:01f702f8b1f9d1314587015f1f038af4d5735e77",
            "@type": "ScholarlyArticle",
            "paperId": "01f702f8b1f9d1314587015f1f038af4d5735e77",
            "corpusId": 10551030,
            "url": "https://www.semanticscholar.org/paper/01f702f8b1f9d1314587015f1f038af4d5735e77",
            "title": "Opposition-Based Learning: A New Scheme for Machine Intelligence",
            "venue": "International Conference on Computational Intelligence for Modelling, Control and Automation and International Conference on Intelligent Agents, Web Technologies and Internet Commerce (CIMCA-IAWTIC'06)",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2005,
            "externalIds": {
                "MAG": "2162745921",
                "DBLP": "conf/cimca/Tizhoosh05",
                "DOI": "10.1109/CIMCA.2005.1631345",
                "CorpusId": 10551030
            },
            "abstract": "Opposition-based learning as a new scheme for machine intelligence is introduced. Estimates and counter-estimates, weights and opposite weights, and actions versus counter-actions are the foundation of this new approach. Examples are provided. Possibilities for extensions of existing learning algorithms are discussed. Preliminary results are provided",
            "referenceCount": 7,
            "citationCount": 1564,
            "influentialCitationCount": 177,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2005-11-28",
            "journal": {
                "name": "International Conference on Computational Intelligence for Modelling, Control and Automation and International Conference on Intelligent Agents, Web Technologies and Internet Commerce (CIMCA-IAWTIC'06)",
                "volume": "1"
            },
            "citationStyles": {
                "bibtex": "@Article{Tizhoosh2005OppositionBasedLA,\n author = {H. Tizhoosh},\n booktitle = {International Conference on Computational Intelligence for Modelling, Control and Automation and International Conference on Intelligent Agents, Web Technologies and Internet Commerce (CIMCA-IAWTIC'06)},\n journal = {International Conference on Computational Intelligence for Modelling, Control and Automation and International Conference on Intelligent Agents, Web Technologies and Internet Commerce (CIMCA-IAWTIC'06)},\n pages = {695-701},\n title = {Opposition-Based Learning: A New Scheme for Machine Intelligence},\n volume = {1},\n year = {2005}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ce0b8b6fca7dc089548cc2e9aaac3bae82bb19da",
            "@type": "ScholarlyArticle",
            "paperId": "ce0b8b6fca7dc089548cc2e9aaac3bae82bb19da",
            "corpusId": 9237797,
            "url": "https://www.semanticscholar.org/paper/ce0b8b6fca7dc089548cc2e9aaac3bae82bb19da",
            "title": "Making machine learning models interpretable",
            "venue": "The European Symposium on Artificial Neural Networks",
            "publicationVenue": {
                "id": "urn:research:93d6c444-c90a-48ee-a3ad-ef1f015bc28a",
                "name": "The European Symposium on Artificial Neural Networks",
                "alternate_names": [
                    "Eur Symp Artif Neural Netw",
                    "ESANN"
                ],
                "issn": null,
                "url": "https://www.esann.org/"
            },
            "year": 2012,
            "externalIds": {
                "MAG": "814282759",
                "DBLP": "conf/esann/VellidoML12",
                "CorpusId": 9237797
            },
            "abstract": "Data of different levels of complexity and of ever growing diversity of characteristics are the raw materials that machine learning practitioners try to model using their wide palette of methods and tools. The obtained models are meant to be a synthetic representation of the available, observed data that captures some of their intrinsic regularities or patterns. Therefore, the use of machine learning techniques for data analysis can be understood as a problem of pattern recognition or, more informally, of knowledge discovery and data mining. There exists a gap, though, between data modeling and knowledge extraction. Models, de- pending on the machine learning techniques employed, can be described in diverse ways but, in order to consider that some knowledge has been achieved from their description, we must take into account the human cog- nitive factor that any knowledge extraction process entails. These models as such can be rendered powerless unless they can be interpreted ,a nd the process of human interpretation follows rules that go well beyond techni- cal prowess. For this reason, interpretability is a paramount quality that machine learning methods should aim to achieve if they are to be applied in practice. This paper is a brief introduction to the special session on interpretable models in machine learning, organized as part of the 20 th European Symposium on Artificial Neural Networks, Computational In- telligence and Machine Learning. It includes a discussion on the several works accepted for the session, with an overview of the context of wider research on interpretability of machine learning models.",
            "referenceCount": 49,
            "citationCount": 289,
            "influentialCitationCount": 7,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Article{Vellido2012MakingML,\n author = {A. Vellido and J. Mart\u00edn-Guerrero and P. Lisboa},\n booktitle = {The European Symposium on Artificial Neural Networks},\n pages = {163-172},\n title = {Making machine learning models interpretable},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7fe7e80bf59a112386211b38ef2ea0b71ae76345",
            "@type": "ScholarlyArticle",
            "paperId": "7fe7e80bf59a112386211b38ef2ea0b71ae76345",
            "corpusId": 3362257,
            "url": "https://www.semanticscholar.org/paper/7fe7e80bf59a112386211b38ef2ea0b71ae76345",
            "title": "Machine Learning that Matters",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2012,
            "externalIds": {
                "DBLP": "conf/icml/Wagstaff12",
                "ArXiv": "1206.4656",
                "MAG": "1754293002",
                "CorpusId": 3362257
            },
            "abstract": "Much of current machine learning (ML) research has lost its connection to problems of import to the larger world of science and society. From this perspective, there exist glaring limitations in the data sets we investigate, the metrics we employ for evaluation, and the degree to which results are communicated back to their originating domains. What changes are needed to how we conduct research to increase the impact that ML has? We present six Impact Challenges to explicitly focus the field's energy and attention, and we discuss existing obstacles that must be addressed. We aim to inspire ongoing discussion and focus on ML that matters.",
            "referenceCount": 22,
            "citationCount": 272,
            "influentialCitationCount": 19,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2012-06-18",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1206.4656"
            },
            "citationStyles": {
                "bibtex": "@Article{Wagstaff2012MachineLT,\n author = {K. Wagstaff},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Machine Learning that Matters},\n volume = {abs/1206.4656},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8e58dc63817a2a26e5a2ddad38d8b1d19d1c3795",
            "@type": "ScholarlyArticle",
            "paperId": "8e58dc63817a2a26e5a2ddad38d8b1d19d1c3795",
            "corpusId": 208909851,
            "url": "https://www.semanticscholar.org/paper/8e58dc63817a2a26e5a2ddad38d8b1d19d1c3795",
            "title": "Machine Unlearning",
            "venue": "IEEE Symposium on Security and Privacy",
            "publicationVenue": {
                "id": "urn:research:29b9c461-963e-4d11-b2ab-92c182243942",
                "name": "IEEE Symposium on Security and Privacy",
                "alternate_names": [
                    "S&P",
                    "IEEE Symp Secur Priv"
                ],
                "issn": null,
                "url": "http://www.ieee-security.org/"
            },
            "year": 2019,
            "externalIds": {
                "ArXiv": "1912.03817",
                "DBLP": "journals/corr/abs-1912-03817",
                "DOI": "10.1109/SP40001.2021.00019",
                "CorpusId": 208909851
            },
            "abstract": "Once users have shared their data online, it is generally difficult for them to revoke access and ask for the data to be deleted. Machine learning (ML) exacerbates this problem because any model trained with said data may have memorized it, putting users at risk of a successful privacy attack exposing their information. Yet, having models unlearn is notoriously difficult.We introduce SISA training, a framework that expedites the unlearning process by strategically limiting the influence of a data point in the training procedure. While our framework is applicable to any learning algorithm, it is designed to achieve the largest improvements for stateful algorithms like stochastic gradient descent for deep neural networks. SISA training reduces the computational overhead associated with unlearning, even in the worst-case setting where unlearning requests are made uniformly across the training set. In some cases, the service provider may have a prior on the distribution of unlearning requests that will be issued by users. We may take this prior into account to partition and order data accordingly, and further decrease overhead from unlearning.Our evaluation spans several datasets from different domains, with corresponding motivations for unlearning. Under no distributional assumptions, for simple learning tasks, we observe that SISA training improves time to unlearn points from the Purchase dataset by 4.63\u00d7, and 2.45\u00d7 for the SVHN dataset, over retraining from scratch. SISA training also provides a speed-up of 1.36\u00d7 in retraining for complex learning tasks such as ImageNet classification; aided by transfer learning, this results in a small degradation in accuracy. Our work contributes to practical data governance in machine unlearning.",
            "referenceCount": 68,
            "citationCount": 296,
            "influentialCitationCount": 61,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-12-09",
            "journal": {
                "name": "2021 IEEE Symposium on Security and Privacy (SP)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Bourtoule2019MachineU,\n author = {Lucas Bourtoule and Varun Chandrasekaran and Christopher A. Choquette-Choo and Hengrui Jia and Adelin Travers and Baiwu Zhang and D. Lie and Nicolas Papernot},\n booktitle = {IEEE Symposium on Security and Privacy},\n journal = {2021 IEEE Symposium on Security and Privacy (SP)},\n pages = {141-159},\n title = {Machine Unlearning},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4187caa4d0d329f47e18377a6cd31ef3f580cfcc",
            "@type": "ScholarlyArticle",
            "paperId": "4187caa4d0d329f47e18377a6cd31ef3f580cfcc",
            "corpusId": 61494,
            "url": "https://www.semanticscholar.org/paper/4187caa4d0d329f47e18377a6cd31ef3f580cfcc",
            "title": "GraphLab: A New Framework For Parallel Machine Learning",
            "venue": "Conference on Uncertainty in Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:f9af8000-42f8-410d-a622-e8811e41660a",
                "name": "Conference on Uncertainty in Artificial Intelligence",
                "alternate_names": [
                    "Uncertainty in Artificial Intelligence",
                    "UAI",
                    "Conf Uncertain Artif Intell",
                    "Uncertain Artif Intell"
                ],
                "issn": null,
                "url": "http://www.auai.org/"
            },
            "year": 2010,
            "externalIds": {
                "DBLP": "conf/uai/LowGKBGH10",
                "ArXiv": "1006.4990",
                "MAG": "2951113132",
                "CorpusId": 61494
            },
            "abstract": "Designing and implementing efficient, provably correct parallel machine learning (ML) algorithms is challenging. Existing high-level parallel abstractions like MapReduce are insufficiently expressive while low-level tools like MPI and Pthreads leave ML experts repeatedly solving the same design challenges. By targeting common patterns in ML, we developed GraphLab, which improves upon abstractions like MapReduce by compactly expressing asynchronous iterative algorithms with sparse computational dependencies while ensuring data consistency and achieving a high degree of parallel performance. We demonstrate the expressiveness of the GraphLab framework by designing and implementing parallel versions of belief propagation, Gibbs sampling, Co-EM, Lasso and Compressed Sensing. We show that using GraphLab we can achieve excellent parallel performance on large scale real-world problems.",
            "referenceCount": 21,
            "citationCount": 891,
            "influentialCitationCount": 100,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2010-06-25",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1408.2041"
            },
            "citationStyles": {
                "bibtex": "@Article{Low2010GraphLabAN,\n author = {Yucheng Low and Joseph E. Gonzalez and Aapo Kyrola and Danny Bickson and Carlos Guestrin and J. Hellerstein},\n booktitle = {Conference on Uncertainty in Artificial Intelligence},\n journal = {ArXiv},\n title = {GraphLab: A New Framework For Parallel Machine Learning},\n volume = {abs/1408.2041},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ea58af907495e97c93997119db4a59fab5cd3683",
            "@type": "ScholarlyArticle",
            "paperId": "ea58af907495e97c93997119db4a59fab5cd3683",
            "corpusId": 10663248,
            "url": "https://www.semanticscholar.org/paper/ea58af907495e97c93997119db4a59fab5cd3683",
            "title": "Deep Machine Learning - A New Frontier in Artificial Intelligence Research [Research Frontier]",
            "venue": "IEEE Computational Intelligence Magazine",
            "publicationVenue": {
                "id": "urn:research:ee372de7-efda-4907-a03f-359292ea27f6",
                "name": "IEEE Computational Intelligence Magazine",
                "alternate_names": [
                    "IEEE Comput Intell Mag"
                ],
                "issn": "1556-603X",
                "url": "https://ieeexplore.ieee.org/servlet/opac?punumber=10207"
            },
            "year": 2010,
            "externalIds": {
                "MAG": "2182924040",
                "DBLP": "journals/cim/ArelRK10",
                "DOI": "10.1109/MCI.2010.938364",
                "CorpusId": 10663248
            },
            "abstract": "This article provides an overview of the mainstream deep learning approaches and research directions proposed over the past decade. It is important to emphasize that each approach has strengths and \"weaknesses, depending on the application and context in \"which it is being used. Thus, this article presents a summary on the current state of the deep machine learning field and some perspective into how it may evolve. Convolutional Neural Networks (CNNs) and Deep Belief Networks (DBNs) (and their respective variations) are focused on primarily because they are well established in the deep learning field and show great promise for future work.",
            "referenceCount": 56,
            "citationCount": 1119,
            "influentialCitationCount": 23,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2010-11-01",
            "journal": {
                "name": "IEEE Computational Intelligence Magazine",
                "volume": "5"
            },
            "citationStyles": {
                "bibtex": "@Article{Arel2010DeepML,\n author = {I. Arel and Derek C. Rose and T. Karnowski},\n booktitle = {IEEE Computational Intelligence Magazine},\n journal = {IEEE Computational Intelligence Magazine},\n pages = {13-18},\n title = {Deep Machine Learning - A New Frontier in Artificial Intelligence Research [Research Frontier]},\n volume = {5},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0d41dd8c5a1a1d78575bd4f4ca5d7af3d471839a",
            "@type": "ScholarlyArticle",
            "paperId": "0d41dd8c5a1a1d78575bd4f4ca5d7af3d471839a",
            "corpusId": 945193,
            "url": "https://www.semanticscholar.org/paper/0d41dd8c5a1a1d78575bd4f4ca5d7af3d471839a",
            "title": "Multiagent Systems: A Survey from a Machine Learning Perspective",
            "venue": "Auton. Robots",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2000,
            "externalIds": {
                "DBLP": "journals/arobots/StoneV00",
                "MAG": "211008150",
                "DOI": "10.1023/A:1008942012299",
                "CorpusId": 945193
            },
            "abstract": null,
            "referenceCount": 199,
            "citationCount": 1341,
            "influentialCitationCount": 54,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www-2.cs.cmu.edu/~mmv/papers/MASsurvey.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2000-06-01",
            "journal": {
                "name": "Autonomous Robots",
                "volume": "8"
            },
            "citationStyles": {
                "bibtex": "@Article{Stone2000MultiagentSA,\n author = {P. Stone and M. Veloso},\n booktitle = {Auton. Robots},\n journal = {Autonomous Robots},\n pages = {345-383},\n title = {Multiagent Systems: A Survey from a Machine Learning Perspective},\n volume = {8},\n year = {2000}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c8e4d8ded0624f13cd7763b8e7a62fe7e36da6d3",
            "@type": "ScholarlyArticle",
            "paperId": "c8e4d8ded0624f13cd7763b8e7a62fe7e36da6d3",
            "corpusId": 226931458,
            "url": "https://www.semanticscholar.org/paper/c8e4d8ded0624f13cd7763b8e7a62fe7e36da6d3",
            "title": "Generalizing from a Few Examples: A Survey on Few-Shot Learning",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2944378183",
                "ArXiv": "1904.05046",
                "CorpusId": 226931458
            },
            "abstract": "Machine learning has been highly successful in data-intensive applications but is often hampered when the data set is small. Recently, Few-Shot Learning (FSL) is proposed to tackle this problem. Using prior knowledge, FSL can rapidly generalize to new tasks containing only a few samples with supervised information. In this paper, we conduct a thorough survey to fully understand FSL. Starting from a formal definition of FSL, we distinguish FSL from several relevant machine learning problems. We then point out that the core issue in FSL is that the empirical risk minimized is unreliable. Based on how prior knowledge can be used to handle this core issue, we categorize FSL methods from three perspectives: (i) data, which uses prior knowledge to augment the supervised experience; (ii) model, which uses prior knowledge to reduce the size of the hypothesis space; and (iii) algorithm, which uses prior knowledge to alter the search for the best hypothesis in the given hypothesis space. With this taxonomy, we review and discuss the pros and cons of each category. Promising directions, in the aspects of the FSL problem setups, techniques, applications and theories, are also proposed to provide insights for future research.",
            "referenceCount": 193,
            "citationCount": 1486,
            "influentialCitationCount": 92,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "2019-04-10",
            "journal": {
                "name": "arXiv: Learning",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Article{Wang2019GeneralizingFA,\n author = {Yaqing Wang and Quanming Yao and J. Kwok and L. Ni},\n journal = {arXiv: Learning},\n title = {Generalizing from a Few Examples: A Survey on Few-Shot Learning},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a25fbcbbae1e8f79c4360d26aa11a3abf1a11972",
            "@type": "ScholarlyArticle",
            "paperId": "a25fbcbbae1e8f79c4360d26aa11a3abf1a11972",
            "corpusId": 740063,
            "url": "https://www.semanticscholar.org/paper/a25fbcbbae1e8f79c4360d26aa11a3abf1a11972",
            "title": "A Survey on Transfer Learning",
            "venue": "IEEE Transactions on Knowledge and Data Engineering",
            "publicationVenue": {
                "id": "urn:research:c6840156-ee10-4d78-8832-7f8909811576",
                "name": "IEEE Transactions on Knowledge and Data Engineering",
                "alternate_names": [
                    "IEEE Trans Knowl Data Eng"
                ],
                "issn": "1041-4347",
                "url": "https://www.computer.org/web/tkde"
            },
            "year": 2010,
            "externalIds": {
                "DBLP": "journals/tkde/PanY10",
                "MAG": "2165698076",
                "DOI": "10.1109/TKDE.2009.191",
                "CorpusId": 740063
            },
            "abstract": "A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data-labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression, and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as covariate shift. We also explore some potential future issues in transfer learning research.",
            "referenceCount": 97,
            "citationCount": 17861,
            "influentialCitationCount": 925,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2010-10-01",
            "journal": {
                "name": "IEEE Transactions on Knowledge and Data Engineering",
                "volume": "22"
            },
            "citationStyles": {
                "bibtex": "@Article{Pan2010ASO,\n author = {Sinno Jialin Pan and Qiang Yang},\n booktitle = {IEEE Transactions on Knowledge and Data Engineering},\n journal = {IEEE Transactions on Knowledge and Data Engineering},\n pages = {1345-1359},\n title = {A Survey on Transfer Learning},\n volume = {22},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:09c5931307cba3f80d3ecc14d02eecfa46463cfe",
            "@type": "ScholarlyArticle",
            "paperId": "09c5931307cba3f80d3ecc14d02eecfa46463cfe",
            "corpusId": 501756,
            "url": "https://www.semanticscholar.org/paper/09c5931307cba3f80d3ecc14d02eecfa46463cfe",
            "title": "MLPACK: a scalable C++ machine learning library",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2012,
            "externalIds": {
                "MAG": "2952062209",
                "DBLP": "journals/corr/abs-1210-6293",
                "ArXiv": "1210.6293",
                "DOI": "10.5555/2567709.2502606",
                "CorpusId": 501756
            },
            "abstract": "MLPACK is a state-of-the-art, scalable, multi-platform C++ machine learning library released in late 2011 offering both a simple, consistent API accessible to novice users and high performance and flexibility to expert users by leveraging modern features of C++. MLPACK provides cutting-edge algorithms whose benchmarks exhibit far better performance than other leading machine learning libraries. MLPACK version 1.0.3, licensed under the LGPL, is available at http://www.mlpack.org.",
            "referenceCount": 17,
            "citationCount": 163,
            "influentialCitationCount": 16,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2012-10-23",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1210.6293"
            },
            "citationStyles": {
                "bibtex": "@Article{Curtin2012MLPACKAS,\n author = {Ryan R. Curtin and J. R. Cline and N. P. Slagle and William B. March and P. Ram and Nishant A. Mehta and Alexander G. Gray},\n booktitle = {Journal of machine learning research},\n journal = {ArXiv},\n title = {MLPACK: a scalable C++ machine learning library},\n volume = {abs/1210.6293},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:58209c6db7b321ea7c75395b23ddb5100cd9bf81",
            "@type": "ScholarlyArticle",
            "paperId": "58209c6db7b321ea7c75395b23ddb5100cd9bf81",
            "corpusId": 3354477,
            "url": "https://www.semanticscholar.org/paper/58209c6db7b321ea7c75395b23ddb5100cd9bf81",
            "title": "Machine Learning for the New York City Power Grid",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "publicationVenue": {
                "id": "urn:research:25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "alternate_names": [
                    "IEEE Trans Pattern Anal Mach Intell"
                ],
                "issn": "0162-8828",
                "url": "http://www.computer.org/tpami/"
            },
            "year": 2012,
            "externalIds": {
                "MAG": "2066178851",
                "DBLP": "journals/pami/RudinWABSCDGHI12",
                "DOI": "10.1109/TPAMI.2011.108",
                "CorpusId": 3354477,
                "PubMed": "21576741"
            },
            "abstract": "Power companies can benefit from the use of knowledge discovery methods and statistical machine learning for preventive maintenance. We introduce a general process for transforming historical electrical grid data into models that aim to predict the risk of failures for components and systems. These models can be used directly by power companies to assist with prioritization of maintenance and repair work. Specialized versions of this process are used to produce (1) feeder failure rankings, (2) cable, joint, terminator, and transformer rankings, (3) feeder Mean Time Between Failure (MTBF) estimates, and (4) manhole events vulnerability rankings. The process in its most general form can handle diverse, noisy, sources that are historical (static), semi-real-time, or real-time, incorporates state-of-the-art machine learning algorithms for prioritization (supervised ranking or MTBF), and includes an evaluation of results via cross-validation and blind test. Above and beyond the ranked lists and MTBF estimates are business management interfaces that allow the prediction capability to be integrated directly into corporate planning and decision support; such interfaces rely on several important properties of our general modeling approach: that machine learning features are meaningful to domain experts, that the processing of data is transparent, and that prediction results are accurate enough to support sound decision making. We discuss the challenges in working with historical electrical grid data that were not designed for predictive purposes. The \u201crawness\u201d of these data contrasts with the accuracy of the statistical models that can be obtained from the process; these models are sufficiently accurate to assist in maintaining New York City's electrical grid.",
            "referenceCount": 47,
            "citationCount": 231,
            "influentialCitationCount": 7,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dspace.mit.edu/bitstream/1721.1/68634/1/TPAMI-2010-10-0753_R1_Rudin.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2012-02-01",
            "journal": {
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "volume": "34"
            },
            "citationStyles": {
                "bibtex": "@Article{Rudin2012MachineLF,\n author = {C. Rudin and D. Waltz and R. Anderson and A. Boulanger and Ansaf Salleb-Aouissi and M. Chow and Haimonti Dutta and Philip Gross and Bert Huang and Steve Ierome},\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n pages = {328-345},\n title = {Machine Learning for the New York City Power Grid},\n volume = {34},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:10bc3b9bf71c4448ea0f4bec441403c0e50a0691",
            "@type": "ScholarlyArticle",
            "paperId": "10bc3b9bf71c4448ea0f4bec441403c0e50a0691",
            "corpusId": 47125649,
            "url": "https://www.semanticscholar.org/paper/10bc3b9bf71c4448ea0f4bec441403c0e50a0691",
            "title": "Machine Learning - An Algorithmic Perspective",
            "venue": "Chapman and Hall / CRC machine learning and pattern recognition series",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2009,
            "externalIds": {
                "MAG": "1549998098",
                "DBLP": "books/daglib/0029547",
                "CorpusId": 47125649
            },
            "abstract": "Written in an easily accessible style, this book provides the ideal blend of theory and practical, applicable knowledge. It covers neural networks, graphical models, reinforcement learning, evolutionary algorithms, dimensionality reduction methods, and the important area of optimization. It treads the fine line between adequate academic rigor and overwhelming students with equations and mathematical concepts. The author includes examples based on widely available datasets and practical and theoretical problems to test understanding and application of the material. The book describes algorithms with code examples backed up by a website that provides working implementations in Python.",
            "referenceCount": 4,
            "citationCount": 1052,
            "influentialCitationCount": 125,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2009-04-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Marsland2009MachineL,\n author = {S. Marsland},\n booktitle = {Chapman and Hall / CRC machine learning and pattern recognition series},\n pages = {I-XVI, 1-390},\n title = {Machine Learning - An Algorithmic Perspective},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2bf7c350a8280e7c593d46a60127f99b21517121",
            "@type": "ScholarlyArticle",
            "paperId": "2bf7c350a8280e7c593d46a60127f99b21517121",
            "corpusId": 199528271,
            "url": "https://www.semanticscholar.org/paper/2bf7c350a8280e7c593d46a60127f99b21517121",
            "title": "On the Variance of the Adaptive Learning Rate and Beyond",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/corr/abs-1908-03265",
                "ArXiv": "1908.03265",
                "MAG": "2968917279",
                "CorpusId": 199528271
            },
            "abstract": "The learning rate warmup heuristic achieves remarkable success in stabilizing training, accelerating convergence and improving generalization for adaptive stochastic optimization algorithms like RMSprop and Adam. Here, we study its mechanism in details. Pursuing the theory behind warmup, we identify a problem of the adaptive learning rate (i.e., it has problematically large variance in the early stage), suggest warmup works as a variance reduction technique, and provide both empirical and theoretical evidence to verify our hypothesis. We further propose RAdam, a new variant of Adam, by introducing a term to rectify the variance of the adaptive learning rate. Extensive experimental results on image classification, language modeling, and neural machine translation verify our intuition and demonstrate the effectiveness and robustness of our proposed method. All implementations are available at: this https URL.",
            "referenceCount": 39,
            "citationCount": 1447,
            "influentialCitationCount": 218,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-08-08",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1908.03265"
            },
            "citationStyles": {
                "bibtex": "@Article{Liu2019OnTV,\n author = {Liyuan Liu and Haoming Jiang and Pengcheng He and Weizhu Chen and Xiaodong Liu and Jianfeng Gao and Jiawei Han},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {On the Variance of the Adaptive Learning Rate and Beyond},\n volume = {abs/1908.03265},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d50c8fd5448e4149b470aeb92eefb270088adc44",
            "@type": "ScholarlyArticle",
            "paperId": "d50c8fd5448e4149b470aeb92eefb270088adc44",
            "corpusId": 22084744,
            "url": "https://www.semanticscholar.org/paper/d50c8fd5448e4149b470aeb92eefb270088adc44",
            "title": "Machine Learning Methods for Ecological Applications",
            "venue": "Springer US",
            "publicationVenue": {
                "id": "urn:research:9e813668-7b2e-4506-9e7e-4a285f318e46",
                "name": "Springer US",
                "alternate_names": [
                    "Springer U"
                ],
                "issn": null,
                "url": null
            },
            "year": 2012,
            "externalIds": {
                "MAG": "1518242595",
                "DOI": "10.1007/978-1-4615-5289-5",
                "CorpusId": 22084744
            },
            "abstract": null,
            "referenceCount": 193,
            "citationCount": 213,
            "influentialCitationCount": 6,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Environmental Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "2012-10-29",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Fielding2012MachineLM,\n author = {A. Fielding},\n booktitle = {Springer US},\n title = {Machine Learning Methods for Ecological Applications},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3448e3c55cf3b1f25aab4719eb094a95dbe7f05e",
            "@type": "ScholarlyArticle",
            "paperId": "3448e3c55cf3b1f25aab4719eb094a95dbe7f05e",
            "corpusId": 208044535,
            "url": "https://www.semanticscholar.org/paper/3448e3c55cf3b1f25aab4719eb094a95dbe7f05e",
            "title": "A survey on semi-supervised learning",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2984353870",
                "DBLP": "journals/ml/EngelenH20",
                "DOI": "10.1007/s10994-019-05855-6",
                "CorpusId": 208044535
            },
            "abstract": null,
            "referenceCount": 225,
            "citationCount": 1271,
            "influentialCitationCount": 57,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/s10994-019-05855-6.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2019-11-15",
            "journal": {
                "name": "Machine Learning",
                "volume": "109"
            },
            "citationStyles": {
                "bibtex": "@Article{Engelen2019ASO,\n author = {Jesper E. van Engelen and H. Hoos},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {373-440},\n title = {A survey on semi-supervised learning},\n volume = {109},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c58390a5563b672bf02f7fc3f8ca264babf3cc3d",
            "@type": "ScholarlyArticle",
            "paperId": "c58390a5563b672bf02f7fc3f8ca264babf3cc3d",
            "corpusId": 263010642,
            "url": "https://www.semanticscholar.org/paper/c58390a5563b672bf02f7fc3f8ca264babf3cc3d",
            "title": "Foundations of Machine Learning",
            "venue": "Adaptive computation and machine learning",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2012,
            "externalIds": {
                "DBLP": "books/daglib/0034861",
                "MAG": "1944672",
                "CorpusId": 263010642
            },
            "abstract": "This graduate-level textbook introduces fundamental concepts and methods in machine learning. It describes several important modern algorithms, provides the theoretical underpinnings of these algorithms, and illustrates key aspects for their application. The authors aim to present novel theoretical tools and concepts while giving concise proofs even for relatively advanced topics. Foundations of Machine Learning fills the need for a general textbook that also offers theoretical details and an emphasis on proofs. Certain topics that are often treated with insufficient attention are discussed in more detail here; for example, entire chapters are devoted to regression, multi-class classification, and ranking. The first three chapters lay the theoretical foundation for what follows, but each remaining chapter is mostly self-contained. The appendix offers a concise probability review, a short introduction to convex optimization, tools for concentration bounds, and several basic properties of matrices and norms used in the book. The book is intended for graduate students and researchers in machine learning, statistics, and related areas; it can be used either as a textbook or as a reference text for a research seminar.",
            "referenceCount": 211,
            "citationCount": 95,
            "influentialCitationCount": 6,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "2012-08-17",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Mohri2012FoundationsOM,\n author = {M. Mohri and Afshin Rostamizadeh and Ameet Talwalkar},\n booktitle = {Adaptive computation and machine learning},\n pages = {I-XII, 1-412},\n title = {Foundations of Machine Learning},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1aad5969ca023d0aefe61d83a3cf6a3cb4d100e0",
            "@type": "ScholarlyArticle",
            "paperId": "1aad5969ca023d0aefe61d83a3cf6a3cb4d100e0",
            "corpusId": 14322423,
            "url": "https://www.semanticscholar.org/paper/1aad5969ca023d0aefe61d83a3cf6a3cb4d100e0",
            "title": "PANFIS: A Novel Incremental Learning Machine",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems",
            "publicationVenue": {
                "id": "urn:research:79c5a18d-0295-432c-aaa5-961d73de6d88",
                "name": "IEEE Transactions on Neural Networks and Learning Systems",
                "alternate_names": [
                    "IEEE Trans Neural Netw Learn Syst"
                ],
                "issn": "2162-237X",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5962385"
            },
            "year": 2014,
            "externalIds": {
                "MAG": "2052345845",
                "DBLP": "journals/tnn/PratamaAAL14",
                "DOI": "10.1109/TNNLS.2013.2271933",
                "CorpusId": 14322423,
                "PubMed": "24806644"
            },
            "abstract": "Most of the dynamics in real-world systems are compiled by shifts and drifts, which are uneasy to be overcome by omnipresent neuro-fuzzy systems. Nonetheless, learning in nonstationary environment entails a system owning high degree of flexibility capable of assembling its rule base autonomously according to the degree of nonlinearity contained in the system. In practice, the rule growing and pruning are carried out merely benefiting from a small snapshot of the complete training data to truncate the computational load and memory demand to the low level. An exposure of a novel algorithm, namely parsimonious network based on fuzzy inference system (PANFIS), is to this end presented herein. PANFIS can commence its learning process from scratch with an empty rule base. The fuzzy rules can be stitched up and expelled by virtue of statistical contributions of the fuzzy rules and injected datum afterward. Identical fuzzy sets may be alluded and blended to be one fuzzy set as a pursuit of a transparent rule base escalating human's interpretability. The learning and modeling performances of the proposed PANFIS are numerically validated using several benchmark problems from real-world or synthetic datasets. The validation includes comparisons with state-of-the-art evolving neuro-fuzzy methods and showcases that our new method can compete and in some cases even outperform these approaches in terms of predictive fidelity and model complexity.",
            "referenceCount": 65,
            "citationCount": 244,
            "influentialCitationCount": 10,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": "IEEE Transactions on Neural Networks and Learning Systems",
                "volume": "25"
            },
            "citationStyles": {
                "bibtex": "@Article{Pratama2014PANFISAN,\n author = {Mahardhika Pratama and S. Anavatti and P. Angelov and E. Lughofer},\n booktitle = {IEEE Transactions on Neural Networks and Learning Systems},\n journal = {IEEE Transactions on Neural Networks and Learning Systems},\n pages = {55-68},\n title = {PANFIS: A Novel Incremental Learning Machine},\n volume = {25},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3bff76c25f7c416834655ba664553b14eb67a11c",
            "@type": "ScholarlyArticle",
            "paperId": "3bff76c25f7c416834655ba664553b14eb67a11c",
            "corpusId": 7596571,
            "url": "https://www.semanticscholar.org/paper/3bff76c25f7c416834655ba664553b14eb67a11c",
            "title": "Sparse Bayesian Learning and the Relevance Vector Machine",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2001,
            "externalIds": {
                "MAG": "1648445109",
                "DBLP": "journals/jmlr/Tipping01",
                "DOI": "10.1162/15324430152748236",
                "CorpusId": 7596571
            },
            "abstract": "This paper introduces a general Bayesian framework for obtaining sparse solutions to regression and classification tasks utilising models linear in the parameters. Although this framework is fully general, we illustrate our approach with a particular specialisation that we denote the 'relevance vector machine' (RVM), a model of identical functional form to the popular and state-of-the-art 'support vector machine' (SVM). We demonstrate that by exploiting a probabilistic Bayesian learning framework, we can derive accurate prediction models which typically utilise dramatically fewer basis functions than a comparable SVM while offering a number of additional advantages. These include the benefits of probabilistic predictions, automatic estimation of 'nuisance' parameters, and the facility to utilise arbitrary basis functions (e.g. non-'Mercer' kernels). We detail the Bayesian framework and associated learning algorithm for the RVM, and give some illustrative examples of its application along with some comparative benchmarks. We offer some explanation for the exceptional degree of sparsity obtained, and discuss and demonstrate some of the advantageous features, and potential extensions, of Bayesian relevance learning.",
            "referenceCount": 39,
            "citationCount": 1716,
            "influentialCitationCount": 160,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2001-09-01",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "1"
            },
            "citationStyles": {
                "bibtex": "@Article{Tipping2001SparseBL,\n author = {Michael E. Tipping},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {211-244},\n title = {Sparse Bayesian Learning and the Relevance Vector Machine},\n volume = {1},\n year = {2001}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:23bd1da0ad4d2ec956346655f0bb0206e13556b8",
            "@type": "ScholarlyArticle",
            "paperId": "23bd1da0ad4d2ec956346655f0bb0206e13556b8",
            "corpusId": 8774525,
            "url": "https://www.semanticscholar.org/paper/23bd1da0ad4d2ec956346655f0bb0206e13556b8",
            "title": "Learning to Control a Brain\u2013Machine Interface for Reaching and Grasping by Primates",
            "venue": "PLoS Biology",
            "publicationVenue": {
                "id": "urn:research:83ff973b-8a0e-4e00-a06a-2cfd9e222de9",
                "name": "PLoS Biology",
                "alternate_names": [
                    "Plo Biology",
                    "PLOS Biology",
                    "PLO Biology"
                ],
                "issn": "1544-9173",
                "url": "http://www.pubmedcentral.nih.gov/tocrender.fcgi?journal=212"
            },
            "year": 2003,
            "externalIds": {
                "MAG": "2041248708",
                "PubMedCentral": "261882",
                "DOI": "10.1371/journal.pbio.0000042",
                "CorpusId": 8774525,
                "PubMed": "14624244"
            },
            "abstract": "Reaching and grasping in primates depend on the coordination of neural activity in large frontoparietal ensembles. Here we demonstrate that primates can learn to reach and grasp virtual objects by controlling a robot arm through a closed-loop brain\u2013machine interface (BMIc) that uses multiple mathematical models to extract several motor parameters (i.e., hand position, velocity, gripping force, and the EMGs of multiple arm muscles) from the electrical activity of frontoparietal neuronal ensembles. As single neurons typically contribute to the encoding of several motor parameters, we observed that high BMIc accuracy required recording from large neuronal ensembles. Continuous BMIc operation by monkeys led to significant improvements in both model predictions and behavioral performance. Using visual feedback, monkeys succeeded in producing robot reach-and-grasp movements even when their arms did not move. Learning to operate the BMIc was paralleled by functional reorganization in multiple cortical areas, suggesting that the dynamic properties of the BMIc were incorporated into motor and sensory cortical representations.",
            "referenceCount": 43,
            "citationCount": 1779,
            "influentialCitationCount": 105,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://journals.plos.org/plosbiology/article/file?id=10.1371/journal.pbio.0000042&type=printable",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Biology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Biology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2003-10-13",
            "journal": {
                "name": "PLoS Biology",
                "volume": "1"
            },
            "citationStyles": {
                "bibtex": "@Article{Carmena2003LearningTC,\n author = {J. Carmena and M. Lebedev and R. Crist and J. E. O\u2019Doherty and David M. Santucci and Dragan F. Dimitrov and Parag G. Patil and C. Henriquez and M. Nicolelis},\n booktitle = {PLoS Biology},\n journal = {PLoS Biology},\n title = {Learning to Control a Brain\u2013Machine Interface for Reaching and Grasping by Primates},\n volume = {1},\n year = {2003}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:77cf2d8a174c5a9a6b41c44203695c1d7f83f391",
            "@type": "ScholarlyArticle",
            "paperId": "77cf2d8a174c5a9a6b41c44203695c1d7f83f391",
            "corpusId": 17327610,
            "url": "https://www.semanticscholar.org/paper/77cf2d8a174c5a9a6b41c44203695c1d7f83f391",
            "title": "Machine learning for medical diagnosis: history, state of the art and perspective",
            "venue": "Artif. Intell. Medicine",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2001,
            "externalIds": {
                "MAG": "2083780116",
                "DBLP": "journals/artmed/Kononenko01",
                "DOI": "10.1016/S0933-3657(01)00077-X",
                "CorpusId": 17327610,
                "PubMed": "11470218"
            },
            "abstract": null,
            "referenceCount": 77,
            "citationCount": 1344,
            "influentialCitationCount": 42,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://lkm.fri.uni-lj.si/xaigor/slo/clanki/aimed00.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2001-08-01",
            "journal": {
                "name": "Artificial intelligence in medicine",
                "volume": "23 1"
            },
            "citationStyles": {
                "bibtex": "@Article{Kononenko2001MachineLF,\n author = {I. Kononenko},\n booktitle = {Artif. Intell. Medicine},\n journal = {Artificial intelligence in medicine},\n pages = {\n          89-109\n        },\n title = {Machine learning for medical diagnosis: history, state of the art and perspective},\n volume = {23 1},\n year = {2001}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:38aff6df1accc456f6cda7d16d4b9ecf418ef21e",
            "@type": "ScholarlyArticle",
            "paperId": "38aff6df1accc456f6cda7d16d4b9ecf418ef21e",
            "corpusId": 183466,
            "url": "https://www.semanticscholar.org/paper/38aff6df1accc456f6cda7d16d4b9ecf418ef21e",
            "title": "Map-Reduce for Machine Learning on Multicore",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2006,
            "externalIds": {
                "MAG": "2109722477",
                "DBLP": "conf/nips/ChuKLYBNO06",
                "CorpusId": 183466
            },
            "abstract": "We are at the beginning of the multicore era. Computers will have increasingly many cores (processors), but there is still no good programming framework for these architectures, and thus no simple and unified way for machine learning to take advantage of the potential speed up. In this paper, we develop a broadly applicable parallel programming method, one that is easily applied to many different learning algorithms. Our work is in distinct contrast to the tradition in machine learning of designing (often ingenious) ways to speed up a single algorithm at a time. Specifically, we show that algorithms that fit the Statistical Query model [15] can be written in a certain \"summation form,\" which allows them to be easily parallelized on multicore computers. We adapt Google's map-reduce [7] paradigm to demonstrate this parallel speed up technique on a variety of learning algorithms including locally weighted linear regression (LWLR), k-means, logistic regression (LR), naive Bayes (NB), SVM, ICA, PCA, gaussian discriminant analysis (GDA), EM, and backpropagation (NN). Our experimental results show basically linear speedup with an increasing number of processors.",
            "referenceCount": 31,
            "citationCount": 1212,
            "influentialCitationCount": 53,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2006-12-04",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Chu2006MapReduceFM,\n author = {Cheng-Tao Chu and Sang Kyun Kim and Yi-An Lin and YuanYuan Yu and G. Bradski and A. Ng and K. Olukotun},\n booktitle = {Neural Information Processing Systems},\n pages = {281-288},\n title = {Map-Reduce for Machine Learning on Multicore},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:fa25610fb8586c2b50a3654edc5bb42fa7fc4729",
            "@type": "ScholarlyArticle",
            "paperId": "fa25610fb8586c2b50a3654edc5bb42fa7fc4729",
            "corpusId": 118901444,
            "url": "https://www.semanticscholar.org/paper/fa25610fb8586c2b50a3654edc5bb42fa7fc4729",
            "title": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2004,
            "externalIds": {
                "MAG": "322679274",
                "DOI": "10.1198/jasa.2004.s339",
                "CorpusId": 118901444
            },
            "abstract": "In the words of the authors, the goal of this book was to \u201cbring together many of the important new ideas in learning, and explain them in a statistical framework.\u201d The authors have been quite successful in achieving this objective, and their work is a welcome addition to the statistics and learning literatures. Statistics has always been interdisciplinary, borrowing ideas from diverse \u008e elds and repaying the debt with contributions, both theoretical and practical, to the other intellectual disciplines. For statistical learning, this cross-fertilization is especially noticeable. This book is a valuable resource, both for the statistician needing an introduction to machine learning and related \u008e elds and for the computer scientist wishing to learn more about statistics. Statisticians will especially appreciate that it is written in their own language. The level of the book is roughly that of a second-year doctoral student in statistics, and it will be useful as a textbook for such students. In a stimulating article, Breiman (2001) argued that statistics has been focused too much on a \u201cdata modeling culture,\u201d where the model is paramount. Breiman argued instead for an \u201calgorithmic modeling culture,\u201d with emphasis on black-box types of prediction. Breiman\u2019s article is controversial, and in his discussion, Efron objects that \u201cprediction is certainly an interesting subject, but Leo\u2019s paper overstates both its role and our profession\u2019s lack of interest in it.\u201d Although I mostly agree with Efron, I worry that the courses offered by most statistics departments include little, if any, treatment of statistical learning and prediction. (Stanford, where Efron and the authors of this book teach, is an exception.) Graduate students in statistics certainly need to know more than they do now about prediction, machine learning, statistical learning, and data mining (not disjoint subjects). I hope that graduate courses covering the topics of this book will become more common in statistics curricula. Most of the book is focused on supervised learning, where one has inputs and outputs from some system and wishes to predict unknown outputs corresponding to known inputs. The methods discussed for supervised learning include linear and logistic regression; basis expansion, such as splines and wavelets; kernel techniques, such as local regression, local likelihood, and radial basis functions; neural networks; additive models; decision trees based on recursive partitioning, such as CART; and support vector machines. There is a \u008e nal chapter on unsupervised learning, including association rules, cluster analysis, self-organizing maps, principal components and curves, and independent component analysis. Many statisticians will be unfamiliar with at least some of these algorithms. Association rules are popular for mining commercial data in what is called \u201cmarket basket analysis.\u201d The aim is to discover types of products often purchased together. Such knowledge can be used to develop marketing strategies, such as store or catalog layouts. Self-organizing maps (SOMs) involve essentially constrained k-means clustering, where prototypes are mapped to a two-dimensional curved coordinate system. Independent components analysis is similar to principal components analysis and factor analysis, but it uses higher-order moments to achieve independence, not merely zero correlation between components. A strength of the book is the attempt to organize a plethora of methods into a coherent whole. The relationships among the methods are emphasized. I know of no other book that covers so much ground. Of course, with such broad coverage, it is not possible to cover any single topic in great depth, so this book will encourage further reading. Fortunately, each chapter includes bibliographic notes surveying the recent literature. These notes and the extensive references provide a good introduction to the learning literature, including much outside of statistics. The book might be more suitable as a textbook if less material were covered in greater depth; however, such a change would compromise the book\u2019s usefulness as a reference, and so I am happier with the book as it was written.",
            "referenceCount": 1,
            "citationCount": 17826,
            "influentialCitationCount": 2005,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "2004-06-01",
            "journal": {
                "name": "Journal of the American Statistical Association",
                "volume": "99"
            },
            "citationStyles": {
                "bibtex": "@Article{Ruppert2004TheEO,\n author = {D. Ruppert},\n journal = {Journal of the American Statistical Association},\n pages = {567 - 567},\n title = {The Elements of Statistical Learning: Data Mining, Inference, and Prediction},\n volume = {99},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e7e25fd534e9e024da329aea546484938df305a5",
            "@type": "ScholarlyArticle",
            "paperId": "e7e25fd534e9e024da329aea546484938df305a5",
            "corpusId": 11921023,
            "url": "https://www.semanticscholar.org/paper/e7e25fd534e9e024da329aea546484938df305a5",
            "title": "Gaussian Processes for Machine Learning (GPML) Toolbox",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2010,
            "externalIds": {
                "DBLP": "journals/jmlr/RasmussenN10",
                "MAG": "2107386393",
                "DOI": "10.5555/1756006.1953029",
                "CorpusId": 11921023
            },
            "abstract": "The GPML toolbox provides a wide range of functionality for Gaussian process (GP) inference and prediction. GPs are specified by mean and covariance functions; we offer a library of simple mean and covariance functions and mechanisms to compose more complex ones. Several likelihood functions are supported including Gaussian and heavy-tailed for regression as well as others suitable for classification. Finally, a range of inference methods is provided, including exact and variational inference, Expectation Propagation, and Laplace's method dealing with non-Gaussian likelihoods and FITC for dealing with large regression tasks.",
            "referenceCount": 8,
            "citationCount": 929,
            "influentialCitationCount": 50,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2010-03-01",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "11"
            },
            "citationStyles": {
                "bibtex": "@Article{Rasmussen2010GaussianPF,\n author = {C. Rasmussen and H. Nickisch},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {3011-3015},\n title = {Gaussian Processes for Machine Learning (GPML) Toolbox},\n volume = {11},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0148bbc80ea2f2526ab019a317639b4fb357f399",
            "@type": "ScholarlyArticle",
            "paperId": "0148bbc80ea2f2526ab019a317639b4fb357f399",
            "corpusId": 8484937,
            "url": "https://www.semanticscholar.org/paper/0148bbc80ea2f2526ab019a317639b4fb357f399",
            "title": "A Machine Learning Approach to Twitter User Classification",
            "venue": "International Conference on Web and Social Media",
            "publicationVenue": {
                "id": "urn:research:7dc964d5-49e6-4c37-b1c4-a7f0de1fa425",
                "name": "International Conference on Web and Social Media",
                "alternate_names": [
                    "Int Conf Weblogs Soc Media",
                    "International Conference on Weblogs and Social Media",
                    "Int Conf Web Soc Media",
                    "ICWSM"
                ],
                "issn": null,
                "url": "http://www.aaai.org/Library/ICWSM/icwsm-library.php"
            },
            "year": 2011,
            "externalIds": {
                "DBLP": "conf/icwsm/PennacchiottiP11",
                "MAG": "184758014",
                "DOI": "10.1609/icwsm.v5i1.14139",
                "CorpusId": 8484937
            },
            "abstract": "\n \n This paper addresses the task of user classification in social media, with an application to Twitter. We automatically infer the values of user attributes such as political orientation or ethnicity by leveraging observable information such as the user behavior, network structure and the linguistic content of the user\u2019s Twitter feed. We employ a machine learning approach which relies on a comprehensive set of features derived from such user information. We report encouraging experimental results on 3 tasks with different characteristics: political affiliation detection, ethnicity identification and detecting affinity for a particular business. Finally, our analysis shows that rich linguistic features prove consistently valuable across the 3 tasks and show great promise for additional user classification needs.\n \n",
            "referenceCount": 23,
            "citationCount": 621,
            "influentialCitationCount": 43,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/ICWSM/article/download/14139/13988",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Political Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2011-07-05",
            "journal": {
                "name": "Proceedings of the International AAAI Conference on Web and Social Media",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Pennacchiotti2011AML,\n author = {M. Pennacchiotti and Ana-Maria Popescu},\n booktitle = {International Conference on Web and Social Media},\n journal = {Proceedings of the International AAAI Conference on Web and Social Media},\n title = {A Machine Learning Approach to Twitter User Classification},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8ccdee6826cbc97256bd4d082ebfa8cdfd2c727f",
            "@type": "ScholarlyArticle",
            "paperId": "8ccdee6826cbc97256bd4d082ebfa8cdfd2c727f",
            "corpusId": 210985022,
            "url": "https://www.semanticscholar.org/paper/8ccdee6826cbc97256bd4d082ebfa8cdfd2c727f",
            "title": "Hidden fluid mechanics: Learning velocity and pressure fields from flow visualizations",
            "venue": "Science",
            "publicationVenue": {
                "id": "urn:research:f59506a8-d8bb-4101-b3d4-c4ac3ed03dad",
                "name": "Science",
                "alternate_names": null,
                "issn": "0193-4511",
                "url": "https://www.jstor.org/journal/science"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3003922491",
                "DOI": "10.1126/science.aaw4741",
                "CorpusId": 210985022,
                "PubMed": "32001523"
            },
            "abstract": "Machine-learning fluid flow Quantifying fluid flow is relevant to disciplines ranging from geophysics to medicine. Flow can be experimentally visualized using, for example, smoke or contrast agents, but extracting velocity and pressure fields from this information is tricky. Raissi et al. developed a machine-learning approach to tackle this problem. Their method exploits the knowledge of Navier-Stokes equations, which govern the dynamics of fluid flow in many scientifically relevant situations. The authors illustrate their approach using examples such as blood flow in an aneurysm. Science, this issue p. 1026 A machine learning approach exploiting the knowledge of Navier-Stokes equations can extract detailed fluid flow information. For centuries, flow visualization has been the art of making fluid motion visible in physical and biological systems. Although such flow patterns can be, in principle, described by the Navier-Stokes equations, extracting the velocity and pressure fields directly from the images is challenging. We addressed this problem by developing hidden fluid mechanics (HFM), a physics-informed deep-learning framework capable of encoding the Navier-Stokes equations into the neural networks while being agnostic to the geometry or the initial and boundary conditions. We demonstrate HFM for several physical and biomedical problems by extracting quantitative information for which direct measurements may not be possible. HFM is robust to low resolution and substantial noise in the observation data, which is important for potential applications.",
            "referenceCount": 30,
            "citationCount": 928,
            "influentialCitationCount": 27,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://europepmc.org/articles/pmc7219083?pdf=render",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-01-30",
            "journal": {
                "name": "Science",
                "volume": "367"
            },
            "citationStyles": {
                "bibtex": "@Article{Raissi2020HiddenFM,\n author = {M. Raissi and A. Yazdani and G. Karniadakis},\n booktitle = {Science},\n journal = {Science},\n pages = {1026 - 1030},\n title = {Hidden fluid mechanics: Learning velocity and pressure fields from flow visualizations},\n volume = {367},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:afd0859a858481d2f36109f68090aebd77456b7f",
            "@type": "ScholarlyArticle",
            "paperId": "afd0859a858481d2f36109f68090aebd77456b7f",
            "corpusId": 2304759,
            "url": "https://www.semanticscholar.org/paper/afd0859a858481d2f36109f68090aebd77456b7f",
            "title": "The security of machine learning",
            "venue": "Machine-mediated learning",
            "publicationVenue": {
                "id": "urn:research:22c9862f-a25e-40cd-9d31-d09e68a293e6",
                "name": "Machine-mediated learning",
                "alternate_names": [
                    "Mach learn",
                    "Machine Learning",
                    "Mach Learn"
                ],
                "issn": "0732-6718",
                "url": "http://www.springer.com/computer/artificial/journal/10994"
            },
            "year": 2010,
            "externalIds": {
                "MAG": "2125908420",
                "DBLP": "journals/ml/BarrenoNJT10",
                "DOI": "10.1007/s10994-010-5188-5",
                "CorpusId": 2304759
            },
            "abstract": null,
            "referenceCount": 45,
            "citationCount": 795,
            "influentialCitationCount": 57,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/s10994-010-5188-5.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2010-11-01",
            "journal": {
                "name": "Machine Learning",
                "volume": "81"
            },
            "citationStyles": {
                "bibtex": "@Article{Barreno2010TheSO,\n author = {M. Barreno and B. Nelson and A. Joseph and Doug J. Tygar},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {121-148},\n title = {The security of machine learning},\n volume = {81},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:80ab1a48aac6c2716cb52334ca2f0872adfe3a6b",
            "@type": "ScholarlyArticle",
            "paperId": "80ab1a48aac6c2716cb52334ca2f0872adfe3a6b",
            "corpusId": 14084223,
            "url": "https://www.semanticscholar.org/paper/80ab1a48aac6c2716cb52334ca2f0872adfe3a6b",
            "title": "Extreme learning machine: algorithm, theory and applications",
            "venue": "Artificial Intelligence Review",
            "publicationVenue": {
                "id": "urn:research:ea8553fe-2467-4367-afee-c4deb3754820",
                "name": "Artificial Intelligence Review",
                "alternate_names": [
                    "Artif Intell Rev"
                ],
                "issn": "0269-2821",
                "url": "https://link.springer.com/journal/10462"
            },
            "year": 2013,
            "externalIds": {
                "DBLP": "journals/air/DingZZXN15",
                "MAG": "1965895201",
                "DOI": "10.1007/s10462-013-9405-z",
                "CorpusId": 14084223
            },
            "abstract": null,
            "referenceCount": 55,
            "citationCount": 429,
            "influentialCitationCount": 17,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2013-04-23",
            "journal": {
                "name": "Artificial Intelligence Review",
                "volume": "44"
            },
            "citationStyles": {
                "bibtex": "@Article{Ding2013ExtremeLM,\n author = {Shifei Ding and Han Zhao and Yanan Zhang and Xinzheng Xu and Ru Nie},\n booktitle = {Artificial Intelligence Review},\n journal = {Artificial Intelligence Review},\n pages = {103 - 115},\n title = {Extreme learning machine: algorithm, theory and applications},\n volume = {44},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e3d772986d176057aca2f5e3eb783da53b559134",
            "@type": "ScholarlyArticle",
            "paperId": "e3d772986d176057aca2f5e3eb783da53b559134",
            "corpusId": 3518190,
            "url": "https://www.semanticscholar.org/paper/e3d772986d176057aca2f5e3eb783da53b559134",
            "title": "Unsupervised Machine Translation Using Monolingual Corpora Only",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2949520888",
                "ArXiv": "1711.00043",
                "DBLP": "conf/iclr/LampleCDR18",
                "CorpusId": 3518190
            },
            "abstract": "Machine translation has recently achieved impressive performance thanks to recent advances in deep learning and the availability of large-scale parallel corpora. There have been numerous attempts to extend these successes to low-resource language pairs, yet requiring tens of thousands of parallel sentences. In this work, we take this research direction to the extreme and investigate whether it is possible to learn to translate even without any parallel data. We propose a model that takes sentences from monolingual corpora in two different languages and maps them into the same latent space. By learning to reconstruct in both languages from this shared feature space, the model effectively learns to translate without using any labeled data. We demonstrate our model on two widely used datasets and two language pairs, reporting BLEU scores of 32.8 and 15.1 on the Multi30k and WMT English-French datasets, without using even a single parallel sentence at training time.",
            "referenceCount": 43,
            "citationCount": 988,
            "influentialCitationCount": 177,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-10-31",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1711.00043"
            },
            "citationStyles": {
                "bibtex": "@Article{Lample2017UnsupervisedMT,\n author = {Guillaume Lample and Ludovic Denoyer and Marc'Aurelio Ranzato},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Unsupervised Machine Translation Using Monolingual Corpora Only},\n volume = {abs/1711.00043},\n year = {2017}\n}\n"
            }
        }
    }
]