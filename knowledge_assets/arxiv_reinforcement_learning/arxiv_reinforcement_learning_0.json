[
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04563v1",
            "title": "Visual Geometry Grounded Deep Structure From Motion",
            "updated": "2023-12-07T18:59:52Z",
            "published": "2023-12-07T18:59:52Z",
            "summary": "Structure-from-motion (SfM) is a long-standing problem in the computer vision\ncommunity, which aims to reconstruct the camera poses and 3D structure of a\nscene from a set of unconstrained 2D images. Classical frameworks solve this\nproblem in an incremental manner by detecting and matching keypoints,\nregistering images, triangulating 3D points, and conducting bundle adjustment.\nRecent research efforts have predominantly revolved around harnessing the power\nof deep learning techniques to enhance specific elements (e.g., keypoint\nmatching), but are still based on the original, non-differentiable pipeline.\nInstead, we propose a new deep pipeline VGGSfM, where each component is fully\ndifferentiable and thus can be trained in an end-to-end manner. To this end, we\nintroduce new mechanisms and simplifications. First, we build on recent\nadvances in deep 2D point tracking to extract reliable pixel-accurate tracks,\nwhich eliminates the need for chaining pairwise matches. Furthermore, we\nrecover all cameras simultaneously based on the image and track features\ninstead of gradually registering cameras. Finally, we optimise the cameras and\ntriangulate 3D points via a differentiable bundle adjustment layer. We attain\nstate-of-the-art performance on three popular datasets, CO3D, IMC Phototourism,\nand ETH3D.",
            "author": [
                "Jianyuan Wang",
                "Nikita Karaev",
                "Christian Rupprecht",
                "David Novotny"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04563v1",
                "http://arxiv.org/pdf/2312.04563v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04561v1",
            "title": "GenDeF: Learning Generative Deformation Field for Video Generation",
            "updated": "2023-12-07T18:59:41Z",
            "published": "2023-12-07T18:59:41Z",
            "summary": "We offer a new perspective on approaching the task of video generation.\nInstead of directly synthesizing a sequence of frames, we propose to render a\nvideo by warping one static image with a generative deformation field (GenDeF).\nSuch a pipeline enjoys three appealing advantages. First, we can sufficiently\nreuse a well-trained image generator to synthesize the static image (also\ncalled canonical image), alleviating the difficulty in producing a video and\nthereby resulting in better visual quality. Second, we can easily convert a\ndeformation field to optical flows, making it possible to apply explicit\nstructural regularizations for motion modeling, leading to temporally\nconsistent results. Third, the disentanglement between content and motion\nallows users to process a synthesized video through processing its\ncorresponding static image without any tuning, facilitating many applications\nlike video editing, keypoint tracking, and video segmentation. Both qualitative\nand quantitative results on three common video generation benchmarks\ndemonstrate the superiority of our GenDeF method.",
            "author": [
                "Wen Wang",
                "Kecheng Zheng",
                "Qiuyu Wang",
                "Hao Chen",
                "Zifan Shi",
                "Ceyuan Yang",
                "Yujun Shen",
                "Chunhua Shen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04561v1",
                "http://arxiv.org/pdf/2312.04561v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04558v1",
            "title": "MonoGaussianAvatar: Monocular Gaussian Point-based Head Avatar",
            "updated": "2023-12-07T18:59:31Z",
            "published": "2023-12-07T18:59:31Z",
            "summary": "The ability to animate photo-realistic head avatars reconstructed from\nmonocular portrait video sequences represents a crucial step in bridging the\ngap between the virtual and real worlds. Recent advancements in head avatar\ntechniques, including explicit 3D morphable meshes (3DMM), point clouds, and\nneural implicit representation have been exploited for this ongoing research.\nHowever, 3DMM-based methods are constrained by their fixed topologies,\npoint-based approaches suffer from a heavy training burden due to the extensive\nquantity of points involved, and the last ones suffer from limitations in\ndeformation flexibility and rendering efficiency. In response to these\nchallenges, we propose MonoGaussianAvatar (Monocular Gaussian Point-based Head\nAvatar), a novel approach that harnesses 3D Gaussian point representation\ncoupled with a Gaussian deformation field to learn explicit head avatars from\nmonocular portrait videos. We define our head avatars with Gaussian points\ncharacterized by adaptable shapes, enabling flexible topology. These points\nexhibit movement with a Gaussian deformation field in alignment with the target\npose and expression of a person, facilitating efficient deformation.\nAdditionally, the Gaussian points have controllable shape, size, color, and\nopacity combined with Gaussian splatting, allowing for efficient training and\nrendering. Experiments demonstrate the superior performance of our method,\nwhich achieves state-of-the-art results among previous methods.",
            "author": [
                "Yufan Chen",
                "Lizhen Wang",
                "Qijing Li",
                "Hongjiang Xiao",
                "Shengping Zhang",
                "Hongxun Yao",
                "Yebin Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04558v1",
                "http://arxiv.org/pdf/2312.04558v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04556v1",
            "title": "Large Language Models for Mathematicians",
            "updated": "2023-12-07T18:59:29Z",
            "published": "2023-12-07T18:59:29Z",
            "summary": "Large language models (LLMs) such as ChatGPT have received immense interest\nfor their general-purpose language understanding and, in particular, their\nability to generate high-quality text or computer code. For many professions,\nLLMs represent an invaluable tool that can speed up and improve the quality of\nwork. In this note, we discuss to what extent they can aid professional\nmathematicians. We first provide a mathematical description of the transformer\nmodel used in all modern language models. Based on recent studies, we then\noutline best practices and potential issues and report on the mathematical\nabilities of language models. Finally, we shed light on the potential of LMMs\nto change how mathematicians work.",
            "author": [
                "Simon Frieder",
                "Julius Berner",
                "Philipp Petersen",
                "Thomas Lukasiewicz"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04556v1",
                "http://arxiv.org/pdf/2312.04556v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG",
                "math.HO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04554v1",
            "title": "Improved Visual Grounding through Self-Consistent Explanations",
            "updated": "2023-12-07T18:59:22Z",
            "published": "2023-12-07T18:59:22Z",
            "summary": "Vision-and-language models trained to match images with text can be combined\nwith visual explanation methods to point to the locations of specific objects\nin an image. Our work shows that the localization --\"grounding\"-- abilities of\nthese models can be further improved by finetuning for self-consistent visual\nexplanations. We propose a strategy for augmenting existing text-image datasets\nwith paraphrases using a large language model, and SelfEQ, a weakly-supervised\nstrategy on visual explanation maps for paraphrases that encourages\nself-consistency. Specifically, for an input textual phrase, we attempt to\ngenerate a paraphrase and finetune the model so that the phrase and paraphrase\nmap to the same region in the image. We posit that this both expands the\nvocabulary that the model is able to handle, and improves the quality of the\nobject locations highlighted by gradient-based visual explanation methods (e.g.\nGradCAM). We demonstrate that SelfEQ improves performance on Flickr30k,\nReferIt, and RefCOCO+ over a strong baseline method and several prior works.\nParticularly, comparing to other methods that do not use any type of box\nannotations, we obtain 84.07% on Flickr30k (an absolute improvement of 4.69%),\n67.40% on ReferIt (an absolute improvement of 7.68%), and 75.10%, 55.49% on\nRefCOCO+ test sets A and B respectively (an absolute improvement of 3.74% on\naverage).",
            "author": [
                "Ruozhen He",
                "Paola Cascante-Bonilla",
                "Ziyan Yang",
                "Alexander C. Berg",
                "Vicente Ordonez"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04554v1",
                "http://arxiv.org/pdf/2312.04554v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04552v1",
            "title": "Generating Illustrated Instructions",
            "updated": "2023-12-07T18:59:20Z",
            "published": "2023-12-07T18:59:20Z",
            "summary": "We introduce the new task of generating Illustrated Instructions, i.e.,\nvisual instructions customized to a user's needs. We identify desiderata unique\nto this task, and formalize it through a suite of automatic and human\nevaluation metrics, designed to measure the validity, consistency, and efficacy\nof the generations. We combine the power of large language models (LLMs)\ntogether with strong text-to-image generation diffusion models to propose a\nsimple approach called StackedDiffusion, which generates such illustrated\ninstructions given text as input. The resulting model strongly outperforms\nbaseline approaches and state-of-the-art multimodal LLMs; and in 30% of cases,\nusers even prefer it to human-generated articles. Most notably, it enables\nvarious new and exciting applications far beyond what static articles on the\nweb can provide, such as personalized instructions complete with intermediate\nsteps and pictures in response to a user's individual situation.",
            "author": [
                "Sachit Menon",
                "Ishan Misra",
                "Rohit Girdhar"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04552v1",
                "http://arxiv.org/pdf/2312.04552v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG",
                "cs.MM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04548v1",
            "title": "Multiview Aerial Visual Recognition (MAVREC): Can Multi-view Improve\n  Aerial Visual Perception?",
            "updated": "2023-12-07T18:59:14Z",
            "published": "2023-12-07T18:59:14Z",
            "summary": "Despite the commercial abundance of UAVs, aerial data acquisition remains\nchallenging, and the existing Asia and North America-centric open-source UAV\ndatasets are small-scale or low-resolution and lack diversity in scene\ncontextuality. Additionally, the color content of the scenes, solar-zenith\nangle, and population density of different geographies influence the data\ndiversity. These two factors conjointly render suboptimal aerial-visual\nperception of the deep neural network (DNN) models trained primarily on the\nground-view data, including the open-world foundational models.\n  To pave the way for a transformative era of aerial detection, we present\nMultiview Aerial Visual RECognition or MAVREC, a video dataset where we record\nsynchronized scenes from different perspectives -- ground camera and\ndrone-mounted camera. MAVREC consists of around 2.5 hours of industry-standard\n2.7K resolution video sequences, more than 0.5 million frames, and 1.1 million\nannotated bounding boxes. This makes MAVREC the largest ground and aerial-view\ndataset, and the fourth largest among all drone-based datasets across all\nmodalities and tasks. Through our extensive benchmarking on MAVREC, we\nrecognize that augmenting object detectors with ground-view images from the\ncorresponding geographical location is a superior pre-training strategy for\naerial detection. Building on this strategy, we benchmark MAVREC with a\ncurriculum-based semi-supervised object detection approach that leverages\nlabeled (ground and aerial) and unlabeled (only aerial) images to enhance the\naerial detection. We publicly release the MAVREC dataset:\nhttps://mavrec.github.io.",
            "author": [
                "Aritra Dutta",
                "Srijan Das",
                "Jacob Nielsen",
                "Rajatsubhra Chakraborty",
                "Mubarak Shah"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04548v1",
                "http://arxiv.org/pdf/2312.04548v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG",
                "I.4.0; I.4.8; I.5.1; I.5.4; I.2.10"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04549v1",
            "title": "PlayFusion: Skill Acquisition via Diffusion from Language-Annotated Play",
            "updated": "2023-12-07T18:59:14Z",
            "published": "2023-12-07T18:59:14Z",
            "summary": "Learning from unstructured and uncurated data has become the dominant\nparadigm for generative approaches in language and vision. Such unstructured\nand unguided behavior data, commonly known as play, is also easier to collect\nin robotics but much more difficult to learn from due to its inherently\nmultimodal, noisy, and suboptimal nature. In this paper, we study this problem\nof learning goal-directed skill policies from unstructured play data which is\nlabeled with language in hindsight. Specifically, we leverage advances in\ndiffusion models to learn a multi-task diffusion model to extract robotic\nskills from play data. Using a conditional denoising diffusion process in the\nspace of states and actions, we can gracefully handle the complexity and\nmultimodality of play data and generate diverse and interesting robot\nbehaviors. To make diffusion models more useful for skill learning, we\nencourage robotic agents to acquire a vocabulary of skills by introducing\ndiscrete bottlenecks into the conditional behavior generation process. In our\nexperiments, we demonstrate the effectiveness of our approach across a wide\nvariety of environments in both simulation and the real world. Results\nvisualizations and videos at https://play-fusion.github.io",
            "author": [
                "Lili Chen",
                "Shikhar Bahl",
                "Deepak Pathak"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04549v1",
                "http://arxiv.org/pdf/2312.04549v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI",
                "cs.CV",
                "cs.LG",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04546v1",
            "title": "Adversarial Learning for Feature Shift Detection and Correction",
            "updated": "2023-12-07T18:58:40Z",
            "published": "2023-12-07T18:58:40Z",
            "summary": "Data shift is a phenomenon present in many real-world applications, and while\nthere are multiple methods attempting to detect shifts, the task of localizing\nand correcting the features originating such shifts has not been studied in\ndepth. Feature shifts can occur in many datasets, including in multi-sensor\ndata, where some sensors are malfunctioning, or in tabular and structured data,\nincluding biomedical, financial, and survey data, where faulty standardization\nand data processing pipelines can lead to erroneous features. In this work, we\nexplore using the principles of adversarial learning, where the information\nfrom several discriminators trained to distinguish between two distributions is\nused to both detect the corrupted features and fix them in order to remove the\ndistribution shift between datasets. We show that mainstream supervised\nclassifiers, such as random forest or gradient boosting trees, combined with\nsimple iterative heuristics, can localize and correct feature shifts,\noutperforming current statistical and neural network-based techniques. The code\nis available at https://github.com/AI-sandbox/DataFix.",
            "author": [
                "Miriam Barrabes",
                "Daniel Mas Montserrat",
                "Margarita Geleta",
                "Xavier Giro-i-Nieto",
                "Alexander G. Ioannidis"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04546v1",
                "http://arxiv.org/pdf/2312.04546v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "stat.AP",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04543v1",
            "title": "HyperDreamer: Hyper-Realistic 3D Content Generation and Editing from a\n  Single Image",
            "updated": "2023-12-07T18:58:09Z",
            "published": "2023-12-07T18:58:09Z",
            "summary": "3D content creation from a single image is a long-standing yet highly\ndesirable task. Recent advances introduce 2D diffusion priors, yielding\nreasonable results. However, existing methods are not hyper-realistic enough\nfor post-generation usage, as users cannot view, render and edit the resulting\n3D content from a full range. To address these challenges, we introduce\nHyperDreamer with several key designs and appealing properties: 1) Viewable:\n360 degree mesh modeling with high-resolution textures enables the creation of\nvisually compelling 3D models from a full range of observation points. 2)\nRenderable: Fine-grained semantic segmentation and data-driven priors are\nincorporated as guidance to learn reasonable albedo, roughness, and specular\nproperties of the materials, enabling semantic-aware arbitrary material\nestimation. 3) Editable: For a generated model or their own data, users can\ninteractively select any region via a few clicks and efficiently edit the\ntexture with text-based guidance. Extensive experiments demonstrate the\neffectiveness of HyperDreamer in modeling region-aware materials with\nhigh-resolution textures and enabling user-friendly editing. We believe that\nHyperDreamer holds promise for advancing 3D content creation and finding\napplications in various domains.",
            "author": [
                "Tong Wu",
                "Zhibing Li",
                "Shuai Yang",
                "Pan Zhang",
                "Xinggang Pan",
                "Jiaqi Wang",
                "Dahua Lin",
                "Ziwei Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04543v1",
                "http://arxiv.org/pdf/2312.04543v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04542v1",
            "title": "SoK: Unintended Interactions among Machine Learning Defenses and Risks",
            "updated": "2023-12-07T18:57:36Z",
            "published": "2023-12-07T18:57:36Z",
            "summary": "Machine learning (ML) models cannot neglect risks to security, privacy, and\nfairness. Several defenses have been proposed to mitigate such risks. When a\ndefense is effective in mitigating one risk, it may correspond to increased or\ndecreased susceptibility to other risks. Existing research lacks an effective\nframework to recognize and explain these unintended interactions. We present\nsuch a framework, based on the conjecture that overfitting and memorization\nunderlie unintended interactions. We survey existing literature on unintended\ninteractions, accommodating them within our framework. We use our framework to\nconjecture on two previously unexplored interactions, and empirically validate\nour conjectures.",
            "author": [
                "Vasisht Duddu",
                "Sebastian Szyller",
                "N. Asokan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04542v1",
                "http://arxiv.org/pdf/2312.04542v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04540v1",
            "title": "Sim-to-Real Causal Transfer: A Metric Learning Approach to\n  Causally-Aware Interaction Representations",
            "updated": "2023-12-07T18:57:03Z",
            "published": "2023-12-07T18:57:03Z",
            "summary": "Modeling spatial-temporal interactions among neighboring agents is at the\nheart of multi-agent problems such as motion forecasting and crowd navigation.\nDespite notable progress, it remains unclear to which extent modern\nrepresentations can capture the causal relationships behind agent interactions.\nIn this work, we take an in-depth look at the causal awareness of these\nrepresentations, from computational formalism to real-world practice. First, we\ncast doubt on the notion of non-causal robustness studied in the recent\nCausalAgents benchmark. We show that recent representations are already\npartially resilient to perturbations of non-causal agents, and yet modeling\nindirect causal effects involving mediator agents remains challenging. To\naddress this challenge, we introduce a metric learning approach that\nregularizes latent representations with causal annotations. Our controlled\nexperiments show that this approach not only leads to higher degrees of causal\nawareness but also yields stronger out-of-distribution robustness. To further\noperationalize it in practice, we propose a sim-to-real causal transfer method\nvia cross-domain multi-task learning. Experiments on pedestrian datasets show\nthat our method can substantially boost generalization, even in the absence of\nreal-world causal annotations. We hope our work provides a new perspective on\nthe challenges and potential pathways towards causally-aware representations of\nmulti-agent interactions. Our code is available at\nhttps://github.com/socialcausality.",
            "author": [
                "Yuejiang Liu",
                "Ahmad Rahimi",
                "Po-Chien Luan",
                "Frano Raji\u010d",
                "Alexandre Alahi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04540v1",
                "http://arxiv.org/pdf/2312.04540v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CV",
                "cs.MA",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04535v1",
            "title": "Trajeglish: Learning the Language of Driving Scenarios",
            "updated": "2023-12-07T18:53:27Z",
            "published": "2023-12-07T18:53:27Z",
            "summary": "A longstanding challenge for self-driving development is simulating dynamic\ndriving scenarios seeded from recorded driving logs. In pursuit of this\nfunctionality, we apply tools from discrete sequence modeling to model how\nvehicles, pedestrians and cyclists interact in driving scenarios. Using a\nsimple data-driven tokenization scheme, we discretize trajectories to\ncentimeter-level resolution using a small vocabulary. We then model the\nmulti-agent sequence of motion tokens with a GPT-like encoder-decoder that is\nautoregressive in time and takes into account intra-timestep interaction\nbetween agents. Scenarios sampled from our model exhibit state-of-the-art\nrealism; our model tops the Waymo Sim Agents Benchmark, surpassing prior work\nalong the realism meta metric by 3.3% and along the interaction metric by 9.9%.\nWe ablate our modeling choices in full autonomy and partial autonomy settings,\nand show that the representations learned by our model can quickly be adapted\nto improve performance on nuScenes. We additionally evaluate the scalability of\nour model with respect to parameter count and dataset size, and use density\nestimates from our model to quantify the saliency of context length and\nintra-timestep interaction for the traffic modeling task.",
            "author": [
                "Jonah Philion",
                "Xue Bin Peng",
                "Sanja Fidler"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04535v1",
                "http://arxiv.org/pdf/2312.04535v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04533v1",
            "title": "Dream2Real: Zero-Shot 3D Object Rearrangement with Vision-Language\n  Models",
            "updated": "2023-12-07T18:51:19Z",
            "published": "2023-12-07T18:51:19Z",
            "summary": "We introduce Dream2Real, a robotics framework which integrates\nvision-language models (VLMs) trained on 2D data into a 3D object rearrangement\npipeline. This is achieved by the robot autonomously constructing a 3D\nrepresentation of the scene, where objects can be rearranged virtually and an\nimage of the resulting arrangement rendered. These renders are evaluated by a\nVLM, so that the arrangement which best satisfies the user instruction is\nselected and recreated in the real world with pick-and-place. This enables\nlanguage-conditioned rearrangement to be performed zero-shot, without needing\nto collect a training dataset of example arrangements. Results on a series of\nreal-world tasks show that this framework is robust to distractors,\ncontrollable by language, capable of understanding complex multi-object\nrelations, and readily applicable to both tabletop and 6-DoF rearrangement\ntasks.",
            "author": [
                "Ivan Kapelyukh",
                "Yifei Ren",
                "Ignacio Alzugaray",
                "Edward Johns"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04533v1",
                "http://arxiv.org/pdf/2312.04533v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04530v1",
            "title": "Camera Height Doesn't Change: Unsupervised Monocular Scale-Aware\n  Road-Scene Depth Estimation",
            "updated": "2023-12-07T18:50:01Z",
            "published": "2023-12-07T18:50:01Z",
            "summary": "Monocular depth estimators either require explicit scale supervision through\nauxiliary sensors or suffer from scale ambiguity, which renders them difficult\nto deploy in downstream applications. A possible source of scale is the sizes\nof objects found in the scene, but inaccurate localization makes them difficult\nto exploit. In this paper, we introduce a novel scale-aware monocular depth\nestimation method called StableCamH that does not require any auxiliary sensor\nor supervision. The key idea is to exploit prior knowledge of object heights in\nthe scene but aggregate the height cues into a single invariant measure common\nto all frames in a road video sequence, namely the camera height. By\nformulating monocular depth estimation as camera height optimization, we\nachieve robust and accurate unsupervised end-to-end training. To realize\nStableCamH, we devise a novel learning-based size prior that can directly\nconvert car appearance into its dimensions. Extensive experiments on KITTI and\nCityscapes show the effectiveness of StableCamH, its state-of-the-art accuracy\ncompared with related methods, and its generalizability. The training framework\nof StableCamH can be used for any monocular depth estimation method and will\nhopefully become a fundamental building block for further work.",
            "author": [
                "Genki Kinoshita",
                "Ko Nishino"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04530v1",
                "http://arxiv.org/pdf/2312.04530v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04529v1",
            "title": "Diffusion Reflectance Map: Single-Image Stochastic Inverse Rendering of\n  Illumination and Reflectance",
            "updated": "2023-12-07T18:50:00Z",
            "published": "2023-12-07T18:50:00Z",
            "summary": "Reflectance bounds the frequency spectrum of illumination in the object\nappearance. In this paper, we introduce the first stochastic inverse rendering\nmethod, which recovers the full frequency spectrum of an illumination jointly\nwith the object reflectance from a single image. Our key idea is to solve this\nblind inverse problem in the reflectance map, an appearance representation\ninvariant to the underlying geometry, by learning to reverse the image\nformation with a novel diffusion model which we refer to as the Diffusion\nReflectance Map Network (DRMNet). Given an observed reflectance map converted\nand completed from the single input image, DRMNet generates a reflectance map\ncorresponding to a perfect mirror sphere while jointly estimating the\nreflectance. The forward process can be understood as gradually filtering a\nnatural illumination with lower and lower frequency reflectance and additive\nGaussian noise. DRMNet learns to invert this process with two subnetworks,\nIllNet and RefNet, which work in concert towards this joint estimation. The\nnetwork is trained on an extensive synthetic dataset and is demonstrated to\ngeneralize to real images, showing state-of-the-art accuracy on established\ndatasets.",
            "author": [
                "Yuto Enyo",
                "Ko Nishino"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04529v1",
                "http://arxiv.org/pdf/2312.04529v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04528v1",
            "title": "Using Large Language Models for Hyperparameter Optimization",
            "updated": "2023-12-07T18:46:50Z",
            "published": "2023-12-07T18:46:50Z",
            "summary": "This paper studies using foundational large language models (LLMs) to make\ndecisions during hyperparameter optimization (HPO). Empirical evaluations\ndemonstrate that in settings with constrained search budgets, LLMs can perform\ncomparably or better than traditional HPO methods like random search and\nBayesian optimization on standard benchmarks. Furthermore, we propose to treat\nthe code specifying our model as a hyperparameter, which the LLM outputs, going\nbeyond the capabilities of existing HPO approaches. Our findings suggest that\nLLMs are a promising tool for improving efficiency in the traditional\ndecision-making problem of hyperparameter optimization.",
            "author": [
                "Michael R. Zhang",
                "Nishkrit Desai",
                "Juhan Bae",
                "Jonathan Lorraine",
                "Jimmy Ba"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04528v1",
                "http://arxiv.org/pdf/2312.04528v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04521v1",
            "title": "Multimodal Industrial Anomaly Detection by Crossmodal Feature Mapping",
            "updated": "2023-12-07T18:41:21Z",
            "published": "2023-12-07T18:41:21Z",
            "summary": "The paper explores the industrial multimodal Anomaly Detection (AD) task,\nwhich exploits point clouds and RGB images to localize anomalies. We introduce\na novel light and fast framework that learns to map features from one modality\nto the other on nominal samples. At test time, anomalies are detected by\npinpointing inconsistencies between observed and mapped features. Extensive\nexperiments show that our approach achieves state-of-the-art detection and\nsegmentation performance in both the standard and few-shot settings on the\nMVTec 3D-AD dataset while achieving faster inference and occupying less memory\nthan previous multimodal AD methods. Moreover, we propose a layer-pruning\ntechnique to improve memory and time efficiency with a marginal sacrifice in\nperformance.",
            "author": [
                "Alex Costanzino",
                "Pierluigi Zama Ramirez",
                "Giuseppe Lisanti",
                "Luigi Di Stefano"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04521v1",
                "http://arxiv.org/pdf/2312.04521v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04519v1",
            "title": "Bootstrapping Autonomous Radars with Self-Supervised Learning",
            "updated": "2023-12-07T18:38:39Z",
            "published": "2023-12-07T18:38:39Z",
            "summary": "The perception of autonomous vehicles using radars has attracted increased\nresearch interest due its ability to operate in fog and bad weather. However,\ntraining radar models is hindered by the cost and difficulty of annotating\nlarge-scale radar data. To overcome this bottleneck, we propose a\nself-supervised learning framework to leverage the large amount of unlabeled\nradar data to pre-train radar-only embeddings for self-driving perception\ntasks. The proposed method combines radar-to-radar and radar-to-vision\ncontrastive losses to learn a general representation from unlabeled radar\nheatmaps paired with their corresponding camera images. When used for\ndownstream object detection, we demonstrate that the proposed self-supervision\nframework can improve the accuracy of state-of-the-art supervised baselines by\n5.8% in mAP.",
            "author": [
                "Yiduo Hao",
                "Sohrab Madani",
                "Junfeng Guan",
                "Mohammed Alloulah",
                "Saurabh Gupta",
                "Haitham Hassanieh"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04519v1",
                "http://arxiv.org/pdf/2312.04519v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04514v1",
            "title": "Channel Charting for Streaming CSI Data",
            "updated": "2023-12-07T18:34:25Z",
            "published": "2023-12-07T18:34:25Z",
            "summary": "Channel charting (CC) applies dimensionality reduction to channel state\ninformation (CSI) data at the infrastructure basestation side with the goal of\nextracting pseudo-position information for each user. The self-supervised\nnature of CC enables predictive tasks that depend on user position without\nrequiring any ground-truth position information. In this work, we focus on the\npractically relevant streaming CSI data scenario, in which CSI is constantly\nestimated. To deal with storage limitations, we develop a novel streaming CC\narchitecture that maintains a small core CSI dataset from which the channel\ncharts are learned. Curation of the core CSI dataset is achieved using a\nmin-max-similarity criterion. Numerical validation with measured CSI data\ndemonstrates that our method approaches the accuracy obtained from the complete\nCSI dataset while using only a fraction of CSI storage and avoiding\ncatastrophic forgetting of old CSI data.",
            "author": [
                "Sueda Taner",
                "Maxime Guillaud",
                "Olav Tirkkonen",
                "Christoph Studer"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04514v1",
                "http://arxiv.org/pdf/2312.04514v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "eess.SP",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04510v1",
            "title": "A Block Metropolis-Hastings Sampler for Controllable Energy-based Text\n  Generation",
            "updated": "2023-12-07T18:30:15Z",
            "published": "2023-12-07T18:30:15Z",
            "summary": "Recent work has shown that energy-based language modeling is an effective\nframework for controllable text generation because it enables flexible\nintegration of arbitrary discriminators. However, because energy-based LMs are\nglobally normalized, approximate techniques like Metropolis-Hastings (MH) are\nrequired for inference. Past work has largely explored simple proposal\ndistributions that modify a single token at a time, like in Gibbs sampling. In\nthis paper, we develop a novel MH sampler that, in contrast, proposes re-writes\nof the entire sequence in each step via iterative prompting of a large language\nmodel. Our new sampler (a) allows for more efficient and accurate sampling from\na target distribution and (b) allows generation length to be determined through\nthe sampling procedure rather than fixed in advance, as past work has required.\nWe perform experiments on two controlled generation tasks, showing both\ndownstream performance gains and more accurate target distribution sampling in\ncomparison with single-token proposal techniques.",
            "author": [
                "Jarad Forristal",
                "Niloofar Mireshghallah",
                "Greg Durrett",
                "Taylor Berg-Kirkpatrick"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04510v1",
                "http://arxiv.org/pdf/2312.04510v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04509v1",
            "title": "In-context learning of state estimators",
            "updated": "2023-12-07T18:29:36Z",
            "published": "2023-12-07T18:29:36Z",
            "summary": "State estimation has a pivotal role in several applications, including but\nnot limited to advanced control design. Especially when dealing with nonlinear\nsystems state estimation is a nontrivial task, often entailing approximations\nand challenging fine-tuning phases. In this work, we propose to overcome these\nchallenges by formulating an in-context state-estimation problem, enabling us\nto learn a state estimator for a class of (nonlinear) systems abstracting from\nparticular instances of the state seen during training. To this end, we extend\nan in-context learning framework recently proposed for system identification,\nshowing via a benchmark numerical example that this approach allows us to (i)\nuse training data directly for the design of the state estimator, (ii) not\nrequiring extensive fine-tuning procedures, while (iii) achieving superior\nperformance compared to state-of-the-art benchmarks.",
            "author": [
                "Riccardo Busetto",
                "Valentina Breschi",
                "Marco Forgione",
                "Dario Piga",
                "Simone Formentin"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04509v1",
                "http://arxiv.org/pdf/2312.04509v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04504v1",
            "title": "Coordination-free Decentralised Federated Learning on Complex Networks:\n  Overcoming Heterogeneity",
            "updated": "2023-12-07T18:24:19Z",
            "published": "2023-12-07T18:24:19Z",
            "summary": "Federated Learning (FL) is a well-known framework for successfully performing\na learning task in an edge computing scenario where the devices involved have\nlimited resources and incomplete data representation. The basic assumption of\nFL is that the devices communicate directly or indirectly with a parameter\nserver that centrally coordinates the whole process, overcoming several\nchallenges associated with it. However, in highly pervasive edge scenarios, the\npresence of a central controller that oversees the process cannot always be\nguaranteed, and the interactions (i.e., the connectivity graph) between devices\nmight not be predetermined, resulting in a complex network structure. Moreover,\nthe heterogeneity of data and devices further complicates the learning process.\nThis poses new challenges from a learning standpoint that we address by\nproposing a communication-efficient Decentralised Federated Learning (DFL)\nalgorithm able to cope with them. Our solution allows devices communicating\nonly with their direct neighbours to train an accurate model, overcoming the\nheterogeneity induced by data and different training histories. Our results\nshow that the resulting local models generalise better than those trained with\ncompeting approaches, and do so in a more communication-efficient way.",
            "author": [
                "Lorenzo Valerio",
                "Chiara Boldrini",
                "Andrea Passarella",
                "J\u00e1nos Kert\u00e9sz",
                "M\u00e1rton Karsai",
                "Gerardo I\u00f1iguez"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04504v1",
                "http://arxiv.org/pdf/2312.04504v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.DC",
                "cs.MA",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04503v1",
            "title": "Data-Driven Robust Reinforcement Learning Control of Uncertain Nonlinear\n  Systems: Towards a Fully-Automated, Insulin-Based Artificial Pancreas",
            "updated": "2023-12-07T18:24:03Z",
            "published": "2023-12-07T18:24:03Z",
            "summary": "In this paper, a novel robust tracking control scheme for a general class of\ndiscrete-time nonlinear systems affected by unknown bounded uncertainty is\npresented. By solving a parameterized optimal tracking control problem subject\nto the unknown nominal system and a suitable cost function, the resulting\noptimal tracking control policy can ensure closed-loop stability by achieving a\nsufficiently small tracking error for the original uncertain nonlinear system.\nThe computation of the optimal tracking controller is accomplished through the\nderivation of a novel Q-function-based $\\lambda$-Policy Iteration algorithm.\nThe proposed algorithm not only enjoys rigorous theoretical guarantees, but\nalso avoids technical weaknesses of conventional reinforcement learning\nmethods. By employing a data-driven, critic-only least squares implementation,\nthe performance of the proposed algorithm is evaluated to the problem of\nfully-automated, insulin-based, closed-loop glucose control for patients\ndiagnosed with Type 1 and Type 2 Diabetes Mellitus. The U.S. FDA-accepted\nDMMS.R simulator from the Epsilon Group is used to conduct a comprehensive in\nsilico clinical campaign on a rich set of virtual subjects under completely\nunannounced meal and exercise settings. Simulation results underline the\nsuperior glycaemic behavior achieved by the derived approach, as well as its\noverall maturity for the design of highly-effective, closed-loop drug delivery\nsystems for personalized medicine.",
            "author": [
                "Alexandros Tanzanakis",
                "John Lygeros"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04503v1",
                "http://arxiv.org/pdf/2312.04503v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04501v1",
            "title": "Graph Metanetworks for Processing Diverse Neural Architectures",
            "updated": "2023-12-07T18:21:52Z",
            "published": "2023-12-07T18:21:52Z",
            "summary": "Neural networks efficiently encode learned information within their\nparameters. Consequently, many tasks can be unified by treating neural networks\nthemselves as input data. When doing so, recent studies demonstrated the\nimportance of accounting for the symmetries and geometry of parameter spaces.\nHowever, those works developed architectures tailored to specific networks such\nas MLPs and CNNs without normalization layers, and generalizing such\narchitectures to other types of networks can be challenging. In this work, we\novercome these challenges by building new metanetworks - neural networks that\ntake weights from other neural networks as input. Put simply, we carefully\nbuild graphs representing the input neural networks and process the graphs\nusing graph neural networks. Our approach, Graph Metanetworks (GMNs),\ngeneralizes to neural architectures where competing methods struggle, such as\nmulti-head attention layers, normalization layers, convolutional layers, ResNet\nblocks, and group-equivariant linear layers. We prove that GMNs are expressive\nand equivariant to parameter permutation symmetries that leave the input neural\nnetwork functions unchanged. We validate the effectiveness of our method on\nseveral metanetwork tasks over diverse neural network architectures.",
            "author": [
                "Derek Lim",
                "Haggai Maron",
                "Marc T. Law",
                "Jonathan Lorraine",
                "James Lucas"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04501v1",
                "http://arxiv.org/pdf/2312.04501v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04474v1",
            "title": "Chain of Code: Reasoning with a Language Model-Augmented Code Emulator",
            "updated": "2023-12-07T17:51:43Z",
            "published": "2023-12-07T17:51:43Z",
            "summary": "Code provides a general syntactic structure to build complex programs and\nperform precise computations when paired with a code interpreter -- we\nhypothesize that language models (LMs) can leverage code-writing to improve\nChain of Thought reasoning not only for logic and arithmetic tasks, but also\nfor linguistic ones (and in particular, those that are a mix of both). For\nexample, consider prompting an LM to write code that counts the number of times\nit detects sarcasm in an essay: the LM may struggle to write an implementation\nfor \"detect_sarcasm(string)\" that can be executed by the interpreter (handling\nthe edge cases would be insurmountable). However, LMs may still produce a valid\nsolution if they are used not only to write the code, but also to selectively\n\"emulate\" the interpreter by generating the expected output of\n\"detect_sarcasm(string)\" and other lines of code (e.g., that the interpreter\ncould not compile). In this work, we propose Chain of Code (CoT), a simple yet\nsurprisingly effective extension that improves LM code-driven reasoning. The\nkey idea is to encourage LMs to format linguistic sub-tasks in a program as\nflexible pseudocode that the compiler can explicitly catch undefined behaviors\nand hand off to simulate with an LM (as an \"LMulator\"). Experiments demonstrate\nthat Chain of Code outperforms Chain of Thought and other baselines across a\nvariety of benchmarks; on BIG-Bench Hard, Chain of Code achieves 84%, a gain of\n12% over Chain of Thought. CoT scales well with large and small models alike,\nand broadens the scope of reasoning questions that LMs can correctly answer by\n\"thinking in code\". Project webpage: https://chain-of-code.github.io/.",
            "author": [
                "Chengshu Li",
                "Jacky Liang",
                "Andy Zeng",
                "Xinyun Chen",
                "Karol Hausman",
                "Dorsa Sadigh",
                "Sergey Levine",
                "Li Fei-Fei",
                "Fei Xia",
                "Brian Ichter"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04474v1",
                "http://arxiv.org/pdf/2312.04474v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04469v1",
            "title": "On the Learnability of Watermarks for Language Models",
            "updated": "2023-12-07T17:41:44Z",
            "published": "2023-12-07T17:41:44Z",
            "summary": "Watermarking of language model outputs enables statistical detection of\nmodel-generated text, which has many applications in the responsible deployment\nof language models. Existing watermarking strategies operate by altering the\ndecoder of an existing language model, and the ability for a language model to\ndirectly learn to generate the watermark would have significant implications\nfor the real-world deployment of watermarks. First, learned watermarks could be\nused to build open models that naturally generate watermarked text, allowing\nfor open models to benefit from watermarking. Second, if watermarking is used\nto determine the provenance of generated text, an adversary can hurt the\nreputation of a victim model by spoofing its watermark and generating damaging\nwatermarked text. To investigate the learnability of watermarks, we propose\nwatermark distillation, which trains a student model to behave like a teacher\nmodel that uses decoding-based watermarking. We test our approach on three\ndistinct decoding-based watermarking strategies and various hyperparameter\nsettings, finding that models can learn to generate watermarked text with high\ndetectability. We also find limitations to learnability, including the loss of\nwatermarking capabilities under fine-tuning on normal text and high sample\ncomplexity when learning low-distortion watermarks.",
            "author": [
                "Chenchen Gu",
                "Xiang Lisa Li",
                "Percy Liang",
                "Tatsunori Hashimoto"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04469v1",
                "http://arxiv.org/pdf/2312.04469v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04464v1",
            "title": "Horizon-Free and Instance-Dependent Regret Bounds for Reinforcement\n  Learning with General Function Approximation",
            "updated": "2023-12-07T17:35:34Z",
            "published": "2023-12-07T17:35:34Z",
            "summary": "To tackle long planning horizon problems in reinforcement learning with\ngeneral function approximation, we propose the first algorithm, termed as\nUCRL-WVTR, that achieves both \\emph{horizon-free} and\n\\emph{instance-dependent}, since it eliminates the polynomial dependency on the\nplanning horizon. The derived regret bound is deemed \\emph{sharp}, as it\nmatches the minimax lower bound when specialized to linear mixture MDPs up to\nlogarithmic factors. Furthermore, UCRL-WVTR is \\emph{computationally efficient}\nwith access to a regression oracle. The achievement of such a horizon-free,\ninstance-dependent, and sharp regret bound hinges upon (i) novel algorithm\ndesigns: weighted value-targeted regression and a high-order moment estimator\nin the context of general function approximation; and (ii) fine-grained\nanalyses: a novel concentration bound of weighted non-linear least squares and\na refined analysis which leads to the tight instance-dependent bound. We also\nconduct comprehensive experiments to corroborate our theoretical findings.",
            "author": [
                "Jiayi Huang",
                "Han Zhong",
                "Liwei Wang",
                "Lin F. Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04464v1",
                "http://arxiv.org/pdf/2312.04464v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04463v1",
            "title": "Leveraging Transformer-based Language Models to Automate Requirements\n  Satisfaction Assessment",
            "updated": "2023-12-07T17:33:31Z",
            "published": "2023-12-07T17:33:31Z",
            "summary": "Requirements Satisfaction Assessment (RSA) evaluates whether the set of\ndesign elements linked to a single requirement provide sufficient coverage of\nthat requirement -- typically meaning that all concepts in the requirement are\naddressed by at least one of the design elements. RSA is an important software\nengineering activity for systems with any form of hierarchical decomposition --\nespecially safety or mission critical ones. In previous studies, researchers\nused basic Information Retrieval (IR) models to decompose requirements and\ndesign elements into chunks, and then evaluated the extent to which chunks of\ndesign elements covered all chunks in the requirement. However, results had low\naccuracy because many critical concepts that extend across the entirety of the\nsentence were not well represented when the sentence was parsed into\nindependent chunks. In this paper we leverage recent advances in natural\nlanguage processing to deliver significantly more accurate results. We propose\ntwo major architectures: Satisfaction BERT (Sat-BERT), and Dual-Satisfaction\nBERT (DSat-BERT), along with their multitask learning variants to improve\nsatisfaction assessments. We perform RSA on five different datasets and compare\nresults from our variants against the chunk-based legacy approach. All\nBERT-based models significantly outperformed the legacy baseline, and Sat-BERT\ndelivered the best results returning an average improvement of 124.75% in Mean\nAverage Precision.",
            "author": [
                "Amrit Poudel",
                "Jinfeng Lin",
                "Jane Cleland-Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04463v1",
                "http://arxiv.org/pdf/2312.04463v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04461v1",
            "title": "PhotoMaker: Customizing Realistic Human Photos via Stacked ID Embedding",
            "updated": "2023-12-07T17:32:29Z",
            "published": "2023-12-07T17:32:29Z",
            "summary": "Recent advances in text-to-image generation have made remarkable progress in\nsynthesizing realistic human photos conditioned on given text prompts. However,\nexisting personalized generation methods cannot simultaneously satisfy the\nrequirements of high efficiency, promising identity (ID) fidelity, and flexible\ntext controllability. In this work, we introduce PhotoMaker, an efficient\npersonalized text-to-image generation method, which mainly encodes an arbitrary\nnumber of input ID images into a stack ID embedding for preserving ID\ninformation. Such an embedding, serving as a unified ID representation, can not\nonly encapsulate the characteristics of the same input ID comprehensively, but\nalso accommodate the characteristics of different IDs for subsequent\nintegration. This paves the way for more intriguing and practically valuable\napplications. Besides, to drive the training of our PhotoMaker, we propose an\nID-oriented data construction pipeline to assemble the training data. Under the\nnourishment of the dataset constructed through the proposed pipeline, our\nPhotoMaker demonstrates better ID preservation ability than test-time\nfine-tuning based methods, yet provides significant speed improvements,\nhigh-quality generation results, strong generalization capabilities, and a wide\nrange of applications. Our project page is available at\nhttps://photo-maker.github.io/",
            "author": [
                "Zhen Li",
                "Mingdeng Cao",
                "Xintao Wang",
                "Zhongang Qi",
                "Ming-Ming Cheng",
                "Ying Shan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04461v1",
                "http://arxiv.org/pdf/2312.04461v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG",
                "cs.MM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04460v1",
            "title": "Probabilistic volumetric speckle suppression in OCT using deep learning",
            "updated": "2023-12-07T17:32:24Z",
            "published": "2023-12-07T17:32:24Z",
            "summary": "We present a deep learning framework for volumetric speckle reduction in\noptical coherence tomography (OCT) based on a conditional generative\nadversarial network (cGAN) that leverages the volumetric nature of OCT data. In\norder to utilize the volumetric nature of OCT data, our network takes partial\nOCT volumes as input, resulting in artifact-free despeckled volumes that\nexhibit excellent speckle reduction and resolution preservation in all three\ndimensions. Furthermore, we address the ongoing challenge of generating ground\ntruth data for supervised speckle suppression deep learning frameworks by using\nvolumetric non-local means despeckling-TNode to generate training data. We show\nthat, while TNode processing is computationally demanding, it serves as a\nconvenient, accessible gold-standard source for training data; our cGAN\nreplicates efficient suppression of speckle while preserving tissue structures\nwith dimensions approaching the system resolution of non-local means\ndespeckling while being two orders of magnitude faster than TNode. We\ndemonstrate fast, effective, and high-quality despeckling of the proposed\nnetwork in different tissue types acquired with three different OCT systems\ncompared to existing deep learning methods. The open-source nature of our work\nfacilitates re-training and deployment in any OCT system with an all-software\nimplementation, working around the challenge of generating high-quality,\nspeckle-free training data.",
            "author": [
                "Bhaskara Rao Chintada",
                "Sebasti\u00e1n Ruiz-Lopera",
                "Ren\u00e9 Restrepo",
                "Brett E. Bouma",
                "Martin Villiger",
                "N\u00e9stor Uribe-Patarroyo"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04460v1",
                "http://arxiv.org/pdf/2312.04460v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "physics.med-ph",
                "physics.optics"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04455v1",
            "title": "Fortify the Shortest Stave in Attention: Enhancing Context Awareness of\n  Large Language Models for Effective Tool Use",
            "updated": "2023-12-07T17:24:51Z",
            "published": "2023-12-07T17:24:51Z",
            "summary": "Recent advancements in large language models (LLMs) have significantly\nexpanded their functionality and skills as tool agents. In this paper, we argue\nthat a waveform pattern in the model's attention allocation has an impact on\nthe tool use performance, which degrades when the position of essential\ninformation hits the trough zone. To address this issue, we propose a novel\ninference method named Attention Buckets. This approach enables LLMs to handle\ncontext by conducting parallel processes, each featuring a unique RoPE angle\nbase that shapes the attention waveform. Attention Buckets ensures that an\nattention trough of a particular process can be compensated with an attention\npeak of another run, reducing the risk of the LLM missing essential information\nresiding within the attention trough. Our extensive experiments on the widely\nrecognized tool use benchmark demonstrate the efficacy of our approach, where a\n7B-parameter open-source model enhanced by Attention Buckets achieves SOTA\nperformance on par with GPT-4.",
            "author": [
                "Yuhan Chen",
                "Ang Lv",
                "Ting-En Lin",
                "Changyu Chen",
                "Yuchuan Wu",
                "Fei Huang",
                "Yongbin Li",
                "Rui Yan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04455v1",
                "http://arxiv.org/pdf/2312.04455v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04447v1",
            "title": "Privacy-preserving quantum federated learning via gradient hiding",
            "updated": "2023-12-07T17:16:30Z",
            "published": "2023-12-07T17:16:30Z",
            "summary": "Distributed quantum computing, particularly distributed quantum machine\nlearning, has gained substantial prominence for its capacity to harness the\ncollective power of distributed quantum resources, transcending the limitations\nof individual quantum nodes. Meanwhile, the critical concern of privacy within\ndistributed computing protocols remains a significant challenge, particularly\nin standard classical federated learning (FL) scenarios where data of\nparticipating clients is susceptible to leakage via gradient inversion attacks\nby the server. This paper presents innovative quantum protocols with quantum\ncommunication designed to address the FL problem, strengthen privacy measures,\nand optimize communication efficiency. In contrast to previous works that\nleverage expressive variational quantum circuits or differential privacy\ntechniques, we consider gradient information concealment using quantum states\nand propose two distinct FL protocols, one based on private inner-product\nestimation and the other on incremental learning. These protocols offer\nsubstantial advancements in privacy preservation with low communication\nresources, forging a path toward efficient quantum communication-assisted FL\nprotocols and contributing to the development of secure distributed quantum\nmachine learning, thus addressing critical privacy concerns in the quantum\ncomputing era.",
            "author": [
                "Changhao Li",
                "Niraj Kumar",
                "Zhixin Song",
                "Shouvanik Chakrabarti",
                "Marco Pistoia"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04447v1",
                "http://arxiv.org/pdf/2312.04447v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.CR",
                "cs.DC",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04435v1",
            "title": "Deep3DSketch: 3D modeling from Free-hand Sketches with View- and\n  Structural-Aware Adversarial Training",
            "updated": "2023-12-07T16:57:38Z",
            "published": "2023-12-07T16:57:38Z",
            "summary": "This work aims to investigate the problem of 3D modeling using single\nfree-hand sketches, which is one of the most natural ways we humans express\nideas. Although sketch-based 3D modeling can drastically make the 3D modeling\nprocess more accessible, the sparsity and ambiguity of sketches bring\nsignificant challenges for creating high-fidelity 3D models that reflect the\ncreators' ideas. In this work, we propose a view- and structural-aware deep\nlearning approach, \\textit{Deep3DSketch}, which tackles the ambiguity and fully\nuses sparse information of sketches, emphasizing the structural information.\nSpecifically, we introduced random pose sampling on both 3D shapes and 2D\nsilhouettes, and an adversarial training scheme with an effective progressive\ndiscriminator to facilitate learning of the shape structures. Extensive\nexperiments demonstrated the effectiveness of our approach, which outperforms\nexisting methods -- with state-of-the-art (SOTA) performance on both synthetic\nand real datasets.",
            "author": [
                "Tianrun Chen",
                "Chenglong Fu",
                "Lanyun Zhu",
                "Papa Mao",
                "Jia Zhang",
                "Ying Zang",
                "Lingyun Sun"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04435v1",
                "http://arxiv.org/pdf/2312.04435v1"
            ],
            "primary_category": "cs.MM",
            "category": [
                "cs.MM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04433v1",
            "title": "DreamVideo: Composing Your Dream Videos with Customized Subject and\n  Motion",
            "updated": "2023-12-07T16:57:26Z",
            "published": "2023-12-07T16:57:26Z",
            "summary": "Customized generation using diffusion models has made impressive progress in\nimage generation, but remains unsatisfactory in the challenging video\ngeneration task, as it requires the controllability of both subjects and\nmotions. To that end, we present DreamVideo, a novel approach to generating\npersonalized videos from a few static images of the desired subject and a few\nvideos of target motion. DreamVideo decouples this task into two stages,\nsubject learning and motion learning, by leveraging a pre-trained video\ndiffusion model. The subject learning aims to accurately capture the fine\nappearance of the subject from provided images, which is achieved by combining\ntextual inversion and fine-tuning of our carefully designed identity adapter.\nIn motion learning, we architect a motion adapter and fine-tune it on the given\nvideos to effectively model the target motion pattern. Combining these two\nlightweight and efficient adapters allows for flexible customization of any\nsubject with any motion. Extensive experimental results demonstrate the\nsuperior performance of our DreamVideo over the state-of-the-art methods for\ncustomized video generation. Our project page is at\nhttps://dreamvideo-t2v.github.io.",
            "author": [
                "Yujie Wei",
                "Shiwei Zhang",
                "Zhiwu Qing",
                "Hangjie Yuan",
                "Zhiheng Liu",
                "Yu Liu",
                "Yingya Zhang",
                "Jingren Zhou",
                "Hongming Shan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04433v1",
                "http://arxiv.org/pdf/2312.04433v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04432v1",
            "title": "FreqFed: A Frequency Analysis-Based Approach for Mitigating Poisoning\n  Attacks in Federated Learning",
            "updated": "2023-12-07T16:56:24Z",
            "published": "2023-12-07T16:56:24Z",
            "summary": "Federated learning (FL) is a collaborative learning paradigm allowing\nmultiple clients to jointly train a model without sharing their training data.\nHowever, FL is susceptible to poisoning attacks, in which the adversary injects\nmanipulated model updates into the federated model aggregation process to\ncorrupt or destroy predictions (untargeted poisoning) or implant hidden\nfunctionalities (targeted poisoning or backdoors). Existing defenses against\npoisoning attacks in FL have several limitations, such as relying on specific\nassumptions about attack types and strategies or data distributions or not\nsufficiently robust against advanced injection techniques and strategies and\nsimultaneously maintaining the utility of the aggregated model. To address the\ndeficiencies of existing defenses, we take a generic and completely different\napproach to detect poisoning (targeted and untargeted) attacks. We present\nFreqFed, a novel aggregation mechanism that transforms the model updates (i.e.,\nweights) into the frequency domain, where we can identify the core frequency\ncomponents that inherit sufficient information about weights. This allows us to\neffectively filter out malicious updates during local training on the clients,\nregardless of attack types, strategies, and clients' data distributions. We\nextensively evaluate the efficiency and effectiveness of FreqFed in different\napplication domains, including image classification, word prediction, IoT\nintrusion detection, and speech recognition. We demonstrate that FreqFed can\nmitigate poisoning attacks effectively with a negligible impact on the utility\nof the aggregated model.",
            "author": [
                "Hossein Fereidooni",
                "Alessandro Pegoraro",
                "Phillip Rieger",
                "Alexandra Dmitrienko",
                "Ahmad-Reza Sadeghi"
            ],
            "link": [
                "http://dx.doi.org/10.14722/ndss.2024.23620",
                "http://arxiv.org/abs/2312.04432v1",
                "http://arxiv.org/pdf/2312.04432v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04423v1",
            "title": "Scalable Knowledge Graph Construction and Inference on Human Genome\n  Variants",
            "updated": "2023-12-07T16:48:32Z",
            "published": "2023-12-07T16:48:32Z",
            "summary": "Real-world knowledge can be represented as a graph consisting of entities and\nrelationships between the entities. The need for efficient and scalable\nsolutions arises when dealing with vast genomic data, like RNA-sequencing.\nKnowledge graphs offer a powerful approach for various tasks in such\nlarge-scale genomic data, such as analysis and inference. In this work,\nvariant-level information extracted from the RNA-sequences of vaccine-na\\\"ive\nCOVID-19 patients have been represented as a unified, large knowledge graph.\nVariant call format (VCF) files containing the variant-level information were\nannotated to include further information for each variant. The data records in\nthe annotated files were then converted to Resource Description Framework (RDF)\ntriples. Each VCF file obtained had an associated CADD scores file that\ncontained the raw and Phred-scaled scores for each variant. An ontology was\ndefined for the VCF and CADD scores files. Using this ontology and the\nextracted information, a large, scalable knowledge graph was created. Available\ngraph storage was then leveraged to query and create datasets for further\ndownstream tasks. We also present a case study using the knowledge graph and\nperform a classification task using graph machine learning. We also draw\ncomparisons between different Graph Neural Networks (GNNs) for the case study.",
            "author": [
                "Shivika Prasanna",
                "Deepthi Rao",
                "Eduardo Simoes",
                "Praveen Rao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04423v1",
                "http://arxiv.org/pdf/2312.04423v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.DB",
                "q-bio.QM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04416v1",
            "title": "Monitoring Sustainable Global Development Along Shared Socioeconomic\n  Pathways",
            "updated": "2023-12-07T16:38:20Z",
            "published": "2023-12-07T16:38:20Z",
            "summary": "Sustainable global development is one of the most prevalent challenges facing\nthe world today, hinging on the equilibrium between socioeconomic growth and\nenvironmental sustainability. We propose approaches to monitor and quantify\nsustainable development along the Shared Socioeconomic Pathways (SSPs),\nincluding mathematically derived scoring algorithms, and machine learning\nmethods. These integrate socioeconomic and environmental datasets, to produce\nan interpretable metric for SSP alignment. An initial study demonstrates\npromising results, laying the groundwork for the application of different\nmethods to the monitoring of sustainable global development.",
            "author": [
                "Michelle W. L. Wan",
                "Jeffrey N. Clark",
                "Edward A. Small",
                "Elena Fillola Mayoral",
                "Ra\u00fal Santos-Rodr\u00edguez"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04416v1",
                "http://arxiv.org/pdf/2312.04416v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04412v1",
            "title": "Developing Elementary Federated Learning Algorithms Leveraging the\n  ChatGPT",
            "updated": "2023-12-07T16:34:47Z",
            "published": "2023-12-07T16:34:47Z",
            "summary": "The Python Testbed for Federated Learning Algorithms is a simple Python FL\nframework easy to use by ML&AI developers who do not need to be professional\nprogrammers, and this paper shows that it is also amenable to emerging AI\ntools. In this paper, we successfully developed three elementary FL algorithms\nusing the following three steps process: (i) specify context, (ii) ask ChatGPT\nto complete server and clients' callback functions, and (iii) verify the\ngenerated code.",
            "author": [
                "Miroslav Popovic",
                "Marko Popovic",
                "Ivan Kastelan",
                "Miodrag Djukic",
                "Ilija Basicevic"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04412v1",
                "http://arxiv.org/pdf/2312.04412v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04411v1",
            "title": "Family Structure, Gender and Subjective Well-being: Effect of Child ren\n  before and after COVID 19 in Japan",
            "updated": "2023-12-07T16:29:18Z",
            "published": "2023-12-07T16:29:18Z",
            "summary": "Grandparents were anticipated to participated in grand-rearing. The COVID-19\npandemic had detached grandparents from rearing grandchildren. The research\nquestions of this study were as follows: How does the change in family\nrelations impact the well-being (SWB) of grandparents and parents? We examined\nhow family structure influenced subjective SWB before and after COVID-19. We\nfocused on the effects of children, grandchildren, and their gender on\ngrandparents and parents. We found that compared with the happiness level\nbefore COVID-19, (1) granddaughters increased their grandmothers SWB after\nCOVID-19, (2) both daughters and sons reduced their fathers SWB after COVID-19,\nwhereas neither daughters nor sons changed their mothers SWB, and (3) the\nnegative effect of sons reduced substantially if their fathers had younger\nbrothers. Learning from interactions with younger brothers in childhood,\nfathers could avoid the deterioration of relationships with their sons, even\nwhen unexpected events possibly changed the lifestyle of the family and their\nrelationship.",
            "author": [
                "Eiji Yamamura",
                "Fumio Ohtake"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04411v1",
                "http://arxiv.org/pdf/2312.04411v1"
            ],
            "primary_category": "econ.GN",
            "category": [
                "econ.GN",
                "q-fin.EC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04404v1",
            "title": "On the Impact of Multi-dimensional Local Differential Privacy on\n  Fairness",
            "updated": "2023-12-07T16:17:34Z",
            "published": "2023-12-07T16:17:34Z",
            "summary": "Automated decision systems are increasingly used to make consequential\ndecisions in people's lives. Due to the sensitivity of the manipulated data as\nwell as the resulting decisions, several ethical concerns need to be addressed\nfor the appropriate use of such technologies, in particular, fairness and\nprivacy. Unlike previous work, which focused on centralized differential\nprivacy (DP) or local DP (LDP) for a single sensitive attribute, in this paper,\nwe examine the impact of LDP in the presence of several sensitive attributes\n(i.e., multi-dimensional data) on fairness. Detailed empirical analysis on\nsynthetic and benchmark datasets revealed very relevant observations. In\nparticular, (1) multi-dimensional LDP is an efficient approach to reduce\ndisparity, (2) the multi-dimensional approach of LDP (independent vs. combined)\nmatters only at low privacy guarantees, and (3) the outcome Y distribution has\nan important effect on which group is more sensitive to the obfuscation. Last,\nwe summarize our findings in the form of recommendations to guide practitioners\nin adopting effective privacy-preserving practices while maintaining fairness\nand utility in ML applications.",
            "author": [
                "karima Makhlouf",
                "Heber H. Arcolezi",
                "Sami Zhioua",
                "Ghassen Ben Brahim",
                "Catuscia Palamidessi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04404v1",
                "http://arxiv.org/pdf/2312.04404v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CR",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04402v1",
            "title": "Semi-Supervised Active Learning for Semantic Segmentation in Unknown\n  Environments Using Informative Path Planning",
            "updated": "2023-12-07T16:16:47Z",
            "published": "2023-12-07T16:16:47Z",
            "summary": "Semantic segmentation enables robots to perceive and reason about their\nenvironments beyond geometry. Most of such systems build upon deep learning\napproaches. As autonomous robots are commonly deployed in initially unknown\nenvironments, pre-training on static datasets cannot always capture the variety\nof domains and limits the robot's perception performance during missions.\nRecently, self-supervised and fully supervised active learning methods emerged\nto improve a robot's vision. These approaches rely on large in-domain\npre-training datasets or require substantial human labelling effort. We propose\na planning method for semi-supervised active learning of semantic segmentation\nthat substantially reduces human labelling requirements compared to fully\nsupervised approaches. We leverage an adaptive map-based planner guided towards\nthe frontiers of unexplored space with high model uncertainty collecting\ntraining data for human labelling. A key aspect of our approach is to combine\nthe sparse high-quality human labels with pseudo labels automatically extracted\nfrom highly certain environment map areas. Experimental results show that our\nmethod reaches segmentation performance close to fully supervised approaches\nwith drastically reduced human labelling effort while outperforming\nself-supervised approaches.",
            "author": [
                "Julius R\u00fcckin",
                "Federico Magistri",
                "Cyrill Stachniss",
                "Marija Popovi\u0107"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04402v1",
                "http://arxiv.org/pdf/2312.04402v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04398v1",
            "title": "Intelligent Anomaly Detection for Lane Rendering Using Transformer with\n  Self-Supervised Pre-Training and Customized Fine-Tuning",
            "updated": "2023-12-07T16:10:10Z",
            "published": "2023-12-07T16:10:10Z",
            "summary": "The burgeoning navigation services using digital maps provide great\nconvenience to drivers. Nevertheless, the presence of anomalies in lane\nrendering map images occasionally introduces potential hazards, as such\nanomalies can be misleading to human drivers and consequently contribute to\nunsafe driving conditions. In response to this concern and to accurately and\neffectively detect the anomalies, this paper transforms lane rendering image\nanomaly detection into a classification problem and proposes a four-phase\npipeline consisting of data pre-processing, self-supervised pre-training with\nthe masked image modeling (MiM) method, customized fine-tuning using\ncross-entropy based loss with label smoothing, and post-processing to tackle it\nleveraging state-of-the-art deep learning techniques, especially those\ninvolving Transformer models. Various experiments verify the effectiveness of\nthe proposed pipeline. Results indicate that the proposed pipeline exhibits\nsuperior performance in lane rendering image anomaly detection, and notably,\nthe self-supervised pre-training with MiM can greatly enhance the detection\naccuracy while significantly reducing the total training time. For instance,\nemploying the Swin Transformer with Uniform Masking as self-supervised\npretraining (Swin-Trans-UM) yielded a heightened accuracy at 94.77% and an\nimproved Area Under The Curve (AUC) score of 0.9743 compared with the pure Swin\nTransformer without pre-training (Swin-Trans) with an accuracy of 94.01% and an\nAUC of 0.9498. The fine-tuning epochs were dramatically reduced to 41 from the\noriginal 280. In conclusion, the proposed pipeline, with its incorporation of\nself-supervised pre-training using MiM and other advanced deep learning\ntechniques, emerges as a robust solution for enhancing the accuracy and\nefficiency of lane rendering image anomaly detection in digital navigation\nsystems.",
            "author": [
                "Yongqi Dong",
                "Xingmin Lu",
                "Ruohan Li",
                "Wei Song",
                "Bart van Arem",
                "Haneen Farah"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04398v1",
                "http://arxiv.org/pdf/2312.04398v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG",
                "eess.IV",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04396v1",
            "title": "Canonical scattering problem in topological metamaterials: Valley-Hall\n  modes through a bend",
            "updated": "2023-12-07T16:08:39Z",
            "published": "2023-12-07T16:08:39Z",
            "summary": "We study the amount of backscattering of Valley Hall modes in a classical\ntopological insulator. In reciprocal systems, the conservation of the valley\nindex has been argued to be at the root of the high-transmission of Valley Hall\nmodes, observed in many experimental realisations. Here, we reconsider this\nhypothesis by quantitatively analysing the canonical scattering problem of\ninterface Valley Hall modes impinging on sharp bends which may or may not\nconserve the valley index. We consider a tight binding model of graphene\nribbons with an interface and compute the reflection and transmission\ncoefficients using a transfer matrix formalism. We find that, in all\nconfigurations, the transmission of Valley Hall modes is close to being\nmaximal, even in cases where the valley index is not conserved. Our results\nreinforce the alternative interpretation of the high-transmission of Valley\nHall modes in reciprocal metamaterials as a consequence of a favorable mode\nmatching on each side of the defect and serve as a reference case for the\ndesign of Valley Hall type metamaterial.",
            "author": [
                "Theo Torres",
                "C\u00e9dric Bellis",
                "R\u00e9gis Cottereau",
                "Antonin Coutant"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04396v1",
                "http://arxiv.org/pdf/2312.04396v1"
            ],
            "primary_category": "cond-mat.mes-hall",
            "category": [
                "cond-mat.mes-hall",
                "physics.class-ph",
                "physics.optics"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04393v1",
            "title": "PhysHOI: Physics-Based Imitation of Dynamic Human-Object Interaction",
            "updated": "2023-12-07T16:06:31Z",
            "published": "2023-12-07T16:06:31Z",
            "summary": "Humans interact with objects all the time. Enabling a humanoid to learn\nhuman-object interaction (HOI) is a key step for future smart animation and\nintelligent robotics systems. However, recent progress in physics-based HOI\nrequires carefully designed task-specific rewards, making the system unscalable\nand labor-intensive. This work focuses on dynamic HOI imitation: teaching\nhumanoid dynamic interaction skills through imitating kinematic HOI\ndemonstrations. It is quite challenging because of the complexity of the\ninteraction between body parts and objects and the lack of dynamic HOI data. To\nhandle the above issues, we present PhysHOI, the first physics-based whole-body\nHOI imitation approach without task-specific reward designs. Except for the\nkinematic HOI representations of humans and objects, we introduce the contact\ngraph to model the contact relations between body parts and objects explicitly.\nA contact graph reward is also designed, which proved to be critical for\nprecise HOI imitation. Based on the key designs, PhysHOI can imitate diverse\nHOI tasks simply yet effectively without prior knowledge. To make up for the\nlack of dynamic HOI scenarios in this area, we introduce the BallPlay dataset\nthat contains eight whole-body basketball skills. We validate PhysHOI on\ndiverse HOI tasks, including whole-body grasping and basketball skills.",
            "author": [
                "Yinhuai Wang",
                "Jing Lin",
                "Ailing Zeng",
                "Zhengyi Luo",
                "Jian Zhang",
                "Lei Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04393v1",
                "http://arxiv.org/pdf/2312.04393v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04386v1",
            "title": "Model-Based Epistemic Variance of Values for Risk-Aware Policy\n  Optimization",
            "updated": "2023-12-07T15:55:58Z",
            "published": "2023-12-07T15:55:58Z",
            "summary": "We consider the problem of quantifying uncertainty over expected cumulative\nrewards in model-based reinforcement learning. In particular, we focus on\ncharacterizing the variance over values induced by a distribution over MDPs.\nPrevious work upper bounds the posterior variance over values by solving a\nso-called uncertainty Bellman equation (UBE), but the over-approximation may\nresult in inefficient exploration. We propose a new UBE whose solution\nconverges to the true posterior variance over values and leads to lower regret\nin tabular exploration problems. We identify challenges to apply the UBE theory\nbeyond tabular problems and propose a suitable approximation. Based on this\napproximation, we introduce a general-purpose policy optimization algorithm,\nQ-Uncertainty Soft Actor-Critic (QU-SAC), that can be applied for either\nrisk-seeking or risk-averse policy optimization with minimal changes.\nExperiments in both online and offline RL demonstrate improved performance\ncompared to other uncertainty estimation methods.",
            "author": [
                "Carlos E. Luis",
                "Alessandro G. Bottero",
                "Julia Vinogradska",
                "Felix Berkenkamp",
                "Jan Peters"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04386v1",
                "http://arxiv.org/pdf/2312.04386v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04382v1",
            "title": "Adversarial Denoising Diffusion Model for Unsupervised Anomaly Detection",
            "updated": "2023-12-07T15:51:19Z",
            "published": "2023-12-07T15:51:19Z",
            "summary": "In this paper, we propose the Adversarial Denoising Diffusion Model (ADDM).\nThe ADDM is based on the Denoising Diffusion Probabilistic Model (DDPM) but\ncomplementarily trained by adversarial learning. The proposed adversarial\nlearning is achieved by classifying model-based denoised samples and samples to\nwhich random Gaussian noise is added to a specific sampling step. With the\naddition of explicit adversarial learning on data samples, ADDM can learn the\nsemantic characteristics of the data more robustly during training, which\nachieves a similar data sampling performance with much fewer sampling steps\nthan DDPM. We apply ADDM to anomaly detection in unsupervised MRI images.\nExperimental results show that the proposed ADDM outperformed existing\ngenerative model-based unsupervised anomaly detection methods. In particular,\ncompared to other DDPM-based anomaly detection methods, the proposed ADDM shows\nbetter performance with the same number of sampling steps and similar\nperformance with 50% fewer sampling steps.",
            "author": [
                "Jongmin Yu",
                "Hyeontaek Oh",
                "Jinhong Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04382v1",
                "http://arxiv.org/pdf/2312.04382v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04377v1",
            "title": "HARQ-IR Aided Short Packet Communications: BLER Analysis and Throughput\n  Maximization",
            "updated": "2023-12-07T15:47:18Z",
            "published": "2023-12-07T15:47:18Z",
            "summary": "This paper introduces hybrid automatic repeat request with incremental\nredundancy (HARQ-IR) to boost the reliability of short packet communications.\nThe finite blocklength information theory and correlated decoding events\ntremendously preclude the analysis of average block error rate (BLER).\nFortunately, the recursive form of average BLER motivates us to calculate its\nvalue through the trapezoidal approximation and Gauss-Laguerre quadrature.\nMoreover, the asymptotic analysis is performed to derive a simple expression\nfor the average BLER at high signal-to-noise ratio (SNR). Then, we study the\nmaximization of long term average throughput (LTAT) via power allocation\nmeanwhile ensuring the power and the BLER constraints. For tractability, the\nasymptotic BLER is employed to solve the problem through geometric programming\n(GP). However, the GP-based solution underestimates the LTAT at low SNR due to\na large approximation error in this case. Alternatively, we also develop a deep\nreinforcement learning (DRL)-based framework to learn power allocation policy.\nIn particular, the optimization problem is transformed into a constrained\nMarkov decision process, which is solved by integrating deep deterministic\npolicy gradient (DDPG) with subgradient method. The numerical results finally\ndemonstrate that the DRL-based method outperforms the GP-based one at low SNR,\nalbeit at the cost of increasing computational burden.",
            "author": [
                "Fuchao He",
                "Zheng Shi",
                "Guanghua Yang",
                "Xiaofan Li",
                "Xinrong Ye",
                "Shaodan Ma"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04377v1",
                "http://arxiv.org/pdf/2312.04377v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "eess.SP",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04374v1",
            "title": "Deep Dynamics: Vehicle Dynamics Modeling with a Physics-Informed Neural\n  Network for Autonomous Racing",
            "updated": "2023-12-07T15:44:56Z",
            "published": "2023-12-07T15:44:56Z",
            "summary": "Autonomous racing is a critical research area for autonomous driving,\npresenting significant challenges in vehicle dynamics modeling, such as\nbalancing model precision and computational efficiency at high speeds\n(>280kmph), where minor errors in modeling have severe consequences. Existing\nphysics-based models for vehicle dynamics require elaborate testing setups and\ntuning, which are hard to implement, time-intensive, and cost-prohibitive.\nConversely, purely data-driven approaches do not generalize well and cannot\nadequately ensure physical constraints on predictions. This paper introduces\nDeep Dynamics, a physics-informed neural network (PINN) for vehicle dynamics\nmodeling of an autonomous racecar. It combines physics coefficient estimation\nand dynamical equations to accurately predict vehicle states at high speeds and\nincludes a unique Physics Guard layer to ensure internal coefficient estimates\nremain within their nominal physical ranges. Open-loop and closed-loop\nperformance assessments, using a physics-based simulator and full-scale\nautonomous Indy racecar data, highlight Deep Dynamics as a promising approach\nfor modeling racecar vehicle dynamics.",
            "author": [
                "John Chrosniak",
                "Jingyun Ning",
                "Madhur Behl"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04374v1",
                "http://arxiv.org/pdf/2312.04374v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI",
                "cs.LG",
                "I.2.9; I.6"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04371v1",
            "title": "A Scalable Network-Aware Multi-Agent Reinforcement Learning Framework\n  for Decentralized Inverter-based Voltage Control",
            "updated": "2023-12-07T15:42:53Z",
            "published": "2023-12-07T15:42:53Z",
            "summary": "This paper addresses the challenges associated with decentralized voltage\ncontrol in power grids due to an increase in distributed generations (DGs).\nTraditional model-based voltage control methods struggle with the rapid energy\nfluctuations and uncertainties of these DGs. While multi-agent reinforcement\nlearning (MARL) has shown potential for decentralized secondary control,\nscalability issues arise when dealing with a large number of DGs. This problem\nlies in the dominant centralized training and decentralized execution (CTDE)\nframework, where the critics take global observations and actions. To overcome\nthese challenges, we propose a scalable network-aware (SNA) framework that\nleverages network structure to truncate the input to the critic's Q-function,\nthereby improving scalability and reducing communication costs during training.\nFurther, the SNA framework is theoretically grounded with provable\napproximation guarantee, and it can seamlessly integrate with multiple\nmulti-agent actor-critic algorithms. The proposed SNA framework is successfully\ndemonstrated in a system with 114 DGs, providing a promising solution for\ndecentralized voltage control in increasingly complex power grid systems.",
            "author": [
                "Han Xu",
                "Jialin Zheng",
                "Guannan Qu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04371v1",
                "http://arxiv.org/pdf/2312.04371v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "cs.LG",
                "cs.MA",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04370v1",
            "title": "Investigating the Design Space of Diffusion Models for Speech\n  Enhancement",
            "updated": "2023-12-07T15:40:55Z",
            "published": "2023-12-07T15:40:55Z",
            "summary": "Diffusion models are a new class of generative models that have shown\noutstanding performance in image generation literature. As a consequence,\nstudies have attempted to apply diffusion models to other tasks, such as speech\nenhancement. A popular approach in adapting diffusion models to speech\nenhancement consists in modelling a progressive transformation between the\nclean and noisy speech signals. However, one popular diffusion model framework\npreviously laid in image generation literature did not account for such a\ntransformation towards the system input, which prevents from relating the\nexisting diffusion-based speech enhancement systems with the aforementioned\ndiffusion model framework. To address this, we extend this framework to account\nfor the progressive transformation between the clean and noisy speech signals.\nThis allows us to apply recent developments from image generation literature,\nand to systematically investigate design aspects of diffusion models that\nremain largely unexplored for speech enhancement, such as the neural network\npreconditioning, the training loss weighting, the stochastic differential\nequation (SDE), or the amount of stochasticity injected in the reverse process.\nWe show that the performance of previous diffusion-based speech enhancement\nsystems cannot be attributed to the progressive transformation between the\nclean and noisy speech signals. Moreover, we show that a proper choice of\npreconditioning, training loss weighting, SDE and sampler allows to outperform\na popular diffusion-based speech enhancement system in terms of perceptual\nmetrics while using fewer sampling steps, thus reducing the computational cost\nby a factor of four.",
            "author": [
                "Philippe Gonzalez",
                "Zheng-Hua Tan",
                "Jan \u00d8stergaard",
                "Jesper Jensen",
                "Tommy Sonne Alstr\u00f8m",
                "Tobias May"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04370v1",
                "http://arxiv.org/pdf/2312.04370v1"
            ],
            "primary_category": "eess.AS",
            "category": [
                "eess.AS",
                "cs.LG",
                "cs.SD"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04361v1",
            "title": "Detecting the prompt optical flashes of gamma-ray bursts with small\n  telescope arrays",
            "updated": "2023-12-07T15:28:44Z",
            "published": "2023-12-07T15:28:44Z",
            "summary": "We present an observational approach for the independent detection of the\nprompt optical emission of long gamma-ray bursts (GRBs). For this purpose, we\nexplore the potential of the Large Array Survey Telescope (LAST). This array of\nsmall optical telescopes can be used to scan a wide region of the sky, and to\nfocus on a smaller field of view with increased sensitivity, as needed. The\nmodularity of the array facilitates dynamic scanning of multiple fields, by\nshifting telescope pointing directions with high cadence. This can\nsignificantly increase the effective sky-coverage of a blind survey on short\ntime scales. For events associated with gamma-ray counterparts, the valuable\nearly-time data can supplement high-energy observations. Regardless of\ngamma-ray association, detections can potentially be used to explore various\nphenomena associated with GRBs, such as orphan afterglows; dirty fireballs; and\nchoked jets. We simulate a sample of GRBs and their respective optical signals\nat early times. After accounting for dynamic cadence, the light curves are\ngiven as input to a machine learning classifier, used to identify astrophysical\ntransients. We find that by dedicating half of a LAST array to a blind search,\none would expect to discover 7-11 GRBs per year, corresponding to an\napproximate intrinsic event rate of 0.12 per square degree per year.",
            "author": [
                "Iftach Sadeh"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04361v1",
                "http://arxiv.org/pdf/2312.04361v1"
            ],
            "primary_category": "astro-ph.HE",
            "category": [
                "astro-ph.HE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04356v1",
            "title": "NeuJeans: Private Neural Network Inference with Joint Optimization of\n  Convolution and Bootstrapping",
            "updated": "2023-12-07T15:23:07Z",
            "published": "2023-12-07T15:23:07Z",
            "summary": "Fully homomorphic encryption (FHE) is a promising cryptographic primitive for\nrealizing private neural network inference (PI) services by allowing a client\nto fully offload the inference task to a cloud server while keeping the client\ndata oblivious to the server. This work proposes NeuJeans, an FHE-based\nsolution for the PI of deep convolutional neural networks (CNNs). NeuJeans\ntackles the critical problem of the enormous computational cost for the FHE\nevaluation of convolutional layers (conv2d), mainly due to the high cost of\ndata reordering and bootstrapping. We first propose an encoding method\nintroducing nested structures inside encoded vectors for FHE, which enables us\nto develop efficient conv2d algorithms with reduced data reordering costs.\nHowever, the new encoding method also introduces additional computations for\nconversion between encoding methods, which could negate its advantages. We\ndiscover that fusing conv2d with bootstrapping eliminates such computations\nwhile reducing the cost of bootstrapping. Then, we devise optimized execution\nflows for various types of conv2d and apply them to end-to-end implementation\nof CNNs. NeuJeans accelerates the performance of conv2d by up to 5.68 times\ncompared to state-of-the-art FHE-based PI work and performs the PI of a CNN at\nthe scale of ImageNet (ResNet18) within a mere few seconds",
            "author": [
                "Jae Hyung Ju",
                "Jaiyoung Park",
                "Jongmin Kim",
                "Donghwan Kim",
                "Jung Ho Ahn"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04356v1",
                "http://arxiv.org/pdf/2312.04356v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04350v1",
            "title": "CLadder: A Benchmark to Assess Causal Reasoning Capabilities of Language\n  Models",
            "updated": "2023-12-07T15:12:12Z",
            "published": "2023-12-07T15:12:12Z",
            "summary": "The ability to perform causal reasoning is widely considered a core feature\nof intelligence. In this work, we investigate whether large language models\n(LLMs) can coherently reason about causality. Much of the existing work in\nnatural language processing (NLP) focuses on evaluating commonsense causal\nreasoning in LLMs, thus failing to assess whether a model can perform causal\ninference in accordance with a set of well-defined formal rules. To address\nthis, we propose a new NLP task, causal inference in natural language, inspired\nby the \"causal inference engine\" postulated by Judea Pearl et al. We compose a\nlarge dataset, CLadder, with 10K samples: based on a collection of causal\ngraphs and queries (associational, interventional, and counterfactual), we\nobtain symbolic questions and ground-truth answers, through an oracle causal\ninference engine. These are then translated into natural language. We evaluate\nmultiple LLMs on our dataset, and we introduce and evaluate a bespoke\nchain-of-thought prompting strategy, CausalCoT. We show that our task is highly\nchallenging for LLMs, and we conduct an in-depth analysis to gain deeper\ninsight into the causal reasoning abilities of LLMs. Our data is open-sourced\nat https://huggingface.co/datasets/causalNLP/cladder, and our code can be found\nat https://github.com/causalNLP/cladder.",
            "author": [
                "Zhijing Jin",
                "Yuen Chen",
                "Felix Leeb",
                "Luigi Gresele",
                "Ojasv Kamal",
                "Zhiheng Lyu",
                "Kevin Blin",
                "Fernando Gonzalez Adauto",
                "Max Kleiman-Weiner",
                "Mrinmaya Sachan",
                "Bernhard Sch\u00f6lkopf"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04350v1",
                "http://arxiv.org/pdf/2312.04350v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04346v1",
            "title": "Improved Efficient Two-Stage Denoising Diffusion Power System\n  Measurement Recovery Against False Data Injection Attacks and Data Losses",
            "updated": "2023-12-07T15:06:06Z",
            "published": "2023-12-07T15:06:06Z",
            "summary": "Measurement uncertainties, represented by cyber-attacks and data losses,\nseriously degrade the quality of power system measurements. Fortunately, the\npowerful generation ability of the denoising diffusion models can enable more\nprecise measurement generation for power system data recovery. However, the\ncontrollable data generation and efficient computing methods of denoising\ndiffusion models for deterministic trajectory still need further investigation.\nTo this end, this paper proposes an improved two-stage denoising diffusion\nmodel (TSDM) to identify and reconstruct the measurements with various\nmeasurement uncertainties. The first stage of the model comprises a\nclassifier-guided conditional anomaly detection component, while the second\nstage involves diffusion-based measurement imputation component. Moreover, the\nproposed TSDM adopts precise means and optimal variances to accelerate the\ndiffusion generation process with subsequence sampling. Extensive numerical\ncase studies demonstrate that the proposed TSDM can accurately recover power\nsystem measurements despite strong randomness under renewable energy\nintegration and highly nonlinear dynamics under complex cyber-physical\ncontingencies. Additionally, the proposed TSDM has stronger robustness compared\nto existing reconstruction networks and exhibits lower computational complexity\nthan general denoising diffusion models.",
            "author": [
                "Jianhua Pei",
                "Jingyu Wang",
                "Dongyuan Shi",
                "Ping Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04346v1",
                "http://arxiv.org/pdf/2312.04346v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04344v1",
            "title": "Enhancing Medical Task Performance in GPT-4V: A Comprehensive Study on\n  Prompt Engineering Strategies",
            "updated": "2023-12-07T15:05:59Z",
            "published": "2023-12-07T15:05:59Z",
            "summary": "OpenAI's latest large vision-language model (LVLM), GPT-4V(ision), has piqued\nconsiderable interest for its potential in medical applications. Despite its\npromise, recent studies and internal reviews highlight its underperformance in\nspecialized medical tasks. This paper explores the boundary of GPT-4V's\ncapabilities in medicine, particularly in processing complex imaging data from\nendoscopies, CT scans, and MRIs etc. Leveraging open-source datasets, we\nassessed its foundational competencies, identifying substantial areas for\nenhancement. Our research emphasizes prompt engineering, an often-underutilized\nstrategy for improving AI responsiveness. Through iterative testing, we refined\nthe model's prompts, significantly improving its interpretative accuracy and\nrelevance in medical imaging. From our comprehensive evaluations, we distilled\n10 effective prompt engineering techniques, each fortifying GPT-4V's medical\nacumen. These methodical enhancements facilitate more reliable, precise, and\nclinically valuable insights from GPT-4V, advancing its operability in critical\nhealthcare environments. Our findings are pivotal for those employing AI in\nmedicine, providing clear, actionable guidance on harnessing GPT-4V's full\ndiagnostic potential.",
            "author": [
                "Pengcheng Chen",
                "Ziyan Huang",
                "Zhongying Deng",
                "Tianbin Li",
                "Yanzhou Su",
                "Haoyu Wang",
                "Jin Ye",
                "Yu Qiao",
                "Junjun He"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04344v1",
                "http://arxiv.org/pdf/2312.04344v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04343v1",
            "title": "Causality and Explainability for Trustworthy Integrated Pest Management",
            "updated": "2023-12-07T15:05:26Z",
            "published": "2023-12-07T15:05:26Z",
            "summary": "Pesticides serve as a common tool in agricultural pest control but\nsignificantly contribute to the climate crisis. To combat this, Integrated Pest\nManagement (IPM) stands as a climate-smart alternative. Despite its potential,\nIPM faces low adoption rates due to farmers' skepticism about its\neffectiveness. To address this challenge, we introduce an advanced data\nanalysis framework tailored to enhance IPM adoption. Our framework provides i)\nrobust pest population predictions across diverse environments with invariant\nand causal learning, ii) interpretable pest presence predictions using\ntransparent models, iii) actionable advice through counterfactual explanations\nfor in-season IPM interventions, iv) field-specific treatment effect\nestimations, and v) assessments of the effectiveness of our advice using causal\ninference. By incorporating these features, our framework aims to alleviate\nskepticism and encourage wider adoption of IPM practices among farmers.",
            "author": [
                "Ilias Tsoumas",
                "Vasileios Sitokonstantinou",
                "Georgios Giannarakis",
                "Evagelia Lampiri",
                "Christos Athanassiou",
                "Gustau Camps-Valls",
                "Charalampos Kontoes",
                "Ioannis Athanasiadis"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04343v1",
                "http://arxiv.org/pdf/2312.04343v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04339v1",
            "title": "Merging by Matching Models in Task Subspaces",
            "updated": "2023-12-07T14:59:15Z",
            "published": "2023-12-07T14:59:15Z",
            "summary": "Model merging aims to cheaply combine individual task-specific models into a\nsingle multitask model. In this work, we view past merging methods as\nleveraging different notions of a ''task subspace'' in which models are matched\nbefore being merged. We connect the task subspace of a given model to its loss\nlandscape and formalize how this approach to model merging can be seen as\nsolving a linear system of equations. While past work has generally been\nlimited to linear systems that have a closed-form solution, we consider using\nthe conjugate gradient method to find a solution. We show that using the\nconjugate gradient method can outperform closed-form solutions, enables merging\nvia linear systems that are otherwise intractable to solve, and flexibly allows\nchoosing from a wide variety of initializations and estimates for the ''task\nsubspace''. We ultimately demonstrate that our merging framework called\n''Matching Models in their Task Subspace'' (MaTS) achieves state-of-the-art\nresults in multitask and intermediate-task model merging. We release all of the\ncode and checkpoints used in our work at https://github.com/r-three/mats.",
            "author": [
                "Derek Tam",
                "Mohit Bansal",
                "Colin Raffel"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04339v1",
                "http://arxiv.org/pdf/2312.04339v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04334v1",
            "title": "Towards a Perceptual Evaluation Framework for Lighting Estimation",
            "updated": "2023-12-07T14:51:12Z",
            "published": "2023-12-07T14:51:12Z",
            "summary": "Progress in lighting estimation is tracked by computing existing image\nquality assessment (IQA) metrics on images from standard datasets. While this\nmay appear to be a reasonable approach, we demonstrate that doing so does not\ncorrelate to human preference when the estimated lighting is used to relight a\nvirtual scene into a real photograph. To study this, we design a controlled\npsychophysical experiment where human observers must choose their preference\namongst rendered scenes lit using a set of lighting estimation algorithms\nselected from the recent literature, and use it to analyse how these algorithms\nperform according to human perception. Then, we demonstrate that none of the\nmost popular IQA metrics from the literature, taken individually, correctly\nrepresent human perception. Finally, we show that by learning a combination of\nexisting IQA metrics, we can more accurately represent human preference. This\nprovides a new perceptual framework to help evaluate future lighting estimation\nalgorithms.",
            "author": [
                "Justine Giroux",
                "Mohammad Reza Karimi Dastjerdi",
                "Yannick Hold-Geoffroy",
                "Javier Vazquez-Corral",
                "Jean-Fran\u00e7ois Lalonde"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04334v1",
                "http://arxiv.org/pdf/2312.04334v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04330v1",
            "title": "Surrogate Modelling for Sea Ice Concentration using Lightweight Neural\n  Ensemble",
            "updated": "2023-12-07T14:48:30Z",
            "published": "2023-12-07T14:48:30Z",
            "summary": "The modeling and forecasting of sea ice conditions in the Arctic region are\nimportant tasks for ship routing, offshore oil production, and environmental\nmonitoring. We propose the adaptive surrogate modeling approach named LANE-SI\n(Lightweight Automated Neural Ensembling for Sea Ice) that uses ensemble of\nrelatively simple deep learning models with different loss functions for\nforecasting of spatial distribution for sea ice concentration in the specified\nwater area. Experimental studies confirm the quality of a long-term forecast\nbased on a deep learning model fitted to the specific water area is comparable\nto resource-intensive physical modeling, and for some periods of the year, it\nis superior. We achieved a 20% improvement against the state-of-the-art\nphysics-based forecast system SEAS5 for the Kara Sea.",
            "author": [
                "Julia Borisova",
                "Nikolay O. Nikitin"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04330v1",
                "http://arxiv.org/pdf/2312.04330v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "physics.ao-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04327v1",
            "title": "Learning to sample in Cartesian MRI",
            "updated": "2023-12-07T14:38:07Z",
            "published": "2023-12-07T14:38:07Z",
            "summary": "Despite its exceptional soft tissue contrast, Magnetic Resonance Imaging\n(MRI) faces the challenge of long scanning times compared to other modalities\nlike X-ray radiography. Shortening scanning times is crucial in clinical\nsettings, as it increases patient comfort, decreases examination costs and\nimproves throughput. Recent advances in compressed sensing (CS) and deep\nlearning allow accelerated MRI acquisition by reconstructing high-quality\nimages from undersampled data. While reconstruction algorithms have received\nmost of the focus, designing acquisition trajectories to optimize\nreconstruction quality remains an open question. This thesis explores two\napproaches to address this gap in the context of Cartesian MRI. First, we\npropose two algorithms, lazy LBCS and stochastic LBCS, that significantly\nimprove upon G\\\"ozc\\\"u et al.'s greedy learning-based CS (LBCS) approach. These\nalgorithms scale to large, clinically relevant scenarios like multi-coil 3D MR\nand dynamic MRI, previously inaccessible to LBCS. Additionally, we demonstrate\nthat generative adversarial networks (GANs) can serve as a natural criterion\nfor adaptive sampling by leveraging variance in the measurement domain to guide\nacquisition. Second, we delve into the underlying structures or assumptions\nthat enable mask design algorithms to perform well in practice. Our experiments\nreveal that state-of-the-art deep reinforcement learning (RL) approaches, while\ncapable of adaptation and long-horizon planning, offer only marginal\nimprovements over stochastic LBCS, which is neither adaptive nor does long-term\nplanning. Altogether, our findings suggest that stochastic LBCS and similar\nmethods represent promising alternatives to deep RL. They shine in particular\nby their scalability and computational efficiency and could be key in the\ndeployment of optimized acquisition trajectories in Cartesian MRI.",
            "author": [
                "Thomas Sanchez"
            ],
            "link": [
                "http://dx.doi.org/10.5075/epfl-thesis-9981",
                "http://arxiv.org/abs/2312.04327v1",
                "http://arxiv.org/pdf/2312.04327v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04326v1",
            "title": "iDesigner: A High-Resolution and Complex-Prompt Following Text-to-Image\n  Diffusion Model for Interior Design",
            "updated": "2023-12-07T14:37:01Z",
            "published": "2023-12-07T14:37:01Z",
            "summary": "With the open-sourcing of text-to-image models (T2I) such as stable diffusion\n(SD) and stable diffusion XL (SD-XL), there is an influx of models fine-tuned\nin specific domains based on the open-source SD model, such as in anime,\ncharacter portraits, etc. However, there are few specialized models in certain\ndomains, such as interior design, which is attributed to the complex textual\ndescriptions and detailed visual elements inherent in design, alongside the\nnecessity for adaptable resolution. Therefore, text-to-image models for\ninterior design are required to have outstanding prompt-following capabilities,\nas well as iterative collaboration with design professionals to achieve the\ndesired outcome. In this paper, we collect and optimize text-image data in the\ndesign field and continue training in both English and Chinese on the basis of\nthe open-source CLIP model. We also proposed a fine-tuning strategy with\ncurriculum learning and reinforcement learning from CLIP feedback to enhance\nthe prompt-following capabilities of our approach so as to improve the quality\nof image generation. The experimental results on the collected dataset\ndemonstrate the effectiveness of the proposed approach, which achieves\nimpressive results and outperforms strong baselines.",
            "author": [
                "Ruyi Gan",
                "Xiaojun Wu",
                "Junyu Lu",
                "Yuanhe Tian",
                "Dixiang Zhang",
                "Ziwei Wu",
                "Renliang Sun",
                "Chang Liu",
                "Jiaxing Zhang",
                "Pingjian Zhang",
                "Yan Song"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04326v1",
                "http://arxiv.org/pdf/2312.04326v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04323v1",
            "title": "Equivariant Scalar Fields for Molecular Docking with Fast Fourier\n  Transforms",
            "updated": "2023-12-07T14:32:32Z",
            "published": "2023-12-07T14:32:32Z",
            "summary": "Molecular docking is critical to structure-based virtual screening, yet the\nthroughput of such workflows is limited by the expensive optimization of\nscoring functions involved in most docking algorithms. We explore how machine\nlearning can accelerate this process by learning a scoring function with a\nfunctional form that allows for more rapid optimization. Specifically, we\ndefine the scoring function to be the cross-correlation of multi-channel ligand\nand protein scalar fields parameterized by equivariant graph neural networks,\nenabling rapid optimization over rigid-body degrees of freedom with fast\nFourier transforms. The runtime of our approach can be amortized at several\nlevels of abstraction, and is particularly favorable for virtual screening\nsettings with a common binding pocket. We benchmark our scoring functions on\ntwo simplified docking-related tasks: decoy pose scoring and rigid conformer\ndocking. Our method attains similar but faster performance on crystal\nstructures compared to the widely-used Vina and Gnina scoring functions, and is\nmore robust on computationally predicted structures. Code is available at\nhttps://github.com/bjing2016/scalar-fields.",
            "author": [
                "Bowen Jing",
                "Tommi Jaakkola",
                "Bonnie Berger"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04323v1",
                "http://arxiv.org/pdf/2312.04323v1"
            ],
            "primary_category": "q-bio.BM",
            "category": [
                "q-bio.BM",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04318v1",
            "title": "MIMo: A Multi-Modal Infant Model for Studying Cognitive Development",
            "updated": "2023-12-07T14:21:31Z",
            "published": "2023-12-07T14:21:31Z",
            "summary": "Human intelligence and human consciousness emerge gradually during the\nprocess of cognitive development. Understanding this development is an\nessential aspect of understanding the human mind and may facilitate the\nconstruction of artificial minds with similar properties. Importantly, human\ncognitive development relies on embodied interactions with the physical and\nsocial environment, which is perceived via complementary sensory modalities.\nThese interactions allow the developing mind to probe the causal structure of\nthe world. This is in stark contrast to common machine learning approaches,\ne.g., for large language models, which are merely passively ``digesting'' large\namounts of training data, but are not in control of their sensory inputs.\nHowever, computational modeling of the kind of self-determined embodied\ninteractions that lead to human intelligence and consciousness is a formidable\nchallenge. Here we present MIMo, an open-source multi-modal infant model for\nstudying early cognitive development through computer simulations. MIMo's body\nis modeled after an 18-month-old child with detailed five-fingered hands. MIMo\nperceives its surroundings via binocular vision, a vestibular system,\nproprioception, and touch perception through a full-body virtual skin, while\ntwo different actuation models allow control of his body. We describe the\ndesign and interfaces of MIMo and provide examples illustrating its use. All\ncode is available at https://github.com/trieschlab/MIMo .",
            "author": [
                "Dominik Mattern",
                "Pierre Schumacher",
                "Francisco M. L\u00f3pez",
                "Marcel C. Raabe",
                "Markus R. Ernst",
                "Arthur Aubret",
                "Jochen Triesch"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04318v1",
                "http://arxiv.org/pdf/2312.04318v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04316v1",
            "title": "Towards Knowledge-driven Autonomous Driving",
            "updated": "2023-12-07T14:17:17Z",
            "published": "2023-12-07T14:17:17Z",
            "summary": "This paper explores the emerging knowledge-driven autonomous driving\ntechnologies. Our investigation highlights the limitations of current\nautonomous driving systems, in particular their sensitivity to data bias,\ndifficulty in handling long-tail scenarios, and lack of interpretability.\nConversely, knowledge-driven methods with the abilities of cognition,\ngeneralization and life-long learning emerge as a promising way to overcome\nthese challenges. This paper delves into the essence of knowledge-driven\nautonomous driving and examines its core components: dataset \\& benchmark,\nenvironment, and driver agent. By leveraging large language models, world\nmodels, neural rendering, and other advanced artificial intelligence\ntechniques, these components collectively contribute to a more holistic,\nadaptive, and intelligent autonomous driving system. The paper systematically\norganizes and reviews previous research efforts in this area, and provides\ninsights and guidance for future research and practical applications of\nautonomous driving. We will continually share the latest updates on\ncutting-edge developments in knowledge-driven autonomous driving along with the\nrelevant valuable open-source resources at:\n\\url{https://github.com/PJLab-ADG/awesome-knowledge-driven-AD}.",
            "author": [
                "Xin Li",
                "Yeqi Bai",
                "Pinlong Cai",
                "Licheng Wen",
                "Daocheng Fu",
                "Bo Zhang",
                "Xuemeng Yang",
                "Xinyu Cai",
                "Tao Ma",
                "Jianfei Guo",
                "Xing Gao",
                "Min Dou",
                "Botian Shi",
                "Yong Liu",
                "Liang He",
                "Yu Qiao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04316v1",
                "http://arxiv.org/pdf/2312.04316v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04314v1",
            "title": "GPT4SGG: Synthesizing Scene Graphs from Holistic and Region-specific\n  Narratives",
            "updated": "2023-12-07T14:11:00Z",
            "published": "2023-12-07T14:11:00Z",
            "summary": "Learning scene graphs from natural language descriptions has proven to be a\ncheap and promising scheme for Scene Graph Generation (SGG). However, such\nunstructured caption data and its processing are troubling the learning an\nacurrate and complete scene graph. This dilema can be summarized as three\npoints. First, traditional language parsers often fail to extract meaningful\nrelationship triplets from caption data. Second, grounding unlocalized objects\nin parsed triplets will meet ambiguity in visual-language alignment. Last,\ncaption data typically are sparse and exhibit bias to partial observations of\nimage content. These three issues make it hard for the model to generate\ncomprehensive and accurate scene graphs. To fill this gap, we propose a simple\nyet effective framework, GPT4SGG, to synthesize scene graphs from holistic and\nregion-specific narratives. The framework discards traditional language parser,\nand localize objects before obtaining relationship triplets. To obtain\nrelationship triplets, holistic and dense region-specific narratives are\ngenerated from the image. With such textual representation of image data and a\ntask-specific prompt, an LLM, particularly GPT-4, directly synthesizes a scene\ngraph as \"pseudo labels\". Experimental results showcase GPT4SGG significantly\nimproves the performance of SGG models trained on image-caption data. We\nbelieve this pioneering work can motivate further research into mining the\nvisual reasoning capabilities of LLMs.",
            "author": [
                "Zuyao Chen",
                "Jinlin Wu",
                "Zhen Lei",
                "Zhaoxiang Zhang",
                "Changwen Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04314v1",
                "http://arxiv.org/pdf/2312.04314v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04312v1",
            "title": "Stochastic-Constrained Stochastic Optimization with Markovian Data",
            "updated": "2023-12-07T14:09:27Z",
            "published": "2023-12-07T14:09:27Z",
            "summary": "This paper considers stochastic-constrained stochastic optimization where the\nstochastic constraint is to satisfy that the expectation of a random function\nis below a certain threshold. In particular, we study the setting where data\nsamples are drawn from a Markov chain and thus are not independent and\nidentically distributed. We generalize the drift-plus-penalty framework, a\nprimal-dual stochastic gradient method developed for the i.i.d. case, to the\nMarkov chain sampling setting. We propose two variants of drift-plus-penalty;\none is for the case when the mixing time of the underlying Markov chain is\nknown while the other is for the case of unknown mixing time. In fact, our\nalgorithms apply to a more general setting of constrained online convex\noptimization where the sequence of constraint functions follows a Markov chain.\nBoth algorithms are adaptive in that the first works without knowledge of the\ntime horizon while the second uses AdaGrad-style algorithm parameters, which is\nof independent interest. We demonstrate the effectiveness of our proposed\nmethods through numerical experiments on classification with fairness\nconstraints.",
            "author": [
                "Yeongjong Kim",
                "Dabeen Lee"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04312v1",
                "http://arxiv.org/pdf/2312.04312v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04311v1",
            "title": "Finding Interpretable Class-Specific Patterns through Efficient Neural\n  Search",
            "updated": "2023-12-07T14:09:18Z",
            "published": "2023-12-07T14:09:18Z",
            "summary": "Discovering patterns in data that best describe the differences between\nclasses allows to hypothesize and reason about class-specific mechanisms. In\nmolecular biology, for example, this bears promise of advancing the\nunderstanding of cellular processes differing between tissues or diseases,\nwhich could lead to novel treatments. To be useful in practice, methods that\ntackle the problem of finding such differential patterns have to be readily\ninterpretable by domain experts, and scalable to the extremely high-dimensional\ndata.\n  In this work, we propose a novel, inherently interpretable binary neural\nnetwork architecture DIFFNAPS that extracts differential patterns from data.\nDiffNaps is scalable to hundreds of thousands of features and robust to noise,\nthus overcoming the limitations of current state-of-the-art methods in\nlarge-scale applications such as in biology. We show on synthetic and real\nworld data, including three biological applications, that, unlike its\ncompetitors, DiffNaps consistently yields accurate, succinct, and interpretable\nclass descriptions",
            "author": [
                "Nils Philipp Walter",
                "Jonas Fischer",
                "Jilles Vreeken"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04311v1",
                "http://arxiv.org/pdf/2312.04311v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "q-bio.QM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04308v1",
            "title": "Multi Actor-Critic DDPG for Robot Action Space Decomposition: A\n  Framework to Control Large 3D Deformation of Soft Linear Objects",
            "updated": "2023-12-07T14:07:17Z",
            "published": "2023-12-07T14:07:17Z",
            "summary": "Robotic manipulation of deformable linear objects (DLOs) has great potential\nfor applications in diverse fields such as agriculture or industry. However, a\nmajor challenge lies in acquiring accurate deformation models that describe the\nrelationship between robot motion and DLO deformations. Such models are\ndifficult to calculate analytically and vary among DLOs. Consequently,\nmanipulating DLOs poses significant challenges, particularly in achieving large\ndeformations that require highly accurate global models. To address these\nchallenges, this paper presents MultiAC6: a new multi Actor-Critic framework\nfor robot action space decomposition to control large 3D deformations of DLOs.\nIn our approach, two deep reinforcement learning (DRL) agents orient and\nposition a robot gripper to deform a DLO into the desired shape. Unlike\nprevious DRL-based studies, MultiAC6 is able to solve the sim-to-real gap,\nachieving large 3D deformations up to 40 cm in real-world settings.\nExperimental results also show that MultiAC6 has a 66\\% higher success rate\nthan a single-agent approach. Further experimental studies demonstrate that\nMultiAC6 generalizes well, without retraining, to DLOs with different lengths\nor materials.",
            "author": [
                "M\u00e9lodie Daniel",
                "Aly Magassouba",
                "Miguel Aranda",
                "Laurent Lequi\u00e8vre",
                "Juan Antonio Corrales Ramon",
                "Roberto Iglesias Rodriguez",
                "Youcef Mezouar"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04308v1",
                "http://arxiv.org/pdf/2312.04308v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04307v1",
            "title": "A Structural-Clustering Based Active Learning for Graph Neural Networks",
            "updated": "2023-12-07T14:04:38Z",
            "published": "2023-12-07T14:04:38Z",
            "summary": "In active learning for graph-structured data, Graph Neural Networks (GNNs)\nhave shown effectiveness. However, a common challenge in these applications is\nthe underutilization of crucial structural information. To address this\nproblem, we propose the Structural-Clustering PageRank method for improved\nActive learning (SPA) specifically designed for graph-structured data. SPA\nintegrates community detection using the SCAN algorithm with the PageRank\nscoring method for efficient and informative sample selection. SPA prioritizes\nnodes that are not only informative but also central in structure. Through\nextensive experiments, SPA demonstrates higher accuracy and macro-F1 score over\nexisting methods across different annotation budgets and achieves significant\nreductions in query time. In addition, the proposed method only adds two\nhyperparameters, $\\epsilon$ and $\\mu$ in the algorithm to finely tune the\nbalance between structural learning and node selection. This simplicity is a\nkey advantage in active learning scenarios, where extensive hyperparameter\ntuning is often impractical.",
            "author": [
                "Ricky Maulana Fajri",
                "Yulong Pei",
                "Lu Yin",
                "Mykola Pechenizkiy"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04307v1",
                "http://arxiv.org/pdf/2312.04307v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04306v1",
            "title": "nerblackbox: A High-level Library for Named Entity Recognition in Python",
            "updated": "2023-12-07T14:04:15Z",
            "published": "2023-12-07T14:04:15Z",
            "summary": "We present nerblackbox, a python library to facilitate the use of\nstate-of-the-art transformer-based models for named entity recognition. It\nprovides simple-to-use yet powerful methods to access data and models from a\nwide range of sources, for fully automated model training and evaluation as\nwell as versatile model inference. While many technical challenges are solved\nand hidden from the user by default, nerblackbox also offers fine-grained\ncontrol and a rich set of customizable features. It is thus targeted both at\napplication-oriented developers as well as machine learning experts and\nresearchers.",
            "author": [
                "Felix Stollenwerk"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04306v1",
                "http://arxiv.org/pdf/2312.04306v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04296v1",
            "title": "Cross-codex Learning for Reliable Scribe Identification in Medieval\n  Manuscripts",
            "updated": "2023-12-07T13:40:20Z",
            "published": "2023-12-07T13:40:20Z",
            "summary": "Historic scribe identification is a substantial task for obtaining\ninformation about the past. Uniform script styles, such as the Carolingian\nminuscule, make it a difficult task for classification to focus on meaningful\nfeatures. Therefore, we demonstrate in this paper the importance of cross-codex\ntraining data for CNN based text-independent off-line scribe identification, to\novercome codex dependent overfitting. We report three main findings: First, we\nfound that preprocessing with masked grayscale images instead of RGB images\nclearly increased the F1-score of the classification results. Second, we\ntrained different neural networks on our complex data, validating time and\naccuracy differences in order to define the most reliable network architecture.\nWith AlexNet, the network with the best trade-off between F1-score and time, we\nachieved for individual classes F1-scores of up to 0,96 on line level and up to\n1.0 on page level in classification. Third, we could replicate the finding that\nthe CNN output can be further improved by implementing a reject option, giving\nmore stable results. We present the results on our large scale open source\ndataset -- the Codex Claustroneoburgensis database (CCl-DB) -- containing a\nsignificant number of writings from different scribes in several codices. We\ndemonstrate for the first time on a dataset with such a variety of codices that\npaleographic decisions can be reproduced automatically and precisely with CNNs.\nThis gives manifold new and fast possibilities for paleographers to gain\ninsights into unlabeled material, but also to develop further hypotheses.",
            "author": [
                "Julius Wei\u00dfmann",
                "Markus Seidl",
                "Anya Dietrich",
                "Martin Haltrich"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04296v1",
                "http://arxiv.org/pdf/2312.04296v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "I.4.9; I.5.4"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04291v1",
            "title": "Simulating the Air Quality Impact of Prescribed Fires Using a Graph\n  Neural Network-Based PM$_{2.5}$ Emissions Forecasting System",
            "updated": "2023-12-07T13:18:36Z",
            "published": "2023-12-07T13:18:36Z",
            "summary": "The increasing size and severity of wildfires across western North America\nhave generated dangerous levels of PM$_{2.5}$ pollution in recent years. In a\nwarming climate, expanding the use of prescribed fires is widely considered to\nbe the most robust fire mitigation strategy. However, reliably forecasting the\npotential air quality impact from these prescribed fires, a critical ingredient\nin determining the fires' location and time, at hourly to daily time scales\nremains a challenging problem. This paper proposes a novel integration of\nprescribed fire simulation with a spatio-temporal graph neural network-based\nPM$_{2.5}$ forecasting model. The experiments in this work focus on determining\nthe optimal time for implementing prescribed fires in California as well as\nquantifying the potential air quality trade-offs involved in conducting more\nprescribed fires outside the fire season.",
            "author": [
                "Kyleen Liao",
                "Jatan Buch",
                "Kara Lamb",
                "Pierre Gentine"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04291v1",
                "http://arxiv.org/pdf/2312.04291v1"
            ],
            "primary_category": "physics.ao-ph",
            "category": [
                "physics.ao-ph",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04289v1",
            "title": "Fast simulation of airfoil flow field via deep neural network",
            "updated": "2023-12-07T13:16:02Z",
            "published": "2023-12-07T13:16:02Z",
            "summary": "Computational Fluid Dynamics (CFD) has become an indispensable tool in the\noptimization design, and evaluation of aircraft aerodynamics. However, solving\nthe Navier-Stokes (NS) equations is a time-consuming, memory demanding and\ncomputationally expensive task. Artificial intelligence offers a promising\navenue for flow field solving. In this work, we propose a novel deep learning\nframework for rapidly reconstructing airfoil flow fields. Channel attention and\nspatial attention modules are utilized in the downsampling stage of the UNet to\nenhance the feature learning capabilities of the deep learning model.\nAdditionally, integrating the predicted flow field values generated by the deep\nlearning model into the NS equation solver validates the credibility of the\nflow field prediction results. The NACA series airfoils were used to validate\nthe prediction accuracy and generalization of the deep learning model. The\nexperimental results represent the deep learning model achieving flow field\nprediction speeds three orders of magnitude faster than CFD solver.\nFurthermore, the CFD solver integrated with deep learning model demonstrates a\nthreefold acceleration compared to CFD solver. By extensively mining historical\nflow field data, an efficient solution is derived for the rapid simulation of\naircraft flow fields.",
            "author": [
                "Kuijun Zuo",
                "Zhengyin Ye",
                "Shuhui Bu",
                "Xianxu Yuan",
                "Weiwei Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04289v1",
                "http://arxiv.org/pdf/2312.04289v1"
            ],
            "primary_category": "physics.flu-dyn",
            "category": [
                "physics.flu-dyn"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04286v1",
            "title": "Effects of multiple edge cracks, shear force, elastic foundation, and\n  boundary conditions on bucking of small-scale pillars",
            "updated": "2023-12-07T13:08:57Z",
            "published": "2023-12-07T13:08:57Z",
            "summary": "The buckling instability of micro- and nanopillars can be an issue when\ndesigning intelligent miniaturized devices and characterizing composite\nmaterials reinforced with beam-like particles on the small-scale. Analytical\nmodeling of the buckling of miniaturized pillars is especially important due to\nthe difficulties in conducting experiments. Here, a well-posed stress-driven\nnonlocal model is developed, which allows the calculation of the critical loads\nand buckling configurations of the miniaturized pillars on an elastic\nfoundation and with arbitrary numbers of edge cracks. The discontinuities in\nbending slopes and deflection at the damaged cross-sections due to the edge\ncracks are captured through the incorporation of both rotational and\ntranslational springs. A comprehensive analysis is conducted to investigate the\ninstability of pillars containing a range of one to four cracks. This analysis\nreveals interesting effects regarding the influence of crack location,\nnonlocality, and elastic foundation on the initial and subsequent critical\nloads and associated buckling configurations. The main findings are: (i) the\nshielding and amplification effects related to a system of cracks become more\nsignificant as the dimensions of pillars reduce, (ii) the influence of the\nshear force at the damaged cross-section related to the translational spring\nmust not be neglected when dealing with higher modes of buckling and long\ncracks, (iii) an elastic foundation decreases the effects of the cracks and\nsize dependency on the buckling loads, and (iv) the effects of the edge cracks\non the critical loads and buckling configurations of the miniaturized pillars\nare highly dependent on the boundary conditions.",
            "author": [
                "Hossein Darban",
                "Raimondo Luciano",
                "Micha\u0142 Basista"
            ],
            "link": [
                "http://dx.doi.org/10.1177/10567895231215558",
                "http://arxiv.org/abs/2312.04286v1",
                "http://arxiv.org/pdf/2312.04286v1"
            ],
            "primary_category": "cond-mat.soft",
            "category": [
                "cond-mat.soft"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04281v1",
            "title": "Factor-Assisted Federated Learning for Personalized Optimization with\n  Heterogeneous Data",
            "updated": "2023-12-07T13:05:47Z",
            "published": "2023-12-07T13:05:47Z",
            "summary": "Federated learning is an emerging distributed machine learning framework\naiming at protecting data privacy. Data heterogeneity is one of the core\nchallenges in federated learning, which could severely degrade the convergence\nrate and prediction performance of deep neural networks. To address this issue,\nwe develop a novel personalized federated learning framework for heterogeneous\ndata, which we refer to as FedSplit. This modeling framework is motivated by\nthe finding that, data in different clients contain both common knowledge and\npersonalized knowledge. Then the hidden elements in each neural layer can be\nsplit into the shared and personalized groups. With this decomposition, a novel\nobjective function is established and optimized. We demonstrate FedSplit\nenjoyers a faster convergence speed than the standard federated learning method\nboth theoretically and empirically. The generalization bound of the FedSplit\nmethod is also studied. To practically implement the proposed method on real\ndatasets, factor analysis is introduced to facilitate the decoupling of hidden\nelements. This leads to a practically implemented model for FedSplit and we\nfurther refer to as FedFac. We demonstrated by simulation studies that, using\nfactor analysis can well recover the underlying shared/personalized\ndecomposition. The superior prediction performance of FedFac is further\nverified empirically by comparison with various state-of-the-art federated\nlearning methods on several real datasets.",
            "author": [
                "Feifei Wang",
                "Huiyun Tang",
                "Yang Li"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04281v1",
                "http://arxiv.org/pdf/2312.04281v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04275v1",
            "title": "Estimating Countries with Similar Maternal Mortality Rate using Cluster\n  Analysis and Pairing Countries with Identical MMR",
            "updated": "2023-12-07T12:54:16Z",
            "published": "2023-12-07T12:54:16Z",
            "summary": "In the evolving world, we require more additionally the young era to flourish\nand evolve into developed land. Most of the population all around the world are\nunaware of the complications involved in the routine they follow while they are\npregnant and how hospital facilities affect maternal health. Maternal Mortality\nis the death of a pregnant woman due to intricacies correlated to pregnancy,\nunderlying circumstances exacerbated by the pregnancy or management of these\nsituations. It is crucial to consider the Maternal Mortality Rate (MMR) in\ndiverse locations and determine which human routines and hospital facilities\ndiminish the Maternal Mortality Rate (MMR). This research aims to examine and\ndiscover the countries which are keeping more lavish threats of MMR and\ncountries alike in MMR encountered. Data is examined and collected for various\ncountries, data consists of the earlier years' observation. From the\nperspective of Machine Learning, Unsupervised Machine Learning is implemented\nto perform Cluster Analysis. Therefore the pairs of countries with similar MMR\nas well as the extreme opposite pair concerning the MMR are found.",
            "author": [
                "S. Nandini",
                "Sanjjushri Varshini R"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04275v1",
                "http://arxiv.org/pdf/2312.04275v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04273v1",
            "title": "Invariant Random Forest: Tree-Based Model Solution for OOD\n  Generalization",
            "updated": "2023-12-07T12:53:05Z",
            "published": "2023-12-07T12:53:05Z",
            "summary": "Out-Of-Distribution (OOD) generalization is an essential topic in machine\nlearning. However, recent research is only focusing on the corresponding\nmethods for neural networks. This paper introduces a novel and effective\nsolution for OOD generalization of decision tree models, named Invariant\nDecision Tree (IDT). IDT enforces a penalty term with regard to the\nunstable/varying behavior of a split across different environments during the\ngrowth of the tree. Its ensemble version, the Invariant Random Forest (IRF), is\nconstructed. Our proposed method is motivated by a theoretical result under\nmild conditions, and validated by numerical tests with both synthetic and real\ndatasets. The superior performance compared to non-OOD tree models implies that\nconsidering OOD generalization for tree models is absolutely necessary and\nshould be given more attention.",
            "author": [
                "Yufan Liao",
                "Qi Wu",
                "Xing Yan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04273v1",
                "http://arxiv.org/pdf/2312.04273v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04248v1",
            "title": "TeMO: Towards Text-Driven 3D Stylization for Multi-Object Meshes",
            "updated": "2023-12-07T12:10:05Z",
            "published": "2023-12-07T12:10:05Z",
            "summary": "Recent progress in the text-driven 3D stylization of a single object has been\nconsiderably promoted by CLIP-based methods. However, the stylization of\nmulti-object 3D scenes is still impeded in that the image-text pairs used for\npre-training CLIP mostly consist of an object. Meanwhile, the local details of\nmultiple objects may be susceptible to omission due to the existing supervision\nmanner primarily relying on coarse-grained contrast of image-text pairs. To\novercome these challenges, we present a novel framework, dubbed TeMO, to parse\nmulti-object 3D scenes and edit their styles under the contrast supervision at\nmultiple levels. We first propose a Decoupled Graph Attention (DGA) module to\ndistinguishably reinforce the features of 3D surface points. Particularly, a\ncross-modal graph is constructed to align the object points accurately and noun\nphrases decoupled from the 3D mesh and textual description. Then, we develop a\nCross-Grained Contrast (CGC) supervision system, where a fine-grained loss\nbetween the words in the textual description and the randomly rendered images\nare constructed to complement the coarse-grained loss. Extensive experiments\nshow that our method can synthesize high-quality stylized content and\noutperform the existing methods over a wide range of multi-object 3D meshes.\nOur code and results will be made publicly available",
            "author": [
                "Xuying Zhang",
                "Bo-Wen Yin",
                "Yuming Chen",
                "Zheng Lin",
                "Yunheng Li",
                "Qibin Hou",
                "Ming-Ming Cheng"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04248v1",
                "http://arxiv.org/pdf/2312.04248v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04238v1",
            "title": "Long-lived Particles Anomaly Detection with Parametrized Quantum\n  Circuits",
            "updated": "2023-12-07T11:50:42Z",
            "published": "2023-12-07T11:50:42Z",
            "summary": "We investigate the possibility to apply quantum machine learning techniques\nfor data analysis, with particular regard to an interesting use-case in\nhigh-energy physics. We propose an anomaly detection algorithm based on a\nparametrized quantum circuit. This algorithm has been trained on a classical\ncomputer and tested with simulations as well as on real quantum hardware. Tests\non NISQ devices have been performed with IBM quantum computers. For the\nexecution on quantum hardware specific hardware driven adaptations have been\ndevised and implemented. The quantum anomaly detection algorithm is able to\ndetect simple anomalies like different characters in handwritten digits as well\nas more complex structures like anomalous patterns in the particle detectors\nproduced by the decay products of long-lived particles produced at a collider\nexperiment. For the high-energy physics application, performance is estimated\nin simulation only, as the quantum circuit is not simple enough to be executed\non the available quantum hardware. This work demonstrates that it is possible\nto perform anomaly detection with quantum algorithms, however, as amplitude\nencoding of classical data is required for the task, due to the noise level in\nthe available quantum hardware, current implementation cannot outperform\nclassic anomaly detection algorithms based on deep neural networks.",
            "author": [
                "Simone Bordoni",
                "Denis Stanev",
                "Tommaso Santantonio",
                "Stefano Giagu"
            ],
            "link": [
                "http://dx.doi.org/10.3390/particles6010016",
                "http://arxiv.org/abs/2312.04238v1",
                "http://arxiv.org/pdf/2312.04238v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "hep-ex"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04234v1",
            "title": "Graph Convolutions Enrich the Self-Attention in Transformers!",
            "updated": "2023-12-07T11:40:32Z",
            "published": "2023-12-07T11:40:32Z",
            "summary": "Transformers, renowned for their self-attention mechanism, have achieved\nstate-of-the-art performance across various tasks in natural language\nprocessing, computer vision, time-series modeling, etc. However, one of the\nchallenges with deep Transformer models is the oversmoothing problem, where\nrepresentations across layers converge to indistinguishable values, leading to\nsignificant performance degradation. We interpret the original self-attention\nas a simple graph filter and redesign it from a graph signal processing (GSP)\nperspective. We propose graph-filter-based self-attention (GFSA) to learn a\ngeneral yet effective one, whose complexity, however, is slightly larger than\nthat of the original self-attention mechanism. We demonstrate that GFSA\nimproves the performance of Transformers in various fields, including computer\nvision, natural language processing, graph pattern classification, speech\nrecognition, and code classification.",
            "author": [
                "Jeongwhan Choi",
                "Hyowon Wi",
                "Jayoung Kim",
                "Yehjin Shin",
                "Kookjin Lee",
                "Nathaniel Trask",
                "Noseong Park"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04234v1",
                "http://arxiv.org/pdf/2312.04234v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04233v1",
            "title": "Fine-tune vision foundation model for crack segmentation in civil\n  infrastructures",
            "updated": "2023-12-07T11:39:11Z",
            "published": "2023-12-07T11:39:11Z",
            "summary": "Large-scale foundation models have become the mainstream method in the field\nof deep learning, while in civil engineering, the scale of AI models is\nstrictly limited. In this work, vision foundation model is introduced for crack\nsegmentation. Two Parameter-efficient fine-tuning methods, adapter and low-rank\nadaptation, are adopted to fine-tune the foundation model in the field of\nsemantic segmentation: Segment Anything Model (SAM). The fine-tuned model\nCrackSAM is much larger than all the existing crack segmentation models, but\nshows excellent performance. To test the zero-shot performance of the proposed\nmethod, two unique datasets related to road and exterior wall cracks are\ncollected, annotated and open-sourced, in total 810 images. Comparative\nexperiments are conducted with twelve mature semantic segmentation models. On\ndatasets with artificial noise and previously unseen datasets, the performance\nof CrackSAM far exceeds that of all state-of-the-art models. CrackSAM exhibits\nremarkable superiority, particularly in challenging conditions such as dim\nlighting, shadows, road markings, construction joints, and other interference\nfactors. Such cross-scenario results demonstrate the outstanding zero-shot\ncapability of foundation models, and provide new ideas for the development of\nvision models in civil engineering.",
            "author": [
                "Kang Ge",
                "Chen Wang",
                "Yutao Guo"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04233v1",
                "http://arxiv.org/pdf/2312.04233v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04231v1",
            "title": "Adventures of Trustworthy Vision-Language Models: A Survey",
            "updated": "2023-12-07T11:31:20Z",
            "published": "2023-12-07T11:31:20Z",
            "summary": "Recently, transformers have become incredibly popular in computer vision and\nvision-language tasks. This notable rise in their usage can be primarily\nattributed to the capabilities offered by attention mechanisms and the\noutstanding ability of transformers to adapt and apply themselves to a variety\nof tasks and domains. Their versatility and state-of-the-art performance have\nestablished them as indispensable tools for a wide array of applications.\nHowever, in the constantly changing landscape of machine learning, the\nassurance of the trustworthiness of transformers holds utmost importance. This\npaper conducts a thorough examination of vision-language transformers,\nemploying three fundamental principles of responsible AI: Bias, Robustness, and\nInterpretability. The primary objective of this paper is to delve into the\nintricacies and complexities associated with the practical use of transformers,\nwith the overarching goal of advancing our comprehension of how to enhance\ntheir reliability and accountability.",
            "author": [
                "Mayank Vatsa",
                "Anubhooti Jain",
                "Richa Singh"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04231v1",
                "http://arxiv.org/pdf/2312.04231v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04226v1",
            "title": "Dynamic Data-Driven Digital Twins for Blockchain Systems",
            "updated": "2023-12-07T11:18:57Z",
            "published": "2023-12-07T11:18:57Z",
            "summary": "In recent years, we have seen an increase in the adoption of blockchain-based\nsystems in non-financial applications, looking to benefit from what the\ntechnology has to offer. Although many fields have managed to include\nblockchain in their core functionalities, the adoption of blockchain, in\ngeneral, is constrained by the so-called trilemma trade-off between\ndecentralization, scalability, and security. In our previous work, we have\nshown that using a digital twin for dynamically managing blockchain systems\nduring runtime can be effective in managing the trilemma trade-off. Our Digital\nTwin leverages DDDAS feedback loop, which is responsible for getting the data\nfrom the system to the digital twin, conducting optimisation, and updating the\nphysical system. This paper examines how leveraging DDDAS feedback loop can\nsupport the optimisation component of the trilemma benefiting from\nReinforcement Learning agents and a simulation component to augment the quality\nof the learned model while reducing the computational overhead required for\ndecision-making.",
            "author": [
                "Georgios Diamantopoulos",
                "Nikos Tziritas",
                "Rami Bahsoon",
                "Georgios Theodoropoulos"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04226v1",
                "http://arxiv.org/pdf/2312.04226v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.AI",
                "cs.DC",
                "cs.PF"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04225v1",
            "title": "TLCE: Transfer-Learning Based Classifier Ensembles for Few-Shot\n  Class-Incremental Learning",
            "updated": "2023-12-07T11:16:00Z",
            "published": "2023-12-07T11:16:00Z",
            "summary": "Few-shot class-incremental learning (FSCIL) struggles to incrementally\nrecognize novel classes from few examples without catastrophic forgetting of\nold classes or overfitting to new classes. We propose TLCE, which ensembles\nmultiple pre-trained models to improve separation of novel and old classes.\nTLCE minimizes interference between old and new classes by mapping old class\nimages to quasi-orthogonal prototypes using episodic training. It then\nensembles diverse pre-trained models to better adapt to novel classes despite\ndata imbalance. Extensive experiments on various datasets demonstrate that our\ntransfer learning ensemble approach outperforms state-of-the-art FSCIL methods.",
            "author": [
                "Shuangmei Wang",
                "Yang Cao",
                "Tieru Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04225v1",
                "http://arxiv.org/pdf/2312.04225v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04216v1",
            "title": "CODEX: A Cluster-Based Method for Explainable Reinforcement Learning",
            "updated": "2023-12-07T11:04:37Z",
            "published": "2023-12-07T11:04:37Z",
            "summary": "Despite the impressive feats demonstrated by Reinforcement Learning (RL),\nthese algorithms have seen little adoption in high-risk, real-world\napplications due to current difficulties in explaining RL agent actions and\nbuilding user trust. We present Counterfactual Demonstrations for Explanation\n(CODEX), a method that incorporates semantic clustering, which can effectively\nsummarize RL agent behavior in the state-action space. Experimentation on the\nMiniGrid and StarCraft II gaming environments reveals the semantic clusters\nretain temporal as well as entity information, which is reflected in the\nconstructed summary of agent behavior. Furthermore, clustering the\ndiscrete+continuous game-state latent representations identifies the most\ncrucial episodic events, demonstrating a relationship between the latent and\nsemantic spaces. This work contributes to the growing body of work that strives\nto unlock the power of RL for widespread use by leveraging and extending\ntechniques from Natural Language Processing.",
            "author": [
                "Timothy K. Mathes",
                "Jessica Inman",
                "Andr\u00e9s Col\u00f3n",
                "Simon Khan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04216v1",
                "http://arxiv.org/pdf/2312.04216v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04215v1",
            "title": "Guided Reconstruction with Conditioned Diffusion Models for Unsupervised\n  Anomaly Detection in Brain MRIs",
            "updated": "2023-12-07T11:03:42Z",
            "published": "2023-12-07T11:03:42Z",
            "summary": "Unsupervised anomaly detection in Brain MRIs aims to identify abnormalities\nas outliers from a healthy training distribution. Reconstruction-based\napproaches that use generative models to learn to reconstruct healthy brain\nanatomy are commonly used for this task. Diffusion models are an emerging class\nof deep generative models that show great potential regarding reconstruction\nfidelity. However, they face challenges in preserving intensity characteristics\nin the reconstructed images, limiting their performance in anomaly detection.\nTo address this challenge, we propose to condition the denoising mechanism of\ndiffusion models with additional information about the image to reconstruct\ncoming from a latent representation of the noise-free input image. This\nconditioning enables high-fidelity reconstruction of healthy brain structures\nwhile aligning local intensity characteristics of input-reconstruction pairs.\nWe evaluate our method's reconstruction quality, domain adaptation features and\nfinally segmentation performance on publicly available data sets with various\npathologies. Using our proposed conditioning mechanism we can reduce the\nfalse-positive predictions and enable a more precise delineation of anomalies\nwhich significantly enhances the anomaly detection performance compared to\nestablished state-of-the-art approaches to unsupervised anomaly detection in\nbrain MRI. Furthermore, our approach shows promise in domain adaptation across\ndifferent MRI acquisitions and simulated contrasts, a crucial property of\ngeneral anomaly detection methods.",
            "author": [
                "Finn Behrendt",
                "Debayan Bhattacharya",
                "Robin Mieling",
                "Lennart Maack",
                "Julia Kr\u00fcger",
                "Roland Opfer",
                "Alexander Schlaefer"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04215v1",
                "http://arxiv.org/pdf/2312.04215v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04214v1",
            "title": "Accurate Distances Measures and Machine Learning of the Texture-Property\n  Relation for Crystallographic Textures Represented by One-Point Statistics",
            "updated": "2023-12-07T11:02:41Z",
            "published": "2023-12-07T11:02:41Z",
            "summary": "The crystallographic texture of metallic materials is a key microstructural\nfeature that is responsible for the anisotropic behavior, e.g., important in\nforming operations. In materials science, crystallographic texture is commonly\ndescribed by the orientation distribution function, which is defined as the\nprobability density function of the orientations of the monocrystal grains\nconforming a polycrystalline material. For representing the orientation\ndistribution function, there are several approaches such as using generalized\nspherical harmonics, orientation histograms, and pole figure images . Measuring\ndistances between crystallographic textures is essential for any task that\nrequires assessing texture similarities, e.g. to guide forming processes.\nTherefore, we introduce novel distance measures based on (i) the Earth Movers\nDistance that takes into account local distance information encoded in\nhistogram-based texture representations and (ii) a distance measure based on\npole figure images. For this purpose, we evaluate and compare existing distance\nmeasures for selected use-cases. The present study gives insights into\nadvantages and drawbacks of using certain texture representations and distance\nmeasures with emphasis on applications in materials design and optimal process\ncontrol.",
            "author": [
                "Tarek Iraki",
                "Lukas Morand",
                "Norbert Link",
                "Stefan Sandfeld",
                "Dirk Helm"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04214v1",
                "http://arxiv.org/pdf/2312.04214v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04209v1",
            "title": "Constrained Hierarchical Clustering via Graph Coarsening and Optimal\n  Cuts",
            "updated": "2023-12-07T10:52:06Z",
            "published": "2023-12-07T10:52:06Z",
            "summary": "Motivated by extracting and summarizing relevant information in short\nsentence settings, such as satisfaction questionnaires, hotel reviews, and\nX/Twitter, we study the problem of clustering words in a hierarchical fashion.\nIn particular, we focus on the problem of clustering with horizontal and\nvertical structural constraints. Horizontal constraints are typically\ncannot-link and must-link among words, while vertical constraints are\nprecedence constraints among cluster levels. We overcome state-of-the-art\nbottlenecks by formulating the problem in two steps: first, as a\nsoft-constrained regularized least-squares which guides the result of a\nsequential graph coarsening algorithm towards the horizontal feasible set.\nThen, flat clusters are extracted from the resulting hierarchical tree by\ncomputing optimal cut heights based on the available constraints. We show that\nthe resulting approach compares very well with respect to existing algorithms\nand is computationally light.",
            "author": [
                "Eliabelle Mauduit",
                "Andrea Simonetto"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04209v1",
                "http://arxiv.org/pdf/2312.04209v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04204v1",
            "title": "Wavelength-multiplexed Delayed Inputs for Memory Enhancement of\n  Microring-based Reservoir Computing",
            "updated": "2023-12-07T10:40:37Z",
            "published": "2023-12-07T10:40:37Z",
            "summary": "We numerically demonstrate a silicon add-drop microring-based reservoir\ncomputing scheme that combines parallel delayed inputs and wavelength division\nmultiplexing. The scheme solves memory-demanding tasks like time-series\nprediction with good performance without requiring external optical feedback.",
            "author": [
                "Bernard J. Giron Castro",
                "Christophe Peucheret",
                "Francesco Da Ros"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04204v1",
                "http://arxiv.org/pdf/2312.04204v1"
            ],
            "primary_category": "cs.NE",
            "category": [
                "cs.NE",
                "cs.ET",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04193v1",
            "title": "Language Model Knowledge Distillation for Efficient Question Answering\n  in Spanish",
            "updated": "2023-12-07T10:21:22Z",
            "published": "2023-12-07T10:21:22Z",
            "summary": "Recent advances in the development of pre-trained Spanish language models has\nled to significant progress in many Natural Language Processing (NLP) tasks,\nsuch as question answering. However, the lack of efficient models imposes a\nbarrier for the adoption of such models in resource-constrained environments.\nTherefore, smaller distilled models for the Spanish language could be proven to\nbe highly scalable and facilitate their further adoption on a variety of tasks\nand scenarios. In this work, we take one step in this direction by developing\nSpanishTinyRoBERTa, a compressed language model based on RoBERTa for efficient\nquestion answering in Spanish. To achieve this, we employ knowledge\ndistillation from a large model onto a lighter model that allows for a wider\nimplementation, even in areas with limited computational resources, whilst\nattaining negligible performance sacrifice. Our experiments show that the dense\ndistilled model can still preserve the performance of its larger counterpart,\nwhile significantly increasing inference speedup. This work serves as a\nstarting point for further research and investigation of model compression\nefforts for Spanish language models across various NLP tasks.",
            "author": [
                "Adri\u00e1n Bazaga",
                "Pietro Li\u00f2",
                "Gos Micklem"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04193v1",
                "http://arxiv.org/pdf/2312.04193v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04189v1",
            "title": "Joint-Individual Fusion Structure with Fusion Attention Module for\n  Multi-Modal Skin Cancer Classification",
            "updated": "2023-12-07T10:16:21Z",
            "published": "2023-12-07T10:16:21Z",
            "summary": "Most convolutional neural network (CNN) based methods for skin cancer\nclassification obtain their results using only dermatological images. Although\ngood classification results have been shown, more accurate results can be\nachieved by considering the patient's metadata, which is valuable clinical\ninformation for dermatologists. Current methods only use the simple joint\nfusion structure (FS) and fusion modules (FMs) for the multi-modal\nclassification methods, there still is room to increase the accuracy by\nexploring more advanced FS and FM. Therefore, in this paper, we design a new\nfusion method that combines dermatological images (dermoscopy images or\nclinical images) and patient metadata for skin cancer classification from the\nperspectives of FS and FM. First, we propose a joint-individual fusion (JIF)\nstructure that learns the shared features of multi-modality data and preserves\nspecific features simultaneously. Second, we introduce a fusion attention (FA)\nmodule that enhances the most relevant image and metadata features based on\nboth the self and mutual attention mechanism to support the decision-making\npipeline. We compare the proposed JIF-MMFA method with other state-of-the-art\nfusion methods on three different public datasets. The results show that our\nJIF-MMFA method improves the classification results for all tested CNN\nbackbones and performs better than the other fusion methods on the three public\ndatasets, demonstrating our method's effectiveness and robustness",
            "author": [
                "Peng Tang",
                "Xintong Yan",
                "Yang Nan",
                "Xiaobin Hu",
                "Xiaobin Hu",
                "Bjoern H Menzee. Sebastian Krammer",
                "Tobias Lasser"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04189v1",
                "http://arxiv.org/pdf/2312.04189v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04180v1",
            "title": "AI and Jobs: Has the Inflection Point Arrived? Evidence from an Online\n  Labor Platform",
            "updated": "2023-12-07T10:06:34Z",
            "published": "2023-12-07T10:06:34Z",
            "summary": "Artificial intelligence (AI) refers to the ability of machines or software to\nmimic or even surpass human intelligence in a given cognitive task. While\nhumans learn by both induction and deduction, the success of current AI is\nrooted in induction, relying on its ability to detect statistical regularities\nin task input -- an ability learnt from a vast amount of training data using\nenormous computation resources. We examine the performance of such a\nstatistical AI in a human task through the lens of four factors, including task\nlearnability, statistical resource, computation resource, and learning\ntechniques, and then propose a three-phase visual framework to understand the\nevolving relation between AI and jobs. Based on this conceptual framework, we\ndevelop a simple economic model of competition to show the existence of an\ninflection point for each occupation. Before AI performance crosses the\ninflection point, human workers always benefit from an improvement in AI\nperformance, but after the inflection point, human workers become worse off\nwhenever such an improvement occurs. To offer empirical evidence, we first\nargue that AI performance has passed the inflection point for the occupation of\ntranslation but not for the occupation of web development. We then study how\nthe launch of ChatGPT, which led to significant improvement of AI performance\non many tasks, has affected workers in these two occupations on a large online\nlabor platform. Consistent with the inflection point conjecture, we find that\ntranslators are negatively affected by the shock both in terms of the number of\naccepted jobs and the earnings from those jobs, while web developers are\npositively affected by the very same shock. Given the potentially large\ndisruption of AI on employment, more studies on more occupations using data\nfrom different platforms are urgently needed.",
            "author": [
                "Dandan Qiao",
                "Huaxia Rui",
                "Qian Xiong"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04180v1",
                "http://arxiv.org/pdf/2312.04180v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CY",
                "econ.GN",
                "q-fin.EC",
                "J.4"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04174v1",
            "title": "Coherent energy and force uncertainty in deep learning force fields",
            "updated": "2023-12-07T09:49:05Z",
            "published": "2023-12-07T09:49:05Z",
            "summary": "In machine learning energy potentials for atomic systems, forces are commonly\nobtained as the negative derivative of the energy function with respect to\natomic positions. To quantify aleatoric uncertainty in the predicted energies,\na widely used modeling approach involves predicting both a mean and variance\nfor each energy value. However, this model is not differentiable under the\nusual white noise assumption, so energy uncertainty does not naturally\ntranslate to force uncertainty. In this work we propose a machine learning\npotential energy model in which energy and force aleatoric uncertainty are\nlinked through a spatially correlated noise process. We demonstrate our\napproach on an equivariant messages passing neural network potential trained on\nenergies and forces on two out-of-equilibrium molecular datasets. Furthermore,\nwe also show how to obtain epistemic uncertainties in this setting based on a\nBayesian interpretation of deep ensemble models.",
            "author": [
                "Peter Bj\u00f8rn J\u00f8rgensen",
                "Jonas Busk",
                "Ole Winther",
                "Mikkel N. Schmidt"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04174v1",
                "http://arxiv.org/pdf/2312.04174v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG",
                "physics.comp-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04171v1",
            "title": "A novel feature selection framework for incomplete data",
            "updated": "2023-12-07T09:45:14Z",
            "published": "2023-12-07T09:45:14Z",
            "summary": "Feature selection on incomplete datasets is an exceptionally challenging\ntask. Existing methods address this challenge by first employing imputation\nmethods to complete the incomplete data and then conducting feature selection\nbased on the imputed data. Since imputation and feature selection are entirely\nindependent steps, the importance of features cannot be considered during\nimputation. However, in real-world scenarios or datasets, different features\nhave varying degrees of importance. To address this, we propose a novel\nincomplete data feature selection framework that considers feature importance.\nThe framework mainly consists of two alternating iterative stages: the M-stage\nand the W-stage. In the M-stage, missing values are imputed based on a given\nfeature importance vector and multiple initial imputation results. In the\nW-stage, an improved reliefF algorithm is employed to learn the feature\nimportance vector based on the imputed data. Specifically, the feature\nimportance vector obtained in the current iteration of the W-stage serves as\ninput for the next iteration of the M-stage. Experimental results on both\nartificially generated and real incomplete datasets demonstrate that the\nproposed method outperforms other approaches significantly.",
            "author": [
                "Cong Guo"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04171v1",
                "http://arxiv.org/pdf/2312.04171v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04168v1",
            "title": "Augmentation-Free Dense Contrastive Knowledge Distillation for Efficient\n  Semantic Segmentation",
            "updated": "2023-12-07T09:37:28Z",
            "published": "2023-12-07T09:37:28Z",
            "summary": "In recent years, knowledge distillation methods based on contrastive learning\nhave achieved promising results on image classification and object detection\ntasks. However, in this line of research, we note that less attention is paid\nto semantic segmentation. Existing methods heavily rely on data augmentation\nand memory buffer, which entail high computational resource demands when\napplying them to handle semantic segmentation that requires to preserve\nhigh-resolution feature maps for making dense pixel-wise predictions. In order\nto address this problem, we present Augmentation-free Dense Contrastive\nKnowledge Distillation (Af-DCD), a new contrastive distillation learning\nparadigm to train compact and accurate deep neural networks for semantic\nsegmentation applications. Af-DCD leverages a masked feature mimicking\nstrategy, and formulates a novel contrastive learning loss via taking advantage\nof tactful feature partitions across both channel and spatial dimensions,\nallowing to effectively transfer dense and structured local knowledge learnt by\nthe teacher model to a target student model while maintaining training\nefficiency. Extensive experiments on five mainstream benchmarks with various\nteacher-student network pairs demonstrate the effectiveness of our approach.\nFor instance, the DeepLabV3-Res18|DeepLabV3-MBV2 model trained by Af-DCD\nreaches 77.03%|76.38% mIOU on Cityscapes dataset when choosing DeepLabV3-Res101\nas the teacher, setting new performance records. Besides that, Af-DCD achieves\nan absolute mIOU improvement of 3.26%|3.04%|2.75%|2.30%|1.42% compared with\nindividually trained counterpart on Cityscapes|Pascal\nVOC|Camvid|ADE20K|COCO-Stuff-164K. Code is available at\nhttps://github.com/OSVAI/Af-DCD",
            "author": [
                "Jiawei Fan",
                "Chao Li",
                "Xiaolong Liu",
                "Meina Song",
                "Anbang Yao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04168v1",
                "http://arxiv.org/pdf/2312.04168v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04167v1",
            "title": "Mixture of Dynamical Variational Autoencoders for Multi-Source\n  Trajectory Modeling and Separation",
            "updated": "2023-12-07T09:36:31Z",
            "published": "2023-12-07T09:36:31Z",
            "summary": "In this paper, we propose a latent-variable generative model called mixture\nof dynamical variational autoencoders (MixDVAE) to model the dynamics of a\nsystem composed of multiple moving sources. A DVAE model is pre-trained on a\nsingle-source dataset to capture the source dynamics. Then, multiple instances\nof the pre-trained DVAE model are integrated into a multi-source mixture model\nwith a discrete observation-to-source assignment latent variable. The posterior\ndistributions of both the discrete observation-to-source assignment variable\nand the continuous DVAE variables representing the sources content/position are\nestimated using a variational expectation-maximization algorithm, leading to\nmulti-source trajectories estimation. We illustrate the versatility of the\nproposed MixDVAE model on two tasks: a computer vision task, namely\nmulti-object tracking, and an audio processing task, namely single-channel\naudio source separation. Experimental results show that the proposed method\nworks well on these two tasks, and outperforms several baseline methods.",
            "author": [
                "Xiaoyu Lin",
                "Laurent Girin",
                "Xavier Alameda-Pineda"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04167v1",
                "http://arxiv.org/pdf/2312.04167v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04166v1",
            "title": "Improving Communication Efficiency of Federated Distillation via\n  Accumulating Local Updates",
            "updated": "2023-12-07T09:36:18Z",
            "published": "2023-12-07T09:36:18Z",
            "summary": "As an emerging federated learning paradigm, federated distillation enables\ncommunication-efficient model training by transmitting only small-scale\nknowledge during the learning process. To further improve the communication\nefficiency of federated distillation, we propose a novel technique, ALU, which\naccumulates multiple rounds of local updates before transferring the knowledge\nto the central server. ALU drastically decreases the frequency of communication\nin federated distillation, thereby significantly reducing the communication\noverhead during the training process. Empirical experiments demonstrate the\nsubstantial effect of ALU in improving the communication efficiency of\nfederated distillation.",
            "author": [
                "Zhiyuan Wu",
                "Sheng Sun",
                "Yuwei Wang",
                "Min Liu",
                "Tian Wen",
                "Wen Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04166v1",
                "http://arxiv.org/pdf/2312.04166v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04163v1",
            "title": "Multi-scale Residual Transformer for VLF Lightning Transients\n  Classification",
            "updated": "2023-12-07T09:26:58Z",
            "published": "2023-12-07T09:26:58Z",
            "summary": "The utilization of Very Low Frequency (VLF) electromagnetic signals in\nnavigation systems is widespread. However, the non-stationary behavior of\nlightning signals can affect VLF electromagnetic signal transmission.\nAccurately classifying lightning signals is important for reducing interference\nand noise in VLF, thereby improving the reliability and overall performance of\nnavigation systems. In recent years, the evolution of deep learning,\nspecifically Convolutional Neural Network (CNNs), has sparked a transformation\nin lightning classification, surpassing traditional statistical methodologies.\nExisting CNN models have limitations as they overlook the diverse attributes of\nlightning signals across different scales and neglect the significance of\ntemporal sequencing in sequential signals. This study introduces an innovative\nmulti-scale residual transform (MRTransformer) that not only has the ability to\ndiscern intricate fine-grained patterns while also weighing the significance of\ndifferent aspects within the input lightning signal sequence. This model\nperforms the attributes of the lightning signal across different scales and the\nlevel of accuracy reached 90% in the classification. In future work, this model\nhas the potential applied to a comprehensive understanding of the localization\nand waveform characteristics of lightning signals.",
            "author": [
                "Jinghao Sun",
                "Tingting Ji",
                "Guoyu Wang",
                "Rui Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04163v1",
                "http://arxiv.org/pdf/2312.04163v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.04160v1",
            "title": "Text as Image: Learning Transferable Adapter for Multi-Label\n  Classification",
            "updated": "2023-12-07T09:22:20Z",
            "published": "2023-12-07T09:22:20Z",
            "summary": "Pre-trained vision-language models have notably accelerated progress of\nopen-world concept recognition. Their impressive zero-shot ability has recently\nbeen transferred to multi-label image classification via prompt tuning,\nenabling to discover novel labels in an open-vocabulary manner. However, this\nparadigm suffers from non-trivial training costs, and becomes computationally\nprohibitive for a large number of candidate labels. To address this issue, we\nnote that vision-language pre-training aligns images and texts in a unified\nembedding space, making it potential for an adapter network to identify labels\nin visual modality while be trained in text modality. To enhance such\ncross-modal transfer ability, a simple yet effective method termed random\nperturbation is proposed, which enables the adapter to search for potential\nvisual embeddings by perturbing text embeddings with noise during training,\nresulting in better performance in visual modality. Furthermore, we introduce\nan effective approach to employ large language models for multi-label\ninstruction-following text generation. In this way, a fully automated pipeline\nfor visual label recognition is developed without relying on any manual data.\nExtensive experiments on public benchmarks show the superiority of our method\nin various multi-label classification tasks.",
            "author": [
                "Xuelin Zhu",
                "Jiuxin Cao",
                "Jian liu",
                "Dongqi Tang",
                "Furong Xu",
                "Weijia Liu",
                "Jiawei Ge",
                "Bo Liu",
                "Qingpei Guo",
                "Tianyi Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04160v1",
                "http://arxiv.org/pdf/2312.04160v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    }
]