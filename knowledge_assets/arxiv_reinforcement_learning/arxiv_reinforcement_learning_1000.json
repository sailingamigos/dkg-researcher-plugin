[
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00910v1",
            "title": "Effectiveness of probabilistic contact tracing in epidemic containment:\n  the role of super-spreaders and transmission paths reconstruction",
            "updated": "2023-12-01T20:19:12Z",
            "published": "2023-12-01T20:19:12Z",
            "summary": "The recent COVID-19 pandemic underscores the significance of early-stage\nnon-pharmacological intervention strategies. The widespread use of masks and\nthe systematic implementation of contact tracing strategies provide a\npotentially equally effective and socially less impactful alternative to more\nconventional approaches, such as large-scale mobility restrictions. However,\nmanual contact tracing faces strong limitations in accessing the network of\ncontacts, and the scalability of currently implemented protocols for\nsmartphone-based digital contact tracing becomes impractical during the rapid\nexpansion phases of the outbreaks, due to the surge in exposure notifications\nand associated tests. A substantial improvement in digital contact tracing can\nbe obtained through the integration of probabilistic techniques for risk\nassessment that can more effectively guide the allocation of new diagnostic\ntests. In this study, we first quantitatively analyze the diagnostic and social\ncosts associated with these containment measures based on contact tracing,\nemploying three state-of-the-art models of SARS-CoV-2 spreading. Our results\nsuggest that probabilistic techniques allow for more effective mitigation at a\nlower cost. Secondly, our findings reveal a remarkable efficacy of\nprobabilistic contact-tracing techniques in capturing backward propagations and\nsuper-spreading events, relevant features of the diffusion of many pathogens,\nincluding SARS-CoV-2.",
            "author": [
                "A. P. Muntoni",
                "F. Mazza",
                "A. Braunstein",
                "G. Catania",
                "L. Dall'Asta"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00910v1",
                "http://arxiv.org/pdf/2312.00910v1"
            ],
            "primary_category": "q-bio.PE",
            "category": [
                "q-bio.PE",
                "cond-mat.stat-mech",
                "cs.AI",
                "cs.LG",
                "physics.soc-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02186v1",
            "title": "Identifying Spurious Correlations using Counterfactual Alignment",
            "updated": "2023-12-01T20:16:02Z",
            "published": "2023-12-01T20:16:02Z",
            "summary": "Models driven by spurious correlations often yield poor generalization\nperformance. We propose the counterfactual alignment method to detect and\nexplore spurious correlations of black box classifiers. Counterfactual images\ngenerated with respect to one classifier can be input into other classifiers to\nsee if they also induce changes in the outputs of these classifiers. The\nrelationship between these responses can be quantified and used to identify\nspecific instances where a spurious correlation exists as well as compute\naggregate statistics over a dataset. Our work demonstrates the ability to\ndetect spurious correlations in face attribute classifiers. This is validated\nby observing intuitive trends in a face attribute classifier as well as\nfabricating spurious correlations and detecting their presence, both visually\nand quantitatively. Further, utilizing the CF alignment method, we demonstrate\nthat we can rectify spurious correlations identified in classifiers.",
            "author": [
                "Joseph Paul Cohen",
                "Louis Blankemeier",
                "Akshay Chaudhari"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02186v1",
                "http://arxiv.org/pdf/2312.02186v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03750v1",
            "title": "Analyzing the Influence of Fake News in the 2024 Elections: A\n  Comprehensive Dataset",
            "updated": "2023-12-01T20:14:16Z",
            "published": "2023-12-01T20:14:16Z",
            "summary": "This work introduces a dataset focused on fake news in US political speeches,\nspecifically examining racial slurs and biases. By scraping and annotating\n40,000 news articles, using advanced NLP tools and human verification, we\nprovide a nuanced understanding of misinformation in political discourse. The\ndataset, designed for machine learning and bias analysis, is a critical\nresource for researchers, policymakers, and educators. It facilitates the\ndevelopment of strategies against misinformation and enhances media literacy,\nmarking a significant contribution to the study of fake news and political\ncommunication. Our dataset, focusing on the analysis of fake news in the\ncontext of the 2024 elections, is publicly accessible for community to work on\nfake news identification. Our dataset, focusing on the analysis of fake news in\nthe context of the 2024 elections, is publicly accessible.",
            "author": [
                "Mizanur Rahman",
                "Shaina Raza"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03750v1",
                "http://arxiv.org/pdf/2312.03750v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00907v1",
            "title": "Extreme Event Prediction with Multi-agent Reinforcement Learning-based\n  Parametrization of Atmospheric and Oceanic Turbulence",
            "updated": "2023-12-01T20:12:16Z",
            "published": "2023-12-01T20:12:16Z",
            "summary": "Global climate models (GCMs) are the main tools for understanding and\npredicting climate change. However, due to limited numerical resolutions, these\nmodels suffer from major structural uncertainties; e.g., they cannot resolve\ncritical processes such as small-scale eddies in atmospheric and oceanic\nturbulence. Thus, such small-scale processes have to be represented as a\nfunction of the resolved scales via closures (parametrization). The accuracy of\nthese closures is particularly important for capturing climate extremes.\nTraditionally, such closures are based on heuristics and simplifying\nassumptions about the unresolved physics. Recently, supervised-learned\nclosures, trained offline on high-fidelity data, have been shown to outperform\nthe classical physics-based closures. However, this approach requires a\nsignificant amount of high-fidelity training data and can also lead to\ninstabilities. Reinforcement learning is emerging as a potent alternative for\ndeveloping such closures as it requires only low-order statistics and leads to\nstable closures. In Scientific Multi-Agent Reinforcement Learning (SMARL)\ncomputational elements serve a dual role of discretization points and learning\nagents. We leverage SMARL and fundamentals of turbulence physics to learn\nclosures for prototypes of atmospheric and oceanic turbulence. The policy is\ntrained using only the enstrophy spectrum, which is nearly invariant and can be\nestimated from a few high-fidelity samples (these few samples are far from\nenough for supervised/offline learning). We show that these closures lead to\nstable low-resolution simulations that, at a fraction of the cost, can\nreproduce the high-fidelity simulations' statistics, including the tails of the\nprobability density functions. The results demonstrate the high potential of\nSMARL for closure modeling for GCMs, especially in the regime of scarce data\nand indirect observations.",
            "author": [
                "Rambod Mojgani",
                "Daniel Waelchli",
                "Yifei Guan",
                "Petros Koumoutsakos",
                "Pedram Hassanzadeh"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00907v1",
                "http://arxiv.org/pdf/2312.00907v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CE",
                "physics.ao-ph",
                "physics.comp-ph",
                "physics.flu-dyn"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00892v1",
            "title": "Black-Litterman Portfolio Optimization with Noisy Intermediate-Scale\n  Quantum Computers",
            "updated": "2023-12-01T19:42:04Z",
            "published": "2023-12-01T19:42:04Z",
            "summary": "In this work, we demonstrate a practical application of noisy\nintermediate-scale quantum (NISQ) algorithms to enhance subroutines in the\nBlack-Litterman (BL) portfolio optimization model. As a proof of concept, we\nimplement a 12-qubit example for selecting 6 assets out of a 12-asset pool. Our\napproach involves predicting investor views with quantum machine learning (QML)\nand addressing the subsequent optimization problem using the variational\nquantum eigensolver (VQE). The solutions obtained from VQE exhibit a high\napproximation ratio behavior, and consistently outperform several common\nportfolio models in backtesting over a long period of time. A unique aspect of\nour VQE scheme is that after the quantum circuit is optimized, only a minimal\nnumber of samplings is required to give a high approximation ratio result since\nthe probability distribution should be concentrated on high-quality solutions.\nWe further emphasize the importance of employing only a small number of final\nsamplings in our scheme by comparing the cost with those obtained from an\nexhaustive search and random sampling. The power of quantum computing can be\nanticipated when dealing with a larger-size problem due to the linear growth of\nthe required qubit resources with the problem size. This is in contrast to\nclassical computing where the search space grows exponentially with the problem\nsize and would quickly reach the limit of classical computers.",
            "author": [
                "Chi-Chun Chen",
                "San-Lin Chung",
                "Hsi-Sheng Goan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00892v1",
                "http://arxiv.org/pdf/2312.00892v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00886v3",
            "title": "Nash Learning from Human Feedback",
            "updated": "2023-12-06T14:07:10Z",
            "published": "2023-12-01T19:26:23Z",
            "summary": "Reinforcement learning from human feedback (RLHF) has emerged as the main\nparadigm for aligning large language models (LLMs) with human preferences.\nTypically, RLHF involves the initial step of learning a reward model from human\nfeedback, often expressed as preferences between pairs of text generations\nproduced by a pre-trained LLM. Subsequently, the LLM's policy is fine-tuned by\noptimizing it to maximize the reward model through a reinforcement learning\nalgorithm. However, an inherent limitation of current reward models is their\ninability to fully represent the richness of human preferences and their\ndependency on the sampling distribution.\n  In this study, we introduce an alternative pipeline for the fine-tuning of\nLLMs using pairwise human feedback. Our approach entails the initial learning\nof a preference model, which is conditioned on two inputs given a prompt,\nfollowed by the pursuit of a policy that consistently generates responses\npreferred over those generated by any competing policy, thus defining the Nash\nequilibrium of this preference model. We term this approach Nash learning from\nhuman feedback (NLHF).\n  In the context of a tabular policy representation, we present a novel\nalgorithmic solution, Nash-MD, founded on the principles of mirror descent.\nThis algorithm produces a sequence of policies, with the last iteration\nconverging to the regularized Nash equilibrium. Additionally, we explore\nparametric representations of policies and introduce gradient descent\nalgorithms for deep-learning architectures. To demonstrate the effectiveness of\nour approach, we present experimental results involving the fine-tuning of a\nLLM for a text summarization task. We believe NLHF offers a compelling avenue\nfor preference learning and policy optimization with the potential of advancing\nthe field of aligning LLMs with human preferences.",
            "author": [
                "R\u00e9mi Munos",
                "Michal Valko",
                "Daniele Calandriello",
                "Mohammad Gheshlaghi Azar",
                "Mark Rowland",
                "Zhaohan Daniel Guo",
                "Yunhao Tang",
                "Matthieu Geist",
                "Thomas Mesnard",
                "Andrea Michi",
                "Marco Selvi",
                "Sertan Girgin",
                "Nikola Momchev",
                "Olivier Bachem",
                "Daniel J. Mankowitz",
                "Doina Precup",
                "Bilal Piot"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00886v3",
                "http://arxiv.org/pdf/2312.00886v3"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.AI",
                "cs.GT",
                "cs.LG",
                "cs.MA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00875v1",
            "title": "A perspective on protein structure prediction using quantum computers",
            "updated": "2023-12-01T19:04:02Z",
            "published": "2023-12-01T19:04:02Z",
            "summary": "Despite the recent advancements by deep learning methods such as AlphaFold2,\n\\textit{in silico} protein structure prediction remains a challenging problem\nin biomedical research. With the rapid evolution of quantum computing, it is\nnatural to ask whether quantum computers can offer some meaningful benefits for\napproaching this problem. Yet, identifying specific problem instances amenable\nto quantum advantage, and estimating quantum resources required are equally\nchallenging tasks. Here, we share our perspective on how to create a framework\nfor systematically selecting protein structure prediction problems that are\namenable for quantum advantage, and estimate quantum resources for such\nproblems on a utility-scale quantum computer. As a proof-of-concept, we\nvalidate our problem selection framework by accurately predicting the structure\nof a catalytic loop of the Zika Virus NS3 Helicase, on quantum hardware.",
            "author": [
                "Hakan Doga",
                "Bryan Raubenolt",
                "Fabio Cumbo",
                "Jayadev Joshi",
                "Frank P. DiFilippo",
                "Jun Qin",
                "Daniel Blankenberg",
                "Omar Shehab"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00875v1",
                "http://arxiv.org/pdf/2312.00875v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00870v1",
            "title": "3DiFACE: Diffusion-based Speech-driven 3D Facial Animation and Editing",
            "updated": "2023-12-01T19:01:05Z",
            "published": "2023-12-01T19:01:05Z",
            "summary": "We present 3DiFACE, a novel method for personalized speech-driven 3D facial\nanimation and editing. While existing methods deterministically predict facial\nanimations from speech, they overlook the inherent one-to-many relationship\nbetween speech and facial expressions, i.e., there are multiple reasonable\nfacial expression animations matching an audio input. It is especially\nimportant in content creation to be able to modify generated motion or to\nspecify keyframes. To enable stochasticity as well as motion editing, we\npropose a lightweight audio-conditioned diffusion model for 3D facial motion.\nThis diffusion model can be trained on a small 3D motion dataset, maintaining\nexpressive lip motion output. In addition, it can be finetuned for specific\nsubjects, requiring only a short video of the person. Through quantitative and\nqualitative evaluations, we show that our method outperforms existing\nstate-of-the-art techniques and yields speech-driven animations with greater\nfidelity and diversity.",
            "author": [
                "Balamurugan Thambiraja",
                "Sadegh Aliakbarian",
                "Darren Cosker",
                "Justus Thies"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00870v1",
                "http://arxiv.org/pdf/2312.00870v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.GR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00785v1",
            "title": "Sequential Modeling Enables Scalable Learning for Large Vision Models",
            "updated": "2023-12-01T18:59:57Z",
            "published": "2023-12-01T18:59:57Z",
            "summary": "We introduce a novel sequential modeling approach which enables learning a\nLarge Vision Model (LVM) without making use of any linguistic data. To do this,\nwe define a common format, \"visual sentences\", in which we can represent raw\nimages and videos as well as annotated data sources such as semantic\nsegmentations and depth reconstructions without needing any meta-knowledge\nbeyond the pixels. Once this wide variety of visual data (comprising 420\nbillion tokens) is represented as sequences, the model can be trained to\nminimize a cross-entropy loss for next token prediction. By training across\nvarious scales of model architecture and data diversity, we provide empirical\nevidence that our models scale effectively. Many different vision tasks can be\nsolved by designing suitable visual prompts at test time.",
            "author": [
                "Yutong Bai",
                "Xinyang Geng",
                "Karttikeya Mangalam",
                "Amir Bar",
                "Alan Yuille",
                "Trevor Darrell",
                "Jitendra Malik",
                "Alexei A Efros"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00785v1",
                "http://arxiv.org/pdf/2312.00785v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00784v1",
            "title": "Making Large Multimodal Models Understand Arbitrary Visual Prompts",
            "updated": "2023-12-01T18:59:56Z",
            "published": "2023-12-01T18:59:56Z",
            "summary": "While existing large vision-language multimodal models focus on whole image\nunderstanding, there is a prominent gap in achieving region-specific\ncomprehension. Current approaches that use textual coordinates or spatial\nencodings often fail to provide a user-friendly interface for visual prompting.\nTo address this challenge, we introduce a novel multimodal model capable of\ndecoding arbitrary visual prompts. This allows users to intuitively mark images\nand interact with the model using natural cues like a \"red bounding box\" or\n\"pointed arrow\". Our simple design directly overlays visual markers onto the\nRGB image, eliminating the need for complex region encodings, yet achieves\nstate-of-the-art performance on region-understanding tasks like Visual7W,\nPointQA, and Visual Commonsense Reasoning benchmark. Furthermore, we present\nViP-Bench, a comprehensive benchmark to assess the capability of models in\nunderstanding visual prompts across multiple dimensions, enabling future\nresearch in this domain. Code, data, and model are publicly available.",
            "author": [
                "Mu Cai",
                "Haotian Liu",
                "Siva Karthik Mustikovela",
                "Gregory P. Meyer",
                "Yuning Chai",
                "Dennis Park",
                "Yong Jae Lee"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00784v1",
                "http://arxiv.org/pdf/2312.00784v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00775v1",
            "title": "Towards Generalizable Zero-Shot Manipulation via Translating Human\n  Interaction Plans",
            "updated": "2023-12-01T18:54:12Z",
            "published": "2023-12-01T18:54:12Z",
            "summary": "We pursue the goal of developing robots that can interact zero-shot with\ngeneric unseen objects via a diverse repertoire of manipulation skills and show\nhow passive human videos can serve as a rich source of data for learning such\ngeneralist robots. Unlike typical robot learning approaches which directly\nlearn how a robot should act from interaction data, we adopt a factorized\napproach that can leverage large-scale human videos to learn how a human would\naccomplish a desired task (a human plan), followed by translating this plan to\nthe robots embodiment. Specifically, we learn a human plan predictor that,\ngiven a current image of a scene and a goal image, predicts the future hand and\nobject configurations. We combine this with a translation module that learns a\nplan-conditioned robot manipulation policy, and allows following humans plans\nfor generic manipulation tasks in a zero-shot manner with no deployment-time\ntraining. Importantly, while the plan predictor can leverage large-scale human\nvideos for learning, the translation module only requires a small amount of\nin-domain data, and can generalize to tasks not seen during training. We show\nthat our learned system can perform over 16 manipulation skills that generalize\nto 40 objects, encompassing 100 real-world tasks for table-top manipulation and\ndiverse in-the-wild manipulation. https://homangab.github.io/hopman/",
            "author": [
                "Homanga Bharadhwaj",
                "Abhinav Gupta",
                "Vikash Kumar",
                "Shubham Tulsiani"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00775v1",
                "http://arxiv.org/pdf/2312.00775v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00774v1",
            "title": "Context Retrieval via Normalized Contextual Latent Interaction for\n  Conversational Agent",
            "updated": "2023-12-01T18:53:51Z",
            "published": "2023-12-01T18:53:51Z",
            "summary": "Conversational agents leveraging AI, particularly deep learning, are emerging\nin both academic research and real-world applications. However, these\napplications still face challenges, including disrespecting knowledge and\nfacts, not personalizing to user preferences, and enormous demand for\ncomputational resources during training and inference. Recent research efforts\nhave been focused on addressing these challenges from various aspects,\nincluding supplementing various types of auxiliary information to the\nconversational agents. However, existing methods are still not able to\neffectively and efficiently exploit relevant information from these auxiliary\nsupplements to further unleash the power of the conversational agents and the\nlanguage models they use. In this paper, we present a novel method, PK-NCLI,\nthat is able to accurately and efficiently identify relevant auxiliary\ninformation to improve the quality of conversational responses by learning the\nrelevance among persona, chat history, and knowledge background through\nlow-level normalized contextual latent interaction. Our experimental results\nindicate that PK-NCLI outperforms the state-of-the-art method, PK-FoCus, by\n47.80%/30.61%/24.14% in terms of perplexity, knowledge grounding, and training\nefficiency, respectively, and maintained the same level of persona grounding\nperformance. We also provide a detailed analysis of how different factors,\nincluding language model choices and trade-offs on training weights, would\naffect the performance of PK-NCLI.",
            "author": [
                "Junfeng Liu",
                "Zhuocheng Mei",
                "Kewen Peng",
                "Ranga Raju Vatsavai"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00774v1",
                "http://arxiv.org/pdf/2312.00774v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.IR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00770v1",
            "title": "Random Forest for Dynamic Risk Prediction or Recurrent Events: A\n  Pseudo-Observation Approach",
            "updated": "2023-12-01T18:48:52Z",
            "published": "2023-12-01T18:48:52Z",
            "summary": "Recurrent events are common in clinical, healthcare, social and behavioral\nstudies. A recent analysis framework for potentially censored recurrent event\ndata is to construct a censored longitudinal data set consisting of times to\nthe first recurrent event in multiple prespecified follow-up windows of length\n$\\tau$. With the staggering number of potential predictors being generated from\ngenetic, -omic, and electronic health records sources, machine learning\napproaches such as the random forest are growing in popularity, as they can\nincorporate information from highly correlated predictors with non-standard\nrelationships. In this paper, we bridge this gap by developing a random forest\napproach for dynamically predicting probabilities of remaining event-free\nduring a subsequent $\\tau$-duration follow-up period from a reconstructed\ncensored longitudinal data set. We demonstrate the increased ability of our\nrandom forest algorithm for predicting the probability of remaining event-free\nover a $\\tau$-duration follow-up period when compared to the recurrent event\nmodeling framework of Xia et al. (2020) in settings where association between\npredictors and recurrent event outcomes is complex in nature. The proposed\nrandom forest algorithm is demonstrated using recurrent exacerbation data from\nthe Azithromycin for the Prevention of Exacerbations of Chronic Obstructive\nPulmonary Disease (Albert et al., 2011).",
            "author": [
                "Abigail Loe",
                "Susan Murray",
                "Zhenke Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00770v1",
                "http://arxiv.org/pdf/2312.00770v1"
            ],
            "primary_category": "stat.AP",
            "category": [
                "stat.AP",
                "stat.ME"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00766v1",
            "title": "Automated Material Properties Extraction For Enhanced Beauty Product\n  Discovery and Makeup Virtual Try-on",
            "updated": "2023-12-01T18:41:22Z",
            "published": "2023-12-01T18:41:22Z",
            "summary": "The multitude of makeup products available can make it challenging to find\nthe ideal match for desired attributes. An intelligent approach for product\ndiscovery is required to enhance the makeup shopping experience to make it more\nconvenient and satisfying. However, enabling accurate and efficient product\ndiscovery requires extracting detailed attributes like color and finish type.\nOur work introduces an automated pipeline that utilizes multiple customized\nmachine learning models to extract essential material attributes from makeup\nproduct images. Our pipeline is versatile and capable of handling various\nmakeup products. To showcase the efficacy of our pipeline, we conduct extensive\nexperiments on eyeshadow products (both single and multi-shade ones), a\nchallenging makeup product known for its diverse range of shapes, colors, and\nfinish types. Furthermore, we demonstrate the applicability of our approach by\nsuccessfully extending it to other makeup categories like lipstick and\nfoundation, showcasing its adaptability and effectiveness across different\nbeauty products. Additionally, we conduct ablation experiments to demonstrate\nthe superiority of our machine learning pipeline over human labeling methods in\nterms of reliability. Our proposed method showcases its effectiveness in\ncross-category product discovery, specifically in recommending makeup products\nthat perfectly match a specified outfit. Lastly, we also demonstrate the\napplication of these material attributes in enabling virtual-try-on experiences\nwhich makes makeup shopping experience significantly more engaging.",
            "author": [
                "Fatemeh Taheri Dezaki",
                "Himanshu Arora",
                "Rahul Suresh",
                "Amin Banitalebi-Dehkordi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00766v1",
                "http://arxiv.org/pdf/2312.00766v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00765v1",
            "title": "Explaining Knock-on Effects of Bias Mitigation",
            "updated": "2023-12-01T18:40:37Z",
            "published": "2023-12-01T18:40:37Z",
            "summary": "In machine learning systems, bias mitigation approaches aim to make outcomes\nfairer across privileged and unprivileged groups. Bias mitigation methods work\nin different ways and have known \"waterfall\" effects, e.g., mitigating bias at\none place may manifest bias elsewhere. In this paper, we aim to characterise\nimpacted cohorts when mitigation interventions are applied. To do so, we treat\nintervention effects as a classification task and learn an explainable\nmeta-classifier to identify cohorts that have altered outcomes. We examine a\nrange of bias mitigation strategies that work at various stages of the model\nlife cycle. We empirically demonstrate that our meta-classifier is able to\nuncover impacted cohorts. Further, we show that all tested mitigation\nstrategies negatively impact a non-trivial fraction of cases, i.e., people who\nreceive unfavourable outcomes solely on account of mitigation efforts. This is\ndespite improvement in fairness metrics. We use these results as a basis to\nargue for more careful audits of static mitigation interventions that go beyond\naggregate metrics.",
            "author": [
                "Svetoslav Nizhnichenkov",
                "Rahul Nair",
                "Elizabeth Daly",
                "Brian Mac Namee"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00765v1",
                "http://arxiv.org/pdf/2312.00765v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00763v1",
            "title": "Beyond ChatBots: ExploreLLM for Structured Thoughts and Personalized\n  Model Responses",
            "updated": "2023-12-01T18:31:28Z",
            "published": "2023-12-01T18:31:28Z",
            "summary": "Large language model (LLM) powered chatbots are primarily text-based today,\nand impose a large interactional cognitive load, especially for exploratory or\nsensemaking tasks such as planning a trip or learning about a new city. Because\nthe interaction is textual, users have little scaffolding in the way of\nstructure, informational \"scent\", or ability to specify high-level preferences\nor goals. We introduce ExploreLLM that allows users to structure thoughts, help\nexplore different options, navigate through the choices and recommendations,\nand to more easily steer models to generate more personalized responses. We\nconduct a user study and show that users find it helpful to use ExploreLLM for\nexploratory or planning tasks, because it provides a useful schema-like\nstructure to the task, and guides users in planning. The study also suggests\nthat users can more easily personalize responses with high-level preferences\nwith ExploreLLM. Together, ExploreLLM points to a future where users interact\nwith LLMs beyond the form of chatbots, and instead designed to support complex\nuser tasks with a tighter integration between natural language and graphical\nuser interfaces.",
            "author": [
                "Xiao Ma",
                "Swaroop Mishra",
                "Ariel Liu",
                "Sophie Su",
                "Jilin Chen",
                "Chinmay Kulkarni",
                "Heng-Tze Cheng",
                "Quoc Le",
                "Ed Chi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00763v1",
                "http://arxiv.org/pdf/2312.00763v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC",
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00863v1",
            "title": "EfficientSAM: Leveraged Masked Image Pretraining for Efficient Segment\n  Anything",
            "updated": "2023-12-01T18:31:00Z",
            "published": "2023-12-01T18:31:00Z",
            "summary": "Segment Anything Model (SAM) has emerged as a powerful tool for numerous\nvision applications. A key component that drives the impressive performance for\nzero-shot transfer and high versatility is a super large Transformer model\ntrained on the extensive high-quality SA-1B dataset. While beneficial, the huge\ncomputation cost of SAM model has limited its applications to wider real-world\napplications. To address this limitation, we propose EfficientSAMs,\nlight-weight SAM models that exhibits decent performance with largely reduced\ncomplexity. Our idea is based on leveraging masked image pretraining, SAMI,\nwhich learns to reconstruct features from SAM image encoder for effective\nvisual representation learning. Further, we take SAMI-pretrained light-weight\nimage encoders and mask decoder to build EfficientSAMs, and finetune the models\non SA-1B for segment anything task. We perform evaluations on multiple vision\ntasks including image classification, object detection, instance segmentation,\nand semantic object detection, and find that our proposed pretraining method,\nSAMI, consistently outperforms other masked image pretraining methods. On\nsegment anything task such as zero-shot instance segmentation, our\nEfficientSAMs with SAMI-pretrained lightweight image encoders perform favorably\nwith a significant gain (e.g., ~4 AP on COCO/LVIS) over other fast SAM models.",
            "author": [
                "Yunyang Xiong",
                "Bala Varadarajan",
                "Lemeng Wu",
                "Xiaoyu Xiang",
                "Fanyi Xiao",
                "Chenchen Zhu",
                "Xiaoliang Dai",
                "Dilin Wang",
                "Fei Sun",
                "Forrest Iandola",
                "Raghuraman Krishnamoorthi",
                "Vikas Chandra"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00863v1",
                "http://arxiv.org/pdf/2312.00863v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00761v2",
            "title": "Deep Unlearning: Fast and Efficient Training-free Approach to Controlled\n  Forgetting",
            "updated": "2023-12-04T01:57:38Z",
            "published": "2023-12-01T18:29:08Z",
            "summary": "Machine unlearning has emerged as a prominent and challenging area of\ninterest, driven in large part by the rising regulatory demands for industries\nto delete user data upon request and the heightened awareness of privacy.\nExisting approaches either retrain models from scratch or use several\nfinetuning steps for every deletion request, often constrained by computational\nresource limitations and restricted access to the original training data. In\nthis work, we introduce a novel class unlearning algorithm designed to\nstrategically eliminate an entire class or a group of classes from the learned\nmodel. To that end, our algorithm first estimates the Retain Space and the\nForget Space, representing the feature or activation spaces for samples from\nclasses to be retained and unlearned, respectively. To obtain these spaces, we\npropose a novel singular value decomposition-based technique that requires\nlayer wise collection of network activations from a few forward passes through\nthe network. We then compute the shared information between these spaces and\nremove it from the forget space to isolate class-discriminatory feature space\nfor unlearning. Finally, we project the model weights in the orthogonal\ndirection of the class-discriminatory space to obtain the unlearned model. We\ndemonstrate our algorithm's efficacy on ImageNet using a Vision Transformer\nwith only $\\sim$1.5% drop in retain accuracy compared to the original model\nwhile maintaining under 1% accuracy on the unlearned class samples. Further,\nour algorithm consistently performs well when subject to Membership Inference\nAttacks showing 7.8% improvement on average across a variety of image\nclassification datasets and network architectures, as compared to other\nbaselines while being $\\sim$6x more computationally efficient.",
            "author": [
                "Sangamesh Kodge",
                "Gobinda Saha",
                "Kaushik Roy"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00761v2",
                "http://arxiv.org/pdf/2312.00761v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CV",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00752v1",
            "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
            "updated": "2023-12-01T18:01:34Z",
            "published": "2023-12-01T18:01:34Z",
            "summary": "Foundation models, now powering most of the exciting applications in deep\nlearning, are almost universally based on the Transformer architecture and its\ncore attention module. Many subquadratic-time architectures such as linear\nattention, gated convolution and recurrent models, and structured state space\nmodels (SSMs) have been developed to address Transformers' computational\ninefficiency on long sequences, but they have not performed as well as\nattention on important modalities such as language. We identify that a key\nweakness of such models is their inability to perform content-based reasoning,\nand make several improvements. First, simply letting the SSM parameters be\nfunctions of the input addresses their weakness with discrete modalities,\nallowing the model to selectively propagate or forget information along the\nsequence length dimension depending on the current token. Second, even though\nthis change prevents the use of efficient convolutions, we design a\nhardware-aware parallel algorithm in recurrent mode. We integrate these\nselective SSMs into a simplified end-to-end neural network architecture without\nattention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$\nhigher throughput than Transformers) and linear scaling in sequence length, and\nits performance improves on real data up to million-length sequences. As a\ngeneral sequence model backbone, Mamba achieves state-of-the-art performance\nacross several modalities such as language, audio, and genomics. On language\nmodeling, our Mamba-3B model outperforms Transformers of the same size and\nmatches Transformers twice its size, both in pretraining and downstream\nevaluation.",
            "author": [
                "Albert Gu",
                "Tri Dao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00752v1",
                "http://arxiv.org/pdf/2312.00752v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00746v1",
            "title": "Deciphering Digital Detectives: Understanding LLM Behaviors and\n  Capabilities in Multi-Agent Mystery Games",
            "updated": "2023-12-01T17:33:57Z",
            "published": "2023-12-01T17:33:57Z",
            "summary": "In this study, we explore the application of Large Language Models (LLMs) in\n\"Jubensha\" (Chinese murder mystery role-playing games), a novel area in\nAI-driven gaming. We introduce the first Chinese dataset specifically for\nJubensha, including character scripts and game rules, to foster AI agent\ndevelopment in this complex narrative environment. Our work also presents a\nunique multi-agent interaction framework using LLMs, allowing AI agents to\nautonomously engage in the game, enhancing the dynamics of Jubensha gameplay.\nTo evaluate these AI agents, we developed specialized methods targeting their\nmastery of case information and reasoning skills. Furthermore, we incorporated\nthe latest advancements in in-context learning to improve the agents'\nperformance in critical aspects like information gathering, murderer detection,\nand logical reasoning. The experimental results validate the effectiveness of\nour proposed methods. This work aims to offer a fresh perspective on\nunderstanding LLM capabilities and establish a new benchmark for evaluating\nlarge language model-based agents to researchers in the field.",
            "author": [
                "Dekun Wu",
                "Haochen Shi",
                "Zhiyuan Sun",
                "Bang Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00746v1",
                "http://arxiv.org/pdf/2312.00746v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "I.2.0; I.2.1; I.2.7"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00742v1",
            "title": "Scalable Meta-Learning with Gaussian Processes",
            "updated": "2023-12-01T17:25:10Z",
            "published": "2023-12-01T17:25:10Z",
            "summary": "Meta-learning is a powerful approach that exploits historical data to quickly\nsolve new tasks from the same distribution. In the low-data regime, methods\nbased on the closed-form posterior of Gaussian processes (GP) together with\nBayesian optimization have achieved high performance. However, these methods\nare either computationally expensive or introduce assumptions that hinder a\nprincipled propagation of uncertainty between task models. This may disrupt the\nbalance between exploration and exploitation during optimization. In this\npaper, we develop ScaML-GP, a modular GP model for meta-learning that is\nscalable in the number of tasks. Our core contribution is a carefully designed\nmulti-task kernel that enables hierarchical training and task scalability.\nConditioning ScaML-GP on the meta-data exposes its modular nature yielding a\ntest-task prior that combines the posteriors of meta-task GPs. In synthetic and\nreal-world meta-learning experiments, we demonstrate that ScaML-GP can learn\nefficiently both with few and many meta-tasks.",
            "author": [
                "Petru Tighineanu",
                "Lukas Grossberger",
                "Paul Baireuther",
                "Kathrin Skubch",
                "Stefan Falkner",
                "Julia Vinogradska",
                "Felix Berkenkamp"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00742v1",
                "http://arxiv.org/pdf/2312.00742v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00740v1",
            "title": "Computing Networks Enabled Semantic Communications",
            "updated": "2023-12-01T17:21:27Z",
            "published": "2023-12-01T17:21:27Z",
            "summary": "Semantic communication has shown great potential in boosting the\neffectiveness and reliability of communications. However, its systems to date\nare mostly enabled by deep learning, which requires demanding computing\nresources. This article proposes a framework for the computing networks enabled\nsemantic communication system, aiming to offer sufficient computing resources\nfor semantic processing and transmission. Key techniques including semantic\nsampling and reconstruction, semantic-channel coding, semantic-aware resource\nallocation and optimization are introduced based on the cloud-edge-end\ncomputing coordination. Two use cases are demonstrated to show advantages of\nthe proposed framework. The article concludes with several future research\ndirections.",
            "author": [
                "Zhijin Qin",
                "Jingkai Ying",
                "Dingxi Yang",
                "Hengjiang Wang",
                "Xiaoming Tao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00740v1",
                "http://arxiv.org/pdf/2312.00740v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00733v1",
            "title": "Provable bounds for noise-free expectation values computed from noisy\n  samples",
            "updated": "2023-12-01T17:12:18Z",
            "published": "2023-12-01T17:12:18Z",
            "summary": "In this paper, we explore the impact of noise on quantum computing,\nparticularly focusing on the challenges when sampling bit strings from noisy\nquantum computers as well as the implications for optimization and machine\nlearning applications. We formally quantify the sampling overhead to extract\ngood samples from noisy quantum computers and relate it to the layer fidelity,\na metric to determine the performance of noisy quantum processors. Further, we\nshow how this allows us to use the Conditional Value at Risk of noisy samples\nto determine provable bounds on noise-free expectation values. We discuss how\nto leverage these bounds for different algorithms and demonstrate our findings\nthrough experiments on a real quantum computer involving up to 127 qubits. The\nresults show a strong alignment with theoretical predictions.",
            "author": [
                "Samantha V. Barron",
                "Daniel J. Egger",
                "Elijah Pelofske",
                "Andreas B\u00e4rtschi",
                "Stephan Eidenbenz",
                "Matthis Lehmkuehler",
                "Stefan Woerner"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00733v1",
                "http://arxiv.org/pdf/2312.00733v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.ET"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00728v1",
            "title": "Soft computing for the posterior of a new matrix t graphical network",
            "updated": "2023-12-01T17:05:04Z",
            "published": "2023-12-01T17:05:04Z",
            "summary": "Modelling noisy data in a network context remains an unavoidable obstacle;\nfortunately, random matrix theory may comprehensively describe network\nenvironments effectively. Thus it necessitates the probabilistic\ncharacterisation of these networks (and accompanying noisy data) using matrix\nvariate models. Denoising network data using a Bayes approach is not common in\nsurveyed literature. This paper adopts the Bayesian viewpoint and introduces a\nnew matrix variate t-model in a prior sense by relying on the matrix variate\ngamma distribution for the noise process, following the Gaussian graphical\nnetwork for the cases when the normality assumption is violated. From a\nstatistical learning viewpoint, such a theoretical consideration indubitably\nbenefits the real-world comprehension of structures causing noisy data with\nnetwork-based attributes as part of machine learning in data science. A full\nstructural learning procedure is provided for calculating and approximating the\nresulting posterior of interest to assess the considered model's network\ncentrality measures. Experiments with synthetic and real-world stock price data\nare performed not only to validate the proposed algorithm's capabilities but\nalso to show that this model has wider flexibility than originally implied in\nBillio et al. (2021).",
            "author": [
                "J. Pillay",
                "A. Bekker",
                "J. T. Ferreira",
                "M. Arashi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00728v1",
                "http://arxiv.org/pdf/2312.00728v1"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02185v1",
            "title": "Virtual Fusion with Contrastive Learning for Single Sensor-based\n  Activity Recognition",
            "updated": "2023-12-01T17:03:27Z",
            "published": "2023-12-01T17:03:27Z",
            "summary": "Various types of sensors can be used for Human Activity Recognition (HAR),\nand each of them has different strengths and weaknesses. Sometimes a single\nsensor cannot fully observe the user's motions from its perspective, which\ncauses wrong predictions. While sensor fusion provides more information for\nHAR, it comes with many inherent drawbacks like user privacy and acceptance,\ncostly set-up, operation, and maintenance. To deal with this problem, we\npropose Virtual Fusion - a new method that takes advantage of unlabeled data\nfrom multiple time-synchronized sensors during training, but only needs one\nsensor for inference. Contrastive learning is adopted to exploit the\ncorrelation among sensors. Virtual Fusion gives significantly better accuracy\nthan training with the same single sensor, and in some cases, it even surpasses\nactual fusion using multiple sensors at test time. We also extend this method\nto a more general version called Actual Fusion within Virtual Fusion (AFVF),\nwhich uses a subset of training sensors during inference. Our method achieves\nstate-of-the-art accuracy and F1-score on UCI-HAR and PAMAP2 benchmark\ndatasets. Implementation is available upon request.",
            "author": [
                "Duc-Anh Nguyen",
                "Cuong Pham",
                "Nhien-An Le-Khac"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02185v1",
                "http://arxiv.org/pdf/2312.02185v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00727v1",
            "title": "Safe Reinforcement Learning in Tensor Reproducing Kernel Hilbert Space",
            "updated": "2023-12-01T17:01:37Z",
            "published": "2023-12-01T17:01:37Z",
            "summary": "This paper delves into the problem of safe reinforcement learning (RL) in a\npartially observable environment with the aim of achieving safe-reachability\nobjectives. In traditional partially observable Markov decision processes\n(POMDP), ensuring safety typically involves estimating the belief in latent\nstates. However, accurately estimating an optimal Bayesian filter in POMDP to\ninfer latent states from observations in a continuous state space poses a\nsignificant challenge, largely due to the intractable likelihood. To tackle\nthis issue, we propose a stochastic model-based approach that guarantees RL\nsafety almost surely in the face of unknown system dynamics and partial\nobservation environments. We leveraged the Predictive State Representation\n(PSR) and Reproducing Kernel Hilbert Space (RKHS) to represent future\nmulti-step observations analytically, and the results in this context are\nprovable. Furthermore, we derived essential operators from the kernel Bayes'\nrule, enabling the recursive estimation of future observations using various\noperators. Under the assumption of \\textit{undercompleness}, a polynomial\nsample complexity is established for the RL algorithm for the infinite size of\nobservation and action spaces, ensuring an $\\epsilon-$suboptimal safe policy\nguarantee.",
            "author": [
                "Xiaoyuan Cheng",
                "Boli Chen",
                "Liz Varga",
                "Yukun Hu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00727v1",
                "http://arxiv.org/pdf/2312.00727v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00720v1",
            "title": "Efficiently Processing Large Relational Joins on GPUs",
            "updated": "2023-12-01T16:55:17Z",
            "published": "2023-12-01T16:55:17Z",
            "summary": "With the growing interest in Machine Learning (ML), Graphic Processing Units\n(GPUs) have become key elements of any computing infrastructure. Their\nwidespread deployment in data centers and the cloud raises the question of how\nto use them beyond ML use cases, with growing interest in employing them in a\ndatabase context. In this paper, we explore and analyze the implementation of\nrelational joins on GPUs from an end-to-end perspective, meaning that we take\nresult materialization into account. We conduct a comprehensive performance\nstudy of state-of-the-art GPU-based join algorithms over diverse synthetic\nworkloads and TPC-H/TPC-DS benchmarks. Without being restricted to the\nconventional setting where each input relation has only one key and one non-key\nwith all attributes being 4-bytes long, we investigate the effect of various\nfactors (e.g., input sizes, number of non-key columns, skewness, data types,\nmatch ratios, and number of joins) on the end-to-end throughput. Furthermore,\nwe propose a technique called \"Gather-from-Transformed-Relations\" (GFTR) to\nreduce the long-ignored yet high materialization cost in GPU-based joins. The\nexperimental evaluation shows significant performance improvements from GFTR,\nwith throughput gains of up to 2.3 times over previous work. The insights\ngained from the performance study not only advance the understanding of\nGPU-based joins but also introduce a structured approach to selecting the most\nefficient GPU join algorithm based on the input relation characteristics.",
            "author": [
                "Bowen Wu",
                "Dimitrios Koutsoukos",
                "Gustavo Alonso"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00720v1",
                "http://arxiv.org/pdf/2312.00720v1"
            ],
            "primary_category": "cs.DB",
            "category": [
                "cs.DB"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00718v1",
            "title": "Removing Biases from Molecular Representations via Information\n  Maximization",
            "updated": "2023-12-01T16:53:15Z",
            "published": "2023-12-01T16:53:15Z",
            "summary": "High-throughput drug screening -- using cell imaging or gene expression\nmeasurements as readouts of drug effect -- is a critical tool in biotechnology\nto assess and understand the relationship between the chemical structure and\nbiological activity of a drug. Since large-scale screens have to be divided\ninto multiple experiments, a key difficulty is dealing with batch effects,\nwhich can introduce systematic errors and non-biological associations in the\ndata. We propose InfoCORE, an Information maximization approach for COnfounder\nREmoval, to effectively deal with batch effects and obtain refined molecular\nrepresentations. InfoCORE establishes a variational lower bound on the\nconditional mutual information of the latent representations given a batch\nidentifier. It adaptively reweighs samples to equalize their implied batch\ndistribution. Extensive experiments on drug screening data reveal InfoCORE's\nsuperior performance in a multitude of tasks including molecular property\nprediction and molecule-phenotype retrieval. Additionally, we show results for\nhow InfoCORE offers a versatile framework and resolves general distribution\nshifts and issues of data fairness by minimizing correlation with spurious\nfeatures or removing sensitive attributes. The code is available at\nhttps://github.com/uhlerlab/InfoCORE.",
            "author": [
                "Chenyu Wang",
                "Sharut Gupta",
                "Caroline Uhler",
                "Tommi Jaakkola"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00718v1",
                "http://arxiv.org/pdf/2312.00718v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "q-bio.BM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00713v1",
            "title": "Nonlinear-manifold reduced order models with domain decomposition",
            "updated": "2023-12-01T16:47:13Z",
            "published": "2023-12-01T16:47:13Z",
            "summary": "A nonlinear-manifold reduced order model (NM-ROM) is a great way of\nincorporating underlying physics principles into a neural network-based\ndata-driven approach. We combine NM-ROMs with domain decomposition (DD) for\nefficient computation. NM-ROMs offer benefits over linear-subspace ROMs\n(LS-ROMs) but can be costly to train due to parameter scaling with the\nfull-order model (FOM) size. To address this, we employ DD on the FOM, compute\nsubdomain NM-ROMs, and then merge them into a global NM-ROM. This approach has\nmultiple advantages: parallel training of subdomain NM-ROMs, fewer parameters\nthan global NM-ROMs, and adaptability to subdomain-specific FOM features. Each\nsubdomain NM-ROM uses a shallow, sparse autoencoder, enabling hyper-reduction\n(HR) for improved computational speed. In this paper, we detail an algebraic DD\nformulation for the FOM, train HR-equipped NM-ROMs for subdomains, and\nnumerically compare them to DD LS-ROMs with HR. Results show a significant\naccuracy boost, on the order of magnitude, for the proposed DD NM-ROMs over DD\nLS-ROMs in solving the 2D steady-state Burgers' equation.",
            "author": [
                "Alejandro N. Diaz",
                "Youngsoo Choi",
                "Matthias Heinkenschloss"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00713v1",
                "http://arxiv.org/pdf/2312.00713v1"
            ],
            "primary_category": "math.NA",
            "category": [
                "math.NA",
                "cs.NA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00710v2",
            "title": "SpaCE: The Spatial Confounding Environment",
            "updated": "2023-12-06T02:00:53Z",
            "published": "2023-12-01T16:42:57Z",
            "summary": "Spatial confounding poses a significant challenge in scientific studies\ninvolving spatial data, where unobserved spatial variables can influence both\ntreatment and outcome, possibly leading to spurious associations. To address\nthis problem, we introduce SpaCE: The Spatial Confounding Environment, the\nfirst toolkit to provide realistic benchmark datasets and tools for\nsystematically evaluating causal inference methods designed to alleviate\nspatial confounding. Each dataset includes training data, true counterfactuals,\na spatial graph with coordinates, and smoothness and confounding scores\ncharacterizing the effect of a missing spatial confounder. It also includes\nrealistic semi-synthetic outcomes and counterfactuals, generated using\nstate-of-the-art machine learning ensembles, following best practices for\ncausal inference benchmarks. The datasets cover real treatment and covariates\nfrom diverse domains, including climate, health and social sciences. SpaCE\nfacilitates an automated end-to-end pipeline, simplifying data loading,\nexperimental setup, and evaluating machine learning and causal inference\nmodels. The SpaCE project provides several dozens of datasets of diverse sizes\nand spatial complexity. It is publicly available as a Python package,\nencouraging community feedback and contributions.",
            "author": [
                "Mauricio Tec",
                "Ana Trisovic",
                "Michelle Audirac",
                "Sophie Woodward",
                "Jie Kate Hu",
                "Naeem Khoshnevis",
                "Francesca Dominici"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00710v2",
                "http://arxiv.org/pdf/2312.00710v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ME",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00708v1",
            "title": "Message-Passing on Hypergraphs: Detectability, Phase Transitions and\n  Higher-Order Information",
            "updated": "2023-12-01T16:42:28Z",
            "published": "2023-12-01T16:42:28Z",
            "summary": "Hypergraphs are widely adopted tools to examine systems with higher-order\ninteractions. Despite recent advancements in methods for community detection in\nthese systems, we still lack a theoretical analysis of their detectability\nlimits. Here, we derive closed-form bounds for community detection in\nhypergraphs. Using a Message-Passing formulation, we demonstrate that\ndetectability depends on hypergraphs' structural properties, such as the\ndistribution of hyperedge sizes or their assortativity. Our formulation enables\na characterization of the entropy of a hypergraph in relation to that of its\nclique expansion, showing that community detection is enhanced when hyperedges\nhighly overlap on pairs of nodes. We develop an efficient Message-Passing\nalgorithm to learn communities and model parameters on large systems.\nAdditionally, we devise an exact sampling routine to generate synthetic data\nfrom our probabilistic model. With these methods, we numerically investigate\nthe boundaries of community detection in synthetic datasets, and extract\ncommunities from real systems. Our results extend the understanding of the\nlimits of community detection in hypergraphs and introduce flexible\nmathematical tools to study systems with higher-order interactions.",
            "author": [
                "Nicol\u00f2 Ruggeri",
                "Alessandro Lonardi",
                "Caterina De Bacco"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00708v1",
                "http://arxiv.org/pdf/2312.00708v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI",
                "cs.IT",
                "math.IT",
                "physics.data-an"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00700v1",
            "title": "GIFT: Generative Interpretable Fine-Tuning Transformers",
            "updated": "2023-12-01T16:33:57Z",
            "published": "2023-12-01T16:33:57Z",
            "summary": "We present GIFT (Generative Interpretable Fine-tuning Transformers) for\nfine-tuning pretrained (often large) Transformer models at downstream tasks in\na parameter-efficient way with built-in interpretability. Our GIFT is a deep\nparameter-residual learning method, which addresses two problems in fine-tuning\na pretrained Transformer model: Where to apply the parameter-efficient\nfine-tuning (PEFT) to be extremely lightweight yet sufficiently expressive, and\nHow to learn the PEFT to better exploit the knowledge of the pretrained model\nin a direct way? For the former, we select the final projection (linear) layer\nin the multi-head self-attention of a Transformer model, and verify its\neffectiveness. For the latter, in contrast to the prior art that directly\nintroduce new model parameters (often in low-rank approximation form) to be\nlearned in fine-tuning with downstream data, we propose a method for learning\nto generate the fine-tuning parameters. Our GIFT is a hyper-Transformer which\ntake as input the pretrained parameters of the projection layer to generate its\nfine-tuning parameters using a proposed Parameter-to-Cluster Attention (PaCa).\nThe PaCa results in a simple clustering-based forward explainer that plays the\nrole of semantic segmentation in testing. In experiments, our proposed GIFT is\ntested on the VTAB benchmark and the fine-grained visual classification (FGVC)\nbenchmark. It obtains significantly better performance than the prior art. Our\ncode is available at https://github.com/savadikarc/gift",
            "author": [
                "Chinmay Savadikar",
                "Xi Song",
                "Tianfu Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00700v1",
                "http://arxiv.org/pdf/2312.00700v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00694v1",
            "title": "Object Detector Differences when using Synthetic and Real Training Data",
            "updated": "2023-12-01T16:27:48Z",
            "published": "2023-12-01T16:27:48Z",
            "summary": "To train well-performing generalizing neural networks, sufficiently large and\ndiverse datasets are needed. Collecting data while adhering to privacy\nlegislation becomes increasingly difficult and annotating these large datasets\nis both a resource-heavy and time-consuming task. An approach to overcome these\ndifficulties is to use synthetic data since it is inherently scalable and can\nbe automatically annotated. However, how training on synthetic data affects the\nlayers of a neural network is still unclear. In this paper, we train the YOLOv3\nobject detector on real and synthetic images from city environments. We perform\na similarity analysis using Centered Kernel Alignment (CKA) to explore the\neffects of training on synthetic data on a layer-wise basis. The analysis\ncaptures the architecture of the detector while showing both different and\nsimilar patterns between different models. With this similarity analysis we\nwant to give insights on how training synthetic data affects each layer and to\ngive a better understanding of the inner workings of complex neural networks.\nThe results show that the largest similarity between a detector trained on real\ndata and a detector trained on synthetic data was in the early layers, and the\nlargest difference was in the head part. The results also show that no major\ndifference in performance or similarity could be seen between frozen and\nunfrozen backbone.",
            "author": [
                "Martin Georg Ljungqvist",
                "Otto Nordander",
                "Markus Skans",
                "Arvid Mildner",
                "Tony Liu",
                "Pierre Nugues"
            ],
            "link": [
                "http://dx.doi.org/10.1007/s42979-023-01704-5",
                "http://arxiv.org/abs/2312.00694v1",
                "http://arxiv.org/pdf/2312.00694v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG",
                "I.4.0; I.2.10; I.5.0"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00690v2",
            "title": "Open-vocabulary object 6D pose estimation",
            "updated": "2023-12-07T09:41:54Z",
            "published": "2023-12-01T16:17:16Z",
            "summary": "We introduce the new setting of open-vocabulary object 6D pose estimation, in\nwhich a textual prompt is used to specify the object of interest. In contrast\nto existing approaches, in our setting (i) the object of interest is specified\nsolely through the textual prompt, (ii) no object model (e.g. CAD or video\nsequence) is required at inference, (iii) the object is imaged from two\ndifferent viewpoints of two different scenes, and (iv) the object was not\nobserved during the training phase. To operate in this setting, we introduce a\nnovel approach that leverages a Vision-Language Model to segment the object of\ninterest from two distinct scenes and to estimate its relative 6D pose. The key\nof our approach is a carefully devised strategy to fuse object-level\ninformation provided by the prompt with local image features, resulting in a\nfeature space that can generalize to novel concepts. We validate our approach\non a new benchmark based on two popular datasets, REAL275 and Toyota-Light,\nwhich collectively encompass 39 object instances appearing in four thousand\nimage pairs. The results demonstrate that our approach outperforms both a\nwell-established hand-crafted method and a recent deep learning-based baseline\nin estimating the relative 6D pose of objects in different scenes. Project\npage: https://jcorsetti.github.io/oryon/.",
            "author": [
                "Jaime Corsetti",
                "Davide Boscaini",
                "Changjae Oh",
                "Andrea Cavallaro",
                "Fabio Poiesi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00690v2",
                "http://arxiv.org/pdf/2312.00690v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00688v1",
            "title": "Towards Transparency in Coreference Resolution: A Quantum-Inspired\n  Approach",
            "updated": "2023-12-01T16:11:38Z",
            "published": "2023-12-01T16:11:38Z",
            "summary": "Guided by grammatical structure, words compose to form sentences, and guided\nby discourse structure, sentences compose to form dialogues and documents. The\ncompositional aspect of sentence and discourse units is often overlooked by\nmachine learning algorithms. A recent initiative called Quantum Natural\nLanguage Processing (QNLP) learns word meanings as points in a Hilbert space\nand acts on them via a translation of grammatical structure into Parametrised\nQuantum Circuits (PQCs). Previous work extended the QNLP translation to\ndiscourse structure using points in a closure of Hilbert spaces. In this paper,\nwe evaluate this translation on a Winograd-style pronoun resolution task. We\ntrain a Variational Quantum Classifier (VQC) for binary classification and\nimplement an end-to-end pronoun resolution system. The simulations executed on\nIBMQ software converged with an F1 score of 87.20%. The model outperformed two\nout of three classical coreference resolution systems and neared\nstate-of-the-art SpanBERT. A mixed quantum-classical model yet improved these\nresults with an F1 score increase of around 6%.",
            "author": [
                "Hadi Wazni",
                "Mehrnoosh Sadrzadeh"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00688v1",
                "http://arxiv.org/pdf/2312.00688v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG",
                "cs.LO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00677v1",
            "title": "Unsupervised Adaptive Implicit Neural Representation Learning for\n  Scan-Specific MRI Reconstruction",
            "updated": "2023-12-01T16:00:16Z",
            "published": "2023-12-01T16:00:16Z",
            "summary": "In recent studies on MRI reconstruction, advances have shown significant\npromise for further accelerating the MRI acquisition. Most state-of-the-art\nmethods require a large amount of fully-sampled data to optimise reconstruction\nmodels, which is impractical and expensive under certain clinical settings. On\nthe other hand, for unsupervised scan-specific reconstruction methods,\noverfitting is likely to happen due to insufficient supervision, while\nrestrictions on acceleration rates and under-sampling patterns further limit\ntheir applicability. To this end, we propose an unsupervised, adaptive\ncoarse-to-fine framework that enhances reconstruction quality without being\nconstrained by the sparsity levels or patterns in under-sampling. The framework\nemploys an implicit neural representation for scan-specific MRI reconstruction,\nlearning a mapping from multi-dimensional coordinates to their corresponding\nsignal intensities. Moreover, we integrate a novel learning strategy that\nprogressively refines the use of acquired k-space signals for self-supervision.\nThis approach effectively adjusts the proportion of supervising signals from\nunevenly distributed information across different frequency bands, thus\nmitigating the issue of overfitting while improving the overall reconstruction.\nComprehensive evaluation on a public dataset, including both 2D and 3D data,\nhas shown that our method outperforms current state-of-the-art scan-specific\nMRI reconstruction techniques, for up to 8-fold under-sampling.",
            "author": [
                "Junwei Yang",
                "Pietro Li\u00f2"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00677v1",
                "http://arxiv.org/pdf/2312.00677v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00674v1",
            "title": "LightCLIP: Learning Multi-Level Interaction for Lightweight\n  Vision-Language Models",
            "updated": "2023-12-01T15:54:55Z",
            "published": "2023-12-01T15:54:55Z",
            "summary": "Vision-language pre-training like CLIP has shown promising performance on\nvarious downstream tasks such as zero-shot image classification and image-text\nretrieval. Most of the existing CLIP-alike works usually adopt relatively large\nimage encoders like ResNet50 and ViT, while the lightweight counterparts are\nrarely discussed. In this paper, we propose a multi-level interaction paradigm\nfor training lightweight CLIP models. Firstly, to mitigate the problem that\nsome image-text pairs are not strictly one-to-one correspondence, we improve\nthe conventional global instance-level alignment objective by softening the\nlabel of negative samples progressively. Secondly, a relaxed bipartite matching\nbased token-level alignment objective is introduced for finer-grained alignment\nbetween image patches and textual words. Moreover, based on the observation\nthat the accuracy of CLIP model does not increase correspondingly as the\nparameters of text encoder increase, an extra objective of masked language\nmodeling (MLM) is leveraged for maximizing the potential of the shortened text\nencoder. In practice, an auxiliary fusion module injecting unmasked image\nembedding into masked text embedding at different network stages is proposed\nfor enhancing the MLM. Extensive experiments show that without introducing\nadditional computational cost during inference, the proposed method achieves a\nhigher performance on multiple downstream tasks.",
            "author": [
                "Ying Nie",
                "Wei He",
                "Kai Han",
                "Yehui Tang",
                "Tianyu Guo",
                "Fanyi Du",
                "Yunhe Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00674v1",
                "http://arxiv.org/pdf/2312.00674v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00671v1",
            "title": "CellMixer: Annotation-free Semantic Cell Segmentation of Heterogeneous\n  Cell Populations",
            "updated": "2023-12-01T15:50:20Z",
            "published": "2023-12-01T15:50:20Z",
            "summary": "In recent years, several unsupervised cell segmentation methods have been\npresented, trying to omit the requirement of laborious pixel-level annotations\nfor the training of a cell segmentation model. Most if not all of these methods\nhandle the instance segmentation task by focusing on the detection of different\ncell instances ignoring their type. While such models prove adequate for\ncertain tasks, like cell counting, other applications require the\nidentification of each cell's type. In this paper, we present CellMixer, an\ninnovative annotation-free approach for the semantic segmentation of\nheterogeneous cell populations. Our augmentation-based method enables the\ntraining of a segmentation model from image-level labels of homogeneous cell\npopulations. Our results show that CellMixer can achieve competitive\nsegmentation performance across multiple cell types and imaging modalities,\ndemonstrating the method's scalability and potential for broader applications\nin medical imaging, cellular biology, and diagnostics.",
            "author": [
                "Mehdi Naouar",
                "Gabriel Kalweit",
                "Anusha Klett",
                "Yannick Vogt",
                "Paula Silvestrini",
                "Diana Laura Infante Ramirez",
                "Roland Mertelsmann",
                "Joschka Boedecker",
                "Maria Kalweit"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00671v1",
                "http://arxiv.org/pdf/2312.00671v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00663v1",
            "title": "Generalized Label-Efficient 3D Scene Parsing via Hierarchical Feature\n  Aligned Pre-Training and Region-Aware Fine-tuning",
            "updated": "2023-12-01T15:47:04Z",
            "published": "2023-12-01T15:47:04Z",
            "summary": "Deep neural network models have achieved remarkable progress in 3D scene\nunderstanding while trained in the closed-set setting and with full labels.\nHowever, the major bottleneck for current 3D recognition approaches is that\nthey do not have the capacity to recognize any unseen novel classes beyond the\ntraining categories in diverse kinds of real-world applications. In the\nmeantime, current state-of-the-art 3D scene understanding approaches primarily\nrequire high-quality labels to train neural networks, which merely perform well\nin a fully supervised manner. This work presents a generalized and simple\nframework for dealing with 3D scene understanding when the labeled scenes are\nquite limited. To extract knowledge for novel categories from the pre-trained\nvision-language models, we propose a hierarchical feature-aligned pre-training\nand knowledge distillation strategy to extract and distill meaningful\ninformation from large-scale vision-language models, which helps benefit the\nopen-vocabulary scene understanding tasks. To leverage the boundary\ninformation, we propose a novel energy-based loss with boundary awareness\nbenefiting from the region-level boundary predictions. To encourage latent\ninstance discrimination and to guarantee efficiency, we propose the\nunsupervised region-level semantic contrastive learning scheme for point\nclouds, using confident predictions of the neural network to discriminate the\nintermediate feature embeddings at multiple stages. Extensive experiments with\nboth indoor and outdoor scenes demonstrated the effectiveness of our approach\nin both data-efficient learning and open-world few-shot learning. All codes,\nmodels, and data are made publicly available at:\nhttps://drive.google.com/drive/folders/1M58V-PtR8DBEwD296zJkNg_m2qq-MTAP?usp=sharing.",
            "author": [
                "Kangcheng Liu",
                "Yong-Jin Liu",
                "Kai Tang",
                "Ming Liu",
                "Baoquan Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00663v1",
                "http://arxiv.org/pdf/2312.00663v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00662v1",
            "title": "Nonparametric Variational Regularisation of Pretrained Transformers",
            "updated": "2023-12-01T15:40:30Z",
            "published": "2023-12-01T15:40:30Z",
            "summary": "The current paradigm of large-scale pre-training and fine-tuning Transformer\nlarge language models has lead to significant improvements across the board in\nnatural language processing. However, such large models are susceptible to\noverfitting to their training data, and as a result the models perform poorly\nwhen the domain changes. Also, due to the model's scale, the cost of\nfine-tuning the model to the new domain is large. Nonparametric Variational\nInformation Bottleneck (NVIB) has been proposed as a regulariser for training\ncross-attention in Transformers, potentially addressing the overfitting\nproblem. We extend the NVIB framework to replace all types of attention\nfunctions in Transformers, and show that existing pretrained Transformers can\nbe reinterpreted as Nonparametric Variational (NV) models using a proposed\nidentity initialisation. We then show that changing the initialisation\nintroduces a novel, information-theoretic post-training regularisation in the\nattention mechanism, which improves out-of-domain generalisation without any\ntraining. This success supports the hypothesis that pretrained Transformers are\nimplicitly NV Bayesian models.",
            "author": [
                "Fabio Fehr",
                "James Henderson"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00662v1",
                "http://arxiv.org/pdf/2312.00662v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00661v1",
            "title": "Dual-Domain Multi-Contrast MRI Reconstruction with Synthesis-based\n  Fusion Network",
            "updated": "2023-12-01T15:40:26Z",
            "published": "2023-12-01T15:40:26Z",
            "summary": "Purpose: To develop an efficient dual-domain reconstruction framework for\nmulti-contrast MRI, with the focus on minimising cross-contrast misalignment in\nboth the image and the frequency domains to enhance optimisation. Theory and\nMethods: Our proposed framework, based on deep learning, facilitates the\noptimisation for under-sampled target contrast using fully-sampled reference\ncontrast that is quicker to acquire. The method consists of three key steps: 1)\nLearning to synthesise data resembling the target contrast from the reference\ncontrast; 2) Registering the multi-contrast data to reduce inter-scan motion;\nand 3) Utilising the registered data for reconstructing the target contrast.\nThese steps involve learning in both domains with regularisation applied to\nensure their consistency. We also compare the reconstruction performance with\nexisting deep learning-based methods using a dataset of brain MRI scans.\nResults: Extensive experiments demonstrate the superiority of our proposed\nframework, for up to an 8-fold acceleration rate, compared to state-of-the-art\nalgorithms. Comprehensive analysis and ablation studies further present the\neffectiveness of the proposed components. Conclusion:Our dual-domain framework\noffers a promising approach to multi-contrast MRI reconstruction. It can also\nbe integrated with existing methods to further enhance the reconstruction.",
            "author": [
                "Junwei Yang",
                "Pietro Li\u00f2"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00661v1",
                "http://arxiv.org/pdf/2312.00661v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00660v1",
            "title": "Resource-constrained knowledge diffusion processes inspired by human\n  peer learning",
            "updated": "2023-12-01T15:39:24Z",
            "published": "2023-12-01T15:39:24Z",
            "summary": "We consider a setting where a population of artificial learners is given, and\nthe objective is to optimize aggregate measures of performance, under\nconstraints on training resources. The problem is motivated by the study of\npeer learning in human educational systems. In this context, we study natural\nknowledge diffusion processes in networks of interacting artificial learners.\nBy `natural', we mean processes that reflect human peer learning where the\nstudents' internal state and learning process is mostly opaque, and the main\ndegree of freedom lies in the formation of peer learning groups by a\ncoordinator who can potentially evaluate the learners before assigning them to\npeer groups. Among else, we empirically show that such processes indeed make\neffective use of the training resources, and enable the design of modular\nneural models that have the capacity to generalize without being prone to\noverfitting noisy labels.",
            "author": [
                "Ehsan Beikihassan",
                "Amy K. Hoover",
                "Ioannis Koutis",
                "Ali Parviz",
                "Niloofar Aghaieabiane"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00660v1",
                "http://arxiv.org/pdf/2312.00660v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00659v1",
            "title": "New physics at the Intensity Frontier: how much can we learn and how?",
            "updated": "2023-12-01T15:38:15Z",
            "published": "2023-12-01T15:38:15Z",
            "summary": "Intensity Frontier experiments are often evaluated by the smallest coupling\nit can probe, irrespective of what particle can be found or the scientific\nsignificance of its detection. In this work, we propose a new framework that\ndetermines the number of events required to characterize new particle\nproperties. For example, we show that Heavy Neutral Leptons require 100 events\nto establish the neutrino mass hierarchy, and 1000 events to reveal the\nMajorana phase of active neutrinos. Ultimately, this framework presents a more\nobjective way to connect experiments to their scientific outcomes.",
            "author": [
                "Oleksii Mikulenko",
                "Kyrylo Bondarenko",
                "Alexey Boyarsky",
                "Oleg Ruchayskiy"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00659v1",
                "http://arxiv.org/pdf/2312.00659v1"
            ],
            "primary_category": "hep-ph",
            "category": [
                "hep-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00656v2",
            "title": "Simple Transferability Estimation for Regression Tasks",
            "updated": "2023-12-04T03:26:35Z",
            "published": "2023-12-01T15:30:54Z",
            "summary": "We consider transferability estimation, the problem of estimating how well\ndeep learning models transfer from a source to a target task. We focus on\nregression tasks, which received little previous attention, and propose two\nsimple and computationally efficient approaches that estimate transferability\nbased on the negative regularized mean squared error of a linear regression\nmodel. We prove novel theoretical results connecting our approaches to the\nactual transferability of the optimal target models obtained from the transfer\nlearning process. Despite their simplicity, our approaches significantly\noutperform existing state-of-the-art regression transferability estimators in\nboth accuracy and efficiency. On two large-scale keypoint regression\nbenchmarks, our approaches yield 12% to 36% better results on average while\nbeing at least 27% faster than previous state-of-the-art methods.",
            "author": [
                "Cuong N. Nguyen",
                "Phong Tran",
                "Lam Si Tung Ho",
                "Vu Dinh",
                "Anh T. Tran",
                "Tal Hassner",
                "Cuong V. Nguyen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00656v2",
                "http://arxiv.org/pdf/2312.00656v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00655v1",
            "title": "Machine Learning for Health symposium 2023 -- Findings track",
            "updated": "2023-12-01T15:30:43Z",
            "published": "2023-12-01T15:30:43Z",
            "summary": "A collection of the accepted Findings papers that were presented at the 3rd\nMachine Learning for Health symposium (ML4H 2023), which was held on December\n10, 2023, in New Orleans, Louisiana, USA. ML4H 2023 invited high-quality\nsubmissions on relevant problems in a variety of health-related disciplines\nincluding healthcare, biomedicine, and public health. Two submission tracks\nwere offered: the archival Proceedings track, and the non-archival Findings\ntrack. Proceedings were targeted at mature work with strong technical\nsophistication and a high impact to health. The Findings track looked for new\nideas that could spark insightful discussion, serve as valuable resources for\nthe community, or could enable new collaborations. Submissions to the\nProceedings track, if not accepted, were automatically considered for the\nFindings track. All the manuscripts submitted to ML4H Symposium underwent a\ndouble-blind peer-review process.",
            "author": [
                "Stefan Hegselmann",
                "Antonio Parziale",
                "Divya Shanmugam",
                "Shengpu Tang",
                "Mercy Nyamewaa Asiedu",
                "Serina Chang",
                "Thomas Hartvigsen",
                "Harvineet Singh"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00655v1",
                "http://arxiv.org/pdf/2312.00655v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "68Txx",
                "I.2; J.3; I.6; I.4"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00857v1",
            "title": "Latent Space Explorer: Visual Analytics for Multimodal Latent Space\n  Exploration",
            "updated": "2023-12-01T15:25:56Z",
            "published": "2023-12-01T15:25:56Z",
            "summary": "Machine learning models built on training data with multiple modalities can\nreveal new insights that are not accessible through unimodal datasets. For\nexample, cardiac magnetic resonance images (MRIs) and electrocardiograms (ECGs)\nare both known to capture useful information about subjects' cardiovascular\nhealth status. A multimodal machine learning model trained from large datasets\ncan potentially predict the onset of heart-related diseases and provide novel\nmedical insights about the cardiovascular system. Despite the potential\nbenefits, it is difficult for medical experts to explore multimodal\nrepresentation models without visual aids and to test the predictive\nperformance of the models on various subpopulations. To address the challenges,\nwe developed a visual analytics system called Latent Space Explorer. Latent\nSpace Explorer provides interactive visualizations that enable users to explore\nthe multimodal representation of subjects, define subgroups of interest,\ninteractively decode data with different modalities with the selected subjects,\nand inspect the accuracy of the embedding in downstream prediction tasks. A\nuser study was conducted with medical experts and their feedback provided\nuseful insights into how Latent Space Explorer can help their analysis and\npossible new direction for further development in the medical domain.",
            "author": [
                "Bum Chul Kwon",
                "Samuel Friedman",
                "Kai Xu",
                "Steven A Lubitz",
                "Anthony Philippakis",
                "Puneet Batra",
                "Patrick T Ellinor",
                "Kenney Ng"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00857v1",
                "http://arxiv.org/pdf/2312.00857v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.HC",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00648v1",
            "title": "SPOT: Self-Training with Patch-Order Permutation for Object-Centric\n  Learning with Autoregressive Transformers",
            "updated": "2023-12-01T15:20:58Z",
            "published": "2023-12-01T15:20:58Z",
            "summary": "Unsupervised object-centric learning aims to decompose scenes into\ninterpretable object entities, termed slots. Slot-based auto-encoders stand out\nas a prominent method for this task. Within them, crucial aspects include\nguiding the encoder to generate object-specific slots and ensuring the decoder\nutilizes them during reconstruction. This work introduces two novel techniques,\n(i) an attention-based self-training approach, which distills superior\nslot-based attention masks from the decoder to the encoder, enhancing object\nsegmentation, and (ii) an innovative patch-order permutation strategy for\nautoregressive transformers that strengthens the role of slot vectors in\nreconstruction. The effectiveness of these strategies is showcased\nexperimentally. The combined approach significantly surpasses prior slot-based\nautoencoder methods in unsupervised object segmentation, especially with\ncomplex real-world images. We provide the implementation code at\nhttps://github.com/gkakogeorgiou/spot .",
            "author": [
                "Ioannis Kakogeorgiou",
                "Spyros Gidaris",
                "Konstantinos Karantzalos",
                "Nikos Komodakis"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00648v1",
                "http://arxiv.org/pdf/2312.00648v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00645v1",
            "title": "Hashmarks: Privacy-Preserving Benchmarks for High-Stakes AI Evaluation",
            "updated": "2023-12-01T15:16:00Z",
            "published": "2023-12-01T15:16:00Z",
            "summary": "There is a growing need to gain insight into language model capabilities that\nrelate to sensitive topics, such as bioterrorism or cyberwarfare. However,\ntraditional open source benchmarks are not fit for the task, due to the\nassociated practice of publishing the correct answers in human-readable form.\nAt the same time, enforcing mandatory closed-quarters evaluations might stifle\ndevelopment and erode trust. In this context, we propose hashmarking, a\nprotocol for evaluating language models in the open without having to disclose\nthe correct answers. In its simplest form, a hashmark is a benchmark whose\nreference solutions have been cryptographically hashed prior to publication.\nFollowing an overview of the proposed evaluation protocol, we go on to assess\nits resilience against traditional attack vectors (e.g. rainbow table attacks),\nas well as against failure modes unique to increasingly capable generative\nmodels.",
            "author": [
                "Paul Bricman"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00645v1",
                "http://arxiv.org/pdf/2312.00645v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CR",
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00644v1",
            "title": "Neural networks for the approximation of Euler's elastica",
            "updated": "2023-12-01T15:07:25Z",
            "published": "2023-12-01T15:07:25Z",
            "summary": "Euler's elastica is a classical model of flexible slender structures,\nrelevant in many industrial applications. Static equilibrium equations can be\nderived via a variational principle. The accurate approximation of solutions of\nthis problem can be challenging due to nonlinearity and constraints. We here\npresent two neural network based approaches for the simulation of this Euler's\nelastica. Starting from a data set of solutions of the discretised static\nequilibria, we train the neural networks to produce solutions for unseen\nboundary conditions. We present a $\\textit{discrete}$ approach learning\ndiscrete solutions from the discrete data. We then consider a\n$\\textit{continuous}$ approach using the same training data set, but learning\ncontinuous solutions to the problem. We present numerical evidence that the\nproposed neural networks can effectively approximate configurations of the\nplanar Euler's elastica for a range of different boundary conditions.",
            "author": [
                "Elena Celledoni",
                "Ergys \u00c7okaj",
                "Andrea Leone",
                "Sigrid Leyendecker",
                "Davide Murari",
                "Brynjulf Owren",
                "Rodrigo T. Sato Mart\u00edn de Almagro",
                "Martina Stavole"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00644v1",
                "http://arxiv.org/pdf/2312.00644v1"
            ],
            "primary_category": "math.NA",
            "category": [
                "math.NA",
                "cs.NA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00855v1",
            "title": "Refine, Discriminate and Align: Stealing Encoders via Sample-Wise\n  Prototypes and Multi-Relational Extraction",
            "updated": "2023-12-01T15:03:29Z",
            "published": "2023-12-01T15:03:29Z",
            "summary": "This paper introduces RDA, a pioneering approach designed to address two\nprimary deficiencies prevalent in previous endeavors aiming at stealing\npre-trained encoders: (1) suboptimal performances attributed to biased\noptimization objectives, and (2) elevated query costs stemming from the\nend-to-end paradigm that necessitates querying the target encoder every epoch.\nSpecifically, we initially Refine the representations of the target encoder for\neach training sample, thereby establishing a less biased optimization objective\nbefore the steal-training phase. This is accomplished via a sample-wise\nprototype, which consolidates the target encoder's representations for a given\nsample's various perspectives. Demanding exponentially fewer queries compared\nto the end-to-end approach, prototypes can be instantiated to guide subsequent\nquery-free training. For more potent efficacy, we develop a multi-relational\nextraction loss that trains the surrogate encoder to Discriminate mismatched\nembedding-prototype pairs while Aligning those matched ones in terms of both\namplitude and angle. In this way, the trained surrogate encoder achieves\nstate-of-the-art results across the board in various downstream datasets with\nlimited queries. Moreover, RDA is shown to be robust to multiple widely-used\ndefenses.",
            "author": [
                "Shuchi Wu",
                "Chuan Ma",
                "Kang Wei",
                "Xiaogang Xu",
                "Ming Ding",
                "Yuwen Qian",
                "Tao Xiang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00855v1",
                "http://arxiv.org/pdf/2312.00855v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00640v1",
            "title": "One to beat them all: \"RYU'' -- a unifying framework for the\n  construction of safe balls",
            "updated": "2023-12-01T15:00:59Z",
            "published": "2023-12-01T15:00:59Z",
            "summary": "In this paper, we put forth a novel framework (named ``RYU'') for the\nconstruction of ``safe'' balls, i.e. regions that provably contain the dual\nsolution of a target optimization problem. We concentrate on the standard setup\nwhere the cost function is the sum of two terms: a closed, proper, convex\nLipschitz-smooth function and a closed, proper, convex function. The RYU\nframework is shown to generalize or improve upon all the results proposed in\nthe last decade for the considered family of optimization problems.",
            "author": [
                "Thu-Le Tran",
                "Cl\u00e9ment Elvira",
                "Hong-Phuong Dang",
                "C\u00e9dric Herzet"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00640v1",
                "http://arxiv.org/pdf/2312.00640v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00639v1",
            "title": "EvE: Exploiting Generative Priors for Radiance Field Enrichment",
            "updated": "2023-12-01T14:59:43Z",
            "published": "2023-12-01T14:59:43Z",
            "summary": "Modeling large-scale scenes from unconstrained image collections in-the-wild\nhas proven to be a major challenge in computer vision. Existing methods\ntackling in-the-wild neural rendering operate in a closed-world setting, where\nknowledge is limited to a scene's captured images within a training set. We\npropose EvE, which is, to the best of our knowledge, the first method\nleveraging generative priors to improve in-the-wild scene modeling. We employ\npre-trained generative networks to enrich K-Planes representations with\nextrinsic knowledge. To this end, we define an alternating training procedure\nto conduct optimization guidance of K-Planes trained on the training set. We\ncarry out extensive experiments and verify the merit of our method on synthetic\ndata as well as real tourism photo collections. EvE enhances rendered scenes\nwith richer details and outperforms the state of the art on the task of novel\nview synthesis in-the-wild. Our project page can be found at\nhttps://eve-nvs.github.io .",
            "author": [
                "Karim Kassab",
                "Antoine Schnepf",
                "Jean-Yves Franceschi",
                "Laurent Caraffa",
                "Jeremie Mary",
                "Val\u00e9rie Gouet-Brunet"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00639v1",
                "http://arxiv.org/pdf/2312.00639v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00854v1",
            "title": "A Probabilistic Neural Twin for Treatment Planning in Peripheral\n  Pulmonary Artery Stenosis",
            "updated": "2023-12-01T14:54:17Z",
            "published": "2023-12-01T14:54:17Z",
            "summary": "The substantial computational cost of high-fidelity models in numerical\nhemodynamics has, so far, relegated their use mainly to offline treatment\nplanning. New breakthroughs in data-driven architectures and optimization\ntechniques for fast surrogate modeling provide an exciting opportunity to\novercome these limitations, enabling the use of such technology for\ntime-critical decisions. We discuss an application to the repair of multiple\nstenosis in peripheral pulmonary artery disease through either transcatheter\npulmonary artery rehabilitation or surgery, where it is of interest to achieve\ndesired pressures and flows at specific locations in the pulmonary artery tree,\nwhile minimizing the risk for the patient. Since different degrees of success\ncan be achieved in practice during treatment, we formulate the problem in\nprobability, and solve it through a sample-based approach. We propose a new\noffline-online pipeline for probabilsitic real-time treatment planning which\ncombines offline assimilation of boundary conditions, model reduction, and\ntraining dataset generation with online estimation of marginal probabilities,\npossibly conditioned on the degree of augmentation observed in already repaired\nlesions. Moreover, we propose a new approach for the parametrization of\narbitrarily shaped vascular repairs through iterative corrections of a\nzero-dimensional approximant. We demonstrate this pipeline for a diseased model\nof the pulmonary artery tree available through the Vascular Model Repository.",
            "author": [
                "John D. Lee",
                "Jakob Richter",
                "Martin R. Pfaller",
                "Jason M. Szafron",
                "Karthik Menon",
                "Andrea Zanoni",
                "Michael R. Ma",
                "Jeffrey A. Feinstein",
                "Jacqueline Kreutzer",
                "Alison L. Marsden",
                "Daniele E. Schiavazzi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00854v1",
                "http://arxiv.org/pdf/2312.00854v1"
            ],
            "primary_category": "physics.med-ph",
            "category": [
                "physics.med-ph",
                "cs.AI",
                "cs.LG",
                "cs.NA",
                "math.NA",
                "stat.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00626v1",
            "title": "Forecasting Trends in Food Security: a Reservoir Computing Approach",
            "updated": "2023-12-01T14:42:37Z",
            "published": "2023-12-01T14:42:37Z",
            "summary": "Early warning systems are an essential tool for effective humanitarian\naction. Advance warnings on impending disasters facilitate timely and targeted\nresponse which help save lives, livelihoods, and scarce financial resources. In\nthis work we present a new quantitative methodology to forecast levels of food\nconsumption for 60 consecutive days, at the sub-national level, in four\ncountries: Mali, Nigeria, Syria, and Yemen. The methodology is built on\npublicly available data from the World Food Programme's integrated global\nhunger monitoring system which collects, processes, and displays daily updates\non key food security metrics, conflict, weather events, and other drivers of\nfood insecurity across 90 countries (https://hungermap.wfp.org/). In this\nstudy, we assessed the performance of various models including ARIMA, XGBoost,\nLSTMs, CNNs, and Reservoir Computing (RC), by comparing their Root Mean Squared\nError (RMSE) metrics. This comprehensive analysis spanned classical\nstatistical, machine learning, and deep learning approaches. Our findings\nhighlight Reservoir Computing as a particularly well-suited model in the field\nof food security given both its notable resistance to over-fitting on limited\ndata samples and its efficient training capabilities. The methodology we\nintroduce establishes the groundwork for a global, data-driven early warning\nsystem designed to anticipate and detect food insecurity.",
            "author": [
                "Joschka Herteux",
                "Christoph R\u00e4th",
                "Amine Baha",
                "Giulia Martini",
                "Duccio Piovani"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00626v1",
                "http://arxiv.org/pdf/2312.00626v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "physics.soc-ph",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00622v1",
            "title": "Practical Path-based Bayesian Optimization",
            "updated": "2023-12-01T14:39:11Z",
            "published": "2023-12-01T14:39:11Z",
            "summary": "There has been a surge in interest in data-driven experimental design with\napplications to chemical engineering and drug manufacturing. Bayesian\noptimization (BO) has proven to be adaptable to such cases, since we can model\nthe reactions of interest as expensive black-box functions. Sometimes, the cost\nof this black-box functions can be separated into two parts: (a) the cost of\nthe experiment itself, and (b) the cost of changing the input parameters. In\nthis short paper, we extend the SnAKe algorithm to deal with both types of\ncosts simultaneously. We further propose extensions to the case of a maximum\nallowable input change, as well as to the multi-objective setting.",
            "author": [
                "Jose Pablo Folch",
                "James Odgers",
                "Shiqiang Zhang",
                "Robert M Lee",
                "Behrang Shafei",
                "David Walz",
                "Calvin Tsay",
                "Mark van der Wilk",
                "Ruth Misener"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00622v1",
                "http://arxiv.org/pdf/2312.00622v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "math.OC",
                "stat.ME"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00852v1",
            "title": "Beyond First-Order Tweedie: Solving Inverse Problems using Latent\n  Diffusion",
            "updated": "2023-12-01T14:36:24Z",
            "published": "2023-12-01T14:36:24Z",
            "summary": "Sampling from the posterior distribution poses a major computational\nchallenge in solving inverse problems using latent diffusion models. Common\nmethods rely on Tweedie's first-order moments, which are known to induce a\nquality-limiting bias. Existing second-order approximations are impractical due\nto prohibitive computational costs, making standard reverse diffusion processes\nintractable for posterior sampling. This paper introduces Second-order Tweedie\nsampler from Surrogate Loss (STSL), a novel sampler that offers efficiency\ncomparable to first-order Tweedie with a tractable reverse process using\nsecond-order approximation. Our theoretical results reveal that the\nsecond-order approximation is lower bounded by our surrogate loss that only\nrequires $O(1)$ compute using the trace of the Hessian, and by the lower bound\nwe derive a new drift term to make the reverse process tractable. Our method\nsurpasses SoTA solvers PSLD and P2L, achieving 4X and 8X reduction in neural\nfunction evaluations, respectively, while notably enhancing sampling quality on\nFFHQ, ImageNet, and COCO benchmarks. In addition, we show STSL extends to\ntext-guided image editing and addresses residual distortions present from\ncorrupted images in leading text-guided image editing methods. To our best\nknowledge, this is the first work to offer an efficient second-order\napproximation in solving inverse problems using latent diffusion and editing\nreal-world images with corruptions.",
            "author": [
                "Litu Rout",
                "Yujia Chen",
                "Abhishek Kumar",
                "Constantine Caramanis",
                "Sanjay Shakkottai",
                "Wen-Sheng Chu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00852v1",
                "http://arxiv.org/pdf/2312.00852v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00616v1",
            "title": "Investigating a domain adaptation approach for integrating different\n  measurement instruments in a longitudinal clinical registry",
            "updated": "2023-12-01T14:28:37Z",
            "published": "2023-12-01T14:28:37Z",
            "summary": "In a longitudinal clinical registry, different measurement instruments might\nhave been used for assessing individuals at different time points. To combine\nthem, we investigate deep learning techniques for obtaining a joint latent\nrepresentation, to which the items of different measurement instruments are\nmapped. This corresponds to domain adaptation, an established concept in\ncomputer science for image data. Using the proposed approach as an example, we\nevaluate the potential of domain adaptation in a longitudinal cohort setting\nwith a rather small number of time points, motivated by an application with\ndifferent motor function measurement instruments in a registry of spinal\nmuscular atrophy (SMA) patients. There, we model trajectories in the latent\nrepresentation by ordinary differential equations (ODEs), where person-specific\nODE parameters are inferred from baseline characteristics. The goodness of fit\nand complexity of the ODE solutions then allows to judge the measurement\ninstrument mappings. We subsequently explore how alignment can be improved by\nincorporating corresponding penalty terms into model fitting. To systematically\ninvestigate the effect of differences between measurement instruments, we\nconsider several scenarios based on modified SMA data, including scenarios\nwhere a mapping should be feasible in principle and scenarios where no perfect\nmapping is available. While misalignment increases in more complex scenarios,\nsome structure is still recovered, even if the availability of measurement\ninstruments depends on patient state. A reasonable mapping is feasible also in\nthe more complex real SMA dataset. These results indicate that domain\nadaptation might be more generally useful in statistical modeling for\nlongitudinal registry data.",
            "author": [
                "Maren Hackenberg",
                "Michelle Pfaffenlehner",
                "Max Behrens",
                "Astrid Pechmann",
                "Janbernd Kirschner",
                "Harald Binder"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00616v1",
                "http://arxiv.org/pdf/2312.00616v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ME",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00601v1",
            "title": "Online Graph Coloring with Predictions",
            "updated": "2023-12-01T14:07:10Z",
            "published": "2023-12-01T14:07:10Z",
            "summary": "We introduce learning augmented algorithms to the online graph coloring\nproblem. Although the simple greedy algorithm FirstFit is known to perform\npoorly in the worst case, we are able to establish a relationship between the\nstructure of any input graph $G$ that is revealed online and the number of\ncolors that FirstFit uses for $G$. Based on this relationship, we propose an\nonline coloring algorithm FirstFitPredictions that extends FirstFit while\nmaking use of machine learned predictions. We show that FirstFitPredictions is\nboth \\emph{consistent} and \\emph{smooth}. Moreover, we develop a novel\nframework for combining online algorithms at runtime specifically for the\nonline graph coloring problem. Finally, we show how this framework can be used\nto robustify by combining it with any classical online coloring algorithm (that\ndisregards the predictions).",
            "author": [
                "Antonios Antoniadis",
                "Hajo Broersma",
                "Yang Meng"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00601v1",
                "http://arxiv.org/pdf/2312.00601v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00600v1",
            "title": "Improving Plasticity in Online Continual Learning via Collaborative\n  Learning",
            "updated": "2023-12-01T14:06:28Z",
            "published": "2023-12-01T14:06:28Z",
            "summary": "Online Continual Learning (CL) solves the problem of learning the\never-emerging new classification tasks from a continuous data stream. Unlike\nits offline counterpart, in online CL, the training data can only be seen once.\nMost existing online CL research regards catastrophic forgetting (i.e., model\nstability) as almost the only challenge. In this paper, we argue that the\nmodel's capability to acquire new knowledge (i.e., model plasticity) is another\nchallenge in online CL. While replay-based strategies have been shown to be\neffective in alleviating catastrophic forgetting, there is a notable gap in\nresearch attention toward improving model plasticity. To this end, we propose\nCollaborative Continual Learning (CCL), a collaborative learning based strategy\nto improve the model's capability in acquiring new concepts. Additionally, we\nintroduce Distillation Chain (DC), a novel collaborative learning scheme to\nboost the training of the models. We adapted CCL-DC to existing representative\nonline CL works. Extensive experiments demonstrate that even if the learners\nare well-trained with state-of-the-art online CL methods, our strategy can\nstill improve model plasticity dramatically, and thereby improve the overall\nperformance by a large margin.",
            "author": [
                "Maorong Wang",
                "Nicolas Michel",
                "Ling Xiao",
                "Toshihiko Yamasaki"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00600v1",
                "http://arxiv.org/pdf/2312.00600v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00598v1",
            "title": "Learning from One Continuous Video Stream",
            "updated": "2023-12-01T14:03:30Z",
            "published": "2023-12-01T14:03:30Z",
            "summary": "We introduce a framework for online learning from a single continuous video\nstream -- the way people and animals learn, without mini-batches, data\naugmentation or shuffling. This poses great challenges given the high\ncorrelation between consecutive video frames and there is very little prior\nwork on it. Our framework allows us to do a first deep dive into the topic and\nincludes a collection of streams and tasks composed from two existing video\ndatasets, plus methodology for performance evaluation that considers both\nadaptation and generalization. We employ pixel-to-pixel modelling as a\npractical and flexible way to switch between pre-training and single-stream\nevaluation as well as between arbitrary tasks, without ever requiring changes\nto models and always using the same pixel loss. Equipped with this framework we\nobtained large single-stream learning gains from pre-training with a novel\nfamily of future prediction tasks, found that momentum hurts, and that the pace\nof weight updates matters. The combination of these insights leads to matching\nthe performance of IID learning with batch size 1, when using the same\narchitecture and without costly replay buffers.",
            "author": [
                "Jo\u00e3o Carreira",
                "Michael King",
                "Viorica P\u0103tr\u0103ucean",
                "Dilara Gokay",
                "C\u0103t\u0103lin Ionescu",
                "Yi Yang",
                "Daniel Zoran",
                "Joseph Heyward",
                "Carl Doersch",
                "Yusuf Aytar",
                "Dima Damen",
                "Andrew Zisserman"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00598v1",
                "http://arxiv.org/pdf/2312.00598v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00596v1",
            "title": "BCN: Batch Channel Normalization for Image Classification",
            "updated": "2023-12-01T14:01:48Z",
            "published": "2023-12-01T14:01:48Z",
            "summary": "Normalization techniques have been widely used in the field of deep learning\ndue to their capability of enabling higher learning rates and are less careful\nin initialization. However, the effectiveness of popular normalization\ntechnologies is typically limited to specific areas. Unlike the standard Batch\nNormalization (BN) and Layer Normalization (LN), where BN computes the mean and\nvariance along the (N,H,W) dimensions and LN computes the mean and variance\nalong the (C,H,W) dimensions (N, C, H and W are the batch, channel, spatial\nheight and width dimension, respectively), this paper presents a novel\nnormalization technique called Batch Channel Normalization (BCN). To exploit\nboth the channel and batch dependence and adaptively and combine the advantages\nof BN and LN based on specific datasets or tasks, BCN separately normalizes\ninputs along the (N, H, W) and (C, H, W) axes, then combines the normalized\noutputs based on adaptive parameters. As a basic block, BCN can be easily\nintegrated into existing models for various applications in the field of\ncomputer vision. Empirical results show that the proposed technique can be\nseamlessly applied to various versions of CNN or Vision Transformer\narchitecture. The code is publicly available at\nhttps://github.com/AfifaKhaled/BatchChannel-Normalization",
            "author": [
                "Afifa Khaled",
                "Chao Li",
                "Jia Ning",
                "Kun He"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00596v1",
                "http://arxiv.org/pdf/2312.00596v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00592v1",
            "title": "Tracking Object Positions in Reinforcement Learning: A Metric for\n  Keypoint Detection (extended version)",
            "updated": "2023-12-01T13:56:28Z",
            "published": "2023-12-01T13:56:28Z",
            "summary": "Reinforcement learning (RL) for robot control typically requires a detailed\nrepresentation of the environment state, including information about\ntask-relevant objects not directly measurable. Keypoint detectors, such as\nspatial autoencoders (SAEs), are a common approach to extracting a\nlow-dimensional representation from high-dimensional image data. SAEs aim at\nspatial features such as object positions, which are often useful\nrepresentations in robotic RL. However, whether an SAE is actually able to\ntrack objects in the scene and thus yields a spatial state representation well\nsuited for RL tasks has rarely been examined due to a lack of established\nmetrics. In this paper, we propose to assess the performance of an SAE instance\nby measuring how well keypoints track ground truth objects in images. We\npresent a computationally lightweight metric and use it to evaluate common\nbaseline SAE architectures on image data from a simulated robot task. We find\nthat common SAEs differ substantially in their spatial extraction capability.\nFurthermore, we validate that SAEs that perform well in our metric achieve\nsuperior performance when used in downstream RL. Thus, our metric is an\neffective and lightweight indicator of RL performance before executing\nexpensive RL training. Building on these insights, we identify three key\nmodifications of SAE architectures to improve tracking performance. We make our\ncode available at anonymous.4open.science/r/sae-rl.",
            "author": [
                "Emma Cramer",
                "Jonas Reiher",
                "Sebastian Trimpe"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00592v1",
                "http://arxiv.org/pdf/2312.00592v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00591v1",
            "title": "Less is More: Learning Reference Knowledge Using No-Reference Image\n  Quality Assessment",
            "updated": "2023-12-01T13:56:01Z",
            "published": "2023-12-01T13:56:01Z",
            "summary": "Image Quality Assessment (IQA) with reference images have achieved great\nsuccess by imitating the human vision system, in which the image quality is\neffectively assessed by comparing the query image with its pristine reference\nimage. However, for the images in the wild, it is quite difficult to access\naccurate reference images. We argue that it is possible to learn reference\nknowledge under the No-Reference Image Quality Assessment (NR-IQA) setting,\nwhich is effective and efficient empirically. Concretely, by innovatively\nintroducing a novel feature distillation method in IQA, we propose a new\nframework to learn comparative knowledge from non-aligned reference images. And\nthen, to achieve fast convergence and avoid overfitting, we further propose an\ninductive bias regularization. Such a framework not only solves the congenital\ndefects of NR-IQA but also improves the feature extraction framework, enabling\nit to express more abundant quality information. Surprisingly, our method\nutilizes less input while obtaining a more significant improvement compared to\nthe teacher models. Extensive experiments on eight standard NR-IQA datasets\ndemonstrate the superior performance to the state-of-the-art NR-IQA methods,\ni.e., achieving the PLCC values of 0.917 (vs. 0.884 in LIVEC) and 0.686 (vs.\n0.661 in LIVEFB).",
            "author": [
                "Xudong Li",
                "Jingyuan Zheng",
                "Xiawu Zheng",
                "Runze Hu",
                "Enwei Zhang",
                "Yuting Gao",
                "Yunhang Shen",
                "Ke Li",
                "Yutao Liu",
                "Pingyang Dai",
                "Yan Zhang",
                "Rongrong Ji"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00591v1",
                "http://arxiv.org/pdf/2312.00591v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00586v1",
            "title": "Explainable Fraud Detection with Deep Symbolic Classification",
            "updated": "2023-12-01T13:50:55Z",
            "published": "2023-12-01T13:50:55Z",
            "summary": "There is a growing demand for explainable, transparent, and data-driven\nmodels within the domain of fraud detection. Decisions made by fraud detection\nmodels need to be explainable in the event of a customer dispute. Additionally,\nthe decision-making process in the model must be transparent to win the trust\nof regulators and business stakeholders. At the same time, fraud detection\nsolutions can benefit from data due to the noisy, dynamic nature of fraud and\nthe availability of large historical data sets. Finally, fraud detection is\nnotorious for its class imbalance: there are typically several orders of\nmagnitude more legitimate transactions than fraudulent ones. In this paper, we\npresent Deep Symbolic Classification (DSC), an extension of the Deep Symbolic\nRegression framework to classification problems. DSC casts classification as a\nsearch problem in the space of all analytic functions composed of a vocabulary\nof variables, constants, and operations and optimizes for an arbitrary\nevaluation metric directly. The search is guided by a deep neural network\ntrained with reinforcement learning. Because the functions are mathematical\nexpressions that are in closed-form and concise, the model is inherently\nexplainable both at the level of a single classification decision and the\nmodel's decision process. Furthermore, the class imbalance problem is\nsuccessfully addressed by optimizing for metrics that are robust to class\nimbalance such as the F1 score. This eliminates the need for oversampling and\nundersampling techniques that plague traditional approaches. Finally, the model\nallows to explicitly balance between the prediction accuracy and the\nexplainability. An evaluation on the PaySim data set demonstrates competitive\npredictive performance with state-of-the-art models, while surpassing them in\nterms of explainability. This establishes DSC as a promising model for fraud\ndetection systems.",
            "author": [
                "Samantha Visbeek",
                "Erman Acar",
                "Floris den Hengst"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00586v1",
                "http://arxiv.org/pdf/2312.00586v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00585v1",
            "title": "Adaptive Parameter-Free Robust Learning using Latent Bernoulli Variables",
            "updated": "2023-12-01T13:50:15Z",
            "published": "2023-12-01T13:50:15Z",
            "summary": "We present an efficient parameter-free approach for statistical learning from\ncorrupted training sets. We identify corrupted and non-corrupted samples using\nlatent Bernoulli variables, and therefore formulate the robust learning problem\nas maximization of the likelihood where latent variables are marginalized out.\nThe resulting optimization problem is solved via variational inference using an\nefficient Expectation-Maximization based method. The proposed approach improves\nover the state-of-the-art by automatically inferring the corruption level and\nidentifying outliers, while adding minimal computational overhead. We\ndemonstrate our robust learning method on a wide variety of machine learning\ntasks including online learning and deep learning where it exhibits ability to\nadapt to different levels of noise and attain high prediction accuracy.",
            "author": [
                "Aleksandr Karakulev",
                "Dave Zachariah",
                "Prashant Singh"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00585v1",
                "http://arxiv.org/pdf/2312.00585v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00584v1",
            "title": "The Ethics of Automating Legal Actors",
            "updated": "2023-12-01T13:48:46Z",
            "published": "2023-12-01T13:48:46Z",
            "summary": "The introduction of large public legal datasets has brought about a\nrenaissance in legal NLP. Many of these datasets are comprised of legal\njudgements - the product of judges deciding cases. This fact, together with the\nway machine learning works, means that several legal NLP models are models of\njudges. While some have argued for the automation of judges, in this position\npiece, we argue that automating the role of the judge raises difficult ethical\nchallenges, in particular for common law legal systems. Our argument follows\nfrom the social role of the judge in actively shaping the law, rather than\nmerely applying it. Since current NLP models come nowhere close to having the\nfacilities necessary for this task, they should not be used to automate judges.\nFurthermore, even in the case the models could achieve human-level\ncapabilities, there would still be remaining ethical concerns inherent in the\nautomation of the legal process.",
            "author": [
                "Josef Valvoda",
                "Alec Thompson",
                "Ryan Cotterell",
                "Simone Teufel"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00584v1",
                "http://arxiv.org/pdf/2312.00584v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00582v1",
            "title": "Design Patterns for Machine Learning Based Systems with\n  Human-in-the-Loop",
            "updated": "2023-12-01T13:46:38Z",
            "published": "2023-12-01T13:46:38Z",
            "summary": "The development and deployment of systems using supervised machine learning\n(ML) remain challenging: mainly due to the limited reliability of prediction\nmodels and the lack of knowledge on how to effectively integrate human\nintelligence into automated decision-making. Humans involvement in the ML\nprocess is a promising and powerful paradigm to overcome the limitations of\npure automated predictions and improve the applicability of ML in practice. We\ncompile a catalog of design patterns to guide developers select and implement\nsuitable human-in-the-loop (HiL) solutions. Our catalog takes into\nconsideration key requirements as the cost of human involvement and model\nretraining. It includes four training patterns, four deployment patterns, and\ntwo orthogonal cooperation patterns.",
            "author": [
                "Jakob Smedegaard Andersen",
                "Walid Maalej"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00582v1",
                "http://arxiv.org/pdf/2312.00582v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00581v1",
            "title": "Pathway to a fully data-driven geotechnics: lessons from materials\n  informatics",
            "updated": "2023-12-01T13:45:42Z",
            "published": "2023-12-01T13:45:42Z",
            "summary": "This paper elucidates the challenges and opportunities inherent in\nintegrating data-driven methodologies into geotechnics, drawing inspiration\nfrom the success of materials informatics. Highlighting the intricacies of soil\ncomplexity, heterogeneity, and the lack of comprehensive data, the discussion\nunderscores the pressing need for community-driven database initiatives and\nopen science movements. By leveraging the transformative power of deep\nlearning, particularly in feature extraction from high-dimensional data and the\npotential of transfer learning, we envision a paradigm shift towards a more\ncollaborative and innovative geotechnics field. The paper concludes with a\nforward-looking stance, emphasizing the revolutionary potential brought about\nby advanced computational tools like large language models in reshaping\ngeotechnics informatics.",
            "author": [
                "Stephen Wu",
                "Yu Otake",
                "Yosuke Higo",
                "Ikumasa Yoshida"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00581v1",
                "http://arxiv.org/pdf/2312.00581v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00851v1",
            "title": "Physics Inspired Criterion for Pruning-Quantization Joint Learning",
            "updated": "2023-12-01T13:25:16Z",
            "published": "2023-12-01T13:25:16Z",
            "summary": "Pruning-quantization joint learning always facilitates the deployment of deep\nneural networks (DNNs) on resource-constrained edge devices. However, most\nexisting methods do not jointly learn a global criterion for pruning and\nquantization in an interpretable way. In this paper, we propose a novel physics\ninspired criterion for pruning-quantization joint learning (PIC-PQ), which is\nexplored from an analogy we first draw between elasticity dynamics (ED) and\nmodel compression (MC). Specifically, derived from Hooke's law in ED, we\nestablish a linear relationship between the filters' importance distribution\nand the filter property (FP) by a learnable deformation scale in the physics\ninspired criterion (PIC). Furthermore, we extend PIC with a relative shift\nvariable for a global view. To ensure feasibility and flexibility, available\nmaximum bitwidth and penalty factor are introduced in quantization bitwidth\nassignment. Experiments on benchmarks of image classification demonstrate that\nPIC-PQ yields a good trade-off between accuracy and bit-operations (BOPs)\ncompression ratio e.g., 54.96X BOPs compression ratio in ResNet56 on CIFAR10\nwith 0.10% accuracy drop and 53.24X in ResNet18 on ImageNet with 0.61% accuracy\ndrop). The code will be available at https://github.com/fanxxxxyi/PIC-PQ.",
            "author": [
                "Weiying Xie",
                "Xiaoyi Fan",
                "Xin Zhang",
                "Yunsong Li",
                "Jie Lei",
                "Leyuan Fang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00851v1",
                "http://arxiv.org/pdf/2312.00851v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00561v1",
            "title": "Interior Point Constrained Reinforcement Learning with Global\n  Convergence Guarantees",
            "updated": "2023-12-01T13:16:39Z",
            "published": "2023-12-01T13:16:39Z",
            "summary": "We consider discounted infinite horizon constrained Markov decision processes\n(CMDPs) where the goal is to find an optimal policy that maximizes the expected\ncumulative reward subject to expected cumulative constraints. Motivated by the\napplication of CMDPs in online learning of safety-critical systems, we focus on\ndeveloping an algorithm that ensures constraint satisfaction during learning.\nTo this end, we develop a zeroth-order interior point approach based on the log\nbarrier function of the CMDP. Under the commonly assumed conditions of Fisher\nnon-degeneracy and bounded transfer error of the policy parameterization, we\nestablish the theoretical properties of the algorithm. In particular, in\ncontrast to existing CMDP approaches that ensure policy feasibility only upon\nconvergence, our algorithm guarantees feasibility of the policies during the\nlearning process and converges to the optimal policy with a sample complexity\nof $O(\\varepsilon^{-6})$. In comparison to the state-of-the-art policy\ngradient-based algorithm, C-NPG-PDA, our algorithm requires an additional\n$O(\\varepsilon^{-2})$ samples to ensure policy feasibility during learning with\nsame Fisher-non-degenerate parameterization.",
            "author": [
                "Tingting Ni",
                "Maryam Kamgarpour"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00561v1",
                "http://arxiv.org/pdf/2312.00561v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00558v2",
            "title": "Initial Results From the First Field Expedition of UAPx to Study\n  Unidentified Anomalous Phenomena",
            "updated": "2023-12-04T17:30:38Z",
            "published": "2023-12-01T13:13:50Z",
            "summary": "In July 2021, faculty from the UAlbany Department of Physics participated in\na week-long field expedition with the organization UAPx to collect data on UAPs\nin Avalon, California, located on Catalina Island, and nearby. This paper\nreviews both the hardware and software techniques which this collaboration\nemployed, and contains a frank discussion of the successes and failures, with a\nsection about how to apply lessons learned to future expeditions. Both\nobservable-light and infrared cameras were deployed, as well as sensors for\nother (non-EM) emissions. A pixel-subtraction method was augmented with other\nsimilarly simple methods to provide initial identification of objects in the\nsky and/or the sea crossing the cameras' fields of view. The first results will\nbe presented based upon approximately one hour in total of triggered\nvisible/night-vision-mode video and over 600 hours of untriggered (far) IR\nvideo recorded, as well as 55 hours of (background) radiation measurements.\nFollowing multiple explanatory resolutions of several ambiguities that were\npotentially anomalous at first, we focus on the primary remaining ambiguity\ncaptured at approximately 4am Pacific Time on Friday, July 16: a dark spot in\nthe visible/near-IR camera possibly coincident with ionizing radiation that has\nthus far resisted a prosaic explanation. We conclude with quantitative\nsuggestions for serious researchers in this still-nascent field of\nhard-science-based UAP studies, with an ultimate goal of identifying UAPs\nwithout confirmation bias toward either mundane or speculative conclusions.",
            "author": [
                "M. Szydagis",
                "K. H. Knuth",
                "B. W. Kugielsky",
                "C. Levy",
                "J. D. McGowan",
                "M. D. Phelan",
                "G. P. Voorhis Jr"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00558v2",
                "http://arxiv.org/pdf/2312.00558v2"
            ],
            "primary_category": "astro-ph.IM",
            "category": [
                "astro-ph.IM",
                "physics.ins-det",
                "physics.pop-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00553v1",
            "title": "A Spatio-Temporal Graph Convolutional Network for Gesture Recognition\n  from High-Density Electromyography",
            "updated": "2023-12-01T13:00:41Z",
            "published": "2023-12-01T13:00:41Z",
            "summary": "Accurate hand gesture prediction is crucial for effective upper-limb\nprosthetic limbs control. As the high flexibility and multiple degrees of\nfreedom exhibited by human hands, there has been a growing interest in\nintegrating deep networks with high-density surface electromyography (HD-sEMG)\ngrids to enhance gesture recognition capabilities. However, many existing\nmethods fall short in fully exploit the specific spatial topology and temporal\ndependencies present in HD-sEMG data. Additionally, these studies are often\nlimited number of gestures and lack generality. Hence, this study introduces a\nnovel gesture recognition method, named STGCN-GR, which leverages\nspatio-temporal graph convolution networks for HD-sEMG-based human-machine\ninterfaces. Firstly, we construct muscle networks based on functional\nconnectivity between channels, creating a graph representation of HD-sEMG\nrecordings. Subsequently, a temporal convolution module is applied to capture\nthe temporal dependences in the HD-sEMG series and a spatial graph convolution\nmodule is employed to effectively learn the intrinsic spatial topology\ninformation among distinct HD-sEMG channels. We evaluate our proposed model on\na public HD-sEMG dataset comprising a substantial number of gestures (i.e.,\n65). Our results demonstrate the remarkable capability of the STGCN-GR method,\nachieving an impressive accuracy of 91.07% in predicting gestures, which\nsurpasses state-of-the-art deep learning methods applied to the same dataset.",
            "author": [
                "Wenjuan Zhong",
                "Yuyang Zhang",
                "Peiwen Fu",
                "Wenxuan Xiong",
                "Mingming Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00553v1",
                "http://arxiv.org/pdf/2312.00553v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00552v1",
            "title": "Improving Unsupervised Relation Extraction by Augmenting Diverse\n  Sentence Pairs",
            "updated": "2023-12-01T12:59:32Z",
            "published": "2023-12-01T12:59:32Z",
            "summary": "Unsupervised relation extraction (URE) aims to extract relations between\nnamed entities from raw text without requiring manual annotations or\npre-existing knowledge bases. In recent studies of URE, researchers put a\nnotable emphasis on contrastive learning strategies for acquiring relation\nrepresentations. However, these studies often overlook two important aspects:\nthe inclusion of diverse positive pairs for contrastive learning and the\nexploration of appropriate loss functions. In this paper, we propose AugURE\nwith both within-sentence pairs augmentation and augmentation through\ncross-sentence pairs extraction to increase the diversity of positive pairs and\nstrengthen the discriminative power of contrastive learning. We also identify\nthe limitation of noise-contrastive estimation (NCE) loss for relation\nrepresentation learning and propose to apply margin loss for sentence pairs.\nExperiments on NYT-FB and TACRED datasets demonstrate that the proposed\nrelation representation learning and a simple K-Means clustering achieves\nstate-of-the-art performance.",
            "author": [
                "Qing Wang",
                "Kang Zhou",
                "Qiao Qiao",
                "Yuepei Li",
                "Qi Li"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00552v1",
                "http://arxiv.org/pdf/2312.00552v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00551v1",
            "title": "Identifying patterns and recommendations of and for sustainable open\n  data initiatives: a benchmarking-driven analysis of open government data\n  initiatives among European countries",
            "updated": "2023-12-01T12:58:17Z",
            "published": "2023-12-01T12:58:17Z",
            "summary": "Open government and open (government) data are seen as tools to create new\nopportunities, eliminate or at least reduce information inequalities and\nimprove public services. More than a decade of these efforts has provided much\nexperience, practices, and perspectives to learn how to better deal with them.\nThis paper focuses on benchmarking of open data initiatives over the years and\nattempts to identify patterns observed among European countries that could lead\nto disparities in the development, growth, and sustainability of open data\necosystems. To do this, we studied benchmarks and indices published over the\nlast years (57 editions of 8 artifacts) and conducted a comparative case study\nof eight European countries, identifying patterns among them considering\ndifferent potentially relevant contexts such as e-government, open government\ndata, open data indices and rankings, and others relevant for the country under\nconsideration. Using a Delphi method, we reached a consensus within a panel of\nexperts and validated a final list of 94 patterns, including their frequency of\noccurrence among studied countries and their effects on the respective\ncountries. Finally, we took a closer look at the developments in identified\ncontexts over the years and defined 21 recommendations for more resilient and\nsustainable open government data initiatives and ecosystems and future steps in\nthis area.",
            "author": [
                "Martin Lnenicka",
                "Anastasija Nikiforova",
                "Mariusz Luterek",
                "Petar Milic",
                "Daniel Rudmark",
                "Sebastian Neumaier",
                "Caterina Santoro",
                "Cesar Casiano Flores",
                "Marijn Janssen",
                "Manuel Pedro Rodr\u00edguez Bol\u00edvar"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00551v1",
                "http://arxiv.org/pdf/2312.00551v1"
            ],
            "primary_category": "econ.GN",
            "category": [
                "econ.GN",
                "q-fin.EC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00548v1",
            "title": "Domain Adaptive Imitation Learning with Visual Observation",
            "updated": "2023-12-01T12:48:41Z",
            "published": "2023-12-01T12:48:41Z",
            "summary": "In this paper, we consider domain-adaptive imitation learning with visual\nobservation, where an agent in a target domain learns to perform a task by\nobserving expert demonstrations in a source domain. Domain adaptive imitation\nlearning arises in practical scenarios where a robot, receiving visual sensory\ndata, needs to mimic movements by visually observing other robots from\ndifferent angles or observing robots of different shapes. To overcome the\ndomain shift in cross-domain imitation learning with visual observation, we\npropose a novel framework for extracting domain-independent behavioral features\nfrom input observations that can be used to train the learner, based on dual\nfeature extraction and image reconstruction. Empirical results demonstrate that\nour approach outperforms previous algorithms for imitation learning from visual\nobservation with domain shift.",
            "author": [
                "Sungho Choi",
                "Seungyul Han",
                "Woojun Kim",
                "Jongseong Chae",
                "Whiyoung Jung",
                "Youngchul Sung"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00548v1",
                "http://arxiv.org/pdf/2312.00548v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00540v1",
            "title": "Target-agnostic Source-free Domain Adaptation for Regression Tasks",
            "updated": "2023-12-01T12:35:18Z",
            "published": "2023-12-01T12:35:18Z",
            "summary": "Unsupervised domain adaptation (UDA) seeks to bridge the domain gap between\nthe target and source using unlabeled target data. Source-free UDA removes the\nrequirement for labeled source data at the target to preserve data privacy and\nstorage. However, work on source-free UDA assumes knowledge of domain gap\ndistribution, and hence is limited to either target-aware or classification\ntask. To overcome it, we propose TASFAR, a novel target-agnostic source-free\ndomain adaptation approach for regression tasks. Using prediction confidence,\nTASFAR estimates a label density map as the target label distribution, which is\nthen used to calibrate the source model on the target domain. We have conducted\nextensive experiments on four regression tasks with various domain gaps,\nnamely, pedestrian dead reckoning for different users, image-based people\ncounting in different scenes, housing-price prediction at different districts,\nand taxi-trip duration prediction from different departure points. TASFAR is\nshown to substantially outperform the state-of-the-art source-free UDA\napproaches by averagely reducing 22% errors for the four tasks and achieve\nnotably comparable accuracy as source-based UDA without using source data.",
            "author": [
                "Tianlang He",
                "Zhiqiu Xia",
                "Jierun Chen",
                "Haoliang Li",
                "S. -H. Gary Chan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00540v1",
                "http://arxiv.org/pdf/2312.00540v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00538v1",
            "title": "A Preconditioned Interior Point Method for Support Vector Machines Using\n  an ANOVA-Decomposition and NFFT-Based Matrix-Vector Products",
            "updated": "2023-12-01T12:27:11Z",
            "published": "2023-12-01T12:27:11Z",
            "summary": "In this paper we consider the numerical solution to the soft-margin support\nvector machine optimization problem. This problem is typically solved using the\nSMO algorithm, given the high computational complexity of traditional\noptimization algorithms when dealing with large-scale kernel matrices. In this\nwork, we propose employing an NFFT-accelerated matrix-vector product using an\nANOVA decomposition for the feature space that is used within an interior point\nmethod for the overall optimization problem. As this method requires the\nsolution of a linear system of saddle point form we suggest a preconditioning\napproach that is based on low-rank approximations of the kernel matrix together\nwith a Krylov subspace solver. We compare the accuracy of the ANOVA-based\nkernel with the default LIBSVM implementation. We investigate the performance\nof the different preconditioners as well as the accuracy of the ANOVA kernel on\nseveral large-scale datasets.",
            "author": [
                "Theresa Wagner",
                "John W. Pearson",
                "Martin Stoll"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00538v1",
                "http://arxiv.org/pdf/2312.00538v1"
            ],
            "primary_category": "math.NA",
            "category": [
                "math.NA",
                "cs.LG",
                "cs.NA",
                "math.OC",
                "05C50, 65F08, 65F10, 65T50, 90C20"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00536v1",
            "title": "Trained MT Metrics Learn to Cope with Machine-translated References",
            "updated": "2023-12-01T12:15:58Z",
            "published": "2023-12-01T12:15:58Z",
            "summary": "Neural metrics trained on human evaluations of MT tend to correlate well with\nhuman judgments, but their behavior is not fully understood. In this paper, we\nperform a controlled experiment and compare a baseline metric that has not been\ntrained on human evaluations (Prism) to a trained version of the same metric\n(Prism+FT). Surprisingly, we find that Prism+FT becomes more robust to\nmachine-translated references, which are a notorious problem in MT evaluation.\nThis suggests that the effects of metric training go beyond the intended effect\nof improving overall correlation with human judgments.",
            "author": [
                "Jannis Vamvas",
                "Tobias Domhan",
                "Sony Trenous",
                "Rico Sennrich",
                "Eva Hasler"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00536v1",
                "http://arxiv.org/pdf/2312.00536v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00535v1",
            "title": "RIS-Based On-the-Air Semantic Communications -- a Diffractional Deep\n  Neural Network Approach",
            "updated": "2023-12-01T12:15:49Z",
            "published": "2023-12-01T12:15:49Z",
            "summary": "Semantic communication has gained significant attention recently due to its\nadvantages in achieving higher transmission efficiency by focusing on semantic\ninformation instead of bit-level information. However, current AI-based\nsemantic communication methods require digital hardware for implementation.\nWith the rapid advancement on reconfigurable intelligence surfaces (RISs), a\nnew approach called on-the-air diffractional deep neural networks (D$^2$NN) can\nbe utilized to enable semantic communications on the wave domain. This paper\nproposes a new paradigm of RIS-based on-the-air semantic communications, where\nthe computational process occurs inherently as wireless signals pass through\nRISs. We present the system model and discuss the data and control flows of\nthis scheme, followed by a performance analysis using image transmission as an\nexample. In comparison to traditional hardware-based approaches, RIS-based\nsemantic communications offer appealing features, such as light-speed\ncomputation, low computational power requirements, and the ability to handle\nmultiple tasks simultaneously.",
            "author": [
                "Shuyi Chen",
                "Yingzhe Hui",
                "Yifan Qin",
                "Yueyi Yuan",
                "Weixiao Meng",
                "Xuewen Luo",
                "Hsiao-Hwa Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00535v1",
                "http://arxiv.org/pdf/2312.00535v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00532v1",
            "title": "DeepDR: Deep Structure-Aware RGB-D Inpainting for Diminished Reality",
            "updated": "2023-12-01T12:12:58Z",
            "published": "2023-12-01T12:12:58Z",
            "summary": "Diminished reality (DR) refers to the removal of real objects from the\nenvironment by virtually replacing them with their background. Modern DR\nframeworks use inpainting to hallucinate unobserved regions. While recent deep\nlearning-based inpainting is promising, the DR use case is complicated by the\nneed to generate coherent structure and 3D geometry (i.e., depth), in\nparticular for advanced applications, such as 3D scene editing. In this paper,\nwe propose DeepDR, a first RGB-D inpainting framework fulfilling all\nrequirements of DR: Plausible image and geometry inpainting with coherent\nstructure, running at real-time frame rates, with minimal temporal artifacts.\nOur structure-aware generative network allows us to explicitly condition color\nand depth outputs on the scene semantics, overcoming the difficulty of\nreconstructing sharp and consistent boundaries in regions with complex\nbackgrounds. Experimental results show that the proposed framework can\noutperform related work qualitatively and quantitatively.",
            "author": [
                "Christina Gsaxner",
                "Shohei Mori",
                "Dieter Schmalstieg",
                "Jan Egger",
                "Gerhard Paar",
                "Werner Bailer",
                "Denis Kalkofen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00532v1",
                "http://arxiv.org/pdf/2312.00532v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00529v1",
            "title": "Algorithm-based diagnostic application for diabetic retinopathy\n  detection",
            "updated": "2023-12-01T12:09:06Z",
            "published": "2023-12-01T12:09:06Z",
            "summary": "Diabetic retinopathy (DR) is a growing health problem worldwide and is a\nleading cause of visual impairment and blindness, especially among working\npeople aged 20-65. Its incidence is increasing along with the number of\ndiabetes cases, and it is more common in developed countries than in developing\ncountries. Recent research in the field of diabetic retinopathy diagnosis is\nusing advanced technologies, such as analysis of images obtained by\nophthalmoscopy. Automatic methods for analyzing eye images based on neural\nnetworks, deep learning and image analysis algorithms can improve the\nefficiency of diagnosis. This paper describes an automatic DR diagnosis method\nthat includes processing and analysis of ophthalmoscopic images of the eye. It\nuses morphological algorithms to identify the optic disc and lesions\ncharacteristic of DR, such as microaneurysms, hemorrhages and exudates.\nAutomated DR diagnosis has the potential to improve the efficiency of early\ndetection of this disease and contribute to reducing the number of cases of\ndiabetes-related visual impairment. The final step was to create an application\nwith a graphical user interface that allowed retinal images taken at\ncooperating ophthalmology offices to be uploaded to the server. These images\nwere then analyzed using a developed algorithm to make a diagnosis.",
            "author": [
                "Agnieszka Cisek",
                "Karolina Korycinska",
                "Leszek Pyziak",
                "Marzena Malicka",
                "Tomasz Wiecek",
                "Grzegorz Gruzel",
                "Kamil Szmuc",
                "Jozef Cebulski",
                "Mariusz Spyra"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00529v1",
                "http://arxiv.org/pdf/2312.00529v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV",
                "q-bio.TO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00517v1",
            "title": "Causal propensity as an antecedent of entrepreneurial intentions in\n  tourism students",
            "updated": "2023-12-01T11:46:22Z",
            "published": "2023-12-01T11:46:22Z",
            "summary": "The tourism sector is a sector with many opportunities for business\ndevelopment. Entrepreneurship in this sector promotes economic growth and job\ncreation. Knowing how entrepreneurial intention develops facilitates its\ntransformation into entrepreneurial behaviour. Entrepreneurial behaviour can\nadopt a causal logic, an effectual logic or a combination of both. Considering\nthe causal logic, decision-making is done through prediction. In this way,\nentrepreneurs try to increase their market share by planning strategies and\nanalysing possible deviations from their plans. Previous literature studies\ncausal entrepreneurial behaviour, as well as variables such as creative\ninnovation, proactive decisions and entrepreneurship training when the\nentrepreneur has already created his or her firm. However, there is an obvious\ngap at a stage prior to the start of entrepreneurial activity when the\nentrepreneurial intention is formed. This paper analyses how creativity,\nproactivity, entrepreneurship education and the propensity for causal behaviour\ninfluence entrepreneurial intentions. To achieve the research objective, we\nanalysed a sample of 464 undergraduate tourism students from two universities\nin southern Spain. We used SmartPLS 3 software to apply a structural equation\nmethodology to the measurement model composed of nine hypotheses. The results\nshow, among other relationships, that causal propensity, entrepreneurship\nlearning programmes and proactivity are antecedents of entrepreneurial\nintentions. These findings have implications for theory, as they fill a gap in\nthe field of entrepreneurial intentions. Considering propensity towards causal\nbehaviour before setting up the firm is unprecedented. Furthermore, the results\nof this study have practical implications for the design of public education\npolicies and the promotion of business creation in the tourism sector.",
            "author": [
                "Alicia Martin-Navarro",
                "Felix Velicia-Martin",
                "Jose Aurelio Medina-Garrido",
                "Ricardo Gouveia Rodrigues"
            ],
            "link": [
                "http://dx.doi.org/10.1007/s11365-022-00826-1",
                "http://arxiv.org/abs/2312.00517v1",
                "http://arxiv.org/pdf/2312.00517v1"
            ],
            "primary_category": "econ.GN",
            "category": [
                "econ.GN",
                "q-fin.EC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00516v1",
            "title": "Spatio-Temporal-Decoupled Masked Pre-training for Traffic Forecasting",
            "updated": "2023-12-01T11:43:49Z",
            "published": "2023-12-01T11:43:49Z",
            "summary": "Accurate forecasting of multivariate traffic flow time series remains\nchallenging due to substantial spatio-temporal heterogeneity and complex\nlong-range correlative patterns. To address this, we propose\nSpatio-Temporal-Decoupled Masked Pre-training (STD-MAE), a novel framework that\nemploys masked autoencoders to learn and encode complex spatio-temporal\ndependencies via pre-training. Specifically, we use two decoupled masked\nautoencoders to reconstruct the traffic data along spatial and temporal axes\nusing a self-supervised pre-training approach. These mask reconstruction\nmechanisms capture the long-range correlations in space and time separately.\nThe learned hidden representations are then used to augment the downstream\nspatio-temporal traffic predictor. A series of quantitative and qualitative\nevaluations on four widely-used traffic benchmarks (PEMS03, PEMS04, PEMS07, and\nPEMS08) are conducted to verify the state-of-the-art performance, with STD-MAE\nexplicitly enhancing the downstream spatio-temporal models' ability to capture\nlong-range intricate spatial and temporal patterns. Codes are available at\nhttps://github.com/Jimmy-7664/STD_MAE.",
            "author": [
                "Haotian Gao",
                "Renhe Jiang",
                "Zheng Dong",
                "Jinliang Deng",
                "Xuan Song"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00516v1",
                "http://arxiv.org/pdf/2312.00516v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00513v1",
            "title": "Summarization-based Data Augmentation for Document Classification",
            "updated": "2023-12-01T11:34:37Z",
            "published": "2023-12-01T11:34:37Z",
            "summary": "Despite the prevalence of pretrained language models in natural language\nunderstanding tasks, understanding lengthy text such as document is still\nchallenging due to the data sparseness problem. Inspired by that humans develop\ntheir ability of understanding lengthy text from reading shorter text, we\npropose a simple yet effective summarization-based data augmentation, SUMMaug,\nfor document classification. We first obtain easy-to-learn examples for the\ntarget document classification task by summarizing the input of the original\ntraining examples, while optionally merging the original labels to conform to\nthe summarized input. We then use the generated pseudo examples to perform\ncurriculum learning. Experimental results on two datasets confirmed the\nadvantage of our method compared to existing baseline methods in terms of\nrobustness and accuracy. We release our code and data at\nhttps://github.com/etsurin/summaug.",
            "author": [
                "Yueguan Wang",
                "Naoki Yoshinaga"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00513v1",
                "http://arxiv.org/pdf/2312.00513v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00512v1",
            "title": "Attack Detection Using Item Vector Shift in Matrix Factorisation\n  Recommenders",
            "updated": "2023-12-01T11:34:01Z",
            "published": "2023-12-01T11:34:01Z",
            "summary": "This paper proposes a novel method for detecting shilling attacks in Matrix\nFactorization (MF)-based Recommender Systems (RS), in which attackers use false\nuser-item feedback to promote a specific item. Unlike existing methods that use\neither use supervised learning to distinguish between attack and genuine\nprofiles or analyse target item rating distributions to detect false ratings,\nour method uses an unsupervised technique to detect false ratings by examining\nshifts in item preference vectors that exploit rating deviations and user\ncharacteristics, making it a promising new direction. The experimental results\ndemonstrate the effectiveness of our approach in various attack scenarios,\nincluding those involving obfuscation techniques.",
            "author": [
                "Sulthana Shams",
                "Douglas Leith"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00512v1",
                "http://arxiv.org/pdf/2312.00512v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00509v1",
            "title": "Bayesian causal discovery from unknown general interventions",
            "updated": "2023-12-01T11:30:51Z",
            "published": "2023-12-01T11:30:51Z",
            "summary": "We consider the problem of learning causal Directed Acyclic Graphs (DAGs)\nusing combinations of observational and interventional experimental data.\nCurrent methods tailored to this setting assume that interventions either\ndestroy parent-child relations of the intervened (target) nodes or only alter\nsuch relations without modifying the parent sets, even when the intervention\ntargets are unknown. We relax this assumption by proposing a Bayesian method\nfor causal discovery from general interventions, which allow for modifications\nof the parent sets of the unknown targets. Even in this framework, DAGs and\ngeneral interventions may be identifiable only up to some equivalence classes.\nWe provide graphical characterizations of such interventional Markov\nequivalence and devise compatible priors for Bayesian inference that guarantee\nscore equivalence of indistinguishable structures. We then develop a Markov\nChain Monte Carlo (MCMC) scheme to approximate the posterior distribution over\nDAGs, intervention targets and induced parent sets. Finally, we evaluate the\nproposed methodology on both simulated and real protein expression data.",
            "author": [
                "Alessandro Mascaro",
                "Federico Castelletti"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00509v1",
                "http://arxiv.org/pdf/2312.00509v1"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME",
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00508v2",
            "title": "PyraTrans: Attention-Enriched Pyramid Transformer for Malicious URL\n  Detection",
            "updated": "2023-12-06T16:46:54Z",
            "published": "2023-12-01T11:27:00Z",
            "summary": "Although advancements in machine learning have driven the development of\nmalicious URL detection technology, current techniques still face significant\nchallenges in their capacity to generalize and their resilience against\nevolving threats. In this paper, we propose PyraTrans, a novel method that\nintegrates pretrained Transformers with pyramid feature learning to detect\nmalicious URL. PyraTrans utilizes a pretrained CharBERT as its foundation and\nis augmented with three interconnected feature modules: 1) Encoder Feature\nExtraction, extracting multi-order feature matrices from each CharBERT encoder\nlayer; 2) Multi-Scale Feature Learning, capturing local contextual insights at\nvarious scales and aggregating information across encoder layers; and 3)\nSpatial Pyramid Attention, focusing on regional-level attention to emphasize\nareas rich in expressive information. The proposed approach addresses the\nlimitations of the Transformer in local feature learning and regional\nrelational awareness, which are vital for capturing URL-specific word patterns,\ncharacter combinations, or structural anomalies. In several challenging\nexperimental scenarios, the proposed method has shown significant improvements\nin accuracy, generalization, and robustness in malicious URL detection. For\ninstance, it achieved a peak F1-score improvement of 40% in class-imbalanced\nscenarios, and exceeded the best baseline result by 14.13% in accuracy in\nadversarial attack scenarios. Additionally, we conduct a case study where our\nmethod accurately identifies all 30 active malicious web pages, whereas two\npior SOTA methods miss 4 and 7 malicious web pages respectively. Codes and data\nare available at:https://github.com/Alixyvtte/PyraTrans.",
            "author": [
                "Ruitong Liu",
                "Yanbin Wang",
                "Zhenhao Guo",
                "Haitao Xu",
                "Zhan Qin",
                "Wenrui Ma",
                "Fan Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00508v2",
                "http://arxiv.org/pdf/2312.00508v2"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00507v1",
            "title": "VEXIR2Vec: An Architecture-Neutral Embedding Framework for Binary\n  Similarity",
            "updated": "2023-12-01T11:22:10Z",
            "published": "2023-12-01T11:22:10Z",
            "summary": "We propose VEXIR2Vec, a code embedding framework for finding similar\nfunctions in binaries. Our representations rely on VEX IR, the intermediate\nrepresentation used by binary analysis tools like Valgrind and angr. Our\nproposed embeddings encode both syntactic and semantic information to represent\na function, and is both application and architecture independent. We also\npropose POV, a custom Peephole Optimization engine that normalizes the VEX IR\nfor effective similarity analysis. We design several optimizations like\ncopy/constant propagation, constant folding, common subexpression elimination\nand load-store elimination in POV.\n  We evaluate our framework on two experiments -- diffing and searching --\ninvolving binaries targeting different architectures, compiled using different\ncompilers and versions, optimization sequences, and obfuscations. We show\nresults on several standard projects and on real-world vulnerabilities. Our\nresults show that VEXIR2Vec achieves superior precision and recall values\ncompared to the state-of-the-art works. Our framework is highly scalable and is\nbuilt as a multi-threaded, parallel library by only using open-source tools.\nVEXIR2Vec achieves about $3.2 \\times$ speedup on the closest competitor, and\norders-of-magnitude speedup on other tools.",
            "author": [
                "S. VenkataKeerthy",
                "Yashas Andaluri",
                "Sayan Dey",
                "Soumya Banerjee",
                "Ramakrishna Upadrasta"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00507v1",
                "http://arxiv.org/pdf/2312.00507v1"
            ],
            "primary_category": "cs.PL",
            "category": [
                "cs.PL",
                "cs.CR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00505v1",
            "title": "Machine learning approaches for parameter reweighting in MC samples of\n  top quark production in CMS",
            "updated": "2023-12-01T11:09:57Z",
            "published": "2023-12-01T11:09:57Z",
            "summary": "In particle physics, Monte Carlo (MC) event generators are needed to compare\ntheory to the measured data. Many MC samples have to be generated to account\nfor theoretical systematic uncertainties, at a significant computational cost.\nTherefore, the MC statistic becomes a limiting factor for most measurements and\nthe significant computational cost of these programs a bottleneck in most\nphysics analyses. In this contribution, the Deep neural network using\nClassification for Tuning and Reweighting (DCTR) approach is evaluated for the\nreweighting of two systematic uncertainties in MC simulations of top quark pair\nproduction within the CMS experiment. DCTR is a method, based on a Deep Neural\nNetwork (DNN) technique, to reweight simulations to different model parameters\nby using the full kinematic information in the event. This methodology avoids\nthe need for simulating the detector response multiple times by incorporating\nthe relevant variations in a single sample.",
            "author": [
                "Valentina Guglielmi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00505v1",
                "http://arxiv.org/pdf/2312.00505v1"
            ],
            "primary_category": "hep-ex",
            "category": [
                "hep-ex"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00502v1",
            "title": "On the Out-Of-Distribution Robustness of Self-Supervised Representation\n  Learning for Phonocardiogram Signals",
            "updated": "2023-12-01T11:06:00Z",
            "published": "2023-12-01T11:06:00Z",
            "summary": "Objective: Despite the recent increase in research activity, deep-learning\nmodels have not yet been widely accepted in medicine. The shortage of\nhigh-quality annotated data often hinders the development of robust and\ngeneralizable models, which do not suffer from degraded effectiveness when\npresented with newly-collected, out-of-distribution (OOD) datasets. Methods:\nContrastive Self-Supervised Learning (SSL) offers a potential solution to the\nscarcity of labeled data as it takes advantage of unlabeled data to increase\nmodel effectiveness and robustness. In this research, we propose applying\ncontrastive SSL for detecting abnormalities in phonocardiogram (PCG) samples by\nlearning a generalized representation of the signal. Specifically, we perform\nan extensive comparative evaluation of a wide range of audio-based\naugmentations and evaluate trained classifiers on multiple datasets across\ndifferent downstream tasks. Results: We experimentally demonstrate that,\ndepending on its training distribution, the effectiveness of a fully-supervised\nmodel can degrade up to 32% when evaluated on unseen data, while SSL models\nonly lose up to 10% or even improve in some cases. Conclusions: Contrastive SSL\npretraining can assist in providing robust classifiers which can generalize to\nunseen, OOD data, without relying on time- and labor-intensive annotation\nprocesses by medical experts. Furthermore, the proposed extensive evaluation\nprotocol sheds light on the most promising and appropriate augmentations for\nrobust PCG signal processing. Significance: We provide researchers and\npractitioners with a roadmap towards producing robust models for PCG\nclassification, in addition to an open-source codebase for developing novel\napproaches.",
            "author": [
                "Aristotelis Ballas",
                "Vasileios Papapanagiotou",
                "Christos Diou"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00502v1",
                "http://arxiv.org/pdf/2312.00502v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.SD",
                "q-bio.QM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00500v1",
            "title": "Global Localization: Utilizing Relative Spatio-Temporal Geometric\n  Constraints from Adjacent and Distant Cameras",
            "updated": "2023-12-01T11:03:07Z",
            "published": "2023-12-01T11:03:07Z",
            "summary": "Re-localizing a camera from a single image in a previously mapped area is\nvital for many computer vision applications in robotics and augmented/virtual\nreality. In this work, we address the problem of estimating the 6 DoF camera\npose relative to a global frame from a single image. We propose to leverage a\nnovel network of relative spatial and temporal geometric constraints to guide\nthe training of a Deep Network for localization. We employ simultaneously\nspatial and temporal relative pose constraints that are obtained not only from\nadjacent camera frames but also from camera frames that are distant in the\nspatio-temporal space of the scene. We show that our method, through these\nconstraints, is capable of learning to localize when little or very sparse\nground-truth 3D coordinates are available. In our experiments, this is less\nthan 1% of available ground-truth data. We evaluate our method on 3 common\nvisual localization datasets and show that it outperforms other direct pose\nestimation methods.",
            "author": [
                "Mohammad Altillawi",
                "Zador Pataki",
                "Shile Li",
                "Ziyuan Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00500v1",
                "http://arxiv.org/pdf/2312.00500v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00487v1",
            "title": "Explainable AI in Diagnosing and Anticipating Leukemia Using Transfer\n  Learning Method",
            "updated": "2023-12-01T10:37:02Z",
            "published": "2023-12-01T10:37:02Z",
            "summary": "This research paper focuses on Acute Lymphoblastic Leukemia (ALL), a form of\nblood cancer prevalent in children and teenagers, characterized by the rapid\nproliferation of immature white blood cells (WBCs). These atypical cells can\noverwhelm healthy cells, leading to severe health consequences. Early and\naccurate detection of ALL is vital for effective treatment and improving\nsurvival rates. Traditional diagnostic methods are time-consuming, costly, and\nprone to errors. The paper proposes an automated detection approach using\ncomputer-aided diagnostic (CAD) models, leveraging deep learning techniques to\nenhance the accuracy and efficiency of leukemia diagnosis. The study utilizes\nvarious transfer learning models like ResNet101V2, VGG19, InceptionV3, and\nInceptionResNetV2 for classifying ALL. The methodology includes using the Local\nInterpretable Model-Agnostic Explanations (LIME) for ensuring the validity and\nreliability of the AI system's predictions. This approach is critical for\novercoming the \"black box\" nature of AI, where decisions made by models are\noften opaque and unaccountable. The paper highlights that the proposed method\nusing the InceptionV3 model achieved an impressive 98.38% accuracy,\noutperforming other tested models. The results, verified by the LIME algorithm,\nshowcase the potential of this method in accurately identifying ALL, providing\na valuable tool for medical practitioners. The research underscores the impact\nof explainable artificial intelligence (XAI) in medical diagnostics, paving the\nway for more transparent and trustworthy AI applications in healthcare.",
            "author": [
                "Wahidul Hasan Abir",
                "Md. Fahim Uddin",
                "Faria Rahman Khanam",
                "Mohammad Monirujjaman Khan"
            ],
            "link": [
                "http://dx.doi.org/10.1155/2022/5140148",
                "http://arxiv.org/abs/2312.00487v1",
                "http://arxiv.org/pdf/2312.00487v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00486v1",
            "title": "REDUCR: Robust Data Downsampling Using Class Priority Reweighting",
            "updated": "2023-12-01T10:34:22Z",
            "published": "2023-12-01T10:34:22Z",
            "summary": "Modern machine learning models are becoming increasingly expensive to train\nfor real-world image and text classification tasks, where massive web-scale\ndata is collected in a streaming fashion. To reduce the training cost, online\nbatch selection techniques have been developed to choose the most informative\ndatapoints. However, these techniques can suffer from poor worst-class\ngeneralization performance due to class imbalance and distributional shifts.\nThis work introduces REDUCR, a robust and efficient data downsampling method\nthat uses class priority reweighting. REDUCR reduces the training data while\npreserving worst-class generalization performance. REDUCR assigns priority\nweights to datapoints in a class-aware manner using an online learning\nalgorithm. We demonstrate the data efficiency and robust performance of REDUCR\non vision and text classification tasks. On web-scraped datasets with\nimbalanced class distributions, REDUCR significantly improves worst-class test\naccuracy (and average accuracy), surpassing state-of-the-art methods by around\n15%.",
            "author": [
                "William Bankes",
                "George Hughes",
                "Ilija Bogunovic",
                "Zi Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00486v1",
                "http://arxiv.org/pdf/2312.00486v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00485v1",
            "title": "Backbone-based Dynamic Graph Spatio-Temporal Network for Epidemic\n  Forecasting",
            "updated": "2023-12-01T10:34:03Z",
            "published": "2023-12-01T10:34:03Z",
            "summary": "Accurate epidemic forecasting is a critical task in controlling disease\ntransmission. Many deep learning-based models focus only on static or dynamic\ngraphs when constructing spatial information, ignoring their relationship.\nAdditionally, these models often rely on recurrent structures, which can lead\nto error accumulation and computational time consumption. To address the\naforementioned problems, we propose a novel model called Backbone-based Dynamic\nGraph Spatio-Temporal Network (BDGSTN). Intuitively, the continuous and smooth\nchanges in graph structure, make adjacent graph structures share a basic\npattern. To capture this property, we use adaptive methods to generate static\nbackbone graphs containing the primary information and temporal models to\ngenerate dynamic temporal graphs of epidemic data, fusing them to generate a\nbackbone-based dynamic graph. To overcome potential limitations associated with\nrecurrent structures, we introduce a linear model DLinear to handle temporal\ndependencies and combine it with dynamic graph convolution for epidemic\nforecasting. Extensive experiments on two datasets demonstrate that BDGSTN\noutperforms baseline models and ablation comparison further verifies the\neffectiveness of model components. Furthermore, we analyze and measure the\nsignificance of backbone and temporal graphs by using information metrics from\ndifferent aspects. Finally, we compare model parameter volume and training time\nto confirm the superior complexity and efficiency of BDGSTN.",
            "author": [
                "Junkai Mao",
                "Yuexing Han",
                "Gouhei Tanaka",
                "Bing Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00485v1",
                "http://arxiv.org/pdf/2312.00485v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "q-bio.PE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00484v1",
            "title": "MultiView Independent Component Analysis with Delays",
            "updated": "2023-12-01T10:33:16Z",
            "published": "2023-12-01T10:33:16Z",
            "summary": "Linear Independent Component Analysis (ICA) is a blind source separation\ntechnique that has been used in various domains to identify independent latent\nsources from observed signals. In order to obtain a higher signal-to-noise\nratio, the presence of multiple views of the same sources can be used. In this\nwork, we present MultiView Independent Component Analysis with Delays (MVICAD).\nThis algorithm builds on the MultiView ICA model by allowing sources to be\ndelayed versions of some shared sources: sources are shared across views up to\nsome unknown latencies that are view- and source-specific. Using simulations,\nwe demonstrate that MVICAD leads to better unmixing of the sources. Moreover,\nas ICA is often used in neuroscience, we show that latencies are age-related\nwhen applied to Cam-CAN, a large-scale magnetoencephalography (MEG) dataset.\nThese results demonstrate that the MVICAD model can reveal rich effects on\nneural signals without human supervision.",
            "author": [
                "Ambroise Heurtebise",
                "Pierre Ablin",
                "Alexandre Gramfort"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00484v1",
                "http://arxiv.org/pdf/2312.00484v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00483v1",
            "title": "MalDicom: A Memory Forensic Framework for Detecting Malicious Payload in\n  DICOM Files",
            "updated": "2023-12-01T10:33:15Z",
            "published": "2023-12-01T10:33:15Z",
            "summary": "Digital Imaging and Communication System (DICOM) is widely used throughout\nthe public health sector for portability in medical imaging. However, these\nDICOM files have vulnerabilities present in the preamble section. Successful\nexploitation of these vulnerabilities can allow attackers to embed executable\ncodes in the 128-Byte preamble of DICOM files. Embedding the malicious\nexecutable will not interfere with the readability or functionality of DICOM\nimagery. However, it will affect the underline system silently upon viewing\nthese files. This paper shows the infiltration of Windows malware executables\ninto DICOM files. On viewing the files, the malicious DICOM will get executed\nand eventually infect the entire hospital network through the radiologist's\nworkstation. The code injection process of executing malware in DICOM files\naffects the hospital networks and workstations' memory. Memory forensics for\nthe infected radiologist's workstation is crucial as it can detect which\nmalware disrupts the hospital environment, and future detection methods can be\ndeployed. In this paper, we consider the machine learning (ML) algorithms to\nconduct memory forensics on three memory dump categories: Trojan, Spyware, and\nRansomware, taken from the CIC-MalMem-2022 dataset. We obtain the highest\naccuracy of 75\\% with the Random Forest model. For estimating the feature\nimportance for ML model prediction, we leveraged the concept of Shapley values.",
            "author": [
                "Ayushi Mishra",
                "Priyanka Bagade"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00483v1",
                "http://arxiv.org/pdf/2312.00483v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00477v1",
            "title": "Interpretable Meta-Learning of Physical Systems",
            "updated": "2023-12-01T10:18:50Z",
            "published": "2023-12-01T10:18:50Z",
            "summary": "Machine learning methods can be a valuable aid in the scientific process, but\nthey need to face challenging settings where data come from inhomogeneous\nexperimental conditions. Recent meta-learning methods have made significant\nprogress in multi-task learning, but they rely on black-box neural networks,\nresulting in high computational costs and limited interpretability. Leveraging\nthe structure of the learning problem, we argue that multi-environment\ngeneralization can be achieved using a simpler learning model, with an affine\nstructure with respect to the learning task. Crucially, we prove that this\narchitecture can identify the physical parameters of the system, enabling\ninterpreable learning. We demonstrate the competitive generalization\nperformance and the low computational cost of our method by comparing it to\nstate-of-the-art algorithms on physical systems, ranging from toy models to\ncomplex, non-analytical systems. The interpretability of our method is\nillustrated with original applications to physical-parameter-induced adaptation\nand to adaptive control.",
            "author": [
                "Matthieu Blanke",
                "Marc Lelarge"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00477v1",
                "http://arxiv.org/pdf/2312.00477v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00476v1",
            "title": "Self-Supervised Learning of Spatial Acoustic Representation with\n  Cross-Channel Signal Reconstruction and Multi-Channel Conformer",
            "updated": "2023-12-01T10:16:02Z",
            "published": "2023-12-01T10:16:02Z",
            "summary": "Supervised learning methods have shown effectiveness in estimating spatial\nacoustic parameters such as time difference of arrival, direct-to-reverberant\nratio and reverberation time. However, they still suffer from the\nsimulation-to-reality generalization problem due to the mismatch between\nsimulated and real-world acoustic characteristics and the deficiency of\nannotated real-world data. To this end, this work proposes a self-supervised\nmethod that takes full advantage of unlabeled data for spatial acoustic\nparameter estimation. First, a new pretext task, i.e. cross-channel signal\nreconstruction (CCSR), is designed to learn a universal spatial acoustic\nrepresentation from unlabeled multi-channel microphone signals. We mask partial\nsignals of one channel and ask the model to reconstruct them, which makes it\npossible to learn spatial acoustic information from unmasked signals and\nextract source information from the other microphone channel. An\nencoder-decoder structure is used to disentangle the two kinds of information.\nBy fine-tuning the pre-trained spatial encoder with a small annotated dataset,\nthis encoder can be used to estimate spatial acoustic parameters. Second, a\nnovel multi-channel audio Conformer (MC-Conformer) is adopted as the encoder\nmodel architecture, which is suitable for both the pretext and downstream\ntasks. It is carefully designed to be able to capture the local and global\ncharacteristics of spatial acoustics exhibited in the time-frequency domain.\nExperimental results of five acoustic parameter estimation tasks on both\nsimulated and real-world data show the effectiveness of the proposed method. To\nthe best of our knowledge, this is the first self-supervised learning method in\nthe field of spatial acoustic representation learning and multi-channel audio\nsignal processing.",
            "author": [
                "Bing Yang",
                "Xiaofei Li"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00476v1",
                "http://arxiv.org/pdf/2312.00476v1"
            ],
            "primary_category": "cs.SD",
            "category": [
                "cs.SD",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00471v1",
            "title": "A Bayesian approach for prompt optimization in pre-trained language\n  models",
            "updated": "2023-12-01T10:10:18Z",
            "published": "2023-12-01T10:10:18Z",
            "summary": "A prompt is a sequence of symbol or tokens, selected from a vocabulary\naccording to some rule, which is prepended/concatenated to a textual query. A\nkey problem is how to select the sequence of tokens: in this paper we formulate\nit as a combinatorial optimization problem. The high dimensionality of the\ntoken space com-pounded by the length of the prompt sequence requires a very\nefficient solution. In this paper we propose a Bayesian optimization method,\nexecuted in a continuous em-bedding of the combinatorial space. In this paper\nwe focus on hard prompt tuning (HPT) which directly searches for discrete\ntokens to be added to the text input with-out requiring access to the large\nlanguage model (LLM) and can be used also when LLM is available only as a\nblack-box. This is critically important if LLMs are made available in the Model\nas a Service (MaaS) manner as in GPT-4. The current manu-script is focused on\nthe optimization of discrete prompts for classification tasks. The discrete\nprompts give rise to difficult combinatorial optimization problem which easily\nbecome intractable given the dimension of the token space in realistic\napplications. The optimization method considered in this paper is Bayesian\noptimization (BO) which has become the dominant approach in black-box\noptimization for its sample efficiency along with its modular structure and\nversatility. In this paper we use BoTorch, a library for Bayesian optimization\nresearch built on top of pyTorch. Albeit preliminary and obtained using a\n'vanilla' version of BO, the experiments on RoB-ERTa on six benchmarks, show a\ngood performance across a variety of tasks and enable an analysis of the\ntradeoff between size of the search space, accuracy and wall clock time.",
            "author": [
                "Antonio Sabbatella",
                "Andrea Ponti",
                "Antonio Candelieri",
                "Ilaria Giordani",
                "Francesco Archetti"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00471v1",
                "http://arxiv.org/pdf/2312.00471v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00462v1",
            "title": "Learning Unorthogonalized Matrices for Rotation Estimation",
            "updated": "2023-12-01T09:56:29Z",
            "published": "2023-12-01T09:56:29Z",
            "summary": "Estimating 3D rotations is a common procedure for 3D computer vision. The\naccuracy depends heavily on the rotation representation. One form of\nrepresentation -- rotation matrices -- is popular due to its continuity,\nespecially for pose estimation tasks. The learning process usually incorporates\northogonalization to ensure orthonormal matrices. Our work reveals, through\ngradient analysis, that common orthogonalization procedures based on the\nGram-Schmidt process and singular value decomposition will slow down training\nefficiency. To this end, we advocate removing orthogonalization from the\nlearning process and learning unorthogonalized `Pseudo' Rotation Matrices\n(PRoM). An optimization analysis shows that PRoM converges faster and to a\nbetter solution. By replacing the orthogonalization incorporated representation\nwith our proposed PRoM in various rotation-related tasks, we achieve\nstate-of-the-art results on large-scale benchmarks for human pose estimation.",
            "author": [
                "Kerui Gu",
                "Zhihao Li",
                "Shiyong Liu",
                "Jianzhuang Liu",
                "Songcen Xu",
                "Youliang Yan",
                "Michael Bi Mi",
                "Kenji Kawaguchi",
                "Angela Yao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00462v1",
                "http://arxiv.org/pdf/2312.00462v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00456v1",
            "title": "Auto-encoding GPS data to reveal individual and collective behaviour",
            "updated": "2023-12-01T09:41:40Z",
            "published": "2023-12-01T09:41:40Z",
            "summary": "We propose an innovative and generic methodology to analyse individual and\ncollective behaviour through individual trajectory data. The work is motivated\nby the analysis of GPS trajectories of fishing vessels collected from\nregulatory tracking data in the context of marine biodiversity conservation and\necosystem-based fisheries management. We build a low-dimensional latent\nrepresentation of trajectories using convolutional neural networks as\nnon-linear mapping. This is done by training a conditional variational\nauto-encoder taking into account covariates. The posterior distributions of the\nlatent representations can be linked to the characteristics of the actual\ntrajectories. The latent distributions of the trajectories are compared with\nthe Bhattacharyya coefficient, which is well-suited for comparing\ndistributions. Using this coefficient, we analyse the variation of the\nindividual behaviour of each vessel during time. For collective behaviour\nanalysis, we build proximity graphs and use an extension of the stochastic\nblock model for multiple networks. This model results in a clustering of the\nindividuals based on their set of trajectories. The application to French\nfishing vessels enables us to obtain groups of vessels whose individual and\ncollective behaviours exhibit spatio-temporal patterns over the period\n2014-2018.",
            "author": [
                "Saint-Clair Chabert-Liddell",
                "Nicolas Bez",
                "Pierre Gloaguen",
                "Sophie Donnet",
                "St\u00e9phanie Mah\u00e9vas"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00456v1",
                "http://arxiv.org/pdf/2312.00456v1"
            ],
            "primary_category": "stat.AP",
            "category": [
                "stat.AP",
                "cs.LG",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00455v1",
            "title": "Meta-Diversity Search in Complex Systems, A Recipe for Artificial\n  Open-Endedness ?",
            "updated": "2023-12-01T09:40:27Z",
            "published": "2023-12-01T09:40:27Z",
            "summary": "Can we build an artificial system that would be able to generate endless\nsurprises if ran \"forever\" in Minecraft? While there is not a single path\ntoward solving that grand challenge, this article presents what we believe to\nbe some working ingredients for the endless generation of novel increasingly\ncomplex artifacts in Minecraft. Our framework for an open-ended system includes\ntwo components: a complex system used to recursively grow and complexify\nartifacts over time, and a discovery algorithm that leverages the concept of\nmeta-diversity search. Since complex systems have shown to enable the emergence\nof considerable complexity from set of simple rules, we believe them to be\ngreat candidates to generate all sort of artifacts in Minecraft. Yet, the space\nof possible artifacts that can be generated by these systems is often unknown,\nchallenging to characterize and explore. Therefore automating the long-term\ndiscovery of novel and increasingly complex artifacts in these systems is an\nexciting research field. To approach these challenges, we formulate the problem\nof meta-diversity search where an artificial \"discovery assistant\"\nincrementally learns a diverse set of representations to characterize behaviors\nand searches to discover diverse patterns within each of them. A successful\ndiscovery assistant should continuously seek for novel sources of diversities\nwhile being able to quickly specialize the search toward a new unknown type of\ndiversity. To implement those ideas in the Minecraft environment, we simulate\nan artificial \"chemistry\" system based on Lenia continuous cellular automaton\nfor generating artifacts, as well as an artificial \"discovery assistant\"\n(called Holmes) for the artifact-discovery process. Holmes incrementally learns\na hierarchy of modular representations to characterize divergent sources of\ndiversity and uses a goal-based intrinsically-motivated exploration as the\ndiversity search strategy.",
            "author": [
                "Mayalen Etcheverry",
                "Bert Wang-Chak Chan",
                "Cl\u00e9ment Moulin-Frier",
                "Pierre-Yves Oudeyer"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00455v1",
                "http://arxiv.org/pdf/2312.00455v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.LG",
                "nlin.CG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00454v1",
            "title": "An Encoding Framework for Binarized Images using HyperDimensional\n  Computing",
            "updated": "2023-12-01T09:34:28Z",
            "published": "2023-12-01T09:34:28Z",
            "summary": "Hyperdimensional Computing (HDC) is a brain-inspired and light-weight machine\nlearning method. It has received significant attention in the literature as a\ncandidate to be applied in the wearable internet of things, near-sensor\nartificial intelligence applications and on-device processing. HDC is\ncomputationally less complex than traditional deep learning algorithms and\ntypically achieves moderate to good classification performance. A key aspect\nthat determines the performance of HDC is the encoding of the input data to the\nhyperdimensional (HD) space. This article proposes a novel light-weight\napproach relying only on native HD arithmetic vector operations to encode\nbinarized images that preserves similarity of patterns at nearby locations by\nusing point of interest selection and local linear mapping. The method reaches\nan accuracy of 97.35% on the test set for the MNIST data set and 84.12% for the\nFashion-MNIST data set. These results outperform other studies using baseline\nHDC with different encoding approaches and are on par with more complex hybrid\nHDC models. The proposed encoding approach also demonstrates a higher\nrobustness to noise and blur compared to the baseline encoding.",
            "author": [
                "Laura Smets",
                "Werner Van Leekwijck",
                "Ing Jyh Tsang",
                "Steven Latr\u00e9"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00454v1",
                "http://arxiv.org/pdf/2312.00454v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00847v2",
            "title": "Handling nonlinearities and uncertainties of fed-batch cultivations with\n  difference of convex functions tube MPC",
            "updated": "2023-12-07T14:35:19Z",
            "published": "2023-12-01T09:24:04Z",
            "summary": "Bioprocesses are often characterized by nonlinear and uncertain dynamics.\nThis poses particular challenges in the context of model predictive control\n(MPC). Several approaches have been proposed to solve this problem, such as\nrobust or stochastic MPC, but they can be computationally expensive when the\nsystem is nonlinear. Recent advances in optimal control theory have shown that\nconcepts from convex optimization, tube-based MPC, and difference of convex\nfunctions (DC) enable stable and robust online process control. The approach is\nbased on systematic DC decompositions of the dynamics and successive\nlinearizations around feasible trajectories. By convexity, the linearization\nerrors can be bounded tightly and treated as bounded disturbances in a robust\ntube-based MPC framework. However, finding the DC composition can be a\ndifficult task. To overcome this problem, we used a neural network with special\nconvex structure to learn the dynamics in DC form and express the uncertainty\nsets using simplices to maximize the product formation rate of a cultivation\nwith uncertain substrate concentration in the feed. The results show that this\nis a promising approach for computationally tractable data-driven robust MPC of\nbioprocesses.",
            "author": [
                "Niels Krausch",
                "Martin Doff-Sotta",
                "Mark Canon",
                "Peter Neubauer",
                "Mariano Nicolas Cruz Bournazou"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00847v2",
                "http://arxiv.org/pdf/2312.00847v2"
            ],
            "primary_category": "q-bio.QM",
            "category": [
                "q-bio.QM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00438v1",
            "title": "Dolphins: Multimodal Language Model for Driving",
            "updated": "2023-12-01T09:10:33Z",
            "published": "2023-12-01T09:10:33Z",
            "summary": "The quest for fully autonomous vehicles (AVs) capable of navigating complex\nreal-world scenarios with human-like understanding and responsiveness. In this\npaper, we introduce Dolphins, a novel vision-language model architected to\nimbibe human-like abilities as a conversational driving assistant. Dolphins is\nadept at processing multimodal inputs comprising video (or image) data, text\ninstructions, and historical control signals to generate informed outputs\ncorresponding to the provided instructions. Building upon the open-sourced\npretrained Vision-Language Model, OpenFlamingo, we first enhance Dolphins's\nreasoning capabilities through an innovative Grounded Chain of Thought (GCoT)\nprocess. Then we tailored Dolphins to the driving domain by constructing\ndriving-specific instruction data and conducting instruction tuning. Through\nthe utilization of the BDD-X dataset, we designed and consolidated four\ndistinct AV tasks into Dolphins to foster a holistic understanding of intricate\ndriving scenarios. As a result, the distinctive features of Dolphins are\ncharacterized into two dimensions: (1) the ability to provide a comprehensive\nunderstanding of complex and long-tailed open-world driving scenarios and solve\na spectrum of AV tasks, and (2) the emergence of human-like capabilities\nincluding gradient-free instant adaptation via in-context learning and error\nrecovery via reflection.",
            "author": [
                "Yingzi Ma",
                "Yulong Cao",
                "Jiachen Sun",
                "Marco Pavone",
                "Chaowei Xiao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00438v1",
                "http://arxiv.org/pdf/2312.00438v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00435v1",
            "title": "Enhancing Image Captioning with Neural Models",
            "updated": "2023-12-01T09:06:56Z",
            "published": "2023-12-01T09:06:56Z",
            "summary": "This research explores the realm of neural image captioning using deep\nlearning models. The study investigates the performance of different neural\narchitecture configurations, focusing on the inject architecture, and proposes\na novel quality metric for evaluating caption generation. Through extensive\nexperimentation and analysis, this work sheds light on the challenges and\nopportunities in image captioning, providing insights into model behavior and\noverfitting. The results reveal that while the merge models exhibit a larger\nvocabulary and higher ROUGE scores, the inject architecture generates relevant\nand concise image captions. The study also highlights the importance of\nrefining training data and optimizing hyperparameters for improved model\nperformance. This research contributes to the growing body of knowledge in\nneural image captioning and encourages further exploration in the field,\nemphasizing the democratization of artificial intelligence.",
            "author": [
                "Pooja Bhatnagar",
                "Sai Mrunaal",
                "Sachin Kamnure"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00435v1",
                "http://arxiv.org/pdf/2312.00435v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.NE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00434v1",
            "title": "PEFTDebias : Capturing debiasing information using PEFTs",
            "updated": "2023-12-01T09:06:06Z",
            "published": "2023-12-01T09:06:06Z",
            "summary": "The increasing use of foundation models highlights the urgent need to address\nand eliminate implicit biases present in them that arise during pretraining. In\nthis paper, we introduce PEFTDebias, a novel approach that employs\nparameter-efficient fine-tuning (PEFT) to mitigate the biases within foundation\nmodels. PEFTDebias consists of two main phases: an upstream phase for acquiring\ndebiasing parameters along a specific bias axis, and a downstream phase where\nthese parameters are incorporated into the model and frozen during the\nfine-tuning process. By evaluating on four datasets across two bias axes namely\ngender and race, we find that downstream biases can be effectively reduced with\nPEFTs. In addition, we show that these parameters possess axis-specific\ndebiasing characteristics, enabling their effective transferability in\nmitigating biases in various downstream tasks. To ensure reproducibility, we\nrelease the code to do our experiments.",
            "author": [
                "Sumit Agarwal",
                "Aditya Srikanth Veerubhotla",
                "Srijan Bansal"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00434v1",
                "http://arxiv.org/pdf/2312.00434v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00427v1",
            "title": "From Mutual Information to Expected Dynamics: New Generalization Bounds\n  for Heavy-Tailed SGD",
            "updated": "2023-12-01T08:50:42Z",
            "published": "2023-12-01T08:50:42Z",
            "summary": "Understanding the generalization abilities of modern machine learning\nalgorithms has been a major research topic over the past decades. In recent\nyears, the learning dynamics of Stochastic Gradient Descent (SGD) have been\nrelated to heavy-tailed dynamics. This has been successfully applied to\ngeneralization theory by exploiting the fractal properties of those dynamics.\nHowever, the derived bounds depend on mutual information (decoupling) terms\nthat are beyond the reach of computability. In this work, we prove\ngeneralization bounds over the trajectory of a class of heavy-tailed dynamics,\nwithout those mutual information terms. Instead, we introduce a geometric\ndecoupling term by comparing the learning dynamics (depending on the empirical\nrisk) with an expected one (depending on the population risk). We further\nupper-bound this geometric term, by using techniques from the heavy-tailed and\nthe fractal literature, making it fully computable. Moreover, as an attempt to\ntighten the bounds, we propose a PAC-Bayesian setting based on perturbed\ndynamics, in which the same geometric term plays a crucial role and can still\nbe bounded using the techniques described above.",
            "author": [
                "Benjamin Dupuis",
                "Paul Viallard"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00427v1",
                "http://arxiv.org/pdf/2312.00427v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00414v1",
            "title": "Large-scale Vision-Language Models Learn Super Images for Efficient and\n  High-Performance Partially Relevant Video Retrieval",
            "updated": "2023-12-01T08:38:27Z",
            "published": "2023-12-01T08:38:27Z",
            "summary": "In this paper, we propose an efficient and high-performance method for\npartially relevant video retrieval (PRVR), which aims to retrieve untrimmed\nlong videos that contain at least one relevant moment to the input text query.\nIn terms of both efficiency and performance, the overlooked bottleneck of\nprevious studies is the visual encoding of dense frames. This guides\nresearchers to choose lightweight visual backbones, yielding sub-optimal\nretrieval performance due to their limited capabilities of learned visual\nrepresentations. However, it is undesirable to simply replace them with\nhigh-performance large-scale vision-and-language models (VLMs) due to their low\nefficiency. To address these issues, instead of dense frames, we focus on super\nimages, which are created by rearranging the video frames in a $N \\times N$\ngrid layout. This reduces the number of visual encodings to $\\frac{1}{N^2}$ and\ncompensates for the low efficiency of large-scale VLMs, allowing us to adopt\nthem as powerful encoders. Surprisingly, we discover that with a simple\nquery-image attention trick, VLMs generalize well to super images effectively\nand demonstrate promising zero-shot performance against SOTA methods\nefficiently. In addition, we propose a fine-tuning approach by incorporating a\nfew trainable modules into the VLM backbones. The experimental results\ndemonstrate that our approaches efficiently achieve the best performance on\nActivityNet Captions and TVR.",
            "author": [
                "Taichi Nishimura",
                "Shota Nakada",
                "Masayoshi Kondo"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00414v1",
                "http://arxiv.org/pdf/2312.00414v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.MM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00413v1",
            "title": "Abstract Syntax Tree for Programming Language Understanding and\n  Representation: How Far Are We?",
            "updated": "2023-12-01T08:37:27Z",
            "published": "2023-12-01T08:37:27Z",
            "summary": "Programming language understanding and representation (a.k.a code\nrepresentation learning) has always been a hot and challenging task in software\nengineering. It aims to apply deep learning techniques to produce numerical\nrepresentations of the source code features while preserving its semantics.\nThese representations can be used for facilitating subsequent code-related\ntasks. The abstract syntax tree (AST), a fundamental code feature, illustrates\nthe syntactic information of the source code and has been widely used in code\nrepresentation learning. However, there is still a lack of systematic and\nquantitative evaluation of how well AST-based code representation facilitates\nsubsequent code-related tasks. In this paper, we first conduct a comprehensive\nempirical study to explore the effectiveness of the AST-based code\nrepresentation in facilitating follow-up code-related tasks. To do so, we\ncompare the performance of models trained with code token sequence (Token for\nshort) based code representation and AST-based code representation on three\npopular types of code-related tasks. Surprisingly, the overall quantitative\nstatistical results demonstrate that models trained with AST-based code\nrepresentation consistently perform worse across all three tasks compared to\nmodels trained with Token-based code representation. Our further quantitative\nanalysis reveals that models trained with AST-based code representation\noutperform models trained with Token-based code representation in certain\nsubsets of samples across all three tasks. We also conduct comprehensive\nexperiments to evaluate and reveal the impact of the choice of AST\nparsing/preprocessing/encoding methods on AST-based code representation and\nsubsequent code-related tasks. Our study provides future researchers with\ndetailed guidance on how to select solutions at each stage to fully exploit\nAST.",
            "author": [
                "Weisong Sun",
                "Chunrong Fang",
                "Yun Miao",
                "Yudu You",
                "Mengzhe Yuan",
                "Yuchen Chen",
                "Quanjun Zhang",
                "An Guo",
                "Xiang Chen",
                "Yang Liu",
                "Zhenyu Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00413v1",
                "http://arxiv.org/pdf/2312.00413v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE",
                "cs.AI",
                "cs.CL",
                "cs.PL",
                "68-04, 68T30",
                "D.2.3; I.2.2; I.2.4"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00411v1",
            "title": "A framework for mining lifestyle profiles through multi-dimensional and\n  high-order mobility feature clustering",
            "updated": "2023-12-01T08:21:05Z",
            "published": "2023-12-01T08:21:05Z",
            "summary": "Human mobility demonstrates a high degree of regularity, which facilitates\nthe discovery of lifestyle profiles. Existing research has yet to fully utilize\nthe regularities embedded in high-order features extracted from human mobility\nrecords in such profiling. This study proposes a progressive feature extraction\nstrategy that mines high-order mobility features from users' moving trajectory\nrecords from the spatial, temporal, and semantic dimensions. Specific features\nare extracted such as travel motifs, rhythms decomposed by discrete Fourier\ntransform (DFT) of mobility time series, and vectorized place semantics by\nword2vec, respectively to the three dimensions, and they are further clustered\nto reveal the users' lifestyle characteristics. An experiment using a\ntrajectory dataset of over 500k users in Shenzhen, China yields seven user\nclusters with different lifestyle profiles that can be well interpreted by\ncommon sense. The results suggest the possibility of fine-grained user\nprofiling through cross-order trajectory feature engineering and clustering.",
            "author": [
                "Yeshuo Shu",
                "Gangcheng Zhang",
                "Keyi Liu",
                "Jintong Tang",
                "Liyan Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00411v1",
                "http://arxiv.org/pdf/2312.00411v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00404v1",
            "title": "A Causality-Aware Pattern Mining Scheme for Group Activity Recognition\n  in a Pervasive Sensor Space",
            "updated": "2023-12-01T07:54:07Z",
            "published": "2023-12-01T07:54:07Z",
            "summary": "Human activity recognition (HAR) is a key challenge in pervasive computing\nand its solutions have been presented based on various disciplines.\nSpecifically, for HAR in a smart space without privacy and accessibility\nissues, data streams generated by deployed pervasive sensors are leveraged. In\nthis paper, we focus on a group activity by which a group of users perform a\ncollaborative task without user identification and propose an efficient group\nactivity recognition scheme which extracts causality patterns from pervasive\nsensor event sequences generated by a group of users to support as good\nrecognition accuracy as the state-of-the-art graphical model. To filter out\nirrelevant noise events from a given data stream, a set of rules is leveraged\nto highlight causally related events. Then, a pattern-tree algorithm extracts\nfrequent causal patterns by means of a growing tree structure. Based on the\nextracted patterns, a weighted sum-based pattern matching algorithm computes\nthe likelihoods of stored group activities to the given test event sequence by\nmeans of matched event pattern counts for group activity recognition. We\nevaluate the proposed scheme using the data collected from our testbed and\nCASAS datasets where users perform their tasks on a daily basis and validate\nits effectiveness in a real environment. Experiment results show that the\nproposed scheme performs higher recognition accuracy and with a small amount of\nruntime overhead than the existing schemes.",
            "author": [
                "Hyunju Kim",
                "Heesuk Son",
                "Dongman Lee"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00404v1",
                "http://arxiv.org/pdf/2312.00404v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.DB"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00401v1",
            "title": "VIoTGPT: Learning to Schedule Vision Tools towards Intelligent Video\n  Internet of Things",
            "updated": "2023-12-01T07:50:53Z",
            "published": "2023-12-01T07:50:53Z",
            "summary": "Video Internet of Things (VIoT) has shown full potential in collecting an\nunprecedented volume of video data. Learning to schedule perceiving models and\nanalyzing the collected videos intelligently will be potential sparks for VIoT.\nIn this paper, to address the challenges posed by the fine-grained and\ninterrelated vision tool usage of VIoT, we build VIoTGPT, the framework based\non LLMs to correctly interact with humans, query knowledge videos, and invoke\nvision models to accomplish complicated tasks. To support VIoTGPT and related\nfuture works, we meticulously crafted the training dataset and established\nbenchmarks involving 11 representative vision models across three categories\nbased on semi-automatic annotations. To guide LLM to act as the intelligent\nagent towards intelligent VIoT, we resort to ReAct instruction tuning based on\nthe collected VIoT dataset to learn the tool capability. Quantitative and\nqualitative experimental results and analyses demonstrate the effectiveness of\nVIoTGPT.",
            "author": [
                "Yaoyao Zhong",
                "Mengshi Qi",
                "Rui Wang",
                "Yuhan Qiu",
                "Yang Zhang",
                "Huadong Ma"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00401v1",
                "http://arxiv.org/pdf/2312.00401v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.MM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00398v1",
            "title": "Learning to Estimate Critical Gait Parameters from Single-View RGB\n  Videos with Transformer-Based Attention Network",
            "updated": "2023-12-01T07:45:27Z",
            "published": "2023-12-01T07:45:27Z",
            "summary": "Musculoskeletal diseases and cognitive impairments in patients lead to\ndifficulties in movement as well as negative effects on their psychological\nhealth. Clinical gait analysis, a vital tool for early diagnosis and treatment,\ntraditionally relies on expensive optical motion capture systems. Recent\nadvances in computer vision and deep learning have opened the door to more\naccessible and cost-effective alternatives. This paper introduces a novel\nspatio-temporal Transformer network to estimate critical gait parameters from\nRGB videos captured by a single-view camera. Empirical evaluations on a public\ndataset of cerebral palsy patients indicate that the proposed framework\nsurpasses current state-of-the-art approaches and show significant improvements\nin predicting general gait parameters (including Walking Speed, Gait Deviation\nIndex - GDI, and Knee Flexion Angle at Maximum Extension), while utilizing\nfewer parameters and alleviating the need for manual feature extraction.",
            "author": [
                "Quoc Hung T. Le",
                "Hieu H. Pham"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00398v1",
                "http://arxiv.org/pdf/2312.00398v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00396v1",
            "title": "GFN-SR: Symbolic Regression with Generative Flow Networks",
            "updated": "2023-12-01T07:38:05Z",
            "published": "2023-12-01T07:38:05Z",
            "summary": "Symbolic regression (SR) is an area of interpretable machine learning that\naims to identify mathematical expressions, often composed of simple functions,\nthat best fit in a given set of covariates $X$ and response $y$. In recent\nyears, deep symbolic regression (DSR) has emerged as a popular method in the\nfield by leveraging deep reinforcement learning to solve the complicated\ncombinatorial search problem. In this work, we propose an alternative framework\n(GFN-SR) to approach SR with deep learning. We model the construction of an\nexpression tree as traversing through a directed acyclic graph (DAG) so that\nGFlowNet can learn a stochastic policy to generate such trees sequentially.\nEnhanced with an adaptive reward baseline, our method is capable of generating\na diverse set of best-fitting expressions. Notably, we observe that GFN-SR\noutperforms other SR algorithms in noisy data regimes, owing to its ability to\nlearn a distribution of rewards over a space of candidate solutions.",
            "author": [
                "Sida Li",
                "Ioana Marinescu",
                "Sebastian Musslick"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00396v1",
                "http://arxiv.org/pdf/2312.00396v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00388v1",
            "title": "LinguaLinked: A Distributed Large Language Model Inference System for\n  Mobile Devices",
            "updated": "2023-12-01T07:19:42Z",
            "published": "2023-12-01T07:19:42Z",
            "summary": "Deploying Large Language Models (LLMs) locally on mobile devices presents a\nsignificant challenge due to their extensive memory requirements. In this\npaper, we introduce LinguaLinked, a system for decentralized, distributed LLM\ninference on mobile devices. LinguaLinked enables collaborative execution of\nthe inference task across multiple trusted devices. LinguaLinked ensures data\nprivacy by processing information locally. LinguaLinked uses three key\nstrategies. First, an optimized model assignment technique segments LLMs and\nuses linear optimization to align segments with each device's capabilities.\nSecond, an optimized data transmission mechanism ensures efficient and\nstructured data flow between model segments while also maintaining the\nintegrity of the original model structure. Finally, LinguaLinked incorporates a\nruntime load balancer that actively monitors and redistributes tasks among\nmobile devices to prevent bottlenecks, enhancing the system's overall\nefficiency and responsiveness. We demonstrate that LinguaLinked facilitates\nefficient LLM inference while maintaining consistent throughput and minimal\nlatency through extensive testing across various mobile devices, from high-end\nto low-end Android devices. In our evaluations, compared to the baseline,\nLinguaLinked achieves an inference performance acceleration of $1.11\\times$ to\n$1.61\\times$ in single-threaded settings, $1.73\\times$ to $2.65\\times$ with\nmulti-threading. Additionally, runtime load balancing yields an overall\ninference acceleration of $1.29\\times$ to $1.32\\times$.",
            "author": [
                "Junchen Zhao",
                "Yurun Song",
                "Simeng Liu",
                "Ian G. Harris",
                "Sangeetha Abdu Jyothi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00388v1",
                "http://arxiv.org/pdf/2312.00388v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.DC",
                "cs.NI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00386v1",
            "title": "Local monotone operator learning using non-monotone operators: MnM-MOL",
            "updated": "2023-12-01T07:15:51Z",
            "published": "2023-12-01T07:15:51Z",
            "summary": "The recovery of magnetic resonance (MR) images from undersampled measurements\nis a key problem that has seen extensive research in recent years. Unrolled\napproaches, which rely on end-to-end training of convolutional neural network\n(CNN) blocks within iterative reconstruction algorithms, offer state-of-the-art\nperformance. These algorithms require a large amount of memory during training,\nmaking them difficult to employ in high-dimensional applications. Deep\nequilibrium (DEQ) models and the recent monotone operator learning (MOL)\napproach were introduced to eliminate the need for unrolling, thus reducing the\nmemory demand during training. Both approaches require a Lipschitz constraint\non the network to ensure that the forward and backpropagation iterations\nconverge. Unfortunately, the constraint often results in reduced performance\ncompared to unrolled methods. The main focus of this work is to relax the\nconstraint on the CNN block in two different ways. Inspired by\nconvex-non-convex regularization strategies, we now impose the monotone\nconstraint on the sum of the gradient of the data term and the CNN block,\nrather than constrain the CNN itself to be a monotone operator. This approach\nenables the CNN to learn possibly non-monotone score functions, which can\ntranslate to improved performance. In addition, we only restrict the operator\nto be monotone in a local neighborhood around the image manifold. Our\ntheoretical results show that the proposed algorithm is guaranteed to converge\nto the fixed point and that the solution is robust to input perturbations,\nprovided that it is initialized close to the true solution. Our empirical\nresults show that the relaxed constraints translate to improved performance and\nthat the approach enjoys robustness to input perturbations similar to MOL.",
            "author": [
                "Maneesh John",
                "Jyothi Rikhab Chand",
                "Mathews Jacob"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00386v1",
                "http://arxiv.org/pdf/2312.00386v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00379v1",
            "title": "Optimal Sample Complexity of Contrastive Learning",
            "updated": "2023-12-01T06:57:11Z",
            "published": "2023-12-01T06:57:11Z",
            "summary": "Contrastive learning is a highly successful technique for learning\nrepresentations of data from labeled tuples, specifying the distance relations\nwithin the tuple. We study the sample complexity of contrastive learning, i.e.\nthe minimum number of labeled tuples sufficient for getting high generalization\naccuracy. We give tight bounds on the sample complexity in a variety of\nsettings, focusing on arbitrary distance functions, both general\n$\\ell_p$-distances, and tree metrics. Our main result is an (almost) optimal\nbound on the sample complexity of learning $\\ell_p$-distances for integer $p$.\nFor any $p \\ge 1$ we show that $\\tilde \\Theta(\\min(nd,n^2))$ labeled tuples are\nnecessary and sufficient for learning $d$-dimensional representations of\n$n$-point datasets. Our results hold for an arbitrary distribution of the input\nsamples and are based on giving the corresponding bounds on the\nVapnik-Chervonenkis/Natarajan dimension of the associated problems. We further\nshow that the theoretical bounds on sample complexity obtained via VC/Natarajan\ndimension can have strong predictive power for experimental results, in\ncontrast with the folklore belief about a substantial gap between the\nstatistical learning theory and the practice of deep learning.",
            "author": [
                "Noga Alon",
                "Dmitrii Avdiukhin",
                "Dor Elboim",
                "Orr Fischer",
                "Grigory Yaroslavtsev"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00379v1",
                "http://arxiv.org/pdf/2312.00379v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00845v1",
            "title": "VMC: Video Motion Customization using Temporal Attention Adaption for\n  Text-to-Video Diffusion Models",
            "updated": "2023-12-01T06:50:11Z",
            "published": "2023-12-01T06:50:11Z",
            "summary": "Text-to-video diffusion models have advanced video generation significantly.\nHowever, customizing these models to generate videos with tailored motions\npresents a substantial challenge. In specific, they encounter hurdles in (a)\naccurately reproducing motion from a target video, and (b) creating diverse\nvisual variations. For example, straightforward extensions of static image\ncustomization methods to video often lead to intricate entanglements of\nappearance and motion data. To tackle this, here we present the Video Motion\nCustomization (VMC) framework, a novel one-shot tuning approach crafted to\nadapt temporal attention layers within video diffusion models. Our approach\nintroduces a novel motion distillation objective using residual vectors between\nconsecutive frames as a motion reference. The diffusion process then preserves\nlow-frequency motion trajectories while mitigating high-frequency\nmotion-unrelated noise in image space. We validate our method against\nstate-of-the-art video generative models across diverse real-world motions and\ncontexts. Our codes, data and the project demo can be found at\nhttps://video-motion-customization.github.io",
            "author": [
                "Hyeonho Jeong",
                "Geon Yeong Park",
                "Jong Chul Ye"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00845v1",
                "http://arxiv.org/pdf/2312.00845v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00373v1",
            "title": "Streaming Bayesian Modeling for predicting Fat-Tailed Customer Lifetime\n  Value",
            "updated": "2023-12-01T06:33:39Z",
            "published": "2023-12-01T06:33:39Z",
            "summary": "We develop an online learning MCMC approach applicable for hierarchical\nbayesian models and GLMS. We also develop a fat-tailed LTV model that\ngeneralizes over several kinds of fat and thin tails. We demonstrate both\ndevelopments on commercial LTV data from a large mobile app.",
            "author": [
                "Alexey V. Calabourdin",
                "Konstantin A. Aksenov"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00373v1",
                "http://arxiv.org/pdf/2312.00373v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.AP",
                "stat.ME",
                "62C10, 62F15"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00364v1",
            "title": "Benchmarking Multi-Domain Active Learning on Image Classification",
            "updated": "2023-12-01T06:11:14Z",
            "published": "2023-12-01T06:11:14Z",
            "summary": "Active learning aims to enhance model performance by strategically labeling\ninformative data points. While extensively studied, its effectiveness on\nlarge-scale, real-world datasets remains underexplored. Existing research\nprimarily focuses on single-source data, ignoring the multi-domain nature of\nreal-world data. We introduce a multi-domain active learning benchmark to\nbridge this gap. Our benchmark demonstrates that traditional single-domain\nactive learning strategies are often less effective than random selection in\nmulti-domain scenarios. We also introduce CLIP-GeoYFCC, a novel large-scale\nimage dataset built around geographical domains, in contrast to existing\ngenre-based domain datasets. Analysis on our benchmark shows that all\nmulti-domain strategies exhibit significant tradeoffs, with no strategy\noutperforming across all datasets or all metrics, emphasizing the need for\nfuture research.",
            "author": [
                "Jiayi Li",
                "Rohan Taori",
                "Tatsunori B. Hashimoto"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00364v1",
                "http://arxiv.org/pdf/2312.00364v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00844v1",
            "title": "Sparse Beats Dense: Rethinking Supervision in Radar-Camera Depth\n  Completion",
            "updated": "2023-12-01T06:04:49Z",
            "published": "2023-12-01T06:04:49Z",
            "summary": "It is widely believed that the dense supervision is better than the sparse\nsupervision in the field of depth completion, but the underlying reasons for\nthis are rarely discussed. In this paper, we find that the challenge of using\nsparse supervision for training Radar-Camera depth prediction models is the\nProjection Transformation Collapse (PTC). The PTC implies that sparse\nsupervision leads the model to learn unexpected collapsed projection\ntransformations between Image/Radar/LiDAR spaces. Building on this insight, we\npropose a novel ``Disruption-Compensation\" framework to handle the PTC, thereby\nrelighting the use of sparse supervision in depth completion tasks. The\ndisruption part deliberately discards position correspondences among\nImage/Radar/LiDAR, while the compensation part leverages 3D spatial and 2D\nsemantic information to compensate for the discarded beneficial position\ncorrespondence. Extensive experimental results demonstrate that our framework\n(sparse supervision) outperforms the state-of-the-art (dense supervision) with\n11.6$\\%$ improvement in mean absolute error and $1.6 \\times$ speedup. The code\nis available at ...",
            "author": [
                "Huadong Li",
                "Minhao Jing",
                "Jiajun Liang",
                "Haoqiang Fan",
                "Renhe Ji"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00844v1",
                "http://arxiv.org/pdf/2312.00844v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00362v1",
            "title": "Dancing with Images: Video Distillation via Static-Dynamic\n  Disentanglement",
            "updated": "2023-12-01T05:59:08Z",
            "published": "2023-12-01T05:59:08Z",
            "summary": "Recently, dataset distillation has paved the way towards efficient machine\nlearning, especially for image datasets. However, the distillation for videos,\ncharacterized by an exclusive temporal dimension, remains an underexplored\ndomain. In this work, we provide the first systematic study of video\ndistillation and introduce a taxonomy to categorize temporal compression. Our\ninvestigation reveals that the temporal information is usually not well learned\nduring distillation , and the temporal dimension of synthetic data contributes\nlittle. The observations motivate our unified framework of disentangling the\ndynamic and static information in the videos. It first distills the videos into\nstill images as static memory and then compensates the dynamic and motion\ninformation with a learnable dynamic memory block. Our method achieves\nstate-of-the-art on video datasets at different scales, with notably smaller\nstorage expenditure. Our code will be publicly available.",
            "author": [
                "Ziyu Wang",
                "Yue Xu",
                "Cewu Lu",
                "Yong-Lu Li"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00362v1",
                "http://arxiv.org/pdf/2312.00362v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00360v2",
            "title": "Efficient Multimodal Semantic Segmentation via Dual-Prompt Learning",
            "updated": "2023-12-04T04:38:17Z",
            "published": "2023-12-01T05:50:44Z",
            "summary": "Multimodal (e.g., RGB-Depth/RGB-Thermal) fusion has shown great potential for\nimproving semantic segmentation in complex scenes (e.g., indoor/low-light\nconditions). Existing approaches often fully fine-tune a dual-branch\nencoder-decoder framework with a complicated feature fusion strategy for\nachieving multimodal semantic segmentation, which is training-costly due to the\nmassive parameter updates in feature extraction and fusion. To address this\nissue, we propose a surprisingly simple yet effective dual-prompt learning\nnetwork (dubbed DPLNet) for training-efficient multimodal (e.g., RGB-D/T)\nsemantic segmentation. The core of DPLNet is to directly adapt a frozen\npre-trained RGB model to multimodal semantic segmentation, reducing parameter\nupdates. For this purpose, we present two prompt learning modules, comprising\nmultimodal prompt generator (MPG) and multimodal feature adapter (MFA). MPG\nworks to fuse the features from different modalities in a compact manner and is\ninserted from shadow to deep stages to generate the multi-level multimodal\nprompts that are injected into the frozen backbone, while MPG adapts prompted\nmultimodal features in the frozen backbone for better multimodal semantic\nsegmentation. Since both the MPG and MFA are lightweight, only a few trainable\nparameters (3.88M, 4.4% of the pre-trained backbone parameters) are introduced\nfor multimodal feature fusion and learning. Using a simple decoder (3.27M\nparameters), DPLNet achieves new state-of-the-art performance or is on a par\nwith other complex approaches on four RGB-D/T semantic segmentation datasets\nwhile satisfying parameter efficiency. Moreover, we show that DPLNet is general\nand applicable to other multimodal tasks such as salient object detection and\nvideo semantic segmentation. Without special design, DPLNet outperforms many\ncomplicated models. Our code will be available at\ngithub.com/ShaohuaDong2021/DPLNet.",
            "author": [
                "Shaohua Dong",
                "Yunhe Feng",
                "Qing Yang",
                "Yan Huang",
                "Dongfang Liu",
                "Heng Fan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00360v2",
                "http://arxiv.org/pdf/2312.00360v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00359v1",
            "title": "Temperature Balancing, Layer-wise Weight Analysis, and Neural Network\n  Training",
            "updated": "2023-12-01T05:38:17Z",
            "published": "2023-12-01T05:38:17Z",
            "summary": "Regularization in modern machine learning is crucial, and it can take various\nforms in algorithmic design: training set, model family, error function,\nregularization terms, and optimizations. In particular, the learning rate,\nwhich can be interpreted as a temperature-like parameter within the statistical\nmechanics of learning, plays a crucial role in neural network training. Indeed,\nmany widely adopted training strategies basically just define the decay of the\nlearning rate over time. This process can be interpreted as decreasing a\ntemperature, using either a global learning rate (for the entire model) or a\nlearning rate that varies for each parameter. This paper proposes TempBalance,\na straightforward yet effective layer-wise learning rate method. TempBalance is\nbased on Heavy-Tailed Self-Regularization (HT-SR) Theory, an approach which\ncharacterizes the implicit self-regularization of different layers in trained\nmodels. We demonstrate the efficacy of using HT-SR-motivated metrics to guide\nthe scheduling and balancing of temperature across all network layers during\nmodel training, resulting in improved performance during testing. We implement\nTempBalance on CIFAR10, CIFAR100, SVHN, and TinyImageNet datasets using\nResNets, VGGs, and WideResNets with various depths and widths. Our results show\nthat TempBalance significantly outperforms ordinary SGD and carefully-tuned\nspectral norm regularization. We also show that TempBalance outperforms a\nnumber of state-of-the-art optimizers and learning rate schedulers.",
            "author": [
                "Yefan Zhou",
                "Tianyu Pang",
                "Keqin Liu",
                "Charles H. Martin",
                "Michael W. Mahoney",
                "Yaoqing Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00359v1",
                "http://arxiv.org/pdf/2312.00359v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00358v1",
            "title": "Impact of Data Augmentation on QCNNs",
            "updated": "2023-12-01T05:28:19Z",
            "published": "2023-12-01T05:28:19Z",
            "summary": "In recent years, Classical Convolutional Neural Networks (CNNs) have been\napplied for image recognition successfully. Quantum Convolutional Neural\nNetworks (QCNNs) are proposed as a novel generalization to CNNs by using\nquantum mechanisms. The quantum mechanisms lead to an efficient training\nprocess in QCNNs by reducing the size of input from $N$ to $log_2N$. This paper\nimplements and compares both CNNs and QCNNs by testing losses and prediction\naccuracy on three commonly used datasets. The datasets include the MNIST\nhand-written digits, Fashion MNIST and cat/dog face images. Additionally, data\naugmentation (DA), a technique commonly used in CNNs to improve the performance\nof classification by generating similar images based on original inputs, is\nalso implemented in QCNNs. Surprisingly, the results showed that data\naugmentation didn't improve QCNNs performance. The reasons and logic behind\nthis result are discussed, hoping to expand our understanding of Quantum\nmachine learning theory.",
            "author": [
                "Leting Zhouli",
                "Peiyong Wang",
                "Udaya Parampalli"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00358v1",
                "http://arxiv.org/pdf/2312.00358v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00357v1",
            "title": "A Generalizable Deep Learning System for Cardiac MRI",
            "updated": "2023-12-01T05:27:29Z",
            "published": "2023-12-01T05:27:29Z",
            "summary": "Cardiac MRI allows for a comprehensive assessment of myocardial structure,\nfunction, and tissue characteristics. Here we describe a foundational vision\nsystem for cardiac MRI, capable of representing the breadth of human\ncardiovascular disease and health. Our deep learning model is trained via\nself-supervised contrastive learning, by which visual concepts in cine-sequence\ncardiac MRI scans are learned from the raw text of the accompanying radiology\nreports. We train and evaluate our model on data from four large academic\nclinical institutions in the United States. We additionally showcase the\nperformance of our models on the UK BioBank, and two additional publicly\navailable external datasets. We explore emergent zero-shot capabilities of our\nsystem, and demonstrate remarkable performance across a range of tasks;\nincluding the problem of left ventricular ejection fraction regression, and the\ndiagnosis of 35 different conditions such as cardiac amyloidosis and\nhypertrophic cardiomyopathy. We show that our deep learning system is capable\nof not only understanding the staggering complexity of human cardiovascular\ndisease, but can be directed towards clinical problems of interest yielding\nimpressive, clinical grade diagnostic accuracy with a fraction of the training\ndata typically required for such tasks.",
            "author": [
                "Rohan Shad",
                "Cyril Zakka",
                "Dhamanpreet Kaur",
                "Robyn Fong",
                "Ross Warren Filice",
                "John Mongan",
                "Kimberly Kalianos",
                "Nishith Khandwala",
                "David Eng",
                "Matthew Leipzig",
                "Walter Witschey",
                "Alejandro de Feria",
                "Victor Ferrari",
                "Euan Ashley",
                "Michael A. Acker",
                "Curtis Langlotz",
                "William Hiesinger"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00357v1",
                "http://arxiv.org/pdf/2312.00357v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV",
                "cs.LG",
                "I.2.10"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00356v1",
            "title": "Transfer learning for predicting source terms of principal component\n  transport in chemically reactive flow",
            "updated": "2023-12-01T05:18:35Z",
            "published": "2023-12-01T05:18:35Z",
            "summary": "The objective of this study is to evaluate whether the number of requisite\ntraining samples can be reduced with the use of various transfer learning\nmodels for predicting, for example, the chemical source terms of the\ndata-driven reduced-order model that represents the homogeneous ignition\nprocess of a hydrogen/air mixture. Principal component analysis is applied to\nreduce the dimensionality of the hydrogen/air mixture in composition space.\nArtificial neural networks (ANNs) are used to tabulate the reaction rates of\nprincipal components, and subsequently, a system of ordinary differential\nequations is solved. As the number of training samples decreases at the target\ntask (i.e.,for T0 > 1000 K and various phi), the reduced-order model fails to\npredict the ignition evolution of a hydrogen/air mixture. Three transfer\nlearning strategies are then applied to the training of the ANN model with a\nsparse dataset. The performance of the reduced-order model with a sparse\ndataset is found to be remarkably enhanced if the training of the ANN model is\nrestricted by a regularization term that controls the degree of knowledge\ntransfer from source to target tasks. To this end, a novel transfer learning\nmethod is introduced, parameter control via partial initialization and\nregularization (PaPIR), whereby the amount of knowledge transferred is\nsystemically adjusted for the initialization and regularization of the ANN\nmodel in the target task. It is found that an additional performance gain can\nbe achieved by changing the initialization scheme of the ANN model in the\ntarget task when the task similarity between source and target tasks is\nrelatively low.",
            "author": [
                "Ki Sung Jung",
                "Tarek Echekki",
                "Jacqueline H. Chen",
                "Mohammad Khalil"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00356v1",
                "http://arxiv.org/pdf/2312.00356v1"
            ],
            "primary_category": "physics.chem-ph",
            "category": [
                "physics.chem-ph",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00353v1",
            "title": "On Exploring the Reasoning Capability of Large Language Models with\n  Knowledge Graphs",
            "updated": "2023-12-01T05:08:47Z",
            "published": "2023-12-01T05:08:47Z",
            "summary": "This paper examines the capacity of LLMs to reason with knowledge graphs\nusing their internal knowledge graph, i.e., the knowledge graph they learned\nduring pre-training. Two research questions are formulated to investigate the\naccuracy of LLMs in recalling information from pre-training knowledge graphs\nand their ability to infer knowledge graph relations from context. To address\nthese questions, we employ LLMs to perform four distinct knowledge graph\nreasoning tasks. Furthermore, we identify two types of hallucinations that may\noccur during knowledge reasoning with LLMs: content and ontology hallucination.\nOur experimental results demonstrate that LLMs can successfully tackle both\nsimple and complex knowledge graph reasoning tasks from their own memory, as\nwell as infer from input context.",
            "author": [
                "Pei-Chi Lo",
                "Yi-Hang Tsai",
                "Ee-Peng Lim",
                "San-Yih Hwang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00353v1",
                "http://arxiv.org/pdf/2312.00353v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00352v1",
            "title": "Quantum Kernel t-Distributed Stochastic Neighbor Embedding",
            "updated": "2023-12-01T05:00:02Z",
            "published": "2023-12-01T05:00:02Z",
            "summary": "Data visualization is important in understanding the characteristics of data\nthat are difficult to see directly. It is used to visualize loss landscapes and\noptimization trajectories to analyze optimization performance. Popular\noptimization analysis is performed by visualizing a loss landscape around the\nreached local or global minimum using principal component analysis. However,\nthis visualization depends on the variational parameters of a quantum circuit\nrather than quantum states, which makes it difficult to understand the\nmechanism of optimization process through the property of quantum states. Here,\nwe propose a quantum data visualization method using quantum kernels, which\nenables us to offer fast and highly accurate visualization of quantum states.\nIn our numerical experiments, we visualize hand-written digits dataset and\napply $k$-nearest neighbor algorithm to the low-dimensional data to\nquantitatively evaluate our proposed method compared with a classical kernel\nmethod. As a result, our proposed method achieves comparable accuracy to the\nstate-of-the-art classical kernel method, meaning that the proposed\nvisualization method based on quantum machine learning does not degrade the\nseparability of the input higher dimensional data. Furthermore, we visualize\nthe optimization trajectories of finding the ground states of transverse field\nIsing model and successfully find the trajectory characteristics. Since quantum\nstates are higher dimensional objects that can only be seen via observables,\nour visualization method, which inherits the similarity of quantum data, would\nbe useful in understanding the behavior of quantum circuits and algorithms.",
            "author": [
                "Yoshiaki Kawase",
                "Kosuke Mitarai",
                "Keisuke Fujii"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00352v1",
                "http://arxiv.org/pdf/2312.00352v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00351v2",
            "title": "Manipulating the Label Space for In-Context Classification",
            "updated": "2023-12-06T04:19:04Z",
            "published": "2023-12-01T04:57:20Z",
            "summary": "After pre-training by generating the next word conditional on previous words,\nthe Language Model (LM) acquires the ability of In-Context Learning (ICL) that\ncan learn a new task conditional on the context of the given in-context\nexamples (ICEs). Similarly, visually-conditioned Language Modelling is also\nused to train Vision-Language Models (VLMs) with ICL ability. However, such\nVLMs typically exhibit weaker classification abilities compared to contrastive\nlearning-based models like CLIP, since the Language Modelling objective does\nnot directly contrast whether an object is paired with a text. To improve the\nICL of classification, using more ICEs to provide more knowledge is a\nstraightforward way. However, this may largely increase the selection time, and\nmore importantly, the inclusion of additional in-context images tends to extend\nthe length of the in-context sequence beyond the processing capacity of a VLM.\nTo alleviate these limitations, we propose to manipulate the label space of\neach ICE to increase its knowledge density, allowing for fewer ICEs to convey\nas much information as a larger set would. Specifically, we propose two\nstrategies which are Label Distribution Enhancement and Visual Descriptions\nEnhancement to improve In-context classification performance on diverse\ndatasets, including the classic ImageNet and more fine-grained datasets like\nCUB-200. Specifically, using our approach on ImageNet, we increase accuracy\nfrom 74.70\\% in a 4-shot setting to 76.21\\% with just 2 shots. surpassing CLIP\nby 0.67\\%. On CUB-200, our method raises 1-shot accuracy from 48.86\\% to\n69.05\\%, 12.15\\% higher than CLIP. The code is given in\nhttps://anonymous.4open.science/r/MLS_ICC.",
            "author": [
                "Haokun Chen",
                "Xu Yang",
                "Yuhang Huang",
                "Zihan Wu",
                "Jing Wang",
                "Xin Geng"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00351v2",
                "http://arxiv.org/pdf/2312.00351v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00349v1",
            "title": "The Case for Scalable, Data-Driven Theory: A Paradigm for Scientific\n  Progress in NLP",
            "updated": "2023-12-01T04:55:29Z",
            "published": "2023-12-01T04:55:29Z",
            "summary": "I propose a paradigm for scientific progress in NLP centered around\ndeveloping scalable, data-driven theories of linguistic structure. The idea is\nto collect data in tightly scoped, carefully defined ways which allow for\nexhaustive annotation of behavioral phenomena of interest, and then use machine\nlearning to construct explanatory theories of these phenomena which can form\nbuilding blocks for intelligible AI systems. After laying some conceptual\ngroundwork, I describe several investigations into data-driven theories of\nshallow semantic structure using Question-Answer driven Semantic Role Labeling\n(QA-SRL), a schema for annotating verbal predicate-argument relations using\nhighly constrained question-answer pairs. While this only scratches the surface\nof the complex language behaviors of interest in AI, I outline principles for\ndata collection and theoretical modeling which can inform future scientific\nprogress. This note summarizes and draws heavily on my PhD thesis.",
            "author": [
                "Julian Michael"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00349v1",
                "http://arxiv.org/pdf/2312.00349v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "I.2.7"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00348v1",
            "title": "Student Activity Recognition in Classroom Environments using Transfer\n  Learning",
            "updated": "2023-12-01T04:51:57Z",
            "published": "2023-12-01T04:51:57Z",
            "summary": "The recent advances in artificial intelligence and deep learning facilitate\nautomation in various applications including home automation, smart\nsurveillance systems, and healthcare among others. Human Activity Recognition\nis one of its emerging applications, which can be implemented in a classroom\nenvironment to enhance safety, efficiency, and overall educational quality.\nThis paper proposes a system for detecting and recognizing the activities of\nstudents in a classroom environment. The dataset has been structured and\nrecorded by the authors since a standard dataset for this task was not\navailable at the time of this study. Transfer learning, a widely adopted method\nwithin the field of deep learning, has proven to be helpful in complex tasks\nlike image and video processing. Pretrained models including VGG-16, ResNet-50,\nInceptionV3, and Xception are used for feature extraction and classification\ntasks. Xception achieved an accuracy of 93%, on the novel classroom dataset,\noutperforming the other three models in consideration. The system proposed in\nthis study aims to introduce a safer and more productive learning environment\nfor students and educators.",
            "author": [
                "Anagha Deshpande",
                "Vedant Deshpande"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00348v1",
                "http://arxiv.org/pdf/2312.00348v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00344v1",
            "title": "TRC: Trust Region Conditional Value at Risk for Safe Reinforcement\n  Learning",
            "updated": "2023-12-01T04:40:47Z",
            "published": "2023-12-01T04:40:47Z",
            "summary": "As safety is of paramount importance in robotics, reinforcement learning that\nreflects safety, called safe RL, has been studied extensively. In safe RL, we\naim to find a policy which maximizes the desired return while satisfying the\ndefined safety constraints. There are various types of constraints, among which\nconstraints on conditional value at risk (CVaR) effectively lower the\nprobability of failures caused by high costs since CVaR is a conditional\nexpectation obtained above a certain percentile. In this paper, we propose a\ntrust region-based safe RL method with CVaR constraints, called TRC. We first\nderive the upper bound on CVaR and then approximate the upper bound in a\ndifferentiable form in a trust region. Using this approximation, a subproblem\nto get policy gradients is formulated, and policies are trained by iteratively\nsolving the subproblem. TRC is evaluated through safe navigation tasks in\nsimulations with various robots and a sim-to-real environment with a Jackal\nrobot from Clearpath. Compared to other safe RL methods, the performance is\nimproved by 1.93 times while the constraints are satisfied in all experiments.",
            "author": [
                "Dohyeong Kim",
                "Songhwai Oh"
            ],
            "link": [
                "http://dx.doi.org/10.1109/LRA.2022.3141829",
                "http://arxiv.org/abs/2312.00344v1",
                "http://arxiv.org/pdf/2312.00344v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00342v1",
            "title": "Efficient Off-Policy Safe Reinforcement Learning Using Trust Region\n  Conditional Value at Risk",
            "updated": "2023-12-01T04:29:19Z",
            "published": "2023-12-01T04:29:19Z",
            "summary": "This paper aims to solve a safe reinforcement learning (RL) problem with risk\nmeasure-based constraints. As risk measures, such as conditional value at risk\n(CVaR), focus on the tail distribution of cost signals, constraining risk\nmeasures can effectively prevent a failure in the worst case. An on-policy safe\nRL method, called TRC, deals with a CVaR-constrained RL problem using a trust\nregion method and can generate policies with almost zero constraint violations\nwith high returns. However, to achieve outstanding performance in complex\nenvironments and satisfy safety constraints quickly, RL methods are required to\nbe sample efficient. To this end, we propose an off-policy safe RL method with\nCVaR constraints, called off-policy TRC. If off-policy data from replay buffers\nis directly used to train TRC, the estimation error caused by the\ndistributional shift results in performance degradation. To resolve this issue,\nwe propose novel surrogate functions, in which the effect of the distributional\nshift can be reduced, and introduce an adaptive trust-region constraint to\nensure a policy not to deviate far from replay buffers. The proposed method has\nbeen evaluated in simulation and real-world environments and satisfied safety\nconstraints within a few steps while achieving high returns even in complex\nrobotic tasks.",
            "author": [
                "Dohyeong Kim",
                "Songhwai Oh"
            ],
            "link": [
                "http://dx.doi.org/10.1109/LRA.2022.3184793",
                "http://arxiv.org/abs/2312.00342v1",
                "http://arxiv.org/pdf/2312.00342v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00336v1",
            "title": "Hypergraph Node Representation Learning with One-Stage Message Passing",
            "updated": "2023-12-01T04:10:00Z",
            "published": "2023-12-01T04:10:00Z",
            "summary": "Hypergraphs as an expressive and general structure have attracted\nconsiderable attention from various research domains. Most existing hypergraph\nnode representation learning techniques are based on graph neural networks, and\nthus adopt the two-stage message passing paradigm (i.e. node -> hyperedge ->\nnode). This paradigm only focuses on local information propagation and does not\neffectively take into account global information, resulting in less optimal\nrepresentations. Our theoretical analysis of representative two-stage message\npassing methods shows that, mathematically, they model different ways of local\nmessage passing through hyperedges, and can be unified into one-stage message\npassing (i.e. node -> node). However, they still only model local information.\nMotivated by this theoretical analysis, we propose a novel one-stage message\npassing paradigm to model both global and local information propagation for\nhypergraphs. We integrate this paradigm into HGraphormer, a Transformer-based\nframework for hypergraph node representation learning. HGraphormer injects the\nhypergraph structure information (local information) into Transformers (global\ninformation) by combining the attention matrix and hypergraph Laplacian.\nExtensive experiments demonstrate that HGraphormer outperforms recent\nhypergraph learning methods on five representative benchmark datasets on the\nsemi-supervised hypernode classification task, setting new state-of-the-art\nperformance, with accuracy improvements between 2.52% and 6.70%. Our code and\ndatasets are available.",
            "author": [
                "Shilin Qu",
                "Weiqing Wang",
                "Yuan-Fang Li",
                "Xin Zhou",
                "Fajie Yuan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00336v1",
                "http://arxiv.org/pdf/2312.00336v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00335v1",
            "title": "Learning Anatomically Consistent Embedding for Chest Radiography",
            "updated": "2023-12-01T04:07:12Z",
            "published": "2023-12-01T04:07:12Z",
            "summary": "Self-supervised learning (SSL) approaches have recently shown substantial\nsuccess in learning visual representations from unannotated images. Compared\nwith photographic images, medical images acquired with the same imaging\nprotocol exhibit high consistency in anatomy. To exploit this anatomical\nconsistency, this paper introduces a novel SSL approach, called PEAC (patch\nembedding of anatomical consistency), for medical image analysis. Specifically,\nin this paper, we propose to learn global and local consistencies via stable\ngrid-based matching, transfer pre-trained PEAC models to diverse downstream\ntasks, and extensively demonstrate that (1) PEAC achieves significantly better\nperformance than the existing state-of-the-art fully/self-supervised methods,\nand (2) PEAC captures the anatomical structure consistency across views of the\nsame patient and across patients of different genders, weights, and healthy\nstatuses, which enhances the interpretability of our method for medical image\nanalysis.",
            "author": [
                "Ziyu Zhou",
                "Haozhe Luo",
                "Jiaxuan Pang",
                "Xiaowei Ding",
                "Michael Gotway",
                "Jianming Liang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00335v1",
                "http://arxiv.org/pdf/2312.00335v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00334v1",
            "title": "UAV-Aided Lifelong Learning for AoI and Energy Optimization in\n  Non-Stationary IoT Networks",
            "updated": "2023-12-01T04:06:45Z",
            "published": "2023-12-01T04:06:45Z",
            "summary": "In this paper, a novel joint energy and age of information (AoI) optimization\nframework for IoT devices in a non-stationary environment is presented. In\nparticular, IoT devices that are distributed in the real-world are required to\nefficiently utilize their computing resources so as to balance the freshness of\ntheir data and their energy consumption. To optimize the performance of IoT\ndevices in such a dynamic setting, a novel lifelong reinforcement learning (RL)\nsolution that enables IoT devices to continuously adapt their policies to each\nnewly encountered environment is proposed. Given that IoT devices have limited\nenergy and computing resources, an unmanned aerial vehicle (UAV) is leveraged\nto visit the IoT devices and update the policy of each device sequentially. As\nsuch, the UAV is exploited as a mobile learning agent that can learn a shared\nknowledge base with a feature base in its training phase, and feature sets of a\nzero-shot learning method in its testing phase, to generalize between the\nenvironments. To optimize the trajectory and flying velocity of the UAV, an\nactor-critic network is leveraged so as to minimize the UAV energy consumption.\nSimulation results show that the proposed lifelong RL solution can outperform\nthe state-of-art benchmarks by enhancing the balanced cost of IoT devices by\n$8.3\\%$ when incorporating warm-start policies for unseen environments. In\naddition, our solution achieves up to $49.38\\%$ reduction in terms of energy\nconsumption by the UAV in comparison to the random flying strategy.",
            "author": [
                "Zhenzhen Gong",
                "Omar Hashash",
                "Yingze Wang",
                "Qimei Cui",
                "Wei Ni",
                "Walid Saad",
                "Kei Sakaguchi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00334v1",
                "http://arxiv.org/pdf/2312.00334v1"
            ],
            "primary_category": "cs.NI",
            "category": [
                "cs.NI",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00333v1",
            "title": "Green Edge AI: A Contemporary Survey",
            "updated": "2023-12-01T04:04:37Z",
            "published": "2023-12-01T04:04:37Z",
            "summary": "Artificial intelligence (AI) technologies have emerged as pivotal enablers\nacross a multitude of industries, including consumer electronics, healthcare,\nand manufacturing, largely due to their resurgence over the past decade. The\ntransformative power of AI is primarily derived from the utilization of deep\nneural networks (DNNs), which require extensive data for training and\nsubstantial computational resources for processing. Consequently, DNN models\nare typically trained and deployed on resource-rich cloud servers. However, due\nto potential latency issues associated with cloud communications, deep learning\n(DL) workflows are increasingly being transitioned to wireless edge networks\nnear end-user devices (EUDs). This shift is designed to support\nlatency-sensitive applications and has given rise to a new paradigm of edge AI,\nwhich will play a critical role in upcoming 6G networks to support ubiquitous\nAI applications. Despite its potential, edge AI faces substantial challenges,\nmostly due to the dichotomy between the resource limitations of wireless edge\nnetworks and the resource-intensive nature of DL. Specifically, the acquisition\nof large-scale data, as well as the training and inference processes of DNNs,\ncan rapidly deplete the battery energy of EUDs. This necessitates an\nenergy-conscious approach to edge AI to ensure both optimal and sustainable\nperformance. In this paper, we present a contemporary survey on green edge AI.\nWe commence by analyzing the principal energy consumption components of edge AI\nsystems to identify the fundamental design principles of green edge AI. Guided\nby these principles, we then explore energy-efficient design methodologies for\nthe three critical tasks in edge AI systems, including training data\nacquisition, edge training, and edge inference. Finally, we underscore\npotential future research directions to further enhance the energy efficiency\nof edge AI.",
            "author": [
                "Yuyi Mao",
                "Xianghao Yu",
                "Kaibin Huang",
                "Ying-Jun Angela Zhang",
                "Jun Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00333v1",
                "http://arxiv.org/pdf/2312.00333v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.IT",
                "cs.NI",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00843v1",
            "title": "Exploring the Robustness of Decentralized Training for Large Language\n  Models",
            "updated": "2023-12-01T04:04:03Z",
            "published": "2023-12-01T04:04:03Z",
            "summary": "Decentralized training of large language models has emerged as an effective\nway to democratize this technology. However, the potential threats associated\nwith this approach have not been carefully discussed, which would hinder the\ndevelopment of decentralized training infrastructures. This paper aims to\ninitiate discussion towards this end by exploring the robustness of\ndecentralized training from three main perspectives. First, we demonstrate the\nvulnerabilities inherent in decentralized training frameworks in terms of\nhardware, data, and models. Second, we highlight the fundamental difference\nbetween decentralized foundation model training and vanilla federated learning,\nwhere the security techniques employed in federated learning cannot be applied\ndirectly. Third, we discuss the essential components required for a robust and\nefficient decentralized training framework and present a case study by modeling\na concrete threat model. Our objective in this vision paper is to emphasize the\nimportance of addressing security concerns in the context of decentralized\ntraining for large language models.",
            "author": [
                "Lin Lu",
                "Chenxi Dai",
                "Wangcheng Tao",
                "Binhang Yuan",
                "Yanan Sun",
                "Pan Zhou"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00843v1",
                "http://arxiv.org/pdf/2312.00843v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00842v1",
            "title": "ESM-NBR: fast and accurate nucleic acid-binding residue prediction via\n  protein language model feature representation and multi-task learning",
            "updated": "2023-12-01T04:00:20Z",
            "published": "2023-12-01T04:00:20Z",
            "summary": "Protein-nucleic acid interactions play a very important role in a variety of\nbiological activities. Accurate identification of nucleic acid-binding residues\nis a critical step in understanding the interaction mechanisms. Although many\ncomputationally based methods have been developed to predict nucleic\nacid-binding residues, challenges remain. In this study, a fast and accurate\nsequence-based method, called ESM-NBR, is proposed. In ESM-NBR, we first use\nthe large protein language model ESM2 to extract discriminative biological\nproperties feature representation from protein primary sequences; then, a\nmulti-task deep learning model composed of stacked bidirectional long\nshort-term memory (BiLSTM) and multi-layer perceptron (MLP) networks is\nemployed to explore common and private information of DNA- and RNA-binding\nresidues with ESM2 feature as input. Experimental results on benchmark data\nsets demonstrate that the prediction performance of ESM2 feature representation\ncomprehensively outperforms evolutionary information-based hidden Markov model\n(HMM) features. Meanwhile, the ESM-NBR obtains the MCC values for DNA-binding\nresidues prediction of 0.427 and 0.391 on two independent test sets, which are\n18.61 and 10.45% higher than those of the second-best methods, respectively.\nMoreover, by completely discarding the time-cost multiple sequence alignment\nprocess, the prediction speed of ESM-NBR far exceeds that of existing methods\n(5.52s for a protein sequence of length 500, which is about 16 times faster\nthan the second-fastest method). A user-friendly standalone package and the\ndata of ESM-NBR are freely available for academic use at:\nhttps://github.com/wwzll123/ESM-NBR.",
            "author": [
                "Wenwu Zeng",
                "Dafeng Lv",
                "Wenjuan Liu",
                "Shaoliang Peng"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00842v1",
                "http://arxiv.org/pdf/2312.00842v1"
            ],
            "primary_category": "q-bio.QM",
            "category": [
                "q-bio.QM",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00330v1",
            "title": "StyleCrafter: Enhancing Stylized Text-to-Video Generation with Style\n  Adapter",
            "updated": "2023-12-01T03:53:21Z",
            "published": "2023-12-01T03:53:21Z",
            "summary": "Text-to-video (T2V) models have shown remarkable capabilities in generating\ndiverse videos. However, they struggle to produce user-desired stylized videos\ndue to (i) text's inherent clumsiness in expressing specific styles and (ii)\nthe generally degraded style fidelity. To address these challenges, we\nintroduce StyleCrafter, a generic method that enhances pre-trained T2V models\nwith a style control adapter, enabling video generation in any style by\nproviding a reference image. Considering the scarcity of stylized video\ndatasets, we propose to first train a style control adapter using style-rich\nimage datasets, then transfer the learned stylization ability to video\ngeneration through a tailor-made finetuning paradigm. To promote content-style\ndisentanglement, we remove style descriptions from the text prompt and extract\nstyle information solely from the reference image using a decoupling learning\nstrategy. Additionally, we design a scale-adaptive fusion module to balance the\ninfluences of text-based content features and image-based style features, which\nhelps generalization across various text and style combinations. StyleCrafter\nefficiently generates high-quality stylized videos that align with the content\nof the texts and resemble the style of the reference images. Experiments\ndemonstrate that our approach is more flexible and efficient than existing\ncompetitors.",
            "author": [
                "Gongye Liu",
                "Menghan Xia",
                "Yong Zhang",
                "Haoxin Chen",
                "Jinbo Xing",
                "Xintao Wang",
                "Yujiu Yang",
                "Ying Shan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00330v1",
                "http://arxiv.org/pdf/2312.00330v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00326v1",
            "title": "Agent-OM: Leveraging Large Language Models for Ontology Matching",
            "updated": "2023-12-01T03:44:54Z",
            "published": "2023-12-01T03:44:54Z",
            "summary": "Ontology matching (OM) enables semantic interoperability between different\nontologies and resolves their conceptual heterogeneity by aligning related\nentities. OM systems currently have two prevailing design paradigms:\nconventional knowledge-based expert systems and newer machine learning-based\npredictive systems. While large language models (LLMs) and LLM-based agents\nhave become revolutionary in data engineering and have been applied creatively\nin various domains, their potential for OM remains underexplored. This study\nintroduces a novel agent-powered LLM-based design paradigm for OM systems. With\nthoughtful consideration of several specific challenges to leverage LLMs for\nOM, we propose a generic framework, namely Agent-OM, consisting of two Siamese\nagents for retrieval and matching, with a set of simple prompt-based OM tools.\nOur framework is implemented in a proof-of-concept system. Evaluations of three\nOntology Alignment Evaluation Initiative (OAEI) tracks over state-of-the-art OM\nsystems show that our system can achieve very close results to the best\nlong-standing performance on simple OM tasks and significantly improve the\nperformance on complex and few-shot OM tasks.",
            "author": [
                "Zhangcheng Qiang",
                "Weiqing Wang",
                "Kerry Taylor"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00326v1",
                "http://arxiv.org/pdf/2312.00326v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL",
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00324v1",
            "title": "Machine Learning for Actionable Warning Identification: A Comprehensive\n  Survey",
            "updated": "2023-12-01T03:38:21Z",
            "published": "2023-12-01T03:38:21Z",
            "summary": "Actionable Warning Identification (AWI) plays a crucial role in improving the\nusability of static code analyzers. With recent advances in Machine Learning\n(ML), various approaches have been proposed to incorporate ML techniques into\nAWI. These ML-based AWI approaches, benefiting from ML's strong ability to\nlearn subtle and previously unseen patterns from historical data, have\ndemonstrated superior performance. However, a comprehensive overview of these\napproaches is missing, which could hinder researchers/practitioners from\nunderstanding the current process and discovering potential for future\nimprovement in the ML-based AWI community. In this paper, we systematically\nreview the state-of-the-art ML-based AWI approaches. First, we employ a\nmeticulous survey methodology and gather 50 primary studies from 2000/01/01 to\n2023/09/01. Then, we outline the typical ML-based AWI workflow, including\nwarning dataset preparation, preprocessing, AWI model construction, and\nevaluation stages. In such a workflow, we categorize ML-based AWI approaches\nbased on the warning output format. Besides, we analyze the techniques used in\neach stage, along with their strengths, weaknesses, and distribution. Finally,\nwe provide practical research directions for future ML-based AWI approaches,\nfocusing on aspects like data improvement (e.g., enhancing the warning labeling\nstrategy) and model exploration (e.g., exploring large language models for\nAWI).",
            "author": [
                "Xiuting Ge",
                "Chunrong Fang",
                "Xuanye Li",
                "Weisong Sun",
                "Daoyuan Wu",
                "Juan Zhai",
                "Shangwei Lin",
                "Zhihong Zhao",
                "Yang Liu",
                "Zhenyu Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00324v1",
                "http://arxiv.org/pdf/2312.00324v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00313v1",
            "title": "Improving Normalization with the James-Stein Estimator",
            "updated": "2023-12-01T03:12:04Z",
            "published": "2023-12-01T03:12:04Z",
            "summary": "Stein's paradox holds considerable sway in high-dimensional statistics,\nhighlighting that the sample mean, traditionally considered the de facto\nestimator, might not be the most efficacious in higher dimensions. To address\nthis, the James-Stein estimator proposes an enhancement by steering the sample\nmeans toward a more centralized mean vector. In this paper, first, we establish\nthat normalization layers in deep learning use inadmissible estimators for mean\nand variance. Next, we introduce a novel method to employ the James-Stein\nestimator to improve the estimation of mean and variance within normalization\nlayers. We evaluate our method on different computer vision tasks: image\nclassification, semantic segmentation, and 3D object classification. Through\nthese evaluations, it is evident that our improved normalization layers\nconsistently yield superior accuracy across all tasks without extra\ncomputational burden. Moreover, recognizing that a plethora of shrinkage\nestimators surpass the traditional estimator in performance, we study two other\nprominent shrinkage estimators: Ridge and LASSO. Additionally, we provide\nvisual representations to intuitively demonstrate the impact of shrinkage on\nthe estimated layer statistics. Finally, we study the effect of regularization\nand batch size on our modified batch normalization. The studies show that our\nmethod is less sensitive to batch size and regularization, improving accuracy\nunder various setups.",
            "author": [
                "Seyedalireza Khoshsirat",
                "Chandra Kambhamettu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00313v1",
                "http://arxiv.org/pdf/2312.00313v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00312v1",
            "title": "Segment Anything Model-guided Collaborative Learning Network for\n  Scribble-supervised Polyp Segmentation",
            "updated": "2023-12-01T03:07:13Z",
            "published": "2023-12-01T03:07:13Z",
            "summary": "Polyp segmentation plays a vital role in accurately locating polyps at an\nearly stage, which holds significant clinical importance for the prevention of\ncolorectal cancer. Various polyp segmentation methods have been developed using\nfully-supervised deep learning techniques. However, pixel-wise annotation for\npolyp images by physicians during the diagnosis is both time-consuming and\nexpensive. Moreover, visual foundation models such as the Segment Anything\nModel (SAM) have shown remarkable performance. Nevertheless, directly applying\nSAM to medical segmentation may not produce satisfactory results due to the\ninherent absence of medical knowledge. In this paper, we propose a novel\nSAM-guided Collaborative Learning Network (SAM-CLNet) for scribble-supervised\npolyp segmentation, enabling a collaborative learning process between our\nsegmentation network and SAM to boost the model performance. Specifically, we\nfirst propose a Cross-level Enhancement and Aggregation Network (CEA-Net) for\nweakly-supervised polyp segmentation. Within CEA-Net, we propose a Cross-level\nEnhancement Module (CEM) that integrates the adjacent features to enhance the\nrepresentation capabilities of different resolution features. Additionally, a\nFeature Aggregation Module (FAM) is employed to capture richer features across\nmultiple levels. Moreover, we present a box-augmentation strategy that combines\nthe segmentation maps generated by CEA-Net with scribble annotations to create\nmore precise prompts. These prompts are then fed into SAM, generating\nsegmentation SAM-guided masks, which can provide additional supervision to\ntrain CEA-Net effectively. Furthermore, we present an Image-level Filtering\nMechanism to filter out unreliable SAM-guided masks. Extensive experimental\nresults show that our SAM-CLNet outperforms state-of-the-art weakly-supervised\nsegmentation methods.",
            "author": [
                "Yiming Zhao",
                "Tao Zhou",
                "Yunqi Gu",
                "Yi Zhou",
                "Yizhe Zhang",
                "Ye Wu",
                "Huazhu Fu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00312v1",
                "http://arxiv.org/pdf/2312.00312v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00305v1",
            "title": "Multiple Testing of Linear Forms for Noisy Matrix Completion",
            "updated": "2023-12-01T02:53:20Z",
            "published": "2023-12-01T02:53:20Z",
            "summary": "Many important tasks of large-scale recommender systems can be naturally cast\nas testing multiple linear forms for noisy matrix completion. These problems,\nhowever, present unique challenges because of the subtle bias-and-variance\ntradeoff of and an intricate dependence among the estimated entries induced by\nthe low-rank structure. In this paper, we develop a general approach to\novercome these difficulties by introducing new statistics for individual tests\nwith sharp asymptotics both marginally and jointly, and utilizing them to\ncontrol the false discovery rate (FDR) via a data splitting and symmetric\naggregation scheme. We show that valid FDR control can be achieved with\nguaranteed power under nearly optimal sample size requirements using the\nproposed methodology. Extensive numerical simulations and real data examples\nare also presented to further illustrate its practical merits.",
            "author": [
                "Wanteng Ma",
                "Lilun Du",
                "Dong Xia",
                "Ming Yuan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00305v1",
                "http://arxiv.org/pdf/2312.00305v1"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME",
                "cs.LG",
                "math.ST",
                "stat.ML",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00304v1",
            "title": "Developmental Pretraining (DPT) for Image Classification Networks",
            "updated": "2023-12-01T02:47:00Z",
            "published": "2023-12-01T02:47:00Z",
            "summary": "In the backdrop of increasing data requirements of Deep Neural Networks for\nobject recognition that is growing more untenable by the day, we present\nDevelopmental PreTraining (DPT) as a possible solution. DPT is designed as a\ncurriculum-based pre-training approach designed to rival traditional\npre-training techniques that are data-hungry. These training approaches also\nintroduce unnecessary features that could be misleading when the network is\nemployed in a downstream classification task where the data is sufficiently\ndifferent from the pre-training data and is scarce. We design the curriculum\nfor DPT by drawing inspiration from human infant visual development. DPT\nemploys a phased approach where carefully-selected primitive and universal\nfeatures like edges and shapes are taught to the network participating in our\npre-training regime. A model that underwent the DPT regime is tested against\nmodels with randomised weights to evaluate the viability of DPT.",
            "author": [
                "Niranjan Rajesh",
                "Debayan Gupta"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00304v1",
                "http://arxiv.org/pdf/2312.00304v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00840v1",
            "title": "Towards Redundancy-Free Sub-networks in Continual Learning",
            "updated": "2023-12-01T02:29:52Z",
            "published": "2023-12-01T02:29:52Z",
            "summary": "Catastrophic Forgetting (CF) is a prominent issue in continual learning.\nParameter isolation addresses this challenge by masking a sub-network for each\ntask to mitigate interference with old tasks. However, these sub-networks are\nconstructed relying on weight magnitude, which does not necessarily correspond\nto the importance of weights, resulting in maintaining unimportant weights and\nconstructing redundant sub-networks. To overcome this limitation, inspired by\ninformation bottleneck, which removes redundancy between adjacent network\nlayers, we propose \\textbf{\\underline{I}nformation \\underline{B}ottleneck\n\\underline{M}asked sub-network (IBM)} to eliminate redundancy within\nsub-networks. Specifically, IBM accumulates valuable information into essential\nweights to construct redundancy-free sub-networks, not only effectively\nmitigating CF by freezing the sub-networks but also facilitating new tasks\ntraining through the transfer of valuable knowledge. Additionally, IBM\ndecomposes hidden representations to automate the construction process and make\nit flexible. Extensive experiments demonstrate that IBM consistently\noutperforms state-of-the-art methods. Notably, IBM surpasses the\nstate-of-the-art parameter isolation method with a 70\\% reduction in the number\nof parameters within sub-networks and an 80\\% decrease in training time.",
            "author": [
                "Cheng Chen",
                "Jingkuan Song",
                "LianLi Gao",
                "Heng Tao Shen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00840v1",
                "http://arxiv.org/pdf/2312.00840v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00296v1",
            "title": "Towards Aligned Canonical Correlation Analysis: Preliminary Formulation\n  and Proof-of-Concept Results",
            "updated": "2023-12-01T02:24:07Z",
            "published": "2023-12-01T02:24:07Z",
            "summary": "Canonical Correlation Analysis (CCA) has been widely applied to jointly embed\nmultiple views of data in a maximally correlated latent space. However, the\nalignment between various data perspectives, which is required by traditional\napproaches, is unclear in many practical cases. In this work we propose a new\nframework Aligned Canonical Correlation Analysis (ACCA), to address this\nchallenge by iteratively solving the alignment and multi-view embedding.",
            "author": [
                "Biqian Cheng",
                "Evangelos E. Papalexakis",
                "Jia Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00296v1",
                "http://arxiv.org/pdf/2312.00296v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00292v1",
            "title": "SEPSIS: I Can Catch Your Lies -- A New Paradigm for Deception Detection",
            "updated": "2023-12-01T02:13:25Z",
            "published": "2023-12-01T02:13:25Z",
            "summary": "Deception is the intentional practice of twisting information. It is a\nnuanced societal practice deeply intertwined with human societal evolution,\ncharacterized by a multitude of facets. This research explores the problem of\ndeception through the lens of psychology, employing a framework that\ncategorizes deception into three forms: lies of omission, lies of commission,\nand lies of influence. The primary focus of this study is specifically on\ninvestigating only lies of omission. We propose a novel framework for deception\ndetection leveraging NLP techniques. We curated an annotated dataset of 876,784\nsamples by amalgamating a popular large-scale fake news dataset and scraped\nnews headlines from the Twitter handle of Times of India, a well-known Indian\nnews media house. Each sample has been labeled with four layers, namely: (i)\nthe type of omission (speculation, bias, distortion, sounds factual, and\nopinion), (ii) colors of lies(black, white, etc), and (iii) the intention of\nsuch lies (to influence, etc) (iv) topic of lies (political, educational,\nreligious, etc). We present a novel multi-task learning pipeline that leverages\nthe dataless merging of fine-tuned language models to address the deception\ndetection task mentioned earlier. Our proposed model achieved an F1 score of\n0.87, demonstrating strong performance across all layers including the type,\ncolor, intent, and topic aspects of deceptive content. Finally, our research\nexplores the relationship between lies of omission and propaganda techniques.\nTo accomplish this, we conducted an in-depth analysis, uncovering compelling\nfindings. For instance, our analysis revealed a significant correlation between\nloaded language and opinion, shedding light on their interconnectedness. To\nencourage further research in this field, we will be making the models and\ndataset available with the MIT License, making it favorable for open-source\nresearch.",
            "author": [
                "Anku Rani",
                "Dwip Dalal",
                "Shreya Gautam",
                "Pankaj Gupta",
                "Vinija Jain",
                "Aman Chadha",
                "Amit Sheth",
                "Amitava Das"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00292v1",
                "http://arxiv.org/pdf/2312.00292v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00290v1",
            "title": "Learning to forecast diagnostic parameters using pre-trained weather\n  embedding",
            "updated": "2023-12-01T02:09:18Z",
            "published": "2023-12-01T02:09:18Z",
            "summary": "Data-driven weather prediction (DDWP) models are increasingly becoming\npopular for weather forecasting. However, while operational weather forecasts\npredict a wide variety of weather variables, DDWPs currently forecast a\nspecific set of key prognostic variables. Non-prognostic (\"diagnostic\")\nvariables are sometimes modeled separately as dependent variables of the\nprognostic variables (c.f. FourCastNet), or by including the diagnostic\nvariable as a target in the DDWP. However, the cost of training and deploying\nbespoke models for each diagnostic variable can increase dramatically with more\ndiagnostic variables, and limit the operational use of such models. Likewise,\nretraining an entire DDWP each time a new diagnostic variable is added is also\ncost-prohibitive. We present an two-stage approach that allows new diagnostic\nvariables to be added to an end-to-end DDWP model without the expensive\nretraining. In the first stage, we train an autoencoder that learns to embed\nprognostic variables into a latent space. In the second stage, the autoencoder\nis frozen and \"downstream\" models are trained to predict diagnostic variables\nusing only the latent representations of prognostic variables as input. Our\nexperiments indicate that models trained using the two-stage approach offer\naccuracy comparable to training bespoke models, while leading to significant\nreduction in resource utilization during training and inference. This approach\nallows for new \"downstream\" models to be developed as needed, without affecting\nexisting models and thus reducing the friction in operationalizing new models.",
            "author": [
                "Peetak P. Mitra",
                "Vivek Ramavajjala"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00290v1",
                "http://arxiv.org/pdf/2312.00290v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00839v2",
            "title": "PipeOptim: Ensuring Effective 1F1B Schedule with Optimizer-Dependent\n  Weight Prediction",
            "updated": "2023-12-05T07:16:55Z",
            "published": "2023-12-01T01:52:38Z",
            "summary": "Asynchronous pipeline model parallelism with a \"1F1B\" (one forward, one\nbackward) schedule generates little bubble overhead and always provides quite a\nhigh throughput. However, the \"1F1B\" schedule inevitably leads to weight\ninconsistency and weight staleness issues due to the cross-training of\ndifferent mini-batches across GPUs. To simultaneously address these two\nproblems, in this paper, we propose an optimizer-dependent weight prediction\nstrategy (a.k.a PipeOptim) for asynchronous pipeline training. The key insight\nof our proposal is that we employ a weight prediction strategy in the forward\npass to ensure that each mini-batch uses consistent and staleness-free weights\nto compute the forward pass. To be concrete, we first construct the weight\nprediction scheme based on the update rule of the used optimizer when training\nthe deep neural network models. Then throughout the \"1F1B\" pipelined training,\neach mini-batch is mandated to execute weight prediction ahead of the forward\npass, subsequently employing the predicted weights to perform the forward pass.\nAs a result, PipeOptim 1) inherits the advantage of the \"1F1B\" schedule and\ngenerates pretty high throughput, and 2) can ensure effective parameter\nlearning regardless of the type of the used optimizer. To verify the\neffectiveness of our proposal, we conducted extensive experimental evaluations\nusing eight different deep-learning models spanning three machine-learning\ntasks including image classification, sentiment analysis, and machine\ntranslation. The experiment results demonstrate that PipeOptim outperforms the\npopular pipelined approaches including GPipe, PipeDream, PipeDream-2BW, and\nSpecTrain. The code of PipeOptim can be accessible at\nhttps://github.com/guanleics/PipeOptim.",
            "author": [
                "Lei Guan",
                "Dongsheng Li",
                "Jiye Liang",
                "Wenjian Wang",
                "Xicheng Lu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00839v2",
                "http://arxiv.org/pdf/2312.00839v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00279v1",
            "title": "Age-Based Scheduling for Mobile Edge Computing: A Deep Reinforcement\n  Learning Approach",
            "updated": "2023-12-01T01:30:49Z",
            "published": "2023-12-01T01:30:49Z",
            "summary": "With the rapid development of Mobile Edge Computing (MEC), various real-time\napplications have been deployed to benefit people's daily lives. The\nperformance of these applications relies heavily on the freshness of collected\nenvironmental information, which can be quantified by its Age of Information\n(AoI). In the traditional definition of AoI, it is assumed that the status\ninformation can be actively sampled and directly used. However, for many\nMEC-enabled applications, the desired status information is updated in an\nevent-driven manner and necessitates data processing. To better serve these\napplications, we propose a new definition of AoI and, based on the redefined\nAoI, we formulate an online AoI minimization problem for MEC systems. Notably,\nthe problem can be interpreted as a Markov Decision Process (MDP), thus\nenabling its solution through Reinforcement Learning (RL) algorithms.\nNevertheless, the traditional RL algorithms are designed for MDPs with\ncompletely unknown system dynamics and hence usually suffer long convergence\ntimes. To accelerate the learning process, we introduce Post-Decision States\n(PDSs) to exploit the partial knowledge of the system's dynamics. We also\ncombine PDSs with deep RL to further improve the algorithm's applicability,\nscalability, and robustness. Numerical results demonstrate that our algorithm\noutperforms the benchmarks under various scenarios.",
            "author": [
                "Xingqiu He",
                "Chaoqun You",
                "Tony Q. S. Quek"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00279v1",
                "http://arxiv.org/pdf/2312.00279v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.NI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00277v1",
            "title": "Text Attribute Control via Closed-Loop Disentanglement",
            "updated": "2023-12-01T01:26:38Z",
            "published": "2023-12-01T01:26:38Z",
            "summary": "Changing an attribute of a text without changing the content usually requires\nto first disentangle the text into irrelevant attributes and content\nrepresentations. After that, in the inference phase, the representation of one\nattribute is tuned to a different value, expecting that the corresponding\nattribute of the text can also be changed accordingly. The usual way of\ndisentanglement is to add some constraints on the latent space of an\nencoder-decoder architecture, including adversarial-based constraints and\nmutual-information-based constraints. However, the previous semi-supervised\nprocesses of attribute change are usually not enough to guarantee the success\nof attribute change and content preservation. In this paper, we propose a novel\napproach to achieve a robust control of attributes while enhancing content\npreservation. In this approach, we use a semi-supervised contrastive learning\nmethod to encourage the disentanglement of attributes in latent spaces.\nDifferently from previous works, we re-disentangle the reconstructed sentence\nand compare the re-disentangled latent space with the original latent space,\nwhich makes a closed-loop disentanglement process. This also helps content\npreservation. In addition, the contrastive learning method is also able to\nreplace the role of minimizing mutual information and adversarial training in\nthe disentanglement process, which alleviates the computation cost. We\nconducted experiments on three text datasets, including the Yelp Service review\ndataset, the Amazon Product review dataset, and the GoEmotions dataset. The\nexperimental results show the effectiveness of our model.",
            "author": [
                "Lei Sha",
                "Thomas Lukasiewicz"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00277v1",
                "http://arxiv.org/pdf/2312.00277v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00276v1",
            "title": "Automating Continual Learning",
            "updated": "2023-12-01T01:25:04Z",
            "published": "2023-12-01T01:25:04Z",
            "summary": "General-purpose learning systems should improve themselves in open-ended\nfashion in ever-changing environments. Conventional learning algorithms for\nneural networks, however, suffer from catastrophic forgetting (CF) --\npreviously acquired skills are forgotten when a new task is learned. Instead of\nhand-crafting new algorithms for avoiding CF, we propose Automated Continual\nLearning (ACL) to train self-referential neural networks to meta-learn their\nown in-context continual (meta-)learning algorithms. ACL encodes all desiderata\n-- good performance on both old and new tasks -- into its meta-learning\nobjectives. Our experiments demonstrate that ACL effectively solves \"in-context\ncatastrophic forgetting\"; our ACL-learned algorithms outperform hand-crafted\nones, e.g., on the Split-MNIST benchmark in the replay-free setting, and\nenables continual learning of diverse tasks consisting of multiple few-shot and\nstandard image classification datasets.",
            "author": [
                "Kazuki Irie",
                "R\u00f3bert Csord\u00e1s",
                "J\u00fcrgen Schmidhuber"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00276v1",
                "http://arxiv.org/pdf/2312.00276v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00837v1",
            "title": "An Adaptive Correspondence Scoring Framework for Unsupervised Image\n  Registration of Medical Images",
            "updated": "2023-12-01T01:11:22Z",
            "published": "2023-12-01T01:11:22Z",
            "summary": "We propose an adaptive training scheme for unsupervised medical image\nregistration. Existing methods rely on image reconstruction as the primary\nsupervision signal. However, nuisance variables (e.g. noise and covisibility)\noften cause the loss of correspondence between medical images, violating the\nLambertian assumption in physical waves (e.g. ultrasound) and consistent\nimaging acquisition. As the unsupervised learning scheme relies on intensity\nconstancy to establish correspondence between images for reconstruction, this\nintroduces spurious error residuals that are not modeled by the typical\ntraining objective. To mitigate this, we propose an adaptive framework that\nre-weights the error residuals with a correspondence scoring map during\ntraining, preventing the parametric displacement estimator from drifting away\ndue to noisy gradients, which leads to performance degradations. To illustrate\nthe versatility and effectiveness of our method, we tested our framework on\nthree representative registration architectures across three medical image\ndatasets along with other baselines. Our proposed adaptive framework\nconsistently outperforms other methods both quantitatively and qualitatively.\nPaired t-tests show that our improvements are statistically significant. The\ncode will be publicly available at\n\\url{https://voldemort108x.github.io/AdaCS/}.",
            "author": [
                "Xiaoran Zhang",
                "John C. Stendahl",
                "Lawrence Staib",
                "Albert J. Sinusas",
                "Alex Wong",
                "James S. Duncan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00837v1",
                "http://arxiv.org/pdf/2312.00837v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00271v2",
            "title": "Towards Clinical Prediction with Transparency: An Explainable AI\n  Approach to Survival Modelling in Residential Aged Care",
            "updated": "2023-12-07T02:49:11Z",
            "published": "2023-12-01T01:11:16Z",
            "summary": "Background: Accurate survival time estimates aid end-of-life medical\ndecision-making. Objectives: Develop an interpretable survival model for\nelderly residential aged care residents using advanced machine learning.\nSetting: A major Australasian residential aged care provider. Participants:\nResidents aged 65+ admitted for long-term care from July 2017 to August 2023.\nSample size: 11,944 residents across 40 facilities. Predictors: Factors include\nage, gender, health status, co-morbidities, cognitive function, mood,\nnutrition, mobility, smoking, sleep, skin integrity, and continence. Outcome:\nProbability of survival post-admission, specifically calibrated for 6-month\nsurvival estimates. Statistical Analysis: Tested CoxPH, EN, RR, Lasso, GB, XGB,\nand RF models in 20 experiments with a 90/10 train/test split. Evaluated\naccuracy using C-index, Harrell's C-index, dynamic AUROC, IBS, and calibrated\nROC. Chose XGB for its performance and calibrated it for 1, 3, 6, and 12-month\npredictions using Platt scaling. Employed SHAP values to analyze predictor\nimpacts. Results: GB, XGB, and RF models showed the highest C-Index values\n(0.714, 0.712, 0.712). The optimal XGB model demonstrated a 6-month survival\nprediction AUROC of 0.746 (95% CI 0.744-0.749). Key mortality predictors\ninclude age, male gender, mobility, health status, pressure ulcer risk, and\nappetite. Conclusions: The study successfully applies machine learning to\ncreate a survival model for aged care, aligning with clinical insights on\nmortality risk factors and enhancing model interpretability and clinical\nutility through explainable AI.",
            "author": [
                "Teo Susnjak",
                "Elise Griffin",
                "Mitchell McCutcheon",
                "Kathleen Potter"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00271v2",
                "http://arxiv.org/pdf/2312.00271v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00268v1",
            "title": "Academic competitions",
            "updated": "2023-12-01T01:01:04Z",
            "published": "2023-12-01T01:01:04Z",
            "summary": "Academic challenges comprise effective means for (i) advancing the state of\nthe art, (ii) putting in the spotlight of a scientific community specific\ntopics and problems, as well as (iii) closing the gap for under represented\ncommunities in terms of accessing and participating in the shaping of research\nfields. Competitions can be traced back for centuries and their achievements\nhave had great influence in our modern world. Recently, they (re)gained\npopularity, with the overwhelming amounts of data that is being generated in\ndifferent domains, as well as the need of pushing the barriers of existing\nmethods, and available tools to handle such data. This chapter provides a\nsurvey of academic challenges in the context of machine learning and related\nfields. We review the most influential competitions in the last few years and\nanalyze challenges per area of knowledge. The aims of scientific challenges,\ntheir goals, major achievements and expectations for the next few years are\nreviewed.",
            "author": [
                "Hugo Jair Escalante",
                "Aleksandra Kruchinina"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00268v1",
                "http://arxiv.org/pdf/2312.00268v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00267v1",
            "title": "Sample Efficient Reinforcement Learning from Human Feedback via Active\n  Exploration",
            "updated": "2023-12-01T00:54:02Z",
            "published": "2023-12-01T00:54:02Z",
            "summary": "Preference-based feedback is important for many applications in reinforcement\nlearning where direct evaluation of a reward function is not feasible. A\nnotable recent example arises in reinforcement learning from human feedback\n(RLHF) on large language models. For many applications of RLHF, the cost of\nacquiring the human feedback can be substantial. In this work, we take\nadvantage of the fact that one can often choose contexts at which to obtain\nhuman feedback in order to most efficiently identify a good policy, and\nformalize this as an offline contextual dueling bandit problem. We give an\nupper-confidence-bound style algorithm for this problem and prove a polynomial\nworst-case regret bound. We then provide empirical confirmation in a synthetic\nsetting that our approach outperforms existing methods. After, we extend the\nsetting and methodology for practical use in RLHF training of large language\nmodels. Here, our method is able to reach better performance with fewer samples\nof human preferences than multiple baselines on three real-world datasets.",
            "author": [
                "Viraj Mehta",
                "Vikramjeet Das",
                "Ojash Neopane",
                "Yijia Dai",
                "Ilija Bogunovic",
                "Jeff Schneider",
                "Willie Neiswanger"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00267v1",
                "http://arxiv.org/pdf/2312.00267v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00265v1",
            "title": "RoboSync: OS for Social Robots with Customizable Behaviour",
            "updated": "2023-12-01T00:47:44Z",
            "published": "2023-12-01T00:47:44Z",
            "summary": "Traditional robotic systems require complex implementations that are not\nalways accessible or easy to use for Human-Robot Interaction (HRI) application\ndevelopers. With the aim of simplifying the implementation of HRI applications,\nthis paper introduces a novel real-time operating system (RTOS) designed for\ncustomizable HRI - RoboSync. By creating multi-level abstraction layers, the\nsystem enables users to define complex emotional and behavioral models without\nneeding deep technical expertise. The system's modular architecture comprises a\nbehavior modeling layer, a machine learning plugin configuration layer, a\nsensor checks customization layer, a scheduler that fits the need of HRI, and a\ncommunication and synchronization layer. This approach not only promotes ease\nof use without highly specialized skills but also ensures real-time\nresponsiveness and adaptability. The primary functionality of the RTOS has been\nimplemented for proof of concept and was tested on a CortexM4 microcontroller,\ndemonstrating its potential for a wide range of lightweight simple-to-implement\nsocial robotics applications.",
            "author": [
                "Cheng Tang",
                "Yijing Feng",
                "Yue Hu"
            ],
            "link": [
                "http://dx.doi.org/10.1007/978-981-99-8718-4_18",
                "http://arxiv.org/abs/2312.00265v1",
                "http://arxiv.org/pdf/2312.00265v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00260v1",
            "title": "Quantum Multiple Kernel Learning in Financial Classification Tasks",
            "updated": "2023-12-01T00:18:43Z",
            "published": "2023-12-01T00:18:43Z",
            "summary": "Financial services is a prospect industry where unlocked near-term quantum\nutility could yield profitable potential, and, in particular, quantum machine\nlearning algorithms could potentially benefit businesses by improving the\nquality of predictive models. Quantum kernel methods have demonstrated success\nin financial, binary classification tasks, like fraud detection, and avoid\nissues found in variational quantum machine learning approaches. However,\nchoosing a suitable quantum kernel for a classical dataset remains a challenge.\nWe propose a hybrid, quantum multiple kernel learning (QMKL) methodology that\ncan improve classification quality over a single kernel approach. We test the\nrobustness of QMKL on several financially relevant datasets using both fidelity\nand projected quantum kernel approaches. We further demonstrate QMKL on quantum\nhardware using an error mitigation pipeline and show the benefits of QMKL in\nthe large qubit regime.",
            "author": [
                "Shungo Miyabe",
                "Brian Quanz",
                "Noriaki Shimada",
                "Abhijit Mitra",
                "Takahiro Yamamoto",
                "Vladimir Rastunkov",
                "Dimitris Alevras",
                "Mekena Metcalf",
                "Daniel J. M. King",
                "Mohammad Mamouei",
                "Matthew D. Jackson",
                "Martin Brown",
                "Philip Intallura",
                "Jae-Eun Park"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00260v1",
                "http://arxiv.org/pdf/2312.00260v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00252v1",
            "title": "PyNeRF: Pyramidal Neural Radiance Fields",
            "updated": "2023-11-30T23:52:46Z",
            "published": "2023-11-30T23:52:46Z",
            "summary": "Neural Radiance Fields (NeRFs) can be dramatically accelerated by spatial\ngrid representations. However, they do not explicitly reason about scale and so\nintroduce aliasing artifacts when reconstructing scenes captured at different\ncamera distances. Mip-NeRF and its extensions propose scale-aware renderers\nthat project volumetric frustums rather than point samples but such approaches\nrely on positional encodings that are not readily compatible with grid methods.\nWe propose a simple modification to grid-based models by training model heads\nat different spatial grid resolutions. At render time, we simply use coarser\ngrids to render samples that cover larger volumes. Our method can be easily\napplied to existing accelerated NeRF methods and significantly improves\nrendering quality (reducing error rates by 20-90% across synthetic and\nunbounded real-world scenes) while incurring minimal performance overhead (as\neach model head is quick to evaluate). Compared to Mip-NeRF, we reduce error\nrates by 20% while training over 60x faster.",
            "author": [
                "Haithem Turki",
                "Michael Zollh\u00f6fer",
                "Christian Richardt",
                "Deva Ramanan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00252v1",
                "http://arxiv.org/pdf/2312.00252v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00249v1",
            "title": "Acoustic Prompt Tuning: Empowering Large Language Models with Audition\n  Capabilities",
            "updated": "2023-11-30T23:43:59Z",
            "published": "2023-11-30T23:43:59Z",
            "summary": "The auditory system plays a substantial role in shaping the overall human\nperceptual experience. While prevailing large language models (LLMs) and visual\nlanguage models (VLMs) have shown their promise in solving a wide variety of\nvision and language understanding tasks, only a few of them can be generalised\nto the audio domain without compromising their domain-specific capacity. In\nthis work, we introduce Acoustic Prompt Turning (APT), a new adapter extending\nLLMs and VLMs to the audio domain by soft prompting only. Specifically, APT\napplies an instruction-aware audio aligner to generate soft prompts,\nconditioned on both input text and sounds, as language model inputs. To\nmitigate the data scarcity in the audio domain, a multi-task learning strategy\nis proposed by formulating diverse audio tasks in a sequence-to-sequence\nmanner. Moreover, we improve the framework of audio language model by using\ninterleaved audio-text embeddings as the input sequence. This improved\nframework imposes zero constraints on the input format and thus is capable of\ntackling more understanding tasks, such as few-shot audio classification and\naudio reasoning. To further evaluate the reasoning ability of audio networks,\nwe propose natural language audio reasoning (NLAR), a new task that analyses\nacross two audio clips by comparison and summarization. Experiments show that\nAPT-enhanced LLMs (namely APT-LLMs) achieve competitive results compared to the\nexpert models (i.e., the networks trained on the targeted datasets) across\nvarious tasks. We finally demonstrate the APT's ability in extending frozen\nVLMs to the audio domain without finetuning, achieving promising results in the\naudio-visual question and answering task. Our code and model weights are\nreleased at https://github.com/JinhuaLiang/APT.",
            "author": [
                "Jinhua Liang",
                "Xubo Liu",
                "Wenwu Wang",
                "Mark D. Plumbley",
                "Huy Phan",
                "Emmanouil Benetos"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00249v1",
                "http://arxiv.org/pdf/2312.00249v1"
            ],
            "primary_category": "eess.AS",
            "category": [
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00246v1",
            "title": "Curvature Explains Loss of Plasticity",
            "updated": "2023-11-30T23:24:45Z",
            "published": "2023-11-30T23:24:45Z",
            "summary": "Loss of plasticity is a phenomenon in which neural networks lose their\nability to learn from new experience. Despite being empirically observed in\nseveral problem settings, little is understood about the mechanisms that lead\nto loss of plasticity. In this paper, we offer a consistent explanation for\nplasticity loss, based on an assertion that neural networks lose directions of\ncurvature during training and that plasticity loss can be attributed to this\nreduction in curvature. To support such a claim, we provide a systematic\nempirical investigation of plasticity loss across several continual supervised\nlearning problems. Our findings illustrate that curvature loss coincides with\nand sometimes precedes plasticity loss, while also showing that previous\nexplanations are insufficient to explain loss of plasticity in all settings.\nLastly, we show that regularizers which mitigate loss of plasticity also\npreserve curvature, motivating a simple distributional regularizer that proves\nto be effective across the problem settings considered.",
            "author": [
                "Alex Lewandowski",
                "Haruto Tanaka",
                "Dale Schuurmans",
                "Marlos C. Machado"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00246v1",
                "http://arxiv.org/pdf/2312.00246v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00245v1",
            "title": "SPAM: Secure & Private Aircraft Management",
            "updated": "2023-11-30T23:16:45Z",
            "published": "2023-11-30T23:16:45Z",
            "summary": "With the rising use of aircrafts for operations ranging from disaster-relief\nto warfare, there is a growing risk of adversarial attacks. Malicious entities\noften only require the location of the aircraft for these attacks. Current\nsatellite-aircraft communication and tracking protocols put aircrafts at risk\nif the satellite is compromised, due to computation being done in plaintext. In\nthis work, we present \\texttt{SPAM}, a private, secure, and accurate system\nthat allows satellites to efficiently manage and maintain tracking angles for\naircraft fleets without learning aircrafts' locations. \\texttt{SPAM} is built\nupon multi-party computation and zero-knowledge proofs to guarantee privacy and\nhigh efficiency. While catered towards aircrafts, \\texttt{SPAM}'s\nzero-knowledge fleet management can be easily extended to the IoT, with very\nlittle overhead.",
            "author": [
                "Yaman Jandali",
                "Nojan Sheybani",
                "Farinaz Koushanfar"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00245v1",
                "http://arxiv.org/pdf/2312.00245v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00243v1",
            "title": "Low Revenue in Display Ad Auctions: Algorithmic Collusion vs.\n  Non-Quasilinear Preferences",
            "updated": "2023-11-30T23:11:33Z",
            "published": "2023-11-30T23:11:33Z",
            "summary": "The transition of display ad exchanges from second-price to first-price\nauctions has raised questions about its impact on revenue. Evaluating this\nshift empirically proves challenging. One key factor is the behavior of\nautomated bidding agents, who are unlikely to use static game-theoretical\nequilibrium strategies instead of favoring dynamic realms that continuously\nadapt and learn independently through the process of exploration and\nexploitation. Thus revenue equivalence between first- and second-price auctions\nmight not hold. Research on algorithmic collusion in display ad auctions found\nrevenue differences between second-price and first-price auctions. First-price\nauctions can induce Q-learning agents to tacitly collude below the Nash\nequilibrium in repeated complete-information auctions with payoff-maximizing\nagents (i.e., agents maximizing value minus price). Our analysis explores\nwide-spread online learning algorithms' convergence behavior in both complete\nand incomplete information models, but does not find a systematic deviance from\nequilibrium behavior. Convergence for Q-learning depends on hyperparameters and\ninitializations, and algorithmic collusion vanishes when competing against\nother learning algorithms. Apart from their learning behavior, the objectives\nreported in the literature extend payoff maximization, often focusing on\nreturn-on-investment or return-on-spend. We derive equilibrium bid functions\nfor such utility models, revealing that revenue equivalence doesn't hold. In\nlow-competition scenarios, the first-price auction often yields lower revenue\nthan the second-price counterpart. These insights offer an alternative\nrationale for the potential revenue decrease in first-price auctions.\nUnderstanding the intricate interplay of auction rules, learning algorithms,\nand utility models is crucial in maximizing revenue in the ever-evolving world\nof display ad exchanges.",
            "author": [
                "Martin Bichler",
                "Alok Gupta",
                "Laura Mathews",
                "Matthias Oberlechner"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00243v1",
                "http://arxiv.org/pdf/2312.00243v1"
            ],
            "primary_category": "cs.GT",
            "category": [
                "cs.GT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00834v1",
            "title": "AV-RIR: Audio-Visual Room Impulse Response Estimation",
            "updated": "2023-11-30T22:58:30Z",
            "published": "2023-11-30T22:58:30Z",
            "summary": "Accurate estimation of Room Impulse Response (RIR), which captures an\nenvironment's acoustic properties, is important for speech processing and AR/VR\napplications. We propose AV-RIR, a novel multi-modal multi-task learning\napproach to accurately estimate the RIR from a given reverberant speech signal\nand the visual cues of its corresponding environment. AV-RIR builds on a novel\nneural codec-based architecture that effectively captures environment geometry\nand materials properties and solves speech dereverberation as an auxiliary task\nby using multi-task learning. We also propose Geo-Mat features that augment\nmaterial information into visual cues and CRIP that improves late reverberation\ncomponents in the estimated RIR via image-to-RIR retrieval by 86%. Empirical\nresults show that AV-RIR quantitatively outperforms previous audio-only and\nvisual-only approaches by achieving 36% - 63% improvement across various\nacoustic metrics in RIR estimation. Additionally, it also achieves higher\npreference scores in human evaluation. As an auxiliary benefit, dereverbed\nspeech from AV-RIR shows competitive performance with the state-of-the-art in\nvarious spoken language processing tasks and outperforms reverberation time\nerror score in the real-world AVSpeech dataset. Qualitative examples of both\nsynthesized reverberant speech and enhanced speech can be found at\nhttps://www.youtube.com/watch?v=tTsKhviukAE.",
            "author": [
                "Anton Ratnarajah",
                "Sreyan Ghosh",
                "Sonal Kumar",
                "Purva Chiniya",
                "Dinesh Manocha"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00834v1",
                "http://arxiv.org/pdf/2312.00834v1"
            ],
            "primary_category": "cs.SD",
            "category": [
                "cs.SD",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00833v1",
            "title": "Lasagna: Layered Score Distillation for Disentangled Object Relighting",
            "updated": "2023-11-30T22:54:41Z",
            "published": "2023-11-30T22:54:41Z",
            "summary": "Professional artists, photographers, and other visual content creators use\nobject relighting to establish their photo's desired effect. Unfortunately,\nmanual tools that allow relighting have a steep learning curve and are\ndifficult to master. Although generative editing methods now enable some forms\nof image editing, relighting is still beyond today's capabilities; existing\nmethods struggle to keep other aspects of the image -- colors, shapes, and\ntextures -- consistent after the edit. We propose Lasagna, a method that\nenables intuitive text-guided relighting control. Lasagna learns a lighting\nprior by using score distillation sampling to distill the prior of a diffusion\nmodel, which has been finetuned on synthetic relighting data. To train Lasagna,\nwe curate a new synthetic dataset ReLiT, which contains 3D object assets re-lit\nfrom multiple light source locations. Despite training on synthetic images,\nquantitative results show that Lasagna relights real-world images while\npreserving other aspects of the input image, outperforming state-of-the-art\ntext-guided image editing methods. Lasagna enables realistic and controlled\nresults on natural images and digital art pieces and is preferred by humans\nover other methods in over 91% of cases. Finally, we demonstrate the\nversatility of our learning objective by extending it to allow colorization,\nanother form of image editing.",
            "author": [
                "Dina Bashkirova",
                "Arijit Ray",
                "Rupayan Mallick",
                "Sarah Adel Bargal",
                "Jianming Zhang",
                "Ranjay Krishna",
                "Kate Saenko"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00833v1",
                "http://arxiv.org/pdf/2312.00833v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00238v1",
            "title": "Self-similarity of Communities of the ABCD Model",
            "updated": "2023-11-30T22:52:39Z",
            "published": "2023-11-30T22:52:39Z",
            "summary": "The Artificial Benchmark for Community Detection (ABCD) graph is a random\ngraph model with community structure and power-law distribution for both\ndegrees and community sizes. The model generates graphs similar to the\nwell-known LFR model but it is faster and can be investigated analytically.\n  In this paper, we show that the ABCD model exhibits some interesting\nself-similar behaviour, namely, the degree distribution of ground-truth\ncommunities is asymptotically the same as the degree distribution of the whole\ngraph (appropriately normalized based on their sizes). As a result, we can not\nonly estimate the number of edges induced by each community but also the number\nof self-loops and multi-edges generated during the process. Understanding these\nquantities is important as (a) rewiring self-loops and multi-edges to keep the\ngraph simple is an expensive part of the algorithm, and (b) every rewiring\ncauses the underlying configuration models to deviate slightly from uniform\nsimple graphs on their corresponding degree sequences.",
            "author": [
                "Jordan Barrett",
                "Bogumil Kaminski",
                "Pawel Pralat",
                "Francois Theberge"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00238v1",
                "http://arxiv.org/pdf/2312.00238v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI",
                "cs.DM",
                "cs.LG",
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00237v1",
            "title": "Negotiated Representations to Prevent Forgetting in Machine Learning\n  Applications",
            "updated": "2023-11-30T22:43:50Z",
            "published": "2023-11-30T22:43:50Z",
            "summary": "Catastrophic forgetting is a significant challenge in the field of machine\nlearning, particularly in neural networks. When a neural network learns to\nperform well on a new task, it often forgets its previously acquired knowledge\nor experiences. This phenomenon occurs because the network adjusts its weights\nand connections to minimize the loss on the new task, which can inadvertently\noverwrite or disrupt the representations that were crucial for the previous\ntasks. As a result, the the performance of the network on earlier tasks\ndeteriorates, limiting its ability to learn and adapt to a sequence of tasks.\nIn this paper, we propose a novel method for preventing catastrophic forgetting\nin machine learning applications, specifically focusing on neural networks. Our\napproach aims to preserve the knowledge of the network across multiple tasks\nwhile still allowing it to learn new information effectively. We demonstrate\nthe effectiveness of our method by conducting experiments on various benchmark\ndatasets, including Split MNIST, Split CIFAR10, Split Fashion MNIST, and Split\nCIFAR100. These datasets are created by dividing the original datasets into\nseparate, non overlapping tasks, simulating a continual learning scenario where\nthe model needs to learn multiple tasks sequentially without forgetting the\nprevious ones. Our proposed method tackles the catastrophic forgetting problem\nby incorporating negotiated representations into the learning process, which\nallows the model to maintain a balance between retaining past experiences and\nadapting to new tasks. By evaluating our method on these challenging datasets,\nwe aim to showcase its potential for addressing catastrophic forgetting and\nimproving the performance of neural networks in continual learning settings.",
            "author": [
                "Nuri Korhan",
                "Ceren \u00d6ner"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00237v1",
                "http://arxiv.org/pdf/2312.00237v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00236v1",
            "title": "Brainformer: Modeling MRI Brain Functions to Machine Vision",
            "updated": "2023-11-30T22:39:23Z",
            "published": "2023-11-30T22:39:23Z",
            "summary": "\"Perception is reality\". Human perception plays a vital role in forming\nbeliefs and understanding reality. Exploring how the human brain works in the\nvisual system facilitates bridging the gap between human visual perception and\ncomputer vision models. However, neuroscientists study the brain via\nNeuroimaging, i.e., Functional Magnetic Resonance Imaging (fMRI), to discover\nthe brain's functions. These approaches face interpretation challenges where\nfMRI data can be complex and require expertise. Therefore, neuroscientists make\ninferences about cognitive processes based on patterns of brain activities,\nwhich can lead to potential misinterpretation or limited functional\nunderstanding. In this work, we first present a simple yet effective\nBrainformer approach, a novel Transformer-based framework, to analyze the\npatterns of fMRI in the human perception system from the machine learning\nperspective. Secondly, we introduce a novel mechanism incorporating fMRI, which\nrepresents the human brain activities, as the supervision for the machine\nvision model. This work also introduces a novel perspective on transferring\nknowledge from human perception to neural networks. Through our experiments, we\ndemonstrated that by leveraging fMRI information, the machine vision model can\nachieve potential results compared to the current State-of-the-art methods in\nvarious image recognition tasks.",
            "author": [
                "Xuan-Bac Nguyen",
                "Xin Li",
                "Samee U. Khan",
                "Khoa Luu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00236v1",
                "http://arxiv.org/pdf/2312.00236v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00234v1",
            "title": "Deep Equilibrium Based Neural Operators for Steady-State PDEs",
            "updated": "2023-11-30T22:34:57Z",
            "published": "2023-11-30T22:34:57Z",
            "summary": "Data-driven machine learning approaches are being increasingly used to solve\npartial differential equations (PDEs). They have shown particularly striking\nsuccesses when training an operator, which takes as input a PDE in some family,\nand outputs its solution. However, the architectural design space, especially\ngiven structural knowledge of the PDE family of interest, is still poorly\nunderstood. We seek to remedy this gap by studying the benefits of weight-tied\nneural network architectures for steady-state PDEs. To achieve this, we first\ndemonstrate that the solution of most steady-state PDEs can be expressed as a\nfixed point of a non-linear operator. Motivated by this observation, we propose\nFNO-DEQ, a deep equilibrium variant of the FNO architecture that directly\nsolves for the solution of a steady-state PDE as the infinite-depth fixed point\nof an implicit operator layer using a black-box root solver and differentiates\nanalytically through this fixed point resulting in $\\mathcal{O}(1)$ training\nmemory. Our experiments indicate that FNO-DEQ-based architectures outperform\nFNO-based baselines with $4\\times$ the number of parameters in predicting the\nsolution to steady-state PDEs such as Darcy Flow and steady-state\nincompressible Navier-Stokes. Finally, we show FNO-DEQ is more robust when\ntrained with datasets with more noisy observations than the FNO-based\nbaselines, demonstrating the benefits of using appropriate inductive biases in\narchitectural design for different neural network based PDE solvers. Further,\nwe show a universal approximation result that demonstrates that FNO-DEQ can\napproximate the solution to any steady-state PDE that can be written as a fixed\npoint equation.",
            "author": [
                "Tanya Marwah",
                "Ashwini Pokle",
                "J. Zico Kolter",
                "Zachary C. Lipton",
                "Jianfeng Lu",
                "Andrej Risteski"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00234v1",
                "http://arxiv.org/pdf/2312.00234v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.NA",
                "math.NA",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00232v1",
            "title": "Uncertainty in Graph Contrastive Learning with Bayesian Neural Networks",
            "updated": "2023-11-30T22:32:24Z",
            "published": "2023-11-30T22:32:24Z",
            "summary": "Graph contrastive learning has shown great promise when labeled data is\nscarce, but large unlabeled datasets are available. However, it often does not\ntake uncertainty estimation into account. We show that a variational Bayesian\nneural network approach can be used to improve not only the uncertainty\nestimates but also the downstream performance on semi-supervised\nnode-classification tasks. Moreover, we propose a new measure of uncertainty\nfor contrastive learning, that is based on the disagreement in likelihood due\nto different positive samples.",
            "author": [
                "Alexander M\u00f6llers",
                "Alexander Immer",
                "Elvin Isufi",
                "Vincent Fortuin"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00232v1",
                "http://arxiv.org/pdf/2312.00232v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00231v1",
            "title": "Learning domain-invariant classifiers for infant cry sounds",
            "updated": "2023-11-30T22:27:57Z",
            "published": "2023-11-30T22:27:57Z",
            "summary": "The issue of domain shift remains a problematic phenomenon in most real-world\ndatasets and clinical audio is no exception. In this work, we study the nature\nof domain shift in a clinical database of infant cry sounds acquired across\ndifferent geographies. We find that though the pitches of infant cries are\nsimilarly distributed regardless of the place of birth, other characteristics\nintroduce peculiar biases into the data. We explore methodologies for\nmitigating the impact of domain shift in a model for identifying neurological\ninjury from cry sounds. We adapt unsupervised domain adaptation methods from\ncomputer vision which learn an audio representation that is domain-invariant to\nhospitals and is task discriminative. We also propose a new approach, target\nnoise injection (TNI), for unsupervised domain adaptation which requires\nneither labels nor training data from the target domain. Our best-performing\nmodel significantly improves target accuracy by 7.2%, without negatively\naffecting the source domain.",
            "author": [
                "Charles C. Onu",
                "Hemanth K. Sheetha",
                "Arsenii Gorin",
                "Doina Precup"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00231v1",
                "http://arxiv.org/pdf/2312.00231v1"
            ],
            "primary_category": "eess.AS",
            "category": [
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00224v1",
            "title": "Unsupervised textile defect detection using convolutional neural\n  networks",
            "updated": "2023-11-30T22:08:06Z",
            "published": "2023-11-30T22:08:06Z",
            "summary": "In this study, we propose a novel motif-based approach for unsupervised\ntextile anomaly detection that combines the benefits of traditional\nconvolutional neural networks with those of an unsupervised learning paradigm.\nIt consists of five main steps: preprocessing, automatic pattern period\nextraction, patch extraction, features selection and anomaly detection. This\nproposed approach uses a new dynamic and heuristic method for feature selection\nwhich avoids the drawbacks of initialization of the number of filters (neurons)\nand their weights, and those of the backpropagation mechanism such as the\nvanishing gradients, which are common practice in the state-of-the-art methods.\nThe design and training of the network are performed in a dynamic and input\ndomain-based manner and, thus, no ad-hoc configurations are required. Before\nbuilding the model, only the number of layers and the stride are defined. We do\nnot initialize the weights randomly nor do we define the filter size or number\nof filters as conventionally done in CNN-based approaches. This reduces effort\nand time spent on hyperparameter initialization and fine-tuning. Only one\ndefect-free sample is required for training and no further labeled data is\nneeded. The trained network is then used to detect anomalies on defective\nfabric samples. We demonstrate the effectiveness of our approach on the\nPatterned Fabrics benchmark dataset. Our algorithm yields reliable and\ncompetitive results (on recall, precision, accuracy and f1- measure) compared\nto state-of-the-art unsupervised approaches, in less time, with efficient\ntraining in a single epoch and a lower computational cost.",
            "author": [
                "Imane Koulali",
                "M. Taner Eskil"
            ],
            "link": [
                "http://dx.doi.org/10.1016/j.asoc.2021.107913",
                "http://arxiv.org/abs/2312.00224v1",
                "http://arxiv.org/pdf/2312.00224v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00223v1",
            "title": "Convolutional Neural Networks for Segmentation of Malignant Pleural\n  Mesothelioma: Analysis of Probability Map Thresholds (CALGB 30901, Alliance)",
            "updated": "2023-11-30T22:07:07Z",
            "published": "2023-11-30T22:07:07Z",
            "summary": "Malignant pleural mesothelioma (MPM) is the most common form of mesothelioma.\nTo assess response to treatment, tumor measurements are acquired and evaluated\nbased on a patient's longitudinal computed tomography (CT) scans. Tumor volume,\nhowever, is the more accurate metric for assessing tumor burden and response.\nAutomated segmentation methods using deep learning can be employed to acquire\nvolume, which otherwise is a tedious task performed manually. The deep\nlearning-based tumor volume and contours can then be compared with a standard\nreference to assess the robustness of the automated segmentations. The purpose\nof this study was to evaluate the impact of probability map threshold on MPM\ntumor delineations generated using a convolutional neural network (CNN).\nEighty-eight CT scans from 21 MPM patients were segmented by a VGG16/U-Net CNN.\nA radiologist modified the contours generated at a 0.5 probability threshold.\nPercent difference of tumor volume and overlap using the Dice Similarity\nCoefficient (DSC) were compared between the standard reference provided by the\nradiologist and CNN outputs for thresholds ranging from 0.001 to 0.9. CNN\nannotations consistently yielded smaller tumor volumes than radiologist\ncontours. Reducing the probability threshold from 0.5 to 0.1 decreased the\nabsolute percent volume difference, on average, from 43.96% to 24.18%. Median\nand mean DSC ranged from 0.58 to 0.60, with a peak at a threshold of 0.5; no\ndistinct threshold was found for percent volume difference. No single output\nthreshold in the CNN probability maps was optimal for both tumor volume and\nDSC. This work underscores the need to assess tumor volume and spatial overlap\nwhen evaluating CNN performance. While automated segmentations may yield\ncomparable tumor volumes to that of the reference standard, the spatial region\ndelineated by the CNN at a specific threshold is equally important.",
            "author": [
                "Mena Shenouda",
                "Eyj\u00f3lfur Gudmundsson",
                "Feng Li",
                "Christopher M. Straus",
                "Hedy L. Kindler",
                "Arkadiusz Z. Dudek",
                "Thomas Stinchcombe",
                "Xiaofei Wang",
                "Adam Starkey",
                "Samuel G. Armato III"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00223v1",
                "http://arxiv.org/pdf/2312.00223v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV",
                "physics.med-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00220v1",
            "title": "Multi-Modal Video Topic Segmentation with Dual-Contrastive Domain\n  Adaptation",
            "updated": "2023-11-30T21:59:05Z",
            "published": "2023-11-30T21:59:05Z",
            "summary": "Video topic segmentation unveils the coarse-grained semantic structure\nunderlying videos and is essential for other video understanding tasks. Given\nthe recent surge in multi-modal, relying solely on a single modality is\narguably insufficient. On the other hand, prior solutions for similar tasks\nlike video scene/shot segmentation cater to short videos with clear visual\nshifts but falter for long videos with subtle changes, such as livestreams. In\nthis paper, we introduce a multi-modal video topic segmenter that utilizes both\nvideo transcripts and frames, bolstered by a cross-modal attention mechanism.\nFurthermore, we propose a dual-contrastive learning framework adhering to the\nunsupervised domain adaptation paradigm, enhancing our model's adaptability to\nlonger, more semantically complex videos. Experiments on short and long video\ncorpora demonstrate that our proposed solution, significantly surpasses\nbaseline methods in terms of both accuracy and transferability, in both intra-\nand cross-domain settings.",
            "author": [
                "Linzi Xing",
                "Quan Tran",
                "Fabian Caba",
                "Franck Dernoncourt",
                "Seunghyun Yoon",
                "Zhaowen Wang",
                "Trung Bui",
                "Giuseppe Carenini"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00220v1",
                "http://arxiv.org/pdf/2312.00220v1"
            ],
            "primary_category": "cs.MM",
            "category": [
                "cs.MM",
                "cs.CL",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00215v1",
            "title": "Learning active tactile perception through belief-space control",
            "updated": "2023-11-30T21:54:42Z",
            "published": "2023-11-30T21:54:42Z",
            "summary": "Robots operating in an open world will encounter novel objects with unknown\nphysical properties, such as mass, friction, or size. These robots will need to\nsense these properties through interaction prior to performing downstream tasks\nwith the objects. We propose a method that autonomously learns tactile\nexploration policies by developing a generative world model that is leveraged\nto 1) estimate the object's physical parameters using a differentiable Bayesian\nfiltering algorithm and 2) develop an exploration policy using an\ninformation-gathering model predictive controller. We evaluate our method on\nthree simulated tasks where the goal is to estimate a desired object property\n(mass, height or toppling height) through physical interaction. We find that\nour method is able to discover policies that efficiently gather information\nabout the desired property in an intuitive manner. Finally, we validate our\nmethod on a real robot system for the height estimation task, where our method\nis able to successfully learn and execute an information-gathering policy from\nscratch.",
            "author": [
                "Jean-Fran\u00e7ois Tremblay",
                "David Meger",
                "Francois Hogan",
                "Gregory Dudek"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00215v1",
                "http://arxiv.org/pdf/2312.00215v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00209v2",
            "title": "On the Interplay Between Stepsize Tuning and Progressive Sharpening",
            "updated": "2023-12-07T08:57:50Z",
            "published": "2023-11-30T21:42:15Z",
            "summary": "Recent empirical work has revealed an intriguing property of deep learning\nmodels by which the sharpness (largest eigenvalue of the Hessian) increases\nthroughout optimization until it stabilizes around a critical value at which\nthe optimizer operates at the edge of stability, given a fixed stepsize (Cohen\net al, 2022). We investigate empirically how the sharpness evolves when using\nstepsize-tuners, the Armijo linesearch and Polyak stepsizes, that adapt the\nstepsize along the iterations to local quantities such as, implicitly, the\nsharpness itself. We find that the surprisingly poor performance of a classical\nArmijo linesearch may be well explained by its tendency to ever-increase the\nsharpness of the objective in the full or large batch regimes. On the other\nhand, we observe that Polyak stepsizes operate generally at the edge of\nstability or even slightly beyond, while outperforming its Armijo and constant\nstepsizes counterparts. We conclude with an analysis that suggests unlocking\nstepsize tuners requires an understanding of the joint dynamics of the step\nsize and the sharpness.",
            "author": [
                "Vincent Roulet",
                "Atish Agarwala",
                "Fabian Pedregosa"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00209v2",
                "http://arxiv.org/pdf/2312.00209v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00207v1",
            "title": "EpiTESTER: Testing Autonomous Vehicles with Epigenetic Algorithm and\n  Attention Mechanism",
            "updated": "2023-11-30T21:40:14Z",
            "published": "2023-11-30T21:40:14Z",
            "summary": "Testing autonomous vehicles (AVs) under various environmental scenarios that\nlead the vehicles to unsafe situations is known to be challenging. Given the\ninfinite possible environmental scenarios, it is essential to find critical\nscenarios efficiently. To this end, we propose a novel testing method, named\nEpiTESTER, by taking inspiration from epigenetics, which enables species to\nadapt to sudden environmental changes. In particular, EpiTESTER adopts gene\nsilencing as its epigenetic mechanism, which regulates gene expression to\nprevent the expression of a certain gene, and the probability of gene\nexpression is dynamically computed as the environment changes. Given different\ndata modalities (e.g., images, lidar point clouds) in the context of AV,\nEpiTESTER benefits from a multi-model fusion transformer to extract high-level\nfeature representations from environmental factors and then calculates\nprobabilities based on these features with the attention mechanism. To assess\nthe cost-effectiveness of EpiTESTER, we compare it with a classical genetic\nalgorithm (GA) (i.e., without any epigenetic mechanism implemented) and\nEpiTESTER with equal probability for each gene. We evaluate EpiTESTER with four\ninitial environments from CARLA, an open-source simulator for autonomous\ndriving research, and an end-to-end AV controller, Interfuser. Our results show\nthat EpiTESTER achieved a promising performance in identifying critical\nscenarios compared to the baselines, showing that applying epigenetic\nmechanisms is a good option for solving practical problems.",
            "author": [
                "Chengjie Lu",
                "Shaukat Ali",
                "Tao Yue"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00207v1",
                "http://arxiv.org/pdf/2312.00207v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE",
                "cs.LG",
                "cs.NE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00206v1",
            "title": "SparseGS: Real-Time 360\u00b0 Sparse View Synthesis using Gaussian\n  Splatting",
            "updated": "2023-11-30T21:38:22Z",
            "published": "2023-11-30T21:38:22Z",
            "summary": "The problem of novel view synthesis has grown significantly in popularity\nrecently with the introduction of Neural Radiance Fields (NeRFs) and other\nimplicit scene representation methods. A recent advance, 3D Gaussian Splatting\n(3DGS), leverages an explicit representation to achieve real-time rendering\nwith high-quality results. However, 3DGS still requires an abundance of\ntraining views to generate a coherent scene representation. In few shot\nsettings, similar to NeRF, 3DGS tends to overfit to training views, causing\nbackground collapse and excessive floaters, especially as the number of\ntraining views are reduced. We propose a method to enable training coherent\n3DGS-based radiance fields of 360 scenes from sparse training views. We find\nthat using naive depth priors is not sufficient and integrate depth priors with\ngenerative and explicit constraints to reduce background collapse, remove\nfloaters, and enhance consistency from unseen viewpoints. Experiments show that\nour method outperforms base 3DGS by up to 30.5% and NeRF-based methods by up to\n15.6% in LPIPS on the MipNeRF-360 dataset with substantially less training and\ninference cost.",
            "author": [
                "Haolin Xiong",
                "Sairisheek Muttukuru",
                "Rishi Upadhyay",
                "Pradyumna Chari",
                "Achuta Kadambi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00206v1",
                "http://arxiv.org/pdf/2312.00206v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00201v1",
            "title": "An integrated framework for developing and evaluating an automated\n  lecture style assessment system",
            "updated": "2023-11-30T21:31:21Z",
            "published": "2023-11-30T21:31:21Z",
            "summary": "The aim of the work presented in this paper is to develop and evaluate an\nintegrated system that provides automated lecture style evaluation, allowing\nteachers to get instant feedback related to the goodness of their lecturing\nstyle. The proposed system aims to promote improvement of lecture quality, that\ncould upgrade the overall student learning experience. The proposed application\nutilizes specific measurable biometric characteristics, such as facial\nexpressions, body activity, speech rate and intonation, hand movement, and\nfacial pose, extracted from a video showing the lecturer from the audience\npoint of view. Measurable biometric features extracted during a lecture are\ncombined to provide teachers with a score reflecting lecture style quality both\nat frame rate and by providing lecture quality metrics for the whole lecture.\nThe acceptance of the proposed lecture style evaluation system was evaluated by\nchief education officers, teachers and students regarding the functionality,\nusefulness of the application, and possible improvements. The results indicate\nthat participants found the application novel and useful in providing automated\nfeedback regarding lecture quality. Furthermore, the performance evaluation of\nthe proposed system was compared with the performance of humans in the task of\nlecture style evaluation. Results indicate that the proposed system not only\nachieves similar performance to human observers, but in some cases, it\noutperforms them.",
            "author": [
                "Eleni Dimitriadou",
                "Andreas Lanitis"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00201v1",
                "http://arxiv.org/pdf/2312.00201v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03748v1",
            "title": "Applying Large Language Models and Chain-of-Thought for Automatic\n  Scoring",
            "updated": "2023-11-30T21:22:43Z",
            "published": "2023-11-30T21:22:43Z",
            "summary": "This study investigates the application of large language models (LLMs),\nspecifically GPT-3.5 and GPT-4, with Chain-of-Though (CoT)in the automatic\nscoring of student-written responses to science assessments. We focused on\novercoming the challenges of accessibility, technical complexity, and lack of\nexplainability that have previously limited the use of automatic assessment\ntools among researchers and educators. We used a testing dataset comprising six\nassessment tasks (three binomial and three trinomial) with 1,650 student\nresponses. We employed six prompt engineering strategies, combining zero-shot\nor few-shot learning with CoT, either alone or alongside item stem and scoring\nrubrics. Results indicated that few-shot (acc = .67) outperformed zero-shot\nlearning (acc = .60), with 12.6\\% increase. CoT, when used without item stem\nand scoring rubrics, did not significantly affect scoring accuracy (acc = .60).\nHowever, CoT prompting paired with contextual item stems and rubrics proved to\nbe a significant contributor to scoring accuracy (13.44\\% increase for\nzero-shot; 3.7\\% increase for few-shot). Using a novel approach PPEAS, we found\na more balanced accuracy across different proficiency categories, highlighting\nthe importance of domain-specific reasoning in enhancing the effectiveness of\nLLMs in scoring tasks. Additionally, we also found that GPT-4 demonstrated\nsuperior performance over GPT-3.5 in various scoring tasks, showing 8.64\\%\ndifference. The study revealed that the single-call strategy with GPT-4,\nparticularly using greedy sampling, outperformed other approaches, including\nensemble voting strategies. This study demonstrates the potential of LLMs in\nfacilitating automatic scoring, emphasizing that CoT enhances accuracy,\nparticularly when used with item stem and scoring rubrics.",
            "author": [
                "Gyeong-Geon Lee",
                "Ehsan Latif",
                "Xuansheng Wu",
                "Ninghao Liu",
                "Xiaoming Zhai"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03748v1",
                "http://arxiv.org/pdf/2312.03748v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00198v1",
            "title": "Optimal Attack and Defense for Reinforcement Learning",
            "updated": "2023-11-30T21:21:47Z",
            "published": "2023-11-30T21:21:47Z",
            "summary": "To ensure the usefulness of Reinforcement Learning (RL) in real systems, it\nis crucial to ensure they are robust to noise and adversarial attacks. In\nadversarial RL, an external attacker has the power to manipulate the victim\nagent's interaction with the environment. We study the full class of online\nmanipulation attacks, which include (i) state attacks, (ii) observation attacks\n(which are a generalization of perceived-state attacks), (iii) action attacks,\nand (iv) reward attacks. We show the attacker's problem of designing a stealthy\nattack that maximizes its own expected reward, which often corresponds to\nminimizing the victim's value, is captured by a Markov Decision Process (MDP)\nthat we call a meta-MDP since it is not the true environment but a higher level\nenvironment induced by the attacked interaction. We show that the attacker can\nderive optimal attacks by planning in polynomial time or learning with\npolynomial sample complexity using standard RL techniques. We argue that the\noptimal defense policy for the victim can be computed as the solution to a\nstochastic Stackelberg game, which can be further simplified into a\npartially-observable turn-based stochastic game (POTBSG). Neither the attacker\nnor the victim would benefit from deviating from their respective optimal\npolicies, thus such solutions are truly robust. Although the defense problem is\nNP-hard, we show that optimal Markovian defenses can be computed (learned) in\npolynomial time (sample complexity) in many scenarios.",
            "author": [
                "Jeremy McMahan",
                "Young Wu",
                "Xiaojin Zhu",
                "Qiaomin Xie"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00198v1",
                "http://arxiv.org/pdf/2312.00198v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CR",
                "cs.GT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00194v1",
            "title": "Robust Concept Erasure via Kernelized Rate-Distortion Maximization",
            "updated": "2023-11-30T21:10:44Z",
            "published": "2023-11-30T21:10:44Z",
            "summary": "Distributed representations provide a vector space that captures meaningful\nrelationships between data instances. The distributed nature of these\nrepresentations, however, entangles together multiple attributes or concepts of\ndata instances (e.g., the topic or sentiment of a text, characteristics of the\nauthor (age, gender, etc), etc). Recent work has proposed the task of concept\nerasure, in which rather than making a concept predictable, the goal is to\nremove an attribute from distributed representations while retaining other\ninformation from the original representation space as much as possible. In this\npaper, we propose a new distance metric learning-based objective, the\nKernelized Rate-Distortion Maximizer (KRaM), for performing concept erasure.\nKRaM fits a transformation of representations to match a specified distance\nmeasure (defined by a labeled concept to erase) using a modified\nrate-distortion function. Specifically, KRaM's objective function aims to make\ninstances with similar concept labels dissimilar in the learned representation\nspace while retaining other information. We find that optimizing KRaM\neffectively erases various types of concepts: categorical, continuous, and\nvector-valued variables from data representations across diverse domains. We\nalso provide a theoretical analysis of several properties of KRaM's objective.\nTo assess the quality of the learned representations, we propose an alignment\nscore to evaluate their similarity with the original representation space.\nAdditionally, we conduct experiments to showcase KRaM's efficacy in various\nsettings, from erasing binary gender variables in word embeddings to\nvector-valued variables in GPT-3 representations.",
            "author": [
                "Somnath Basu Roy Chowdhury",
                "Nicholas Monath",
                "Avinava Dubey",
                "Amr Ahmed",
                "Snigdha Chaturvedi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00194v1",
                "http://arxiv.org/pdf/2312.00194v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00192v1",
            "title": "Benchmarking and Enhancing Disentanglement in Concept-Residual Models",
            "updated": "2023-11-30T21:07:26Z",
            "published": "2023-11-30T21:07:26Z",
            "summary": "Concept bottleneck models (CBMs) are interpretable models that first predict\na set of semantically meaningful features, i.e., concepts, from observations\nthat are subsequently used to condition a downstream task. However, the model's\nperformance strongly depends on the engineered features and can severely suffer\nfrom incomplete sets of concepts. Prior works have proposed a side channel -- a\nresidual -- that allows for unconstrained information flow to the downstream\ntask, thus improving model performance but simultaneously introducing\ninformation leakage, which is undesirable for interpretability. This work\nproposes three novel approaches to mitigate information leakage by\ndisentangling concepts and residuals, investigating the critical balance\nbetween model performance and interpretability. Through extensive empirical\nanalysis on the CUB, OAI, and CIFAR 100 datasets, we assess the performance of\neach disentanglement method and provide insights into when they work best.\nFurther, we show how each method impacts the ability to intervene over the\nconcepts and their subsequent impact on task performance.",
            "author": [
                "Renos Zabounidis",
                "Ini Oguntola",
                "Konghao Zhao",
                "Joseph Campbell",
                "Simon Stepputtis",
                "Katia Sycara"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00192v1",
                "http://arxiv.org/pdf/2312.00192v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00191v1",
            "title": "Enhancing Ligand Pose Sampling for Molecular Docking",
            "updated": "2023-11-30T21:02:37Z",
            "published": "2023-11-30T21:02:37Z",
            "summary": "Deep learning promises to dramatically improve scoring functions for\nmolecular docking, leading to substantial advances in binding pose prediction\nand virtual screening. To train scoring functions-and to perform molecular\ndocking-one must generate a set of candidate ligand binding poses.\nUnfortunately, the sampling protocols currently used to generate candidate\nposes frequently fail to produce any poses close to the correct, experimentally\ndetermined pose, unless information about the correct pose is provided. This\nlimits the accuracy of learned scoring functions and molecular docking. Here,\nwe describe two improved protocols for pose sampling: GLOW (auGmented sampLing\nwith sOftened vdW potential) and a novel technique named IVES (IteratiVe\nEnsemble Sampling). Our benchmarking results demonstrate the effectiveness of\nour methods in improving the likelihood of sampling accurate poses, especially\nfor binding pockets whose shape changes substantially when different ligands\nbind. This improvement is observed across both experimentally determined and\nAlphaFold-generated protein structures. Additionally, we present datasets of\ncandidate ligand poses generated using our methods for each of around 5,000\nprotein-ligand cross-docking pairs, for training and testing scoring functions.\nTo benefit the research community, we provide these cross-docking datasets and\nan open-source Python implementation of GLOW and IVES at\nhttps://github.com/drorlab/GLOW_IVES .",
            "author": [
                "Patricia Suriana",
                "Ron O. Dror"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00191v1",
                "http://arxiv.org/pdf/2312.00191v1"
            ],
            "primary_category": "q-bio.BM",
            "category": [
                "q-bio.BM",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00189v1",
            "title": "HeTriNet: Heterogeneous Graph Triplet Attention Network for\n  Drug-Target-Disease Interaction",
            "updated": "2023-11-30T20:55:57Z",
            "published": "2023-11-30T20:55:57Z",
            "summary": "Modeling the interactions between drugs, targets, and diseases is paramount\nin drug discovery and has significant implications for precision medicine and\npersonalized treatments. Current approaches frequently consider drug-target or\ndrug-disease interactions individually, ignoring the interdependencies among\nall three entities. Within human metabolic systems, drugs interact with protein\ntargets in cells, influencing target activities and subsequently impacting\nbiological pathways to promote healthy functions and treat diseases. Moving\nbeyond binary relationships and exploring tighter triple relationships is\nessential to understanding drugs' mechanism of action (MoAs). Moreover,\nidentifying the heterogeneity of drugs, targets, and diseases, along with their\ndistinct characteristics, is critical to model these complex interactions\nappropriately. To address these challenges, we effectively model the\ninterconnectedness of all entities in a heterogeneous graph and develop a novel\nHeterogeneous Graph Triplet Attention Network (\\texttt{HeTriNet}).\n\\texttt{HeTriNet} introduces a novel triplet attention mechanism within this\nheterogeneous graph structure. Beyond pairwise attention as the importance of\nan entity for the other one, we define triplet attention to model the\nimportance of pairs for entities in the drug-target-disease triplet prediction\nproblem. Experimental results on real-world datasets show that\n\\texttt{HeTriNet} outperforms several baselines, demonstrating its remarkable\nproficiency in uncovering novel drug-target-disease relationships.",
            "author": [
                "Farhan Tanvir",
                "Khaled Mohammed Saifuddin",
                "Tanvir Hossain",
                "Arunkumar Bagavathi",
                "Esra Akbas"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00189v1",
                "http://arxiv.org/pdf/2312.00189v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "q-bio.BM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00184v1",
            "title": "Galaxy Classification: A machine learning approach for classifying\n  shapes using numerical data",
            "updated": "2023-11-30T20:47:16Z",
            "published": "2023-11-30T20:47:16Z",
            "summary": "The classification of galaxies as spirals or ellipticals is a crucial task in\nunderstanding their formation and evolution. With the arrival of large-scale\nastronomical surveys, such as the Sloan Digital Sky Survey (SDSS), astronomers\nnow have access to images of a vast number of galaxies. However, the visual\ninspection of these images is an impossible task for humans due to the sheer\nnumber of galaxies to be analyzed. To solve this problem, the Galaxy Zoo\nproject was created to engage thousands of citizen scientists to classify the\ngalaxies based on their visual features. In this paper, we present a machine\nlearning model for galaxy classification using numerical data from the Galaxy\nZoo[5] project. Our model utilizes a convolutional neural network architecture\nto extract features from galaxy images and classify them into spirals or\nellipticals. We demonstrate the effectiveness of our model by comparing its\nperformance with that of human classifiers using a subset of the Galaxy Zoo\ndataset. Our results show that our model achieves high accuracy in classifying\ngalaxies and has the potential to significantly enhance our understanding of\nthe formation and evolution of galaxies.",
            "author": [
                "Anusha Guruprasad"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00184v1",
                "http://arxiv.org/pdf/2312.00184v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "stat.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00173v1",
            "title": "Fool the Hydra: Adversarial Attacks against Multi-view Object Detection\n  Systems",
            "updated": "2023-11-30T20:11:44Z",
            "published": "2023-11-30T20:11:44Z",
            "summary": "Adversarial patches exemplify the tangible manifestation of the threat posed\nby adversarial attacks on Machine Learning (ML) models in real-world scenarios.\nRobustness against these attacks is of the utmost importance when designing\ncomputer vision applications, especially for safety-critical domains such as\nCCTV systems. In most practical situations, monitoring open spaces requires\nmulti-view systems to overcome acquisition challenges such as occlusion\nhandling. Multiview object systems are able to combine data from multiple\nviews, and reach reliable detection results even in difficult environments.\nDespite its importance in real-world vision applications, the vulnerability of\nmultiview systems to adversarial patches is not sufficiently investigated. In\nthis paper, we raise the following question: Does the increased performance and\ninformation sharing across views offer as a by-product robustness to\nadversarial patches? We first conduct a preliminary analysis showing promising\nrobustness against off-the-shelf adversarial patches, even in an extreme\nsetting where we consider patches applied to all views by all persons in\nWildtrack benchmark. However, we challenged this observation by proposing two\nnew attacks: (i) In the first attack, targeting a multiview CNN, we maximize\nthe global loss by proposing gradient projection to the different views and\naggregating the obtained local gradients. (ii) In the second attack, we focus\non a Transformer-based multiview framework. In addition to the focal loss, we\nalso maximize the transformer-specific loss by dissipating its attention\nblocks. Our results show a large degradation in the detection performance of\nvictim multiview systems with our first patch attack reaching an attack success\nrate of 73% , while our second proposed attack reduced the performance of its\ntarget detector by 62%",
            "author": [
                "Bilel Tarchoun",
                "Quazi Mishkatul Alam",
                "Nael Abu-Ghazaleh",
                "Ihsen Alouani"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00173v1",
                "http://arxiv.org/pdf/2312.00173v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00170v1",
            "title": "Non-uniform Online Learning: Towards Understanding Induction",
            "updated": "2023-11-30T20:02:25Z",
            "published": "2023-11-30T20:02:25Z",
            "summary": "Can a physicist make only finite errors in the endless pursuit of the law of\nnature? This millennium-old question of inductive inference is a fundamental,\nyet mysterious problem in philosophy, lacking rigorous justifications. While\nclassic online learning theory and inductive inference share a similar\nsequential decision-making spirit, the former's reliance on an adaptive\nadversary and worst-case error bounds limits its applicability to the latter.\nIn this work, we introduce the concept of non-uniform online learning, which we\nargue aligns more closely with the principles of inductive reasoning. This\nsetting assumes a predetermined ground-truth hypothesis and considers\nnon-uniform, hypothesis-wise error bounds. In the realizable setting, we\nprovide a complete characterization of learnability with finite error: a\nhypothesis class is non-uniform learnable if and only if it's a countable union\nof Littlestone classes, no matter the observations are adaptively chosen or iid\nsampled. Additionally, we propose a necessary condition for the weaker\ncriterion of consistency which we conjecture to be tight. To further promote\nour theory, we extend our result to the more realistic agnostic setting,\nshowing that any countable union of Littlestone classes can be learnt with\nregret $\\tilde{O}(\\sqrt{T})$. We hope this work could offer a new perspective\nof interpreting the power of induction from an online learning viewpoint.",
            "author": [
                "Zhou Lu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00170v1",
                "http://arxiv.org/pdf/2312.00170v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00157v1",
            "title": "Universal Backdoor Attacks",
            "updated": "2023-11-30T19:37:47Z",
            "published": "2023-11-30T19:37:47Z",
            "summary": "Web-scraped datasets are vulnerable to data poisoning, which can be used for\nbackdooring deep image classifiers during training. Since training on large\ndatasets is expensive, a model is trained once and re-used many times. Unlike\nadversarial examples, backdoor attacks often target specific classes rather\nthan any class learned by the model. One might expect that targeting many\nclasses through a naive composition of attacks vastly increases the number of\npoison samples. We show this is not necessarily true and more efficient,\nuniversal data poisoning attacks exist that allow controlling\nmisclassifications from any source class into any target class with a small\nincrease in poison samples. Our idea is to generate triggers with salient\ncharacteristics that the model can learn. The triggers we craft exploit a\nphenomenon we call inter-class poison transferability, where learning a trigger\nfrom one class makes the model more vulnerable to learning triggers for other\nclasses. We demonstrate the effectiveness and robustness of our universal\nbackdoor attacks by controlling models with up to 6,000 classes while poisoning\nonly 0.15% of the training dataset.",
            "author": [
                "Benjamin Schneider",
                "Nils Lukas",
                "Florian Kerschbaum"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00157v1",
                "http://arxiv.org/pdf/2312.00157v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CR",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00827v1",
            "title": "A Unified Framework for Connecting Noise Modeling to Boost Noise\n  Detection",
            "updated": "2023-11-30T19:24:47Z",
            "published": "2023-11-30T19:24:47Z",
            "summary": "Noisy labels can impair model performance, making the study of learning with\nnoisy labels an important topic. Two conventional approaches are noise modeling\nand noise detection. However, these two methods are typically studied\nindependently, and there has been limited work on their collaboration. In this\nwork, we explore the integration of these two approaches, proposing an\ninterconnected structure with three crucial blocks: noise modeling, source\nknowledge identification, and enhanced noise detection using noise\nsource-knowledge-integration methods. This collaboration structure offers\nadvantages such as discriminating hard negatives and preserving genuinely clean\nlabels that might be suspiciously noisy. Our experiments on four datasets,\nfeaturing three types of noise and different combinations of each block,\ndemonstrate the efficacy of these components' collaboration. Our collaborative\nstructure methods achieve up to a 10% increase in top-1 classification accuracy\nin synthesized noise datasets and 3-5% in real-world noisy datasets. The\nresults also suggest that these components make distinct contributions to\noverall performance across various noise scenarios. These findings provide\nvaluable insights for designing noisy label learning methods customized for\nspecific noise scenarios in the future. Our code is accessible to the public.",
            "author": [
                "Siqi Wang",
                "Chau Pham",
                "Bryan A. Plummer"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00827v1",
                "http://arxiv.org/pdf/2312.00827v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00143v1",
            "title": "The ZTF Source Classification Project: III. A Catalog of Variable\n  Sources",
            "updated": "2023-11-30T19:05:10Z",
            "published": "2023-11-30T19:05:10Z",
            "summary": "The classification of variable objects provides insight into a wide variety\nof astrophysics ranging from stellar interiors to galactic nuclei. The Zwicky\nTransient Facility (ZTF) provides time series observations that record the\nvariability of more than a billion sources. The scale of these data\nnecessitates automated approaches to make a thorough analysis. Building on\nprevious work, this paper reports the results of the ZTF Source Classification\nProject (SCoPe), which trains neural network and XGBoost machine learning (ML)\nalgorithms to perform dichotomous classification of variable ZTF sources using\na manually constructed training set containing 170,632 light curves. We find\nthat several classifiers achieve high precision and recall scores, suggesting\nthe reliability of their predictions for 112,476,749 light curves across 40 ZTF\nfields. We also identify the most important features for XGB classification and\ncompare the performance of the two ML algorithms, finding a pattern of higher\nprecision among XGB classifiers. The resulting classification catalog is\navailable to the public, and the software developed for SCoPe is open-source\nand adaptable to future time-domain surveys.",
            "author": [
                "Brian F. Healy",
                "Michael W. Coughlin",
                "Ashish A. Mahabal",
                "Theophile J. du Laz",
                "Andrew Drake",
                "Matthew J. Graham",
                "Lynne A. Hillenbrand",
                "Jan van Roestel",
                "Paula Szkody",
                "LeighAnna Zielske",
                "Mohammed Guiga",
                "Muhammad Yusuf Hassan",
                "Jill L. Hughes",
                "Guy Nir",
                "Saagar Parikh",
                "Sungmin Park",
                "Palak Purohit",
                "Umaa Rebbapragada",
                "Draco Reed",
                "Avery Wold",
                "Joshua S. Bloom",
                "Frank J. Masci",
                "Reed Riddle",
                "Roger Smith"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00143v1",
                "http://arxiv.org/pdf/2312.00143v1"
            ],
            "primary_category": "astro-ph.IM",
            "category": [
                "astro-ph.IM",
                "astro-ph.SR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00137v1",
            "title": "The Multiverse of Dynamic Mode Decomposition Algorithms",
            "updated": "2023-11-30T19:00:50Z",
            "published": "2023-11-30T19:00:50Z",
            "summary": "Dynamic Mode Decomposition (DMD) is a popular data-driven analysis technique\nused to decompose complex, nonlinear systems into a set of modes, revealing\nunderlying patterns and dynamics through spectral analysis. This review\npresents a comprehensive and pedagogical examination of DMD, emphasizing the\nrole of Koopman operators in transforming complex nonlinear dynamics into a\nlinear framework. A distinctive feature of this review is its focus on the\nrelationship between DMD and the spectral properties of Koopman operators, with\nparticular emphasis on the theory and practice of DMD algorithms for spectral\ncomputations. We explore the diverse \"multiverse\" of DMD methods, categorized\ninto three main areas: linear regression-based methods, Galerkin\napproximations, and structure-preserving techniques. Each category is studied\nfor its unique contributions and challenges, providing a detailed overview of\nsignificant algorithms and their applications as outlined in Table 1. We\ninclude a MATLAB package with examples and applications to enhance the\npractical understanding of these methods. This review serves as both a\npractical guide and a theoretical reference for various DMD methods, accessible\nto both experts and newcomers, and enabling readers to delve into their areas\nof interest in the expansive field of DMD.",
            "author": [
                "Matthew J. Colbrook"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00137v1",
                "http://arxiv.org/pdf/2312.00137v1"
            ],
            "primary_category": "math.DS",
            "category": [
                "math.DS",
                "cs.LG",
                "cs.NA",
                "math.NA",
                "math.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00128v1",
            "title": "Low latency optical-based mode tracking with machine learning deployed\n  on FPGAs on a tokamak",
            "updated": "2023-11-30T19:00:03Z",
            "published": "2023-11-30T19:00:03Z",
            "summary": "Active feedback control in magnetic confinement fusion devices is desirable\nto mitigate plasma instabilities and enable robust operation. Optical\nhigh-speed cameras provide a powerful, non-invasive diagnostic and can be\nsuitable for these applications. In this study, we process fast camera data, at\nrates exceeding 100kfps, on $\\textit{in situ}$ Field Programmable Gate Array\n(FPGA) hardware to track magnetohydrodynamic (MHD) mode evolution and generate\ncontrol signals in real-time. Our system utilizes a convolutional neural\nnetwork (CNN) model which predicts the $n$=1 MHD mode amplitude and phase using\ncamera images with better accuracy than other tested non-deep-learning-based\nmethods. By implementing this model directly within the standard FPGA readout\nhardware of the high-speed camera diagnostic, our mode tracking system achieves\na total trigger-to-output latency of 17.6$\\mu$s and a throughput of up to\n120kfps. This study at the High Beta Tokamak-Extended Pulse (HBT-EP) experiment\ndemonstrates an FPGA-based high-speed camera data acquisition and processing\nsystem, enabling application in real-time machine-learning-based tokamak\ndiagnostic and control as well as potential applications in other scientific\ndomains.",
            "author": [
                "Yumou Wei",
                "Ryan F. Forelli",
                "Chris Hansen",
                "Jeffrey P. Levesque",
                "Nhan Tran",
                "Joshua C. Agar",
                "Giuseppe Di Guglielmo",
                "Michael E. Mauel",
                "Gerald A. Navratil"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00128v1",
                "http://arxiv.org/pdf/2312.00128v1"
            ],
            "primary_category": "physics.plasm-ph",
            "category": [
                "physics.plasm-ph",
                "cs.AR",
                "cs.LG",
                "physics.ins-det"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00123v1",
            "title": "Flow Matching Beyond Kinematics: Generating Jets with Particle-ID and\n  Trajectory Displacement Information",
            "updated": "2023-11-30T19:00:02Z",
            "published": "2023-11-30T19:00:02Z",
            "summary": "We introduce the first generative model trained on the JetClass dataset. Our\nmodel generates jets at the constituent level, and it is a\npermutation-equivariant continuous normalizing flow (CNF) trained with the flow\nmatching technique. It is conditioned on the jet type, so that a single model\ncan be used to generate the ten different jet types of JetClass. For the first\ntime, we also introduce a generative model that goes beyond the kinematic\nfeatures of jet constituents. The JetClass dataset includes more features, such\nas particle-ID and track impact parameter, and we demonstrate that our CNF can\naccurately model all of these additional features as well. Our generative model\nfor JetClass expands on the versatility of existing jet generation techniques,\nenhancing their potential utility in high-energy physics research, and offering\na more comprehensive understanding of the generated jets.",
            "author": [
                "Joschka Birk",
                "Erik Buhmann",
                "Cedric Ewen",
                "Gregor Kasieczka",
                "David Shih"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00123v1",
                "http://arxiv.org/pdf/2312.00123v1"
            ],
            "primary_category": "hep-ph",
            "category": [
                "hep-ph",
                "cs.LG",
                "hep-ex",
                "physics.data-an"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00125v1",
            "title": "Scalable Bayesian uncertainty quantification with data-driven priors for\n  radio interferometric imaging",
            "updated": "2023-11-30T19:00:02Z",
            "published": "2023-11-30T19:00:02Z",
            "summary": "Next-generation radio interferometers like the Square Kilometer Array have\nthe potential to unlock scientific discoveries thanks to their unprecedented\nangular resolution and sensitivity. One key to unlocking their potential\nresides in handling the deluge and complexity of incoming data. This challenge\nrequires building radio interferometric imaging methods that can cope with the\nmassive data sizes and provide high-quality image reconstructions with\nuncertainty quantification (UQ). This work proposes a method coined QuantifAI\nto address UQ in radio-interferometric imaging with data-driven (learned)\npriors for high-dimensional settings. Our model, rooted in the Bayesian\nframework, uses a physically motivated model for the likelihood. The model\nexploits a data-driven convex prior, which can encode complex information\nlearned implicitly from simulations and guarantee the log-concavity of the\nposterior. We leverage probability concentration phenomena of high-dimensional\nlog-concave posteriors that let us obtain information about the posterior,\navoiding MCMC sampling techniques. We rely on convex optimisation methods to\ncompute the MAP estimation, which is known to be faster and better scale with\ndimension than MCMC sampling strategies. Our method allows us to compute local\ncredible intervals, i.e., Bayesian error bars, and perform hypothesis testing\nof structure on the reconstructed image. In addition, we propose a novel\nblazing-fast method to compute pixel-wise uncertainties at different scales. We\ndemonstrate our method by reconstructing radio-interferometric images in a\nsimulated setting and carrying out fast and scalable UQ, which we validate with\nMCMC sampling. Our method shows an improved image quality and more meaningful\nuncertainties than the benchmark method based on a sparsity-promoting prior.\nQuantifAI's source code: https://github.com/astro-informatics/QuantifAI.",
            "author": [
                "Tob\u00edas I. Liaudat",
                "Matthijs Mars",
                "Matthew A. Price",
                "Marcelo Pereyra",
                "Marta M. Betcke",
                "Jason D. McEwen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00125v1",
                "http://arxiv.org/pdf/2312.00125v1"
            ],
            "primary_category": "astro-ph.IM",
            "category": [
                "astro-ph.IM",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00119v1",
            "title": "Anomaly Detection in Collider Physics via Factorized Observables",
            "updated": "2023-11-30T19:00:00Z",
            "published": "2023-11-30T19:00:00Z",
            "summary": "To maximize the discovery potential of high-energy colliders, experimental\nsearches should be sensitive to unforeseen new physics scenarios. This goal has\nmotivated the use of machine learning for unsupervised anomaly detection. In\nthis paper, we introduce a new anomaly detection strategy called FORCE:\nfactorized observables for regressing conditional expectations. Our approach is\nbased on the inductive bias of factorization, which is the idea that the\nphysics governing different energy scales can be treated as approximately\nindependent. Assuming factorization holds separately for signal and background\nprocesses, the appearance of non-trivial correlations between low- and\nhigh-energy observables is a robust indicator of new physics. Under the most\nrestrictive form of factorization, a machine-learned model trained to identify\nsuch correlations will in fact converge to the optimal new physics classifier.\nWe test FORCE on a benchmark anomaly detection task for the Large Hadron\nCollider involving collimated sprays of particles called jets. By teasing out\ncorrelations between the kinematics and substructure of jets, our method can\nreliably extract percent-level signal fractions. This strategy for uncovering\nnew physics adds to the growing toolbox of anomaly detection methods for\ncollider physics with a complementary set of assumptions.",
            "author": [
                "Eric M. Metodiev",
                "Jesse Thaler",
                "Raymond Wynne"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00119v1",
                "http://arxiv.org/pdf/2312.00119v1"
            ],
            "primary_category": "hep-ph",
            "category": [
                "hep-ph",
                "hep-ex",
                "physics.data-an"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00093v1",
            "title": "GraphDreamer: Compositional 3D Scene Synthesis from Scene Graphs",
            "updated": "2023-11-30T18:59:58Z",
            "published": "2023-11-30T18:59:58Z",
            "summary": "As pretrained text-to-image diffusion models become increasingly powerful,\nrecent efforts have been made to distill knowledge from these text-to-image\npretrained models for optimizing a text-guided 3D model. Most of the existing\nmethods generate a holistic 3D model from a plain text input. This can be\nproblematic when the text describes a complex scene with multiple objects,\nbecause the vectorized text embeddings are inherently unable to capture a\ncomplex description with multiple entities and relationships. Holistic 3D\nmodeling of the entire scene further prevents accurate grounding of text\nentities and concepts. To address this limitation, we propose GraphDreamer, a\nnovel framework to generate compositional 3D scenes from scene graphs, where\nobjects are represented as nodes and their interactions as edges. By exploiting\nnode and edge information in scene graphs, our method makes better use of the\npretrained text-to-image diffusion model and is able to fully disentangle\ndifferent objects without image-level supervision. To facilitate modeling of\nobject-wise relationships, we use signed distance fields as representation and\nimpose a constraint to avoid inter-penetration of objects. To avoid manual\nscene graph creation, we design a text prompt for ChatGPT to generate scene\ngraphs based on text inputs. We conduct both qualitative and quantitative\nexperiments to validate the effectiveness of GraphDreamer in generating\nhigh-fidelity compositional 3D scenes with disentangled object entities.",
            "author": [
                "Gege Gao",
                "Weiyang Liu",
                "Anpei Chen",
                "Andreas Geiger",
                "Bernhard Sch\u00f6lkopf"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00093v1",
                "http://arxiv.org/pdf/2312.00093v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18838v1",
            "title": "Dataset Distillation in Large Data Era",
            "updated": "2023-11-30T18:59:56Z",
            "published": "2023-11-30T18:59:56Z",
            "summary": "Dataset distillation aims to generate a smaller but representative subset\nfrom a large dataset, which allows a model to be trained efficiently, meanwhile\nevaluating on the original testing data distribution to achieve decent\nperformance. Many prior works have aimed to align with diverse aspects of the\noriginal datasets, such as matching the training weight trajectories, gradient,\nfeature/BatchNorm distributions, etc. In this work, we show how to distill\nvarious large-scale datasets such as full ImageNet-1K/21K under a conventional\ninput resolution of 224$\\times$224 to achieve the best accuracy over all\nprevious approaches, including SRe$^2$L, TESLA and MTT. To achieve this, we\nintroduce a simple yet effective ${\\bf C}$urriculum ${\\bf D}$ata ${\\bf\nA}$ugmentation ($\\texttt{CDA}$) during data synthesis that obtains the accuracy\non large-scale ImageNet-1K and 21K with 63.2% under IPC (Images Per Class) 50\nand 36.1% under IPC 20, respectively. Finally, we show that, by integrating all\nour enhancements together, the proposed model beats the current\nstate-of-the-art by more than 4% Top-1 accuracy on ImageNet-1K/21K and for the\nfirst time, reduces the gap to its full-data training counterpart to less than\nabsolute 15%. Moreover, this work represents the inaugural success in dataset\ndistillation on larger-scale ImageNet-21K under the standard 224$\\times$224\nresolution. Our code and distilled ImageNet-21K dataset of 20 IPC, 2K recovery\nbudget are available at https://github.com/VILA-Lab/SRe2L/tree/main/CDA.",
            "author": [
                "Zeyuan Yin",
                "Zhiqiang Shen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18838v1",
                "http://arxiv.org/pdf/2311.18838v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18839v1",
            "title": "TrafficMOT: A Challenging Dataset for Multi-Object Tracking in Complex\n  Traffic Scenarios",
            "updated": "2023-11-30T18:59:56Z",
            "published": "2023-11-30T18:59:56Z",
            "summary": "Multi-object tracking in traffic videos is a crucial research area, offering\nimmense potential for enhancing traffic monitoring accuracy and promoting road\nsafety measures through the utilisation of advanced machine learning\nalgorithms. However, existing datasets for multi-object tracking in traffic\nvideos often feature limited instances or focus on single classes, which cannot\nwell simulate the challenges encountered in complex traffic scenarios. To\naddress this gap, we introduce TrafficMOT, an extensive dataset designed to\nencompass diverse traffic situations with complex scenarios. To validate the\ncomplexity and challenges presented by TrafficMOT, we conducted comprehensive\nempirical studies using three different settings: fully-supervised,\nsemi-supervised, and a recent powerful zero-shot foundation model Tracking\nAnything Model (TAM). The experimental results highlight the inherent\ncomplexity of this dataset, emphasising its value in driving advancements in\nthe field of traffic monitoring and multi-object tracking.",
            "author": [
                "Lihao Liu",
                "Yanqi Cheng",
                "Zhongying Deng",
                "Shujun Wang",
                "Dongdong Chen",
                "Xiaowei Hu",
                "Pietro Li\u00f2",
                "Carola-Bibiane Sch\u00f6nlieb",
                "Angelica Aviles-Rivero"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18839v1",
                "http://arxiv.org/pdf/2311.18839v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18840v1",
            "title": "Just Add $\u03c0$! Pose Induced Video Transformers for Understanding\n  Activities of Daily Living",
            "updated": "2023-11-30T18:59:56Z",
            "published": "2023-11-30T18:59:56Z",
            "summary": "Video transformers have become the de facto standard for human action\nrecognition, yet their exclusive reliance on the RGB modality still limits\ntheir adoption in certain domains. One such domain is Activities of Daily\nLiving (ADL), where RGB alone is not sufficient to distinguish between visually\nsimilar actions, or actions observed from multiple viewpoints. To facilitate\nthe adoption of video transformers for ADL, we hypothesize that the\naugmentation of RGB with human pose information, known for its sensitivity to\nfine-grained motion and multiple viewpoints, is essential. Consequently, we\nintroduce the first Pose Induced Video Transformer: PI-ViT (or $\\pi$-ViT), a\nnovel approach that augments the RGB representations learned by video\ntransformers with 2D and 3D pose information. The key elements of $\\pi$-ViT are\ntwo plug-in modules, 2D Skeleton Induction Module and 3D Skeleton Induction\nModule, that are responsible for inducing 2D and 3D pose information into the\nRGB representations. These modules operate by performing pose-aware auxiliary\ntasks, a design choice that allows $\\pi$-ViT to discard the modules during\ninference. Notably, $\\pi$-ViT achieves the state-of-the-art performance on\nthree prominent ADL datasets, encompassing both real-world and large-scale\nRGB-D datasets, without requiring poses or additional computational overhead at\ninference.",
            "author": [
                "Dominick Reilly",
                "Srijan Das"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18840v1",
                "http://arxiv.org/pdf/2311.18840v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18837v1",
            "title": "VIDiff: Translating Videos via Multi-Modal Instructions with Diffusion\n  Models",
            "updated": "2023-11-30T18:59:52Z",
            "published": "2023-11-30T18:59:52Z",
            "summary": "Diffusion models have achieved significant success in image and video\ngeneration. This motivates a growing interest in video editing tasks, where\nvideos are edited according to provided text descriptions. However, most\nexisting approaches only focus on video editing for short clips and rely on\ntime-consuming tuning or inference. We are the first to propose Video\nInstruction Diffusion (VIDiff), a unified foundation model designed for a wide\nrange of video tasks. These tasks encompass both understanding tasks (such as\nlanguage-guided video object segmentation) and generative tasks (video editing\nand enhancement). Our model can edit and translate the desired results within\nseconds based on user instructions. Moreover, we design an iterative\nauto-regressive method to ensure consistency in editing and enhancing long\nvideos. We provide convincing generative results for diverse input videos and\nwritten instructions, both qualitatively and quantitatively. More examples can\nbe found at our website https://ChenHsing.github.io/VIDiff.",
            "author": [
                "Zhen Xing",
                "Qi Dai",
                "Zihao Zhang",
                "Hui Zhang",
                "Han Hu",
                "Zuxuan Wu",
                "Yu-Gang Jiang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18837v1",
                "http://arxiv.org/pdf/2311.18837v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG",
                "cs.MM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00116v1",
            "title": "S2ST: Image-to-Image Translation in the Seed Space of Latent Diffusion",
            "updated": "2023-11-30T18:59:49Z",
            "published": "2023-11-30T18:59:49Z",
            "summary": "Image-to-image translation (I2IT) refers to the process of transforming\nimages from a source domain to a target domain while maintaining a fundamental\nconnection in terms of image content. In the past few years, remarkable\nadvancements in I2IT were achieved by Generative Adversarial Networks (GANs),\nwhich nevertheless struggle with translations requiring high precision.\nRecently, Diffusion Models have established themselves as the engine of choice\nfor image generation. In this paper we introduce S2ST, a novel framework\ndesigned to accomplish global I2IT in complex photorealistic images, such as\nday-to-night or clear-to-rain translations of automotive scenes. S2ST operates\nwithin the seed space of a Latent Diffusion Model, thereby leveraging the\npowerful image priors learned by the latter. We show that S2ST surpasses\nstate-of-the-art GAN-based I2IT methods, as well as diffusion-based approaches,\nfor complex automotive scenes, improving fidelity while respecting the target\ndomain's appearance across a variety of domains. Notably, S2ST obviates the\nnecessity for training domain-specific translation networks.",
            "author": [
                "Or Greenberg",
                "Eran Kishon",
                "Dani Lischinski"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00116v1",
                "http://arxiv.org/pdf/2312.00116v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18834v1",
            "title": "ART$\\boldsymbol{\\cdot}$V: Auto-Regressive Text-to-Video Generation with\n  Diffusion Models",
            "updated": "2023-11-30T18:59:47Z",
            "published": "2023-11-30T18:59:47Z",
            "summary": "We present ART$\\boldsymbol{\\cdot}$V, an efficient framework for\nauto-regressive video generation with diffusion models. Unlike existing methods\nthat generate entire videos in one-shot, ART$\\boldsymbol{\\cdot}$V generates a\nsingle frame at a time, conditioned on the previous ones. The framework offers\nthree distinct advantages. First, it only learns simple continual motions\nbetween adjacent frames, therefore avoiding modeling complex long-range motions\nthat require huge training data. Second, it preserves the high-fidelity\ngeneration ability of the pre-trained image diffusion models by making only\nminimal network modifications. Third, it can generate arbitrarily long videos\nconditioned on a variety of prompts such as text, image or their combinations,\nmaking it highly versatile and flexible. To combat the common drifting issue in\nAR models, we propose masked diffusion model which implicitly learns which\ninformation can be drawn from reference images rather than network predictions,\nin order to reduce the risk of generating inconsistent appearances that cause\ndrifting. Moreover, we further enhance generation coherence by conditioning it\non the initial frame, which typically contains minimal noise. This is\nparticularly useful for long video generation. When trained for only two weeks\non four GPUs, ART$\\boldsymbol{\\cdot}$V already can generate videos with natural\nmotions, rich details and a high level of aesthetic quality. Besides, it\nenables various appealing applications, e.g., composing a long video from\nmultiple text prompts.",
            "author": [
                "Wenming Weng",
                "Ruoyu Feng",
                "Yanhui Wang",
                "Qi Dai",
                "Chunyu Wang",
                "Dacheng Yin",
                "Zhiyuan Zhao",
                "Kai Qiu",
                "Jianmin Bao",
                "Yuhui Yuan",
                "Chong Luo",
                "Yueyi Zhang",
                "Zhiwei Xiong"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18834v1",
                "http://arxiv.org/pdf/2311.18834v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00115v1",
            "title": "A Video is Worth 10,000 Words: Training and Benchmarking with Diverse\n  Captions for Better Long Video Retrieval",
            "updated": "2023-11-30T18:59:45Z",
            "published": "2023-11-30T18:59:45Z",
            "summary": "Existing long video retrieval systems are trained and tested in the\nparagraph-to-video retrieval regime, where every long video is described by a\nsingle long paragraph. This neglects the richness and variety of possible valid\ndescriptions of a video, which could be described in moment-by-moment detail,\nor in a single phrase summary, or anything in between. To provide a more\nthorough evaluation of the capabilities of long video retrieval systems, we\npropose a pipeline that leverages state-of-the-art large language models to\ncarefully generate a diverse set of synthetic captions for long videos. We\nvalidate this pipeline's fidelity via rigorous human inspection. We then\nbenchmark a representative set of video language models on these synthetic\ncaptions using a few long video datasets, showing that they struggle with the\ntransformed data, especially the shortest captions. We also propose a\nlightweight fine-tuning method, where we use a contrastive loss to learn a\nhierarchical embedding loss based on the differing levels of information among\nthe various captions. Our method improves performance both on the downstream\nparagraph-to-video retrieval task (+1.1% R@1 on ActivityNet), as well as for\nthe various long video retrieval metrics we compute using our synthetic data\n(+3.6% R@1 for short descriptions on ActivityNet). For data access and other\ndetails, please refer to our project website at\nhttps://mgwillia.github.io/10k-words.",
            "author": [
                "Matthew Gwilliam",
                "Michael Cogswell",
                "Meng Ye",
                "Karan Sikka",
                "Abhinav Shrivastava",
                "Ajay Divakaran"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00115v1",
                "http://arxiv.org/pdf/2312.00115v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18829v1",
            "title": "MicroCinema: A Divide-and-Conquer Approach for Text-to-Video Generation",
            "updated": "2023-11-30T18:59:30Z",
            "published": "2023-11-30T18:59:30Z",
            "summary": "We present MicroCinema, a straightforward yet effective framework for\nhigh-quality and coherent text-to-video generation. Unlike existing approaches\nthat align text prompts with video directly, MicroCinema introduces a\nDivide-and-Conquer strategy which divides the text-to-video into a two-stage\nprocess: text-to-image generation and image\\&text-to-video generation. This\nstrategy offers two significant advantages. a) It allows us to take full\nadvantage of the recent advances in text-to-image models, such as Stable\nDiffusion, Midjourney, and DALLE, to generate photorealistic and highly\ndetailed images. b) Leveraging the generated image, the model can allocate less\nfocus to fine-grained appearance details, prioritizing the efficient learning\nof motion dynamics. To implement this strategy effectively, we introduce two\ncore designs. First, we propose the Appearance Injection Network, enhancing the\npreservation of the appearance of the given image. Second, we introduce the\nAppearance Noise Prior, a novel mechanism aimed at maintaining the capabilities\nof pre-trained 2D diffusion models. These design elements empower MicroCinema\nto generate high-quality videos with precise motion, guided by the provided\ntext prompts. Extensive experiments demonstrate the superiority of the proposed\nframework. Concretely, MicroCinema achieves SOTA zero-shot FVD of 342.86 on\nUCF-101 and 377.40 on MSR-VTT. See\nhttps://wangyanhui666.github.io/MicroCinema.github.io/ for video samples.",
            "author": [
                "Yanhui Wang",
                "Jianmin Bao",
                "Wenming Weng",
                "Ruoyu Feng",
                "Dacheng Yin",
                "Tao Yang",
                "Jingxu Zhang",
                "Qi Dai Zhiyuan Zhao",
                "Chunyu Wang",
                "Kai Qiu",
                "Yuhui Yuan",
                "Xiaoyan Sun",
                "Chong Luo",
                "Baining Guo"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18829v1",
                "http://arxiv.org/pdf/2311.18829v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00112v1",
            "title": "DynMF: Neural Motion Factorization for Real-time Dynamic View Synthesis\n  with 3D Gaussian Splatting",
            "updated": "2023-11-30T18:59:11Z",
            "published": "2023-11-30T18:59:11Z",
            "summary": "Accurately and efficiently modeling dynamic scenes and motions is considered\nso challenging a task due to temporal dynamics and motion complexity. To\naddress these challenges, we propose DynMF, a compact and efficient\nrepresentation that decomposes a dynamic scene into a few neural trajectories.\nWe argue that the per-point motions of a dynamic scene can be decomposed into a\nsmall set of explicit or learned trajectories. Our carefully designed neural\nframework consisting of a tiny set of learned basis queried only in time allows\nfor rendering speed similar to 3D Gaussian Splatting, surpassing 120 FPS, while\nat the same time, requiring only double the storage compared to static scenes.\nOur neural representation adequately constrains the inherently underconstrained\nmotion field of a dynamic scene leading to effective and fast optimization.\nThis is done by biding each point to motion coefficients that enforce the\nper-point sharing of basis trajectories. By carefully applying a sparsity loss\nto the motion coefficients, we are able to disentangle the motions that\ncomprise the scene, independently control them, and generate novel motion\ncombinations that have never been seen before. We can reach state-of-the-art\nrender quality within just 5 minutes of training and in less than half an hour,\nwe can synthesize novel views of dynamic scenes with superior photorealistic\nquality. Our representation is interpretable, efficient, and expressive enough\nto offer real-time view synthesis of complex dynamic scene motions, in\nmonocular and multi-view scenarios.",
            "author": [
                "Agelos Kratimenos",
                "Jiahui Lei",
                "Kostas Daniilidis"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00112v1",
                "http://arxiv.org/pdf/2312.00112v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18827v1",
            "title": "Motion-Conditioned Image Animation for Video Editing",
            "updated": "2023-11-30T18:59:06Z",
            "published": "2023-11-30T18:59:06Z",
            "summary": "We introduce MoCA, a Motion-Conditioned Image Animation approach for video\nediting. It leverages a simple decomposition of the video editing problem into\nimage editing followed by motion-conditioned image animation. Furthermore,\ngiven the lack of robust evaluation datasets for video editing, we introduce a\nnew benchmark that measures edit capability across a wide variety of tasks,\nsuch as object replacement, background changes, style changes, and motion\nedits. We present a comprehensive human evaluation of the latest video editing\nmethods along with MoCA, on our proposed benchmark. MoCA establishes a new\nstate-of-the-art, demonstrating greater human preference win-rate, and\noutperforming notable recent approaches including Dreamix (63%), MasaCtrl\n(75%), and Tune-A-Video (72%), with especially significant improvements for\nmotion edits.",
            "author": [
                "Wilson Yan",
                "Andrew Brown",
                "Pieter Abbeel",
                "Rohit Girdhar",
                "Samaneh Azadi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18827v1",
                "http://arxiv.org/pdf/2311.18827v1"
            ],
            "primary_category": "cs.GR",
            "category": [
                "cs.GR",
                "cs.AI",
                "cs.CV",
                "cs.LG",
                "cs.MM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18826v2",
            "title": "Geometry-Aware Normalizing Wasserstein Flows for Optimal Causal\n  Inference",
            "updated": "2023-12-05T18:57:28Z",
            "published": "2023-11-30T18:59:05Z",
            "summary": "This manuscript enriches the framework of continuous normalizing flows (CNFs)\nwithin causal inference, primarily to augment the geometric properties of\nparametric submodels used in targeted maximum likelihood estimation (TMLE). By\nintroducing an innovative application of CNFs, we construct a refined series of\nparametric submodels that enable a directed interpolation between the prior\ndistribution $p_0$ and the empirical distribution $p_1$. This proposed\nmethodology serves to optimize the semiparametric efficiency bound in causal\ninference by orchestrating CNFs to align with Wasserstein gradient flows. Our\napproach not only endeavors to minimize the mean squared error in the\nestimation but also imbues the estimators with geometric sophistication,\nthereby enhancing robustness against misspecification. This robustness is\ncrucial, as it alleviates the dependence on the standard $n^{\\frac{1}{4}}$ rate\nfor a doubly-robust perturbation direction in TMLE. By incorporating robust\noptimization principles and differential geometry into the estimators, the\ndeveloped geometry-aware CNFs represent a significant advancement in the\npursuit of doubly robust causal inference.",
            "author": [
                "Kaiwen Hou"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18826v2",
                "http://arxiv.org/pdf/2311.18826v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00826v1",
            "title": "DEVIAS: Learning Disentangled Video Representations of Action and Scene\n  for Holistic Video Understanding",
            "updated": "2023-11-30T18:58:44Z",
            "published": "2023-11-30T18:58:44Z",
            "summary": "When watching a video, humans can naturally extract human actions from the\nsurrounding scene context, even when action-scene combinations are unusual.\nHowever, unlike humans, video action recognition models often learn\nscene-biased action representations from the spurious correlation in training\ndata, leading to poor performance in out-of-context scenarios. While\nscene-debiased models achieve improved performance in out-of-context scenarios,\nthey often overlook valuable scene information in the data. Addressing this\nchallenge, we propose Disentangled VIdeo representations of Action and Scene\n(DEVIAS), which aims to achieve holistic video understanding. Disentangled\naction and scene representations with our method could provide flexibility to\nadjust the emphasis on action or scene information depending on downstream task\nand dataset characteristics. Disentangled action and scene representations\ncould be beneficial for both in-context and out-of-context video understanding.\nTo this end, we employ slot attention to learn disentangled action and scene\nrepresentations with a single model, along with auxiliary tasks that further\nguide slot attention. We validate the proposed method on both in-context\ndatasets: UCF-101 and Kinetics-400, and out-of-context datasets: SCUBA and HAT.\nOur proposed method shows favorable performance across different datasets\ncompared to the baselines, demonstrating its effectiveness in diverse video\nunderstanding scenarios.",
            "author": [
                "Kyungho Bae",
                "Geo Ahn",
                "Youngrae Kim",
                "Jinwoo Choi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00826v1",
                "http://arxiv.org/pdf/2312.00826v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18824v1",
            "title": "An Adaptive Framework for Generalizing Network Traffic Prediction\n  towards Uncertain Environments",
            "updated": "2023-11-30T18:58:38Z",
            "published": "2023-11-30T18:58:38Z",
            "summary": "We have developed a new framework using time-series analysis for dynamically\nassigning mobile network traffic prediction models in previously unseen\nwireless environments. Our framework selectively employs learned behaviors,\noutperforming any single model with over a 50% improvement relative to current\nstudies. More importantly, it surpasses traditional approaches without needing\nprior knowledge of a cell. While this paper focuses on network traffic\nprediction using our adaptive forecasting framework, this framework can also be\napplied to other machine learning applications in uncertain environments.\n  The framework begins with unsupervised clustering of time-series data to\nidentify unique trends and seasonal patterns. Subsequently, we apply supervised\nlearning for traffic volume prediction within each cluster. This specialization\ntowards specific traffic behaviors occurs without penalties from spatial and\ntemporal variations. Finally, the framework adaptively assigns trained models\nto new, previously unseen cells. By analyzing real-time measurements of a cell,\nour framework intelligently selects the most suitable cluster for that cell at\nany given time, with cluster assignment dynamically adjusting to\nspatio-temporal fluctuations.",
            "author": [
                "Alexander Downey",
                "Evren Tuna",
                "Alkan Soysal"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18824v1",
                "http://arxiv.org/pdf/2311.18824v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "cs.LG",
                "cs.NI",
                "eess.SP",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18823v1",
            "title": "Initializing Models with Larger Ones",
            "updated": "2023-11-30T18:58:26Z",
            "published": "2023-11-30T18:58:26Z",
            "summary": "Weight initialization plays an important role in neural network training.\nWidely used initialization methods are proposed and evaluated for networks that\nare trained from scratch. However, the growing number of pretrained models now\noffers new opportunities for tackling this classical problem of weight\ninitialization. In this work, we introduce weight selection, a method for\ninitializing smaller models by selecting a subset of weights from a pretrained\nlarger model. This enables the transfer of knowledge from pretrained weights to\nsmaller models. Our experiments demonstrate that weight selection can\nsignificantly enhance the performance of small models and reduce their training\ntime. Notably, it can also be used together with knowledge distillation. Weight\nselection offers a new approach to leverage the power of pretrained models in\nresource-constrained settings, and we hope it can be a useful tool for training\nsmall models in the large-model era. Code is available at\nhttps://github.com/OscarXZQ/weight-selection.",
            "author": [
                "Zhiqiu Xu",
                "Yanjie Chen",
                "Kirill Vishniakov",
                "Yida Yin",
                "Zhiqiang Shen",
                "Trevor Darrell",
                "Lingjie Liu",
                "Zhuang Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18823v1",
                "http://arxiv.org/pdf/2311.18823v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18817v1",
            "title": "Dichotomy of Early and Late Phase Implicit Biases Can Provably Induce\n  Grokking",
            "updated": "2023-11-30T18:55:38Z",
            "published": "2023-11-30T18:55:38Z",
            "summary": "Recent work by Power et al. (2022) highlighted a surprising \"grokking\"\nphenomenon in learning arithmetic tasks: a neural net first \"memorizes\" the\ntraining set, resulting in perfect training accuracy but near-random test\naccuracy, and after training for sufficiently longer, it suddenly transitions\nto perfect test accuracy. This paper studies the grokking phenomenon in\ntheoretical setups and shows that it can be induced by a dichotomy of early and\nlate phase implicit biases. Specifically, when training homogeneous neural nets\nwith large initialization and small weight decay on both classification and\nregression tasks, we prove that the training process gets trapped at a solution\ncorresponding to a kernel predictor for a long time, and then a very sharp\ntransition to min-norm/max-margin predictors occurs, leading to a dramatic\nchange in test accuracy.",
            "author": [
                "Kaifeng Lyu",
                "Jikai Jin",
                "Zhiyuan Li",
                "Simon S. Du",
                "Jason D. Lee",
                "Wei Hu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18817v1",
                "http://arxiv.org/pdf/2311.18817v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18815v1",
            "title": "IMMA: Immunizing text-to-image Models against Malicious Adaptation",
            "updated": "2023-11-30T18:55:16Z",
            "published": "2023-11-30T18:55:16Z",
            "summary": "Advancements in text-to-image models and fine-tuning methods have led to the\nincreasing risk of malicious adaptation, i.e., fine-tuning to generate harmful\nunauthorized content. Recent works, e.g., Glaze or MIST, have developed\ndata-poisoning techniques which protect the data against adaptation methods. In\nthis work, we consider an alternative paradigm for protection. We propose to\n``immunize'' the model by learning model parameters that are difficult for the\nadaptation methods when fine-tuning malicious content; in short IMMA. Empirical\nresults show IMMA's effectiveness against malicious adaptations, including\nmimicking the artistic style and learning of inappropriate/unauthorized\ncontent, over three adaptation methods: LoRA, Textual-Inversion, and\nDreamBooth.",
            "author": [
                "Yijia Zheng",
                "Raymond A. Yeh"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18815v1",
                "http://arxiv.org/pdf/2311.18815v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18814v1",
            "title": "Is Underwater Image Enhancement All Object Detectors Need?",
            "updated": "2023-11-30T18:54:08Z",
            "published": "2023-11-30T18:54:08Z",
            "summary": "Underwater object detection is a crucial and challenging problem in marine\nengineering and aquatic robot. The difficulty is partly because of the\ndegradation of underwater images caused by light selective absorption and\nscattering. Intuitively, enhancing underwater images can benefit high-level\napplications like underwater object detection. However, it is still unclear\nwhether all object detectors need underwater image enhancement as\npre-processing. We therefore pose the questions \"Does underwater image\nenhancement really improve underwater object detection?\" and \"How does\nunderwater image enhancement contribute to underwater object detection?\". With\nthese two questions, we conduct extensive studies. Specifically, we use 18\nstate-of-the-art underwater image enhancement algorithms, covering traditional,\nCNN-based, and GAN-based algorithms, to pre-process underwater object detection\ndata. Then, we retrain 7 popular deep learning-based object detectors using the\ncorresponding results enhanced by different algorithms, obtaining 126\nunderwater object detection models. Coupled with 7 object detection models\nretrained using raw underwater images, we employ these 133 models to\ncomprehensively analyze the effect of underwater image enhancement on\nunderwater object detection. We expect this study can provide sufficient\nexploration to answer the aforementioned questions and draw more attention of\nthe community to the joint problem of underwater image enhancement and\nunderwater object detection. The pre-trained models and results are publicly\navailable and will be regularly updated. Project page:\nhttps://github.com/BIGWangYuDong/lqit/tree/main/configs/detection/uw_enhancement_affect_detection.",
            "author": [
                "Yudong Wang",
                "Jichang Guo",
                "Wanru He",
                "Huan Gao",
                "Huihui Yue",
                "Zenan Zhang",
                "Chongyi Li"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18814v1",
                "http://arxiv.org/pdf/2311.18814v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00583v1",
            "title": "MD-Splatting: Learning Metric Deformation from 4D Gaussians in Highly\n  Deformable Scenes",
            "updated": "2023-11-30T18:53:03Z",
            "published": "2023-11-30T18:53:03Z",
            "summary": "Accurate 3D tracking in highly deformable scenes with occlusions and shadows\ncan facilitate new applications in robotics, augmented reality, and generative\nAI. However, tracking under these conditions is extremely challenging due to\nthe ambiguity that arises with large deformations, shadows, and occlusions. We\nintroduce MD-Splatting, an approach for simultaneous 3D tracking and novel view\nsynthesis, using video captures of a dynamic scene from various camera poses.\nMD-Splatting builds on recent advances in Gaussian splatting, a method that\nlearns the properties of a large number of Gaussians for state-of-the-art and\nfast novel view synthesis. MD-Splatting learns a deformation function to\nproject a set of Gaussians with non-metric, thus canonical, properties into\nmetric space. The deformation function uses a neural-voxel encoding and a\nmultilayer perceptron (MLP) to infer Gaussian position, rotation, and a shadow\nscalar. We enforce physics-inspired regularization terms based on local\nrigidity, conservation of momentum, and isometry, which leads to trajectories\nwith smaller trajectory errors. MD-Splatting achieves high-quality 3D tracking\non highly deformable scenes with shadows and occlusions. Compared to\nstate-of-the-art, we improve 3D tracking by an average of 23.9 %, while\nsimultaneously achieving high-quality novel view synthesis. With sufficient\ntexture such as in scene 6, MD-Splatting achieves a median tracking error of\n3.39 mm on a cloth of 1 x 1 meters in size. Project website:\nhttps://md-splatting.github.io/.",
            "author": [
                "Bardienus P. Duisterhof",
                "Zhao Mandi",
                "Yunchao Yao",
                "Jia-Wei Liu",
                "Mike Zheng Shou",
                "Shuran Song",
                "Jeffrey Ichnowski"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00583v1",
                "http://arxiv.org/pdf/2312.00583v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18807v1",
            "title": "Pre-registration for Predictive Modeling",
            "updated": "2023-11-30T18:52:10Z",
            "published": "2023-11-30T18:52:10Z",
            "summary": "Amid rising concerns of reproducibility and generalizability in predictive\nmodeling, we explore the possibility and potential benefits of introducing\npre-registration to the field. Despite notable advancements in predictive\nmodeling, spanning core machine learning tasks to various scientific\napplications, challenges such as overlooked contextual factors, data-dependent\ndecision-making, and unintentional re-use of test data have raised questions\nabout the integrity of results. To address these issues, we propose adapting\npre-registration practices from explanatory modeling to predictive modeling. We\ndiscuss current best practices in predictive modeling and their limitations,\nintroduce a lightweight pre-registration template, and present a qualitative\nstudy with machine learning researchers to gain insight into the effectiveness\nof pre-registration in preventing biased estimates and promoting more reliable\nresearch outcomes. We conclude by exploring the scope of problems that\npre-registration can address in predictive modeling and acknowledging its\nlimitations within this context.",
            "author": [
                "Jake M. Hofman",
                "Angelos Chatzimparmpas",
                "Amit Sharma",
                "Duncan J. Watts",
                "Jessica Hullman"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18807v1",
                "http://arxiv.org/pdf/2311.18807v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ME"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18806v1",
            "title": "Efficient Baseline for Quantitative Precipitation Forecasting in\n  Weather4cast 2023",
            "updated": "2023-11-30T18:51:50Z",
            "published": "2023-11-30T18:51:50Z",
            "summary": "Accurate precipitation forecasting is indispensable for informed\ndecision-making across various industries. However, the computational demands\nof current models raise environmental concerns. We address the critical need\nfor accurate precipitation forecasting while considering the environmental\nimpact of computational resources and propose a minimalist U-Net architecture\nto be used as a baseline for future weather forecasting initiatives.",
            "author": [
                "Akshay Punjabi",
                "Pablo Izquierdo Ayala"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18806v1",
                "http://arxiv.org/pdf/2311.18806v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "physics.ao-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18803v2",
            "title": "BioCLIP: A Vision Foundation Model for the Tree of Life",
            "updated": "2023-12-04T16:13:21Z",
            "published": "2023-11-30T18:49:43Z",
            "summary": "Images of the natural world, collected by a variety of cameras, from drones\nto individual phones, are increasingly abundant sources of biological\ninformation. There is an explosion of computational methods and tools,\nparticularly computer vision, for extracting biologically relevant information\nfrom images for science and conservation. Yet most of these are bespoke\napproaches designed for a specific task and are not easily adaptable or\nextendable to new questions, contexts, and datasets. A vision model for general\norganismal biology questions on images is of timely need. To approach this, we\ncurate and release TreeOfLife-10M, the largest and most diverse ML-ready\ndataset of biology images. We then develop BioCLIP, a foundation model for the\ntree of life, leveraging the unique properties of biology captured by\nTreeOfLife-10M, namely the abundance and variety of images of plants, animals,\nand fungi, together with the availability of rich structured biological\nknowledge. We rigorously benchmark our approach on diverse fine-grained biology\nclassification tasks, and find that BioCLIP consistently and substantially\noutperforms existing baselines (by 17% to 20% absolute). Intrinsic evaluation\nreveals that BioCLIP has learned a hierarchical representation conforming to\nthe tree of life, shedding light on its strong generalizability. Our code,\nmodels and data will be made available at\nhttps://github.com/Imageomics/bioclip.",
            "author": [
                "Samuel Stevens",
                "Jiaman Wu",
                "Matthew J Thompson",
                "Elizabeth G Campolongo",
                "Chan Hee Song",
                "David Edward Carlyn",
                "Li Dong",
                "Wasila M Dahdul",
                "Charles Stewart",
                "Tanya Berger-Wolf",
                "Wei-Lun Chao",
                "Yu Su"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18803v2",
                "http://arxiv.org/pdf/2311.18803v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18801v1",
            "title": "Distributed Global Structure-from-Motion with a Deep Front-End",
            "updated": "2023-11-30T18:47:18Z",
            "published": "2023-11-30T18:47:18Z",
            "summary": "While initial approaches to Structure-from-Motion (SfM) revolved around both\nglobal and incremental methods, most recent applications rely on incremental\nsystems to estimate camera poses due to their superior robustness. Though there\nhas been tremendous progress in SfM `front-ends' powered by deep models learned\nfrom data, the state-of-the-art (incremental) SfM pipelines still rely on\nclassical SIFT features, developed in 2004. In this work, we investigate\nwhether leveraging the developments in feature extraction and matching helps\nglobal SfM perform on par with the SOTA incremental SfM approach (COLMAP). To\ndo so, we design a modular SfM framework that allows us to easily combine\ndevelopments in different stages of the SfM pipeline. Our experiments show that\nwhile developments in deep-learning based two-view correspondence estimation do\ntranslate to improvements in point density for scenes reconstructed with global\nSfM, none of them outperform SIFT when comparing with incremental SfM results\non a range of datasets. Our SfM system is designed from the ground up to\nleverage distributed computation, enabling us to parallelize computation on\nmultiple machines and scale to large scenes.",
            "author": [
                "Ayush Baid",
                "John Lambert",
                "Travis Driver",
                "Akshay Krishnan",
                "Hayk Stepanyan",
                "Frank Dellaert"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18801v1",
                "http://arxiv.org/pdf/2311.18801v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18789v1",
            "title": "Unsupervised learning architecture based on neural Darwinism and\n  Hopfield networks recognizes symbols with high accuracy",
            "updated": "2023-11-30T18:39:20Z",
            "published": "2023-11-30T18:39:20Z",
            "summary": "This paper introduces a novel unsupervised learning paradigm inspired by\nGerald Edelman's theory of neuronal group selection (\"Neural Darwinism\"). The\npresented automaton learns to recognize arbitrary symbols (e.g., letters of an\nalphabet) when they are presented repeatedly, as they are when children learn\nto read. On a second hierarchical level, the model creates abstract categories\nrepresenting the learnt symbols. The fundamental computational unit are simple\nMcCulloch-Pitts neurons arranged into fully-connected groups (Hopfield networks\nwith randomly initialized weights), which are \"selected\", in an evolutionary\nsense, through symbol presentation. The learning process is fully tractable and\neasily interpretable for humans, in contrast to most neural network\narchitectures. Computational properties of Hopfield networks enabling pattern\nrecognition are discussed. In simulations, the model achieves high accuracy in\nlearning the letters of the Latin alphabet, presented as binary patterns on a\ngrid. This paper is a proof of concept with no claims to state-of-the-art\nperformance in letter recognition, but hopefully inspires new thinking in\nbio-inspired machine learning.",
            "author": [
                "Mario Stepanik"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18789v1",
                "http://arxiv.org/pdf/2311.18789v1"
            ],
            "primary_category": "cs.NE",
            "category": [
                "cs.NE",
                "q-bio.NC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18787v1",
            "title": "Communication-Efficient Federated Optimization over Semi-Decentralized\n  Networks",
            "updated": "2023-11-30T18:37:15Z",
            "published": "2023-11-30T18:37:15Z",
            "summary": "In large-scale federated and decentralized learning, communication efficiency\nis one of the most challenging bottlenecks. While gossip communication -- where\nagents can exchange information with their connected neighbors -- is more\ncost-effective than communicating with the remote server, it often requires a\ngreater number of communication rounds, especially for large and sparse\nnetworks. To tackle the trade-off, we examine the communication efficiency\nunder a semi-decentralized communication protocol, in which agents can perform\nboth agent-to-agent and agent-to-server communication in a probabilistic\nmanner. We design a tailored communication-efficient algorithm over\nsemi-decentralized networks, referred to as PISCO, which inherits the\nrobustness to data heterogeneity thanks to gradient tracking and allows\nmultiple local updates for saving communication. We establish the convergence\nrate of PISCO for nonconvex problems and show that PISCO enjoys a linear\nspeedup in terms of the number of agents and local updates. Our numerical\nresults highlight the superior communication efficiency of PISCO and its\nresilience to data heterogeneity and various network topologies.",
            "author": [
                "He Wang",
                "Yuejie Chi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18787v1",
                "http://arxiv.org/pdf/2311.18787v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.DC",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00111v1",
            "title": "Multimodal Learning for Crystalline Materials",
            "updated": "2023-11-30T18:35:29Z",
            "published": "2023-11-30T18:35:29Z",
            "summary": "Artificial intelligence (AI) has revolutionized the field of materials\nscience by improving the prediction of properties and accelerating the\ndiscovery of novel materials. In recent years, publicly available material data\nrepositories containing data for various material properties have grown\nrapidly. In this work, we introduce Multimodal Learning for Crystalline\nMaterials (MLCM), a new method for training a foundation model for crystalline\nmaterials via multimodal alignment, where high-dimensional material properties\n(i.e. modalities) are connected in a shared latent space to produce highly\nuseful material representations. We show the utility of MLCM on multiple axes:\n(i) MLCM achieves state-of-the-art performance for material property prediction\non the challenging Materials Project database; (ii) MLCM enables a novel,\nhighly accurate method for inverse design, allowing one to screen for stable\nmaterial with desired properties; and (iii) MLCM allows the extraction of\ninterpretable emergent features that may provide insight to material\nscientists. Further, we explore several novel methods for aligning an arbitrary\nnumber of modalities, improving upon prior art in multimodal learning that\nfocuses on bimodal alignment. Our work brings innovations from the ongoing AI\nrevolution into the domain of materials science and identifies materials as a\ntestbed for the next generation of AI.",
            "author": [
                "Viggo Moro",
                "Charlotte Loh",
                "Rumen Dangovski",
                "Ali Ghorashi",
                "Andrew Ma",
                "Zhuo Chen",
                "Peter Y. Lu",
                "Thomas Christensen",
                "Marin Solja\u010di\u0107"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00111v1",
                "http://arxiv.org/pdf/2312.00111v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cond-mat.mtrl-sci"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03747v1",
            "title": "Classifying patient voice in social media data using neural networks: A\n  comparison of AI models on different data sources and therapeutic domains",
            "updated": "2023-11-30T18:35:24Z",
            "published": "2023-11-30T18:35:24Z",
            "summary": "It is essential that healthcare professionals and members of the healthcare\ncommunity can access and easily understand patient experiences in the real\nworld, so that care standards can be improved and driven towards personalised\ndrug treatment. Social media platforms and message boards are deemed suitable\nsources of patient experience information, as patients have been observed to\ndiscuss and exchange knowledge, look for and provide support online. This paper\ntests the hypothesis that not all online patient experience information can be\ntreated and collected in the same way, as a result of the inherent differences\nin the way individuals talk about their journeys, in different therapeutic\ndomains and or data sources.\n  We used linguistic analysis to understand and identify similarities between\ndatasets, across patient language, between data sources (Reddit, SocialGist)\nand therapeutic domains (cardiovascular, oncology, immunology, neurology). We\ndetected common vocabulary used by patients in the same therapeutic domain\nacross data sources, except for immunology patients, who use unique vocabulary\nbetween the two data sources, and compared to all other datasets. We combined\nlinguistically similar datasets to train classifiers (CNN, transformer) to\naccurately identify patient experience posts from social media, a task we refer\nto as patient voice classification. The cardiovascular and neurology\ntransformer classifiers perform the best in their respective comparisons for\nthe Reddit data source, achieving F1-scores of 0.865 and 1.0 respectively. The\noverall best performing classifier is the transformer classifier trained on all\ndata collected for this experiment, achieving F1-scores ranging between 0.863\nand 0.995 across all therapeutic domain and data source specific test datasets.",
            "author": [
                "Giorgos Lysandrou",
                "Roma English Owen",
                "Vanja Popovic",
                "Grant Le Brun",
                "Beatrice Alex",
                "Elizabeth A. L. Fairley"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03747v1",
                "http://arxiv.org/pdf/2312.03747v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18780v1",
            "title": "MultiResFormer: Transformer with Adaptive Multi-Resolution Modeling for\n  General Time Series Forecasting",
            "updated": "2023-11-30T18:24:33Z",
            "published": "2023-11-30T18:24:33Z",
            "summary": "Transformer-based models have greatly pushed the boundaries of time series\nforecasting recently. Existing methods typically encode time series data into\n$\\textit{patches}$ using one or a fixed set of patch lengths. This, however,\ncould result in a lack of ability to capture the variety of intricate temporal\ndependencies present in real-world multi-periodic time series. In this paper,\nwe propose MultiResFormer, which dynamically models temporal variations by\nadaptively choosing optimal patch lengths. Concretely, at the beginning of each\nlayer, time series data is encoded into several parallel branches, each using a\ndetected periodicity, before going through the transformer encoder block. We\nconduct extensive evaluations on long- and short-term forecasting datasets\ncomparing MultiResFormer with state-of-the-art baselines. MultiResFormer\noutperforms patch-based Transformer baselines on long-term forecasting tasks\nand also consistently outperforms CNN baselines by a large margin, while using\nmuch fewer parameters than these baselines.",
            "author": [
                "Linfeng Du",
                "Ji Xin",
                "Alex Labach",
                "Saba Zuberi",
                "Maksims Volkovs",
                "Rahul G. Krishnan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18780v1",
                "http://arxiv.org/pdf/2311.18780v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18775v1",
            "title": "CoDi-2: In-Context, Interleaved, and Interactive Any-to-Any Generation",
            "updated": "2023-11-30T18:21:25Z",
            "published": "2023-11-30T18:21:25Z",
            "summary": "We present CoDi-2, a versatile and interactive Multimodal Large Language\nModel (MLLM) that can follow complex multimodal interleaved instructions,\nconduct in-context learning (ICL), reason, chat, edit, etc., in an any-to-any\ninput-output modality paradigm. By aligning modalities with language for both\nencoding and generation, CoDi-2 empowers Large Language Models (LLMs) to not\nonly understand complex modality-interleaved instructions and in-context\nexamples, but also autoregressively generate grounded and coherent multimodal\noutputs in the continuous feature space. To train CoDi-2, we build a\nlarge-scale generation dataset encompassing in-context multimodal instructions\nacross text, vision, and audio. CoDi-2 demonstrates a wide range of zero-shot\ncapabilities for multimodal generation, such as in-context learning, reasoning,\nand compositionality of any-to-any modality generation through multi-round\ninteractive conversation. CoDi-2 surpasses previous domain-specific models on\ntasks such as subject-driven image generation, vision transformation, and audio\nediting. CoDi-2 signifies a substantial breakthrough in developing a\ncomprehensive multimodal foundation model adept at interpreting in-context\nlanguage-vision-audio interleaved instructions and producing multimodal\noutputs.",
            "author": [
                "Zineng Tang",
                "Ziyi Yang",
                "Mahmoud Khademi",
                "Yang Liu",
                "Chenguang Zhu",
                "Mohit Bansal"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18775v1",
                "http://arxiv.org/pdf/2311.18775v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.CL",
                "cs.LG",
                "cs.SD",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18773v1",
            "title": "Spacewalk-18: A Benchmark for Multimodal and Long-form Procedural Video\n  Understanding in Novel Domains",
            "updated": "2023-11-30T18:19:23Z",
            "published": "2023-11-30T18:19:23Z",
            "summary": "Learning from videos is an emerging research area that enables robots to\nacquire skills from human demonstrations, such as procedural videos. To do\nthis, video-language models must be able to obtain structured understandings,\nsuch as the temporal segmentation of a demonstration into sequences of actions\nand skills, and to generalize the understandings to novel domains. In pursuit\nof this goal, we introduce Spacewalk-18, a benchmark containing two tasks: (1)\nstep recognition and (2) intra-video retrieval over a dataset of temporally\nsegmented and labeled tasks in International Space Station spacewalk\nrecordings. In tandem, the two tasks quantify a model's ability to make use of:\n(1) out-of-domain visual information; (2) a high temporal context window; and\n(3) multimodal (text + video) domains. This departs from existing benchmarks\nfor procedural video understanding, which typically deal with short context\nlengths and can be solved with a single modality. Spacewalk-18, with its\ninherent multimodal and long-form complexity, exposes the high difficulty of\ntask recognition and segmentation. We find that state-of-the-art methods\nperform poorly on our benchmark, demonstrating that the goal of generalizable\nprocedural video understanding models is far out and underscoring the need to\ndevelop new approaches to these tasks. Data, model, and code will be publicly\nreleased.",
            "author": [
                "Rohan Myer Krishnan",
                "Zitian Tang",
                "Zhiqiu Yu",
                "Chen Sun"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18773v1",
                "http://arxiv.org/pdf/2311.18773v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18769v1",
            "title": "Online Change Points Detection for Linear Dynamical Systems with Finite\n  Sample Guarantees",
            "updated": "2023-11-30T18:08:16Z",
            "published": "2023-11-30T18:08:16Z",
            "summary": "The problem of online change point detection is to detect abrupt changes in\nproperties of time series, ideally as soon as possible after those changes\noccur. Existing work on online change point detection either assumes i.i.d\ndata, focuses on asymptotic analysis, does not present theoretical guarantees\non the trade-off between detection accuracy and detection delay, or is only\nsuitable for detecting single change points. In this work, we study the online\nchange point detection problem for linear dynamical systems with unknown\ndynamics, where the data exhibits temporal correlations and the system could\nhave multiple change points. We develop a data-dependent threshold that can be\nused in our test that allows one to achieve a pre-specified upper bound on the\nprobability of making a false alarm. We further provide a finite-sample-based\nbound for the probability of detecting a change point. Our bound demonstrates\nhow parameters used in our algorithm affect the detection probability and\ndelay, and provides guidance on the minimum required time between changes to\nguarantee detection.",
            "author": [
                "Lei Xin",
                "George Chiu",
                "Shreyas Sundaram"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18769v1",
                "http://arxiv.org/pdf/2311.18769v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.SY",
                "eess.SY",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18768v1",
            "title": "Evaluating the Impact of Flaky Simulators on Testing Autonomous Driving\n  Systems",
            "updated": "2023-11-30T18:08:02Z",
            "published": "2023-11-30T18:08:02Z",
            "summary": "Simulators are widely used to test Autonomous Driving Systems (ADS), but\ntheir potential flakiness can lead to inconsistent test results. We investigate\ntest flakiness in simulation-based testing of ADS by addressing two key\nquestions: (1) How do flaky ADS simulations impact automated testing that\nrelies on randomized algorithms? and (2) Can machine learning (ML) effectively\nidentify flaky ADS tests while decreasing the required number of test reruns?\nOur empirical results, obtained from two widely-used open-source ADS simulators\nand five diverse ADS test setups, show that test flakiness in ADS is a common\noccurrence and can significantly impact the test results obtained by randomized\nalgorithms. Further, our ML classifiers effectively identify flaky ADS tests\nusing only a single test run, achieving F1-scores of $85$%, $82$% and $96$% for\nthree different ADS test setups. Our classifiers significantly outperform our\nnon-ML baseline, which requires executing tests at least twice, by $31$%,\n$21$%, and $13$% in F1-score performance, respectively. We conclude with a\ndiscussion on the scope, implications and limitations of our study. We provide\nour complete replication package in a Github repository.",
            "author": [
                "Mohammad Hossein Amini",
                "Shervin Naseri",
                "Shiva Nejati"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18768v1",
                "http://arxiv.org/pdf/2311.18768v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18765v2",
            "title": "MLLMs-Augmented Visual-Language Representation Learning",
            "updated": "2023-12-01T15:38:31Z",
            "published": "2023-11-30T18:05:52Z",
            "summary": "Visual-language pre-training (VLP) has achieved remarkable success in\nmulti-modal tasks, largely attributed to the availability of large-scale\nimage-text datasets. In this work, we demonstrate that multi-modal large\nlanguage models (MLLMs) can enhance visual-language representation learning by\nimproving data quality. Our approach is simple, utilizing MLLMs to extend\nmultiple captions for each image. To prevent the bias introduced by MLLMs'\nhallucinations and intrinsic caption styles, we propose \"text shearing\" to\nmaintain the same length for extended captions as that of the original\ncaptions. In image-text retrieval, our method consistently obtains 5.6 ~ 35.0%\nand 16.8 ~ 46.1% improvement on R@1 under the fine-tuning and zero-shot\nsettings, respectively. Notably, we obtain zero-shot results that are\ncomparable to fine-tuning on target datasets, which encourages more exploration\nof the versatile use of MLLMs.",
            "author": [
                "Yanqing Liu",
                "Kai Wang",
                "Wenqi Shao",
                "Ping Luo",
                "Yu Qiao",
                "Mike Zheng Shou",
                "Kaipeng Zhang",
                "Yang You"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18765v2",
                "http://arxiv.org/pdf/2311.18765v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18763v1",
            "title": "Continual Diffusion with STAMINA: STack-And-Mask INcremental Adapters",
            "updated": "2023-11-30T18:04:21Z",
            "published": "2023-11-30T18:04:21Z",
            "summary": "Recent work has demonstrated a remarkable ability to customize text-to-image\ndiffusion models to multiple, fine-grained concepts in a sequential (i.e.,\ncontinual) manner while only providing a few example images for each concept.\nThis setting is known as continual diffusion. Here, we ask the question: Can we\nscale these methods to longer concept sequences without forgetting? Although\nprior work mitigates the forgetting of previously learned concepts, we show\nthat its capacity to learn new tasks reaches saturation over longer sequences.\nWe address this challenge by introducing a novel method, STack-And-Mask\nINcremental Adapters (STAMINA), which is composed of low-ranked\nattention-masked adapters and customized MLP tokens. STAMINA is designed to\nenhance the robust fine-tuning properties of LoRA for sequential concept\nlearning via learnable hard-attention masks parameterized with low rank MLPs,\nenabling precise, scalable learning via sparse adaptation. Notably, all\nintroduced trainable parameters can be folded back into the model after\ntraining, inducing no additional inference parameter costs. We show that\nSTAMINA outperforms the prior SOTA for the setting of text-to-image continual\ncustomization on a 50-concept benchmark composed of landmarks and human faces,\nwith no stored replay data. Additionally, we extended our method to the setting\nof continual learning for image classification, demonstrating that our gains\nalso translate to state-of-the-art performance in this standard benchmark.",
            "author": [
                "James Seale Smith",
                "Yen-Chang Hsu",
                "Zsolt Kira",
                "Yilin Shen",
                "Hongxia Jin"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18763v1",
                "http://arxiv.org/pdf/2311.18763v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00589v1",
            "title": "Merlin:Empowering Multimodal LLMs with Foresight Minds",
            "updated": "2023-11-30T17:57:34Z",
            "published": "2023-11-30T17:57:34Z",
            "summary": "Humans possess the remarkable ability to foresee the future to a certain\nextent based on present observations, a skill we term as foresight minds.\nHowever, this capability remains largely under explored within existing\nMultimodal Large Language Models (MLLMs), hindering their capacity to learn the\nfundamental principles of how things operate and the intentions behind the\nobserved subjects. To address this issue, we introduce the integration of\nfuture modeling into the existing learning frameworks of MLLMs. By utilizing\nthe subject trajectory, a highly structured representation of a consecutive\nframe sequence, as a learning objective, we aim to bridge the gap between the\npast and the future. We propose two innovative methods to empower MLLMs with\nforesight minds, Foresight Pre-Training (FPT) and Foresight Instruction-Tuning\n(FIT), which are inspired by the modern learning paradigm of LLMs.\nSpecifically, FPT jointly training various tasks centered on trajectories,\nenabling MLLMs to learn how to attend and predict entire trajectories from a\ngiven initial observation. Then, FIT requires MLLMs to first predict\ntrajectories of related objects and then reason about potential future events\nbased on them. Aided by FPT and FIT, we build a novel and unified MLLM named\nMerlin that supports multi-images input and analysis about potential actions of\nmultiple objects for the future reasoning. Experimental results show Merlin\npowerful foresight minds with impressive performance on both future reasoning\nand visual comprehension tasks.",
            "author": [
                "En Yu",
                "Liang Zhao",
                "Yana Wei",
                "Jinrong Yang",
                "Dongming Wu",
                "Lingyu Kong",
                "Haoran Wei",
                "Tiancai Wang",
                "Zheng Ge",
                "Xiangyu Zhang",
                "Wenbing Tao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00589v1",
                "http://arxiv.org/pdf/2312.00589v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18751v1",
            "title": "Language Model Agents Suffer from Compositional Generalization in Web\n  Automation",
            "updated": "2023-11-30T17:50:47Z",
            "published": "2023-11-30T17:50:47Z",
            "summary": "Language model agents (LMA) recently emerged as a promising paradigm on\nmuti-step decision making tasks, often outperforming humans and other\nreinforcement learning agents. Despite the promise, their performance on\nreal-world applications that often involve combinations of tasks is still\nunderexplored. In this work, we introduce a new benchmark, called CompWoB -- 50\nnew compositional web automation tasks reflecting more realistic assumptions.\nWe show that while existing prompted LMAs (gpt-3.5-turbo or gpt-4) achieve\n94.0% average success rate on base tasks, their performance degrades to 24.9%\nsuccess rate on compositional tasks. On the other hand, transferred LMAs\n(finetuned only on base tasks) show less generalization gap, dropping from\n85.4% to 54.8%. By balancing data distribution across tasks, we train a new\nmodel, HTML-T5++, that surpasses human-level performance (95.2%) on MiniWoB,\nand achieves the best zero-shot performance on CompWoB (61.5%). While these\nhighlight the promise of small-scale finetuned and transferred models for\ncompositional generalization, their performance further degrades under\ndifferent instruction compositions changing combinational order. In contrast to\nthe recent remarkable success of LMA, our benchmark and detailed analysis\nemphasize the necessity of building LMAs that are robust and generalizable to\ntask compositionality for real-world deployment.",
            "author": [
                "Hiroki Furuta",
                "Yutaka Matsuo",
                "Aleksandra Faust",
                "Izzeddin Gur"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18751v1",
                "http://arxiv.org/pdf/2311.18751v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18749v1",
            "title": "TransCORALNet: A Two-Stream Transformer CORAL Networks for Supply Chain\n  Credit Assessment Cold Start",
            "updated": "2023-11-30T17:47:02Z",
            "published": "2023-11-30T17:47:02Z",
            "summary": "This paper proposes an interpretable two-stream transformer CORAL networks\n(TransCORALNet) for supply chain credit assessment under the segment industry\nand cold start problem. The model aims to provide accurate credit assessment\nprediction for new supply chain borrowers with limited historical data. Here,\nthe two-stream domain adaptation architecture with correlation alignment\n(CORAL) loss is used as a core model and is equipped with transformer, which\nprovides insights about the learned features and allow efficient\nparallelization during training. Thanks to the domain adaptation capability of\nthe proposed model, the domain shift between the source and target domain is\nminimized. Therefore, the model exhibits good generalization where the source\nand target do not follow the same distribution, and a limited amount of target\nlabeled instances exist. Furthermore, we employ Local Interpretable\nModel-agnostic Explanations (LIME) to provide more insight into the model\nprediction and identify the key features contributing to supply chain credit\nassessment decisions. The proposed model addresses four significant supply\nchain credit assessment challenges: domain shift, cold start, imbalanced-class\nand interpretability. Experimental results on a real-world data set demonstrate\nthe superiority of TransCORALNet over a number of state-of-the-art baselines in\nterms of accuracy. The code is available on GitHub\nhttps://github.com/JieJieNiu/TransCORALN .",
            "author": [
                "Jie Shi",
                "Arno P. J. M. Siebes",
                "Siamak Mehrkanoon"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18749v1",
                "http://arxiv.org/pdf/2311.18749v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "q-fin.RM",
                "I.2; I.5"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18746v1",
            "title": "A data-science pipeline to enable the Interpretability of Many-Objective\n  Feature Selection",
            "updated": "2023-11-30T17:44:22Z",
            "published": "2023-11-30T17:44:22Z",
            "summary": "Many-Objective Feature Selection (MOFS) approaches use four or more\nobjectives to determine the relevance of a subset of features in a supervised\nlearning task. As a consequence, MOFS typically returns a large set of\nnon-dominated solutions, which have to be assessed by the data scientist in\norder to proceed with the final choice. Given the multi-variate nature of the\nassessment, which may include criteria (e.g. fairness) not related to\npredictive accuracy, this step is often not straightforward and suffers from\nthe lack of existing tools. For instance, it is common to make use of a tabular\npresentation of the solutions, which provide little information about the\ntrade-offs and the relations between criteria over the set of solutions.\n  This paper proposes an original methodology to support data scientists in the\ninterpretation and comparison of the MOFS outcome by combining post-processing\nand visualisation of the set of solutions. The methodology supports the data\nscientist in the selection of an optimal feature subset by providing her with\nhigh-level information at three different levels: objectives, solutions, and\nindividual features.\n  The methodology is experimentally assessed on two feature selection tasks\nadopting a GA-based MOFS with six objectives (number of selected features,\nbalanced accuracy, F1-Score, variance inflation factor, statistical parity, and\nequalised odds). The results show the added value of the methodology in the\nselection of the final subset of features.",
            "author": [
                "Uchechukwu F. Njoku",
                "Alberto Abell\u00f3",
                "Besim Bilalli",
                "Gianluca Bontempi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18746v1",
                "http://arxiv.org/pdf/2311.18746v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18744v1",
            "title": "$\\mathbb{Z}_2\\times \\mathbb{Z}_2$ Equivariant Quantum Neural Networks:\n  Benchmarking against Classical Neural Networks",
            "updated": "2023-11-30T17:41:46Z",
            "published": "2023-11-30T17:41:46Z",
            "summary": "This paper presents a comprehensive comparative analysis of the performance\nof Equivariant Quantum Neural Networks (EQNN) and Quantum Neural Networks\n(QNN), juxtaposed against their classical counterparts: Equivariant Neural\nNetworks (ENN) and Deep Neural Networks (DNN). We evaluate the performance of\neach network with two toy examples for a binary classification task, focusing\non model complexity (measured by the number of parameters) and the size of the\ntraining data set. Our results show that the $\\mathbb{Z}_2\\times \\mathbb{Z}_2$\nEQNN and the QNN provide superior performance for smaller parameter sets and\nmodest training data samples.",
            "author": [
                "Zhongtian Dong",
                "Mar\u00e7al Comajoan Cara",
                "Gopal Ramesh Dahale",
                "Roy T. Forestano",
                "Sergei Gleyzer",
                "Daniel Justice",
                "Kyoungchul Kong",
                "Tom Magorsch",
                "Konstantin T. Matchev",
                "Katia Matcheva",
                "Eyup B. Unlu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18744v1",
                "http://arxiv.org/pdf/2311.18744v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.LG",
                "hep-ph",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18743v3",
            "title": "AlignBench: Benchmarking Chinese Alignment of Large Language Models",
            "updated": "2023-12-05T16:04:15Z",
            "published": "2023-11-30T17:41:30Z",
            "summary": "Alignment has become a critical step for instruction-tuned Large Language\nModels (LLMs) to become helpful assistants. However, effective evaluation of\nalignment for emerging Chinese LLMs is still significantly lacking, calling for\nreal-scenario grounded, open-ended, challenging and automatic evaluations\ntailored for alignment. To fill in this gap, we introduce AlignBench, a\ncomprehensive multi-dimensional benchmark for evaluating LLMs' alignment in\nChinese. Equipped with a human-in-the-loop data curation pipeline, our\nbenchmark employs a rule-calibrated multi-dimensional LLM-as-Judge with\nChain-of-Thought to generate explanations and final ratings as evaluations,\nensuring high reliability and interpretability. Furthermore, we report\nAlignBench evaluated by CritiqueLLM, a dedicated Chinese evaluator LLM that\nrecovers 95% of GPT-4's evaluation ability. We will provide public APIs for\nevaluating AlignBench with CritiqueLLM to facilitate the evaluation of LLMs'\nChinese alignment. All evaluation codes, data, and LLM generations are\navailable at \\url{https://github.com/THUDM/AlignBench}.",
            "author": [
                "Xiao Liu",
                "Xuanyu Lei",
                "Shengyuan Wang",
                "Yue Huang",
                "Zhuoer Feng",
                "Bosi Wen",
                "Jiale Cheng",
                "Pei Ke",
                "Yifan Xu",
                "Weng Lam Tam",
                "Xiaohan Zhang",
                "Lichao Sun",
                "Hongning Wang",
                "Jing Zhang",
                "Minlie Huang",
                "Yuxiao Dong",
                "Jie Tang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18743v3",
                "http://arxiv.org/pdf/2311.18743v3"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18741v1",
            "title": "VREM-FL: Mobility-Aware Computation-Scheduling Co-Design for Vehicular\n  Federated Learning",
            "updated": "2023-11-30T17:38:54Z",
            "published": "2023-11-30T17:38:54Z",
            "summary": "Assisted and autonomous driving are rapidly gaining momentum, and will soon\nbecome a reality. Among their key enablers, artificial intelligence and machine\nlearning are expected to play a prominent role, also thanks to the massive\namount of data that smart vehicles will collect from their onboard sensors. In\nthis domain, federated learning is one of the most effective and promising\ntechniques for training global machine learning models, while preserving data\nprivacy at the vehicles and optimizing communications resource usage. In this\nwork, we propose VREM-FL, a computation-scheduling co-design for vehicular\nfederated learning that leverages mobility of vehicles in conjunction with\nestimated 5G radio environment maps. VREM-FL jointly optimizes the global model\nlearned at the server while wisely allocating communication resources. This is\nachieved by orchestrating local computations at the vehicles in conjunction\nwith the transmission of their local model updates in an adaptive and\npredictive fashion, by exploiting radio channel maps. The proposed algorithm\ncan be tuned to trade model training time for radio resource usage.\nExperimental results demonstrate the efficacy of utilizing radio maps. VREM-FL\noutperforms literature benchmarks for both a linear regression model (learning\ntime reduced by 28%) and a deep neural network for a semantic image\nsegmentation task (doubling the number of model updates within the same time\nwindow).",
            "author": [
                "Luca Ballotta",
                "Nicol\u00f2 Dal Fabbro",
                "Giovanni Perin",
                "Luca Schenato",
                "Michele Rossi",
                "Giuseppe Piro"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18741v1",
                "http://arxiv.org/pdf/2311.18741v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.AI",
                "cs.DC",
                "cs.LG",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18736v1",
            "title": "Controlgym: Large-Scale Safety-Critical Control Environments for\n  Benchmarking Reinforcement Learning Algorithms",
            "updated": "2023-11-30T17:34:05Z",
            "published": "2023-11-30T17:34:05Z",
            "summary": "We introduce controlgym, a library of thirty-six safety-critical industrial\ncontrol settings, and ten infinite-dimensional partial differential equation\n(PDE)-based control problems. Integrated within the OpenAI Gym/Gymnasium (Gym)\nframework, controlgym allows direct applications of standard reinforcement\nlearning (RL) algorithms like stable-baselines3. Our control environments\ncomplement those in Gym with continuous, unbounded action and observation\nspaces, motivated by real-world control applications. Moreover, the PDE control\nenvironments uniquely allow the users to extend the state dimensionality of the\nsystem to infinity while preserving the intrinsic dynamics. This feature is\ncrucial for evaluating the scalability of RL algorithms for control. This\nproject serves the learning for dynamics & control (L4DC) community, aiming to\nexplore key questions: the convergence of RL algorithms in learning control\npolicies; the stability and robustness issues of learning-based controllers;\nand the scalability of RL algorithms to high- and potentially\ninfinite-dimensional systems. We open-source the controlgym project at\nhttps://github.com/xiangyuan-zhang/controlgym.",
            "author": [
                "Xiangyuan Zhang",
                "Weichao Mao",
                "Saviz Mowlavi",
                "Mouhacine Benosman",
                "Tamer Ba\u015far"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18736v1",
                "http://arxiv.org/pdf/2311.18736v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.AI",
                "cs.CE",
                "cs.LG",
                "cs.SY",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18735v1",
            "title": "Dimension Mixer: A Generalized Method for Structured Sparsity in Deep\n  Neural Networks",
            "updated": "2023-11-30T17:30:45Z",
            "published": "2023-11-30T17:30:45Z",
            "summary": "The recent success of multiple neural architectures like CNNs, Transformers,\nand MLP-Mixers motivated us to look for similarities and differences between\nthem. We found that these architectures can be interpreted through the lens of\na general concept of dimension mixing. Research on coupling flows and the\nbutterfly transform shows that partial and hierarchical signal mixing schemes\nare sufficient for efficient and expressive function approximation. In this\nwork, we study group-wise sparse, non-linear, multi-layered and learnable\nmixing schemes of inputs and find that they are complementary to many standard\nneural architectures. Following our observations and drawing inspiration from\nthe Fast Fourier Transform, we generalize Butterfly Structure to use non-linear\nmixer function allowing for MLP as mixing function called Butterfly MLP. We\nwere also able to mix along sequence dimension for Transformer-based\narchitectures called Butterfly Attention. Experiments on CIFAR and LRA datasets\ndemonstrate that the proposed Non-Linear Butterfly Mixers are efficient and\nscale well when the host architectures are used as mixing function.\nAdditionally, we propose Patch-Only MLP-Mixer for processing spatial 2D signals\ndemonstrating a different dimension mixing strategy.",
            "author": [
                "Suman Sapkota",
                "Binod Bhattarai"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18735v1",
                "http://arxiv.org/pdf/2311.18735v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18732v1",
            "title": "Indoor Millimeter Wave Localization using Multiple Self-Supervised Tiny\n  Neural Networks",
            "updated": "2023-11-30T17:27:22Z",
            "published": "2023-11-30T17:27:22Z",
            "summary": "We consider the localization of a mobile millimeter-wave client in a large\nindoor environment using multilayer perceptron neural networks (NNs). Instead\nof training and deploying a single deep model, we proceed by choosing among\nmultiple tiny NNs trained in a self-supervised manner. The main challenge then\nbecomes to determine and switch to the best NN among the available ones, as an\nincorrect NN will fail to localize the client. In order to upkeep the\nlocalization accuracy, we propose two switching schemes: one based on a Kalman\nfilter, and one based on the statistical distribution of the training data. We\nanalyze the proposed schemes via simulations, showing that our approach\noutperforms both geometric localization schemes and the use of a single NN.",
            "author": [
                "Anish Shastri",
                "Andres Garcia-Saavedra",
                "Paolo Casari"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18732v1",
                "http://arxiv.org/pdf/2311.18732v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP",
                "cs.LG",
                "cs.NI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18729v1",
            "title": "Learning One-Shot 4D Head Avatar Synthesis using Synthetic Data",
            "updated": "2023-11-30T17:26:33Z",
            "published": "2023-11-30T17:26:33Z",
            "summary": "Existing one-shot 4D head synthesis methods usually learn from monocular\nvideos with the aid of 3DMM reconstruction, yet the latter is evenly\nchallenging which restricts them from reasonable 4D head synthesis. We present\na method to learn one-shot 4D head synthesis via large-scale synthetic data.\nThe key is to first learn a part-wise 4D generative model from monocular images\nvia adversarial learning, to synthesize multi-view images of diverse identities\nand full motions as training data; then leverage a transformer-based animatable\ntriplane reconstructor to learn 4D head reconstruction using the synthetic\ndata. A novel learning strategy is enforced to enhance the generalizability to\nreal images by disentangling the learning process of 3D reconstruction and\nreenactment. Experiments demonstrate our superiority over the prior art.",
            "author": [
                "Yu Deng",
                "Duomin Wang",
                "Xiaohang Ren",
                "Xingyu Chen",
                "Baoyuan Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18729v1",
                "http://arxiv.org/pdf/2311.18729v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18727v1",
            "title": "Automatic Functional Differentiation in JAX",
            "updated": "2023-11-30T17:23:40Z",
            "published": "2023-11-30T17:23:40Z",
            "summary": "We extend JAX with the capability to automatically differentiate higher-order\nfunctions (functionals and operators). By representing functions as a\ngeneralization of arrays, we seamlessly use JAX's existing primitive system to\nimplement higher-order functions. We present a set of primitive operators that\nserve as foundational building blocks for constructing several key types of\nfunctionals. For every introduced primitive operator, we derive and implement\nboth linearization and transposition rules, aligning with JAX's internal\nprotocols for forward and reverse mode automatic differentiation. This\nenhancement allows for functional differentiation in the same syntax\ntraditionally use for functions. The resulting functional gradients are\nthemselves functions ready to be invoked in python. We showcase this tool's\nefficacy and simplicity through applications where functional derivatives are\nindispensable. The source code of this work is released at\nhttps://github.com/sail-sg/autofd .",
            "author": [
                "Min Lin"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18727v1",
                "http://arxiv.org/pdf/2311.18727v1"
            ],
            "primary_category": "cs.PL",
            "category": [
                "cs.PL",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18725v1",
            "title": "AI in Pharma for Personalized Sequential Decision-Making: Methods,\n  Applications and Opportunities",
            "updated": "2023-11-30T17:23:17Z",
            "published": "2023-11-30T17:23:17Z",
            "summary": "In the pharmaceutical industry, the use of artificial intelligence (AI) has\nseen consistent growth over the past decade. This rise is attributed to major\nadvancements in statistical machine learning methodologies, computational\ncapabilities and the increased availability of large datasets. AI techniques\nare applied throughout different stages of drug development, ranging from drug\ndiscovery to post-marketing benefit-risk assessment. Kolluri et al. provided a\nreview of several case studies that span these stages, featuring key\napplications such as protein structure prediction, success probability\nestimation, subgroup identification, and AI-assisted clinical trial monitoring.\nFrom a regulatory standpoint, there was a notable uptick in submissions\nincorporating AI components in 2021. The most prevalent therapeutic areas\nleveraging AI were oncology (27%), psychiatry (15%), gastroenterology (12%),\nand neurology (11%). The paradigm of personalized or precision medicine has\ngained significant traction in recent research, partly due to advancements in\nAI techniques \\cite{hamburg2010path}. This shift has had a transformative\nimpact on the pharmaceutical industry. Departing from the traditional\n\"one-size-fits-all\" model, personalized medicine incorporates various\nindividual factors, such as environmental conditions, lifestyle choices, and\nhealth histories, to formulate customized treatment plans. By utilizing\nsophisticated machine learning algorithms, clinicians and researchers are\nbetter equipped to make informed decisions in areas such as disease prevention,\ndiagnosis, and treatment selection, thereby optimizing health outcomes for each\nindividual.",
            "author": [
                "Yuhan Li",
                "Hongtao Zhang",
                "Keaven Anderson",
                "Songzi Li",
                "Ruoqing Zhu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18725v1",
                "http://arxiv.org/pdf/2311.18725v1"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME",
                "cs.LG",
                "stat.AP",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18724v1",
            "title": "Routing-Guided Learned Product Quantization for Graph-Based Approximate\n  Nearest Neighbor Search",
            "updated": "2023-11-30T17:22:55Z",
            "published": "2023-11-30T17:22:55Z",
            "summary": "Given a vector dataset $\\mathcal{X}$, a query vector $\\vec{x}_q$, graph-based\nApproximate Nearest Neighbor Search (ANNS) aims to build a proximity graph (PG)\nas an index of $\\mathcal{X}$ and approximately return vectors with minimum\ndistances to $\\vec{x}_q$ by searching over the PG index. It suffers from the\nlarge-scale $\\mathcal{X}$ because a PG with full vectors is too large to fit\ninto the memory, e.g., a billion-scale $\\mathcal{X}$ in 128 dimensions would\nconsume nearly 600 GB memory. To solve this, Product Quantization (PQ)\nintegrated graph-based ANNS is proposed to reduce the memory usage, using\nsmaller compact codes of quantized vectors in memory instead of the large\noriginal vectors. Existing PQ methods do not consider the important routing\nfeatures of PG, resulting in low-quality quantized vectors that affect the\nANNS's effectiveness. In this paper, we present an end-to-end Routing-guided\nlearned Product Quantization (RPQ) for graph-based ANNS. It consists of (1) a\n\\textit{differentiable quantizer} used to make the standard discrete PQ\ndifferentiable to suit for back-propagation of end-to-end learning, (2) a\n\\textit{sampling-based feature extractor} used to extract neighborhood and\nrouting features of a PG, and (3) a \\textit{multi-feature joint training\nmodule} with two types of feature-aware losses to continuously optimize the\ndifferentiable quantizer. As a result, the inherent features of a PG would be\nembedded into the learned PQ, generating high-quality quantized vectors.\nMoreover, we integrate our RPQ with the state-of-the-art DiskANN and existing\npopular PGs to improve their performance. Comprehensive experiments on\nreal-world large-scale datasets (from 1M to 1B) demonstrate RPQ's superiority,\ne.g., 1.7$\\times$-4.2$\\times$ improvement on QPS at the same recall@10 of 95\\%.",
            "author": [
                "Qiang Yue",
                "Xiaoliang Xu",
                "Yuxiang Wang",
                "Yikun Tao",
                "Xuliyuan Luo"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18724v1",
                "http://arxiv.org/pdf/2311.18724v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18718v1",
            "title": "Steering Deep Feature Learning with Backward Aligned Feature Updates",
            "updated": "2023-11-30T17:19:18Z",
            "published": "2023-11-30T17:19:18Z",
            "summary": "Deep learning succeeds by doing hierarchical feature learning, yet tuning\nHyper-Parameters (HP) such as initialization scales, learning rates etc., only\ngive indirect control over this behavior. In this paper, we propose the\nalignment between the feature updates and the backward pass as a key notion to\npredict, measure and control feature learning. On the one hand, we show that\nwhen alignment holds, the magnitude of feature updates after one SGD step is\nrelated to the magnitude of the forward and backward passes by a simple and\ngeneral formula. This leads to techniques to automatically adjust HPs\n(initialization scales and learning rates) at initialization and throughout\ntraining to attain a desired feature learning behavior. On the other hand, we\nshow that, at random initialization, this alignment is determined by the\nspectrum of a certain kernel, and that well-conditioned layer-to-layer\nJacobians (aka dynamical isometry) implies alignment. Finally, we investigate\nReLU MLPs and ResNets in the large width-then-depth limit. Combining hints from\nrandom matrix theory and numerical experiments, we show that (i) in MLP with\niid initializations, alignment degenerates with depth, making it impossible to\nstart training, and that (ii) in ResNets, the branch scale\n$1/\\sqrt{\\text{depth}}$ is the only one maintaining non-trivial alignment at\ninfinite depth.",
            "author": [
                "L\u00e9na\u00efc Chizat",
                "Praneeth Netrapalli"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18718v1",
                "http://arxiv.org/pdf/2311.18718v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "68T07"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00105v1",
            "title": "Improving the Robustness of Quantized Deep Neural Networks to White-Box\n  Attacks using Stochastic Quantization and Information-Theoretic Ensemble\n  Training",
            "updated": "2023-11-30T17:15:58Z",
            "published": "2023-11-30T17:15:58Z",
            "summary": "Most real-world applications that employ deep neural networks (DNNs) quantize\nthem to low precision to reduce the compute needs. We present a method to\nimprove the robustness of quantized DNNs to white-box adversarial attacks. We\nfirst tackle the limitation of deterministic quantization to fixed ``bins'' by\nintroducing a differentiable Stochastic Quantizer (SQ). We explore the\nhypothesis that different quantizations may collectively be more robust than\neach quantized DNN. We formulate a training objective to encourage different\nquantized DNNs to learn different representations of the input image. The\ntraining objective captures diversity and accuracy via mutual information\nbetween ensemble members. Through experimentation, we demonstrate substantial\nimprovement in robustness against $L_\\infty$ attacks even if the attacker is\nallowed to backpropagate through SQ (e.g., > 50\\% accuracy to PGD(5/255) on\nCIFAR10 without adversarial training), compared to vanilla DNNs as well as\nexisting ensembles of quantized DNNs. We extend the method to detect attacks\nand generate robustness profiles in the adversarial information plane (AIP),\ntowards a unified analysis of different threat models by correlating the MI and\naccuracy.",
            "author": [
                "Saurabh Farkya",
                "Aswin Raghavan",
                "Avi Ziskind"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00105v1",
                "http://arxiv.org/pdf/2312.00105v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.CR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18715v1",
            "title": "Accelerating Flow Simulations using Online Dynamic Mode Decomposition",
            "updated": "2023-11-30T17:15:15Z",
            "published": "2023-11-30T17:15:15Z",
            "summary": "We develop an on-the-fly reduced-order model (ROM) integrated with a flow\nsimulation, gradually replacing a corresponding full-order model (FOM) of a\nphysics solver. Unlike offline methods requiring a separate FOM-only simulation\nprior to model reduction, our approach constructs a ROM dynamically during the\nsimulation, replacing the FOM when deemed credible. Dynamic mode decomposition\n(DMD) is employed for online ROM construction, with a single snapshot vector\nused for rank-1 updates in each iteration. Demonstrated on a flow over a\ncylinder with Re = 100, our hybrid FOM/ROM simulation is verified in terms of\nthe Strouhal number, resulting in a 4.4 times speedup compared to the FOM\nsolver.",
            "author": [
                "Seung Won Suh",
                "Seung Whan Chung",
                "Peer-Timo Bremer",
                "Youngsoo Choi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18715v1",
                "http://arxiv.org/pdf/2311.18715v1"
            ],
            "primary_category": "physics.flu-dyn",
            "category": [
                "physics.flu-dyn",
                "cs.NA",
                "math.NA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18710v1",
            "title": "Meta-Prior: Meta learning for Adaptive Inverse Problem Solvers",
            "updated": "2023-11-30T17:02:27Z",
            "published": "2023-11-30T17:02:27Z",
            "summary": "Deep neural networks have become a foundational tool for addressing imaging\ninverse problems. They are typically trained for a specific task, with a\nsupervised loss to learn a mapping from the observations to the image to\nrecover. However, real-world imaging challenges often lack ground truth data,\nrendering traditional supervised approaches ineffective. Moreover, for each new\nimaging task, a new model needs to be trained from scratch, wasting time and\nresources. To overcome these limitations, we introduce a novel approach based\non meta-learning. Our method trains a meta-model on a diverse set of imaging\ntasks that allows the model to be efficiently fine-tuned for specific tasks\nwith few fine-tuning steps. We show that the proposed method extends to the\nunsupervised setting, where no ground truth data is available. In its bilevel\nformulation, the outer level uses a supervised loss, that evaluates how well\nthe fine-tuned model performs, while the inner loss can be either supervised or\nunsupervised, relying only on the measurement operator. This allows the\nmeta-model to leverage a few ground truth samples for each task while being\nable to generalize to new imaging tasks. We show that in simple settings, this\napproach recovers the Bayes optimal estimator, illustrating the soundness of\nour approach. We also demonstrate our method's effectiveness on various tasks,\nincluding image processing and magnetic resonance imaging.",
            "author": [
                "Matthieu Terris",
                "Thomas Moreau"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18710v1",
                "http://arxiv.org/pdf/2311.18710v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00103v1",
            "title": "DeepEn2023: Energy Datasets for Edge Artificial Intelligence",
            "updated": "2023-11-30T16:54:36Z",
            "published": "2023-11-30T16:54:36Z",
            "summary": "Climate change poses one of the most significant challenges to humanity. As a\nresult of these climatic changes, the frequency of weather, climate, and\nwater-related disasters has multiplied fivefold over the past 50 years,\nresulting in over 2 million deaths and losses exceeding $3.64 trillion USD.\nLeveraging AI-powered technologies for sustainable development and combating\nclimate change is a promising avenue. Numerous significant publications are\ndedicated to using AI to improve renewable energy forecasting, enhance waste\nmanagement, and monitor environmental changes in real time. However, very few\nresearch studies focus on making AI itself environmentally sustainable. This\noversight regarding the sustainability of AI within the field might be\nattributed to a mindset gap and the absence of comprehensive energy datasets.\nIn addition, with the ubiquity of edge AI systems and applications, especially\non-device learning, there is a pressing need to measure, analyze, and optimize\ntheir environmental sustainability, such as energy efficiency. To this end, in\nthis paper, we propose large-scale energy datasets for edge AI, named\nDeepEn2023, covering a wide range of kernels, state-of-the-art deep neural\nnetwork models, and popular edge AI applications. We anticipate that DeepEn2023\nwill improve transparency in sustainability in on-device deep learning across a\nrange of edge AI systems and applications. For more information, including\naccess to the dataset and code, please visit\nhttps://amai-gsu.github.io/DeepEn2023.",
            "author": [
                "Xiaolong Tu",
                "Anik Mallik",
                "Haoxin Wang",
                "Jiang Xie"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00103v1",
                "http://arxiv.org/pdf/2312.00103v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.PF"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18703v1",
            "title": "Predictable Reinforcement Learning Dynamics through Entropy Rate\n  Minimization",
            "updated": "2023-11-30T16:53:32Z",
            "published": "2023-11-30T16:53:32Z",
            "summary": "In Reinforcement Learning (RL), agents have no incentive to exhibit\npredictable behaviors, and are often pushed (through e.g. policy entropy\nregularization) to randomize their actions in favor of exploration. From a\nhuman perspective, this makes RL agents hard to interpret and predict, and from\na safety perspective, even harder to formally verify. We propose a novel method\nto induce predictable behavior in RL agents, referred to as\nPredictability-Aware RL (PA-RL), which employs the state sequence entropy rate\nas a predictability measure. We show how the entropy rate can be formulated as\nan average reward objective, and since its entropy reward function is\npolicy-dependent, we introduce an action-dependent surrogate entropy enabling\nthe use of PG methods. We prove that deterministic policies minimizing the\naverage surrogate reward exist and also minimize the actual entropy rate, and\nshow how, given a learned dynamical model, we are able to approximate the value\nfunction associated to the true entropy rate. Finally, we demonstrate the\neffectiveness of the approach in RL tasks inspired by human-robot use-cases,\nand show how it produces agents with more predictable behavior while achieving\nnear-optimal rewards.",
            "author": [
                "Daniel Jarne Ornia",
                "Giannis Delimpaltadakis",
                "Jens Kober",
                "Javier Alonso-Mora"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18703v1",
                "http://arxiv.org/pdf/2311.18703v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03746v1",
            "title": "Evaluating Large Language Model Creativity from a Literary Perspective",
            "updated": "2023-11-30T16:46:25Z",
            "published": "2023-11-30T16:46:25Z",
            "summary": "This paper assesses the potential for large language models (LLMs) to serve\nas assistive tools in the creative writing process, by means of a single,\nin-depth case study. In the course of the study, we develop interactive and\nmulti-voice prompting strategies that interleave background descriptions (scene\nsetting, plot elements), instructions that guide composition, samples of text\nin the target style, and critical discussion of the given samples. We\nqualitatively evaluate the results from a literary critical perspective, as\nwell as from the standpoint of computational creativity (a sub-field of\nartificial intelligence). Our findings lend support to the view that the\nsophistication of the results that can be achieved with an LLM mirrors the\nsophistication of the prompting.",
            "author": [
                "Murray Shanahan",
                "Catherine Clarke"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03746v1",
                "http://arxiv.org/pdf/2312.03746v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18695v1",
            "title": "Seg2Reg: Differentiable 2D Segmentation to 1D Regression Rendering for\n  360 Room Layout Reconstruction",
            "updated": "2023-11-30T16:42:24Z",
            "published": "2023-11-30T16:42:24Z",
            "summary": "State-of-the-art single-view 360-degree room layout reconstruction methods\nformulate the problem as a high-level 1D (per-column) regression task. On the\nother hand, traditional low-level 2D layout segmentation is simpler to learn\nand can represent occluded regions, but it requires complex post-processing for\nthe targeting layout polygon and sacrifices accuracy. We present Seg2Reg to\nrender 1D layout depth regression from the 2D segmentation map in a\ndifferentiable and occlusion-aware way, marrying the merits of both sides.\nSpecifically, our model predicts floor-plan density for the input\nequirectangular 360-degree image. Formulating the 2D layout representation as a\ndensity field enables us to employ `flattened' volume rendering to form 1D\nlayout depth regression. In addition, we propose a novel 3D warping\naugmentation on layout to improve generalization. Finally, we re-implement\nrecent room layout reconstruction methods into our codebase for benchmarking\nand explore modern backbones and training techniques to serve as the strong\nbaseline. Our model significantly outperforms previous arts. The code will be\nmade available upon publication.",
            "author": [
                "Cheng Sun",
                "Wei-En Tai",
                "Yu-Lin Shih",
                "Kuan-Wei Chen",
                "Yong-Jing Syu",
                "Kent Selwyn The",
                "Yu-Chiang Frank Wang",
                "Hwann-Tzong Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18695v1",
                "http://arxiv.org/pdf/2311.18695v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18694v1",
            "title": "Balancing Summarization and Change Detection in Graph Streams",
            "updated": "2023-11-30T16:39:46Z",
            "published": "2023-11-30T16:39:46Z",
            "summary": "This study addresses the issue of balancing graph summarization and graph\nchange detection. Graph summarization compresses large-scale graphs into a\nsmaller scale. However, the question remains: To what extent should the\noriginal graph be compressed? This problem is solved from the perspective of\ngraph change detection, aiming to detect statistically significant changes\nusing a stream of summary graphs. If the compression rate is extremely high,\nimportant changes can be ignored, whereas if the compression rate is extremely\nlow, false alarms may increase with more memory. This implies that there is a\ntrade-off between compression rate in graph summarization and accuracy in\nchange detection. We propose a novel quantitative methodology to balance this\ntrade-off to simultaneously realize reliable graph summarization and change\ndetection. We introduce a probabilistic structure of hierarchical latent\nvariable model into a graph, thereby designing a parameterized summary graph on\nthe basis of the minimum description length principle. The parameter specifying\nthe summary graph is then optimized so that the accuracy of change detection is\nguaranteed to suppress Type I error probability (probability of raising false\nalarms) to be less than a given confidence level. First, we provide a\ntheoretical framework for connecting graph summarization with change detection.\nThen, we empirically demonstrate its effectiveness on synthetic and real\ndatasets.",
            "author": [
                "Shintaro Fukushima",
                "Kenji Yamanishi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18694v1",
                "http://arxiv.org/pdf/2311.18694v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.IT",
                "cs.LG",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18684v1",
            "title": "Handling Cost and Constraints with Off-Policy Deep Reinforcement\n  Learning",
            "updated": "2023-11-30T16:31:04Z",
            "published": "2023-11-30T16:31:04Z",
            "summary": "By reusing data throughout training, off-policy deep reinforcement learning\nalgorithms offer improved sample efficiency relative to on-policy approaches.\nFor continuous action spaces, the most popular methods for off-policy learning\ninclude policy improvement steps where a learned state-action ($Q$) value\nfunction is maximized over selected batches of data. These updates are often\npaired with regularization to combat associated overestimation of $Q$ values.\nWith an eye toward safety, we revisit this strategy in environments with\n\"mixed-sign\" reward functions; that is, with reward functions that include\nindependent positive (incentive) and negative (cost) terms. This setting is\ncommon in real-world applications, and may be addressed with or without\nconstraints on the cost terms. We find the combination of function\napproximation and a term that maximizes $Q$ in the policy update to be\nproblematic in such environments, because systematic errors in value estimation\nimpact the contributions from the competing terms asymmetrically. This results\nin overemphasis of either incentives or costs and may severely limit learning.\nWe explore two remedies to this issue. First, consistent with prior work, we\nfind that periodic resetting of $Q$ and policy networks can be used to reduce\nvalue estimation error and improve learning in this setting. Second, we\nformulate novel off-policy actor-critic methods for both unconstrained and\nconstrained learning that do not explicitly maximize $Q$ in the policy update.\nWe find that this second approach, when applied to continuous action spaces\nwith mixed-sign rewards, consistently and significantly outperforms\nstate-of-the-art methods augmented by resetting. We further find that our\napproach produces agents that are both competitive with popular methods overall\nand more reliably competent on frequently-studied control problems that do not\nhave mixed-sign rewards.",
            "author": [
                "Jared Markowitz",
                "Jesse Silverberg",
                "Gary Collins"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18684v1",
                "http://arxiv.org/pdf/2311.18684v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18672v1",
            "title": "A Comparison Between Invariant and Equivariant Classical and Quantum\n  Graph Neural Networks",
            "updated": "2023-11-30T16:19:13Z",
            "published": "2023-11-30T16:19:13Z",
            "summary": "Machine learning algorithms are heavily relied on to understand the vast\namounts of data from high-energy particle collisions at the CERN Large Hadron\nCollider (LHC). The data from such collision events can naturally be\nrepresented with graph structures. Therefore, deep geometric methods, such as\ngraph neural networks (GNNs), have been leveraged for various data analysis\ntasks in high-energy physics. One typical task is jet tagging, where jets are\nviewed as point clouds with distinct features and edge connections between\ntheir constituent particles. The increasing size and complexity of the LHC\nparticle datasets, as well as the computational models used for their analysis,\ngreatly motivate the development of alternative fast and efficient\ncomputational paradigms such as quantum computation. In addition, to enhance\nthe validity and robustness of deep networks, one can leverage the fundamental\nsymmetries present in the data through the use of invariant inputs and\nequivariant layers. In this paper, we perform a fair and comprehensive\ncomparison between classical graph neural networks (GNNs) and equivariant graph\nneural networks (EGNNs) and their quantum counterparts: quantum graph neural\nnetworks (QGNNs) and equivariant quantum graph neural networks (EQGNN). The\nfour architectures were benchmarked on a binary classification task to classify\nthe parton-level particle initiating the jet. Based on their AUC scores, the\nquantum networks were shown to outperform the classical networks. However,\nseeing the computational advantage of the quantum networks in practice may have\nto wait for the further development of quantum technology and its associated\nAPIs.",
            "author": [
                "Roy T. Forestano",
                "Mar\u00e7al Comajoan Cara",
                "Gopal Ramesh Dahale",
                "Zhongtian Dong",
                "Sergei Gleyzer",
                "Daniel Justice",
                "Kyoungchul Kong",
                "Tom Magorsch",
                "Konstantin T. Matchev",
                "Katia Matcheva",
                "Eyup B. Unlu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18672v1",
                "http://arxiv.org/pdf/2311.18672v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.LG",
                "hep-ph",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18664v1",
            "title": "Multi-task learning with cross-task consistency for improved depth\n  estimation in colonoscopy",
            "updated": "2023-11-30T16:13:17Z",
            "published": "2023-11-30T16:13:17Z",
            "summary": "Colonoscopy screening is the gold standard procedure for assessing\nabnormalities in the colon and rectum, such as ulcers and cancerous polyps.\nMeasuring the abnormal mucosal area and its 3D reconstruction can help quantify\nthe surveyed area and objectively evaluate disease burden. However, due to the\ncomplex topology of these organs and variable physical conditions, for example,\nlighting, large homogeneous texture, and image modality estimating distance\nfrom the camera aka depth) is highly challenging. Moreover, most colonoscopic\nvideo acquisition is monocular, making the depth estimation a non-trivial\nproblem. While methods in computer vision for depth estimation have been\nproposed and advanced on natural scene datasets, the efficacy of these\ntechniques has not been widely quantified on colonoscopy datasets. As the\ncolonic mucosa has several low-texture regions that are not well pronounced,\nlearning representations from an auxiliary task can improve salient feature\nextraction, allowing estimation of accurate camera depths. In this work, we\npropose to develop a novel multi-task learning (MTL) approach with a shared\nencoder and two decoders, namely a surface normal decoder and a depth estimator\ndecoder. Our depth estimator incorporates attention mechanisms to enhance\nglobal context awareness. We leverage the surface normal prediction to improve\ngeometric feature extraction. Also, we apply a cross-task consistency loss\namong the two geometrically related tasks, surface normal and camera depth. We\ndemonstrate an improvement of 14.17% on relative error and 10.4% improvement on\n$\\delta_{1}$ accuracy over the most accurate baseline state-of-the-art BTS\napproach. All experiments are conducted on a recently released C3VD dataset;\nthus, we provide a first benchmark of state-of-the-art methods.",
            "author": [
                "Pedro Esteban Chavarrias Solano",
                "Andrew Bulpitt",
                "Venkataraman Subramanian",
                "Sharib Ali"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18664v1",
                "http://arxiv.org/pdf/2311.18664v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.MM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18663v1",
            "title": "Choosing the parameter of the Fermat distance: navigating geometry and\n  noise",
            "updated": "2023-11-30T16:11:12Z",
            "published": "2023-11-30T16:11:12Z",
            "summary": "The Fermat distance has been recently established as a useful tool for\nmachine learning tasks when a natural distance is not directly available to the\npractitioner or to improve the results given by Euclidean distances by\nexploding the geometrical and statistical properties of the dataset. This\ndistance depends on a parameter $\\alpha$ that greatly impacts the performance\nof subsequent tasks. Ideally, the value of $\\alpha$ should be large enough to\nnavigate the geometric intricacies inherent to the problem. At the same, it\nshould remain restrained enough to sidestep any deleterious ramifications\nstemming from noise during the process of distance estimation. We study both\ntheoretically and through simulations how to select this parameter.",
            "author": [
                "Fr\u00e9d\u00e9ric Chazal",
                "Laure Ferraris",
                "Pablo Groisman",
                "Matthieu Jonckheere",
                "Fr\u00e9d\u00e9ric Pascal",
                "Facundo Sapienza"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18663v1",
                "http://arxiv.org/pdf/2311.18663v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.AI",
                "cs.LG",
                "math.PR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18662v2",
            "title": "Solving the Team Orienteering Problem with Transformers",
            "updated": "2023-12-01T09:48:02Z",
            "published": "2023-11-30T16:10:35Z",
            "summary": "Route planning for a fleet of vehicles is an important task in applications\nsuch as package delivery, surveillance, or transportation. This problem is\nusually modeled as a Combinatorial Optimization problem named as Team\nOrienteering Problem. The most popular Team Orienteering Problem solvers are\nmainly based on either linear programming, which provides accurate solutions by\nemploying a large computation time that grows with the size of the problem, or\nheuristic methods, which usually find suboptimal solutions in a shorter amount\nof time. In this paper, a multi-agent route planning system capable of solving\nthe Team Orienteering Problem in a very fast and accurate manner is presented.\nThe proposed system is based on a centralized Transformer neural network that\ncan learn to encode the scenario (modeled as a graph) and the context of the\nagents to provide fast and accurate solutions. Several experiments have been\nperformed to demonstrate that the presented system can outperform most of the\nstate-of-the-art works in terms of computation speed. In addition, the code is\npublicly available at http://gti.ssr.upm.es/data.",
            "author": [
                "Daniel Fuertes",
                "Carlos R. del-Blanco",
                "Fernando Jaureguizar",
                "Narciso Garc\u00eda"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18662v2",
                "http://arxiv.org/pdf/2311.18662v2"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18661v1",
            "title": "Learning Part Segmentation from Synthetic Animals",
            "updated": "2023-11-30T16:10:04Z",
            "published": "2023-11-30T16:10:04Z",
            "summary": "Semantic part segmentation provides an intricate and interpretable\nunderstanding of an object, thereby benefiting numerous downstream tasks.\nHowever, the need for exhaustive annotations impedes its usage across diverse\nobject types. This paper focuses on learning part segmentation from synthetic\nanimals, leveraging the Skinned Multi-Animal Linear (SMAL) models to scale up\nexisting synthetic data generated by computer-aided design (CAD) animal models.\nCompared to CAD models, SMAL models generate data with a wider range of poses\nobserved in real-world scenarios. As a result, our first contribution is to\nconstruct a synthetic animal dataset of tigers and horses with more pose\ndiversity, termed Synthetic Animal Parts (SAP). We then benchmark Syn-to-Real\nanimal part segmentation from SAP to PartImageNet, namely SynRealPart, with\nexisting semantic segmentation domain adaptation methods and further improve\nthem as our second contribution. Concretely, we examine three Syn-to-Real\nadaptation methods but observe relative performance drop due to the innate\ndifference between the two tasks. To address this, we propose a simple yet\neffective method called Class-Balanced Fourier Data Mixing (CB-FDM). Fourier\nData Mixing aligns the spectral amplitudes of synthetic images with real\nimages, thereby making the mixed images have more similar frequency content to\nreal images. We further use Class-Balanced Pseudo-Label Re-Weighting to\nalleviate the imbalanced class distribution. We demonstrate the efficacy of\nCB-FDM on SynRealPart over previous methods with significant performance\nimprovements. Remarkably, our third contribution is to reveal that the learned\nparts from synthetic tiger and horse are transferable across all quadrupeds in\nPartImageNet, further underscoring the utility and potential applications of\nanimal part segmentation.",
            "author": [
                "Jiawei Peng",
                "Ju He",
                "Prakhar Kaushik",
                "Zihao Xiao",
                "Jiteng Mu",
                "Alan Yuille"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18661v1",
                "http://arxiv.org/pdf/2311.18661v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18659v1",
            "title": "Comparison of Autoscaling Frameworks for Containerised\n  Machine-Learning-Applications in a Local and Cloud Environment",
            "updated": "2023-11-30T16:08:17Z",
            "published": "2023-11-30T16:08:17Z",
            "summary": "When deploying machine learning (ML) applications, the automated allocation\nof computing resources-commonly referred to as autoscaling-is crucial for\nmaintaining a consistent inference time under fluctuating workloads. The\nobjective is to maximize the Quality of Service metrics, emphasizing\nperformance and availability, while minimizing resource costs. In this paper,\nwe compare scalable deployment techniques across three levels of scaling: at\nthe application level (TorchServe, RayServe) and the container level (K3s) in a\nlocal environment (production server), as well as at the container and machine\nlevels in a cloud environment (Amazon Web Services Elastic Container Service\nand Elastic Kubernetes Service). The comparison is conducted through the study\nof mean and standard deviation of inference time in a multi-client scenario,\nalong with upscaling response times. Based on this analysis, we propose a\ndeployment strategy for both local and cloud-based environments.",
            "author": [
                "Christian Schroeder",
                "Rene Boehm",
                "Alexander Lampe"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18659v1",
                "http://arxiv.org/pdf/2311.18659v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC",
                "94-04",
                "I.2.11"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00102v2",
            "title": "FedEmb: A Vertical and Hybrid Federated Learning Algorithm using Network\n  And Feature Embedding Aggregation",
            "updated": "2023-12-04T14:27:37Z",
            "published": "2023-11-30T16:01:51Z",
            "summary": "Federated learning (FL) is an emerging paradigm for decentralized training of\nmachine learning models on distributed clients, without revealing the data to\nthe central server. The learning scheme may be horizontal, vertical or hybrid\n(both vertical and horizontal). Most existing research work with deep neural\nnetwork (DNN) modelling is focused on horizontal data distributions, while\nvertical and hybrid schemes are much less studied. In this paper, we propose a\ngeneralized algorithm FedEmb, for modelling vertical and hybrid DNN-based\nlearning. The idea of our algorithm is characterised by higher inference\naccuracy, stronger privacy-preserving properties, and lower client-server\ncommunication bandwidth demands as compared with existing work. The\nexperimental results show that FedEmb is an effective method to tackle both\nsplit feature & subject space decentralized problems, shows 0.3% to 4.2%\ninference accuracy improvement with limited privacy revealing for datasets\nstored in local clients, and reduces 88.9 % time complexity over vertical\nbaseline method.",
            "author": [
                "Fanfei Meng",
                "Lele Zhang",
                "Yu Chen",
                "Yuxin Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00102v2",
                "http://arxiv.org/pdf/2312.00102v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00101v1",
            "title": "Towards Unsupervised Representation Learning: Learning, Evaluating and\n  Transferring Visual Representations",
            "updated": "2023-11-30T15:57:55Z",
            "published": "2023-11-30T15:57:55Z",
            "summary": "Unsupervised representation learning aims at finding methods that learn\nrepresentations from data without annotation-based signals. Abstaining from\nannotations not only leads to economic benefits but may - and to some extent\nalready does - result in advantages regarding the representation's structure,\nrobustness, and generalizability to different tasks. In the long run,\nunsupervised methods are expected to surpass their supervised counterparts due\nto the reduction of human intervention and the inherently more general setup\nthat does not bias the optimization towards an objective originating from\nspecific annotation-based signals. While major advantages of unsupervised\nrepresentation learning have been recently observed in natural language\nprocessing, supervised methods still dominate in vision domains for most tasks.\nIn this dissertation, we contribute to the field of unsupervised (visual)\nrepresentation learning from three perspectives: (i) Learning representations:\nWe design unsupervised, backpropagation-free Convolutional Self-Organizing\nNeural Networks (CSNNs) that utilize self-organization- and Hebbian-based\nlearning rules to learn convolutional kernels and masks to achieve deeper\nbackpropagation-free models. (ii) Evaluating representations: We build upon the\nwidely used (non-)linear evaluation protocol to define pretext- and\ntarget-objective-independent metrics for measuring and investigating the\nobjective function mismatch between various unsupervised pretext tasks and\ntarget tasks. (iii) Transferring representations: We contribute CARLANE, the\nfirst 3-way sim-to-real domain adaptation benchmark for 2D lane detection, and\na method based on prototypical self-supervised learning. Finally, we contribute\na content-consistent unpaired image-to-image translation method that utilizes\nmasks, global and local discriminators, and similarity sampling to mitigate\ncontent inconsistencies.",
            "author": [
                "Bonifaz Stuhr"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00101v1",
                "http://arxiv.org/pdf/2312.00101v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.GR",
                "cs.LG",
                "I.2; I.3; I.4; I.5; I.6"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18649v1",
            "title": "Simple Semantic-Aided Few-Shot Learning",
            "updated": "2023-11-30T15:57:34Z",
            "published": "2023-11-30T15:57:34Z",
            "summary": "Learning from a limited amount of data, namely Few-Shot Learning, stands out\nas a challenging computer vision task. Several works exploit semantics and\ndesign complicated semantic fusion mechanisms to compensate for rare\nrepresentative features within restricted data. However, relying on naive\nsemantics such as class names introduces biases due to their brevity, while\nacquiring extensive semantics from external knowledge takes a huge time and\neffort. This limitation severely constrains the potential of semantics in\nfew-shot learning. In this paper, we design an automatic way called Semantic\nEvolution to generate high-quality semantics. The incorporation of high-quality\nsemantics alleviates the need for complex network structures and learning\nalgorithms used in previous works. Hence, we employ a simple two-layer network\ntermed Semantic Alignment Network to transform semantics and visual features\ninto robust class prototypes with rich discriminative features for few-shot\nclassification. The experimental results show our framework outperforms all\nprevious methods on five benchmarks, demonstrating a simple network with\nhigh-quality semantics can beat intricate multi-modal modules on few-shot\nclassification tasks.",
            "author": [
                "Hai Zhang",
                "Junzhe Xu",
                "Shanlin Jiang",
                "Zhenan He"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18649v1",
                "http://arxiv.org/pdf/2311.18649v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18645v1",
            "title": "Stochastic Vision Transformers with Wasserstein Distance-Aware Attention",
            "updated": "2023-11-30T15:53:37Z",
            "published": "2023-11-30T15:53:37Z",
            "summary": "Self-supervised learning is one of the most promising approaches to acquiring\nknowledge from limited labeled data. Despite the substantial advancements made\nin recent years, self-supervised models have posed a challenge to\npractitioners, as they do not readily provide insight into the model's\nconfidence and uncertainty. Tackling this issue is no simple feat, primarily\ndue to the complexity involved in implementing techniques that can make use of\nthe latent representations learned during pre-training without relying on\nexplicit labels. Motivated by this, we introduce a new stochastic vision\ntransformer that integrates uncertainty and distance awareness into\nself-supervised learning (SSL) pipelines. Instead of the conventional\ndeterministic vector embedding, our novel stochastic vision transformer encodes\nimage patches into elliptical Gaussian distributional embeddings. Notably, the\nattention matrices of these stochastic representational embeddings are computed\nusing Wasserstein distance-based attention, effectively capitalizing on the\ndistributional nature of these embeddings. Additionally, we propose a\nregularization term based on Wasserstein distance for both pre-training and\nfine-tuning processes, thereby incorporating distance awareness into latent\nrepresentations. We perform extensive experiments across different tasks such\nas in-distribution generalization, out-of-distribution detection, dataset\ncorruption, semi-supervised settings, and transfer learning to other datasets\nand tasks. Our proposed method achieves superior accuracy and calibration,\nsurpassing the self-supervised baseline in a wide range of experiments on a\nvariety of datasets.",
            "author": [
                "Franciskus Xaverius Erick",
                "Mina Rezaei",
                "Johanna Paula M\u00fcller",
                "Bernhard Kainz"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18645v1",
                "http://arxiv.org/pdf/2311.18645v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18641v1",
            "title": "CrimeGAT: Leveraging Graph Attention Networks for Enhanced Predictive\n  Policing in Criminal Networks",
            "updated": "2023-11-30T15:47:59Z",
            "published": "2023-11-30T15:47:59Z",
            "summary": "In this paper, we present CrimeGAT, a novel application of Graph Attention\nNetworks (GATs) for predictive policing in criminal networks. Criminal networks\npose unique challenges for predictive analytics due to their complex structure,\nmulti-relational links, and dynamic behavior. Traditional methods often fail to\ncapture these complexities, leading to suboptimal predictions. To address these\nchallenges, we propose the use of GATs, which can effectively leverage both\nnode features and graph structure to make predictions. Our proposed CrimeGAT\nmodel integrates attention mechanisms to weigh the importance of a node's\nneighbors, thereby capturing the local and global structures of criminal\nnetworks. We formulate the problem as learning a function that maps node\nfeatures and graph structure to a prediction of future criminal activity. The\nexperimental results on real-world datasets demonstrate that CrimeGAT\nout-performs conventional methods in predicting criminal activities, thereby\nproviding a powerful tool for law enforcement agencies to proactively deploy\nresources. Furthermore, the interpretable nature of the attentionmechanism\ninGATs offers insights into the key players and relationships in criminal\nnetworks. This research opens new avenues for applying deep learning techniques\nin the Aeld of predictive policing and criminal network analysis.",
            "author": [
                "Chen Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18641v1",
                "http://arxiv.org/pdf/2311.18641v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18640v1",
            "title": "The detection, extraction and parameter estimation of extreme-mass-ratio\n  inspirals with deep learning",
            "updated": "2023-11-30T15:46:44Z",
            "published": "2023-11-30T15:46:44Z",
            "summary": "One of the primary goals of space-borne gravitational wave detectors is to\ndetect and analyze extreme-mass-ratio inspirals (EMRIs). This endeavor presents\na significant challenge due to the complex and lengthy EMRI signals, further\ncompounded by their inherently faint nature. In this letter, we introduce a\n2-layer Convolutional Neural Network (CNN) approach to detect EMRI signals for\nspace-borne detectors, achieving a true positive rate (TPR) of 96.9 % at a 1 %\nfalse positive rate (FPR) for signal-to-noise ratio (SNR) from 50 to 100.\nEspecially, the key intrinsic parameters of EMRIs such as mass and spin of the\nsupermassive black hole (SMBH) and the initial eccentricity of the orbit can be\ninferred directly by employing a VGG network. The mass and spin of the SMBH can\nbe determined at 99 % and 92 % respectively. This will greatly reduce the\nparameter spaces and computing cost for the following Bayesian parameter\nestimation. Our model also has a low dependency on the accuracy of the waveform\nmodel. This study underscores the potential of deep learning methods in EMRI\ndata analysis, enabling the rapid detection of EMRI signals and efficient\nparameter estimation .",
            "author": [
                "Qianyun Yun",
                "Wen-Biao Han",
                "Yi-Yang Guo",
                "He Wang",
                "Minghui Du"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18640v1",
                "http://arxiv.org/pdf/2311.18640v1"
            ],
            "primary_category": "gr-qc",
            "category": [
                "gr-qc",
                "astro-ph.IM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18639v1",
            "title": "Targeted Reduction of Causal Models",
            "updated": "2023-11-30T15:46:22Z",
            "published": "2023-11-30T15:46:22Z",
            "summary": "Why does a phenomenon occur? Addressing this question is central to most\nscientific inquiries based on empirical observations, and often heavily relies\non simulations of scientific models. As models become more intricate,\ndeciphering the causes behind these phenomena in high-dimensional spaces of\ninterconnected variables becomes increasingly challenging. Causal machine\nlearning may assist scientists in the discovery of relevant and interpretable\npatterns of causation in simulations. We introduce Targeted Causal Reduction\n(TCR), a method for turning complex models into a concise set of causal factors\nthat explain a specific target phenomenon. We derive an information theoretic\nobjective to learn TCR from interventional data or simulations and propose\nalgorithms to optimize this objective efficiently. TCR's ability to generate\ninterpretable high-level explanations from complex models is demonstrated on\ntoy and mechanical systems, illustrating its potential to assist scientists in\nthe study of complex phenomena in a broad range of disciplines.",
            "author": [
                "Armin Keki\u0107",
                "Bernhard Sch\u00f6lkopf",
                "Michel Besserve"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18639v1",
                "http://arxiv.org/pdf/2311.18639v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00099v1",
            "title": "Online Influence Maximization: Concept and Algorithm",
            "updated": "2023-11-30T15:24:05Z",
            "published": "2023-11-30T15:24:05Z",
            "summary": "In this survey, we offer an extensive overview of the Online Influence\nMaximization (IM) problem by covering both theoretical aspects and practical\napplications. For the integrity of the article and because the online algorithm\ntakes an offline oracle as a subroutine, we first make a clear definition of\nthe Offline IM problem and summarize those commonly used Offline IM algorithms,\nwhich include traditional approximation or heuristic algorithms and ML-based\nalgorithms. Then, we give a standard definition of the Online IM problem and a\nbasic Combinatorial Multi-Armed Bandit (CMAB) framework, CMAB-T. Here, we\nsummarize three types of feedback in the CMAB model and discuss in detail how\nto study the Online IM problem based on the CMAB-T model. This paves the way\nfor solving the Online IM problem by using online learning methods.\nFurthermore, we have covered almost all Online IM algorithms up to now,\nfocusing on characteristics and theoretical guarantees of online algorithms for\ndifferent feedback types. Here, we elaborately explain their working principle\nand how to obtain regret bounds. Besides, we also collect plenty of innovative\nideas about problem definition and algorithm designs and pioneering works for\nvariants of the Online IM problem and their corresponding algorithms. Finally,\nwe encapsulate current challenges and outline prospective research directions\nfrom four distinct perspectives.",
            "author": [
                "Jianxiong Guo"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00099v1",
                "http://arxiv.org/pdf/2312.00099v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18620v1",
            "title": "Data-driven prediction of tool wear using Bayesian-regularized\n  artificial neural networks",
            "updated": "2023-11-30T15:22:20Z",
            "published": "2023-11-30T15:22:20Z",
            "summary": "The prediction of tool wear helps minimize costs and enhance product quality\nin manufacturing. While existing data-driven models using machine learning and\ndeep learning have contributed to the accurate prediction of tool wear, they\noften lack generality and require substantial training data for high accuracy.\nIn this paper, we propose a new data-driven model that uses Bayesian\nRegularized Artificial Neural Networks (BRANNs) to precisely predict milling\ntool wear. BRANNs combine the strengths and leverage the benefits of artificial\nneural networks (ANNs) and Bayesian regularization, whereby ANNs learn complex\npatterns and Bayesian regularization handles uncertainty and prevents\noverfitting, resulting in a more generalized model. We treat both process\nparameters and monitoring sensor signals as BRANN input parameters. We\nconducted an extensive experimental study featuring four different experimental\ndata sets, including the NASA Ames milling dataset, the 2010 PHM Data Challenge\ndataset, the NUAA Ideahouse tool wear dataset, and an in-house performed\nend-milling of the Ti6Al4V dataset. We inspect the impact of input features,\ntraining data size, hidden units, training algorithms, and transfer functions\non the performance of the proposed BRANN model and demonstrate that it\noutperforms existing state-of-the-art models in terms of accuracy and\nreliability.",
            "author": [
                "Tam T. Truong",
                "Jay Airao",
                "Panagiotis Karras",
                "Faramarz Hojati",
                "Bahman Azarhoushang",
                "Ramin Aghababaei"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18620v1",
                "http://arxiv.org/pdf/2311.18620v1"
            ],
            "primary_category": "cs.CE",
            "category": [
                "cs.CE",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18618v1",
            "title": "JPPF: Multi-task Fusion for Consistent Panoptic-Part Segmentation",
            "updated": "2023-11-30T15:17:46Z",
            "published": "2023-11-30T15:17:46Z",
            "summary": "Part-aware panoptic segmentation is a problem of computer vision that aims to\nprovide a semantic understanding of the scene at multiple levels of\ngranularity. More precisely, semantic areas, object instances, and semantic\nparts are predicted simultaneously. In this paper, we present our Joint\nPanoptic Part Fusion (JPPF) that combines the three individual segmentations\neffectively to obtain a panoptic-part segmentation. Two aspects are of utmost\nimportance for this: First, a unified model for the three problems is desired\nthat allows for mutually improved and consistent representation learning.\nSecond, balancing the combination so that it gives equal importance to all\nindividual results during fusion. Our proposed JPPF is parameter-free and\ndynamically balances its input. The method is evaluated and compared on the\nCityscapes Panoptic Parts (CPP) and Pascal Panoptic Parts (PPP) datasets in\nterms of PartPQ and Part-Whole Quality (PWQ). In extensive experiments, we\nverify the importance of our fair fusion, highlight its most significant impact\nfor areas that can be further segmented into parts, and demonstrate the\ngeneralization capabilities of our design without fine-tuning on 5 additional\ndatasets.",
            "author": [
                "Shishir Muralidhara",
                "Sravan Kumar Jagadeesh",
                "Ren\u00e9 Schuster",
                "Didier Stricker"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18618v1",
                "http://arxiv.org/pdf/2311.18618v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18612v1",
            "title": "Cancer-Net PCa-Gen: Synthesis of Realistic Prostate Diffusion Weighted\n  Imaging Data via Anatomic-Conditional Controlled Latent Diffusion",
            "updated": "2023-11-30T15:11:03Z",
            "published": "2023-11-30T15:11:03Z",
            "summary": "In Canada, prostate cancer is the most common form of cancer in men and\naccounted for 20% of new cancer cases for this demographic in 2022. Due to\nrecent successes in leveraging machine learning for clinical decision support,\nthere has been significant interest in the development of deep neural networks\nfor prostate cancer diagnosis, prognosis, and treatment planning using\ndiffusion weighted imaging (DWI) data. A major challenge hindering widespread\nadoption in clinical use is poor generalization of such networks due to\nscarcity of large-scale, diverse, balanced prostate imaging datasets for\ntraining such networks. In this study, we explore the efficacy of latent\ndiffusion for generating realistic prostate DWI data through the introduction\nof an anatomic-conditional controlled latent diffusion strategy. To the best of\nthe authors' knowledge, this is the first study to leverage conditioning for\nsynthesis of prostate cancer imaging. Experimental results show that the\nproposed strategy, which we call Cancer-Net PCa-Gen, enhances synthesis of\ndiverse prostate images through controllable tumour locations and better\nanatomical and textural fidelity. These crucial features make it well-suited\nfor augmenting real patient data, enabling neural networks to be trained on a\nmore diverse and comprehensive data distribution. The Cancer-Net PCa-Gen\nframework and sample images have been made publicly available at\nhttps://www.kaggle.com/datasets/deetsadi/cancer-net-pca-gen-dataset as a part\nof a global open-source initiative dedicated to accelerating advancement in\nmachine learning to aid clinicians in the fight against cancer.",
            "author": [
                "Aditya Sridhar",
                "Chi-en Amy Tai",
                "Hayden Gunraj",
                "Yuhao Chen",
                "Alexander Wong"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18612v1",
                "http://arxiv.org/pdf/2311.18612v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18610v1",
            "title": "DiffCAD: Weakly-Supervised Probabilistic CAD Model Retrieval and\n  Alignment from an RGB Image",
            "updated": "2023-11-30T15:10:21Z",
            "published": "2023-11-30T15:10:21Z",
            "summary": "Perceiving 3D structures from RGB images based on CAD model primitives can\nenable an effective, efficient 3D object-based representation of scenes.\nHowever, current approaches rely on supervision from expensive annotations of\nCAD models associated with real images, and encounter challenges due to the\ninherent ambiguities in the task -- both in depth-scale ambiguity in monocular\nperception, as well as inexact matches of CAD database models to real\nobservations. We thus propose DiffCAD, the first weakly-supervised\nprobabilistic approach to CAD retrieval and alignment from an RGB image. We\nformulate this as a conditional generative task, leveraging diffusion to learn\nimplicit probabilistic models capturing the shape, pose, and scale of CAD\nobjects in an image. This enables multi-hypothesis generation of different\nplausible CAD reconstructions, requiring only a few hypotheses to characterize\nambiguities in depth/scale and inexact shape matches. Our approach is trained\nonly on synthetic data, leveraging monocular depth and mask estimates to enable\nrobust zero-shot adaptation to various real target domains. Despite being\ntrained solely on synthetic data, our multi-hypothesis approach can even\nsurpass the supervised state-of-the-art on the Scan2CAD dataset by 5.9% with 8\nhypotheses.",
            "author": [
                "Daoyi Gao",
                "D\u00e1vid Rozenberszki",
                "Stefan Leutenegger",
                "Angela Dai"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18610v1",
                "http://arxiv.org/pdf/2311.18610v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18609v1",
            "title": "ArthModel: Enhance Arithmetic Skills to Large Language Model",
            "updated": "2023-11-30T15:06:50Z",
            "published": "2023-11-30T15:06:50Z",
            "summary": "With the great success of ChatGPT, the research of large language models has\nbecome increasingly popular. However, the models have several limitations, such\nas toxicity and pool performance of arithmetic solving. Meanwhile, LLM may have\nsome potential abilities that have yet to be exploited. In this paper, we\nchoose a different way to enhance the arithmetic ability of LLM. We propose to\ntrain LLM to generate a postfix expression related to the arithmetic problem\nand incorporate it with small pretrained models. Moreover, this small model\ntransfers the token embeddings into real dense numbers and invokes native\nfunctions of a deep learning platform to get the correct answer. To generate\nthe final result, we propose prompt injection for adding the result outputs by\nthe small model to LLM. This work provides different ways of thinking, training\nand using a language model. The codes and models will be released at\n\\url{https://github.com/eteced/arithmetic_finetuning_v1}.",
            "author": [
                "Yingdi Guo"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18609v1",
                "http://arxiv.org/pdf/2311.18609v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18608v1",
            "title": "Contrastive Denoising Score for Text-guided Latent Diffusion Image\n  Editing",
            "updated": "2023-11-30T15:06:10Z",
            "published": "2023-11-30T15:06:10Z",
            "summary": "With the remarkable advent of text-to-image diffusion models, image editing\nmethods have become more diverse and continue to evolve. A promising recent\napproach in this realm is Delta Denoising Score (DDS) - an image editing\ntechnique based on Score Distillation Sampling (SDS) framework that leverages\nthe rich generative prior of text-to-image diffusion models. However, relying\nsolely on the difference between scoring functions is insufficient for\npreserving specific structural elements from the original image, a crucial\naspect of image editing. Inspired by the similarity and importance differences\nbetween DDS and the contrastive learning for unpaired image-to-image\ntranslation (CUT), here we present an embarrassingly simple yet very powerful\nmodification of DDS, called Contrastive Denoising Score (CDS), for latent\ndiffusion models (LDM). Specifically, to enforce structural correspondence\nbetween the input and output while maintaining the controllability of contents,\nwe introduce a straightforward approach to regulate structural consistency\nusing CUT loss within the DDS framework. To calculate this loss, instead of\nemploying auxiliary networks, we utilize the intermediate features of LDM, in\nparticular, those from the self-attention layers, which possesses rich spatial\ninformation. Our approach enables zero-shot image-to-image translation and\nneural radiance field (NeRF) editing, achieving a well-balanced interplay\nbetween maintaining the structural details and transforming content.\nQualitative results and comparisons demonstrates the effectiveness of our\nproposed method. Project page with code is available at\nhttps://hyelinnam.github.io/CDS/.",
            "author": [
                "Hyelin Nam",
                "Gihyun Kwon",
                "Geon Yeong Park",
                "Jong Chul Ye"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18608v1",
                "http://arxiv.org/pdf/2311.18608v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18605v2",
            "title": "Learning Triangular Distribution in Visual World",
            "updated": "2023-12-04T05:05:03Z",
            "published": "2023-11-30T15:02:13Z",
            "summary": "Convolution neural network is successful in pervasive vision tasks, including\nlabel distribution learning, which usually takes the form of learning an\ninjection from the non-linear visual features to the well-defined labels.\nHowever, how the discrepancy between features is mapped to the label\ndiscrepancy is ambient, and its correctness is not guaranteed. To address these\nproblems, we study the mathematical connection between feature and its label,\npresenting a general and simple framework for label distribution learning. We\npropose a so-called Triangular Distribution Transform (TDT) to build an\ninjective function between feature and label, guaranteeing that any symmetric\nfeature discrepancy linearly reflects the difference between labels. The\nproposed TDT can be used as a plug-in in mainstream backbone networks to\naddress different label distribution learning tasks. Experiments on Facial Age\nRecognition, Illumination Chromaticity Estimation, and Aesthetics assessment\nshow that TDT achieves on-par or better results than the prior arts.",
            "author": [
                "Ping Chen",
                "Xingpeng Zhang",
                "Chengtao Zhou",
                "Dichao Fan",
                "Peng Tu",
                "Le Zhang",
                "Yanlin Qian"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18605v2",
                "http://arxiv.org/pdf/2311.18605v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18598v1",
            "title": "Generalisable Agents for Neural Network Optimisation",
            "updated": "2023-11-30T14:45:51Z",
            "published": "2023-11-30T14:45:51Z",
            "summary": "Optimising deep neural networks is a challenging task due to complex training\ndynamics, high computational requirements, and long training times. To address\nthis difficulty, we propose the framework of Generalisable Agents for Neural\nNetwork Optimisation (GANNO) -- a multi-agent reinforcement learning (MARL)\napproach that learns to improve neural network optimisation by dynamically and\nresponsively scheduling hyperparameters during training. GANNO utilises an\nagent per layer that observes localised network dynamics and accordingly takes\nactions to adjust these dynamics at a layerwise level to collectively improve\nglobal performance. In this paper, we use GANNO to control the layerwise\nlearning rate and show that the framework can yield useful and responsive\nschedules that are competitive with handcrafted heuristics. Furthermore, GANNO\nis shown to perform robustly across a wide variety of unseen initial\nconditions, and can successfully generalise to harder problems than it was\ntrained on. Our work presents an overview of the opportunities that this\nparadigm offers for training neural networks, along with key challenges that\nremain to be overcome.",
            "author": [
                "Kale-ab Tessera",
                "Callum Rhys Tilbury",
                "Sasha Abramowitz",
                "Ruan de Kock",
                "Omayma Mahjoub",
                "Benjamin Rosman",
                "Sara Hooker",
                "Arnu Pretorius"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18598v1",
                "http://arxiv.org/pdf/2311.18598v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.MA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18588v1",
            "title": "Optimizing ZX-Diagrams with Deep Reinforcement Learning",
            "updated": "2023-11-30T14:29:18Z",
            "published": "2023-11-30T14:29:18Z",
            "summary": "ZX-diagrams are a powerful graphical language for the description of quantum\nprocesses with applications in fundamental quantum mechanics, quantum circuit\noptimization, tensor network simulation, and many more. The utility of\nZX-diagrams relies on a set of local transformation rules that can be applied\nto them without changing the underlying quantum process they describe. These\nrules can be exploited to optimize the structure of ZX-diagrams for a range of\napplications. However, finding an optimal sequence of transformation rules is\ngenerally an open problem. In this work, we bring together ZX-diagrams with\nreinforcement learning, a machine learning technique designed to discover an\noptimal sequence of actions in a decision-making problem and show that a\ntrained reinforcement learning agent can significantly outperform other\noptimization techniques like a greedy strategy or simulated annealing. The use\nof graph neural networks to encode the policy of the agent enables\ngeneralization to diagrams much bigger than seen during the training phase.",
            "author": [
                "Maximilian N\u00e4gele",
                "Florian Marquardt"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18588v1",
                "http://arxiv.org/pdf/2311.18588v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18587v2",
            "title": "Continuous 16-bit Training: Accelerating 32-bit Pre-Trained Neural\n  Networks",
            "updated": "2023-12-01T02:51:32Z",
            "published": "2023-11-30T14:28:25Z",
            "summary": "In the field of deep learning, the prevalence of models initially trained\nwith 32-bit precision is a testament to its robustness and accuracy. However,\nthe continuous evolution of these models often demands further training, which\ncan be resource-intensive. This study introduces a novel approach where we\ncontinue the training of these pre-existing 32-bit models using 16-bit\nprecision. This technique not only caters to the need for efficiency in\ncomputational resources but also significantly improves the speed of additional\ntraining phases. By adopting 16-bit precision for ongoing training, we are able\nto substantially decrease memory requirements and computational burden, thereby\naccelerating the training process in a resource-limited setting. Our\nexperiments show that this method maintains the high standards of accuracy set\nby the original 32-bit training while providing a much-needed boost in training\nspeed. This approach is especially pertinent in today's context, where most\nmodels are initially trained in 32-bit and require periodic updates and\nrefinements. The findings from our research suggest that this strategy of\n16-bit continuation training can be a key solution for sustainable and\nefficient deep learning, offering a practical way to enhance pre-trained models\nrapidly and in a resource-conscious manner.",
            "author": [
                "Juyoung Yun"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18587v2",
                "http://arxiv.org/pdf/2311.18587v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18578v1",
            "title": "Communication-Efficient Heterogeneous Federated Learning with\n  Generalized Heavy-Ball Momentum",
            "updated": "2023-11-30T14:17:57Z",
            "published": "2023-11-30T14:17:57Z",
            "summary": "Federated Learning (FL) is the state-of-the-art approach for learning from\ndecentralized data in privacy-constrained scenarios. As the current literature\nreports, the main problems associated with FL refer to system and statistical\nchallenges: the former ones demand for efficient learning from edge devices,\nincluding lowering communication bandwidth and frequency, while the latter\nrequire algorithms robust to non-iidness. State-of-art approaches either\nguarantee convergence at increased communication cost or are not sufficiently\nrobust to handle extreme heterogeneous local distributions. In this work we\npropose a novel generalization of the heavy-ball momentum, and present FedHBM\nto effectively address statistical heterogeneity in FL without introducing any\ncommunication overhead. We conduct extensive experimentation on common FL\nvision and NLP datasets, showing that our FedHBM algorithm empirically yields\nbetter model quality and higher convergence speed w.r.t. the state-of-art,\nespecially in pathological non-iid scenarios. While being designed for\ncross-silo settings, we show how FedHBM is applicable in moderate-to-high\ncross-device scenarios, and how good model initializations (e.g. pre-training)\ncan be exploited for prompt acceleration. Extended experimentation on\nlarge-scale real-world federated datasets further corroborates the\neffectiveness of our approach for real-world FL applications.",
            "author": [
                "Riccardo Zaccone",
                "Carlo Masone",
                "Marco Ciccone"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18578v1",
                "http://arxiv.org/pdf/2311.18578v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18575v1",
            "title": "Class Distribution Shifts in Zero-Shot Learning: Learning Robust\n  Representations",
            "updated": "2023-11-30T14:14:31Z",
            "published": "2023-11-30T14:14:31Z",
            "summary": "Distribution shifts between training and deployment data often affect the\nperformance of machine learning models. In this paper, we explore a setting\nwhere a hidden variable induces a shift in the distribution of classes. These\ndistribution shifts are particularly challenging for zero-shot classifiers, as\nthey rely on representations learned from training classes, but are deployed on\nnew, unseen ones. We introduce an algorithm to learn data representations that\nare robust to such class distribution shifts in zero-shot verification tasks.\nWe show that our approach, which combines hierarchical data sampling with\nout-of-distribution generalization techniques, improves generalization to\ndiverse class distributions in both simulations and real-world datasets.",
            "author": [
                "Yuli Slavutsky",
                "Yuval Benjamini"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18575v1",
                "http://arxiv.org/pdf/2311.18575v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18574v1",
            "title": "Multi-scale Iterative Refinement towards Robust and Versatile Molecular\n  Docking",
            "updated": "2023-11-30T14:09:20Z",
            "published": "2023-11-30T14:09:20Z",
            "summary": "Molecular docking is a key computational tool utilized to predict the binding\nconformations of small molecules to protein targets, which is fundamental in\nthe design of novel drugs. Despite recent advancements in geometric deep\nlearning-based approaches leading to improvements in blind docking efficiency,\nthese methods have encountered notable challenges, such as limited\ngeneralization performance on unseen proteins, the inability to concurrently\naddress the settings of blind docking and site-specific docking, and the\nfrequent occurrence of physical implausibilities such as inter-molecular steric\nclash. In this study, we introduce DeltaDock, a robust and versatile framework\ndesigned for efficient molecular docking to overcome these challenges.\nDeltaDock operates in a two-step process: rapid initial complex structures\nsampling followed by multi-scale iterative refinement of the initial\nstructures. In the initial stage, to sample accurate structures with high\nefficiency, we develop a ligand-dependent binding site prediction model founded\non large protein models and graph neural networks. This model is then paired\nwith GPU-accelerated sampling algorithms. The sampled structures are updated\nusing a multi-scale iterative refinement module that captures both\nprotein-ligand atom-atom interactions and residue-atom interactions in the\nfollowing stage. Distinct from previous geometric deep learning methods that\nare conditioned on the blind docking setting, DeltaDock demonstrates superior\nperformance in both blind docking and site-specific docking settings.\nComprehensive experimental results reveal that DeltaDock consistently surpasses\nbaseline methods in terms of docking accuracy. Furthermore, it displays\nremarkable generalization capabilities and proficiency for predicting\nphysically valid structures, thereby attesting to its robustness and\nreliability in various scenarios.",
            "author": [
                "Jiaxian Yan",
                "Zaixi Zhang",
                "Kai Zhang",
                "Qi Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18574v1",
                "http://arxiv.org/pdf/2311.18574v1"
            ],
            "primary_category": "q-bio.BM",
            "category": [
                "q-bio.BM",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00098v1",
            "title": "Identifying tourist destinations from movie scenes using Deep Learning",
            "updated": "2023-11-30T14:09:17Z",
            "published": "2023-11-30T14:09:17Z",
            "summary": "Movies wield significant influence in our lives, playing a pivotal role in\nthe tourism industry of any country. The inclusion of picturesque landscapes,\nwaterfalls, and mountains as backdrops in films serves to enhance the allure of\nspecific scenarios. Recognizing the impact of movies on tourism, this paper\nintroduces a method for identifying tourist destinations featured in films. We\npropose the development of a deep learning model capable of recognizing these\nlocations during movie viewing. The model is trained on a dataset comprising\nmajor tourism destinations worldwide. Through this research, the goal is to\nenable viewers to identify the real-world locations depicted in movie scenes,\noffering a novel way to connect cinema with global travel experiences.",
            "author": [
                "Mahendran Narayanan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00098v1",
                "http://arxiv.org/pdf/2312.00098v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18572v1",
            "title": "Overcoming Label Noise for Source-free Unsupervised Video Domain\n  Adaptation",
            "updated": "2023-11-30T14:06:27Z",
            "published": "2023-11-30T14:06:27Z",
            "summary": "Despite the progress seen in classification methods, current approaches for\nhandling videos with distribution shifts in source and target domains remain\nsource-dependent as they require access to the source data during the\nadaptation stage. In this paper, we present a self-training based source-free\nvideo domain adaptation approach to address this challenge by bridging the gap\nbetween the source and the target domains. We use the source pre-trained model\nto generate pseudo-labels for the target domain samples, which are inevitably\nnoisy. Thus, we treat the problem of source-free video domain adaptation as\nlearning from noisy labels and argue that the samples with correct\npseudo-labels can help us in adaptation. To this end, we leverage the\ncross-entropy loss as an indicator of the correctness of the pseudo-labels and\nuse the resulting small-loss samples from the target domain for fine-tuning the\nmodel. We further enhance the adaptation performance by implementing a\nteacher-student framework, in which the teacher, which is updated gradually,\nproduces reliable pseudo-labels. Meanwhile, the student undergoes fine-tuning\non the target domain videos using these generated pseudo-labels to improve its\nperformance. Extensive experimental evaluations show that our methods, termed\nas CleanAdapt, CleanAdapt + TS, achieve state-of-the-art results, outperforming\nthe existing approaches on various open datasets. Our source code is publicly\navailable at https://avijit9.github.io/CleanAdapt.",
            "author": [
                "Avijit Dasgupta",
                "C. V. Jawahar",
                "Karteek Alahari"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18572v1",
                "http://arxiv.org/pdf/2311.18572v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18561v1",
            "title": "Periodic Vibration Gaussian: Dynamic Urban Scene Reconstruction and\n  Real-time Rendering",
            "updated": "2023-11-30T13:53:50Z",
            "published": "2023-11-30T13:53:50Z",
            "summary": "Modeling dynamic, large-scale urban scenes is challenging due to their highly\nintricate geometric structures and unconstrained dynamics in both space and\ntime. Prior methods often employ high-level architectural priors, separating\nstatic and dynamic elements, resulting in suboptimal capture of their\nsynergistic interactions. To address this challenge, we present a unified\nrepresentation model, called Periodic Vibration Gaussian (PVG). PVG builds upon\nthe efficient 3D Gaussian splatting technique, originally designed for static\nscene representation, by introducing periodic vibration-based temporal\ndynamics. This innovation enables PVG to elegantly and uniformly represent the\ncharacteristics of various objects and elements in dynamic urban scenes. To\nenhance temporally coherent representation learning with sparse training data,\nwe introduce a novel flow-based temporal smoothing mechanism and a\nposition-aware adaptive control strategy. Extensive experiments on Waymo Open\nDataset and KITTI benchmarks demonstrate that PVG surpasses state-of-the-art\nalternatives in both reconstruction and novel view synthesis for both dynamic\nand static scenes. Notably, PVG achieves this without relying on manually\nlabeled object bounding boxes or expensive optical flow estimation. Moreover,\nPVG exhibits 50/6000-fold acceleration in training/rendering over the best\nalternative.",
            "author": [
                "Yurui Chen",
                "Chun Gu",
                "Junzhe Jiang",
                "Xiatian Zhu",
                "Li Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18561v1",
                "http://arxiv.org/pdf/2311.18561v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18559v1",
            "title": "FediOS: Decoupling Orthogonal Subspaces for Personalization in\n  Feature-skew Federated Learning",
            "updated": "2023-11-30T13:50:38Z",
            "published": "2023-11-30T13:50:38Z",
            "summary": "Personalized federated learning (pFL) enables collaborative training among\nmultiple clients to enhance the capability of customized local models. In pFL,\nclients may have heterogeneous (also known as non-IID) data, which poses a key\nchallenge in how to decouple the data knowledge into generic knowledge for\nglobal sharing and personalized knowledge for preserving local personalization.\nA typical way of pFL focuses on label distribution skew, and they adopt a\ndecoupling scheme where the model is split into a common feature extractor and\ntwo prediction heads (generic and personalized). However, such a decoupling\nscheme cannot solve the essential problem of feature skew heterogeneity,\nbecause a common feature extractor cannot decouple the generic and personalized\nfeatures. Therefore, in this paper, we rethink the architecture decoupling\ndesign for feature-skew pFL and propose an effective pFL method called FediOS.\nIn FediOS, we reformulate the decoupling into two feature extractors (generic\nand personalized) and one shared prediction head. Orthogonal projections are\nused for clients to map the generic features into one common subspace and\nscatter the personalized features into different subspaces to achieve\ndecoupling for them. In addition, a shared prediction head is trained to\nbalance the importance of generic and personalized features during inference.\nExtensive experiments on four vision datasets demonstrate our method reaches\nstate-of-the-art pFL performances under feature skew heterogeneity.",
            "author": [
                "Lingzhi Gao",
                "Zexi Li",
                "Yang Lu",
                "Chao Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18559v1",
                "http://arxiv.org/pdf/2311.18559v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18558v1",
            "title": "Learning Radio Environments by Differentiable Ray Tracing",
            "updated": "2023-11-30T13:50:21Z",
            "published": "2023-11-30T13:50:21Z",
            "summary": "Ray tracing (RT) is instrumental in 6G research in order to generate\nspatially-consistent and environment-specific channel impulse responses (CIRs).\nWhile acquiring accurate scene geometries is now relatively straightforward,\ndetermining material characteristics requires precise calibration using channel\nmeasurements. We therefore introduce a novel gradient-based calibration method,\ncomplemented by differentiable parametrizations of material properties,\nscattering and antenna patterns. Our method seamlessly integrates with\ndifferentiable ray tracers that enable the computation of derivatives of CIRs\nwith respect to these parameters. Essentially, we approach field computation as\na large computational graph wherein parameters are trainable akin to weights of\na neural network (NN). We have validated our method using both synthetic data\nand real-world indoor channel measurements, employing a distributed\nmultiple-input multiple-output (MIMO) channel sounder.",
            "author": [
                "Jakob Hoydis",
                "Fay\u00e7al A\u00eft Aoudia",
                "Sebastian Cammerer",
                "Florian Euchner",
                "Merlin Nimier-David",
                "Stephan ten Brink",
                "Alexander Keller"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18558v1",
                "http://arxiv.org/pdf/2311.18558v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "cs.LG",
                "cs.NI",
                "eess.SP",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18557v1",
            "title": "Can semi-supervised learning use all the data effectively? A lower bound\n  perspective",
            "updated": "2023-11-30T13:48:50Z",
            "published": "2023-11-30T13:48:50Z",
            "summary": "Prior works have shown that semi-supervised learning algorithms can leverage\nunlabeled data to improve over the labeled sample complexity of supervised\nlearning (SL) algorithms. However, existing theoretical analyses focus on\nregimes where the unlabeled data is sufficient to learn a good decision\nboundary using unsupervised learning (UL) alone. This begs the question: Can\nSSL algorithms simultaneously improve upon both UL and SL? To this end, we\nderive a tight lower bound for 2-Gaussian mixture models that explicitly\ndepends on the labeled and the unlabeled dataset size as well as the\nsignal-to-noise ratio of the mixture distribution. Surprisingly, our result\nimplies that no SSL algorithm can improve upon the minimax-optimal statistical\nerror rates of SL or UL algorithms for these distributions. Nevertheless, we\nshow empirically on real-world data that SSL algorithms can still outperform UL\nand SL methods. Therefore, our work suggests that, while proving performance\ngains for SSL algorithms is possible, it requires careful tracking of\nconstants.",
            "author": [
                "Alexandru \u0162ifrea",
                "Gizem Y\u00fcce",
                "Amartya Sanyal",
                "Fanny Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18557v1",
                "http://arxiv.org/pdf/2311.18557v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18553v1",
            "title": "Heterogeneous Graph-based Trajectory Prediction using Local Map Context\n  and Social Interactions",
            "updated": "2023-11-30T13:46:05Z",
            "published": "2023-11-30T13:46:05Z",
            "summary": "Precisely predicting the future trajectories of surrounding traffic\nparticipants is a crucial but challenging problem in autonomous driving, due to\ncomplex interactions between traffic agents, map context and traffic rules.\nVector-based approaches have recently shown to achieve among the best\nperformances on trajectory prediction benchmarks. These methods model simple\ninteractions between traffic agents but don't distinguish between relation-type\nand attributes like their distance along the road. Furthermore, they represent\nlanes only by sequences of vectors representing center lines and ignore context\ninformation like lane dividers and other road elements. We present a novel\napproach for vector-based trajectory prediction that addresses these\nshortcomings by leveraging three crucial sources of information: First, we\nmodel interactions between traffic agents by a semantic scene graph, that\naccounts for the nature and important features of their relation. Second, we\nextract agent-centric image-based map features to model the local map context.\nFinally, we generate anchor paths to enforce the policy in multi-modal\nprediction to permitted trajectories only. Each of these three enhancements\nshows advantages over the baseline model HoliGraph.",
            "author": [
                "Daniel Grimm",
                "Maximilian Zipfl",
                "Felix Hertlein",
                "Alexander Naumann",
                "J\u00fcrgen L\u00fcttin",
                "Steffen Thoma",
                "Stefan Schmid",
                "Lavdim Halilaj",
                "Achim Rettinger",
                "J. Marius Z\u00f6llner"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18553v1",
                "http://arxiv.org/pdf/2311.18553v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18547v1",
            "title": "Real-Time Vibration-Based Bearing Fault Diagnosis Under Time-Varying\n  Speed Conditions",
            "updated": "2023-11-30T13:30:00Z",
            "published": "2023-11-30T13:30:00Z",
            "summary": "Detection of rolling-element bearing faults is crucial for implementing\nproactive maintenance strategies and for minimizing the economic and\noperational consequences of unexpected failures. However, many existing\ntechniques are developed and tested under strictly controlled conditions,\nlimiting their adaptability to the diverse and dynamic settings encountered in\npractical applications. This paper presents an efficient real-time\nconvolutional neural network (CNN) for diagnosing multiple bearing faults under\nvarious noise levels and time-varying rotational speeds. Additionally, we\npropose a novel Fisher-based spectral separability analysis (SSA) method to\nelucidate the effectiveness of the designed CNN model. We conducted experiments\non both healthy bearings and bearings afflicted with inner race, outer race,\nand roller ball faults. The experimental results show the superiority of our\nmodel over the current state-of-the-art approach in three folds: it achieves\nsubstantial accuracy gains of up to 15.8%, it is robust to noise with high\nperformance across various signal-to-noise ratios, and it runs in real-time\nwith processing durations five times less than acquisition. Additionally, by\nusing the proposed SSA technique, we offer insights into the model's\nperformance and underscore its effectiveness in tackling real-world challenges.",
            "author": [
                "Tuomas Jalonen",
                "Mohammad Al-Sa'd",
                "Serkan Kiranyaz",
                "Moncef Gabbouj"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18547v1",
                "http://arxiv.org/pdf/2311.18547v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18545v2",
            "title": "Decentralized Deepfake Detection Blockchain Network using Dynamic\n  Algorithm management",
            "updated": "2023-12-01T21:23:59Z",
            "published": "2023-11-30T13:27:00Z",
            "summary": "Deepfake technology is a major threat to the integrity of digital media. This\npaper presents a comprehensive framework for a blockchain-based decentralized\nsystem designed to tackle the escalating challenge of digital content\nintegrity. The proposed system integrates advanced deep learning algorithms\nwith the immutable and transparent nature of blockchain technology to create a\ntrustless environment where authenticity can be verified without relying on a\nsingle centralized authority. Furthermore, the system utilizes smart contracts\nfor dynamic algorithm management and token-based incentives further enhances\nthe system's effectiveness and adaptability. The decentralized architecture of\nthe system democratizes the process of verifying digital content and introduces\na novel approach to combat deepfakes. The collaborative and adjustable nature\nof this system sets a new benchmark for digital media integrity, offering a\nmore robust digital media environment.",
            "author": [
                "Dipankar Sarkar"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18545v2",
                "http://arxiv.org/pdf/2311.18545v2"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18543v1",
            "title": "CrimeGraphNet: Link Prediction in Criminal Networks with Graph\n  Convolutional Networks",
            "updated": "2023-11-30T13:25:46Z",
            "published": "2023-11-30T13:25:46Z",
            "summary": "In this paper, we introduce CrimeGraphNet, a novel approach for link\nprediction in criminal networks utilizingGraph Convolutional Networks (GCNs).\nCriminal networks are intricate and dynamic, with covert links that are\nchallenging to uncover. Accurate prediction of these links can aid in proactive\ncrime prevention and investigation. Existing methods often fail to capture the\ncomplex interconnections in such networks. They also struggle in scenarios\nwhere only limited labeled data is available for training. To address these\nchallenges, we propose CrimeGraphNet, which leverages the power of GCNs for\nlink prediction in these networks. The GCNmodel effectively captures\ntopological features and node characteristics, making it well-suited for this\ntask. We evaluate CrimeGraphNet on several real-world criminal network\ndatasets. Our results demonstrate that CrimeGraphNet outperforms existing\nmethods in terms of prediction accuracy, robustness, and computational\nefAciency. Furthermore, our approach enables the extraction of meaningful\ninsights from the predicted links, thereby contributing to a better\nunderstanding of the underlying criminal activities. Overall, CrimeGraphNet\nrepresents a signiAcant step forward in the use of deep learning for criminal\nnetwork analysis.",
            "author": [
                "Chen Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18543v1",
                "http://arxiv.org/pdf/2311.18543v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18540v1",
            "title": "Match me if you can: Semantic Correspondence Learning with Unpaired\n  Images",
            "updated": "2023-11-30T13:22:15Z",
            "published": "2023-11-30T13:22:15Z",
            "summary": "Recent approaches for semantic correspondence have focused on obtaining\nhigh-quality correspondences using a complicated network, refining the\nambiguous or noisy matching points. Despite their performance improvements,\nthey remain constrained by the limited training pairs due to costly point-level\nannotations. This paper proposes a simple yet effective method that performs\ntraining with unlabeled pairs to complement both limited image pairs and sparse\npoint pairs, requiring neither extra labeled keypoints nor trainable modules.\nWe fundamentally extend the data quantity and variety by augmenting new\nunannotated pairs not primitively provided as training pairs in benchmarks.\nUsing a simple teacher-student framework, we offer reliable pseudo\ncorrespondences to the student network via machine supervision. Finally, the\nperformance of our network is steadily improved by the proposed iterative\ntraining, putting back the student as a teacher to generate refined labels and\ntrain a new student repeatedly. Our models outperform the milestone baselines,\nincluding state-of-the-art methods on semantic correspondence benchmarks.",
            "author": [
                "Jiwon Kim",
                "Byeongho Heo",
                "Sangdoo Yun",
                "Seungryong Kim",
                "Dongyoon Han"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18540v1",
                "http://arxiv.org/pdf/2311.18540v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18537v1",
            "title": "MaXTron: Mask Transformer with Trajectory Attention for Video Panoptic\n  Segmentation",
            "updated": "2023-11-30T13:20:09Z",
            "published": "2023-11-30T13:20:09Z",
            "summary": "Video panoptic segmentation requires consistently segmenting (for both\n`thing' and `stuff' classes) and tracking objects in a video over time. In this\nwork, we present MaXTron, a general framework that exploits Mask XFormer with\nTrajectory Attention to tackle the task. MaXTron enriches an off-the-shelf mask\ntransformer by leveraging trajectory attention. The deployed mask transformer\ntakes as input a short clip consisting of only a few frames and predicts the\nclip-level segmentation. To enhance the temporal consistency, MaXTron employs\nwithin-clip and cross-clip tracking modules, efficiently utilizing trajectory\nattention. Originally designed for video classification, trajectory attention\nlearns to model the temporal correspondences between neighboring frames and\naggregates information along the estimated motion paths. However, it is\nnontrivial to directly extend trajectory attention to the per-pixel dense\nprediction tasks due to its quadratic dependency on input size. To alleviate\nthe issue, we propose to adapt the trajectory attention for both the dense\npixel features and object queries, aiming to improve the short-term and\nlong-term tracking results, respectively. Particularly, in our within-clip\ntracking module, we propose axial-trajectory attention that effectively\ncomputes the trajectory attention for tracking dense pixels sequentially along\nthe height- and width-axes. The axial decomposition significantly reduces the\ncomputational complexity for dense pixel features. In our cross-clip tracking\nmodule, since the object queries in mask transformer are learned to encode the\nobject information, we are able to capture the long-term temporal connections\nby applying trajectory attention to object queries, which learns to track each\nobject across different clips. Without bells and whistles, MaXTron demonstrates\nstate-of-the-art performances on video segmentation benchmarks.",
            "author": [
                "Ju He",
                "Qihang Yu",
                "Inkyu Shin",
                "Xueqing Deng",
                "Xiaohui Shen",
                "Alan Yuille",
                "Liang-Chieh Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18537v1",
                "http://arxiv.org/pdf/2311.18537v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00095v1",
            "title": "Textual-Knowledge-Guided Numerical Feature Discovery Method for Power\n  Demand Forecasting",
            "updated": "2023-11-30T13:17:22Z",
            "published": "2023-11-30T13:17:22Z",
            "summary": "Power demand forecasting is a crucial and challenging task for new power\nsystem and integrated energy system. However, as public feature databases and\nthe theoretical mechanism of power demand changes are unavailable, the known\nfeatures of power demand fluctuation are much limited. Recently, multimodal\nlearning approaches have shown great vitality in machine learning and AIGC. In\nthis paper, we interact two modal data and propose a textual-knowledge-guided\nnumerical feature discovery (TKNFD) method for short-term power demand\nforecasting. TKNFD extensively accumulates qualitative textual knowledge,\nexpands it into a candidate feature-type set, collects numerical data of these\nfeatures, and eventually builds four-dimensional multivariate source-tracking\ndatabases (4DM-STDs). Next, TKNFD presents a two-level quantitative feature\nidentification strategy independent of forecasting models, finds 43-48\nfeatures, and systematically analyses feature contribution and dependency\ncorrelation. Benchmark experiments in two different regions around the world\ndemonstrate that the forecasting accuracy of TKNFD-discovered features reliably\noutperforms that of SoTA feature schemes by 16.84% to 36.36% MAPE. In\nparticular, TKNFD reveals many unknown features, especially several dominant\nfeatures in the unknown energy and astronomical dimensions, which extend the\nknowledge on the origin of strong randomness and non-linearity in power demand\nfluctuation. Besides, 4DM-STDs can serve as public baseline databases.",
            "author": [
                "Zifan Ning",
                "Min Jin"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00095v1",
                "http://arxiv.org/pdf/2312.00095v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18531v1",
            "title": "Dataset Distillation via the Wasserstein Metric",
            "updated": "2023-11-30T13:15:28Z",
            "published": "2023-11-30T13:15:28Z",
            "summary": "Dataset distillation (DD) offers a compelling approach in computer vision,\nwith the goal of condensing extensive datasets into smaller synthetic versions\nwithout sacrificing much of the model performance. In this paper, we continue\nto study the methods for DD, by addressing its conceptually core objective: how\nto capture the essential representation of extensive datasets in smaller,\nsynthetic forms.\n  We propose a novel approach utilizing the Wasserstein distance, a metric\nrooted in optimal transport theory, to enhance distribution matching in DD. Our\nmethod leverages the Wasserstein barycenter, offering a geometrically\nmeaningful way to quantify distribution differences and effectively capture the\ncentroid of a set of distributions. Our approach retains the computational\nbenefits of distribution matching-based methods while achieving new\nstate-of-the-art performance on several benchmarks.\n  To provide useful prior for learning the images, we embed the synthetic data\ninto the feature space of pretrained classification models to conduct\ndistribution matching. Extensive testing on various high-resolution datasets\nconfirms the effectiveness and adaptability of our method, indicating the\npromising yet unexplored capabilities of Wasserstein metrics in dataset\ndistillation.",
            "author": [
                "Haoyang Liu",
                "Tiancheng Xing",
                "Luwei Li",
                "Vibhu Dalal",
                "Jingrui He",
                "Haohan Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18531v1",
                "http://arxiv.org/pdf/2311.18531v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00094v1",
            "title": "Fast ODE-based Sampling for Diffusion Models in Around 5 Steps",
            "updated": "2023-11-30T13:07:19Z",
            "published": "2023-11-30T13:07:19Z",
            "summary": "Sampling from diffusion models can be treated as solving the corresponding\nordinary differential equations (ODEs), with the aim of obtaining an accurate\nsolution with as few number of function evaluations (NFE) as possible.\nRecently, various fast samplers utilizing higher-order ODE solvers have emerged\nand achieved better performance than the initial first-order one. However,\nthese numerical methods inherently result in certain approximation errors,\nwhich significantly degrades sample quality with extremely small NFE (e.g.,\naround 5). In contrast, based on the geometric observation that each sampling\ntrajectory almost lies in a two-dimensional subspace embedded in the ambient\nspace, we propose Approximate MEan-Direction Solver (AMED-Solver) that\neliminates truncation errors by directly learning the mean direction for fast\ndiffusion sampling. Besides, our method can be easily used as a plugin to\nfurther improve existing ODE-based samplers. Extensive experiments on image\nsynthesis with the resolution ranging from 32 to 256 demonstrate the\neffectiveness of our method. With only 5 NFE, we achieve 7.14 FID on CIFAR-10,\n13.75 FID on ImageNet 64$\\times$64, and 12.79 FID on LSUN Bedroom. Our code is\navailable at https://github.com/zhyzhouu/amed-solver.",
            "author": [
                "Zhenyu Zhou",
                "Defang Chen",
                "Can Wang",
                "Chun Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00094v1",
                "http://arxiv.org/pdf/2312.00094v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18527v1",
            "title": "InfoFlowNet: A Multi-head Attention-based Self-supervised Learning Model\n  with Surrogate Approach for Uncovering Brain Effective Connectivity",
            "updated": "2023-11-30T13:06:04Z",
            "published": "2023-11-30T13:06:04Z",
            "summary": "Deciphering brain network topology can enhance the depth of neuroscientific\nknowledge and facilitate the development of neural engineering methods.\nEffective connectivity, a measure of brain network dynamics, is particularly\nuseful for investigating the directional influences among different brain\nregions. In this study, we introduce a novel brain causal inference model named\nInfoFlowNet, which leverages the self-attention mechanism to capture\nassociations among electroencephalogram (EEG) time series. The proposed method\nestimates the magnitude of directional information flow (dIF) among EEG\nprocesses by measuring the loss of model inference resulting from the shuffling\nof the time order of the original time series. To evaluate the feasibility of\nInfoFlowNet, we conducted experiments using a synthetic time series and two EEG\ndatasets. The results demonstrate that InfoFlowNet can extract time-varying\ncausal relationships among processes, reflected in the fluctuation of dIF\nvalues. Compared with the Granger causality model and temporal causal discovery\nframework, InfoFlowNet can identify more significant causal edges underlying\nEEG processes while maintaining an acceptable computation time. Our work\ndemonstrates the potential of InfoFlowNet for analyzing effective connectivity\nin EEG data. The findings highlight the importance of effective connectivity in\nunderstanding the complex dynamics of the brain network.",
            "author": [
                "Chun-Hsiang Chuang",
                "Shao-Xun Fang",
                "Chih-Sheng Huang",
                "Weiping Ding"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18527v1",
                "http://arxiv.org/pdf/2311.18527v1"
            ],
            "primary_category": "q-bio.NC",
            "category": [
                "q-bio.NC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18526v1",
            "title": "HOT: Higher-Order Dynamic Graph Representation Learning with Efficient\n  Transformers",
            "updated": "2023-11-30T13:05:39Z",
            "published": "2023-11-30T13:05:39Z",
            "summary": "Many graph representation learning (GRL) problems are dynamic, with millions\nof edges added or removed per second. A fundamental workload in this setting is\ndynamic link prediction: using a history of graph updates to predict whether a\ngiven pair of vertices will become connected. Recent schemes for link\nprediction in such dynamic settings employ Transformers, modeling individual\ngraph updates as single tokens. In this work, we propose HOT: a model that\nenhances this line of works by harnessing higher-order (HO) graph structures;\nspecifically, k-hop neighbors and more general subgraphs containing a given\npair of vertices. Harnessing such HO structures by encoding them into the\nattention matrix of the underlying Transformer results in higher accuracy of\nlink prediction outcomes, but at the expense of increased memory pressure. To\nalleviate this, we resort to a recent class of schemes that impose hierarchy on\nthe attention matrix, significantly reducing memory footprint. The final design\noffers a sweetspot between high accuracy and low memory utilization. HOT\noutperforms other dynamic GRL schemes, for example achieving 9%, 7%, and 15%\nhigher accuracy than - respectively - DyGFormer, TGN, and GraphMixer, for the\nMOOC dataset. Our design can be seamlessly extended towards other dynamic GRL\nworkloads.",
            "author": [
                "Maciej Besta",
                "Afonso Claudino Catarino",
                "Lukas Gianinazzi",
                "Nils Blach",
                "Piotr Nyczyk",
                "Hubert Niewiadomski",
                "Torsten Hoefler"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18526v1",
                "http://arxiv.org/pdf/2311.18526v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18525v1",
            "title": "Detecting Anomalous Network Communication Patterns Using Graph\n  Convolutional Networks",
            "updated": "2023-11-30T13:03:49Z",
            "published": "2023-11-30T13:03:49Z",
            "summary": "To protect an organizations' endpoints from sophisticated cyberattacks,\nadvanced detection methods are required. In this research, we present\nGCNetOmaly: a graph convolutional network (GCN)-based variational autoencoder\n(VAE) anomaly detector trained on data that include connection events among\ninternal and external machines. As input, the proposed GCN-based VAE model\nreceives two matrices: (i) the normalized adjacency matrix, which represents\nthe connections among the machines, and (ii) the feature matrix, which includes\nvarious features (demographic, statistical, process-related, and Node2vec\nstructural features) that are used to profile the individual nodes/machines.\nAfter training the model on data collected for a predefined time window, the\nmodel is applied on the same data; the reconstruction score obtained by the\nmodel for a given machine then serves as the machine's anomaly score.\nGCNetOmaly was evaluated on real, large-scale data logged by Carbon Black EDR\nfrom a large financial organization's automated teller machines (ATMs) as well\nas communication with Active Directory (AD) servers in two setups: unsupervised\nand supervised. The results of our evaluation demonstrate GCNetOmaly's\neffectiveness in detecting anomalous behavior of machines on unsupervised data.",
            "author": [
                "Yizhak Vaisman",
                "Gilad Katz",
                "Yuval Elovici",
                "Asaf Shabtai"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18525v1",
                "http://arxiv.org/pdf/2311.18525v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18521v1",
            "title": "Combining deep generative models with extreme value theory for synthetic\n  hazard simulation: a multivariate and spatially coherent approach",
            "updated": "2023-11-30T12:55:51Z",
            "published": "2023-11-30T12:55:51Z",
            "summary": "Climate hazards can cause major disasters when they occur simultaneously as\ncompound hazards. To understand the distribution of climate risk and inform\nadaptation policies, scientists need to simulate a large number of physically\nrealistic and spatially coherent events. Current methods are limited by\ncomputational constraints and the probabilistic spatial distribution of\ncompound events is not given sufficient attention. The bottleneck in current\napproaches lies in modelling the dependence structure between variables, as\ninference on parametric models suffers from the curse of dimensionality.\nGenerative adversarial networks (GANs) are well-suited to such a problem due to\ntheir ability to implicitly learn the distribution of data in high-dimensional\nsettings. We employ a GAN to model the dependence structure for daily maximum\nwind speed, significant wave height, and total precipitation over the Bay of\nBengal, combining this with traditional extreme value theory for controlled\nextrapolation of the tails. Once trained, the model can be used to efficiently\ngenerate thousands of realistic compound hazard events, which can inform\nclimate risk assessments for climate adaptation and disaster preparedness. The\nmethod developed is flexible and transferable to other multivariate and spatial\nclimate datasets.",
            "author": [
                "Alison Peard",
                "Jim Hall"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18521v1",
                "http://arxiv.org/pdf/2311.18521v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18520v1",
            "title": "Calibration-free online test-time adaptation for electroencephalography\n  motor imagery decoding",
            "updated": "2023-11-30T12:53:43Z",
            "published": "2023-11-30T12:53:43Z",
            "summary": "Providing a promising pathway to link the human brain with external devices,\nBrain-Computer Interfaces (BCIs) have seen notable advancements in decoding\ncapabilities, primarily driven by increasingly sophisticated techniques,\nespecially deep learning. However, achieving high accuracy in real-world\nscenarios remains a challenge due to the distribution shift between sessions\nand subjects. In this paper we will explore the concept of online test-time\nadaptation (OTTA) to continuously adapt the model in an unsupervised fashion\nduring inference time. Our approach guarantees the preservation of privacy by\neliminating the requirement to access the source data during the adaptation\nprocess. Additionally, OTTA achieves calibration-free operation by not\nrequiring any session- or subject-specific data. We will investigate the task\nof electroencephalography (EEG) motor imagery decoding using a lightweight\narchitecture together with different OTTA techniques like alignment, adaptive\nbatch normalization, and entropy minimization. We examine two datasets and\nthree distinct data settings for a comprehensive analysis. Our adaptation\nmethods produce state-of-the-art results, potentially instigating a shift in\ntransfer learning for BCI decoding towards online adaptation.",
            "author": [
                "Martin Wimpff",
                "Mario D\u00f6bler",
                "Bin Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18520v1",
                "http://arxiv.org/pdf/2311.18520v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC",
                "cs.AI",
                "cs.LG",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18512v1",
            "title": "Revisiting Proposal-based Object Detection",
            "updated": "2023-11-30T12:40:23Z",
            "published": "2023-11-30T12:40:23Z",
            "summary": "This paper revisits the pipeline for detecting objects in images with\nproposals. For any object detector, the obtained box proposals or queries need\nto be classified and regressed towards ground truth boxes. The common solution\nfor the final predictions is to directly maximize the overlap between each\nproposal and the ground truth box, followed by a winner-takes-all ranking or\nnon-maximum suppression. In this work, we propose a simple yet effective\nalternative. For proposal regression, we solve a simpler problem where we\nregress to the area of intersection between proposal and ground truth. In this\nway, each proposal only specifies which part contains the object, avoiding a\nblind inpainting problem where proposals need to be regressed beyond their\nvisual scope. In turn, we replace the winner-takes-all strategy and obtain the\nfinal prediction by taking the union over the regressed intersections of a\nproposal group surrounding an object. Our revisited approach comes with minimal\nchanges to the detection pipeline and can be plugged into any existing method.\nWe show that our approach directly improves canonical object detection and\ninstance segmentation architectures, highlighting the utility of\nintersection-based regression and grouping.",
            "author": [
                "Aritra Bhowmik",
                "Martin R. Oswald",
                "Pascal Mettes",
                "Cees G. M. Snoek"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18512v1",
                "http://arxiv.org/pdf/2311.18512v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18508v1",
            "title": "DifAugGAN: A Practical Diffusion-style Data Augmentation for GAN-based\n  Single Image Super-resolution",
            "updated": "2023-11-30T12:37:53Z",
            "published": "2023-11-30T12:37:53Z",
            "summary": "It is well known the adversarial optimization of GAN-based image\nsuper-resolution (SR) methods makes the preceding SR model generate unpleasant\nand undesirable artifacts, leading to large distortion. We attribute the cause\nof such distortions to the poor calibration of the discriminator, which hampers\nits ability to provide meaningful feedback to the generator for learning\nhigh-quality images. To address this problem, we propose a simple but\nnon-travel diffusion-style data augmentation scheme for current GAN-based SR\nmethods, known as DifAugGAN. It involves adapting the diffusion process in\ngenerative diffusion models for improving the calibration of the discriminator\nduring training motivated by the successes of data augmentation schemes in the\nfield to achieve good calibration. Our DifAugGAN can be a Plug-and-Play\nstrategy for current GAN-based SISR methods to improve the calibration of the\ndiscriminator and thus improve SR performance. Extensive experimental\nevaluations demonstrate the superiority of DifAugGAN over state-of-the-art\nGAN-based SISR methods across both synthetic and real-world datasets,\nshowcasing notable advancements in both qualitative and quantitative results.",
            "author": [
                "Axi Niu",
                "Kang Zhang",
                "Joshua Tian Jin Tee",
                "Trung X. Pham",
                "Jinqiu Sun",
                "Chang D. Yoo",
                "In So Kweon",
                "Yanning Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18508v1",
                "http://arxiv.org/pdf/2311.18508v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18506v1",
            "title": "Global Convergence of Online Identification for Mixed Linear Regression",
            "updated": "2023-11-30T12:30:42Z",
            "published": "2023-11-30T12:30:42Z",
            "summary": "Mixed linear regression (MLR) is a powerful model for characterizing\nnonlinear relationships by utilizing a mixture of linear regression sub-models.\nThe identification of MLR is a fundamental problem, where most of the existing\nresults focus on offline algorithms, rely on independent and identically\ndistributed (i.i.d) data assumptions, and provide local convergence results\nonly. This paper investigates the online identification and data clustering\nproblems for two basic classes of MLRs, by introducing two corresponding new\nonline identification algorithms based on the expectation-maximization (EM)\nprinciple. It is shown that both algorithms will converge globally without\nresorting to the traditional i.i.d data assumptions. The main challenge in our\ninvestigation lies in the fact that the gradient of the maximum likelihood\nfunction does not have a unique zero, and a key step in our analysis is to\nestablish the stability of the corresponding differential equation in order to\napply the celebrated Ljung's ODE method. It is also shown that the\nwithin-cluster error and the probability that the new data is categorized into\nthe correct cluster are asymptotically the same as those in the case of known\nparameters. Finally, numerical simulations are provided to verify the\neffectiveness of our online algorithms.",
            "author": [
                "Yujing Liu",
                "Zhixin Liu",
                "Lei Guo"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18506v1",
                "http://arxiv.org/pdf/2311.18506v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG",
                "cs.SY",
                "eess.SY",
                "math.ST",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18503v1",
            "title": "End-to-End Retrieval with Learned Dense and Sparse Representations Using\n  Lucene",
            "updated": "2023-11-30T12:28:43Z",
            "published": "2023-11-30T12:28:43Z",
            "summary": "The bi-encoder architecture provides a framework for understanding\nmachine-learned retrieval models based on dense and sparse vector\nrepresentations. Although these representations capture parametric realizations\nof the same underlying conceptual framework, their respective implementations\nof top-$k$ similarity search require the coordination of different software\ncomponents (e.g., inverted indexes, HNSW indexes, and toolkits for neural\ninference), often knitted together in complex architectures. In this work, we\nask the following question: What's the simplest design, in terms of requiring\nthe fewest changes to existing infrastructure, that can support end-to-end\nretrieval with modern dense and sparse representations? The answer appears to\nbe that Lucene is sufficient, as we demonstrate in Anserini, a toolkit for\nreproducible information retrieval research. That is, effective retrieval with\nmodern single-vector neural models can be efficiently performed directly in\nJava on the CPU. We examine the implications of this design for information\nretrieval researchers pushing the state of the art as well as for software\nengineers building production search systems.",
            "author": [
                "Haonan Chen",
                "Carlos Lassance",
                "Jimmy Lin"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18503v1",
                "http://arxiv.org/pdf/2311.18503v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18501v1",
            "title": "Perturbation-based Analysis of Compositional Data",
            "updated": "2023-11-30T12:27:15Z",
            "published": "2023-11-30T12:27:15Z",
            "summary": "Existing statistical methods for compositional data analysis are inadequate\nfor many modern applications for two reasons. First, modern compositional\ndatasets, for example in microbiome research, display traits such as\nhigh-dimensionality and sparsity that are poorly modelled with traditional\napproaches. Second, assessing -- in an unbiased way -- how summary statistics\nof a composition (e.g., racial diversity) affect a response variable is not\nstraightforward. In this work, we propose a framework based on hypothetical\ndata perturbations that addresses both issues. Unlike existing methods for\ncompositional data, we do not transform the data and instead use perturbations\nto define interpretable statistical functionals on the compositions themselves,\nwhich we call average perturbation effects. These average perturbation effects,\nwhich can be employed in many applications, naturally account for confounding\nthat biases frequently used marginal dependence analyses. We show how average\nperturbation effects can be estimated efficiently by deriving a\nperturbation-dependent reparametrization and applying semiparametric estimation\ntechniques. We analyze the proposed estimators empirically on simulated data\nand demonstrate advantages over existing techniques on US census and microbiome\ndata. For all proposed estimators, we provide confidence intervals with uniform\nasymptotic coverage guarantees.",
            "author": [
                "Anton Rask Lundborg",
                "Niklas Pfister"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18501v1",
                "http://arxiv.org/pdf/2311.18501v1"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME",
                "math.ST",
                "stat.ML",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18498v1",
            "title": "Data-Agnostic Model Poisoning against Federated Learning: A Graph\n  Autoencoder Approach",
            "updated": "2023-11-30T12:19:10Z",
            "published": "2023-11-30T12:19:10Z",
            "summary": "This paper proposes a novel, data-agnostic, model poisoning attack on\nFederated Learning (FL), by designing a new adversarial graph autoencoder\n(GAE)-based framework. The attack requires no knowledge of FL training data and\nachieves both effectiveness and undetectability. By listening to the benign\nlocal models and the global model, the attacker extracts the graph structural\ncorrelations among the benign local models and the training data features\nsubstantiating the models. The attacker then adversarially regenerates the\ngraph structural correlations while maximizing the FL training loss, and\nsubsequently generates malicious local models using the adversarial graph\nstructure and the training data features of the benign ones. A new algorithm is\ndesigned to iteratively train the malicious local models using GAE and\nsub-gradient descent. The convergence of FL under attack is rigorously proved,\nwith a considerably large optimality gap. Experiments show that the FL accuracy\ndrops gradually under the proposed attack and existing defense mechanisms fail\nto detect it. The attack can give rise to an infection across all benign\ndevices, making it a serious threat to FL.",
            "author": [
                "Kai Li",
                "Jingjing Zheng",
                "Xin Yuan",
                "Wei Ni",
                "Ozgur B. Akan",
                "H. Vincent Poor"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18498v1",
                "http://arxiv.org/pdf/2311.18498v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18496v1",
            "title": "Accurate Segmentation of Optic Disc And Cup from Multiple Pseudo-labels\n  by Noise-Aware Learning",
            "updated": "2023-11-30T12:17:16Z",
            "published": "2023-11-30T12:17:16Z",
            "summary": "Optic disc and cup segmentation play a crucial role in automating the\nscreening and diagnosis of optic glaucoma. While data-driven convolutional\nneural networks (CNNs) show promise in this area, the inherent ambiguity of\nsegmenting object and background boundaries in the task of optic disc and cup\nsegmentation leads to noisy annotations that impact model performance. To\naddress this, we propose an innovative label-denoising method of Multiple\nPseudo-labels Noise-aware Network (MPNN) for accurate optic disc and cup\nsegmentation. Specifically, the Multiple Pseudo-labels Generation and Guided\nDenoising (MPGGD) module generates pseudo-labels by multiple different\ninitialization networks trained on true labels, and the pixel-level consensus\ninformation extracted from these pseudo-labels guides to differentiate clean\npixels from noisy pixels. The training framework of the MPNN is constructed by\na teacher-student architecture to learn segmentation from clean pixels and\nnoisy pixels. Particularly, such a framework adeptly leverages (i) reliable and\nfundamental insights from clean pixels and (ii) the supplementary knowledge\nwithin noisy pixels via multiple perturbation-based unsupervised consistency.\nCompared to other label-denoising methods, comprehensive experimental results\non the RIGA dataset demonstrate our method's excellent performance and\nsignificant denoising ability.",
            "author": [
                "Tengjin Weng",
                "Yang Shen",
                "Zhidong Zhao",
                "Zhiming Cheng",
                "Shuai Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18496v1",
                "http://arxiv.org/pdf/2311.18496v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18495v1",
            "title": "Improving Adversarial Transferability via Model Alignment",
            "updated": "2023-11-30T12:15:49Z",
            "published": "2023-11-30T12:15:49Z",
            "summary": "Neural networks are susceptible to adversarial perturbations that are\ntransferable across different models. In this paper, we introduce a novel model\nalignment technique aimed at improving a given source model's ability in\ngenerating transferable adversarial perturbations. During the alignment\nprocess, the parameters of the source model are fine-tuned to minimize an\nalignment loss. This loss measures the divergence in the predictions between\nthe source model and another, independently trained model, referred to as the\nwitness model. To understand the effect of model alignment, we conduct a\ngeometric anlaysis of the resulting changes in the loss landscape. Extensive\nexperiments on the ImageNet dataset, using a variety of model architectures,\ndemonstrate that perturbations generated from aligned source models exhibit\nsignificantly higher transferability than those from the original source model.",
            "author": [
                "Avery Ma",
                "Amir-massoud Farahmand",
                "Yangchen Pan",
                "Philip Torr",
                "Jindong Gu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18495v1",
                "http://arxiv.org/pdf/2311.18495v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18494v1",
            "title": "PRS: Sharp Feature Priors for Resolution-Free Surface Remeshing",
            "updated": "2023-11-30T12:15:45Z",
            "published": "2023-11-30T12:15:45Z",
            "summary": "Surface reconstruction with preservation of geometric features is a\nchallenging computer vision task. Despite significant progress in implicit\nshape reconstruction, state-of-the-art mesh extraction methods often produce\naliased, perceptually distorted surfaces and lack scalability to\nhigh-resolution 3D shapes. We present a data-driven approach for automatic\nfeature detection and remeshing that requires only a coarse, aliased mesh as\ninput and scales to arbitrary resolution reconstructions. We define and learn a\ncollection of surface-based fields to (1) capture sharp geometric features in\nthe shape with an implicit vertexwise model and (2) approximate improvements in\nnormals alignment obtained by applying edge-flips with an edgewise model. To\nsupport scaling to arbitrary complexity shapes, we learn our fields using local\ntriangulated patches, fusing estimates on complete surface meshes. Our feature\nremeshing algorithm integrates the learned fields as sharp feature priors and\noptimizes vertex placement and mesh connectivity for maximum expected surface\nimprovement. On a challenging collection of high-resolution shape\nreconstructions in the ABC dataset, our algorithm improves over\nstate-of-the-art by 26% normals F-score and 42% perceptual\n$\\text{RMSE}_{\\text{v}}$.",
            "author": [
                "Natalia Soboleva",
                "Olga Gorbunova",
                "Maria Ivanova",
                "Evgeny Burnaev",
                "Matthias Nie\u00dfner",
                "Denis Zorin",
                "Alexey Artemov"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18494v1",
                "http://arxiv.org/pdf/2311.18494v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00823v1",
            "title": "Adaptive Multi-Modality Prompt Learning",
            "updated": "2023-11-30T12:10:22Z",
            "published": "2023-11-30T12:10:22Z",
            "summary": "Although current prompt learning methods have successfully been designed to\neffectively reuse the large pre-trained models without fine-tuning their large\nnumber of parameters, they still have limitations to be addressed, i.e.,\nwithout considering the adverse impact of meaningless patches in every image\nand without simultaneously considering in-sample generalization and\nout-of-sample generalization. In this paper, we propose an adaptive\nmulti-modality prompt learning to address the above issues. To do this, we\nemploy previous text prompt learning and propose a new image prompt learning.\nThe image prompt learning achieves in-sample and out-of-sample generalization,\nby first masking meaningless patches and then padding them with the learnable\nparameters and the information from texts. Moreover, each of the prompts\nprovides auxiliary information to each other, further strengthening these two\nkinds of generalization. Experimental results on real datasets demonstrate that\nour method outperforms SOTA methods, in terms of different downstream tasks.",
            "author": [
                "Zongqian Wu",
                "Yujing Liu",
                "Mengmeng Zhan",
                "Jialie Shen",
                "Ping Hu",
                "Xiaofeng Zhu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00823v1",
                "http://arxiv.org/pdf/2312.00823v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18491v1",
            "title": "ZeST-NeRF: Using temporal aggregation for Zero-Shot Temporal NeRFs",
            "updated": "2023-11-30T12:06:15Z",
            "published": "2023-11-30T12:06:15Z",
            "summary": "In the field of media production, video editing techniques play a pivotal\nrole. Recent approaches have had great success at performing novel view image\nsynthesis of static scenes. But adding temporal information adds an extra layer\nof complexity. Previous models have focused on implicitly representing static\nand dynamic scenes using NeRF. These models achieve impressive results but are\ncostly at training and inference time. They overfit an MLP to describe the\nscene implicitly as a function of position. This paper proposes ZeST-NeRF, a\nnew approach that can produce temporal NeRFs for new scenes without retraining.\nWe can accurately reconstruct novel views using multi-view synthesis techniques\nand scene flow-field estimation, trained only with unrelated scenes. We\ndemonstrate how existing state-of-the-art approaches from a range of fields\ncannot adequately solve this new task and demonstrate the efficacy of our\nsolution. The resulting network improves quantitatively by 15% and produces\nsignificantly better visual results.",
            "author": [
                "Violeta Men\u00e9ndez Gonz\u00e1lez",
                "Andrew Gilbert",
                "Graeme Phillipson",
                "Stephen Jolly",
                "Simon Hadfield"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18491v1",
                "http://arxiv.org/pdf/2311.18491v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.GR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18473v1",
            "title": "DGMem: Learning Visual Navigation Policy without Any Labels by Dynamic\n  Graph Memory",
            "updated": "2023-11-30T11:29:58Z",
            "published": "2023-11-30T11:29:58Z",
            "summary": "In recent years, learning-based approaches have demonstrated significant\npromise in addressing intricate navigation tasks. Traditional methods for\ntraining deep neural network navigation policies rely on meticulously designed\nreward functions or extensive teleoperation datasets as navigation\ndemonstrations. However, the former is often confined to simulated\nenvironments, and the latter demands substantial human labor, making it a\ntime-consuming process. Our vision is for robots to autonomously learn\nnavigation skills and adapt their behaviors to environmental changes without\nany human intervention. In this work, we discuss the self-supervised navigation\nproblem and present Dynamic Graph Memory (DGMem), which facilitates training\nonly with on-board observations. With the help of DGMem, agents can actively\nexplore their surroundings, autonomously acquiring a comprehensive navigation\npolicy in a data-efficient manner without external feedback. Our method is\nevaluated in photorealistic 3D indoor scenes, and empirical studies demonstrate\nthe effectiveness of DGMem.",
            "author": [
                "Wenzhe Cai",
                "Teng Wang",
                "Guangran Cheng",
                "Lele Xu",
                "Changyin Sun"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18473v1",
                "http://arxiv.org/pdf/2311.18473v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18460v1",
            "title": "Causal Fairness under Unobserved Confounding: A Neural Sensitivity\n  Framework",
            "updated": "2023-11-30T11:11:26Z",
            "published": "2023-11-30T11:11:26Z",
            "summary": "Fairness for machine learning predictions is widely required in practice for\nlegal, ethical, and societal reasons. Existing work typically focuses on\nsettings without unobserved confounding, even though unobserved confounding can\nlead to severe violations of causal fairness and, thus, unfair predictions. In\nthis work, we analyze the sensitivity of causal fairness to unobserved\nconfounding. Our contributions are three-fold. First, we derive bounds for\ncausal fairness metrics under different sources of unobserved confounding. This\nenables practitioners to examine the sensitivity of their machine learning\nmodels to unobserved confounding in fairness-critical applications. Second, we\npropose a novel neural framework for learning fair predictions, which allows us\nto offer worst-case guarantees of the extent to which causal fairness can be\nviolated due to unobserved confounding. Third, we demonstrate the effectiveness\nof our framework in a series of experiments, including a real-world case study\nabout predicting prison sentences. To the best of our knowledge, ours is the\nfirst work to study causal fairness under unobserved confounding. To this end,\nour work is of direct practical value as a refutation strategy to ensure the\nfairness of predictions in high-stakes applications.",
            "author": [
                "Maresa Schr\u00f6der",
                "Dennis Frauen",
                "Stefan Feuerriegel"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18460v1",
                "http://arxiv.org/pdf/2311.18460v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CY",
                "stat.ME"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00092v1",
            "title": "Mixture of Gaussian-distributed Prototypes with Generative Modelling for\n  Interpretable Image Classification",
            "updated": "2023-11-30T11:01:37Z",
            "published": "2023-11-30T11:01:37Z",
            "summary": "Prototypical-part interpretable methods, e.g., ProtoPNet, enhance\ninterpretability by connecting classification predictions to class-specific\ntraining prototypes, thereby offering an intuitive insight into their\ndecision-making. Current methods rely on a discriminative classifier trained\nwith point-based learning techniques that provide specific values for\nprototypes. Such prototypes have relatively low representation power due to\ntheir sparsity and potential redundancy, with each prototype containing no\nvariability measure. In this paper, we present a new generative learning of\nprototype distributions, named Mixture of Gaussian-distributed Prototypes\n(MGProto), which are represented by Gaussian mixture models (GMM). Such an\napproach enables the learning of more powerful prototype representations since\neach learned prototype will own a measure of variability, which naturally\nreduces the sparsity given the spread of the distribution around each\nprototype, and we also integrate a prototype diversity objective function into\nthe GMM optimisation to reduce redundancy. Incidentally, the generative nature\nof MGProto offers a new and effective way for detecting out-of-distribution\nsamples. To improve the compactness of MGProto, we further propose to prune\nGaussian-distributed prototypes with a low prior. Experiments on CUB-200-2011,\nStanford Cars, Stanford Dogs, and Oxford-IIIT Pets datasets show that MGProto\nachieves state-of-the-art classification and OoD detection performances with\nencouraging interpretability results.",
            "author": [
                "Chong Wang",
                "Yuanhong Chen",
                "Fengbei Liu",
                "Davis James McCarthy",
                "Helen Frazer",
                "Gustavo Carneiro"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00092v1",
                "http://arxiv.org/pdf/2312.00092v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18451v1",
            "title": "How Much Is Hidden in the NAS Benchmarks? Few-Shot Adaptation of a NAS\n  Predictor",
            "updated": "2023-11-30T10:51:46Z",
            "published": "2023-11-30T10:51:46Z",
            "summary": "Neural architecture search has proven to be a powerful approach to designing\nand refining neural networks, often boosting their performance and efficiency\nover manually-designed variations, but comes with computational overhead. While\nthere has been a considerable amount of research focused on lowering the cost\nof NAS for mainstream tasks, such as image classification, a lot of those\nimprovements stem from the fact that those tasks are well-studied in the\nbroader context. Consequently, applicability of NAS to emerging and\nunder-represented domains is still associated with a relatively high cost\nand/or uncertainty about the achievable gains. To address this issue, we turn\nour focus towards the recent growth of publicly available NAS benchmarks in an\nattempt to extract general NAS knowledge, transferable across different tasks\nand search spaces. We borrow from the rich field of meta-learning for few-shot\nadaptation and carefully study applicability of those methods to NAS, with a\nspecial focus on the relationship between task-level correlation (domain shift)\nand predictor transferability; which we deem critical for improving NAS on\ndiverse tasks. In our experiments, we use 6 NAS benchmarks in conjunction,\nspanning in total 16 NAS settings -- our meta-learning approach not only shows\nsuperior (or matching) performance in the cross-validation experiments but also\nsuccessful extrapolation to a new search space and tasks.",
            "author": [
                "Hrushikesh Loya",
                "\u0141ukasz Dudziak",
                "Abhinav Mehrotra",
                "Royson Lee",
                "Javier Fernandez-Marques",
                "Nicholas D. Lane",
                "Hongkai Wen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18451v1",
                "http://arxiv.org/pdf/2311.18451v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18450v1",
            "title": "Lessons from Building CodeBuddy: A Contextualized AI Coding Assistant",
            "updated": "2023-11-30T10:51:26Z",
            "published": "2023-11-30T10:51:26Z",
            "summary": "With their exceptional natural language processing capabilities, tools based\non Large Language Models (LLMs) like ChatGPT and Co-Pilot have swiftly become\nindispensable resources in the software developer's toolkit. While recent\nstudies suggest the potential productivity gains these tools can unlock, users\nstill encounter drawbacks, such as generic or incorrect answers. Additionally,\nthe pursuit of improved responses often leads to extensive prompt engineering\nefforts, diverting valuable time from writing code that delivers actual value.\nTo address these challenges, a new breed of tools, built atop LLMs, is\nemerging. These tools aim to mitigate drawbacks by employing techniques like\nfine-tuning or enriching user prompts with contextualized information.\n  In this paper, we delve into the lessons learned by a software development\nteam venturing into the creation of such a contextualized LLM-based\napplication, using retrieval-based techniques, called CodeBuddy. Over a\nfour-month period, the team, despite lacking prior professional experience in\nLLM-based applications, built the product from scratch. Following the initial\nproduct release, we engaged with the development team responsible for the code\ngenerative components. Through interviews and analysis of the application's\nissue tracker, we uncover various intriguing challenges that teams working on\nLLM-based applications might encounter. For instance, we found three main group\nof lessons: LLM-based lessons, User-based lessons, and Technical lessons. By\nunderstanding these lessons, software development teams could become better\nprepared to build LLM-based applications.",
            "author": [
                "gustavo Pinto",
                "Cleidson de Souza",
                "Jo\u00e3o Batista Neto",
                "Alberto de Souza",
                "Tarc\u00edsio Gotto",
                "Edward Monteiro"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18450v1",
                "http://arxiv.org/pdf/2311.18450v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18437v1",
            "title": "The Sliding Regret in Stochastic Bandits: Discriminating Index and\n  Randomized Policies",
            "updated": "2023-11-30T10:37:03Z",
            "published": "2023-11-30T10:37:03Z",
            "summary": "This paper studies the one-shot behavior of no-regret algorithms for\nstochastic bandits. Although many algorithms are known to be asymptotically\noptimal with respect to the expected regret, over a single run, their\npseudo-regret seems to follow one of two tendencies: it is either smooth or\nbumpy. To measure this tendency, we introduce a new notion: the sliding regret,\nthat measures the worst pseudo-regret over a time-window of fixed length\nsliding to infinity. We show that randomized methods (e.g. Thompson Sampling\nand MED) have optimal sliding regret, while index policies, although possibly\nasymptotically optimal for the expected regret, have the worst possible sliding\nregret under regularity conditions on their index (e.g. UCB, UCB-V, KL-UCB,\nMOSS, IMED etc.). We further analyze the average bumpiness of the pseudo-regret\nof index policies via the regret of exploration, that we show to be suboptimal\nas well.",
            "author": [
                "Victor Boone"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18437v1",
                "http://arxiv.org/pdf/2311.18437v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.SY",
                "eess.SY",
                "math.OC",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18434v1",
            "title": "Exploring the Temperature-Dependent Phase Transition in Modern Hopfield\n  Networks",
            "updated": "2023-11-30T10:34:29Z",
            "published": "2023-11-30T10:34:29Z",
            "summary": "The recent discovery of a connection between Transformers and Modern Hopfield\nNetworks (MHNs) has reignited the study of neural networks from a physical\nenergy-based perspective. This paper focuses on the pivotal effect of the\ninverse temperature hyperparameter $\\beta$ on the distribution of energy minima\nof the MHN. To achieve this, the distribution of energy minima is tracked in a\nsimplified MHN in which equidistant normalised patterns are stored. This\nnetwork demonstrates a phase transition at a critical temperature\n$\\beta_{\\text{c}}$, from a single global attractor towards highly pattern\nspecific minima as $\\beta$ is increased. Importantly, the dynamics are not\nsolely governed by the hyperparameter $\\beta$ but are instead determined by an\neffective inverse temperature $\\beta_{\\text{eff}}$ which also depends on the\ndistribution and size of the stored patterns. Recognizing the role of\nhyperparameters in the MHN could, in the future, aid researchers in the domain\nof Transformers to optimise their initial choices, potentially reducing the\nnecessity for time and energy expensive hyperparameter fine-tuning.",
            "author": [
                "Felix Koulischer",
                "C\u00e9dric Goemaere",
                "Tom van der Meersch",
                "Johannes Deleu",
                "Thomas Demeester"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18434v1",
                "http://arxiv.org/pdf/2311.18434v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cond-mat.dis-nn"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18433v1",
            "title": "E2PNet: Event to Point Cloud Registration with Spatio-Temporal\n  Representation Learning",
            "updated": "2023-11-30T10:33:49Z",
            "published": "2023-11-30T10:33:49Z",
            "summary": "Event cameras have emerged as a promising vision sensor in recent years due\nto their unparalleled temporal resolution and dynamic range. While registration\nof 2D RGB images to 3D point clouds is a long-standing problem in computer\nvision, no prior work studies 2D-3D registration for event cameras. To this\nend, we propose E2PNet, the first learning-based method for event-to-point\ncloud registration. The core of E2PNet is a novel feature representation\nnetwork called Event-Points-to-Tensor (EP2T), which encodes event data into a\n2D grid-shaped feature tensor. This grid-shaped feature enables matured\nRGB-based frameworks to be easily used for event-to-point cloud registration,\nwithout changing hyper-parameters and the training procedure. EP2T treats the\nevent input as spatio-temporal point clouds. Unlike standard 3D learning\narchitectures that treat all dimensions of point clouds equally, the novel\nsampling and information aggregation modules in EP2T are designed to handle the\ninhomogeneity of the spatial and temporal dimensions. Experiments on the MVSEC\nand VECtor datasets demonstrate the superiority of E2PNet over hand-crafted and\nother learning-based methods. Compared to RGB-based registration, E2PNet is\nmore robust to extreme illumination or fast motion due to the use of event\ndata. Beyond 2D-3D registration, we also show the potential of EP2T for other\nvision tasks such as flow estimation, event-to-image reconstruction and object\nrecognition. The source code can be found at:\nhttps://github.com/Xmu-qcj/E2PNet.",
            "author": [
                "Xiuhong Lin",
                "Changjie Qiu",
                "Zhipeng Cai",
                "Siqi Shen",
                "Yu Zang",
                "Weiquan Liu",
                "Xuesheng Bian",
                "Matthias M\u00fcller",
                "Cheng Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18433v1",
                "http://arxiv.org/pdf/2311.18433v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18431v1",
            "title": "On the convergence of adaptive first order methods: proximal gradient\n  and alternating minimization algorithms",
            "updated": "2023-11-30T10:29:43Z",
            "published": "2023-11-30T10:29:43Z",
            "summary": "Building upon recent works on linesearch-free adaptive proximal gradient\nmethods, this paper proposes AdaPG$^{\\pi,r}$, a framework that unifies and\nextends existing results by providing larger stepsize policies and improved\nlower bounds. Different choices of the parameters $\\pi$ and $r$ are discussed\nand the efficacy of the resulting methods is demonstrated through numerical\nsimulations. In an attempt to better understand the underlying theory, its\nconvergence is established in a more general setting that allows for\ntime-varying parameters. Finally, an adaptive alternating minimization\nalgorithm is presented by exploring the dual setting. This algorithm not only\nincorporates additional adaptivity, but also expands its applicability beyond\nstandard strongly convex settings.",
            "author": [
                "Puya Latafat",
                "Andreas Themelis",
                "Panagiotis Patrinos"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18431v1",
                "http://arxiv.org/pdf/2311.18431v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "cs.LG",
                "65K05, 90C06, 90C25, 90C30, 90C47"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18430v1",
            "title": "Vehicular Cooperative Maneuvers -- Quo Vaditis?",
            "updated": "2023-11-30T10:27:29Z",
            "published": "2023-11-30T10:27:29Z",
            "summary": "Vehicles will not only get more and more automated, but they will also\ncooperate in new ways. Currently, human-driven vehicles begin to communicate\nwith each other using vehicle-to-everything technology. Future vehicles will\nuse communication to share sensor data and even negotiate cooperative\nmaneuvers. This lets them learn more about the environment and improves traffic\nflow and passenger comfort as more predictable maneuvers are likely to lead to\na smoother ride. This paper introduces the most important concepts around\ncooperative vehicular maneuvers. We also summarize currently open challenges\nand questions to answer before a deployment can begin. Afterward, we give some\nperspectives on the further evolution of cooperative maneuvers and beyond.",
            "author": [
                "Bernhard H\u00e4fner",
                "J\u00f6rg Ott",
                "Georg Albrecht Schmitt"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18430v1",
                "http://arxiv.org/pdf/2311.18430v1"
            ],
            "primary_category": "cs.NI",
            "category": [
                "cs.NI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18426v2",
            "title": "Convergence Analysis of Fractional Gradient Descent",
            "updated": "2023-12-04T06:27:04Z",
            "published": "2023-11-30T10:24:07Z",
            "summary": "Fractional derivatives are a well-studied generalization of integer order\nderivatives. Naturally, for optimization, it is of interest to understand the\nconvergence properties of gradient descent using fractional derivatives.\nConvergence analysis of fractional gradient descent is currently limited both\nin the methods analyzed and the settings analyzed. This paper aims to fill in\nthese gaps by analyzing variations of fractional gradient descent in smooth and\nconvex, smooth and strongly convex, and smooth and non-convex settings. First,\nnovel bounds will be established bridging fractional and integer derivatives.\nThen, these bounds will be applied to the aforementioned settings to prove\n$O(1/T)$ convergence for smooth and convex functions and linear convergence for\nsmooth and strongly convex functions. Additionally, we prove $O(1/T)$\nconvergence for smooth and non-convex functions using an extended notion of\nsmoothness that is more natural for fractional derivatives. Finally, empirical\nresults will be presented on the potential speed up of fractional gradient\ndescent over standard gradient descent as well as the challenges of predicting\nwhich will be faster in general.",
            "author": [
                "Ashwani Aggarwal"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18426v2",
                "http://arxiv.org/pdf/2311.18426v2"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "cs.LG",
                "cs.NA",
                "math.NA",
                "G.1.6"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18424v1",
            "title": "Multiple Disciplinary Data Work Practices in Artificial Intelligence\n  Research: a Healthcare Case Study in the UK",
            "updated": "2023-11-30T10:19:33Z",
            "published": "2023-11-30T10:19:33Z",
            "summary": "Developing artificial intelligence (AI) tools for healthcare is a multiple\ndisciplinary effort, bringing data scientists, clinicians, patients and other\ndisciplines together. In this paper, we explore the AI development workflow and\nhow participants navigate the challenges and tensions of sharing and generating\nknowledge across disciplines. Through an inductive thematic analysis of 13\nsemi-structured interviews with participants in a large research consortia, our\nfindings suggest that multiple disciplinarity heavily impacts work practices.\nParticipants faced challenges to learn the languages of other disciplines and\nneeded to adapt the tools used for sharing and communicating with their\naudience, particularly those from a clinical or patient perspective. Large\nhealth datasets also posed certain restrictions on work practices. We\nidentified meetings as a key platform for facilitating exchanges between\ndisciplines and allowing for the blending and creation of knowledge. Finally,\nwe discuss design implications for data science and collaborative tools, and\nrecommendations for future research.",
            "author": [
                "Rafael Henkin",
                "Elizabeth Remfry",
                "Duncan J. Reynolds",
                "Megan Clinch",
                "Michael R. Barnes"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18424v1",
                "http://arxiv.org/pdf/2311.18424v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC",
                "cs.AI",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18398v1",
            "title": "RainAI -- Precipitation Nowcasting from Satellite Data",
            "updated": "2023-11-30T09:49:16Z",
            "published": "2023-11-30T09:49:16Z",
            "summary": "This paper presents a solution to the Weather4Cast 2023 competition, where\nthe goal is to forecast high-resolution precipitation with an 8-hour lead time\nusing lower-resolution satellite radiance images. We propose a simple, yet\neffective method for spatiotemporal feature learning using a 2D U-Net model,\nthat outperforms the official 3D U-Net baseline in both performance and\nefficiency. We place emphasis on refining the dataset, through importance\nsampling and dataset preparation, and show that such techniques have a\nsignificant impact on performance. We further study an alternative\ncross-entropy loss function that improves performance over the standard mean\nsquared error loss, while also enabling models to produce probabilistic\noutputs. Additional techniques are explored regarding the generation of\npredictions at different lead times, specifically through Conditioning Lead\nTime. Lastly, to generate high-resolution forecasts, we evaluate standard and\nlearned upsampling methods. The code and trained parameters are available at\nhttps://github.com/rafapablos/w4c23-rainai.",
            "author": [
                "Rafael Pablos Sarabia",
                "Joachim Nyborg",
                "Morten Birk",
                "Ira Assent"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18398v1",
                "http://arxiv.org/pdf/2311.18398v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG",
                "physics.ao-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18393v1",
            "title": "Data-efficient Deep Reinforcement Learning for Vehicle Trajectory\n  Control",
            "updated": "2023-11-30T09:38:59Z",
            "published": "2023-11-30T09:38:59Z",
            "summary": "Advanced vehicle control is a fundamental building block in the development\nof autonomous driving systems. Reinforcement learning (RL) promises to achieve\ncontrol performance superior to classical approaches while keeping\ncomputational demands low during deployment. However, standard RL approaches\nlike soft-actor critic (SAC) require extensive amounts of training data to be\ncollected and are thus impractical for real-world application. To address this\nissue, we apply recently developed data-efficient deep RL methods to vehicle\ntrajectory control. Our investigation focuses on three methods, so far\nunexplored for vehicle control: randomized ensemble double Q-learning (REDQ),\nprobabilistic ensembles with trajectory sampling and model predictive path\nintegral optimizer (PETS-MPPI), and model-based policy optimization (MBPO). We\nfind that in the case of trajectory control, the standard model-based RL\nformulation used in approaches like PETS-MPPI and MBPO is not suitable. We,\ntherefore, propose a new formulation that splits dynamics prediction and\nvehicle localization. Our benchmark study on the CARLA simulator reveals that\nthe three identified data-efficient deep RL approaches learn control strategies\non a par with or better than SAC, yet reduce the required number of environment\ninteractions by more than one order of magnitude.",
            "author": [
                "Bernd Frauenknecht",
                "Tobias Ehlgen",
                "Sebastian Trimpe"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18393v1",
                "http://arxiv.org/pdf/2311.18393v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18392v1",
            "title": "Augmented Reality Technology in Teaching about Physics: A systematic\n  review of opportunities and challenges",
            "updated": "2023-11-30T09:35:36Z",
            "published": "2023-11-30T09:35:36Z",
            "summary": "The use of augmented reality (AR) allows for the integration of digital\ninformation onto our perception of the physical world. In this article, we\npresent a comprehensive review of previously published literature on the\nimplementation of augmented reality in physics education, at the school and the\nuniversity level. Our review includes an analysis of 96 papers from the Scopus\nand Eric databases, all of which were published between January 1st, 2012 and\nJanuary 1st, 2023. We evaluated how AR has been used for facilitating learning\nabout physics. Potential AR-based learning activities for different physics\ntopics have been summarized and opportunities, as well as challenges associated\nwith AR-based learning of physics have been reported. It has been shown that AR\ntechnologies may facilitate physics learning by: providing complementary\nvisualizations, optimizing cognitive load, allowing for haptic learning,\nreducing task completion time and promoting collaborative inquiry. The\npotential disadvantages of using AR in physics teaching are mainly related to\nthe shortcomings of software and hardware technologies (e.g., camera freeze,\nvisualization delay) and extraneous cognitive load (e.g., paying more attention\nto secondary details than to constructing target knowledge).",
            "author": [
                "A. Vidak",
                "I. Movre \u0160api\u0107",
                "V. Me\u0161i\u0107",
                "V. Gomzi"
            ],
            "link": [
                "http://dx.doi.org/10.1088/1361-6404/ad0e84",
                "http://arxiv.org/abs/2311.18392v1",
                "http://arxiv.org/pdf/2311.18392v1"
            ],
            "primary_category": "physics.ed-ph",
            "category": [
                "physics.ed-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18387v1",
            "title": "On Exact Inversion of DPM-Solvers",
            "updated": "2023-11-30T09:30:15Z",
            "published": "2023-11-30T09:30:15Z",
            "summary": "Diffusion probabilistic models (DPMs) are a key component in modern\ngenerative models. DPM-solvers have achieved reduced latency and enhanced\nquality significantly, but have posed challenges to find the exact inverse\n(i.e., finding the initial noise from the given image). Here we investigate the\nexact inversions for DPM-solvers and propose algorithms to perform them when\nsamples are generated by the first-order as well as higher-order DPM-solvers.\nFor each explicit denoising step in DPM-solvers, we formulated the inversions\nusing implicit methods such as gradient descent or forward step method to\nensure the robustness to large classifier-free guidance unlike the prior\napproach using fixed-point iteration. Experimental results demonstrated that\nour proposed exact inversion methods significantly reduced the error of both\nimage and noise reconstructions, greatly enhanced the ability to distinguish\ninvisible watermarks and well prevented unintended background changes\nconsistently during image editing. Project page:\n\\url{https://smhongok.github.io/inv-dpm.html}.",
            "author": [
                "Seongmin Hong",
                "Kyeonghyun Lee",
                "Suh Yoon Jeon",
                "Hyewon Bae",
                "Se Young Chun"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18387v1",
                "http://arxiv.org/pdf/2311.18387v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18377v1",
            "title": "Transfer Learning across Different Chemical Domains: Virtual Screening\n  of Organic Materials with Deep Learning Models Pretrained on Small Molecule\n  and Chemical Reaction Data",
            "updated": "2023-11-30T09:20:24Z",
            "published": "2023-11-30T09:20:24Z",
            "summary": "Machine learning prediction of organic materials properties is an efficient\nvirtual screening method ahead of more expensive screening methods. However,\nthis approach has suffered from insufficient labeled data on organic materials\nto train state-of-the-art machine learning models. In this study, we\ndemonstrate that drug-like small molecule and chemical reaction databases can\nbe used to pretrain the BERT model for the virtual screening of organic\nmaterials. Among the BERT models fine-tuned by five virtual screening tasks on\norganic materials, the USPTO-SMILES pretrained BERT model had R2 > 0.90 for two\ntasks and R2 > 0.82 for one, which was generally superior to the same models\npretrained by the small molecule or organic materials databases, as well as to\nthe other three traditional machine learning models trained directly on the\nvirtual screening task data. The superior performance of the USPTO-SMILES\npretrained BERT model is due to the greater variety of organic building blocks\nin the USPTO database and the broader coverage of the chemical space. The even\nbetter performance of the BERT model pretrained externally from a chemical\nreaction database with additional sources of chemical reactions strengthens our\nproof of concept that transfer learning across different chemical domains is\npractical for the virtual screening of organic materials.",
            "author": [
                "Chengwei Zhang",
                "Yushuang Zhai",
                "Ziyang Gong",
                "Yuan-Bin She",
                "Yun-Fang Yang",
                "An Su"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18377v1",
                "http://arxiv.org/pdf/2311.18377v1"
            ],
            "primary_category": "physics.chem-ph",
            "category": [
                "physics.chem-ph",
                "cs.LG",
                "q-bio.BM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18376v1",
            "title": "Age Effects on Decision-Making, Drift Diffusion Model",
            "updated": "2023-11-30T09:19:12Z",
            "published": "2023-11-30T09:19:12Z",
            "summary": "Training can improve human decision-making performance. After several\ntraining sessions, a person can quickly and accurately complete a task.\nHowever, decision-making is always a trade-off between accuracy and response\ntime. Factors such as age and drug abuse can affect the decision-making\nprocess. This study examines how training can improve the performance of\ndifferent age groups in completing a random dot motion (RDM) task. The\nparticipants are divided into two groups: old and young. They undergo a\nthree-phase training and then repeat the same RDM task. The hierarchical\ndrift-diffusion model analyzes the subjects' responses and determines how the\nmodel's parameters change after training for both age groups. The results show\nthat after training, the participants were able to accumulate sensory\ninformation faster, and the model drift rate increased. However, their decision\nboundary decreased as they became more confident and had a lower\ndecision-making threshold. Additionally, the old group had a higher boundary\nand lower drift rate in both pre and post-training, and there was less\ndifference between the two group parameters after training.",
            "author": [
                "Zahra Kavian",
                "Kimia Hajisadeghi",
                "Yashar Rezazadeh",
                "Mehrbod Faraji",
                "Reza Ebrahimpour"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18376v1",
                "http://arxiv.org/pdf/2311.18376v1"
            ],
            "primary_category": "q-bio.NC",
            "category": [
                "q-bio.NC",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18373v1",
            "title": "A Survey on Deep Learning for Polyp Segmentation: Techniques, Challenges\n  and Future Trends",
            "updated": "2023-11-30T09:14:37Z",
            "published": "2023-11-30T09:14:37Z",
            "summary": "Early detection and assessment of polyps play a crucial role in the\nprevention and treatment of colorectal cancer (CRC). Polyp segmentation\nprovides an effective solution to assist clinicians in accurately locating and\nsegmenting polyp regions. In the past, people often relied on manually\nextracted lower-level features such as color, texture, and shape, which often\nhad issues capturing global context and lacked robustness to complex scenarios.\nWith the advent of deep learning, more and more outstanding medical image\nsegmentation algorithms based on deep learning networks have emerged, making\nsignificant progress in this field. This paper provides a comprehensive review\nof polyp segmentation algorithms. We first review some traditional algorithms\nbased on manually extracted features and deep segmentation algorithms, then\ndetail benchmark datasets related to the topic. Specifically, we carry out a\ncomprehensive evaluation of recent deep learning models and results based on\npolyp sizes, considering the pain points of research topics and differences in\nnetwork structures. Finally, we discuss the challenges of polyp segmentation\nand future trends in this field. The models, benchmark datasets, and source\ncode links we collected are all published at\nhttps://github.com/taozh2017/Awesome-Polyp-Segmentation.",
            "author": [
                "Jiaxin Mei",
                "Tao Zhou",
                "Kaiwen Huang",
                "Yizhe Zhang",
                "Yi Zhou",
                "Ye Wu",
                "Huazhu Fu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18373v1",
                "http://arxiv.org/pdf/2311.18373v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18364v1",
            "title": "Hubness Reduction Improves Sentence-BERT Semantic Spaces",
            "updated": "2023-11-30T09:03:49Z",
            "published": "2023-11-30T09:03:49Z",
            "summary": "Semantic representations of text, i.e. representations of natural language\nwhich capture meaning by geometry, are essential for areas such as information\nretrieval and document grouping. High-dimensional trained dense vectors have\nreceived much attention in recent years as such representations. We investigate\nthe structure of semantic spaces that arise from embeddings made with\nSentence-BERT and find that the representations suffer from a well-known\nproblem in high dimensions called hubness. Hubness results in asymmetric\nneighborhood relations, such that some texts (the hubs) are neighbours of many\nother texts while most texts (so-called anti-hubs), are neighbours of few or no\nother texts. We quantify the semantic quality of the embeddings using hubness\nscores and error rate of a neighbourhood based classifier. We find that when\nhubness is high, we can reduce error rate and hubness using hubness reduction\nmethods. We identify a combination of two methods as resulting in the best\nreduction. For example, on one of the tested pretrained models, this combined\nmethod can reduce hubness by about 75% and error rate by about 9%. Thus, we\nargue that mitigating hubness in the embedding space provides better semantic\nrepresentations of text.",
            "author": [
                "Beatrix M. G. Nielsen",
                "Lars Kai Hansen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18364v1",
                "http://arxiv.org/pdf/2311.18364v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18358v1",
            "title": "TIDE: Test Time Few Shot Object Detection",
            "updated": "2023-11-30T09:00:44Z",
            "published": "2023-11-30T09:00:44Z",
            "summary": "Few-shot object detection (FSOD) aims to extract semantic knowledge from\nlimited object instances of novel categories within a target domain. Recent\nadvances in FSOD focus on fine-tuning the base model based on a few objects via\nmeta-learning or data augmentation. Despite their success, the majority of them\nare grounded with parametric readjustment to generalize on novel objects, which\nface considerable challenges in Industry 5.0, such as (i) a certain amount of\nfine-tuning time is required, and (ii) the parameters of the constructed model\nbeing unavailable due to the privilege protection, making the fine-tuning fail.\nSuch constraints naturally limit its application in scenarios with real-time\nconfiguration requirements or within black-box settings. To tackle the\nchallenges mentioned above, we formalize a novel FSOD task, referred to as Test\nTIme Few Shot DEtection (TIDE), where the model is un-tuned in the\nconfiguration procedure. To that end, we introduce an asymmetric architecture\nfor learning a support-instance-guided dynamic category classifier. Further, a\ncross-attention module and a multi-scale resizer are provided to enhance the\nmodel performance. Experimental results on multiple few-shot object detection\nplatforms reveal that the proposed TIDE significantly outperforms existing\ncontemporary methods. The implementation codes are available at\nhttps://github.com/deku-0621/TIDE",
            "author": [
                "Weikai Li",
                "Hongfeng Wei",
                "Yanlai Wu",
                "Jie Yang",
                "Yudi Ruan",
                "Yuan Li",
                "Ying Tang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18358v1",
                "http://arxiv.org/pdf/2311.18358v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18356v1",
            "title": "Towards Comparable Active Learning",
            "updated": "2023-11-30T08:54:32Z",
            "published": "2023-11-30T08:54:32Z",
            "summary": "Active Learning has received significant attention in the field of machine\nlearning for its potential in selecting the most informative samples for\nlabeling, thereby reducing data annotation costs. However, we show that the\nreported lifts in recent literature generalize poorly to other domains leading\nto an inconclusive landscape in Active Learning research. Furthermore, we\nhighlight overlooked problems for reproducing AL experiments that can lead to\nunfair comparisons and increased variance in the results. This paper addresses\nthese issues by providing an Active Learning framework for a fair comparison of\nalgorithms across different tasks and domains, as well as a fast and performant\noracle algorithm for evaluation. To the best of our knowledge, we propose the\nfirst AL benchmark that tests algorithms in 3 major domains: Tabular, Image,\nand Text. We report empirical results for 6 widely used algorithms on 7\nreal-world and 2 synthetic datasets and aggregate them into a domain-specific\nranking of AL algorithms.",
            "author": [
                "Thorben Werner",
                "Johannes Burchert",
                "Lars Schmidt-Thieme"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18356v1",
                "http://arxiv.org/pdf/2311.18356v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00090v1",
            "title": "Tree-based Forecasting of Day-ahead Solar Power Generation from Granular\n  Meteorological Features",
            "updated": "2023-11-30T08:47:37Z",
            "published": "2023-11-30T08:47:37Z",
            "summary": "Accurate forecasts for day-ahead photovoltaic (PV) power generation are\ncrucial to support a high PV penetration rate in the local electricity grid and\nto assure stability in the grid. We use state-of-the-art tree-based machine\nlearning methods to produce such forecasts and, unlike previous studies, we\nhereby account for (i) the effects various meteorological as well as\nastronomical features have on PV power production, and this (ii) at coarse as\nwell as granular spatial locations. To this end, we use data from Belgium and\nforecast day-ahead PV power production at an hourly resolution. The insights\nfrom our study can assist utilities, decision-makers, and other stakeholders in\noptimizing grid operations, economic dispatch, and in facilitating the\nintegration of distributed PV power into the electricity grid.",
            "author": [
                "Nick Berlanger",
                "Noah van Ophoven",
                "Tim Verdonck",
                "Ines Wilms"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00090v1",
                "http://arxiv.org/pdf/2312.00090v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18355v1",
            "title": "Guided Demonstrations Using Automated Excuse Generation",
            "updated": "2023-11-30T08:45:19Z",
            "published": "2023-11-30T08:45:19Z",
            "summary": "Teaching task-level directives to robots via demonstration is a popular tool\nto expand the robot's capabilities to interact with its environment. While\ncurrent learning from demonstration systems primarily focuses on abstracting\nthe task-level knowledge to the robot, these systems lack the ability to\nunderstand which part of the task can be already solved given the robot's prior\nknowledge. Therefore, instead of only requiring demonstrations of the missing\npieces, these systems will require a demonstration of the complete task, which\nis cumbersome, repetitive, and can discourage people from helping the robot by\nperforming the demonstrations. Therefore, we propose to use the notion of\n\"excuses\" to identify the smallest change in the robot state that makes a task,\ncurrently not solvable by the robot, solvable -- as a means to solicit more\ntargeted demonstrations from a human. These excuses are generated automatically\nusing combinatorial search over possible changes that can be made to the\nrobot's state and choosing the minimum changes that make it solvable. These\nexcuses then serve as guidance for the demonstrator who can use it to decide\nwhat to demonstrate to the robot in order to make this requested change\npossible, thereby making the original task solvable for the robot without\nhaving to demonstrate it in its entirety. By working with symbolic state\ndescriptions, the excuses can be directly communicated and intuitively\nunderstood by a human demonstrator. We show empirically and in a user study\nthat the use of excuses reduces the demonstration time by 54% and leads to a\n74% reduction in demonstration size.",
            "author": [
                "Maximilian Diehl",
                "Tathagata Chakraborti",
                "Karinne Ramirez-Amaro"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18355v1",
                "http://arxiv.org/pdf/2311.18355v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18352v1",
            "title": "URLLC-Awared Resource Allocation for Heterogeneous Vehicular Edge\n  Computing",
            "updated": "2023-11-30T08:44:46Z",
            "published": "2023-11-30T08:44:46Z",
            "summary": "Vehicular edge computing (VEC) is a promising technology to support real-time\nvehicular applications, where vehicles offload intensive computation tasks to\nthe nearby VEC server for processing. However, the traditional VEC that relies\non single communication technology cannot well meet the communication\nrequirement for task offloading, thus the heterogeneous VEC integrating the\nadvantages of dedicated short-range communications (DSRC), millimeter-wave\n(mmWave) and cellular-based vehicle to infrastructure (C-V2I) is introduced to\nenhance the communication capacity. The communication resource allocation and\ncomputation resource allocation may significantly impact on the ultra-reliable\nlow-latency communication (URLLC) performance and the VEC system utility, in\nthis case, how to do the resource allocations is becoming necessary. In this\npaper, we consider a heterogeneous VEC with multiple communication technologies\nand various types of tasks, and propose an effective resource allocation policy\nto minimize the system utility while satisfying the URLLC requirement. We first\nformulate an optimization problem to minimize the system utility under the\nURLLC constraint which modeled by the moment generating function (MGF)-based\nstochastic network calculus (SNC), then we present a Lyapunov-guided deep\nreinforcement learning (DRL) method to convert and solve the optimization\nproblem. Extensive simulation experiments illustrate that the proposed resource\nallocation approach is effective.",
            "author": [
                "Qiong Wu",
                "Wenhua Wang",
                "Pingyi Fan",
                "Qiang Fan",
                "Jiangzhou Wang",
                "Khaled B. Letaief"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18352v1",
                "http://arxiv.org/pdf/2311.18352v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18350v1",
            "title": "Unveiling Backdoor Risks Brought by Foundation Models in Heterogeneous\n  Federated Learning",
            "updated": "2023-11-30T08:42:32Z",
            "published": "2023-11-30T08:42:32Z",
            "summary": "The foundation models (FMs) have been used to generate synthetic public\ndatasets for the heterogeneous federated learning (HFL) problem where each\nclient uses a unique model architecture. However, the vulnerabilities of\nintegrating FMs, especially against backdoor attacks, are not well-explored in\nthe HFL contexts. In this paper, we introduce a novel backdoor attack mechanism\nfor HFL that circumvents the need for client compromise or ongoing\nparticipation in the FL process. This method plants and transfers the backdoor\nthrough a generated synthetic public dataset, which could help evade existing\nbackdoor defenses in FL by presenting normal client behaviors. Empirical\nexperiments across different HFL configurations and benchmark datasets\ndemonstrate the effectiveness of our attack compared to traditional\nclient-based attacks. Our findings reveal significant security risks in\ndeveloping robust FM-assisted HFL systems. This research contributes to\nenhancing the safety and integrity of FL systems, highlighting the need for\nadvanced security measures in the era of FMs.",
            "author": [
                "Xi Li",
                "Chen Wu",
                "Jiaqi Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18350v1",
                "http://arxiv.org/pdf/2311.18350v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18348v1",
            "title": "Reconstructing Historical Climate Fields With Deep Learning",
            "updated": "2023-11-30T08:34:12Z",
            "published": "2023-11-30T08:34:12Z",
            "summary": "Historical records of climate fields are often sparse due to missing\nmeasurements, especially before the introduction of large-scale satellite\nmissions. Several statistical and model-based methods have been introduced to\nfill gaps and reconstruct historical records. Here, we employ a recently\nintroduced deep-learning approach based on Fourier convolutions, trained on\nnumerical climate model output, to reconstruct historical climate fields. Using\nthis approach we are able to realistically reconstruct large and irregular\nareas of missing data, as well as reconstruct known historical events such as\nstrong El Ni\\~no and La Ni\\~na with very little given information. Our method\noutperforms the widely used statistical kriging method as well as other recent\nmachine learning approaches. The model generalizes to higher resolutions than\nthe ones it was trained on and can be used on a variety of climate fields.\nMoreover, it allows inpainting of masks never seen before during the model\ntraining.",
            "author": [
                "Nils Bochow",
                "Anna Poltronieri",
                "Martin Rypdal",
                "Niklas Boers"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18348v1",
                "http://arxiv.org/pdf/2311.18348v1"
            ],
            "primary_category": "physics.geo-ph",
            "category": [
                "physics.geo-ph",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18346v1",
            "title": "Efficient Model-Based Concave Utility Reinforcement Learning through\n  Greedy Mirror Descent",
            "updated": "2023-11-30T08:32:50Z",
            "published": "2023-11-30T08:32:50Z",
            "summary": "Many machine learning tasks can be solved by minimizing a convex function of\nan occupancy measure over the policies that generate them. These include\nreinforcement learning, imitation learning, among others. This more general\nparadigm is called the Concave Utility Reinforcement Learning problem (CURL).\nSince CURL invalidates classical Bellman equations, it requires new algorithms.\nWe introduce MD-CURL, a new algorithm for CURL in a finite horizon Markov\ndecision process. MD-CURL is inspired by mirror descent and uses a non-standard\nregularization to achieve convergence guarantees and a simple closed-form\nsolution, eliminating the need for computationally expensive projection steps\ntypically found in mirror descent approaches. We then extend CURL to an online\nlearning scenario and present Greedy MD-CURL, a new method adapting MD-CURL to\nan online, episode-based setting with partially unknown dynamics. Like MD-CURL,\nthe online version Greedy MD-CURL benefits from low computational complexity,\nwhile guaranteeing sub-linear or even logarithmic regret, depending on the\nlevel of information available on the underlying dynamics.",
            "author": [
                "Bianca Marin Moreno",
                "Margaux Br\u00e9g\u00e8re",
                "Pierre Gaillard",
                "Nadia Oudjane"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18346v1",
                "http://arxiv.org/pdf/2311.18346v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "physics.data-an",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18345v1",
            "title": "Situating the social issues of image generation models in the model life\n  cycle: a sociotechnical approach",
            "updated": "2023-11-30T08:32:32Z",
            "published": "2023-11-30T08:32:32Z",
            "summary": "The race to develop image generation models is intensifying, with a rapid\nincrease in the number of text-to-image models available. This is coupled with\ngrowing public awareness of these technologies. Though other generative AI\nmodels--notably, large language models--have received recent critical attention\nfor the social and other non-technical issues they raise, there has been\nrelatively little comparable examination of image generation models. This paper\nreports on a novel, comprehensive categorization of the social issues\nassociated with image generation models. At the intersection of machine\nlearning and the social sciences, we report the results of a survey of the\nliterature, identifying seven issue clusters arising from image generation\nmodels: data issues, intellectual property, bias, privacy, and the impacts on\nthe informational, cultural, and natural environments. We situate these social\nissues in the model life cycle, to aid in considering where potential issues\narise, and mitigation may be needed. We then compare these issue clusters with\nwhat has been reported for large language models. Ultimately, we argue that the\nrisks posed by image generation models are comparable in severity to the risks\nposed by large language models, and that the social impact of image generation\nmodels must be urgently considered.",
            "author": [
                "Amelia Katirai",
                "Noa Garcia",
                "Kazuki Ide",
                "Yuta Nakashima",
                "Atsuo Kishimoto"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18345v1",
                "http://arxiv.org/pdf/2311.18345v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18341v2",
            "title": "Learning Robust Precipitation Forecaster by Temporal Frame Interpolation",
            "updated": "2023-12-01T16:46:10Z",
            "published": "2023-11-30T08:22:08Z",
            "summary": "Recent advances in deep learning have significantly elevated weather\nprediction models. However, these models often falter in real-world scenarios\ndue to their sensitivity to spatial-temporal shifts. This issue is particularly\nacute in weather forecasting, where models are prone to overfit to local and\ntemporal variations, especially when tasked with fine-grained predictions. In\nthis paper, we address these challenges by developing a robust precipitation\nforecasting model that demonstrates resilience against such spatial-temporal\ndiscrepancies. We introduce Temporal Frame Interpolation (TFI), a novel\ntechnique that enhances the training dataset by generating synthetic samples\nthrough interpolating adjacent frames from satellite imagery and ground radar\ndata, thus improving the model's robustness against frame noise. Moreover, we\nincorporate a unique Multi-Level Dice (ML-Dice) loss function, leveraging the\nordinal nature of rainfall intensities to improve the model's performance. Our\napproach has led to significant improvements in forecasting precision,\nculminating in our model securing \\textit{1st place} in the transfer learning\nleaderboard of the \\textit{Weather4cast'23} competition. This achievement not\nonly underscores the effectiveness of our methodologies but also establishes a\nnew standard for deep learning applications in weather forecasting. Our code\nand weights have been public on \\url{https://github.com/Secilia-Cxy/UNetTFI}.",
            "author": [
                "Lu Han",
                "Xu-Yang Chen",
                "Han-Jia Ye",
                "De-Chuan Zhan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18341v2",
                "http://arxiv.org/pdf/2311.18341v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "physics.ao-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18340v1",
            "title": "Neuromorphic Incremental on-chip Learning with Hebbian Weight\n  Consolidation",
            "updated": "2023-11-30T08:19:03Z",
            "published": "2023-11-30T08:19:03Z",
            "summary": "As next-generation implantable brain-machine interfaces become pervasive on\nedge device, incrementally learning new tasks in bio-plasticity ways is\nurgently demanded for Neuromorphic chips. Due to the inherent characteristics\nof its structure, spiking neural networks are naturally well-suited for\nBMI-chips. Here we propose Hebbian Weight Consolidation, as well as an on-chip\nlearning framework. HWC selectively masks synapse modifications for previous\ntasks, retaining them to store new knowledge from subsequent tasks while\npreserving the old knowledge. Leveraging the bio-plasticity of dendritic\nspines, the intrinsic self-organizing nature of Hebbian Weight Consolidation\naligns naturally with the incremental learning paradigm, facilitating robust\nlearning outcomes. By reading out spikes layer by layer and performing\nback-propagation on the external micro-controller unit, MLoC can efficiently\naccomplish on-chip learning. Experiments show that our HWC algorithm up to\n23.19% outperforms lower bound that without incremental learning algorithm,\nparticularly in more challenging monkey behavior decoding scenarios. Taking\ninto account on-chip computing on Synsense Speck 2e chip, our proposed\nalgorithm exhibits an improvement of 11.06%. This study demonstrates the\nfeasibility of employing incremental learning for high-performance neural\nsignal decoding in next-generation brain-machine interfaces.",
            "author": [
                "Zifan Ning",
                "Chaojin Chen",
                "Xiang Cheng",
                "Wangzi Yao",
                "Tielin Zhang",
                "Bo Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18340v1",
                "http://arxiv.org/pdf/2311.18340v1"
            ],
            "primary_category": "cs.ET",
            "category": [
                "cs.ET"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18332v1",
            "title": "Multilevel Saliency-Guided Self-Supervised Learning for Image Anomaly\n  Detection",
            "updated": "2023-11-30T08:03:53Z",
            "published": "2023-11-30T08:03:53Z",
            "summary": "Anomaly detection (AD) is a fundamental task in computer vision. It aims to\nidentify incorrect image data patterns which deviate from the normal ones.\nConventional methods generally address AD by preparing augmented negative\nsamples to enforce self-supervised learning. However, these techniques\ntypically do not consider semantics during augmentation, leading to the\ngeneration of unrealistic or invalid negative samples. Consequently, the\nfeature extraction network can be hindered from embedding critical features. In\nthis study, inspired by visual attention learning approaches, we propose\nCutSwap, which leverages saliency guidance to incorporate semantic cues for\naugmentation. Specifically, we first employ LayerCAM to extract multilevel\nimage features as saliency maps and then perform clustering to obtain multiple\ncentroids. To fully exploit saliency guidance, on each map, we select a pixel\npair from the cluster with the highest centroid saliency to form a patch pair.\nSuch a patch pair includes highly similar context information with dense\nsemantic correlations. The resulting negative sample is created by swapping the\nlocations of the patch pair. Compared to prior augmentation methods, CutSwap\ngenerates more subtle yet realistic negative samples to facilitate quality\nfeature learning. Extensive experimental and ablative evaluations demonstrate\nthat our method achieves state-of-the-art AD performance on two mainstream AD\nbenchmark datasets.",
            "author": [
                "Jianjian Qin",
                "Chunzhi Gu",
                "Jun Yu",
                "Chao Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18332v1",
                "http://arxiv.org/pdf/2311.18332v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18331v1",
            "title": "MRFP: Learning Generalizable Semantic Segmentation from Sim-2-Real with\n  Multi-Resolution Feature Perturbation",
            "updated": "2023-11-30T08:02:49Z",
            "published": "2023-11-30T08:02:49Z",
            "summary": "Deep neural networks have shown exemplary performance on semantic scene\nunderstanding tasks on source domains, but due to the absence of style\ndiversity during training, enhancing performance on unseen target domains using\nonly single source domain data remains a challenging task. Generation of\nsimulated data is a feasible alternative to retrieving large style-diverse\nreal-world datasets as it is a cumbersome and budget-intensive process.\nHowever, the large domain-specific inconsistencies between simulated and\nreal-world data pose a significant generalization challenge in semantic\nsegmentation. In this work, to alleviate this problem, we propose a novel\nMultiResolution Feature Perturbation (MRFP) technique to randomize\ndomain-specific fine-grained features and perturb style of coarse features. Our\nexperimental results on various urban-scene segmentation datasets clearly\nindicate that, along with the perturbation of style-information, perturbation\nof fine-feature components is paramount to learn domain invariant robust\nfeature maps for semantic segmentation models. MRFP is a simple and\ncomputationally efficient, transferable module with no additional learnable\nparameters or objective functions, that helps state-of-the-art deep neural\nnetworks to learn robust domain invariant features for simulation-to-real\nsemantic segmentation.",
            "author": [
                "Sumanth Udupa",
                "Prajwal Gurunath",
                "Aniruddh Sikdar",
                "Suresh Sundaram"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18331v1",
                "http://arxiv.org/pdf/2311.18331v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18327v1",
            "title": "Deep Reinforcement Learning Based Optimal Energy Management of\n  Multi-energy Microgrids with Uncertainties",
            "updated": "2023-11-30T07:58:39Z",
            "published": "2023-11-30T07:58:39Z",
            "summary": "Multi-energy microgrid (MEMG) offers an effective approach to deal with\nenergy demand diversification and new energy consumption on the consumer side.\nIn MEMG, it is critical to deploy an energy management system (EMS) for\nefficient utilization of energy and reliable operation of the system. To help\nEMS formulate optimal dispatching schemes, a deep reinforcement learning\n(DRL)-based MEMG energy management scheme with renewable energy source (RES)\nuncertainty is proposed in this paper. To accurately describe the operating\nstate of the MEMG, the off-design performance model of energy conversion\ndevices is considered in scheduling. The nonlinear optimal dispatching model is\nexpressed as a Markov decision process (MDP) and is then addressed by the twin\ndelayed deep deterministic policy gradient (TD3) algorithm. In addition, to\naccurately describe the uncertainty of RES, the conditional-least squares\ngenerative adversarial networks (C-LSGANs) method based on RES forecast power\nis proposed to construct the scenarios set of RES power generation. The\ngenerated data of RES is used for scheduling to obtain caps and floors for the\npurchase of electricity and natural gas. Based on this, the superior energy\nsupply sector can formulate solutions in advance to tackle the uncertainty of\nRES. Finally, the simulation analysis demonstrates the validity and superiority\nof the method.",
            "author": [
                "Yang Cui",
                "Yang Xu",
                "Yang Li",
                "Yijian Wang",
                "Xinpeng Zou"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18327v1",
                "http://arxiv.org/pdf/2311.18327v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18326v1",
            "title": "AESTRA: Deep Learning for Precise Radial Velocity Estimation in the\n  Presence of Stellar Activity",
            "updated": "2023-11-30T07:57:02Z",
            "published": "2023-11-30T07:57:02Z",
            "summary": "Stellar activity interferes with precise radial velocity measurements and\nlimits our ability to detect and characterize planets, particularly Earth-like\nplanets. We introduce \\aestra (Auto-Encoding STellar Radial-velocity and\nActivity), a deep learning method for precise radial velocity measurements. It\ncombines a spectrum auto-encoder, which learns to create realistic models of\nthe star's rest-frame spectrum, and a radial-velocity estimator, which learns\nto identify true Doppler shifts in the presence of spurious shifts due to\nline-profile variations. Being self-supervised, \\aestra does not need \"ground\ntruth\" radial velocities for training, making it applicable to exoplanet host\nstars for which the truth is unknown. In tests involving 1,000 simulated\nspectra, \\aestra can detect planetary signals as low as 0.1 m/s even in the\npresence of 3 m/s of activity-induced noise and 0.3 m/s of photon noise per\nspectrum.",
            "author": [
                "Yan Liang",
                "Joshua N. Winn",
                "Peter Melchior"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18326v1",
                "http://arxiv.org/pdf/2311.18326v1"
            ],
            "primary_category": "astro-ph.EP",
            "category": [
                "astro-ph.EP",
                "astro-ph.IM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00088v1",
            "title": "Anomaly Detection via Learning-Based Sequential Controlled Sensing",
            "updated": "2023-11-30T07:49:33Z",
            "published": "2023-11-30T07:49:33Z",
            "summary": "In this paper, we address the problem of detecting anomalies among a given\nset of binary processes via learning-based controlled sensing. Each process is\nparameterized by a binary random variable indicating whether the process is\nanomalous. To identify the anomalies, the decision-making agent is allowed to\nobserve a subset of the processes at each time instant. Also, probing each\nprocess has an associated cost. Our objective is to design a sequential\nselection policy that dynamically determines which processes to observe at each\ntime with the goal to minimize the delay in making the decision and the total\nsensing cost. We cast this problem as a sequential hypothesis testing problem\nwithin the framework of Markov decision processes. This formulation utilizes\nboth a Bayesian log-likelihood ratio-based reward and an entropy-based reward.\nThe problem is then solved using two approaches: 1) a deep reinforcement\nlearning-based approach where we design both deep Q-learning and policy\ngradient actor-critic algorithms; and 2) a deep active inference-based\napproach. Using numerical experiments, we demonstrate the efficacy of our\nalgorithms and show that our algorithms adapt to any unknown statistical\ndependence pattern of the processes.",
            "author": [
                "Geethu Joseph",
                "Chen Zhong",
                "M. Cenk Gursoy",
                "Senem Velipasalar",
                "Pramod K. Varshney"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00088v1",
                "http://arxiv.org/pdf/2312.00088v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.SY",
                "eess.SP",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18316v1",
            "title": "Learning for Semantic Knowledge Base-Guided Online Feature Transmission\n  in Dynamic Channels",
            "updated": "2023-11-30T07:35:56Z",
            "published": "2023-11-30T07:35:56Z",
            "summary": "With the proliferation of edge computing, efficient AI inference on edge\ndevices has become essential for intelligent applications such as autonomous\nvehicles and VR/AR. In this context, we address the problem of efficient remote\nobject recognition by optimizing feature transmission between mobile devices\nand edge servers. We propose an online optimization framework to address the\nchallenge of dynamic channel conditions and device mobility in an end-to-end\ncommunication system. Our approach builds upon existing methods by leveraging a\nsemantic knowledge base to drive multi-level feature transmission, accounting\nfor temporal factors and dynamic elements throughout the transmission process.\nTo solve the online optimization problem, we design a novel soft\nactor-critic-based deep reinforcement learning system with a carefully designed\nreward function for real-time decision-making, overcoming the optimization\ndifficulty of the NP-hard problem and achieving the minimization of semantic\nloss while respecting latency constraints. Numerical results showcase the\nsuperiority of our approach compared to traditional greedy methods under\nvarious system setups.",
            "author": [
                "Xiangyu Gao",
                "Yaping Sun",
                "Dongyu Wei",
                "Xiaodong Xu",
                "Hao Chen",
                "Hao Yin",
                "Shuguang Cui"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18316v1",
                "http://arxiv.org/pdf/2311.18316v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "cs.LG",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18313v1",
            "title": "Automatic Implementation of Neural Networks through Reaction Networks --\n  Part I: Circuit Design and Convergence Analysis",
            "updated": "2023-11-30T07:31:36Z",
            "published": "2023-11-30T07:31:36Z",
            "summary": "Information processing relying on biochemical interactions in the cellular\nenvironment is essential for biological organisms. The implementation of\nmolecular computational systems holds significant interest and potential in the\nfields of synthetic biology and molecular computation. This two-part article\naims to introduce a programmable biochemical reaction network (BCRN) system\nendowed with mass action kinetics that realizes the fully connected neural\nnetwork (FCNN) and has the potential to act automatically in vivo. In part I,\nthe feedforward propagation computation, the backpropagation component, and all\nbridging processes of FCNN are ingeniously designed as specific BCRN modules\nbased on their dynamics. This approach addresses a design gap in the\nbiochemical assignment module and judgment termination module and provides a\nnovel precise and robust realization of bi-molecular reactions for the learning\nprocess. Through equilibrium approaching, we demonstrate that the designed BCRN\nsystem achieves FCNN functionality with exponential convergence to target\ncomputational results, thereby enhancing the theoretical support for such work.\nFinally, the performance of this construction is further evaluated on two\ntypical logic classification problems.",
            "author": [
                "Yuzhen Fan",
                "Xiaoyu Zhang",
                "Chuanhou Gao",
                "Denis Dochain"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18313v1",
                "http://arxiv.org/pdf/2311.18313v1"
            ],
            "primary_category": "math.DS",
            "category": [
                "math.DS",
                "cs.LG",
                "cs.NE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18311v1",
            "title": "Anisotropic Neural Representation Learning for High-Quality Neural\n  Rendering",
            "updated": "2023-11-30T07:29:30Z",
            "published": "2023-11-30T07:29:30Z",
            "summary": "Neural radiance fields (NeRFs) have achieved impressive view synthesis\nresults by learning an implicit volumetric representation from multi-view\nimages. To project the implicit representation into an image, NeRF employs\nvolume rendering that approximates the continuous integrals of rays as an\naccumulation of the colors and densities of the sampled points. Although this\napproximation enables efficient rendering, it ignores the direction information\nin point intervals, resulting in ambiguous features and limited reconstruction\nquality. In this paper, we propose an anisotropic neural representation\nlearning method that utilizes learnable view-dependent features to improve\nscene representation and reconstruction. We model the volumetric function as\nspherical harmonic (SH)-guided anisotropic features, parameterized by\nmultilayer perceptrons, facilitating ambiguity elimination while preserving the\nrendering efficiency. To achieve robust scene reconstruction without anisotropy\noverfitting, we regularize the energy of the anisotropic features during\ntraining. Our method is flexiable and can be plugged into NeRF-based\nframeworks. Extensive experiments show that the proposed representation can\nboost the rendering quality of various NeRFs and achieve state-of-the-art\nrendering performance on both synthetic and real-world scenes.",
            "author": [
                "Y. Wang",
                "J. Xu",
                "Y. Zeng",
                "Y. Gong"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18311v1",
                "http://arxiv.org/pdf/2311.18311v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00087v1",
            "title": "Generative Artificial Intelligence in Learning Analytics:\n  Contextualising Opportunities and Challenges through the Learning Analytics\n  Cycle",
            "updated": "2023-11-30T07:25:34Z",
            "published": "2023-11-30T07:25:34Z",
            "summary": "Generative artificial intelligence (GenAI), exemplified by ChatGPT,\nMidjourney, and other state-of-the-art large language models and diffusion\nmodels, holds significant potential for transforming education and enhancing\nhuman productivity. While the prevalence of GenAI in education has motivated\nnumerous research initiatives, integrating these technologies within the\nlearning analytics (LA) cycle and their implications for practical\ninterventions remain underexplored. This paper delves into the prospective\nopportunities and challenges GenAI poses for advancing LA. We present a concise\noverview of the current GenAI landscape and contextualise its potential roles\nwithin Clow's generic framework of the LA cycle. We posit that GenAI can play\npivotal roles in analysing unstructured data, generating synthetic learner\ndata, enriching multimodal learner interactions, advancing interactive and\nexplanatory analytics, and facilitating personalisation and adaptive\ninterventions. As the lines blur between learners and GenAI tools, a renewed\nunderstanding of learners is needed. Future research can delve deep into\nframeworks and methodologies that advocate for human-AI collaboration. The LA\ncommunity can play a pivotal role in capturing data about human and AI\ncontributions and exploring how they can collaborate most effectively. As LA\nadvances, it is essential to consider the pedagogical implications and broader\nsocioeconomic impact of GenAI for ensuring an inclusive future.",
            "author": [
                "Lixiang Yan",
                "Roberto Martinez-Maldonado",
                "Dragan Ga\u0161evi\u0107"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00087v1",
                "http://arxiv.org/pdf/2312.00087v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY",
                "cs.AI",
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18307v1",
            "title": "Categorical Traffic Transformer: Interpretable and Diverse Behavior\n  Prediction with Tokenized Latent",
            "updated": "2023-11-30T07:25:24Z",
            "published": "2023-11-30T07:25:24Z",
            "summary": "Adept traffic models are critical to both planning and closed-loop simulation\nfor autonomous vehicles (AV), and key design objectives include accuracy,\ndiverse multimodal behaviors, interpretability, and downstream compatibility.\nRecently, with the advent of large language models (LLMs), an additional\ndesirable feature for traffic models is LLM compatibility. We present\nCategorical Traffic Transformer (CTT), a traffic model that outputs both\ncontinuous trajectory predictions and tokenized categorical predictions (lane\nmodes, homotopies, etc.). The most outstanding feature of CTT is its fully\ninterpretable latent space, which enables direct supervision of the latent\nvariable from the ground truth during training and avoids mode collapse\ncompletely. As a result, CTT can generate diverse behaviors conditioned on\ndifferent latent modes with semantic meanings while beating SOTA on prediction\naccuracy. In addition, CTT's ability to input and output tokens enables\nintegration with LLMs for common-sense reasoning and zero-shot generalization.",
            "author": [
                "Yuxiao Chen",
                "Sander Tonkens",
                "Marco Pavone"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18307v1",
                "http://arxiv.org/pdf/2311.18307v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18306v1",
            "title": "PAUNet: Precipitation Attention-based U-Net for rain prediction from\n  satellite radiance data",
            "updated": "2023-11-30T07:22:55Z",
            "published": "2023-11-30T07:22:55Z",
            "summary": "This paper introduces Precipitation Attention-based U-Net (PAUNet), a deep\nlearning architecture for predicting precipitation from satellite radiance\ndata, addressing the challenges of the Weather4cast 2023 competition. PAUNet is\na variant of U-Net and Res-Net, designed to effectively capture the large-scale\ncontextual information of multi-band satellite images in visible, water vapor,\nand infrared bands through encoder convolutional layers with center cropping\nand attention mechanisms. We built upon the Focal Precipitation Loss including\nan exponential component (e-FPL), which further enhanced the importance across\ndifferent precipitation categories, particularly medium and heavy rain. Trained\non a substantial dataset from various European regions, PAUNet demonstrates\nnotable accuracy with a higher Critical Success Index (CSI) score than the\nbaseline model in predicting rainfall over multiple time slots. PAUNet's\narchitecture and training methodology showcase improvements in precipitation\nforecasting, crucial for sectors like emergency services and retail and supply\nchain management.",
            "author": [
                "P. Jyoteeshkumar Reddy",
                "Harish Baki",
                "Sandeep Chinta",
                "Richard Matear",
                "John Taylor"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18306v1",
                "http://arxiv.org/pdf/2311.18306v1"
            ],
            "primary_category": "physics.ao-ph",
            "category": [
                "physics.ao-ph",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00084v1",
            "title": "Can Protective Perturbation Safeguard Personal Data from Being Exploited\n  by Stable Diffusion?",
            "updated": "2023-11-30T07:17:43Z",
            "published": "2023-11-30T07:17:43Z",
            "summary": "Stable Diffusion has established itself as a foundation model in generative\nAI artistic applications, receiving widespread research and application. Some\nrecent fine-tuning methods have made it feasible for individuals to implant\npersonalized concepts onto the basic Stable Diffusion model with minimal\ncomputational costs on small datasets. However, these innovations have also\ngiven rise to issues like facial privacy forgery and artistic copyright\ninfringement. In recent studies, researchers have explored the addition of\nimperceptible adversarial perturbations to images to prevent potential\nunauthorized exploitation and infringements when personal data is used for\nfine-tuning Stable Diffusion. Although these studies have demonstrated the\nability to protect images, it is essential to consider that these methods may\nnot be entirely applicable in real-world scenarios. In this paper, we\nsystematically evaluate the use of perturbations to protect images within a\npractical threat model. The results suggest that these approaches may not be\nsufficient to safeguard image privacy and copyright effectively. Furthermore,\nwe introduce a purification method capable of removing protected perturbations\nwhile preserving the original image structure to the greatest extent possible.\nExperiments reveal that Stable Diffusion can effectively learn from purified\nimages over all protective methods.",
            "author": [
                "Zhengyue Zhao",
                "Jinhao Duan",
                "Kaidi Xu",
                "Chenan Wang",
                "Rui Zhangp Zidong Dup Qi Guo",
                "Xing Hu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00084v1",
                "http://arxiv.org/pdf/2312.00084v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00083v1",
            "title": "BAM-DETR: Boundary-Aligned Moment Detection Transformer for Temporal\n  Sentence Grounding in Videos",
            "updated": "2023-11-30T07:16:11Z",
            "published": "2023-11-30T07:16:11Z",
            "summary": "Temporal sentence grounding aims to localize moments relevant to a language\ndescription. Recently, DETR-like approaches have shown notable progress by\ndecoding the center and length of a target moment from learnable queries.\nHowever, they suffer from the issue of center misalignment raised by the\ninherent ambiguity of moment centers, leading to inaccurate predictions. To\nremedy this problem, we introduce a novel boundary-oriented moment formulation.\nIn our paradigm, the model no longer needs to find the precise center but\ninstead suffices to predict any anchor point within the interval, from which\nthe onset and offset are directly estimated. Based on this idea, we design a\nBoundary-Aligned Moment Detection Transformer (BAM-DETR), equipped with a\ndual-pathway decoding process. Specifically, it refines the anchor and\nboundaries within parallel pathways using global and boundary-focused\nattention, respectively. This separate design allows the model to focus on\ndesirable regions, enabling precise refinement of moment predictions. Further,\nwe propose a quality-based ranking method, ensuring that proposals with high\nlocalization qualities are prioritized over incomplete ones. Extensive\nexperiments verify the advantages of our methods, where our model records new\nstate-of-the-art results on three benchmarks. Code is at\nhttps://github.com/Pilhyeon/BAM-DETR.",
            "author": [
                "Pilhyeon Lee",
                "Hyeran Byun"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00083v1",
                "http://arxiv.org/pdf/2312.00083v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18303v1",
            "title": "OmniMotionGPT: Animal Motion Generation with Limited Data",
            "updated": "2023-11-30T07:14:00Z",
            "published": "2023-11-30T07:14:00Z",
            "summary": "Our paper aims to generate diverse and realistic animal motion sequences from\ntextual descriptions, without a large-scale animal text-motion dataset. While\nthe task of text-driven human motion synthesis is already extensively studied\nand benchmarked, it remains challenging to transfer this success to other\nskeleton structures with limited data. In this work, we design a model\narchitecture that imitates Generative Pretraining Transformer (GPT), utilizing\nprior knowledge learned from human data to the animal domain. We jointly train\nmotion autoencoders for both animal and human motions and at the same time\noptimize through the similarity scores among human motion encoding, animal\nmotion encoding, and text CLIP embedding. Presenting the first solution to this\nproblem, we are able to generate animal motions with high diversity and\nfidelity, quantitatively and qualitatively outperforming the results of\ntraining human motion generation baselines on animal data. Additionally, we\nintroduce AnimalML3D, the first text-animal motion dataset with 1240 animation\nsequences spanning 36 different animal identities. We hope this dataset would\nmediate the data scarcity problem in text-driven animal motion generation,\nproviding a new playground for the research community.",
            "author": [
                "Zhangsihao Yang",
                "Mingyuan Zhou",
                "Mengyi Shan",
                "Bingbing Wen",
                "Ziwei Xuan",
                "Mitch Hill",
                "Junjie Bai",
                "Guo-Jun Qi",
                "Yalin Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18303v1",
                "http://arxiv.org/pdf/2311.18303v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18300v1",
            "title": "Multi-label Annotation for Visual Multi-Task Learning Models",
            "updated": "2023-11-30T07:10:13Z",
            "published": "2023-11-30T07:10:13Z",
            "summary": "Deep learning requires large amounts of data, and a well-defined pipeline for\nlabeling and augmentation. Current solutions support numerous computer vision\ntasks with dedicated annotation types and formats, such as bounding boxes,\npolygons, and key points. These annotations can be combined into a single data\nformat to benefit approaches such as multi-task models. However, to our\nknowledge, no available labeling tool supports the export functionality for a\ncombined benchmark format, and no augmentation library supports transformations\nfor the combination of all. In this work, these functionalities are presented,\nwith visual data annotation and augmentation to train a multi-task model\n(object detection, segmentation, and key point extraction). The tools are\ndemonstrated in two robot perception use cases.",
            "author": [
                "G. Sharma",
                "A. Angleraud",
                "R. Pieters"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18300v1",
                "http://arxiv.org/pdf/2311.18300v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18296v1",
            "title": "Perceptual Group Tokenizer: Building Perception with Iterative Grouping",
            "updated": "2023-11-30T07:00:14Z",
            "published": "2023-11-30T07:00:14Z",
            "summary": "Human visual recognition system shows astonishing capability of compressing\nvisual information into a set of tokens containing rich representations without\nlabel supervision. One critical driving principle behind it is perceptual\ngrouping. Despite being widely used in computer vision in the early 2010s, it\nremains a mystery whether perceptual grouping can be leveraged to derive a\nneural visual recognition backbone that generates as powerful representations.\nIn this paper, we propose the Perceptual Group Tokenizer, a model that entirely\nrelies on grouping operations to extract visual features and perform\nself-supervised representation learning, where a series of grouping operations\nare used to iteratively hypothesize the context for pixels or superpixels to\nrefine feature representations. We show that the proposed model can achieve\ncompetitive performance compared to state-of-the-art vision architectures, and\ninherits desirable properties including adaptive computation without\nre-training, and interpretability. Specifically, Perceptual Group Tokenizer\nachieves 80.3% on ImageNet-1K self-supervised learning benchmark with linear\nprobe evaluation, marking a new progress under this paradigm.",
            "author": [
                "Zhiwei Deng",
                "Ting Chen",
                "Yang Li"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18296v1",
                "http://arxiv.org/pdf/2311.18296v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18274v1",
            "title": "Semiparametric Efficient Inference in Adaptive Experiments",
            "updated": "2023-11-30T06:25:06Z",
            "published": "2023-11-30T06:25:06Z",
            "summary": "We consider the problem of efficient inference of the Average Treatment\nEffect in a sequential experiment where the policy governing the assignment of\nsubjects to treatment or control can change over time. We first provide a\ncentral limit theorem for the Adaptive Augmented Inverse-Probability Weighted\nestimator, which is semiparametric efficient, under weaker assumptions than\nthose previously made in the literature. This central limit theorem enables\nefficient inference at fixed sample sizes. We then consider a sequential\ninference setting, deriving both asymptotic and nonasymptotic confidence\nsequences that are considerably tighter than previous methods. These\nanytime-valid methods enable inference under data-dependent stopping times\n(sample sizes). Additionally, we use propensity score truncation techniques\nfrom the recent off-policy estimation literature to reduce the finite sample\nvariance of our estimator without affecting the asymptotic variance. Empirical\nresults demonstrate that our methods yield narrower confidence sequences than\nthose previously developed in the literature while maintaining time-uniform\nerror control.",
            "author": [
                "Thomas Cook",
                "Alan Mishler",
                "Aaditya Ramdas"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18274v1",
                "http://arxiv.org/pdf/2311.18274v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG",
                "stat.ME"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18273v1",
            "title": "HKUST at SemEval-2023 Task 1: Visual Word Sense Disambiguation with\n  Context Augmentation and Visual Assistance",
            "updated": "2023-11-30T06:23:15Z",
            "published": "2023-11-30T06:23:15Z",
            "summary": "Visual Word Sense Disambiguation (VWSD) is a multi-modal task that aims to\nselect, among a batch of candidate images, the one that best entails the target\nword's meaning within a limited context. In this paper, we propose a\nmulti-modal retrieval framework that maximally leverages pretrained\nVision-Language models, as well as open knowledge bases and datasets. Our\nsystem consists of the following key components: (1) Gloss matching: a\npretrained bi-encoder model is used to match contexts with proper senses of the\ntarget words; (2) Prompting: matched glosses and other textual information,\nsuch as synonyms, are incorporated using a prompting template; (3) Image\nretrieval: semantically matching images are retrieved from large open datasets\nusing prompts as queries; (4) Modality fusion: contextual information from\ndifferent modalities are fused and used for prediction. Although our system\ndoes not produce the most competitive results at SemEval-2023 Task 1, we are\nstill able to beat nearly half of the teams. More importantly, our experiments\nreveal acute insights for the field of Word Sense Disambiguation (WSD) and\nmulti-modal learning. Our code is available on GitHub.",
            "author": [
                "Zhuohao Yin",
                "Xin Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18273v1",
                "http://arxiv.org/pdf/2311.18273v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.MM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18266v1",
            "title": "Prompt-Based Exemplar Super-Compression and Regeneration for\n  Class-Incremental Learning",
            "updated": "2023-11-30T05:59:31Z",
            "published": "2023-11-30T05:59:31Z",
            "summary": "Replay-based methods in class-incremental learning (CIL) have attained\nremarkable success, as replaying the exemplars of old classes can significantly\nmitigate catastrophic forgetting. Despite their effectiveness, the inherent\nmemory restrictions of CIL result in saving a limited number of exemplars with\npoor diversity, leading to data imbalance and overfitting issues. In this\npaper, we introduce a novel exemplar super-compression and regeneration method,\nESCORT, which substantially increases the quantity and enhances the diversity\nof exemplars. Rather than storing past images, we compress images into visual\nand textual prompts, e.g., edge maps and class tags, and save the prompts\ninstead, reducing the memory usage of each exemplar to 1/24 of the original\nsize. In subsequent learning phases, diverse high-resolution exemplars are\ngenerated from the prompts by a pre-trained diffusion model, e.g., ControlNet.\nTo minimize the domain gap between generated exemplars and real images, we\npropose partial compression and diffusion-based data augmentation, allowing us\nto utilize an off-the-shelf diffusion model without fine-tuning it on the\ntarget dataset. Therefore, the same diffusion model can be downloaded whenever\nit is needed, incurring no memory consumption. Comprehensive experiments\ndemonstrate that our method significantly improves model performance across\nmultiple CIL benchmarks, e.g., 5.0 percentage points higher than the previous\nstate-of-the-art on 10-phase Caltech-256 dataset.",
            "author": [
                "Ruxiao Duan",
                "Yaoyao Liu",
                "Jieneng Chen",
                "Adam Kortylewski",
                "Alan Yuille"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18266v1",
                "http://arxiv.org/pdf/2311.18266v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00820v1",
            "title": "Non-Cross Diffusion for Semantic Consistency",
            "updated": "2023-11-30T05:53:39Z",
            "published": "2023-11-30T05:53:39Z",
            "summary": "In diffusion models, deviations from a straight generative flow are a common\nissue, resulting in semantic inconsistencies and suboptimal generations. To\naddress this challenge, we introduce `Non-Cross Diffusion', an innovative\napproach in generative modeling for learning ordinary differential equation\n(ODE) models. Our methodology strategically incorporates an ascending dimension\nof input to effectively connect points sampled from two distributions with\nuncrossed paths. This design is pivotal in ensuring enhanced semantic\nconsistency throughout the inference process, which is especially critical for\napplications reliant on consistent generative flows, including various\ndistillation methods and deterministic sampling, which are fundamental in image\nediting and interpolation tasks. Our empirical results demonstrate the\neffectiveness of Non-Cross Diffusion, showing a substantial reduction in\nsemantic inconsistencies at different inference steps and a notable enhancement\nin the overall performance of diffusion models.",
            "author": [
                "Ziyang Zheng",
                "Ruiyuan Gao",
                "Qiang Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00820v1",
                "http://arxiv.org/pdf/2312.00820v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18261v1",
            "title": "Learning Exactly Linearizable Deep Dynamics Models",
            "updated": "2023-11-30T05:40:55Z",
            "published": "2023-11-30T05:40:55Z",
            "summary": "Research on control using models based on machine-learning methods has now\nshifted to the practical engineering stage. Achieving high performance and\ntheoretically guaranteeing the safety of the system is critical for such\napplications. In this paper, we propose a learning method for exactly\nlinearizable dynamical models that can easily apply various control theories to\nensure stability, reliability, etc., and to provide a high degree of freedom of\nexpression. As an example, we present a design that combines simple linear\ncontrol and control barrier functions. The proposed model is employed for the\nreal-time control of an automotive engine, and the results demonstrate good\npredictive performance and stable control under constraints.",
            "author": [
                "Ryuta Moriyasu",
                "Masayuki Kusunoki",
                "Kenji Kashima"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18261v1",
                "http://arxiv.org/pdf/2311.18261v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.LG",
                "cs.SY",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18260v2",
            "title": "Consensus, dissensus and synergy between clinicians and specialist\n  foundation models in radiology report generation",
            "updated": "2023-12-06T17:16:07Z",
            "published": "2023-11-30T05:38:34Z",
            "summary": "Radiology reports are an instrumental part of modern medicine, informing key\nclinical decisions such as diagnosis and treatment. The worldwide shortage of\nradiologists, however, restricts access to expert care and imposes heavy\nworkloads, contributing to avoidable errors and delays in report delivery.\nWhile recent progress in automated report generation with vision-language\nmodels offer clear potential in ameliorating the situation, the path to\nreal-world adoption has been stymied by the challenge of evaluating the\nclinical quality of AI-generated reports. In this study, we build a\nstate-of-the-art report generation system for chest radiographs,\n\\textit{Flamingo-CXR}, by fine-tuning a well-known vision-language foundation\nmodel on radiology data. To evaluate the quality of the AI-generated reports, a\ngroup of 16 certified radiologists provide detailed evaluations of AI-generated\nand human written reports for chest X-rays from an intensive care setting in\nthe United States and an inpatient setting in India. At least one radiologist\n(out of two per case) preferred the AI report to the ground truth report in\nover 60$\\%$ of cases for both datasets. Amongst the subset of AI-generated\nreports that contain errors, the most frequently cited reasons were related to\nthe location and finding, whereas for human written reports, most mistakes were\nrelated to severity and finding. This disparity suggested potential\ncomplementarity between our AI system and human experts, prompting us to\ndevelop an assistive scenario in which \\textit{Flamingo-CXR} generates a\nfirst-draft report, which is subsequently revised by a clinician. This is the\nfirst demonstration of clinician-AI collaboration for report writing, and the\nresultant reports are assessed to be equivalent or preferred by at least one\nradiologist to reports written by experts alone in 80$\\%$ of in-patient cases\nand 60$\\%$ of intensive care cases.",
            "author": [
                "Ryutaro Tanno",
                "David G. T. Barrett",
                "Andrew Sellergren",
                "Sumedh Ghaisas",
                "Sumanth Dathathri",
                "Abigail See",
                "Johannes Welbl",
                "Karan Singhal",
                "Shekoofeh Azizi",
                "Tao Tu",
                "Mike Schaekermann",
                "Rhys May",
                "Roy Lee",
                "SiWai Man",
                "Zahra Ahmed",
                "Sara Mahdavi",
                "Danielle Belgrave",
                "Vivek Natarajan",
                "Shravya Shetty",
                "Pushmeet Kohli",
                "Po-Sen Huang",
                "Alan Karthikesalingam",
                "Ira Ktena"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18260v2",
                "http://arxiv.org/pdf/2311.18260v2"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CL",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18257v1",
            "title": "Diffusion Models Without Attention",
            "updated": "2023-11-30T05:15:35Z",
            "published": "2023-11-30T05:15:35Z",
            "summary": "In recent advancements in high-fidelity image generation, Denoising Diffusion\nProbabilistic Models (DDPMs) have emerged as a key player. However, their\napplication at high resolutions presents significant computational challenges.\nCurrent methods, such as patchifying, expedite processes in UNet and\nTransformer architectures but at the expense of representational capacity.\nAddressing this, we introduce the Diffusion State Space Model (DiffuSSM), an\narchitecture that supplants attention mechanisms with a more scalable state\nspace model backbone. This approach effectively handles higher resolutions\nwithout resorting to global compression, thus preserving detailed image\nrepresentation throughout the diffusion process. Our focus on FLOP-efficient\narchitectures in diffusion training marks a significant step forward.\nComprehensive evaluations on both ImageNet and LSUN datasets at two resolutions\ndemonstrate that DiffuSSMs are on par or even outperform existing diffusion\nmodels with attention modules in FID and Inception Score metrics while\nsignificantly reducing total FLOP usage.",
            "author": [
                "Jing Nathan Yan",
                "Jiatao Gu",
                "Alexander M. Rush"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18257v1",
                "http://arxiv.org/pdf/2311.18257v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18254v1",
            "title": "Sketch Input Method Editor: A Comprehensive Dataset and Methodology for\n  Systematic Input Recognition",
            "updated": "2023-11-30T05:05:38Z",
            "published": "2023-11-30T05:05:38Z",
            "summary": "With the recent surge in the use of touchscreen devices, free-hand sketching\nhas emerged as a promising modality for human-computer interaction. While\nprevious research has focused on tasks such as recognition, retrieval, and\ngeneration of familiar everyday objects, this study aims to create a Sketch\nInput Method Editor (SketchIME) specifically designed for a professional C4I\nsystem. Within this system, sketches are utilized as low-fidelity prototypes\nfor recommending standardized symbols in the creation of comprehensive\nsituation maps. This paper also presents a systematic dataset comprising 374\nspecialized sketch types, and proposes a simultaneous recognition and\nsegmentation architecture with multilevel supervision between recognition and\nsegmentation to improve performance and enhance interpretability. By\nincorporating few-shot domain adaptation and class-incremental learning, the\nnetwork's ability to adapt to new users and extend to new task-specific classes\nis significantly enhanced. Results from experiments conducted on both the\nproposed dataset and the SPG dataset illustrate the superior performance of the\nproposed architecture. Our dataset and code are publicly available at\nhttps://github.com/Anony517/SketchIME.",
            "author": [
                "Guangming Zhu",
                "Siyuan Wang",
                "Qing Cheng",
                "Kelong Wu",
                "Hao Li",
                "Liang Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18254v1",
                "http://arxiv.org/pdf/2311.18254v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18252v1",
            "title": "Navigating Privacy and Copyright Challenges Across the Data Lifecycle of\n  Generative AI",
            "updated": "2023-11-30T05:03:08Z",
            "published": "2023-11-30T05:03:08Z",
            "summary": "The advent of Generative AI has marked a significant milestone in artificial\nintelligence, demonstrating remarkable capabilities in generating realistic\nimages, texts, and data patterns. However, these advancements come with\nheightened concerns over data privacy and copyright infringement, primarily due\nto the reliance on vast datasets for model training. Traditional approaches\nlike differential privacy, machine unlearning, and data poisoning only offer\nfragmented solutions to these complex issues. Our paper delves into the\nmultifaceted challenges of privacy and copyright protection within the data\nlifecycle. We advocate for integrated approaches that combines technical\ninnovation with ethical foresight, holistically addressing these concerns by\ninvestigating and devising solutions that are informed by the lifecycle\nperspective. This work aims to catalyze a broader discussion and inspire\nconcerted efforts towards data privacy and copyright integrity in Generative\nAI.",
            "author": [
                "Dawen Zhang",
                "Boming Xia",
                "Yue Liu",
                "Xiwei Xu",
                "Thong Hoang",
                "Zhenchang Xing",
                "Mark Staples",
                "Qinghua Lu",
                "Liming Zhu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18252v1",
                "http://arxiv.org/pdf/2311.18252v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE",
                "cs.AI",
                "cs.CY",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18251v1",
            "title": "Can Large Language Models Be Good Companions? An LLM-Based Eyewear\n  System with Conversational Common Ground",
            "updated": "2023-11-30T04:59:34Z",
            "published": "2023-11-30T04:59:34Z",
            "summary": "Developing chatbots as personal companions has long been a goal of artificial\nintelligence researchers. Recent advances in Large Language Models (LLMs) have\ndelivered a practical solution for endowing chatbots with anthropomorphic\nlanguage capabilities. However, it takes more than LLMs to enable chatbots that\ncan act as companions. Humans use their understanding of individual\npersonalities to drive conversations. Chatbots also require this capability to\nenable human-like companionship. They should act based on personalized,\nreal-time, and time-evolving knowledge of their owner. We define such essential\nknowledge as the \\textit{common ground} between chatbots and their owners, and\nwe propose to build a common-ground-aware dialogue system from an LLM-based\nmodule, named \\textit{OS-1}, to enable chatbot companionship. Hosted by\neyewear, OS-1 can sense the visual and audio signals the user receives and\nextract real-time contextual semantics. Those semantics are categorized and\nrecorded to formulate historical contexts from which the user's profile is\ndistilled and evolves over time, i.e., OS-1 gradually learns about its user.\nOS-1 combines knowledge from real-time semantics, historical contexts, and\nuser-specific profiles to produce a common-ground-aware prompt input into the\nLLM module. The LLM's output is converted to audio, spoken to the wearer when\nappropriate.We conduct laboratory and in-field studies to assess OS-1's ability\nto build common ground between the chatbot and its user. The technical\nfeasibility and capabilities of the system are also evaluated. OS-1, with its\ncommon-ground awareness, can significantly improve user satisfaction and\npotentially lead to downstream tasks such as personal emotional support and\nassistance.",
            "author": [
                "Zhenyu Xu",
                "Hailin Xu",
                "Zhouyang Lu",
                "Yingying Zhao",
                "Rui Zhu",
                "Yujiang Wang",
                "Mingzhi Dong",
                "Yuhu Chang",
                "Qin Lv",
                "Robert P. Dick",
                "Fan Yang",
                "Tun Lu",
                "Ning Gu",
                "Li Shang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18251v1",
                "http://arxiv.org/pdf/2311.18251v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18246v1",
            "title": "Combined Scheduling, Memory Allocation and Tensor Replacement for\n  Minimizing Off-Chip Data Accesses of DNN Accelerators",
            "updated": "2023-11-30T04:36:25Z",
            "published": "2023-11-30T04:36:25Z",
            "summary": "Specialized hardware accelerators have been extensively used for Deep Neural\nNetworks (DNNs) to provide power/performance benefits. These accelerators\ncontain specialized hardware that supports DNN operators, and scratchpad memory\nfor storing the tensor operands. Often, the size of the scratchpad is\ninsufficient to store all the tensors needed for the computation, and\nadditional data accesses are needed to move tensors back and forth from host\nmemory during the computation with significant power/performance overhead. The\nvolume of these additional data accesses depends on the operator schedule, and\nmemory allocation (specific locations selected for the tensors in the\nscratchpad). We propose an optimization framework, named COSMA, for mapping\nDNNs to an accelerator that finds the optimal operator schedule, memory\nallocation and tensor replacement that minimizes the additional data accesses.\nCOSMA provides an Integer Linear Programming (ILP) formulation to generate the\noptimal solution for mapping a DNN to the accelerator for a given scratchpad\nsize. We demonstrate that, using an off-the-shelf ILP solver, COSMA obtains the\noptimal solution in seconds for a wide-range of state-of-the-art DNNs for\ndifferent applications. Further, it out-performs existing methods by reducing\non average 84% of the non-compulsory data accesses. We further propose a\ndivide-and-conquer heuristic to scale up to certain complex DNNs generated by\nNeural Architecture Search, and this heuristic solution reduces on average 85%\ndata accesses compared with other works.",
            "author": [
                "Yi Li",
                "Aarti Gupta",
                "Sharad Malik"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18246v1",
                "http://arxiv.org/pdf/2311.18246v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00819v1",
            "title": "Large Language Models for Travel Behavior Prediction",
            "updated": "2023-11-30T04:35:55Z",
            "published": "2023-11-30T04:35:55Z",
            "summary": "Travel behavior prediction is a fundamental task in transportation demand\nmanagement. The conventional methods for travel behavior prediction rely on\nnumerical data to construct mathematical models and calibrate model parameters\nto represent human preferences. Recent advancement in large language models\n(LLMs) has shown great reasoning abilities to solve complex problems. In this\nstudy, we propose to use LLMs to predict travel behavior with prompt\nengineering without data-based parameter learning. Specifically, we carefully\ndesign our prompts that include 1) task description, 2) travel characteristics,\n3) individual attributes, and 4) guides of thinking with domain knowledge, and\nask the LLMs to predict an individual's travel behavior and explain the\nresults. We select the travel mode choice task as a case study. Results show\nthat, though no training samples are provided, LLM-based predictions have\ncompetitive accuracy and F1-score as canonical supervised learning methods such\nas multinomial logit, random forest, and neural networks. LLMs can also output\nreasons that support their prediction. However, though in most of the cases,\nthe output explanations are reasonable, we still observe cases that violate\nlogic or with hallucinations.",
            "author": [
                "Baichuan Mo",
                "Hanyong Xu",
                "Dingyi Zhuang",
                "Ruoyun Ma",
                "Xiaotong Guo",
                "Jinhua Zhao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00819v1",
                "http://arxiv.org/pdf/2312.00819v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18245v1",
            "title": "Automatic Detection of Alzheimer's Disease with Multi-Modal Fusion of\n  Clinical MRI Scans",
            "updated": "2023-11-30T04:32:28Z",
            "published": "2023-11-30T04:32:28Z",
            "summary": "The aging population of the U.S. drives the prevalence of Alzheimer's\ndisease. Brookmeyer et al. forecasts approximately 15 million Americans will\nhave either clinical AD or mild cognitive impairment by 2060. In response to\nthis urgent call, methods for early detection of Alzheimer's disease have been\ndeveloped for prevention and pre-treatment. Notably, literature on the\napplication of deep learning in the automatic detection of the disease has been\nproliferating. This study builds upon previous literature and maintains a focus\non leveraging multi-modal information to enhance automatic detection. We aim to\npredict the stage of the disease - Cognitively Normal (CN), Mildly Cognitive\nImpairment (MCI), and Alzheimer's Disease (AD), based on two different types of\nbrain MRI scans. We design an AlexNet-based deep learning model that learns the\nsynergy of complementary information from both T1 and FLAIR MRI scans.",
            "author": [
                "Long Chen",
                "Liben Chen",
                "Binfeng Xu",
                "Wenxin Zhang",
                "Narges Razavian"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18245v1",
                "http://arxiv.org/pdf/2311.18245v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18244v1",
            "title": "Poisoning Attacks Against Contrastive Recommender Systems",
            "updated": "2023-11-30T04:25:28Z",
            "published": "2023-11-30T04:25:28Z",
            "summary": "Contrastive learning (CL) has recently gained significant popularity in the\nfield of recommendation. Its ability to learn without heavy reliance on labeled\ndata is a natural antidote to the data sparsity issue. Previous research has\nfound that CL can not only enhance recommendation accuracy but also\ninadvertently exhibit remarkable robustness against noise. However, this paper\nidentifies a vulnerability of CL-based recommender systems: Compared with their\nnon-CL counterparts, they are even more susceptible to poisoning attacks that\naim to promote target items. Our analysis points to the uniform dispersion of\nrepresentations led by the CL loss as the very factor that accounts for this\nvulnerability. We further theoretically and empirically demonstrate that the\noptimization of CL loss can lead to smooth spectral values of representations.\nBased on these insights, we attempt to reveal the potential poisoning attacks\nagainst CL-based recommender systems. The proposed attack encompasses a\ndual-objective framework: One that induces a smoother spectral value\ndistribution to amplify the CL loss's inherent dispersion effect, named\ndispersion promotion; and the other that directly elevates the visibility of\ntarget items, named rank promotion. We validate the destructiveness of our\nattack model through extensive experimentation on four datasets. By shedding\nlight on these vulnerabilities, we aim to facilitate the development of more\nrobust CL-based recommender systems.",
            "author": [
                "Zongwei Wang",
                "Junliang Yu",
                "Min Gao",
                "Hongzhi Yin",
                "Bin Cui",
                "Shazia Sadiq"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18244v1",
                "http://arxiv.org/pdf/2311.18244v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.CR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18243v1",
            "title": "DKiS: Decay weight invertible image steganography with private key",
            "updated": "2023-11-30T04:21:10Z",
            "published": "2023-11-30T04:21:10Z",
            "summary": "Image steganography, the practice of concealing information within another\nimage, traditionally faces security challenges when its methods become publicly\nknown. To counteract this, we introduce a novel private key-based image\nsteganography technique. This approach ensures the security of hidden\ninformation, requiring a corresponding private key for access, irrespective of\nthe public knowledge of the steganography method. We present experimental\nevidence demonstrating our method's effectiveness, showcasing its real-world\napplicability. Additionally, we identified a critical challenge in the\ninvertible image steganography process: the transfer of non-essential, or\n`garbage', information from the secret to the host pipeline. To address this,\nwe introduced the decay weight to control the information transfer, filtering\nout irrelevant data and enhancing the performance of image steganography. Our\ncode is publicly accessible at https://github.com/yanghangAI/DKiS, and a\npractical demonstration is available at http://yanghang.site/hidekey.",
            "author": [
                "Hang Yang",
                "Yitian Xu",
                "Xuhua Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18243v1",
                "http://arxiv.org/pdf/2311.18243v1"
            ],
            "primary_category": "cs.MM",
            "category": [
                "cs.MM",
                "cs.CR",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18237v1",
            "title": "Label-efficient Training of Small Task-specific Models by Leveraging\n  Vision Foundation Models",
            "updated": "2023-11-30T04:07:44Z",
            "published": "2023-11-30T04:07:44Z",
            "summary": "Large Vision Foundation Models (VFMs) pretrained on massive datasets exhibit\nimpressive performance on various downstream tasks, especially with limited\nlabeled target data. However, due to their high memory and compute\nrequirements, these models cannot be deployed in resource constrained settings.\nThis raises an important question: How can we utilize the knowledge from a\nlarge VFM to train a small task-specific model for a new target task with\nlimited labeled training data? In this work, we answer this question by\nproposing a simple and highly effective task-oriented knowledge transfer\napproach to leverage pretrained VFMs for effective training of small\ntask-specific models. Our experimental results on four target tasks under\nlimited labeled data settings show that the proposed knowledge transfer\napproach outperforms task-agnostic VFM distillation, web-scale CLIP pretraining\nand supervised ImageNet pretraining by 1-10.5%, 2-22% and 2-14%, respectively.\nWe also show that the dataset used for transferring knowledge has a significant\neffect on the final target task performance, and propose an image\nretrieval-based approach for curating effective transfer sets.",
            "author": [
                "Raviteja Vemulapalli",
                "Hadi Pouransari",
                "Fartash Faghri",
                "Sachin Mehta",
                "Mehrdad Farajtabar",
                "Mohammad Rastegari",
                "Oncel Tuzel"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18237v1",
                "http://arxiv.org/pdf/2311.18237v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18232v1",
            "title": "LMRL Gym: Benchmarks for Multi-Turn Reinforcement Learning with Language\n  Models",
            "updated": "2023-11-30T03:59:31Z",
            "published": "2023-11-30T03:59:31Z",
            "summary": "Large language models (LLMs) provide excellent text-generation capabilities,\nbut standard prompting and generation methods generally do not lead to\nintentional or goal-directed agents and might necessitate considerable prompt\ntuning. This becomes particularly apparent in multi-turn conversations: even\nthe best current LLMs rarely ask clarifying questions, engage in explicit\ninformation gathering, or take actions now that lead to better decisions after\nmultiple turns. Reinforcement learning has the potential to leverage the\npowerful modeling capabilities of LLMs, as well as their internal\nrepresentation of textual interactions, to create capable goal-directed\nlanguage agents. This can enable intentional and temporally extended\ninteractions, such as with humans, through coordinated persuasion and carefully\ncrafted questions, or in goal-directed play through text games to bring about\ndesired final outcomes. However, enabling this requires the community to\ndevelop stable and reliable reinforcement learning algorithms that can\neffectively train LLMs. Developing such algorithms requires tasks that can\ngauge progress on algorithm design, provide accessible and reproducible\nevaluations for multi-turn interactions, and cover a range of task properties\nand challenges in improving reinforcement learning algorithms. Our paper\nintroduces the LMRL-Gym benchmark for evaluating multi-turn RL for LLMs,\ntogether with an open-source research framework containing a basic toolkit for\ngetting started on multi-turn RL with offline value-based and policy-based RL\nmethods. Our benchmark consists of 8 different language tasks, which require\nmultiple rounds of language interaction and cover a range of tasks in\nopen-ended dialogue and text games.",
            "author": [
                "Marwa Abdulhai",
                "Isadora White",
                "Charlie Snell",
                "Charles Sun",
                "Joey Hong",
                "Yuexiang Zhai",
                "Kelvin Xu",
                "Sergey Levine"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18232v1",
                "http://arxiv.org/pdf/2311.18232v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18224v1",
            "title": "Reasoning with the Theory of Mind for Pragmatic Semantic Communication",
            "updated": "2023-11-30T03:36:19Z",
            "published": "2023-11-30T03:36:19Z",
            "summary": "In this paper, a pragmatic semantic communication framework that enables\neffective goal-oriented information sharing between two-intelligent agents is\nproposed. In particular, semantics is defined as the causal state that\nencapsulates the fundamental causal relationships and dependencies among\ndifferent features extracted from data. The proposed framework leverages the\nemerging concept in machine learning (ML) called theory of mind (ToM). It\nemploys a dynamic two-level (wireless and semantic) feedback mechanism to\ncontinuously fine-tune neural network components at the transmitter. Thanks to\nthe ToM, the transmitter mimics the actual mental state of the receiver's\nreasoning neural network operating semantic interpretation. Then, the estimated\nmental state at the receiver is dynamically updated thanks to the proposed\ndynamic two-level feedback mechanism. At the lower level, conventional channel\nquality metrics are used to optimize the channel encoding process based on the\nwireless communication channel's quality, ensuring an efficient mapping of\nsemantic representations to a finite constellation. Additionally, a semantic\nfeedback level is introduced, providing information on the receiver's perceived\nsemantic effectiveness with minimal overhead. Numerical evaluations demonstrate\nthe framework's ability to achieve efficient communication with a reduced\namount of bits while maintaining the same semantics, outperforming conventional\nsystems that do not exploit the ToM-based reasoning.",
            "author": [
                "Christo Kurisummoottil Thomas",
                "Emilio Calvanese Strinati",
                "Walid Saad"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18224v1",
                "http://arxiv.org/pdf/2311.18224v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "cs.AI",
                "cs.LG",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18222v1",
            "title": "AI-predicted protein deformation encodes energy landscape perturbation",
            "updated": "2023-11-30T03:33:05Z",
            "published": "2023-11-30T03:33:05Z",
            "summary": "AI algorithms proved excellent predictors of protein structure, but whether\ntheir exceptional accuracy is merely due to megascale regression or these\nalgorithms learn the underlying physics remains an open question. Here, we\nperform a stringent test for the existence of such learning in the Alphafold2\n(AF) algorithm: We use AF to predict the subtle structural deformation induced\nby single mutations, quantified by strain, and compare with experimental\ndatasets of corresponding perturbations in folding free energy $\\Delta\\Delta\nG$. Unexpectedly, we find that physical strain alone -- without any additional\ndata or computation -- correlates almost as well with $\\Delta \\Delta G$ as\nstate-of-the-art energy-based and machine-learning predictors.This indicates\nthat the AF-predicted structures alone encode fine details about the energy\nlandscape. In particular, the structures encode significant information on\nstability, enough to estimate (de-)stabilizing effects of mutations, thus\npaving the way for the development of novel, structure-based stability\npredictors for protein design and evolution.",
            "author": [
                "John M Mcbride",
                "Tsvi Tlusty"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18222v1",
                "http://arxiv.org/pdf/2311.18222v1"
            ],
            "primary_category": "q-bio.BM",
            "category": [
                "q-bio.BM",
                "physics.bio-ph",
                "q-bio.QM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18221v1",
            "title": "Determining the core-collapse supernova explosion mechanism with current\n  and future gravitational-wave observatories",
            "updated": "2023-11-30T03:31:52Z",
            "published": "2023-11-30T03:31:52Z",
            "summary": "Gravitational waves are emitted from deep within a core-collapse supernova,\nwhich may enable us to determine the mechanism of the explosion from a\ngravitational-wave detection. Previous studies suggested that it is possible to\ndetermine if the explosion mechanism is neutrino-driven or magneto-rotationally\npowered from the gravitational-wave signal. However, long duration\nmagneto-rotational waveforms, that cover the full explosion phase, were not\navailable during the time of previous studies, and explosions were just assumed\nto be magneto-rotationally driven if the model was rapidly rotating. Therefore,\nwe perform an updated study using new 3D long-duration magneto-rotational\ncore-collapse supernova waveforms that cover the full explosion phase, injected\ninto noise for the Advanced LIGO, Einstein Telescope and NEMO\ngravitational-wave detectors. We also include a category for failed explosions\nin our signal classification results. We then determine the explosion mechanism\nof the signals using three different methods: Bayesian model selection,\ndictionary learning, and convolutional neural networks. The three different\nmethods are able to distinguish between neutrino-driven driven explosions and\nmagneto-rotational explosions, however they can only distinguish between the\nnon-exploding and neutrino-driven explosions for signals with a high signal to\nnoise ratio.",
            "author": [
                "Jade Powell",
                "Alberto Iess",
                "Miquel Llorens-Monteagudo",
                "Martin Obergaulinger",
                "Bernhard M\u00fcller",
                "Alejandro Torres-Forn\u00e9",
                "Elena Cuoco",
                "Jos\u00e9 A. Font"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18221v1",
                "http://arxiv.org/pdf/2311.18221v1"
            ],
            "primary_category": "astro-ph.HE",
            "category": [
                "astro-ph.HE",
                "gr-qc"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18219v1",
            "title": "FoldExplorer: Fast and Accurate Protein Structure Search with\n  Sequence-Enhanced Graph Embedding",
            "updated": "2023-11-30T03:29:20Z",
            "published": "2023-11-30T03:29:20Z",
            "summary": "The advent of highly accurate protein structure prediction methods has fueled\nan exponential expansion of the protein structure database. Consequently, there\nis a rising demand for rapid and precise structural homolog search. Traditional\nalignment-based methods are dedicated to precise comparisons between pairs,\nexhibiting high accuracy. However, their sluggish processing speed is no longer\nadequate for managing the current massive volume of data. In response to this\nchallenge, we propose a novel deep-learning approach FoldExplorer. It harnesses\nthe powerful capabilities of graph attention neural networks and protein large\nlanguage models for protein structures and sequences data processing to\ngenerate embeddings for protein structures. The structural embeddings can be\nused for fast and accurate protein search. The embeddings also provide insights\ninto the protein space. FoldExplorer demonstrates a substantial performance\nimprovement of 5% to 8% over the current state-of-the-art algorithm on the\nbenchmark datasets. Meanwhile, FoldExplorer does not compromise on search speed\nand excels particularly in searching on a large-scale dataset.",
            "author": [
                "Yuan Liu",
                "Hong-Bin Shen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18219v1",
                "http://arxiv.org/pdf/2311.18219v1"
            ],
            "primary_category": "q-bio.BM",
            "category": [
                "q-bio.BM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18213v1",
            "title": "Beyond Two-Tower Matching: Learning Sparse Retrievable\n  Cross-Interactions for Recommendation",
            "updated": "2023-11-30T03:13:36Z",
            "published": "2023-11-30T03:13:36Z",
            "summary": "Two-tower models are a prevalent matching framework for recommendation, which\nhave been widely deployed in industrial applications. The success of two-tower\nmatching attributes to its efficiency in retrieval among a large number of\nitems, since the item tower can be precomputed and used for fast Approximate\nNearest Neighbor (ANN) search. However, it suffers two main challenges,\nincluding limited feature interaction capability and reduced accuracy in online\nserving. Existing approaches attempt to design novel late interactions instead\nof dot products, but they still fail to support complex feature interactions or\nlose retrieval efficiency. To address these challenges, we propose a new\nmatching paradigm named SparCode, which supports not only sophisticated feature\ninteractions but also efficient retrieval. Specifically, SparCode introduces an\nall-to-all interaction module to model fine-grained query-item interactions.\nBesides, we design a discrete code-based sparse inverted index jointly trained\nwith the model to achieve effective and efficient model inference. Extensive\nexperiments have been conducted on open benchmark datasets to demonstrate the\nsuperiority of our framework. The results show that SparCode significantly\nimproves the accuracy of candidate item matching while retaining the same level\nof retrieval efficiency with two-tower models. Our source code will be\navailable at MindSpore/models.",
            "author": [
                "Liangcai Su",
                "Fan Yan",
                "Jieming Zhu",
                "Xi Xiao",
                "Haoyi Duan",
                "Zhou Zhao",
                "Zhenhua Dong",
                "Ruiming Tang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18213v1",
                "http://arxiv.org/pdf/2311.18213v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18208v1",
            "title": "SMaRt: Improving GANs with Score Matching Regularity",
            "updated": "2023-11-30T03:05:14Z",
            "published": "2023-11-30T03:05:14Z",
            "summary": "Generative adversarial networks (GANs) usually struggle in learning from\nhighly diverse data, whose underlying manifold is complex. In this work, we\nrevisit the mathematical foundations of GANs, and theoretically reveal that the\nnative adversarial loss for GAN training is insufficient to fix the problem of\nsubsets with positive Lebesgue measure of the generated data manifold lying out\nof the real data manifold. Instead, we find that score matching serves as a\nvalid solution to this issue thanks to its capability of persistently pushing\nthe generated data points towards the real data manifold. We thereby propose to\nimprove the optimization of GANs with score matching regularity (SMaRt).\nRegarding the empirical evidences, we first design a toy example to show that\ntraining GANs by the aid of a ground-truth score function can help reproduce\nthe real data distribution more accurately, and then confirm that our approach\ncan consistently boost the synthesis performance of various state-of-the-art\nGANs on real-world datasets with pre-trained diffusion models acting as the\napproximate score function. For instance, when training Aurora on the ImageNet\n64x64 dataset, we manage to improve FID from 8.87 to 7.11, on par with the\nperformance of one-step consistency model. The source code will be made public.",
            "author": [
                "Mengfei Xia",
                "Yujun Shen",
                "Ceyuan Yang",
                "Ran Yi",
                "Wenping Wang",
                "Yong-jin Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18208v1",
                "http://arxiv.org/pdf/2311.18208v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18207v2",
            "title": "Towards Assessing and Benchmarking Risk-Return Tradeoff of Off-Policy\n  Evaluation",
            "updated": "2023-12-04T18:37:30Z",
            "published": "2023-11-30T02:56:49Z",
            "summary": "Off-Policy Evaluation (OPE) aims to assess the effectiveness of\ncounterfactual policies using only offline logged data and is often used to\nidentify the top-k promising policies for deployment in online A/B tests.\nExisting evaluation metrics for OPE estimators primarily focus on the\n\"accuracy\" of OPE or that of downstream policy selection, neglecting\nrisk-return tradeoff in the subsequent online policy deployment. To address\nthis issue, we draw inspiration from portfolio evaluation in finance and\ndevelop a new metric, called SharpeRatio@k, which measures the risk-return\ntradeoff of policy portfolios formed by an OPE estimator under varying online\nevaluation budgets (k). We validate our metric in two example scenarios,\ndemonstrating its ability to effectively distinguish between low-risk and\nhigh-risk estimators and to accurately identify the most efficient estimator.\nThis efficient estimator is characterized by its capability to form the most\nadvantageous policy portfolios, maximizing returns while minimizing risks\nduring online deployment, a nuance that existing metrics typically overlook. To\nfacilitate a quick, accurate, and consistent evaluation of OPE via\nSharpeRatio@k, we have also integrated this metric into an open-source\nsoftware, SCOPE-RL. Employing SharpeRatio@k and SCOPE-RL, we conduct\ncomprehensive benchmarking experiments on various estimators and RL tasks,\nfocusing on their risk-return tradeoff. These experiments offer several\ninteresting directions and suggestions for future OPE research.",
            "author": [
                "Haruka Kiyohara",
                "Ren Kishimoto",
                "Kosuke Kawakami",
                "Ken Kobayashi",
                "Kazuhide Nakata",
                "Yuta Saito"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18207v2",
                "http://arxiv.org/pdf/2311.18207v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18206v2",
            "title": "SCOPE-RL: A Python Library for Offline Reinforcement Learning and\n  Off-Policy Evaluation",
            "updated": "2023-12-04T18:42:03Z",
            "published": "2023-11-30T02:56:43Z",
            "summary": "This paper introduces SCOPE-RL, a comprehensive open-source Python software\ndesigned for offline reinforcement learning (offline RL), off-policy evaluation\n(OPE), and selection (OPS). Unlike most existing libraries that focus solely on\neither policy learning or evaluation, SCOPE-RL seamlessly integrates these two\nkey aspects, facilitating flexible and complete implementations of both offline\nRL and OPE processes. SCOPE-RL put particular emphasis on its OPE modules,\noffering a range of OPE estimators and robust evaluation-of-OPE protocols. This\napproach enables more in-depth and reliable OPE compared to other packages. For\ninstance, SCOPE-RL enhances OPE by estimating the entire reward distribution\nunder a policy rather than its mere point-wise expected value. Additionally,\nSCOPE-RL provides a more thorough evaluation-of-OPE by presenting the\nrisk-return tradeoff in OPE results, extending beyond mere accuracy evaluations\nin existing OPE literature. SCOPE-RL is designed with user accessibility in\nmind. Its user-friendly APIs, comprehensive documentation, and a variety of\neasy-to-follow examples assist researchers and practitioners in efficiently\nimplementing and experimenting with various offline RL methods and OPE\nestimators, tailored to their specific problem contexts. The documentation of\nSCOPE-RL is available at https://scope-rl.readthedocs.io/en/latest/.",
            "author": [
                "Haruka Kiyohara",
                "Ren Kishimoto",
                "Kosuke Kawakami",
                "Ken Kobayashi",
                "Kazuhide Nakata",
                "Yuta Saito"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18206v2",
                "http://arxiv.org/pdf/2311.18206v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18203v1",
            "title": "A Novel Interface Database of Graphene Nanoribbon from Density\n  Functional Theory",
            "updated": "2023-11-30T02:43:10Z",
            "published": "2023-11-30T02:43:10Z",
            "summary": "Interfaces play a crucial role in determining the overall performance and\nfunctionality of electronic devices and systems. Driven by the data science,\nmachine learning (ML) reveals excellent guidance for material selection and\ndevice design, in which an advanced database is crucial for training models\nwith state-of-the-art (SOTA) precision. However, a systematic database of\ninterfaces is still in its infancy due to the difficulties in collecting raw\ndata in experiment and the expensive first-principles computational cost in\ndensity functional theory (DFT). In this paper, we construct ample interface\nstructures of graphene nanoribbons (GNR), whose interfacial morphology can be\nprecisely fabricated based on specific molecular precursors. The GNR interfaces\nserve as promising candidates since their bandgaps can be modulated. Their\nphysical properties including energy bands and density of states (DOS) maps are\nobtained under reasonable calculation parameters. This database can provide\ntheoretical guidance for the design of electronic devices and accelerate the ML\nstudy of various physical quantities.",
            "author": [
                "Ao Wu",
                "Jiangxue Huang",
                "Qijun Huang",
                "Jin He",
                "Hao Wang",
                "Sheng Chang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18203v1",
                "http://arxiv.org/pdf/2311.18203v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci",
                "cond-mat.mes-hall"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02184v1",
            "title": "Channel-Feedback-Free Transmission for Downlink FD-RAN: A Radio Map\n  based Complex-valued Precoding Network Approach",
            "updated": "2023-11-30T02:41:04Z",
            "published": "2023-11-30T02:41:04Z",
            "summary": "As the demand for high-quality services proliferates, an innovative network\narchitecture, the fully-decoupled RAN (FD-RAN), has emerged for more flexible\nspectrum resource utilization and lower network costs. However, with the\ndecoupling of uplink base stations and downlink base stations in FD-RAN, the\ntraditional transmission mechanism, which relies on real-time channel feedback,\nis not suitable as the receiver is not able to feedback accurate and timely\nchannel state information to the transmitter. This paper proposes a novel\ntransmission scheme without relying on physical layer channel feedback.\nSpecifically, we design a radio map based complex-valued precoding\nnetwork~(RMCPNet) model, which outputs the base station precoding based on user\nlocation. RMCPNet comprises multiple subnets, with each subnet responsible for\nextracting unique modal features from diverse input modalities. Furthermore,\nthe multi-modal embeddings derived from these distinct subnets are integrated\nwithin the information fusion layer, culminating in a unified representation.\nWe also develop a specific RMCPNet training algorithm that employs the negative\nspectral efficiency as the loss function. We evaluate the performance of the\nproposed scheme on the public DeepMIMO dataset and show that RMCPNet can\nachieve 16\\% and 76\\% performance improvements over the conventional\nreal-valued neural network and statistical codebook approach, respectively.",
            "author": [
                "Jiwei Zhao",
                "Jiacheng Chen",
                "Zeyu Sun",
                "Yuhang Shi",
                "Haibo Zhou",
                "Xuemin",
                "Shen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02184v1",
                "http://arxiv.org/pdf/2312.02184v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "cs.LG",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00080v1",
            "title": "PDB-Struct: A Comprehensive Benchmark for Structure-based Protein Design",
            "updated": "2023-11-30T02:37:55Z",
            "published": "2023-11-30T02:37:55Z",
            "summary": "Structure-based protein design has attracted increasing interest, with\nnumerous methods being introduced in recent years. However, a universally\naccepted method for evaluation has not been established, since the wet-lab\nvalidation can be overly time-consuming for the development of new algorithms,\nand the $\\textit{in silico}$ validation with recovery and perplexity metrics is\nefficient but may not precisely reflect true foldability. To address this gap,\nwe introduce two novel metrics: refoldability-based metric, which leverages\nhigh-accuracy protein structure prediction models as a proxy for wet lab\nexperiments, and stability-based metric, which assesses whether models can\nassign high likelihoods to experimentally stable proteins. We curate datasets\nfrom high-quality CATH protein data, high-throughput $\\textit{de novo}$\ndesigned proteins, and mega-scale experimental mutagenesis experiments, and in\ndoing so, present the $\\textbf{PDB-Struct}$ benchmark that evaluates both\nrecent and previously uncompared protein design methods. Experimental results\nindicate that ByProt, ProteinMPNN, and ESM-IF perform exceptionally well on our\nbenchmark, while ESM-Design and AF-Design fall short on the refoldability\nmetric. We also show that while some methods exhibit high sequence recovery,\nthey do not perform as well on our new benchmark. Our proposed benchmark paves\nthe way for a fair and comprehensive evaluation of protein design methods in\nthe future. Code is available at https://github.com/WANG-CR/PDB-Struct.",
            "author": [
                "Chuanrui Wang",
                "Bozitao Zhong",
                "Zuobai Zhang",
                "Narendra Chaudhary",
                "Sanchit Misra",
                "Jian Tang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00080v1",
                "http://arxiv.org/pdf/2312.00080v1"
            ],
            "primary_category": "q-bio.QM",
            "category": [
                "q-bio.QM",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00079v1",
            "title": "HiFi Tuner: High-Fidelity Subject-Driven Fine-Tuning for Diffusion\n  Models",
            "updated": "2023-11-30T02:33:29Z",
            "published": "2023-11-30T02:33:29Z",
            "summary": "This paper explores advancements in high-fidelity personalized image\ngeneration through the utilization of pre-trained text-to-image diffusion\nmodels. While previous approaches have made significant strides in generating\nversatile scenes based on text descriptions and a few input images, challenges\npersist in maintaining the subject fidelity within the generated images. In\nthis work, we introduce an innovative algorithm named HiFi Tuner to enhance the\nappearance preservation of objects during personalized image generation. Our\nproposed method employs a parameter-efficient fine-tuning framework, comprising\na denoising process and a pivotal inversion process. Key enhancements include\nthe utilization of mask guidance, a novel parameter regularization technique,\nand the incorporation of step-wise subject representations to elevate the\nsample fidelity. Additionally, we propose a reference-guided generation\napproach that leverages the pivotal inversion of a reference image to mitigate\nunwanted subject variations and artifacts. We further extend our method to a\nnovel image editing task: substituting the subject in an image through textual\nmanipulations. Experimental evaluations conducted on the DreamBooth dataset\nusing the Stable Diffusion model showcase promising results. Fine-tuning solely\non textual embeddings improves CLIP-T score by 3.6 points and improves DINO\nscore by 9.6 points over Textual Inversion. When fine-tuning all parameters,\nHiFi Tuner improves CLIP-T score by 1.2 points and improves DINO score by 1.2\npoints over DreamBooth, establishing a new state of the art.",
            "author": [
                "Zhonghao Wang",
                "Wei Wei",
                "Yang Zhao",
                "Zhisheng Xiao",
                "Mark Hasegawa-Johnson",
                "Humphrey Shi",
                "Tingbo Hou"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00079v1",
                "http://arxiv.org/pdf/2312.00079v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18198v1",
            "title": "S-T CRF: Spatial-Temporal Conditional Random Field for Human Trajectory\n  Prediction",
            "updated": "2023-11-30T02:33:01Z",
            "published": "2023-11-30T02:33:01Z",
            "summary": "Trajectory prediction is of significant importance in computer vision.\nAccurate pedestrian trajectory prediction benefits autonomous vehicles and\nrobots in planning their motion. Pedestrians' trajectories are greatly\ninfluenced by their intentions. Prior studies having introduced various deep\nlearning methods only pay attention to the spatial and temporal information of\ntrajectory, overlooking the explicit intention information. In this study, we\nintroduce a novel model, termed the \\textbf{S-T CRF}:\n\\textbf{S}patial-\\textbf{T}emporal \\textbf{C}onditional \\textbf{R}andom\n\\textbf{F}ield, which judiciously incorporates intention information besides\nspatial and temporal information of trajectory. This model uses a Conditional\nRandom Field (CRF) to generate a representation of future intentions, greatly\nimproving the prediction of subsequent trajectories when combined with\nspatial-temporal representation. Furthermore, the study innovatively devises a\nspace CRF loss and a time CRF loss, meticulously designed to enhance\ninteraction constraints and temporal dynamics, respectively. Extensive\nexperimental evaluations on dataset ETH/UCY and SDD demonstrate that the\nproposed method surpasses existing baseline approaches.",
            "author": [
                "Pengqian Han",
                "Jiamou Liu",
                "Jialing He",
                "Zeyu Zhang",
                "Song Yang",
                "Yanni Tang",
                "Partha Roop"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18198v1",
                "http://arxiv.org/pdf/2311.18198v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18194v1",
            "title": "Positional Information Matters for Invariant In-Context Learning: A Case\n  Study of Simple Function Classes",
            "updated": "2023-11-30T02:26:55Z",
            "published": "2023-11-30T02:26:55Z",
            "summary": "In-context learning (ICL) refers to the ability of a model to condition on a\nfew in-context demonstrations (input-output examples of the underlying task) to\ngenerate the answer for a new query input, without updating parameters. Despite\nthe impressive ICL ability of LLMs, it has also been found that ICL in LLMs is\nsensitive to input demonstrations and limited to short context lengths. To\nunderstand the limitations and principles for successful ICL, we conduct an\ninvestigation with ICL linear regression of transformers. We characterize\nseveral Out-of-Distribution (OOD) cases for ICL inspired by realistic LLM ICL\nfailures and compare transformers with DeepSet, a simple yet powerful\narchitecture for ICL. Surprisingly, DeepSet outperforms transformers across a\nvariety of distribution shifts, implying that preserving permutation invariance\nsymmetry to input demonstrations is crucial for OOD ICL. The phenomenon\nspecifies a fundamental requirement by ICL, which we termed as ICL invariance.\nNevertheless, the positional encodings in LLMs will break ICL invariance. To\nthis end, we further evaluate transformers with identical positional encodings\nand find preserving ICL invariance in transformers achieves state-of-the-art\nperformance across various ICL distribution shifts",
            "author": [
                "Yongqiang Chen",
                "Binghui Xie",
                "Kaiwen Zhou",
                "Bo Han",
                "Yatao Bian",
                "James Cheng"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18194v1",
                "http://arxiv.org/pdf/2311.18194v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18192v1",
            "title": "Powers of the Universe: Empowering primary school students with the\n  powers of ten notation",
            "updated": "2023-11-30T02:24:12Z",
            "published": "2023-11-30T02:24:12Z",
            "summary": "Numbers, both very large and very small, are crucially important for\nunderstanding the modern world. This paper assesses trials of a mathematics and\nphysics module called Powers of the Universe in which arithmetic with extreme\nnumbers (large and small) is developed through early learning of the powers of\nten notation. We trialled a 6-hour progression of lessons based on activities\nand group learning with students aged 7-13 years. We measured students' ability\nto estimate, compare, and calculate extreme numbers using pre and post-tests to\nevaluate the program. Results demonstrated students' strong enthusiasm and\npositive learning outcomes in areas normally assumed to be beyond the\ncapability of students in this age group. We discuss the age dependence of some\nresults and suggest an optimum strategy for enhancing primary school\nmathematics. The module has been delivered, as part of a broader five-module\nprogram called Maths for Einstein's Universe, that aims to reduce maths anxiety\nthrough programs with direct relevance to the modern world and reduced emphasis\non exactness.",
            "author": [
                "Anastasia Lonshakova",
                "David G. Blair",
                "David F. Treagust",
                "Marjan Zadnik"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18192v1",
                "http://arxiv.org/pdf/2311.18192v1"
            ],
            "primary_category": "physics.ed-ph",
            "category": [
                "physics.ed-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18190v1",
            "title": "Toward the Tradeoffs between Privacy, Fairness and Utility in Federated\n  Learning",
            "updated": "2023-11-30T02:19:35Z",
            "published": "2023-11-30T02:19:35Z",
            "summary": "Federated Learning (FL) is a novel privacy-protection distributed machine\nlearning paradigm that guarantees user privacy and prevents the risk of data\nleakage due to the advantage of the client's local training. Researchers have\nstruggled to design fair FL systems that ensure fairness of results. However,\nthe interplay between fairness and privacy has been less studied. Increasing\nthe fairness of FL systems can have an impact on user privacy, while an\nincrease in user privacy can affect fairness. In this work, on the client side,\nwe use fairness metrics, such as Demographic Parity (DemP), Equalized Odds\n(EOs), and Disparate Impact (DI), to construct the local fair model. To protect\nthe privacy of the client model, we propose a privacy-protection fairness FL\nmethod. The results show that the accuracy of the fair model with privacy\nincreases because privacy breaks the constraints of the fairness metrics. In\nour experiments, we conclude the relationship between privacy, fairness and\nutility, and there is a tradeoff between these.",
            "author": [
                "Kangkang Sun",
                "Xiaojin Zhang",
                "Xi Lin",
                "Gaolei Li",
                "Jing Wang",
                "Jianhua Li"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18190v1",
                "http://arxiv.org/pdf/2311.18190v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18188v1",
            "title": "Leveraging cache to enable SLU on tiny devices",
            "updated": "2023-11-30T02:15:07Z",
            "published": "2023-11-30T02:15:07Z",
            "summary": "This paper addresses spoken language understanding (SLU) on\nmicrocontroller-like embedded devices, integrating on-device execution with\ncloud offloading in a novel fashion. We exploit temporal locality in a device's\nspeech inputs and accordingly reuse recent SLU inferences. Our idea is simple:\nlet the device match new inputs against cached results, and only offload\nunmatched inputs to the cloud for full inference. Realization of this idea,\nhowever, is non-trivial: the device needs to compare acoustic features in a\nrobust, low-cost way. To this end, we present XYZ, a speech cache for tiny\ndevices. It matches speech inputs at two levels of representations: first by\nclustered sequences of raw sound units, then as sequences of phonemes. Working\nin tandem, the two representations offer complementary cost/accuracy tradeoffs.\nTo further boost accuracy, our cache is learning: with the mismatched and then\noffloaded inputs, it continuously finetunes the device's feature extractors\n(with the assistance of the cloud). We implement XYZ on an off-the-shelf STM32\nmicrocontroller. The resultant implementation has a small memory footprint of\n2MB. Evaluated on challenging speech benchmarks, our system resolves 45%--90%\nof inputs on device, reducing the average latency by up to 80% compared to\noffloading to popular cloud speech services. Our benefit is pronounced even in\nadversarial settings -- noisy environments, cold cache, or one device shared by\na number of users.",
            "author": [
                "Afsara Benazir",
                "Zhiming Xu",
                "Felix Xiaozhu Lin"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18188v1",
                "http://arxiv.org/pdf/2311.18188v1"
            ],
            "primary_category": "eess.AS",
            "category": [
                "eess.AS",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18177v1",
            "title": "An Effective Universal Polynomial Basis for Spectral Graph Neural\n  Networks",
            "updated": "2023-11-30T01:48:42Z",
            "published": "2023-11-30T01:48:42Z",
            "summary": "Spectral Graph Neural Networks (GNNs), also referred to as graph filters have\ngained increasing prevalence for heterophily graphs. Optimal graph filters rely\non Laplacian eigendecomposition for Fourier transform. In an attempt to avert\nthe prohibitive computations, numerous polynomial filters by leveraging\ndistinct polynomials have been proposed to approximate the desired graph\nfilters. However, polynomials in the majority of polynomial filters are\npredefined and remain fixed across all graphs, failing to accommodate the\ndiverse heterophily degrees across different graphs. To tackle this issue, we\nfirst investigate the correlation between polynomial bases of desired graph\nfilters and the degrees of graph heterophily via a thorough theoretical\nanalysis. Afterward, we develop an adaptive heterophily basis by incorporating\ngraph heterophily degrees. Subsequently, we integrate this heterophily basis\nwith the homophily basis, creating a universal polynomial basis UniBasis. In\nconsequence, we devise a general polynomial filter UniFilter. Comprehensive\nexperiments on both real-world and synthetic datasets with varying heterophily\ndegrees significantly support the superiority of UniFilter, demonstrating the\neffectiveness and generality of UniBasis, as well as its promising capability\nas a new method for graph analysis.",
            "author": [
                "Keke Huang",
                "Pietro Li\u00f2"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18177v1",
                "http://arxiv.org/pdf/2311.18177v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.SI",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00078v1",
            "title": "Enhancing Cross-domain Click-Through Rate Prediction via Explicit\n  Feature Augmentation",
            "updated": "2023-11-30T01:42:11Z",
            "published": "2023-11-30T01:42:11Z",
            "summary": "Cross-domain CTR (CDCTR) prediction is an important research topic that\nstudies how to leverage meaningful data from a related domain to help CTR\nprediction in target domain. Most existing CDCTR works design implicit ways to\ntransfer knowledge across domains such as parameter-sharing that regularizes\nthe model training in target domain. More effectively, recent researchers\npropose explicit techniques to extract user interest knowledge and transfer\nthis knowledge to target domain. However, the proposed method mainly faces two\nissues: 1) it usually requires a super domain, i.e. an extremely large source\ndomain, to cover most users or items of target domain, and 2) the extracted\nuser interest knowledge is static no matter what the context is in target\ndomain. These limitations motivate us to develop a more flexible and efficient\ntechnique to explicitly transfer knowledge. In this work, we propose a\ncross-domain augmentation network (CDAnet) being able to perform explicit\nknowledge transfer between two domains. Specifically, CDAnet contains a\ndesigned translation network and an augmentation network which are trained\nsequentially. The translation network computes latent features from two domains\nand learns meaningful cross-domain knowledge of each input in target domain by\nusing a designed cross-supervised feature translator. Later the augmentation\nnetwork employs the explicit cross-domain knowledge as augmented information to\nboost the target domain CTR prediction. Through extensive experiments on two\npublic benchmarks and one industrial production dataset, we show CDAnet can\nlearn meaningful translated features and largely improve the performance of CTR\nprediction. CDAnet has been conducted online A/B test in image2product\nretrieval at Taobao app, bringing an absolute 0.11 point CTR improvement, a\nrelative 0.64% deal growth and a relative 1.26% GMV increase.",
            "author": [
                "Xu Chen",
                "Zida Cheng",
                "Jiangchao Yao",
                "Chen Ju",
                "Weilin Huang",
                "Jinsong Lan",
                "Xiaoyi Zeng",
                "Shuai Xiao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00078v1",
                "http://arxiv.org/pdf/2312.00078v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18174v1",
            "title": "Packrat: Automatic Reconfiguration for Latency Minimization in CPU-based\n  DNN Serving",
            "updated": "2023-11-30T01:36:46Z",
            "published": "2023-11-30T01:36:46Z",
            "summary": "In this paper, we investigate how to push the performance limits of serving\nDeep Neural Network (DNN) models on CPU-based servers. Specifically, we observe\nthat while intra-operator parallelism across multiple threads is an effective\nway to reduce inference latency, it provides diminishing returns. Our primary\ninsight is that instead of running a single instance of a model with all\navailable threads on a server, running multiple instances each with smaller\nbatch sizes and fewer threads for intra-op parallelism can provide lower\ninference latency. However, the right configuration is hard to determine\nmanually since it is workload- (DNN model and batch size used by the serving\nsystem) and deployment-dependent (number of CPU cores on server). We present\nPackrat, a new serving system for online inference that given a model and batch\nsize ($B$) algorithmically picks the optimal number of instances ($i$), the\nnumber of threads each should be allocated ($t$), and the batch sizes each\nshould operate on ($b$) that minimizes latency. Packrat is built as an\nextension to TorchServe and supports online reconfigurations to avoid serving\ndowntime. Averaged across a range of batch sizes, Packrat improves inference\nlatency by 1.43$\\times$ to 1.83$\\times$ on a range of commonly used DNNs.",
            "author": [
                "Ankit Bhardwaj",
                "Amar Phanishayee",
                "Deepak Narayanan",
                "Mihail Tarta",
                "Ryan Stutsman"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18174v1",
                "http://arxiv.org/pdf/2311.18174v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18168v1",
            "title": "Probabilistic Speech-Driven 3D Facial Motion Synthesis: New Benchmarks,\n  Methods, and Applications",
            "updated": "2023-11-30T01:14:43Z",
            "published": "2023-11-30T01:14:43Z",
            "summary": "We consider the task of animating 3D facial geometry from speech signal.\nExisting works are primarily deterministic, focusing on learning a one-to-one\nmapping from speech signal to 3D face meshes on small datasets with limited\nspeakers. While these models can achieve high-quality lip articulation for\nspeakers in the training set, they are unable to capture the full and diverse\ndistribution of 3D facial motions that accompany speech in the real world.\nImportantly, the relationship between speech and facial motion is one-to-many,\ncontaining both inter-speaker and intra-speaker variations and necessitating a\nprobabilistic approach. In this paper, we identify and address key challenges\nthat have so far limited the development of probabilistic models: lack of\ndatasets and metrics that are suitable for training and evaluating them, as\nwell as the difficulty of designing a model that generates diverse results\nwhile remaining faithful to a strong conditioning signal as speech. We first\npropose large-scale benchmark datasets and metrics suitable for probabilistic\nmodeling. Then, we demonstrate a probabilistic model that achieves both\ndiversity and fidelity to speech, outperforming other methods across the\nproposed benchmarks. Finally, we showcase useful applications of probabilistic\nmodels trained on these large-scale datasets: we can generate diverse\nspeech-driven 3D facial motion that matches unseen speaker styles extracted\nfrom reference clips; and our synthetic meshes can be used to improve the\nperformance of downstream audio-visual models.",
            "author": [
                "Karren D. Yang",
                "Anurag Ranjan",
                "Jen-Hao Rick Chang",
                "Raviteja Vemulapalli",
                "Oncel Tuzel"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18168v1",
                "http://arxiv.org/pdf/2311.18168v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18162v1",
            "title": "An Exponential Reduction in Training Data Sizes for Machine Learning\n  Derived Entanglement Witnesses",
            "updated": "2023-11-30T00:45:04Z",
            "published": "2023-11-30T00:45:04Z",
            "summary": "In this work, we propose an improved method of training linear support vector\nmachines (SVMs) to generate entanglement witnesses for systems of 3, 4, and 5\nqubits. SVMs generate hyperplanes represented by a weighted sum of expectation\nvalues of local observables, whose coefficients are optimized to provide a\npositive sum for all separable states and a negative sum for as many entangled\nstates as possible near a specific target state. We use the eigenstates of\ngeneralized Pauli matrices as training data, and correct the witnesses with a\ndifferential program. This method requires only $ O(6^n)$ training states,\nwhereas an existing method needs$ O(2^{4^n})$. We use this method to construct\nwitnesses of 4 and 5 qubit GHZ states with coefficients agreeing with\nstabilizer formalism witnesses to within 6.5 percent and 1 percent,\nrespectively. We also use the same training states to generate 4 and 5 qubit W\nstate witnesses. Finally, we propose methods for physical and computational\nverification of these witnesses.",
            "author": [
                "Aiden R. Rosebush",
                "Alexander C. B. Greenwood",
                "Brian T. Kirby",
                "Li Qian"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18162v1",
                "http://arxiv.org/pdf/2311.18162v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00076v1",
            "title": "Towards A Foundation Model For Trajectory Intelligence",
            "updated": "2023-11-30T00:34:09Z",
            "published": "2023-11-30T00:34:09Z",
            "summary": "We present the results of training a large trajectory model using real-world\nuser check-in data. Our approach follows a pre-train and fine-tune paradigm,\nwhere a base model is pre-trained via masked trajectory modeling and then\nadapted through fine-tuning for various downstream tasks. To address challenges\nposed by noisy data and large spatial vocabularies, we propose a novel spatial\ntokenization block. Our empirical analysis utilizes a comprehensive dataset of\nover 2 billion check-ins generated by more than 6 million users. Through\nfine-tuning on 3 downstream tasks we demonstrate that our base model has\neffectively learned valuable underlying patterns in raw data, enabling its\napplication in meaningful trajectory intelligence tasks. Despite some\nlimitations, we believe this work represents an important step forward in the\nrealization of a foundation model for trajectory intelligence.",
            "author": [
                "Alameen Najjar"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00076v1",
                "http://arxiv.org/pdf/2312.00076v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CY",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18159v1",
            "title": "Compact3D: Compressing Gaussian Splat Radiance Field Models with Vector\n  Quantization",
            "updated": "2023-11-30T00:29:13Z",
            "published": "2023-11-30T00:29:13Z",
            "summary": "3D Gaussian Splatting is a new method for modeling and rendering 3D radiance\nfields that achieves much faster learning and rendering time compared to SOTA\nNeRF methods. However, it comes with a drawback in the much larger storage\ndemand compared to NeRF methods since it needs to store the parameters for\nseveral 3D Gaussians. We notice that many Gaussians may share similar\nparameters, so we introduce a simple vector quantization method based on\n\\kmeans algorithm to quantize the Gaussian parameters. Then, we store the small\ncodebook along with the index of the code for each Gaussian. Moreover, we\ncompress the indices further by sorting them and using a method similar to\nrun-length encoding. We do extensive experiments on standard benchmarks as well\nas a new benchmark which is an order of magnitude larger than the standard\nbenchmarks. We show that our simple yet effective method can reduce the storage\ncost for the original 3D Gaussian Splatting method by a factor of almost\n$20\\times$ with a very small drop in the quality of rendered images.",
            "author": [
                "KL Navaneet",
                "Kossar Pourahmadi Meibodi",
                "Soroush Abbasi Koohpayegani",
                "Hamed Pirsiavash"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18159v1",
                "http://arxiv.org/pdf/2311.18159v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18158v1",
            "title": "HiPA: Enabling One-Step Text-to-Image Diffusion Models via\n  High-Frequency-Promoting Adaptation",
            "updated": "2023-11-30T00:14:07Z",
            "published": "2023-11-30T00:14:07Z",
            "summary": "Diffusion models have revolutionized text-to-image generation, but their\nreal-world applications are hampered by the extensive time needed for hundreds\nof diffusion steps. Although progressive distillation has been proposed to\nspeed up diffusion sampling to 2-8 steps, it still falls short in one-step\ngeneration, and necessitates training multiple student models, which is highly\nparameter-extensive and time-consuming. To overcome these limitations, we\nintroduce High-frequency-Promoting Adaptation (HiPA), a parameter-efficient\napproach to enable one-step text-to-image diffusion. Grounded in the insight\nthat high-frequency information is essential but highly lacking in one-step\ndiffusion, HiPA focuses on training one-step, low-rank adaptors to specifically\nenhance the under-represented high-frequency abilities of advanced diffusion\nmodels. The learned adaptors empower these diffusion models to generate\nhigh-quality images in just a single step. Compared with progressive\ndistillation, HiPA achieves much better performance in one-step text-to-image\ngeneration (37.3 $\\rightarrow$ 23.8 in FID-5k on MS-COCO 2017) and 28.6x\ntraining speed-up (108.8 $\\rightarrow$ 3.8 A100 GPU days), requiring only 0.04%\ntraining parameters (7,740 million $\\rightarrow$ 3.3 million). We also\ndemonstrate HiPA's effectiveness in text-guided image editing, inpainting and\nsuper-resolution tasks, where our adapted models consistently deliver\nhigh-quality outputs in just one diffusion step. The source code will be\nreleased.",
            "author": [
                "Yifan Zhang",
                "Bryan Hooi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18158v1",
                "http://arxiv.org/pdf/2311.18158v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18154v1",
            "title": "Data-Driven Shape Sensing in Continuum Manipulators via Sliding\n  Resistive Flex Sensors",
            "updated": "2023-11-29T23:55:06Z",
            "published": "2023-11-29T23:55:06Z",
            "summary": "We introduce a novel shape-sensing method using Resistive Flex Sensors (RFS)\nembedded in cable-driven Continuum Dexterous Manipulators (CDMs). The RFS is\npredominantly sensitive to deformation rather than direct forces, making it a\ndistinctive tool for shape sensing. The RFS unit we designed is a considerably\nless expensive and robust alternative, offering comparable accuracy and\nreal-time performance to existing shape sensing methods used for the CDMs\nproposed for minimally-invasive surgery. Our design allows the RFS to move\nalong and inside the CDM conforming to its curvature, offering the ability to\ncapture resistance metrics from various bending positions without the need for\nelaborate sensor setups. The RFS unit is calibrated using an overhead camera\nand a ResNet machine learning framework. Experiments using a 3D printed\nprototype of the CDM achieved an average shape estimation error of 0.968 mm\nwith a standard error of 0.275 mm. The response time of the model was\napproximately 1.16 ms, making real-time shape sensing feasible. While this\npreliminary study successfully showed the feasibility of our approach for\nC-shape CDM deformations with non-constant curvatures, we are currently\nextending the results to show the feasibility for adapting to more complex CDM\nconfigurations such as S-shape created in obstructed environments or in\npresence of the external forces.",
            "author": [
                "Chenhan Zhang",
                "Shaopeng Jiang",
                "Heyun Wang",
                "Joshua Liu",
                "Amit Jain",
                "Mehran Armand"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18154v1",
                "http://arxiv.org/pdf/2311.18154v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18149v1",
            "title": "STF: Spatial Temporal Fusion for Trajectory Prediction",
            "updated": "2023-11-29T23:31:40Z",
            "published": "2023-11-29T23:31:40Z",
            "summary": "Trajectory prediction is a challenging task that aims to predict the future\ntrajectory of vehicles or pedestrians over a short time horizon based on their\nhistorical positions. The main reason is that the trajectory is a kind of\ncomplex data, including spatial and temporal information, which is crucial for\naccurate prediction. Intuitively, the more information the model can capture,\nthe more precise the future trajectory can be predicted. However, previous\nworks based on deep learning methods processed spatial and temporal information\nseparately, leading to inadequate spatial information capture, which means they\nfailed to capture the complete spatial information. Therefore, it is of\nsignificance to capture information more fully and effectively on vehicle\ninteractions. In this study, we introduced an integrated 3D graph that\nincorporates both spatial and temporal edges. Based on this, we proposed the\nintegrated 3D graph, which considers the cross-time interaction information. In\nspecific, we design a Spatial-Temporal Fusion (STF) model including Multi-layer\nperceptions (MLP) and Graph Attention (GAT) to capture the spatial and temporal\ninformation historical trajectories simultaneously on the 3D graph. Our\nexperiment on the ApolloScape Trajectory Datasets shows that the proposed STF\noutperforms several baseline methods, especially on the long-time-horizon\ntrajectory prediction.",
            "author": [
                "Pengqian Han",
                "Partha Roop",
                "Jiamou Liu",
                "Tianzhe Bao",
                "Yifei Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18149v1",
                "http://arxiv.org/pdf/2311.18149v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18144v1",
            "title": "Dynamical phase transition in quantum neural networks with large depth",
            "updated": "2023-11-29T23:14:33Z",
            "published": "2023-11-29T23:14:33Z",
            "summary": "Understanding the training dynamics of quantum neural networks is a\nfundamental task in quantum information science with wide impact in physics,\nchemistry and machine learning. In this work, we show that the late-time\ntraining dynamics of quantum neural networks can be described by the\ngeneralized Lotka-Volterra equations, which lead to a dynamical phase\ntransition. When the targeted value of cost function crosses the minimum\nachievable value from above to below, the dynamics evolve from a frozen-kernel\nphase to a frozen-error phase, showing a duality between the quantum neural\ntangent kernel and the total error. In both phases, the convergence towards the\nfixed point is exponential, while at the critical point becomes polynomial. Via\nmapping the Hessian of the training dynamics to a Hamiltonian in the imaginary\ntime, we reveal the nature of the phase transition to be second-order with the\nexponent $\\nu=1$, where scale invariance and closing gap are observed at\ncritical point. We also provide a non-perturbative analytical theory to explain\nthe phase transition via a restricted Haar ensemble at late time, when the\noutput state approaches the steady state. The theory findings are verified\nexperimentally on IBM quantum devices.",
            "author": [
                "Bingzhi Zhang",
                "Junyu Liu",
                "Xiao-Chuan Wu",
                "Liang Jiang",
                "Quntao Zhuang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18144v1",
                "http://arxiv.org/pdf/2311.18144v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cond-mat.stat-mech",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18138v1",
            "title": "Algorithmic Persuasion Through Simulation: Information Design in the Age\n  of Generative AI",
            "updated": "2023-11-29T23:01:33Z",
            "published": "2023-11-29T23:01:33Z",
            "summary": "How can an informed sender persuade a receiver, having only limited\ninformation about the receiver's beliefs? Motivated by research showing\ngenerative AI can simulate economic agents, we initiate the study of\ninformation design with an oracle. We assume the sender can learn more about\nthe receiver by querying this oracle, e.g., by simulating the receiver's\nbehavior. Aside from AI motivations such as general-purpose Large Language\nModels (LLMs) and problem-specific machine learning models, alternate\nmotivations include customer surveys and querying a small pool of live users.\n  Specifically, we study Bayesian Persuasion where the sender has a\nsecond-order prior over the receiver's beliefs. After a fixed number of queries\nto an oracle to refine this prior, the sender commits to an information\nstructure. Upon receiving the message, the receiver takes a payoff-relevant\naction maximizing her expected utility given her posterior beliefs. We design\npolynomial-time querying algorithms that optimize the sender's expected utility\nin this Bayesian Persuasion game. As a technical contribution, we show that\nqueries form partitions of the space of receiver beliefs that can be used to\nquantify the sender's knowledge.",
            "author": [
                "Keegan Harris",
                "Nicole Immorlica",
                "Brendan Lucier",
                "Aleksandrs Slivkins"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18138v1",
                "http://arxiv.org/pdf/2311.18138v1"
            ],
            "primary_category": "cs.GT",
            "category": [
                "cs.GT",
                "cs.AI",
                "econ.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18134v1",
            "title": "A computational model of behavioral adaptation to solve the credit\n  assignment problem",
            "updated": "2023-11-29T22:53:28Z",
            "published": "2023-11-29T22:53:28Z",
            "summary": "The adaptive fitness of an organism in its ecological niche is highly reliant\nupon its ability to associate an environmental or internal stimulus with a\nbehavior response through reinforcement. This simple but powerful observation\nhas been successfully applied in a number of contexts within computational\nneuroscience and reinforcement learning to model both human and animal\nbehaviors. However, a critical challenge faced by these models is the credit\nassignment problem which asks how past behavior comes to be associated with a\ndelayed reinforcement signal. In this paper we reformulate the credit\nassignment problem to ask how past stimuli come to be linked to adaptive\nbehavioral responses in the context of a simple neuronal circuit. We propose a\nbiologically plausible variant of a spiking neural network which can model a\nwide variety of behavioral, learning, and evolutionary phenomena. Our model\nsuggests one fundamental mechanism, potentially in use in the brains of both\nsimple and complex organisms, that would allow it to associate a behavior with\nan adaptive response. We present results that showcase the model's versatility\nand biological plausibility in a number of tasks related to classical and\noperant conditioning including behavioral chaining. We then provide further\nsimulations to demonstrate how adaptive behaviors such as reflexes and simple\ncategory detection may have evolved using our model. Our results indicate the\npotential for further modifications and extensions of our model to replicate\nmore sophisticated and biologically plausible behavioral, learning, and\nintelligence phenomena found throughout the animal kingdom.",
            "author": [
                "Roy E. Clymer",
                "Sanjeev V. Namjoshi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18134v1",
                "http://arxiv.org/pdf/2311.18134v1"
            ],
            "primary_category": "q-bio.NC",
            "category": [
                "q-bio.NC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18130v1",
            "title": "The Trifecta: Three simple techniques for training deeper\n  Forward-Forward networks",
            "updated": "2023-11-29T22:44:32Z",
            "published": "2023-11-29T22:44:32Z",
            "summary": "Modern machine learning models are able to outperform humans on a variety of\nnon-trivial tasks. However, as the complexity of the models increases, they\nconsume significant amounts of power and still struggle to generalize\neffectively to unseen data. Local learning, which focuses on updating subsets\nof a model's parameters at a time, has emerged as a promising technique to\naddress these issues. Recently, a novel local learning algorithm, called\nForward-Forward, has received widespread attention due to its innovative\napproach to learning. Unfortunately, its application has been limited to\nsmaller datasets due to scalability issues. To this end, we propose The\nTrifecta, a collection of three simple techniques that synergize exceptionally\nwell and drastically improve the Forward-Forward algorithm on deeper networks.\nOur experiments demonstrate that our models are on par with similarly\nstructured, backpropagation-based models in both training speed and test\naccuracy on simple datasets. This is achieved by the ability to learn\nrepresentations that are informative locally, on a layer-by-layer basis, and\nretain their informativeness when propagated to deeper layers in the\narchitecture. This leads to around 84\\% accuracy on CIFAR-10, a notable\nimprovement (25\\%) over the original FF algorithm. These results highlight the\npotential of Forward-Forward as a genuine competitor to backpropagation and as\na promising research avenue.",
            "author": [
                "Thomas Dooms",
                "Ing Jyh Tsang",
                "Jose Oramas"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18130v1",
                "http://arxiv.org/pdf/2311.18130v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV",
                "68T07"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18129v1",
            "title": "Mixed-Precision Quantization for Federated Learning on\n  Resource-Constrained Heterogeneous Devices",
            "updated": "2023-11-29T22:43:40Z",
            "published": "2023-11-29T22:43:40Z",
            "summary": "While federated learning (FL) systems often utilize quantization to battle\ncommunication and computational bottlenecks, they have heretofore been limited\nto deploying fixed-precision quantization schemes. Meanwhile, the concept of\nmixed-precision quantization (MPQ), where different layers of a deep learning\nmodel are assigned varying bit-width, remains unexplored in the FL settings. We\npresent a novel FL algorithm, FedMPQ, which introduces mixed-precision\nquantization to resource-heterogeneous FL systems. Specifically, local models,\nquantized so as to satisfy bit-width constraint, are trained by optimizing an\nobjective function that includes a regularization term which promotes reduction\nof precision in some of the layers without significant performance degradation.\nThe server collects local model updates, de-quantizes them into full-precision\nmodels, and then aggregates them into a global model. To initialize the next\nround of local training, the server relies on the information learned in the\nprevious training round to customize bit-width assignments of the models\ndelivered to different clients. In extensive benchmarking experiments on\nseveral model architectures and different datasets in both iid and non-iid\nsettings, FedMPQ outperformed the baseline FL schemes that utilize\nfixed-precision quantization while incurring only a minor computational\noverhead on the participating devices.",
            "author": [
                "Huancheng Chen",
                "Haris Vikalo"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18129v1",
                "http://arxiv.org/pdf/2311.18129v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18128v1",
            "title": "Dynamic Scheduling of a Multiclass Queue in the Halfin-Whitt Regime: A\n  Computational Approach for High-Dimensional Problems",
            "updated": "2023-11-29T22:38:33Z",
            "published": "2023-11-29T22:38:33Z",
            "summary": "We consider a multi-class queueing model of a telephone call center, in which\na system manager dynamically allocates available servers to customer calls.\nCalls can terminate through either service completion or customer abandonment,\nand the manager strives to minimize the expected total of holding costs plus\nabandonment costs over a finite horizon. Focusing on the Halfin-Whitt heavy\ntraffic regime, we derive an approximating diffusion control problem, and\nbuilding on earlier work by Han et al. (2018), develop a simulation-based\ncomputational method for solution of such problems, one that relies heavily on\ndeep neural network technology. Using this computational method, we propose a\npolicy for the original (pre-limit) call center scheduling problem. Finally,\nthe performance of this policy is assessed using test problems based on\npublicly available call center data. For the test problems considered so far,\nour policy does as well as the best benchmark we could find. Moreover, our\nmethod is computationally feasible at least up to dimension 100, that is, for\ncall centers with 100 or more distinct customer classes.",
            "author": [
                "Bar\u0131\u015f Ata",
                "Ebru Ka\u015f\u0131karalar"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18128v1",
                "http://arxiv.org/pdf/2311.18128v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.LG",
                "cs.SY",
                "math.AP",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00073v1",
            "title": "Binary perceptrons capacity via fully lifted random duality theory",
            "updated": "2023-11-29T22:22:32Z",
            "published": "2023-11-29T22:22:32Z",
            "summary": "We study the statistical capacity of the classical binary perceptrons with\ngeneral thresholds $\\kappa$. After recognizing the connection between the\ncapacity and the bilinearly indexed (bli) random processes, we utilize a recent\nprogress in studying such processes to characterize the capacity. In\nparticular, we rely on \\emph{fully lifted} random duality theory (fl RDT)\nestablished in \\cite{Stojnicflrdt23} to create a general framework for studying\nthe perceptrons' capacities. Successful underlying numerical evaluations are\nrequired for the framework (and ultimately the entire fl RDT machinery) to\nbecome fully practically operational. We present results obtained in that\ndirections and uncover that the capacity characterizations are achieved on the\nsecond (first non-trivial) level of \\emph{stationarized} full lifting. The\nobtained results \\emph{exactly} match the replica symmetry breaking predictions\nobtained through statistical physics replica methods in \\cite{KraMez89}. Most\nnotably, for the famous zero-threshold scenario, $\\kappa=0$, we uncover the\nwell known $\\alpha\\approx0.8330786$ scaled capacity.",
            "author": [
                "Mihailo Stojnic"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00073v1",
                "http://arxiv.org/pdf/2312.00073v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR",
                "cond-mat.dis-nn",
                "cs.IT",
                "math.IT",
                "math.ST",
                "stat.ML",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00818v1",
            "title": "The perpetual motion machine of AI-generated data and the distraction of\n  ChatGPT-as-scientist",
            "updated": "2023-11-29T21:52:34Z",
            "published": "2023-11-29T21:52:34Z",
            "summary": "Since ChatGPT works so well, are we on the cusp of solving science with AI?\nIs not AlphaFold2 suggestive that the potential of LLMs in biology and the\nsciences more broadly is limitless? Can we use AI itself to bridge the lack of\ndata in the sciences in order to then train an AI? Herein we present a\ndiscussion of these topics.",
            "author": [
                "Jennifer Listgarten"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00818v1",
                "http://arxiv.org/pdf/2312.00818v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18103v1",
            "title": "Corner-to-Center Long-range Context Model for Efficient Learned Image\n  Compression",
            "updated": "2023-11-29T21:40:28Z",
            "published": "2023-11-29T21:40:28Z",
            "summary": "In the framework of learned image compression, the context model plays a\npivotal role in capturing the dependencies among latent representations. To\nreduce the decoding time resulting from the serial autoregressive context\nmodel, the parallel context model has been proposed as an alternative that\nnecessitates only two passes during the decoding phase, thus facilitating\nefficient image compression in real-world scenarios. However, performance\ndegradation occurs due to its incomplete casual context. To tackle this issue,\nwe conduct an in-depth analysis of the performance degradation observed in\nexisting parallel context models, focusing on two aspects: the Quantity and\nQuality of information utilized for context prediction and decoding. Based on\nsuch analysis, we propose the \\textbf{Corner-to-Center transformer-based\nContext Model (C$^3$M)} designed to enhance context and latent predictions and\nimprove rate-distortion performance. Specifically, we leverage the\nlogarithmic-based prediction order to predict more context features from corner\nto center progressively. In addition, to enlarge the receptive field in the\nanalysis and synthesis transformation, we use the Long-range Crossing Attention\nModule (LCAM) in the encoder/decoder to capture the long-range semantic\ninformation by assigning the different window shapes in different channels.\nExtensive experimental evaluations show that the proposed method is effective\nand outperforms the state-of-the-art parallel methods. Finally, according to\nthe subjective analysis, we suggest that improving the detailed representation\nin transformer-based image compression is a promising direction to be explored.",
            "author": [
                "Yang Sui",
                "Ding Ding",
                "Xiang Pan",
                "Xiaozhong Xu",
                "Shan Liu",
                "Bo Yuan",
                "Zhenzhong Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18103v1",
                "http://arxiv.org/pdf/2311.18103v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18102v1",
            "title": "PatchBMI-Net: Lightweight Facial Patch-based Ensemble for BMI Prediction",
            "updated": "2023-11-29T21:39:24Z",
            "published": "2023-11-29T21:39:24Z",
            "summary": "Due to an alarming trend related to obesity affecting 93.3 million adults in\nthe United States alone, body mass index (BMI) and body weight have drawn\nsignificant interest in various health monitoring applications. Consequently,\nseveral studies have proposed self-diagnostic facial image-based BMI prediction\nmethods for healthy weight monitoring. These methods have mostly used\nconvolutional neural network (CNN) based regression baselines, such as VGG19,\nResNet50, and Efficient-NetB0, for BMI prediction from facial images. However,\nthe high computational requirement of these heavy-weight CNN models limits\ntheir deployment to resource-constrained mobile devices, thus deterring weight\nmonitoring using smartphones. This paper aims to develop a lightweight facial\npatch-based ensemble (PatchBMI-Net) for BMI prediction to facilitate the\ndeployment and weight monitoring using smartphones. Extensive experiments on\nBMI-annotated facial image datasets suggest that our proposed PatchBMI-Net\nmodel can obtain Mean Absolute Error (MAE) in the range [3.58, 6.51] with a\nsize of about 3.3 million parameters. On cross-comparison with heavyweight\nmodels, such as ResNet-50 and Xception, trained for BMI prediction from facial\nimages, our proposed PatchBMI-Net obtains equivalent MAE along with the model\nsize reduction of about 5.4x and the average inference time reduction of about\n3x when deployed on Apple-14 smartphone. Thus, demonstrating performance\nefficiency as well as low latency for on-device deployment and weight\nmonitoring using smartphone applications.",
            "author": [
                "Parshuram N. Aarotale",
                "Twyla Hill",
                "Ajita Rattani"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18102v1",
                "http://arxiv.org/pdf/2311.18102v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18098v1",
            "title": "Adaptive Early Exiting for Collaborative Inference over Noisy Wireless\n  Channels",
            "updated": "2023-11-29T21:31:59Z",
            "published": "2023-11-29T21:31:59Z",
            "summary": "Collaborative inference systems are one of the emerging solutions for\ndeploying deep neural networks (DNNs) at the wireless network edge. Their main\nidea is to divide a DNN into two parts, where the first is shallow enough to be\nreliably executed at edge devices of limited computational power, while the\nsecond part is executed at an edge server with higher computational\ncapabilities. The main advantage of such systems is that the input of the DNN\ngets compressed as the subsequent layers of the shallow part extract only the\ninformation necessary for the task. As a result, significant communication\nsavings can be achieved compared to transmitting raw input samples. In this\nwork, we study early exiting in the context of collaborative inference, which\nallows obtaining inference results at the edge device for certain samples,\nwithout the need to transmit the partially processed data to the edge server at\nall, leading to further communication savings. The central part of our system\nis the transmission-decision (TD) mechanism, which, given the information from\nthe early exit, and the wireless channel conditions, decides whether to keep\nthe early exit prediction or transmit the data to the edge server for further\nprocessing. In this paper, we evaluate various TD mechanisms and show\nexperimentally, that for an image classification task over the wireless edge,\nproper utilization of early exits can provide both performance gains and\nsignificant communication savings.",
            "author": [
                "Mikolaj Jankowski",
                "Deniz Gunduz",
                "Krystian Mikolajczyk"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18098v1",
                "http://arxiv.org/pdf/2311.18098v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.IT",
                "cs.NI",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18094v1",
            "title": "Self-Driving Telescopes: Autonomous Scheduling of Astronomical\n  Observation Campaigns with Offline Reinforcement Learning",
            "updated": "2023-11-29T21:23:30Z",
            "published": "2023-11-29T21:23:30Z",
            "summary": "Modern astronomical experiments are designed to achieve multiple scientific\ngoals, from studies of galaxy evolution to cosmic acceleration. These goals\nrequire data of many different classes of night-sky objects, each of which has\na particular set of observational needs. These observational needs are\ntypically in strong competition with one another. This poses a challenging\nmulti-objective optimization problem that remains unsolved. The effectiveness\nof Reinforcement Learning (RL) as a valuable paradigm for training autonomous\nsystems has been well-demonstrated, and it may provide the basis for\nself-driving telescopes capable of optimizing the scheduling for astronomy\ncampaigns. Simulated datasets containing examples of interactions between a\ntelescope and a discrete set of sky locations on the celestial sphere can be\nused to train an RL model to sequentially gather data from these several\nlocations to maximize a cumulative reward as a measure of the quality of the\ndata gathered. We use simulated data to test and compare multiple\nimplementations of a Deep Q-Network (DQN) for the task of optimizing the\nschedule of observations from the Stone Edge Observatory (SEO). We combine\nmultiple improvements on the DQN and adjustments to the dataset, showing that\nDQNs can achieve an average reward of 87%+-6% of the maximum achievable reward\nin each state on the test set. This is the first comparison of offline RL\nalgorithms for a particular astronomical challenge and the first open-source\nframework for performing such a comparison and assessment task.",
            "author": [
                "Franco Terranova",
                "M. Voetberg",
                "Brian Nord",
                "Amanda Pagul"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18094v1",
                "http://arxiv.org/pdf/2311.18094v1"
            ],
            "primary_category": "astro-ph.IM",
            "category": [
                "astro-ph.IM",
                "astro-ph.CO",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00069v1",
            "title": "SICKLE: A Multi-Sensor Satellite Imagery Dataset Annotated with Multiple\n  Key Cropping Parameters",
            "updated": "2023-11-29T21:20:58Z",
            "published": "2023-11-29T21:20:58Z",
            "summary": "The availability of well-curated datasets has driven the success of Machine\nLearning (ML) models. Despite greater access to earth observation data in\nagriculture, there is a scarcity of curated and labelled datasets, which limits\nthe potential of its use in training ML models for remote sensing (RS) in\nagriculture. To this end, we introduce a first-of-its-kind dataset called\nSICKLE, which constitutes a time-series of multi-resolution imagery from 3\ndistinct satellites: Landsat-8, Sentinel-1 and Sentinel-2. Our dataset\nconstitutes multi-spectral, thermal and microwave sensors during January 2018 -\nMarch 2021 period. We construct each temporal sequence by considering the\ncropping practices followed by farmers primarily engaged in paddy cultivation\nin the Cauvery Delta region of Tamil Nadu, India; and annotate the\ncorresponding imagery with key cropping parameters at multiple resolutions\n(i.e. 3m, 10m and 30m). Our dataset comprises 2,370 season-wise samples from\n388 unique plots, having an average size of 0.38 acres, for classifying 21 crop\ntypes across 4 districts in the Delta, which amounts to approximately 209,000\nsatellite images. Out of the 2,370 samples, 351 paddy samples from 145 plots\nare annotated with multiple crop parameters; such as the variety of paddy, its\ngrowing season and productivity in terms of per-acre yields. Ours is also one\namong the first studies that consider the growing season activities pertinent\nto crop phenology (spans sowing, transplanting and harvesting dates) as\nparameters of interest. We benchmark SICKLE on three tasks: crop type, crop\nphenology (sowing, transplanting, harvesting), and yield prediction",
            "author": [
                "Depanshu Sani",
                "Sandeep Mahato",
                "Sourabh Saini",
                "Harsh Kumar Agarwal",
                "Charu Chandra Devshali",
                "Saket Anand",
                "Gaurav Arora",
                "Thiagarajan Jayaraman"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00069v1",
                "http://arxiv.org/pdf/2312.00069v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18090v1",
            "title": "Fleming-Viot helps speed up variational quantum algorithms in the\n  presence of barren plateaus",
            "updated": "2023-11-29T21:18:23Z",
            "published": "2023-11-29T21:18:23Z",
            "summary": "Inspired by the Fleming-Viot stochastic process, we propose a variant of\nVariational Quantum Algorithms benefitting from a parallel implementation of\nthe classical step of learning parameters of a given variational form, with the\naim of avoiding regions of the parameter space known as barren plateaus. In the\nFleming-Viot tradition, parallel searches are called particles. In our proposed\napproach, the search by a Fleming-Viot particle is stopped when it encounters a\nregion where the gradient is too small or noisy. The stopped particle continues\nthe search after being regenerated at another potentially more interesting\nlocation of the parameter space, biasing the exploration away from barren\nplateaus. We analyze the behavior of the Fleming-Viot particles from a\ntheoretical standpoint, backed up with numerical experiments on synthetic\nproblems as well as on selected instances of the Max-Cut problem on graphs,\nwhich show that our method performs better than plain-vanilla variants when\nthere are large barren plateaus.",
            "author": [
                "Daniel Mastropietro",
                "Georgios Korpas",
                "Vyacheslav Kungurtsev",
                "Jakub Marecek"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18090v1",
                "http://arxiv.org/pdf/2311.18090v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18083v1",
            "title": "Meta Co-Training: Two Views are Better than One",
            "updated": "2023-11-29T21:11:58Z",
            "published": "2023-11-29T21:11:58Z",
            "summary": "In many practical computer vision scenarios unlabeled data is plentiful, but\nlabels are scarce and difficult to obtain. As a result, semi-supervised\nlearning which leverages unlabeled data to boost the performance of supervised\nclassifiers have received significant attention in recent literature. One major\nclass of semi-supervised algorithms is co-training. In co-training two\ndifferent models leverage different independent and sufficient \"views\" of the\ndata to jointly make better predictions. During co-training each model creates\npseudo labels on unlabeled points which are used to improve the other model. We\nshow that in the common case when independent views are not available we can\nconstruct such views inexpensively using pre-trained models. Co-training on the\nconstructed views yields a performance improvement over any of the individual\nviews we construct and performance comparable with recent approaches in\nsemi-supervised learning, but has some undesirable properties. To alleviate the\nissues present with co-training we present Meta Co-Training which is an\nextension of the successful Meta Pseudo Labels approach to multiple views. Our\nmethod achieves new state-of-the-art performance on ImageNet-10% with very few\ntraining resources, as well as outperforming prior semi-supervised work on\nseveral other fine-grained image classification datasets.",
            "author": [
                "Jay C. Rothenberger",
                "Dimitrios I. Diochnos"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18083v1",
                "http://arxiv.org/pdf/2311.18083v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG",
                "I.2.6; I.4.10"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18078v1",
            "title": "The Forecastability of Underlying Building Electricity Demand from Time\n  Series Data",
            "updated": "2023-11-29T20:47:47Z",
            "published": "2023-11-29T20:47:47Z",
            "summary": "Forecasting building energy consumption has become a promising solution in\nBuilding Energy Management Systems for energy saving and optimization.\nFurthermore, it can play an important role in the efficient management of the\noperation of a smart grid. Different data-driven approaches to forecast the\nfuture energy demand of buildings at different scale, and over various time\nhorizons, can be found in the scientific literature, including extensive\nMachine Learning and Deep Learning approaches. However, the identification of\nthe most accurate forecaster model which can be utilized to predict the energy\ndemand of such a building is still challenging.In this paper, the design and\nimplementation of a data-driven approach to predict how forecastable the future\nenergy demand of a building is, without first utilizing a data-driven\nforecasting model, is presented. The investigation utilizes a historical\nelectricity consumption time series data set with a half-hour interval that has\nbeen collected from a group of residential buildings located in the City of\nLondon, United Kingdom",
            "author": [
                "Mohamad Khalil",
                "A. Stephen McGough",
                "Hussain Kazmi",
                "Sara Walker"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18078v1",
                "http://arxiv.org/pdf/2311.18078v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18076v1",
            "title": "A Nystr\u00f6m method with missing distances",
            "updated": "2023-11-29T20:43:49Z",
            "published": "2023-11-29T20:43:49Z",
            "summary": "We study the problem of determining the configuration of $n$ points, referred\nto as mobile nodes, by utilizing pairwise distances to $m$ fixed points known\nas anchor nodes. In the standard setting, we have information about the\ndistances between anchors (anchor-anchor) and between anchors and mobile nodes\n(anchor-mobile), but the distances between mobile nodes (mobile-mobile) are not\nknown. For this setup, the Nystr\\\"om method is a viable technique for\nestimating the positions of the mobile nodes. This study focuses on the setting\nwhere the anchor-mobile block of the distance matrix contains only partial\ndistance information. First, we establish a relationship between the columns of\nthe anchor-mobile block in the distance matrix and the columns of the\ncorresponding block in the Gram matrix via a graph Laplacian. Exploiting this\nconnection, we introduce a novel sampling model that frames the position\nestimation problem as low-rank recovery of an inner product matrix, given a\nsubset of its expansion coefficients in a special non-orthogonal basis. This\nbasis and its dual basis--the central elements of our model--are explicitly\nderived. Our analysis is grounded in a specific centering of the points that is\nunique to the Nystr\\\"om method. With this in mind, we extend previous work in\nEuclidean distance geometry by providing a general dual basis approach for\npoints centered anywhere.",
            "author": [
                "Samuel Lichtenberg",
                "Abiy Tasissa"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18076v1",
                "http://arxiv.org/pdf/2311.18076v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "cs.LG",
                "cs.RO",
                "eess.SP",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18074v1",
            "title": "Game Projection and Robustness for Game-Theoretic Autonomous Driving",
            "updated": "2023-11-29T20:40:02Z",
            "published": "2023-11-29T20:40:02Z",
            "summary": "Game-theoretic approaches are envisioned to bring human-like reasoning skills\nand decision-making processes for autonomous vehicles (AVs). However,\nchallenges including game complexity and incomplete information still remain to\nbe addressed before they can be sufficiently practical for real-world use. Game\ncomplexity refers to the difficulties of solving a multi-player game, which\ninclude solution existence, algorithm convergence, and scalability. To address\nthese difficulties, a potential game based framework was developed in our\nrecent work. However, conditions on cost function design need to be enforced to\nmake the game a potential game. This paper relaxes the conditions and makes the\npotential game approach applicable to more general scenarios, even including\nthe ones that cannot be molded as a potential game. Incomplete information\nrefers to the ego vehicle's lack of knowledge of other traffic agents' cost\nfunctions. Cost function deviations between the ego vehicle estimated/learned\nother agents' cost functions and their actual ones are often inevitable. This\nmotivates us to study the robustness of a game-theoretic solution. This paper\ndefines the robustness margin of a game solution as the maximum magnitude of\ncost function deviations that can be accommodated in a game without changing\nthe optimality of the game solution. With this definition, closed-form\nrobustness margins are derived. Numerical studies using highway lane-changing\nscenarios are reported.",
            "author": [
                "Mushuang Liu",
                "H. Eric Tseng",
                "Dimitar Filev",
                "Anouck Girard",
                "Ilya Kolmanovsky"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18074v1",
                "http://arxiv.org/pdf/2311.18074v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18072v1",
            "title": "Self-Supervised Learning for Large-Scale Preventive Security Constrained\n  DC Optimal Power Flow",
            "updated": "2023-11-29T20:36:35Z",
            "published": "2023-11-29T20:36:35Z",
            "summary": "Security-Constrained Optimal Power Flow (SCOPF) plays a crucial role in power\ngrid stability but becomes increasingly complex as systems grow. This paper\nintroduces PDL-SCOPF, a self-supervised end-to-end primal-dual learning\nframework for producing near-optimal solutions to large-scale SCOPF problems in\nmilliseconds. Indeed, PDL-SCOPF remedies the limitations of supervised\ncounterparts that rely on training instances with their optimal solutions,\nwhich becomes impractical for large-scale SCOPF problems. PDL-SCOPF mimics an\nAugmented Lagrangian Method (ALM) for training primal and dual networks that\nlearn the primal solutions and the Lagrangian multipliers, respectively, to the\nunconstrained optimizations. In addition, PDL-SCOPF incorporates a repair layer\nto ensure the feasibility of the power balance in the nominal case, and a\nbinary search layer to compute, using the Automatic Primary Response (APR), the\ngenerator dispatches in the contingencies. The resulting differentiable program\ncan then be trained end-to-end using the objective function of the SCOPF and\nthe power balance constraints of the contingencies. Experimental results\ndemonstrate that the PDL-SCOPF delivers accurate feasible solutions with\nminimal optimality gaps. The framework underlying PDL-SCOPF aims at bridging\nthe gap between traditional optimization methods and machine learning,\nhighlighting the potential of self-supervised end-to-end primal-dual learning\nfor large-scale optimization tasks.",
            "author": [
                "Seonho Park",
                "Pascal Van Hentenryck"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18072v1",
                "http://arxiv.org/pdf/2311.18072v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18068v2",
            "title": "ALSTER: A Local Spatio-Temporal Expert for Online 3D Semantic\n  Reconstruction",
            "updated": "2023-12-03T08:33:52Z",
            "published": "2023-11-29T20:30:18Z",
            "summary": "We propose an online 3D semantic segmentation method that incrementally\nreconstructs a 3D semantic map from a stream of RGB-D frames. Unlike offline\nmethods, ours is directly applicable to scenarios with real-time constraints,\nsuch as robotics or mixed reality. To overcome the inherent challenges of\nonline methods, we make two main contributions. First, to effectively extract\ninformation from the input RGB-D video stream, we jointly estimate geometry and\nsemantic labels per frame in 3D. A key focus of our approach is to reason about\nsemantic entities both in the 2D input and the local 3D domain to leverage\ndifferences in spatial context and network architectures. Our method predicts\n2D features using an off-the-shelf segmentation network. The extracted 2D\nfeatures are refined by a lightweight 3D network to enable reasoning about the\nlocal 3D structure. Second, to efficiently deal with an infinite stream of\ninput RGB-D frames, a subsequent network serves as a temporal expert predicting\nthe incremental scene updates by leveraging 2D, 3D, and past information in a\nlearned manner. These updates are then integrated into a global scene\nrepresentation. Using these main contributions, our method can enable scenarios\nwith real-time constraints and can scale to arbitrary scene sizes by processing\nand updating the scene only in a local region defined by the new measurement.\nOur experiments demonstrate improved results compared to existing online\nmethods that purely operate in local regions and show that complementary\nsources of information can boost the performance. We provide a thorough\nablation study on the benefits of different architectural as well as\nalgorithmic design decisions. Our method yields competitive results on the\npopular ScanNet benchmark and SceneNN dataset.",
            "author": [
                "Silvan Weder",
                "Francis Engelmann",
                "Johannes L. Sch\u00f6nberger",
                "Akihito Seki",
                "Marc Pollefeys",
                "Martin R. Oswald"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18068v2",
                "http://arxiv.org/pdf/2311.18068v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18063v1",
            "title": "TurkishBERTweet: Fast and Reliable Large Language Model for Social Media\n  Analysis",
            "updated": "2023-11-29T20:22:44Z",
            "published": "2023-11-29T20:22:44Z",
            "summary": "Turkish is one of the most popular languages in the world. Wide us of this\nlanguage on social media platforms such as Twitter, Instagram, or Tiktok and\nstrategic position of the country in the world politics makes it appealing for\nthe social network researchers and industry. To address this need, we introduce\nTurkishBERTweet, the first large scale pre-trained language model for Turkish\nsocial media built using almost 900 million tweets. The model shares the same\narchitecture as base BERT model with smaller input length, making\nTurkishBERTweet lighter than BERTurk and can have significantly lower inference\ntime. We trained our model using the same approach for RoBERTa model and\nevaluated on two text classification tasks: Sentiment Classification and Hate\nSpeech Detection. We demonstrate that TurkishBERTweet outperforms the other\navailable alternatives on generalizability and its lower inference time gives\nsignificant advantage to process large-scale datasets. We also compared our\nmodels with the commercial OpenAI solutions in terms of cost and performance to\ndemonstrate TurkishBERTweet is scalable and cost-effective solution. As part of\nour research, we released TurkishBERTweet and fine-tuned LoRA adapters for the\nmentioned tasks under the MIT License to facilitate future research and\napplications on Turkish social media. Our TurkishBERTweet model is available\nat: https://github.com/ViralLab/TurkishBERTweet",
            "author": [
                "Ali Najafi",
                "Onur Varol"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18063v1",
                "http://arxiv.org/pdf/2311.18063v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18062v1",
            "title": "Understanding Your Agent: Leveraging Large Language Models for Behavior\n  Explanation",
            "updated": "2023-11-29T20:16:23Z",
            "published": "2023-11-29T20:16:23Z",
            "summary": "Intelligent agents such as robots are increasingly deployed in real-world,\nsafety-critical settings. It is vital that these agents are able to explain the\nreasoning behind their decisions to human counterparts; however, their behavior\nis often produced by uninterpretable models such as deep neural networks. We\npropose an approach to generate natural language explanations for an agent's\nbehavior based only on observations of states and actions, thus making our\nmethod independent from the underlying model's representation. For such models,\nwe first learn a behavior representation and subsequently use it to produce\nplausible explanations with minimal hallucination while affording user\ninteraction with a pre-trained large language model. We evaluate our method in\na multi-agent search-and-rescue environment and demonstrate the effectiveness\nof our explanations for agents executing various behaviors. Through user\nstudies and empirical experiments, we show that our approach generates\nexplanations as helpful as those produced by a human domain expert while\nenabling beneficial interactions such as clarification and counterfactual\nqueries.",
            "author": [
                "Xijia Zhang",
                "Yue Guo",
                "Simon Stepputtis",
                "Katia Sycara",
                "Joseph Campbell"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18062v1",
                "http://arxiv.org/pdf/2311.18062v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18061v2",
            "title": "TransNAS-TSAD: Harnessing Transformers for Multi-Objective Neural\n  Architecture Search in Time Series Anomaly Detection",
            "updated": "2023-12-03T00:41:46Z",
            "published": "2023-11-29T20:13:32Z",
            "summary": "The surge in real-time data collection across various industries has\nunderscored the need for advanced anomaly detection in both univariate and\nmultivariate time series data. Traditional methods, while comprehensive, often\nstruggle to capture the complex interdependencies in such data. This paper\nintroduces TransNAS-TSAD, a novel framework that synergizes transformer\narchitecture with neural architecture search (NAS), enhanced through NSGA-II\nalgorithm optimization. This innovative approach effectively tackles the\ncomplexities of both univariate and multivariate time series, balancing\ncomputational efficiency with detection accuracy. Our evaluation reveals that\nTransNAS-TSAD surpasses conventional anomaly detection models, demonstrating\nmarked improvements in diverse data scenarios. We also propose the\nEfficiency-Accuracy-Complexity Score (EACS) as a new metric for assessing model\nperformance, emphasizing the crucial balance between accuracy and computational\nresources. TransNAS-TSAD sets a new benchmark in time series anomaly detection,\noffering a versatile, efficient solution for complex real-world applications.\nThis research paves the way for future developments in the field, highlighting\nits potential in a wide range of industry applications.",
            "author": [
                "Ijaz Ul Haq",
                "Byung Suk Lee"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18061v2",
                "http://arxiv.org/pdf/2311.18061v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.NE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18056v1",
            "title": "ReLU-QP: A GPU-Accelerated Quadratic Programming Solver for\n  Model-Predictive Control",
            "updated": "2023-11-29T20:06:29Z",
            "published": "2023-11-29T20:06:29Z",
            "summary": "We present ReLU-QP, a GPU-accelerated solver for quadratic programs (QPs)\nthat is capable of solving high-dimensional control problems at real-time\nrates. ReLU-QP is derived by exactly reformulating the Alternating Direction\nMethod of Multipliers (ADMM) algorithm for solving QPs as a deep, weight-tied\nneural network with rectified linear unit (ReLU) activations. This\nreformulation enables the deployment of ReLU-QP on GPUs using standard\nmachine-learning toolboxes. We evaluate the performance of ReLU-QP across three\nmodel-predictive control (MPC) benchmarks: stabilizing random linear dynamical\nsystems with control limits, balancing an Atlas humanoid robot on a single\nfoot, and tracking whole-body reference trajectories on a quadruped equipped\nwith a six-degree-of-freedom arm. These benchmarks indicate that ReLU-QP is\ncompetitive with state-of-the-art CPU-based solvers for small-to-medium-scale\nproblems and offers order-of-magnitude speed improvements for larger-scale\nproblems.",
            "author": [
                "Arun L. Bishop",
                "John Z. Zhang",
                "Swaminathan Gurumurthy",
                "Kevin Tracy",
                "Zachary Manchester"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18056v1",
                "http://arxiv.org/pdf/2311.18056v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00067v1",
            "title": "Predicting breast cancer with AI for individual risk-adjusted MRI\n  screening and early detection",
            "updated": "2023-11-29T19:52:53Z",
            "published": "2023-11-29T19:52:53Z",
            "summary": "Women with an increased life-time risk of breast cancer undergo supplemental\nannual screening MRI. We propose to predict the risk of developing breast\ncancer within one year based on the current MRI, with the objective of reducing\nscreening burden and facilitating early detection. An AI algorithm was\ndeveloped on 53,858 breasts from 12,694 patients who underwent screening or\ndiagnostic MRI and accrued over 12 years, with 2,331 confirmed cancers. A first\nU-Net was trained to segment lesions and identify regions of concern. A second\nconvolutional network was trained to detect malignant cancer using features\nextracted by the U-Net. This network was then fine-tuned to estimate the risk\nof developing cancer within a year in cases that radiologists considered normal\nor likely benign. Risk predictions from this AI were evaluated with a\nretrospective analysis of 9,183 breasts from a high-risk screening cohort,\nwhich were not used for training. Statistical analysis focused on the tradeoff\nbetween number of omitted exams versus negative predictive value, and number of\npotential early detections versus positive predictive value. The AI algorithm\nidentified regions of concern that coincided with future tumors in 52% of\nscreen-detected cancers. Upon directed review, a radiologist found that 71.3%\nof cancers had a visible correlate on the MRI prior to diagnosis, 65% of these\ncorrelates were identified by the AI model. Reevaluating these regions in 10%\nof all cases with higher AI-predicted risk could have resulted in up to 33%\nearly detections by a radiologist. Additionally, screening burden could have\nbeen reduced in 16% of lower-risk cases by recommending a later follow-up\nwithout compromising current interval cancer rate. With increasing datasets and\nimproving image quality we expect this new AI-aided, adaptive screening to\nmeaningfully reduce screening burden and improve early detection.",
            "author": [
                "Lukas Hirsch",
                "Yu Huang",
                "Hernan A. Makse",
                "Danny F. Martinez",
                "Mary Hughes",
                "Sarah Eskreis-Winkler",
                "Katja Pinker",
                "Elizabeth Morris",
                "Lucas C. Parra",
                "Elizabeth J. Sutton"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00067v1",
                "http://arxiv.org/pdf/2312.00067v1"
            ],
            "primary_category": "physics.med-ph",
            "category": [
                "physics.med-ph",
                "cs.CV",
                "cs.LG",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18048v1",
            "title": "An Interventional Perspective on Identifiability in Gaussian LTI Systems\n  with Independent Component Analysis",
            "updated": "2023-11-29T19:51:35Z",
            "published": "2023-11-29T19:51:35Z",
            "summary": "We investigate the relationship between system identification and\nintervention design in dynamical systems. While previous research demonstrated\nhow identifiable representation learning methods, such as Independent Component\nAnalysis (ICA), can reveal cause-effect relationships, it relied on a passive\nperspective without considering how to collect data. Our work shows that in\nGaussian Linear Time-Invariant (LTI) systems, the system parameters can be\nidentified by introducing diverse intervention signals in a multi-environment\nsetting. By harnessing appropriate diversity assumptions motivated by the ICA\nliterature, our findings connect experiment design and representational\nidentifiability in dynamical systems. We corroborate our findings on synthetic\nand (simulated) physical data. Additionally, we show that Hidden Markov Models,\nin general, and (Gaussian) LTI systems, in particular, fulfil a generalization\nof the Causal de Finetti theorem with continuous parameters.",
            "author": [
                "Goutham Rajendran",
                "Patrik Reizinger",
                "Wieland Brendel",
                "Pradeep Ravikumar"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18048v1",
                "http://arxiv.org/pdf/2311.18048v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CE",
                "cs.SY",
                "eess.SY",
                "stat.ME"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00066v1",
            "title": "Exploring Factors Affecting Pedestrian Crash Severity Using TabNet: A\n  Deep Learning Approach",
            "updated": "2023-11-29T19:44:52Z",
            "published": "2023-11-29T19:44:52Z",
            "summary": "This study presents the first investigation of pedestrian crash severity\nusing the TabNet model, a novel tabular deep learning method exceptionally\nsuited for analyzing the tabular data inherent in transportation safety\nresearch. Through the application of TabNet to a comprehensive dataset from\nUtah covering the years 2010 to 2022, we uncover intricate factors contributing\nto pedestrian crash severity. The TabNet model, capitalizing on its\ncompatibility with structured data, demonstrates remarkable predictive\naccuracy, eclipsing that of traditional models. It identifies critical\nvariables, such as pedestrian age, involvement in left or right turns, lighting\nconditions, and alcohol consumption, which significantly influence crash\noutcomes. The utilization of SHapley Additive exPlanations (SHAP) enhances our\nability to interpret the TabNet model's predictions, ensuring transparency and\nunderstandability in our deep learning approach. The insights derived from our\nanalysis provide a valuable compass for transportation safety engineers and\npolicymakers, enabling the identification of pivotal factors that affect\npedestrian crash severity. Such knowledge is instrumental in formulating\nprecise, data-driven interventions aimed at bolstering pedestrian safety across\ndiverse urban and rural settings.",
            "author": [
                "Amir Rafe",
                "Patrick A. Singleton"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00066v1",
                "http://arxiv.org/pdf/2312.00066v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00065v2",
            "title": "Unsupervised Keypoints from Pretrained Diffusion Models",
            "updated": "2023-12-05T19:36:01Z",
            "published": "2023-11-29T19:43:38Z",
            "summary": "Unsupervised learning of keypoints and landmarks has seen significant\nprogress with the help of modern neural network architectures, but performance\nis yet to match the supervised counterpart, making their practicability\nquestionable. We leverage the emergent knowledge within text-to-image diffusion\nmodels, towards more robust unsupervised keypoints. Our core idea is to find\ntext embeddings that would cause the generative model to consistently attend to\ncompact regions in images (i.e. keypoints). To do so, we simply optimize the\ntext embedding such that the cross-attention maps within the denoising network\nare localized as Gaussians with small standard deviations. We validate our\nperformance on multiple datasets: the CelebA, CUB-200-2011, Tai-Chi-HD,\nDeepFashion, and Human3.6m datasets. We achieve significantly improved\naccuracy, sometimes even outperforming supervised ones, particularly for data\nthat is non-aligned and less curated. Our code is publicly available and can be\nfound through our project page: https://ubc-vision.github.io/StableKeypoints/",
            "author": [
                "Eric Hedlin",
                "Gopal Sharma",
                "Shweta Mahajan",
                "Xingzhe He",
                "Hossam Isack",
                "Abhishek Kar Helge Rhodin",
                "Andrea Tagliasacchi",
                "Kwang Moo Yi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00065v2",
                "http://arxiv.org/pdf/2312.00065v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18044v1",
            "title": "Transfer Learning in Robotics: An Upcoming Breakthrough? A Review of\n  Promises and Challenges",
            "updated": "2023-11-29T19:40:10Z",
            "published": "2023-11-29T19:40:10Z",
            "summary": "Transfer learning is a conceptually-enticing paradigm in pursuit of truly\nintelligent embodied agents. The core concept -- reusing prior knowledge to\nlearn in and from novel situations -- is successfully leveraged by humans to\nhandle novel situations. In recent years, transfer learning has received\nrenewed interest from the community from different perspectives, including\nimitation learning, domain adaptation, and transfer of experience from\nsimulation to the real world, among others. In this paper, we unify the concept\nof transfer learning in robotics and provide the first taxonomy of its kind\nconsidering the key concepts of robot, task, and environment. Through a review\nof the promises and challenges in the field, we identify the need of\ntransferring at different abstraction levels, the need of quantifying the\ntransfer gap and the quality of transfer, as well as the dangers of negative\ntransfer. Via this position paper, we hope to channel the effort of the\ncommunity towards the most significant roadblocks to realize the full potential\nof transfer learning in robotics.",
            "author": [
                "No\u00e9mie Jaquier",
                "Michael C. Welle",
                "Andrej Gams",
                "Kunpeng Yao",
                "Bernardo Fichera",
                "Aude Billard",
                "Ale\u0161 Ude",
                "Tamim Asfour",
                "Danica Kragi\u0107"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18044v1",
                "http://arxiv.org/pdf/2311.18044v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18040v1",
            "title": "Evaluating Trustworthiness of AI-Enabled Decision Support Systems:\n  Validation of the Multisource AI Scorecard Table (MAST)",
            "updated": "2023-11-29T19:34:15Z",
            "published": "2023-11-29T19:34:15Z",
            "summary": "The Multisource AI Scorecard Table (MAST) is a checklist tool based on\nanalytic tradecraft standards to inform the design and evaluation of\ntrustworthy AI systems. In this study, we evaluate whether MAST is associated\nwith people's trust perceptions in AI-enabled decision support systems\n(AI-DSSs). Evaluating trust in AI-DSSs poses challenges to researchers and\npractitioners. These challenges include identifying the components,\ncapabilities, and potential of these systems, many of which are based on the\ncomplex deep learning algorithms that drive DSS performance and preclude\ncomplete manual inspection. We developed two interactive, AI-DSS test\nenvironments using the MAST criteria. One emulated an identity verification\ntask in security screening, and another emulated a text summarization system to\naid in an investigative reporting task. Each test environment had one version\ndesigned to match low-MAST ratings, and another designed to match high-MAST\nratings, with the hypothesis that MAST ratings would be positively related to\nthe trust ratings of these systems. A total of 177 subject matter experts were\nrecruited to interact with and evaluate these systems. Results generally show\nhigher MAST ratings for the high-MAST conditions compared to the low-MAST\ngroups, and that measures of trust perception are highly correlated with the\nMAST ratings. We conclude that MAST can be a useful tool for designing and\nevaluating systems that will engender high trust perceptions, including AI-DSS\nthat may be used to support visual screening and text summarization tasks.\nHowever, higher MAST ratings may not translate to higher joint performance.",
            "author": [
                "Pouria Salehi",
                "Yang Ba",
                "Nayoung Kim",
                "Ahmadreza Mosallanezhad",
                "Anna Pan",
                "Myke C. Cohen",
                "Yixuan Wang",
                "Jieqiong Zhao",
                "Shawaiz Bhatti",
                "James Sung",
                "Erik Blasch",
                "Michelle V. Mancenido",
                "Erin K. Chiou"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18040v1",
                "http://arxiv.org/pdf/2311.18040v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18035v1",
            "title": "TransOpt: Transformer-based Representation Learning for Optimization\n  Problem Classification",
            "updated": "2023-11-29T19:20:47Z",
            "published": "2023-11-29T19:20:47Z",
            "summary": "We propose a representation of optimization problem instances using a\ntransformer-based neural network architecture trained for the task of problem\nclassification of the 24 problem classes from the Black-box Optimization\nBenchmarking (BBOB) benchmark. We show that transformer-based methods can be\ntrained to recognize problem classes with accuracies in the range of 70\\%-80\\%\nfor different problem dimensions, suggesting the possible application of\ntransformer architectures in acquiring representations for black-box\noptimization problems.",
            "author": [
                "Gjorgjina Cenikj",
                "Ga\u0161per Petelin",
                "Tome Eftimov"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18035v1",
                "http://arxiv.org/pdf/2311.18035v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18034v1",
            "title": "Hyperpolyglot LLMs: Cross-Lingual Interpretability in Token Embeddings",
            "updated": "2023-11-29T19:20:14Z",
            "published": "2023-11-29T19:20:14Z",
            "summary": "Cross-lingual transfer learning is an important property of multilingual\nlarge language models (LLMs). But how do LLMs represent relationships between\nlanguages? Every language model has an input layer that maps tokens to vectors.\nThis ubiquitous layer of language models is often overlooked. We find that\nsimilarities between these input embeddings are highly interpretable and that\nthe geometry of these embeddings differs between model families. In one case\n(XLM-RoBERTa), embeddings encode language: tokens in different writing systems\ncan be linearly separated with an average of 99.2% accuracy. Another family\n(mT5) represents cross-lingual semantic similarity: the 50 nearest neighbors\nfor any token represent an average of 7.61 writing systems, and are frequently\ntranslations. This result is surprising given that there is no explicit\nparallel cross-lingual training corpora and no explicit incentive for\ntranslations in pre-training objectives. Our research opens the door for\ninvestigations in 1) The effect of pre-training and model architectures on\nrepresentations of languages and 2) The applications of cross-lingual\nrepresentations embedded in language models.",
            "author": [
                "Andrea W Wen-Yi",
                "David Mimno"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18034v1",
                "http://arxiv.org/pdf/2311.18034v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18029v1",
            "title": "A Bag of Receptive Fields for Time Series Extrinsic Predictions",
            "updated": "2023-11-29T19:13:10Z",
            "published": "2023-11-29T19:13:10Z",
            "summary": "High-dimensional time series data poses challenges due to its dynamic nature,\nvarying lengths, and presence of missing values. This kind of data requires\nextensive preprocessing, limiting the applicability of existing Time Series\nClassification and Time Series Extrinsic Regression techniques. For this\nreason, we propose BORF, a Bag-Of-Receptive-Fields model, which incorporates\nnotions from time series convolution and 1D-SAX to handle univariate and\nmultivariate time series with varying lengths and missing values. We evaluate\nBORF on Time Series Classification and Time Series Extrinsic Regression tasks\nusing the full UEA and UCR repositories, demonstrating its competitive\nperformance against state-of-the-art methods. Finally, we outline how this\nrepresentation can naturally provide saliency and feature-based explanations.",
            "author": [
                "Francesco Spinnato",
                "Riccardo Guidotti",
                "Anna Monreale",
                "Mirco Nanni"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18029v1",
                "http://arxiv.org/pdf/2311.18029v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18028v1",
            "title": "Filtered Semi-Markov CRF",
            "updated": "2023-11-29T19:11:55Z",
            "published": "2023-11-29T19:11:55Z",
            "summary": "Semi-Markov CRF has been proposed as an alternative to the traditional Linear\nChain CRF for text segmentation tasks such as Named Entity Recognition (NER).\nUnlike CRF, which treats text segmentation as token-level prediction, Semi-CRF\nconsiders segments as the basic unit, making it more expressive. However,\nSemi-CRF suffers from two major drawbacks: (1) quadratic complexity over\nsequence length, as it operates on every span of the input sequence, and (2)\ninferior performance compared to CRF for sequence labeling tasks like NER. In\nthis paper, we introduce Filtered Semi-Markov CRF, a variant of Semi-CRF that\naddresses these issues by incorporating a filtering step to eliminate\nirrelevant segments, reducing complexity and search space. Our approach is\nevaluated on several NER benchmarks, where it outperforms both CRF and Semi-CRF\nwhile being significantly faster. The implementation of our method is available\non \\href{https://github.com/urchade/Filtered-Semi-Markov-CRF}{Github}.",
            "author": [
                "Urchade Zaratiana",
                "Nadi Tomeh",
                "Niama El Khbir",
                "Pierre Holat",
                "Thierry Charnois"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18028v1",
                "http://arxiv.org/pdf/2311.18028v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18027v1",
            "title": "Enhancing Data-Assimilation in CFD using Graph Neural Networks",
            "updated": "2023-11-29T19:11:40Z",
            "published": "2023-11-29T19:11:40Z",
            "summary": "We present a novel machine learning approach for data assimilation applied in\nfluid mechanics, based on adjoint-optimization augmented by Graph Neural\nNetworks (GNNs) models. We consider as baseline the Reynolds-Averaged\nNavier-Stokes (RANS) equations, where the unknown is the meanflow and a closure\nmodel based on the Reynolds-stress tensor is required for correctly computing\nthe solution. An end-to-end process is cast; first, we train a GNN model for\nthe closure term. Second, the GNN model is introduced in the training process\nof data assimilation, where the RANS equations act as a physics constraint for\na consistent prediction. We obtain our results using direct numerical\nsimulations based on a Finite Element Method (FEM) solver; a two-fold interface\nbetween the GNN model and the solver allows the GNN's predictions to be\nincorporated into post-processing steps of the FEM analysis. The proposed\nscheme provides an excellent reconstruction of the meanflow without any\nfeatures selection; preliminary results show promising generalization\nproperties over unseen flow configurations.",
            "author": [
                "Michele Quattromini",
                "Michele Alessandro Bucci",
                "Stefania Cherubini",
                "Onofrio Semeraro"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18027v1",
                "http://arxiv.org/pdf/2311.18027v1"
            ],
            "primary_category": "physics.flu-dyn",
            "category": [
                "physics.flu-dyn",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18025v1",
            "title": "A Probabilistic Method to Predict Classifier Accuracy on Larger Datasets\n  given Small Pilot Data",
            "updated": "2023-11-29T19:10:15Z",
            "published": "2023-11-29T19:10:15Z",
            "summary": "Practitioners building classifiers often start with a smaller pilot dataset\nand plan to grow to larger data in the near future. Such projects need a\ntoolkit for extrapolating how much classifier accuracy may improve from a 2x,\n10x, or 50x increase in data size. While existing work has focused on finding a\nsingle \"best-fit\" curve using various functional forms like power laws, we\nargue that modeling and assessing the uncertainty of predictions is critical\nyet has seen less attention. In this paper, we propose a Gaussian process model\nto obtain probabilistic extrapolations of accuracy or similar performance\nmetrics as dataset size increases. We evaluate our approach in terms of error,\nlikelihood, and coverage across six datasets. Though we focus on medical tasks\nand image modalities, our open source approach generalizes to any kind of\nclassifier.",
            "author": [
                "Ethan Harvey",
                "Wansu Chen",
                "David M. Kent",
                "Michael C. Hughes"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18025v1",
                "http://arxiv.org/pdf/2311.18025v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18022v1",
            "title": "A trainable manifold for accurate approximation with ReLU Networks",
            "updated": "2023-11-29T19:09:48Z",
            "published": "2023-11-29T19:09:48Z",
            "summary": "We present a novel technique for exercising greater control of the weights of\nReLU activated neural networks to produce more accurate function\napproximations. Many theoretical works encode complex operations into ReLU\nnetworks using smaller base components. In these works, a common base component\nis a constant width approximation to x^2, which has exponentially decaying\nerror with respect to depth. We extend this block to represent a greater range\nof convex one-dimensional functions. We derive a manifold of weights such that\nthe output of these new networks utilizes exponentially many piecewise-linear\nsegments. This manifold guides their training process to overcome drawbacks\nassociated with random initialization and unassisted gradient descent. We train\nthese networks to approximate functions which do not necessarily lie on the\nmanifold, showing a significant reduction of error values over conventional\napproaches.",
            "author": [
                "Max Milkert",
                "Forrest Laine"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18022v1",
                "http://arxiv.org/pdf/2311.18022v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00817v1",
            "title": "TimelyGPT: Recurrent Convolutional Transformer for Long Time-series\n  Representation",
            "updated": "2023-11-29T19:09:28Z",
            "published": "2023-11-29T19:09:28Z",
            "summary": "Pre-trained models (PTMs) have gained prominence in Natural Language\nProcessing and Computer Vision domains. When it comes to time-series PTMs,\ntheir development has been limited. Previous research on time-series\ntransformers has mainly been devoted to small-scale tasks, yet these models\nhave not consistently outperformed traditional models. Additionally, the\nperformance of these transformers on large-scale data remains unexplored. These\nfindings raise doubts about Transformer's capabilities to scale up and capture\ntemporal dependencies. In this study, we re-examine time-series transformers\nand identify the shortcomings of prior studies. Drawing from these insights, we\nthen introduce a pioneering architecture called Timely Generative Pre-trained\nTransformer (\\model). This architecture integrates recurrent attention and\ntemporal convolution modules to effectively capture global-local temporal\ndependencies in long sequences. The relative position embedding with time decay\ncan effectively deal with trend and periodic patterns from time-series. Our\nexperiments show that \\model~excels in modeling continuously monitored\nbiosignal as well as irregularly-sampled time-series data commonly observed in\nlongitudinal electronic health records. This breakthrough suggests a priority\nshift in time-series deep learning research, moving from small-scale modeling\nfrom scratch to large-scale pre-training.",
            "author": [
                "Ziyang Song",
                "Qincheng Lu",
                "Hao Xu",
                "Yue Li"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00817v1",
                "http://arxiv.org/pdf/2312.00817v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18021v1",
            "title": "Understanding and Improving In-Context Learning on Vision-language\n  Models",
            "updated": "2023-11-29T19:08:11Z",
            "published": "2023-11-29T19:08:11Z",
            "summary": "Recently, in-context learning (ICL) on large language models (LLMs) has\nreceived great attention, and this technique can also be applied to\nvision-language models (VLMs) built upon LLMs. These VLMs can respond to\nqueries by conditioning responses on a series of multimodal demonstrations,\nwhich comprise images, queries, and answers. Though ICL has been extensively\nstudied on LLMs, its research on VLMs remains limited. The inclusion of\nadditional visual information in the demonstrations motivates the following\nresearch questions: which of the two modalities in the demonstration is more\nsignificant? How can we select effective multimodal demonstrations to enhance\nICL performance? This study investigates the significance of both visual and\nlanguage information. Our findings indicate that ICL in VLMs is predominantly\ndriven by the textual information in the demonstrations whereas the visual\ninformation in the demonstrations barely affects the ICL performance.\nSubsequently, we provide an understanding of the findings by analyzing the\nmodel information flow and comparing model inner states given different ICL\nsettings. Motivated by our analysis, we propose a simple yet effective\napproach, termed Mixed Modality In-Context Example Selection (MMICES), which\nconsiders both visual and language modalities when selecting demonstrations and\nshows better ICL performance. Extensive experiments are conducted to support\nour findings, understanding, and improvement of the ICL performance of VLMs.",
            "author": [
                "Shuo Chen",
                "Zhen Han",
                "Bailan He",
                "Mark Buckley",
                "Philip Torr",
                "Volker Tresp",
                "Jindong Gu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18021v1",
                "http://arxiv.org/pdf/2311.18021v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00063v1",
            "title": "MoMask: Generative Masked Modeling of 3D Human Motions",
            "updated": "2023-11-29T19:04:10Z",
            "published": "2023-11-29T19:04:10Z",
            "summary": "We introduce MoMask, a novel masked modeling framework for text-driven 3D\nhuman motion generation. In MoMask, a hierarchical quantization scheme is\nemployed to represent human motion as multi-layer discrete motion tokens with\nhigh-fidelity details. Starting at the base layer, with a sequence of motion\ntokens obtained by vector quantization, the residual tokens of increasing\norders are derived and stored at the subsequent layers of the hierarchy. This\nis consequently followed by two distinct bidirectional transformers. For the\nbase-layer motion tokens, a Masked Transformer is designated to predict\nrandomly masked motion tokens conditioned on text input at training stage.\nDuring generation (i.e. inference) stage, starting from an empty sequence, our\nMasked Transformer iteratively fills up the missing tokens; Subsequently, a\nResidual Transformer learns to progressively predict the next-layer tokens\nbased on the results from current layer. Extensive experiments demonstrate that\nMoMask outperforms the state-of-art methods on the text-to-motion generation\ntask, with an FID of 0.045 (vs e.g. 0.141 of T2M-GPT) on the HumanML3D dataset,\nand 0.228 (vs 0.514) on KIT-ML, respectively. MoMask can also be seamlessly\napplied in related tasks without further model fine-tuning, such as text-guided\ntemporal inpainting.",
            "author": [
                "Chuan Guo",
                "Yuxuan Mu",
                "Muhammad Gohar Javed",
                "Sen Wang",
                "Li Cheng"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00063v1",
                "http://arxiv.org/pdf/2312.00063v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18017v1",
            "title": "Learning an Effective Evolution Equation for Particle-Mesh Simulations\n  Across Cosmologies",
            "updated": "2023-11-29T19:03:37Z",
            "published": "2023-11-29T19:03:37Z",
            "summary": "Particle-mesh simulations trade small-scale accuracy for speed compared to\ntraditional, computationally expensive N-body codes in cosmological\nsimulations. In this work, we show how a data-driven model could be used to\nlearn an effective evolution equation for the particles, by correcting the\nerrors of the particle-mesh potential incurred on small scales during\nsimulations. We find that our learnt correction yields evolution equations that\ngeneralize well to new, unseen initial conditions and cosmologies. We further\ndemonstrate that the resulting corrected maps can be used in a simulation-based\ninference framework to yield an unbiased inference of cosmological parameters.\nThe model, a network implemented in Fourier space, is exclusively trained on\nthe particle positions and velocities.",
            "author": [
                "Nicolas Payot",
                "Pablo Lemos",
                "Laurence Perreault-Levasseur",
                "Carolina Cuesta-Lazaro",
                "Chirag Modi",
                "Yashar Hezaveh"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18017v1",
                "http://arxiv.org/pdf/2311.18017v1"
            ],
            "primary_category": "astro-ph.CO",
            "category": [
                "astro-ph.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18015v1",
            "title": "Predicting the Spectroscopic Features of Galaxies by Applying Manifold\n  Learning on Their Broad-Band Colors: Proof of Concept and Potential\n  Applications for Euclid, Roman, and Rubin LSST",
            "updated": "2023-11-29T19:01:34Z",
            "published": "2023-11-29T19:01:34Z",
            "summary": "Entering the era of large-scale galaxy surveys which will deliver\nunprecedented amounts of photometric and spectroscopic data, there is a growing\nneed for more efficient, data driven, and less model-dependent techniques to\nanalyze spectral energy distribution of galaxies. In this work, we demonstrate\nthat by taking advantage of manifold learning approaches, we can estimate\nspectroscopic features of large samples of galaxies from their broadband\nphotometry when spectroscopy is available only for a fraction of the sample.\nThis will be done by applying the Self Organizing Map (SOM) algorithm on\nbroadband colors of galaxies and mapping partially available spectroscopic\ninformation into the trained maps. In this pilot study, we focus on estimating\n4000A break in a magnitude-limited sample of galaxies in the COSMOS field. We\nuse observed galaxy colors (ugrizYJH) as well as spectroscopic measurements for\na fraction of the sample from LEGA-C and zCOSMOS spectroscopic surveys to\nestimate this feature for our parent photometric sample. We recover the D4000\nfeature for galaxies which only have broadband colors with uncertainties about\ntwice of the uncertainty of the employed spectroscopic surveys. Using these\nmeasurements we observe a positive correlation between D4000 and stellar mass\nof the galaxies in our sample with weaker D4000 features for higher redshift\ngalaxies at fixed stellar masses. These can be explained with downsizing\nscenario for the formation of galaxies and the decrease in their specific star\nformation rate as well as the aging of their stellar populations over this time\nperiod.",
            "author": [
                "Marziye Jafariyazani",
                "Daniel Masters",
                "Andreas Faisst",
                "Harry Teplitz",
                "Olivier Ilbert"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18015v1",
                "http://arxiv.org/pdf/2311.18015v1"
            ],
            "primary_category": "astro-ph.GA",
            "category": [
                "astro-ph.GA",
                "astro-ph.IM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18012v1",
            "title": "Bayesian Imaging for Radio Interferometry with Score-Based Priors",
            "updated": "2023-11-29T19:01:05Z",
            "published": "2023-11-29T19:01:05Z",
            "summary": "The inverse imaging task in radio interferometry is a key limiting factor to\nretrieving Bayesian uncertainties in radio astronomy in a computationally\neffective manner. We use a score-based prior derived from optical images of\ngalaxies to recover images of protoplanetary disks from the DSHARP survey. We\ndemonstrate that our method produces plausible posterior samples despite the\nmisspecified galaxy prior. We show that our approach produces results which are\ncompetitive with existing radio interferometry imaging algorithms.",
            "author": [
                "Noe Dia",
                "M. J. Yantovski-Barth",
                "Alexandre Adam",
                "Micah Bowles",
                "Pablo Lemos",
                "Anna M. M. Scaife",
                "Yashar Hezaveh",
                "Laurence Perreault-Levasseur"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18012v1",
                "http://arxiv.org/pdf/2311.18012v1"
            ],
            "primary_category": "astro-ph.IM",
            "category": [
                "astro-ph.IM",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18010v1",
            "title": "Active learning meets fractal decision boundaries: a cautionary tale\n  from the Sitnikov three-body problem",
            "updated": "2023-11-29T19:00:35Z",
            "published": "2023-11-29T19:00:35Z",
            "summary": "Chaotic systems such as the gravitational N-body problem are ubiquitous in\nastronomy. Machine learning (ML) is increasingly deployed to predict the\nevolution of such systems, e.g. with the goal of speeding up simulations.\nStrategies such as active Learning (AL) are a natural choice to optimize ML\ntraining. Here we showcase an AL failure when predicting the stability of the\nSitnikov three-body problem, the simplest case of N-body problem displaying\nchaotic behavior. We link this failure to the fractal nature of our\nclassification problem's decision boundary. This is a potential pitfall in\noptimizing large sets of N-body simulations via AL in the context of star\ncluster physics, galactic dynamics, or cosmology.",
            "author": [
                "Nicolas Payot",
                "Mario Pasquato",
                "Alessandro Alberto Trani",
                "Yashar Hezaveh",
                "Laurence Perreault-Levasseur"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18010v1",
                "http://arxiv.org/pdf/2311.18010v1"
            ],
            "primary_category": "astro-ph.GA",
            "category": [
                "astro-ph.GA",
                "astro-ph.IM",
                "nlin.CD"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18007v1",
            "title": "Towards out-of-distribution generalization in large-scale astronomical\n  surveys: robust networks learn similar representations",
            "updated": "2023-11-29T19:00:05Z",
            "published": "2023-11-29T19:00:05Z",
            "summary": "The generalization of machine learning (ML) models to out-of-distribution\n(OOD) examples remains a key challenge in extracting information from upcoming\nastronomical surveys. Interpretability approaches are a natural way to gain\ninsights into the OOD generalization problem. We use Centered Kernel Alignment\n(CKA), a similarity measure metric of neural network representations, to\nexamine the relationship between representation similarity and performance of\npre-trained Convolutional Neural Networks (CNNs) on the CAMELS Multifield\nDataset. We find that when models are robust to a distribution shift, they\nproduce substantially different representations across their layers on OOD\ndata. However, when they fail to generalize, these representations change less\nfrom layer to layer on OOD data. We discuss the potential application of\nsimilarity representation in guiding model design, training strategy, and\nmitigating the OOD problem by incorporating CKA as an inductive bias during\ntraining.",
            "author": [
                "Yash Gondhalekar",
                "Sultan Hassan",
                "Naomi Saphra",
                "Sambatra Andrianomena"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18007v1",
                "http://arxiv.org/pdf/2311.18007v1"
            ],
            "primary_category": "astro-ph.IM",
            "category": [
                "astro-ph.IM",
                "astro-ph.GA",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18002v1",
            "title": "Echoes in the Noise: Posterior Samples of Faint Galaxy Surface\n  Brightness Profiles with Score-Based Likelihoods and Priors",
            "updated": "2023-11-29T19:00:03Z",
            "published": "2023-11-29T19:00:03Z",
            "summary": "Examining the detailed structure of galaxy populations provides valuable\ninsights into their formation and evolution mechanisms. Significant barriers to\nsuch analysis are the non-trivial noise properties of real astronomical images\nand the point spread function (PSF) which blurs structure. Here we present a\nframework which combines recent advances in score-based likelihood\ncharacterization and diffusion model priors to perform a Bayesian analysis of\nimage deconvolution. The method, when applied to minimally processed\n\\emph{Hubble Space Telescope} (\\emph{HST}) data, recovers structures which have\notherwise only become visible in next-generation \\emph{James Webb Space\nTelescope} (\\emph{JWST}) imaging.",
            "author": [
                "Alexandre Adam",
                "Connor Stone",
                "Connor Bottrell",
                "Ronan Legin",
                "Yashar Hezaveh",
                "Laurence Perreault-Levasseur"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18002v1",
                "http://arxiv.org/pdf/2311.18002v1"
            ],
            "primary_category": "astro-ph.IM",
            "category": [
                "astro-ph.IM",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17987v1",
            "title": "Closing the dark photon window to thermal dark matter",
            "updated": "2023-11-29T19:00:00Z",
            "published": "2023-11-29T19:00:00Z",
            "summary": "The nature of dark matter remains a central question in particle physics,\ncosmology, and astrophysics. The prevailing hypothesis postulates that dark\nmatter consists of particles that interact only weakly with Standard Model\nparticles. However, the knowledge of dark matter properties beyond these\ninteractions is limited. This study explores a scenario involving a dark photon\nas a mediator between dark matter and the Standard Model, akin to the photon's\nrole in electromagnetism. Recent cosmological and experimental evidence impose\nconstraints on this scenario, focusing on results from direct detection\nexperiments such as PICO-60, XENON-1T, and PANDAX-4T. The results reveal severe\nconstraints, effectively closing the window for laboratory searches for dark\nphotons as mediators between the Standard Model and the dark sector (dark\nelectrons) in the secluded dark matter scenario. The findings underscore the\nneed for alternative explanations and offer fresh perspectives on the ongoing\nquest to understand dark matter and its interactions since they are nearly\nindependent of the dark electron fraction content for the total dark matter.\nThis analysis significantly narrows down the parameter space for thermal dark\nmatter scenarios involving a dark photon portal, reinforcing the urgency of\nexploring alternative models and designing new experiments to unravel the\nmysteries surrounding the nature of dark matter.",
            "author": [
                "Leon M. G. de la Vega",
                "R. Ferro-Hernandez",
                "A. Garc\u00eda-Viltres",
                "Eduardo Peinado",
                "E. V\u00e1zquez-J\u00e1uregui"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17987v1",
                "http://arxiv.org/pdf/2311.17987v1"
            ],
            "primary_category": "hep-ph",
            "category": [
                "hep-ph",
                "hep-ex",
                "hep-th"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17994v1",
            "title": "Machine Learning the Operator Content of the Critical Self-Dual\n  Ising-Higgs Gauge Model",
            "updated": "2023-11-29T19:00:00Z",
            "published": "2023-11-29T19:00:00Z",
            "summary": "We study the critical properties of the Ising-Higgs gauge theory in $(2+1)D$\nalong the self-dual line which have recently been a subject of debate. For the\nfirst time, using machine learning techniques, we determine the low energy\noperator content of the associated field theory. Our approach enables us to\nlargely refute the existence of an emergent current operator and with it the\nstanding conjecture that this transition is of the $XY^*$ universality class.\nWe contrast these results with the ones obtained for the $(2+1)D$ Ashkin-Teller\ntransverse field Ising model where we find the expected current operator. Our\nnumerical technique extends the recently proposed Real-Space Mutual Information\nallowing us to extract sub-leading non-linear operators. This allows a\ncontrolled and computationally scalable approach to target CFT spectrum and\ndiscern universality classes beyond $(1+1)D$ from Monte Carlo data.",
            "author": [
                "Lior Oppenheim",
                "Maciej Koch-Janusz",
                "Snir Gazit",
                "Zohar Ringel"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17994v1",
                "http://arxiv.org/pdf/2311.17994v1"
            ],
            "primary_category": "cond-mat.str-el",
            "category": [
                "cond-mat.str-el",
                "cond-mat.dis-nn",
                "cond-mat.stat-mech",
                "hep-lat",
                "hep-th"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17921v2",
            "title": "Do text-free diffusion models learn discriminative visual\n  representations?",
            "updated": "2023-11-30T03:02:58Z",
            "published": "2023-11-29T18:59:59Z",
            "summary": "While many unsupervised learning models focus on one family of tasks, either\ngenerative or discriminative, we explore the possibility of a unified\nrepresentation learner: a model which addresses both families of tasks\nsimultaneously. We identify diffusion models, a state-of-the-art method for\ngenerative tasks, as a prime candidate. Such models involve training a U-Net to\niteratively predict and remove noise, and the resulting model can synthesize\nhigh-fidelity, diverse, novel images. We find that the intermediate feature\nmaps of the U-Net are diverse, discriminative feature representations. We\npropose a novel attention mechanism for pooling feature maps and further\nleverage this mechanism as DifFormer, a transformer feature fusion of features\nfrom different diffusion U-Net blocks and noise steps. We also develop DifFeed,\na novel feedback mechanism tailored to diffusion. We find that diffusion models\nare better than GANs, and, with our fusion and feedback mechanisms, can compete\nwith state-of-the-art unsupervised image representation learning methods for\ndiscriminative tasks - image classification with full and semi-supervision,\ntransfer for fine-grained classification, object detection and segmentation,\nand semantic segmentation. Our project website\n(https://mgwillia.github.io/diffssl/) and code\n(https://github.com/soumik-kanad/diffssl) are available publicly.",
            "author": [
                "Soumik Mukhopadhyay",
                "Matthew Gwilliam",
                "Yosuke Yamaguchi",
                "Vatsal Agarwal",
                "Namitha Padmanabhan",
                "Archana Swaminathan",
                "Tianyi Zhou",
                "Abhinav Shrivastava"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17921v2",
                "http://arxiv.org/pdf/2311.17921v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17922v1",
            "title": "A Simple Recipe for Language-guided Domain Generalized Segmentation",
            "updated": "2023-11-29T18:59:59Z",
            "published": "2023-11-29T18:59:59Z",
            "summary": "Generalization to new domains not seen during training is one of the\nlong-standing goals and challenges in deploying neural networks in real-world\napplications. Existing generalization techniques necessitate substantial data\naugmentation, potentially sourced from external datasets, and aim at learning\ninvariant representations by imposing various alignment constraints.\nLarge-scale pretraining has recently shown promising generalization\ncapabilities, along with the potential of bridging different modalities. For\ninstance, the recent advent of vision-language models like CLIP has opened the\ndoorway for vision models to exploit the textual modality. In this paper, we\nintroduce a simple framework for generalizing semantic segmentation networks by\nemploying language as the source of randomization. Our recipe comprises three\nkey ingredients: i) the preservation of the intrinsic CLIP robustness through\nminimal fine-tuning, ii) language-driven local style augmentation, and iii)\nrandomization by locally mixing the source and augmented styles during\ntraining. Extensive experiments report state-of-the-art results on various\ngeneralization benchmarks. The code will be made available.",
            "author": [
                "Mohammad Fahes",
                "Tuan-Hung Vu",
                "Andrei Bursuc",
                "Patrick P\u00e9rez",
                "Raoul de Charette"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17922v1",
                "http://arxiv.org/pdf/2311.17922v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17916v1",
            "title": "WyCryst: Wyckoff Inorganic Crystal Generator Framework",
            "updated": "2023-11-29T18:59:31Z",
            "published": "2023-11-29T18:59:31Z",
            "summary": "Generative design marks a significant data-driven advancement in the\nexploration of novel inorganic materials, which entails learning the symmetry\nequivalent to the crystal structure prediction (CSP) task and subsequent\nlearning of their target properties. Many generative models have been developed\nin the last few years. However, these models so far lack the capacity to\nproduce crystals that obey the fundamental rules of crystal symmetry. This is\nbecause an important step in these previous approaches involves energy\nrelaxation on the generated crystal structures to find the ground state crystal\nstructure, typically using Density Functional Theory (DFT). More often than\nnot, this changes the symmetry of the structure, thereby changing the desired\nproperty and hence invalidating the original CSP. To address this, we introduce\na generative design framework (WyCryst), composed of three pivotal components:\n1) a Wyckoff position based inorganic crystal representation, 2) a\nproperty-directed VAE model and 3) an automated DFT workflow for structure\nrefinement. By implementing loss functions that punish non-realistic crystal\nstructures, our model selectively generates materials that follow the ground\ntruth of crystal symmetry in the form of Wyckoff representation for each Space\nGroup. In leave-one-out validation experiments, we successfully reproduce a\nvariety of existing materials: CaTiO3 (space group, SG No. 62 and 221), CsPbI3\n(SG No. 221), BaTiO3 (SG No. 160), and CuInS2 (SG No.122) for both ground state\nas well as polymorphic crystal structure predictions for desired compositions.\nWe also generate several new ternary materials not found in the inorganic\nmaterials database (Materials Project), which are proved to be stable,\nretaining their symmetry, and we also check their phonon stability, using our\nautomated DFT workflow highlighting the validity of our approach.",
            "author": [
                "Ruiming Zhu",
                "Wei Nong",
                "Shuya Yamazaki",
                "Kedar Hippalgaonkar"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17916v1",
                "http://arxiv.org/pdf/2311.17916v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci",
                "physics.comp-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17914v1",
            "title": "Reconstruction of electromagnetic showers in calorimeters using Deep\n  Learning",
            "updated": "2023-11-29T18:59:05Z",
            "published": "2023-11-29T18:59:05Z",
            "summary": "The precise reconstruction of properties of photons and electrons in modern\nhigh energy physics detectors, such as the CMS or Atlas experiments, plays a\ncrucial role in numerous physics results. Conventional geometrical algorithms\nare used to reconstruct the energy and position of these particles from the\nshowers they induce in the electromagnetic calorimeter. Despite their accuracy\nand efficiency, these methods still suffer from several limitations, such as\nlow-energy background and limited capacity to reconstruct close-by particles.\nThis paper introduces an innovative machine-learning technique to measure the\nenergy and position of photons and electrons based on convolutional and graph\nneural networks, taking the geometry of the CMS electromagnetic calorimeter as\nan example. The developed network demonstrates a significant improvement in\nresolution both for photon energy and position predictions compared to the\nalgorithm used in CMS. Notably, one of the main advantages of this new approach\nis its ability to better distinguish between multiple close-by electromagnetic\nshowers.",
            "author": [
                "Polina Simkina",
                "Fabrice Couderc",
                "Julie Malcl\u00e8s",
                "Mehmet \u00d6zg\u00fcr Sahin"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17914v1",
                "http://arxiv.org/pdf/2311.17914v1"
            ],
            "primary_category": "hep-ex",
            "category": [
                "hep-ex"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17910v1",
            "title": "HUGS: Human Gaussian Splats",
            "updated": "2023-11-29T18:56:32Z",
            "published": "2023-11-29T18:56:32Z",
            "summary": "Recent advances in neural rendering have improved both training and rendering\ntimes by orders of magnitude. While these methods demonstrate state-of-the-art\nquality and speed, they are designed for photogrammetry of static scenes and do\nnot generalize well to freely moving humans in the environment. In this work,\nwe introduce Human Gaussian Splats (HUGS) that represents an animatable human\ntogether with the scene using 3D Gaussian Splatting (3DGS). Our method takes\nonly a monocular video with a small number of (50-100) frames, and it\nautomatically learns to disentangle the static scene and a fully animatable\nhuman avatar within 30 minutes. We utilize the SMPL body model to initialize\nthe human Gaussians. To capture details that are not modeled by SMPL (e.g.\ncloth, hairs), we allow the 3D Gaussians to deviate from the human body model.\nUtilizing 3D Gaussians for animated humans brings new challenges, including the\nartifacts created when articulating the Gaussians. We propose to jointly\noptimize the linear blend skinning weights to coordinate the movements of\nindividual Gaussians during animation. Our approach enables novel-pose\nsynthesis of human and novel view synthesis of both the human and the scene. We\nachieve state-of-the-art rendering quality with a rendering speed of 60 FPS\nwhile being ~100x faster to train over previous work. Our code will be\nannounced here: https://github.com/apple/ml-hugs",
            "author": [
                "Muhammed Kocabas",
                "Jen-Hao Rick Chang",
                "James Gabriel",
                "Oncel Tuzel",
                "Anurag Ranjan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17910v1",
                "http://arxiv.org/pdf/2311.17910v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17901v1",
            "title": "SODA: Bottleneck Diffusion Models for Representation Learning",
            "updated": "2023-11-29T18:53:34Z",
            "published": "2023-11-29T18:53:34Z",
            "summary": "We introduce SODA, a self-supervised diffusion model, designed for\nrepresentation learning. The model incorporates an image encoder, which\ndistills a source view into a compact representation, that, in turn, guides the\ngeneration of related novel views. We show that by imposing a tight bottleneck\nbetween the encoder and a denoising decoder, and leveraging novel view\nsynthesis as a self-supervised objective, we can turn diffusion models into\nstrong representation learners, capable of capturing visual semantics in an\nunsupervised manner. To the best of our knowledge, SODA is the first diffusion\nmodel to succeed at ImageNet linear-probe classification, and, at the same\ntime, it accomplishes reconstruction, editing and synthesis tasks across a wide\nrange of datasets. Further investigation reveals the disentangled nature of its\nemergent latent space, that serves as an effective interface to control and\nmanipulate the model's produced images. All in all, we aim to shed light on the\nexciting and promising potential of diffusion models, not only for image\ngeneration, but also for learning rich and robust representations.",
            "author": [
                "Drew A. Hudson",
                "Daniel Zoran",
                "Mateusz Malinowski",
                "Andrew K. Lampinen",
                "Andrew Jaegle",
                "James L. McClelland",
                "Loic Matthey",
                "Felix Hill",
                "Alexander Lerchner"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17901v1",
                "http://arxiv.org/pdf/2311.17901v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17898v2",
            "title": "Knowledge Pursuit Prompting for Zero-Shot Multimodal Synthesis",
            "updated": "2023-11-30T18:59:01Z",
            "published": "2023-11-29T18:51:46Z",
            "summary": "Hallucinations and unfaithful synthesis due to inaccurate prompts with\ninsufficient semantic details are widely observed in multimodal generative\nmodels. A prevalent strategy to align multiple modalities is to fine-tune the\ngenerator with a large number of annotated text-image pairs. However, such a\nprocedure is labor-consuming and resource-draining. The key question we ask is:\ncan we enhance the quality and faithfulness of text-driven generative models\nbeyond extensive text-image pair annotations? To address this question, we\npropose Knowledge Pursuit Prompting (KPP), a zero-shot framework that\niteratively incorporates external knowledge to help generators produce reliable\nvisual content. Instead of training generators to handle generic prompts, KPP\nemploys a recursive knowledge query process to gather informative external\nfacts from the knowledge base, instructs a language model to compress the\nacquired knowledge for prompt refinement, and utilizes text-driven generators\nfor visual synthesis. The entire process is zero-shot, without accessing the\narchitectures and parameters of generative models. We evaluate the framework\nacross multiple text-driven generative tasks (image, 3D rendering, and video)\non datasets of different domains. We further demonstrate the extensibility and\nadaptability of KPP through varying foundation model bases and instructions.\nOur results show that KPP is capable of generating faithful and semantically\nrich content across diverse visual domains, offering a promising solution to\nimprove multimodal generative models.",
            "author": [
                "Jinqi Luo",
                "Kwan Ho Ryan Chan",
                "Dimitris Dimos",
                "Ren\u00e9 Vidal"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17898v2",
                "http://arxiv.org/pdf/2311.17898v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17983v1",
            "title": "Improving Faithfulness for Vision Transformers",
            "updated": "2023-11-29T18:51:21Z",
            "published": "2023-11-29T18:51:21Z",
            "summary": "Vision Transformers (ViTs) have achieved state-of-the-art performance for\nvarious vision tasks. One reason behind the success lies in their ability to\nprovide plausible innate explanations for the behavior of neural architectures.\nHowever, ViTs suffer from issues with explanation faithfulness, as their focal\npoints are fragile to adversarial attacks and can be easily changed with even\nslight perturbations on the input image. In this paper, we propose a rigorous\napproach to mitigate these issues by introducing Faithful ViTs (FViTs). Briefly\nspeaking, an FViT should have the following two properties: (1) The top-$k$\nindices of its self-attention vector should remain mostly unchanged under input\nperturbation, indicating stable explanations; (2) The prediction distribution\nshould be robust to perturbations. To achieve this, we propose a new method\ncalled Denoised Diffusion Smoothing (DDS), which adopts randomized smoothing\nand diffusion-based denoising. We theoretically prove that processing ViTs\ndirectly with DDS can turn them into FViTs. We also show that Gaussian noise is\nnearly optimal for both $\\ell_2$ and $\\ell_\\infty$-norm cases. Finally, we\ndemonstrate the effectiveness of our approach through comprehensive experiments\nand evaluations. Specifically, we compare our FViTs with other baselines\nthrough visual interpretation and robustness accuracy under adversarial\nattacks. Results show that FViTs are more robust against adversarial attacks\nwhile maintaining the explainability of attention, indicating higher\nfaithfulness.",
            "author": [
                "Lijie Hu",
                "Yixin Liu",
                "Ninghao Liu",
                "Mengdi Huai",
                "Lichao Sun",
                "Di Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17983v1",
                "http://arxiv.org/pdf/2311.17983v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17895v1",
            "title": "Measurements of the Thermal and Ionization State of the Intergalactic\n  Medium during the Cosmic Afternoon",
            "updated": "2023-11-29T18:48:51Z",
            "published": "2023-11-29T18:48:51Z",
            "summary": "We perform the first measurement of the thermal and ionization state of the\nintergalactic medium (IGM) across 0.9 < z < 1.5 using 301 \\lya absorption lines\nfitted from 12 HST STIS quasar spectra, with a total pathlength of \\Delta\nz=2.1. We employ the machine-learning-based inference method that uses joint\nb-N distributions obtained from \\lyaf decomposition. Our results show that the\nHI photoionization rates, \\Gamma, are in good agreement with the recent UV\nbackground synthesis models, with \\log (\\Gamma/s^{-1})={-11.79}^{0.18}_{-0.15},\n-11.98}^{0.09}_{-0.09}, and {-12.32}^{0.10}_{-0.12} at z=1.4, 1.2, and 1\nrespectively. We obtain the IGM temperature at the mean density, T_0, and the\nadiabatic index, \\gamma, as [\\log (T_0/K), \\gamma]= [{4.13}^{+0.12}_{-0.10},\n{1.34}^{+0.10}_{-0.15}], [{3.79}^{+0.11}_{-0.11}, {1.70}^{+0.09}_{-0.09}] and\n[{4.12}^{+0.15}_{-0.25}, {1.34}^{+0.21}_{-0.26}] at z=1.4, 1.2 and 1\nrespectively. Our measurements of T_0 at z=1.4 and 1.2 are consistent with the\nexpected trend from z<3 temperature measurements as well as theoretical\nexpectations that, in the absence of any non-standard heating, the IGM should\ncool down after HeII reionization. Whereas, our T_0 measurements at z=1 show\nunexpectedly high IGM temperature. However, because of the relatively large\nuncertainty in these measurements of the order of \\Delta T_0~5000 K, mostly\nemanating from the limited redshift path length of available data in these\nbins, we can not definitively conclude whether the IGM cools down at z<1.5.\nLastly, we generate a mock dataset to test the constraining power of future\nmeasurement with larger datasets. The results demonstrate that, with redshift\npathlength \\Delta z \\sim 2 for each redshift bin, three times the current\ndataset, we can constrain the T_0 of IGM within 1500K. Such precision would be\nsufficient to conclusively constrain the history of IGM thermal evolution at z\n< 1.5.",
            "author": [
                "Teng Hu",
                "Vikram Khaire",
                "Joseph F. Hennawi",
                "Todd M. Tripp",
                "Jose O\u00f1orbe",
                "Michael Walther",
                "Zarija Lukic"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17895v1",
                "http://arxiv.org/pdf/2311.17895v1"
            ],
            "primary_category": "astro-ph.CO",
            "category": [
                "astro-ph.CO",
                "astro-ph.GA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17892v1",
            "title": "A Pipeline For Discourse Circuits From CCG",
            "updated": "2023-11-29T18:46:29Z",
            "published": "2023-11-29T18:46:29Z",
            "summary": "There is a significant disconnect between linguistic theory and modern NLP\npractice, which relies heavily on inscrutable black-box architectures.\nDisCoCirc is a newly proposed model for meaning that aims to bridge this\ndivide, by providing neuro-symbolic models that incorporate linguistic\nstructure. DisCoCirc represents natural language text as a `circuit' that\ncaptures the core semantic information of the text. These circuits can then be\ninterpreted as modular machine learning models. Additionally, DisCoCirc fulfils\nanother major aim of providing an NLP model that can be implemented on\nnear-term quantum computers.\n  In this paper we describe a software pipeline that converts English text to\nits DisCoCirc representation. The pipeline achieves coverage over a large\nfragment of the English language. It relies on Combinatory Categorial Grammar\n(CCG) parses of the input text as well as coreference resolution information.\nThis semantic and syntactic information is used in several steps to convert the\ntext into a simply-typed $\\lambda$-calculus term, and then into a circuit\ndiagram. This pipeline will enable the application of the DisCoCirc framework\nto NLP tasks, using both classical and quantum approaches.",
            "author": [
                "Jonathon Liu",
                "Razin A. Shaikh",
                "Benjamin Rodatz",
                "Richie Yeung",
                "Bob Coecke"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17892v1",
                "http://arxiv.org/pdf/2311.17892v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17885v1",
            "title": "Are ensembles getting better all the time?",
            "updated": "2023-11-29T18:32:37Z",
            "published": "2023-11-29T18:32:37Z",
            "summary": "Ensemble methods combine the predictions of several base models. We study\nwhether or not including more models in an ensemble always improve its average\nperformance. Such a question depends on the kind of ensemble considered, as\nwell as the predictive metric chosen. We focus on situations where all members\nof the ensemble are a priori expected to perform as well, which is the case of\nseveral popular methods like random forests or deep ensembles. In this setting,\nwe essentially show that ensembles are getting better all the time if, and only\nif, the considered loss function is convex. More precisely, in that case, the\naverage loss of the ensemble is a decreasing function of the number of models.\nWhen the loss function is nonconvex, we show a series of results that can be\nsummarised by the insight that ensembles of good models keep getting better,\nand ensembles of bad models keep getting worse. To this end, we prove a new\nresult on the monotonicity of tail probabilities that may be of independent\ninterest. We illustrate our results on a simple machine learning problem\n(diagnosing melanomas using neural nets).",
            "author": [
                "Pierre-Alexandre Mattei",
                "Damien Garreau"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17885v1",
                "http://arxiv.org/pdf/2311.17885v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG",
                "math.ST",
                "stat.ME",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17869v1",
            "title": "SAIBench: A Structural Interpretation of AI for Science Through\n  Benchmarks",
            "updated": "2023-11-29T18:17:35Z",
            "published": "2023-11-29T18:17:35Z",
            "summary": "Artificial Intelligence for Science (AI4S) is an emerging research field that\nutilizes machine learning advancements to tackle complex scientific\ncomputational issues, aiming to enhance computational efficiency and accuracy.\nHowever, the data-driven nature of AI4S lacks the correctness or accuracy\nassurances of conventional scientific computing, posing challenges when\ndeploying AI4S models in real-world applications. To mitigate these, more\ncomprehensive benchmarking procedures are needed to better understand AI4S\nmodels. This paper introduces a novel benchmarking approach, known as\nstructural interpretation, which addresses two key requirements: identifying\nthe trusted operating range in the problem space and tracing errors back to\ntheir computational components. This method partitions both the problem and\nmetric spaces, facilitating a structural exploration of these spaces. The\npractical utility and effectiveness of structural interpretation are\nillustrated through its application to three distinct AI4S workloads:\nmachine-learning force fields (MLFF), jet tagging, and precipitation\nnowcasting. The benchmarks effectively model the trusted operating range, trace\nerrors, and reveal novel perspectives for refining the model, training process,\nand data sampling strategy. This work is part of the SAIBench project, an AI4S\nbenchmarking suite.",
            "author": [
                "Yatao Li",
                "Jianfeng Zhan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17869v1",
                "http://arxiv.org/pdf/2311.17869v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17858v1",
            "title": "On the Limits of Regression Adjustment",
            "updated": "2023-11-29T18:04:39Z",
            "published": "2023-11-29T18:04:39Z",
            "summary": "Regression adjustment, sometimes known as Controlled-experiment Using\nPre-Experiment Data (CUPED), is an important technique in internet\nexperimentation. It decreases the variance of effect size estimates, often\ncutting confidence interval widths in half or more while never making them\nworse. It does so by carefully regressing the goal metric against\npre-experiment features to reduce the variance. The tremendous gains of\nregression adjustment begs the question: How much better can we do by\nengineering better features from pre-experiment data, for example by using\nmachine learning techniques or synthetic controls? Could we even reduce the\nvariance in our effect sizes arbitrarily close to zero with the right\npredictors? Unfortunately, our answer is negative. A simple form of regression\nadjustment, which uses just the pre-experiment values of the goal metric,\ncaptures most of the benefit. Specifically, under a mild assumption that\nobservations closer in time are easier to predict that ones further away in\ntime, we upper bound the potential gains of more sophisticated feature\nengineering, with respect to the gains of this simple form of regression\nadjustment. The maximum reduction in variance is $50\\%$ in Theorem 1, or\nequivalently, the confidence interval width can be reduced by at most an\nadditional $29\\%$.",
            "author": [
                "Daniel Ting",
                "Kenneth Hung"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17858v1",
                "http://arxiv.org/pdf/2311.17858v1"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME",
                "econ.EM",
                "62P30",
                "G.3"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17856v1",
            "title": "Leveraging Graph Diffusion Models for Network Refinement Tasks",
            "updated": "2023-11-29T18:02:29Z",
            "published": "2023-11-29T18:02:29Z",
            "summary": "Most real-world networks are noisy and incomplete samples from an unknown\ntarget distribution. Refining them by correcting corruptions or inferring\nunobserved regions typically improves downstream performance. Inspired by the\nimpressive generative capabilities that have been used to correct corruptions\nin images, and the similarities between \"in-painting\" and filling in missing\nnodes and edges conditioned on the observed graph, we propose a novel graph\ngenerative framework, SGDM, which is based on subgraph diffusion. Our framework\nnot only improves the scalability and fidelity of graph diffusion models, but\nalso leverages the reverse process to perform novel, conditional generation\ntasks. In particular, through extensive empirical analysis and a set of novel\nmetrics, we demonstrate that our proposed model effectively supports the\nfollowing refinement tasks for partially observable networks: T1: denoising\nextraneous subgraphs, T2: expanding existing subgraphs and T3: performing\n\"style\" transfer by regenerating a particular subgraph to match the\ncharacteristics of a different node or subgraph.",
            "author": [
                "Puja Trivedi",
                "Ryan Rossi",
                "David Arbour",
                "Tong Yu",
                "Franck Dernoncourt",
                "Sungchul Kim",
                "Nedim Lipka",
                "Namyong Park",
                "Nesreen K. Ahmed",
                "Danai Koutra"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17856v1",
                "http://arxiv.org/pdf/2311.17856v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17855v1",
            "title": "Maximum Entropy Model Correction in Reinforcement Learning",
            "updated": "2023-11-29T18:00:41Z",
            "published": "2023-11-29T18:00:41Z",
            "summary": "We propose and theoretically analyze an approach for planning with an\napproximate model in reinforcement learning that can reduce the adverse impact\nof model error. If the model is accurate enough, it accelerates the convergence\nto the true value function too. One of its key components is the MaxEnt Model\nCorrection (MoCo) procedure that corrects the model's next-state distributions\nbased on a Maximum Entropy density estimation formulation. Based on MoCo, we\nintroduce the Model Correcting Value Iteration (MoCoVI) algorithm, and its\nsampled-based variant MoCoDyna. We show that MoCoVI and MoCoDyna's convergence\ncan be much faster than the conventional model-free algorithms. Unlike\ntraditional model-based algorithms, MoCoVI and MoCoDyna effectively utilize an\napproximate model and still converge to the correct value function.",
            "author": [
                "Amin Rakhsha",
                "Mete Kemertas",
                "Mohammad Ghavamzadeh",
                "Amir-massoud Farahmand"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17855v1",
                "http://arxiv.org/pdf/2311.17855v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.SY",
                "eess.SY",
                "math.OC",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17853v2",
            "title": "On the Adversarial Robustness of Graph Contrastive Learning Methods",
            "updated": "2023-11-30T19:03:33Z",
            "published": "2023-11-29T17:59:18Z",
            "summary": "Contrastive learning (CL) has emerged as a powerful framework for learning\nrepresentations of images and text in a self-supervised manner while enhancing\nmodel robustness against adversarial attacks. More recently, researchers have\nextended the principles of contrastive learning to graph-structured data,\ngiving birth to the field of graph contrastive learning (GCL). However, whether\nGCL methods can deliver the same advantages in adversarial robustness as their\ncounterparts in the image and text domains remains an open question. In this\npaper, we introduce a comprehensive robustness evaluation protocol tailored to\nassess the robustness of GCL models. We subject these models to adaptive\nadversarial attacks targeting the graph structure, specifically in the evasion\nscenario. We evaluate node and graph classification tasks using diverse\nreal-world datasets and attack strategies. With our work, we aim to offer\ninsights into the robustness of GCL methods and hope to open avenues for\npotential future research directions.",
            "author": [
                "Filippo Guerranti",
                "Zinuo Yi",
                "Anna Starovoit",
                "Rafiq Kamel",
                "Simon Geisler",
                "Stephan G\u00fcnnemann"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17853v2",
                "http://arxiv.org/pdf/2311.17853v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17852v1",
            "title": "A Computing-in-Memory-based One-Class Hyperdimensional Computing Model\n  for Outlier Detection",
            "updated": "2023-11-29T17:56:51Z",
            "published": "2023-11-29T17:56:51Z",
            "summary": "In this work, we present ODHD, an algorithm for outlier detection based on\nhyperdimensional computing (HDC), a non-classical learning paradigm. Along with\nthe HDC-based algorithm, we propose IM-ODHD, a computing-in-memory (CiM)\nimplementation based on hardware/software (HW/SW) codesign for improved latency\nand energy efficiency. The training and testing phases of ODHD may be performed\nwith conventional CPU/GPU hardware or our IM-ODHD, SRAM-based CiM architecture\nusing the proposed HW/SW codesign techniques. We evaluate the performance of\nODHD on six datasets from different application domains using three metrics,\nnamely accuracy, F1 score, and ROC-AUC, and compare it with multiple baseline\nmethods such as OCSVM, isolation forest, and autoencoder. The experimental\nresults indicate that ODHD outperforms all the baseline methods in terms of\nthese three metrics on every dataset for both CPU/GPU and CiM implementations.\nFurthermore, we perform an extensive design space exploration to demonstrate\nthe tradeoff between delay, energy efficiency, and performance of ODHD. We\ndemonstrate that the HW/SW codesign implementation of the outlier detection on\nIM-ODHD is able to outperform the GPU-based implementation of ODHD by at least\n293x/419x in terms of training/testing latency (and on average 16.0x/15.9x in\nterms of training/testing energy consumption).",
            "author": [
                "Ruixuan Wang",
                "Sabrina Hassan Moon",
                "Xiaobo Sharon Hu",
                "Xun Jiao",
                "Dayane Reis"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17852v1",
                "http://arxiv.org/pdf/2311.17852v1"
            ],
            "primary_category": "cs.AR",
            "category": [
                "cs.AR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17851v1",
            "title": "Evaluating VLMs for Score-Based, Multi-Probe Annotation of 3D Objects",
            "updated": "2023-11-29T17:54:22Z",
            "published": "2023-11-29T17:54:22Z",
            "summary": "Unlabeled 3D objects present an opportunity to leverage pretrained vision\nlanguage models (VLMs) on a range of annotation tasks -- from describing object\nsemantics to physical properties. An accurate response must take into account\nthe full appearance of the object in 3D, various ways of phrasing the\nquestion/prompt, and changes in other factors that affect the response. We\npresent a method to marginalize over any factors varied across VLM queries,\nutilizing the VLM's scores for sampled responses. We first show that this\nprobabilistic aggregation can outperform a language model (e.g., GPT4) for\nsummarization, for instance avoiding hallucinations when there are contrasting\ndetails between responses. Secondly, we show that aggregated annotations are\nuseful for prompt-chaining; they help improve downstream VLM predictions (e.g.,\nof object material when the object's type is specified as an auxiliary input in\nthe prompt). Such auxiliary inputs allow ablating and measuring the\ncontribution of visual reasoning over language-only reasoning. Using these\nevaluations, we show how VLMs can approach, without additional training or\nin-context learning, the quality of human-verified type and material\nannotations on the large-scale Objaverse dataset.",
            "author": [
                "Rishabh Kabra",
                "Loic Matthey",
                "Alexander Lerchner",
                "Niloy J. Mitra"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17851v1",
                "http://arxiv.org/pdf/2311.17851v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17846v1",
            "title": "Towards Real-World Focus Stacking with Deep Learning",
            "updated": "2023-11-29T17:49:33Z",
            "published": "2023-11-29T17:49:33Z",
            "summary": "Focus stacking is widely used in micro, macro, and landscape photography to\nreconstruct all-in-focus images from multiple frames obtained with focus\nbracketing, that is, with shallow depth of field and different focus planes.\nExisting deep learning approaches to the underlying multi-focus image fusion\nproblem have limited applicability to real-world imagery since they are\ndesigned for very short image sequences (two to four images), and are typically\ntrained on small, low-resolution datasets either acquired by light-field\ncameras or generated synthetically. We introduce a new dataset consisting of 94\nhigh-resolution bursts of raw images with focus bracketing, with pseudo ground\ntruth computed from the data using state-of-the-art commercial software. This\ndataset is used to train the first deep learning algorithm for focus stacking\ncapable of handling bursts of sufficient length for real-world applications.\nQualitative experiments demonstrate that it is on par with existing commercial\nsolutions in the long-burst, realistic regime while being significantly more\ntolerant to noise. The code and dataset are available at\nhttps://github.com/araujoalexandre/FocusStackingDataset.",
            "author": [
                "Alexandre Araujo",
                "Jean Ponce",
                "Julien Mairal"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17846v1",
                "http://arxiv.org/pdf/2311.17846v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17842v1",
            "title": "Look Before You Leap: Unveiling the Power of GPT-4V in Robotic\n  Vision-Language Planning",
            "updated": "2023-11-29T17:46:25Z",
            "published": "2023-11-29T17:46:25Z",
            "summary": "In this study, we are interested in imbuing robots with the capability of\nphysically-grounded task planning. Recent advancements have shown that large\nlanguage models (LLMs) possess extensive knowledge useful in robotic tasks,\nespecially in reasoning and planning. However, LLMs are constrained by their\nlack of world grounding and dependence on external affordance models to\nperceive environmental information, which cannot jointly reason with LLMs. We\nargue that a task planner should be an inherently grounded, unified multimodal\nsystem. To this end, we introduce Robotic Vision-Language Planning (ViLa), a\nnovel approach for long-horizon robotic planning that leverages vision-language\nmodels (VLMs) to generate a sequence of actionable steps. ViLa directly\nintegrates perceptual data into its reasoning and planning process, enabling a\nprofound understanding of commonsense knowledge in the visual world, including\nspatial layouts and object attributes. It also supports flexible multimodal\ngoal specification and naturally incorporates visual feedback. Our extensive\nevaluation, conducted in both real-robot and simulated environments,\ndemonstrates ViLa's superiority over existing LLM-based planners, highlighting\nits effectiveness in a wide array of open-world manipulation tasks.",
            "author": [
                "Yingdong Hu",
                "Fanqi Lin",
                "Tong Zhang",
                "Li Yi",
                "Yang Gao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17842v1",
                "http://arxiv.org/pdf/2311.17842v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI",
                "cs.CL",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17840v1",
            "title": "A quasi-polynomial time algorithm for Multi-Dimensional Scaling via LP\n  hierarchies",
            "updated": "2023-11-29T17:42:05Z",
            "published": "2023-11-29T17:42:05Z",
            "summary": "Multi-dimensional Scaling (MDS) is a family of methods for embedding\npair-wise dissimilarities between $n$ objects into low-dimensional space. MDS\nis widely used as a data visualization tool in the social and biological\nsciences, statistics, and machine learning. We study the Kamada-Kawai\nformulation of MDS: given a set of non-negative dissimilarities $\\{d_{i,j}\\}_{i\n, j \\in [n]}$ over $n$ points, the goal is to find an embedding\n$\\{x_1,\\dots,x_n\\} \\subset \\mathbb{R}^k$ that minimizes \\[ \\text{OPT} =\n\\min_{x} \\mathbb{E}_{i,j \\in [n]} \\left[ \\left(1-\\frac{\\|x_i -\nx_j\\|}{d_{i,j}}\\right)^2 \\right] \\]\n  Despite its popularity, our theoretical understanding of MDS is extremely\nlimited. Recently, Demaine, Hesterberg, Koehler, Lynch, and Urschel\n(arXiv:2109.11505) gave the first approximation algorithm with provable\nguarantees for Kamada-Kawai, which achieves an embedding with cost $\\text{OPT}\n+\\epsilon$ in $n^2 \\cdot 2^{\\tilde{\\mathcal{O}}(k \\Delta^4 / \\epsilon^2)}$\ntime, where $\\Delta$ is the aspect ratio of the input dissimilarities. In this\nwork, we give the first approximation algorithm for MDS with quasi-polynomial\ndependency on $\\Delta$: for target dimension $k$, we achieve a solution with\ncost $\\mathcal{O}(\\text{OPT}^{ \\hspace{0.04in}1/k } \\cdot \\log(\\Delta/\\epsilon)\n)+ \\epsilon$ in time $n^{ \\mathcal{O}(1)} \\cdot 2^{\\tilde{\\mathcal{O}}( k^2\n(\\log(\\Delta)/\\epsilon)^{k/2 + 1} ) }$.\n  Our approach is based on a novel analysis of a conditioning-based rounding\nscheme for the Sherali-Adams LP Hierarchy. Crucially, our analysis exploits the\ngeometry of low-dimensional Euclidean space, allowing us to avoid an\nexponential dependence on the aspect ratio $\\Delta$. We believe our\ngeometry-aware treatment of the Sherali-Adams Hierarchy is an important step\ntowards developing general-purpose techniques for efficient metric optimization\nalgorithms.",
            "author": [
                "Ainesh Bakshi",
                "Vincent Cohen-Addad",
                "Samuel B. Hopkins",
                "Rajesh Jayaram",
                "Silvio Lattanzi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17840v1",
                "http://arxiv.org/pdf/2311.17840v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17834v2",
            "title": "SPiC-E : Structural Priors in 3D Diffusion Models using Cross-Entity\n  Attention",
            "updated": "2023-11-30T12:59:21Z",
            "published": "2023-11-29T17:36:49Z",
            "summary": "We are witnessing rapid progress in automatically generating and manipulating\n3D assets due to the availability of pretrained text-image diffusion models.\nHowever, time-consuming optimization procedures are required for synthesizing\neach sample, hindering their potential for democratizing 3D content creation.\nConversely, 3D diffusion models now train on million-scale 3D datasets,\nyielding high-quality text-conditional 3D samples within seconds. In this work,\nwe present SPiC-E - a neural network that adds structural guidance to 3D\ndiffusion models, extending their usage beyond text-conditional generation. At\nits core, our framework introduces a cross-entity attention mechanism that\nallows for multiple entities (in particular, paired input and guidance 3D\nshapes) to interact via their internal representations within the denoising\nnetwork. We utilize this mechanism for learning task-specific structural priors\nin 3D diffusion models from auxiliary guidance shapes. We show that our\napproach supports a variety of applications, including 3D stylization, semantic\nshape editing and text-conditional abstraction-to-3D, which transforms\nprimitive-based abstractions into highly-expressive shapes. Extensive\nexperiments demonstrate that SPiC-E achieves SOTA performance over these tasks\nwhile often being considerably faster than alternative methods. Importantly,\nthis is accomplished without tailoring our approach for any specific task.",
            "author": [
                "Etai Sella",
                "Gal Fiebelman",
                "Noam Atia",
                "Hadar Averbuch-Elor"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17834v2",
                "http://arxiv.org/pdf/2311.17834v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17833v1",
            "title": "Analyzing and Explaining Image Classifiers via Diffusion Guidance",
            "updated": "2023-11-29T17:35:29Z",
            "published": "2023-11-29T17:35:29Z",
            "summary": "While deep learning has led to huge progress in complex image classification\ntasks like ImageNet, unexpected failure modes, e.g. via spurious features, call\ninto question how reliably these classifiers work in the wild. Furthermore, for\nsafety-critical tasks the black-box nature of their decisions is problematic,\nand explanations or at least methods which make decisions plausible are needed\nurgently. In this paper, we address these problems by generating images that\noptimize a classifier-derived objective using a framework for guided image\ngeneration. We analyze the behavior and decisions of image classifiers by\nvisual counterfactual explanations (VCEs), detection of systematic mistakes by\nanalyzing images where classifiers maximally disagree, and visualization of\nneurons to verify potential spurious features. In this way, we validate\nexisting observations, e.g. the shape bias of adversarially robust models, as\nwell as novel failure modes, e.g. systematic errors of zero-shot CLIP\nclassifiers, or identify harmful spurious features. Moreover, our VCEs\noutperform previous work while being more versatile.",
            "author": [
                "Maximilian Augustin",
                "Yannic Neuhaus",
                "Matthias Hein"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17833v1",
                "http://arxiv.org/pdf/2311.17833v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17978v1",
            "title": "AutArch: An AI-assisted workflow for object detection and automated\n  recording in archaeological catalogues",
            "updated": "2023-11-29T17:24:04Z",
            "published": "2023-11-29T17:24:04Z",
            "summary": "Compiling large datasets from published resources, such as archaeological\nfind catalogues presents fundamental challenges: identifying relevant content\nand manually recording it is a time-consuming, repetitive and error-prone task.\nFor the data to be useful, it must be of comparable quality and adhere to the\nsame recording standards, which is hardly ever the case in archaeology. Here,\nwe present a new data collection method exploiting recent advances in\nArtificial Intelligence. Our software uses an object detection neural network\ncombined with further classification networks to speed up, automate, and\nstandardise data collection from legacy resources, such as archaeological\ndrawings and photographs in large unsorted PDF files. The AI-assisted workflow\ndetects common objects found in archaeological catalogues, such as graves,\nskeletons, ceramics, ornaments, stone tools and maps, and spatially relates and\nanalyses these objects on the page to extract real-life attributes, such as the\nsize and orientation of a grave based on the north arrow and the scale. A\ngraphical interface allows for and assists with manual validation. We\ndemonstrate the benefits of this approach by collecting a range of shapes and\nnumerical attributes from richly-illustrated archaeological catalogues, and\nbenchmark it in a real-world experiment with ten users. Moreover, we record\ngeometric whole-outlines through contour detection, an alternative to\nlandmark-based geometric morphometrics not achievable by hand.",
            "author": [
                "Kevin Klein",
                "Alyssa Wohde",
                "Alexander V. Gorelik",
                "Volker Heyd",
                "Yoan Diekmann",
                "Maxime Brami"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17978v1",
                "http://arxiv.org/pdf/2311.17978v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17815v1",
            "title": "A Survey on Design Methodologies for Accelerating Deep Learning on\n  Heterogeneous Architectures",
            "updated": "2023-11-29T17:10:16Z",
            "published": "2023-11-29T17:10:16Z",
            "summary": "In recent years, the field of Deep Learning has seen many disruptive and\nimpactful advancements. Given the increasing complexity of deep neural\nnetworks, the need for efficient hardware accelerators has become more and more\npressing to design heterogeneous HPC platforms. The design of Deep Learning\naccelerators requires a multidisciplinary approach, combining expertise from\nseveral areas, spanning from computer architecture to approximate computing,\ncomputational models, and machine learning algorithms. Several methodologies\nand tools have been proposed to design accelerators for Deep Learning,\nincluding hardware-software co-design approaches, high-level synthesis methods,\nspecific customized compilers, and methodologies for design space exploration,\nmodeling, and simulation. These methodologies aim to maximize the exploitable\nparallelism and minimize data movement to achieve high performance and energy\nefficiency. This survey provides a holistic review of the most influential\ndesign methodologies and EDA tools proposed in recent years to implement Deep\nLearning accelerators, offering the reader a wide perspective in this rapidly\nevolving field. In particular, this work complements the previous survey\nproposed by the same authors in [203], which focuses on Deep Learning hardware\naccelerators for heterogeneous HPC platforms.",
            "author": [
                "Fabrizio Ferrandi",
                "Serena Curzel",
                "Leandro Fiorin",
                "Daniele Ielmini",
                "Cristina Silvano",
                "Francesco Conti",
                "Alessio Burrello",
                "Francesco Barchi",
                "Luca Benini",
                "Luciano Lavagno",
                "Teodoro Urso",
                "Enrico Calore",
                "Sebastiano Fabio Schifano",
                "Cristian Zambelli",
                "Maurizio Palesi",
                "Giuseppe Ascia",
                "Enrico Russo",
                "Nicola Petra",
                "Davide De Caro",
                "Gennaro Di Meo",
                "Valeria Cardellini",
                "Salvatore Filippone",
                "Francesco Lo Presti",
                "Francesco Silvestri",
                "Paolo Palazzari",
                "Stefania Perri"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17815v1",
                "http://arxiv.org/pdf/2311.17815v1"
            ],
            "primary_category": "cs.AR",
            "category": [
                "cs.AR",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17812v2",
            "title": "DAP: Domain-aware Prompt Learning for Vision-and-Language Navigation",
            "updated": "2023-11-30T11:28:59Z",
            "published": "2023-11-29T17:03:37Z",
            "summary": "Following language instructions to navigate in unseen environments is a\nchallenging task for autonomous embodied agents. With strong representation\ncapabilities, pretrained vision-and-language models are widely used in VLN.\nHowever, most of them are trained on web-crawled general-purpose datasets,\nwhich incurs a considerable domain gap when used for VLN tasks. To address the\nproblem, we propose a novel and model-agnostic domain-aware prompt learning\n(DAP) framework. For equipping the pretrained models with specific object-level\nand scene-level cross-modal alignment in VLN tasks, DAP applies a low-cost\nprompt tuning paradigm to learn soft visual prompts for extracting in-domain\nimage semantics. Specifically, we first generate a set of in-domain image-text\npairs with the help of the CLIP model. Then we introduce soft visual prompts in\nthe input space of the visual encoder in a pretrained model. DAP injects\nin-domain visual knowledge into the visual encoder of the pretrained model in\nan efficient way. Experimental results on both R2R and REVERIE show the\nsuperiority of DAP compared to existing state-of-the-art methods.",
            "author": [
                "Ting Liu",
                "Yue Hu",
                "Wansen Wu",
                "Youkai Wang",
                "Kai Xu",
                "Quanjun Yin"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17812v2",
                "http://arxiv.org/pdf/2311.17812v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17804v1",
            "title": "Aggregation Model Hyperparameters Matter in Digital Pathology",
            "updated": "2023-11-29T16:54:25Z",
            "published": "2023-11-29T16:54:25Z",
            "summary": "Digital pathology has significantly advanced disease detection and\npathologist efficiency through the analysis of gigapixel whole-slide images\n(WSI). In this process, WSIs are first divided into patches, for which a\nfeature extractor model is applied to obtain feature vectors, which are\nsubsequently processed by an aggregation model to predict the respective WSI\nlabel. With the rapid evolution of representation learning, numerous new\nfeature extractor models, often termed foundational models, have emerged.\nTraditional evaluation methods, however, rely on fixed aggregation model\nhyperparameters, a framework we identify as potentially biasing the results.\nOur study uncovers a co-dependence between feature extractor models and\naggregation model hyperparameters, indicating that performance comparability\ncan be skewed based on the chosen hyperparameters. By accounting for this\nco-dependency, we find that the performance of many current feature extractor\nmodels is notably similar. We support this insight by evaluating seven feature\nextractor models across three different datasets with 162 different aggregation\nmodel configurations. This comprehensive approach provides a more nuanced\nunderstanding of the relationship between feature extractors and aggregation\nmodels, leading to a fairer and more accurate assessment of feature extractor\nmodels in digital pathology.",
            "author": [
                "Gustav Bredell",
                "Marcel Fischer",
                "Przemyslaw Szostak",
                "Samaneh Abbasi-Sureshjani",
                "Alvaro Gomariz"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17804v1",
                "http://arxiv.org/pdf/2311.17804v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17801v1",
            "title": "Towards Efficient Hyperdimensional Computing Using Photonics",
            "updated": "2023-11-29T16:51:21Z",
            "published": "2023-11-29T16:51:21Z",
            "summary": "Over the past few years, silicon photonics-based computing has emerged as a\npromising alternative to CMOS-based computing for Deep Neural Networks (DNN).\nUnfortunately, the non-linear operations and the high-precision requirements of\nDNNs make it extremely challenging to design efficient silicon photonics-based\nsystems for DNN inference and training. Hyperdimensional Computing (HDC) is an\nemerging, brain-inspired machine learning technique that enjoys several\nadvantages over existing DNNs, including being lightweight, requiring\nlow-precision operands, and being robust to noise introduced by the\nnonidealities in the hardware. For HDC, computing in-memory (CiM) approaches\nhave been widely used, as CiM reduces the data transfer cost if the operands\ncan fit into the memory. However, inefficient multi-bit operations, high write\nlatency, and low endurance make CiM ill-suited for HDC. On the other hand, the\nexisting electro-photonic DNN accelerators are inefficient for HDC because they\nare specifically optimized for matrix multiplication in DNNs and consume a lot\nof power with high-precision data converters.\n  In this paper, we argue that photonic computing and HDC complement each other\nbetter than photonic computing and DNNs, or CiM and HDC. We propose PhotoHDC,\nthe first-ever electro-photonic accelerator for HDC training and inference,\nsupporting the basic, record-based, and graph encoding schemes. Evaluating with\npopular datasets, we show that our accelerator can achieve two to five orders\nof magnitude lower EDP than the state-of-the-art electro-photonic DNN\naccelerators for implementing HDC training and inference. PhotoHDC also\nachieves four orders of magnitude lower energy-delay product than CiM-based\naccelerators for both HDC training and inference.",
            "author": [
                "Farbin Fayza",
                "Cansu Demirkiran",
                "Hanning Chen",
                "Che-Kai Liu",
                "Avi Mohan",
                "Hamza Errahmouni",
                "Sanggeon Yun",
                "Mohsen Imani",
                "David Zhang",
                "Darius Bunandar",
                "Ajay Joshi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17801v1",
                "http://arxiv.org/pdf/2311.17801v1"
            ],
            "primary_category": "cs.ET",
            "category": [
                "cs.ET",
                "cs.AR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17798v1",
            "title": "Adaptive Circuit Learning of Born Machine: Towards Realization of\n  Amplitude Embedding and Data Loading",
            "updated": "2023-11-29T16:47:31Z",
            "published": "2023-11-29T16:47:31Z",
            "summary": "With the progress in the quantum algorithm in recent years, much of the\nexisting literature claims the exponential quantum advantage against their\nclassical counterpart. However, many of these successes hinge on the assumption\nthat arbitrary states can be efficiently prepared in quantum circuits. In\nreality, crafting a circuit to prepare a generic $n$-qubit quantum state\ndemands an operation count on the order of $\\mathcal{O}(2^n)$, which is\nprohibitively demanding for the quantum algorithm to demonstrate its advantage\nagainst the classical one. To tackle this data-loading problem, numerous\nstrategies have been put forward. Nonetheless, most of these approaches only\nconsider a very simple and easy-to-implement circuit structure, which has been\nshown to suffer from serious optimization issues.\n  In this study, we harness quantum circuits as Born machines to generate\nprobability distributions. Drawing inspiration from methods used to investigate\nelectronic structures in quantum chemistry and condensed matter physics, we\npresent a novel algorithm \"Adaptive Circuit Learning of Born Machine\" (ACLBM)\nthat dynamically expands the ansatz circuit. Our algorithm is tailored to\nselectively integrate two-qubit entangled gates that best capture the complex\nentanglement present within the target state. Empirical results underscore the\nproficiency of our approach in encoding real-world data through amplitude\nembedding, demonstrating not only compliance with but also enhancement over the\nperformance benchmarks set by previous research.",
            "author": [
                "Chun-Tse Li",
                "Hao-Chung Cheng"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17798v1",
                "http://arxiv.org/pdf/2311.17798v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17797v1",
            "title": "Learning to Simulate: Generative Metamodeling via Quantile Regression",
            "updated": "2023-11-29T16:46:24Z",
            "published": "2023-11-29T16:46:24Z",
            "summary": "Stochastic simulation models, while effective in capturing the dynamics of\ncomplex systems, are often too slow to run for real-time decision-making.\nMetamodeling techniques are widely used to learn the relationship between a\nsummary statistic of the outputs (e.g., the mean or quantile) and the inputs of\nthe simulator, so that it can be used in real time. However, this methodology\nrequires the knowledge of an appropriate summary statistic in advance, making\nit inflexible for many practical situations. In this paper, we propose a new\nmetamodeling concept, called generative metamodeling, which aims to construct a\n\"fast simulator of the simulator\". This technique can generate random outputs\nsubstantially faster than the original simulation model, while retaining an\napproximately equal conditional distribution given the same inputs. Once\nconstructed, a generative metamodel can instantaneously generate a large amount\nof random outputs as soon as the inputs are specified, thereby facilitating the\nimmediate computation of any summary statistic for real-time decision-making.\nFurthermore, we propose a new algorithm -- quantile-regression-based generative\nmetamodeling (QRGMM) -- and study its convergence and rate of convergence.\nExtensive numerical experiments are conducted to investigate the empirical\nperformance of QRGMM, compare it with other state-of-the-art generative\nalgorithms, and demonstrate its usefulness in practical real-time\ndecision-making.",
            "author": [
                "L. Jeff Hong",
                "Yanxi Hou",
                "Qingkai Zhang",
                "Xiaowei Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17797v1",
                "http://arxiv.org/pdf/2311.17797v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ME"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17795v1",
            "title": "Marginal Laplacian Score",
            "updated": "2023-11-29T16:45:43Z",
            "published": "2023-11-29T16:45:43Z",
            "summary": "High-dimensional imbalanced data poses a machine learning challenge. In the\nabsence of sufficient or high-quality labels, unsupervised feature selection\nmethods are crucial for the success of subsequent algorithms. Therefore, there\nis a growing need for unsupervised feature selection algorithms focused on\nimbalanced data. Thus, we propose a Marginal Laplacian Score (MLS) a\nmodification of the well-known Laplacian Score (LS) to be better suited for\nimbalance data. We introduce an assumption that the minority class or anomalous\nappear more frequently in the margin of the features. Consequently, MLS aims to\npreserve the local structure of the data set's margin. As MLS is better suited\nfor handling imbalanced data, we propose its integration into modern feature\nselection methods that utilize the Laplacian score. We integrate the MLS\nalgorithm into the Differentiable Unsupervised Feature Selection (DUFS),\nresulting in DUFS-MLS. The proposed methods demonstrate robust and improved\nperformance on synthetic and public data sets.",
            "author": [
                "Guy Hay",
                "Ohad Volk"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17795v1",
                "http://arxiv.org/pdf/2311.17795v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML",
                "I.5.0"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17790v1",
            "title": "FAT-HuBERT: Front-end Adaptive Training of Hidden-unit BERT for\n  Distortion-Invariant Robust Speech Recognition",
            "updated": "2023-11-29T16:35:13Z",
            "published": "2023-11-29T16:35:13Z",
            "summary": "Advancements in monaural speech enhancement (SE) techniques have greatly\nimproved the perceptual quality of speech. However, integrating these\ntechniques into automatic speech recognition (ASR) systems has not yielded the\nexpected performance gains, primarily due to the introduction of distortions\nduring the SE process. In this paper, we propose a novel approach called\nFAT-HuBERT, which leverages distortion-invariant self-supervised learning (SSL)\nto enhance the robustness of ASR. To address the distortions introduced by the\nSE frontends, we introduce layer-wise fusion modules that incorporate features\nextracted from both observed noisy signals and enhanced signals. During\ntraining, the SE frontend is randomly selected from a pool of models. We\nevaluate the performance of FAT-HuBERT on simulated noisy speech generated from\nLibriSpeech as well as real-world noisy speech from the CHiME-4 1-channel\ndataset. The experimental results demonstrate a significant relative reduction\nin word error rate (WER).",
            "author": [
                "Dongning Yang",
                "Wei Wang",
                "Yanmin Qian"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17790v1",
                "http://arxiv.org/pdf/2311.17790v1"
            ],
            "primary_category": "cs.SD",
            "category": [
                "cs.SD",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17786v1",
            "title": "DSS: Synthesizing long Digital Ink using Data augmentation, Style\n  encoding and Split generation",
            "updated": "2023-11-29T16:33:19Z",
            "published": "2023-11-29T16:33:19Z",
            "summary": "As text generative models can give increasingly long answers, we tackle the\nproblem of synthesizing long text in digital ink. We show that the commonly\nused models for this task fail to generalize to long-form data and how this\nproblem can be solved by augmenting the training data, changing the model\narchitecture and the inference procedure. These methods use contrastive\nlearning technique and are tailored specifically for the handwriting domain.\nThey can be applied to any encoder-decoder model that works with digital ink.\nWe demonstrate that our method reduces the character error rate on long-form\nEnglish data by half compared to baseline RNN and by 16% compared to the\nprevious approach that aims at addressing the same problem. We show that all\nthree parts of the method improve recognizability of generated inks. In\naddition, we evaluate synthesized data in a human study and find that people\nperceive most of generated data as real.",
            "author": [
                "Aleksandr Timofeev",
                "Anastasiia Fadeeva",
                "Andrei Afonin",
                "Claudiu Musat",
                "Andrii Maksai"
            ],
            "link": [
                "http://dx.doi.org/10.1007/978-3-031-41685-9_14",
                "http://arxiv.org/abs/2311.17786v1",
                "http://arxiv.org/pdf/2311.17786v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17783v1",
            "title": "Identifying Dynamic Regulation with Adversarial Surrogates",
            "updated": "2023-11-29T16:27:27Z",
            "published": "2023-11-29T16:27:27Z",
            "summary": "Homeostasis, the ability to maintain a stable internal environment in the\nface of perturbations, is essential for the functioning of living systems.\nGiven observations of a system, or even a detailed model of one, it is both\nvaluable and extremely challenging to extract the control objectives of the\nhomeostatic mechanisms. Lacking a clear separation between plant and\ncontroller, frameworks such as inverse optimal control and inverse\nreinforcement learning are unable to identify the homeostatic mechanisms. A\nrecently developed data-driven algorithm, Identifying Regulation with\nAdversarial Surrogates (IRAS), detects highly regulated or conserved quantities\nas the solution of a min-max optimization scheme that automates classical\nsurrogate data methods. Yet, the definition of homeostasis as regulation within\nnarrow limits is too strict for biological systems which show sustained\noscillations such as circadian rhythms. In this work, we introduce Identifying\nDynamic Regulation with Adversarial Surrogates (IDRAS), a generalization of the\nIRAS algorithm, capable of identifying control objectives that are regulated\nwith respect to a dynamical reference value. We test the algorithm on\nsimulation data from realistic biological models and benchmark physical\nsystems, demonstrating excellent empirical results.",
            "author": [
                "Ron Teichner",
                "Naama Brenner",
                "Ron Meir"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17783v1",
                "http://arxiv.org/pdf/2311.17783v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17781v1",
            "title": "Propagate & Distill: Towards Effective Graph Learners Using\n  Propagation-Embracing MLPs",
            "updated": "2023-11-29T16:26:24Z",
            "published": "2023-11-29T16:26:24Z",
            "summary": "Recent studies attempted to utilize multilayer perceptrons (MLPs) to solve\nsemisupervised node classification on graphs, by training a student MLP by\nknowledge distillation from a teacher graph neural network (GNN). While\nprevious studies have focused mostly on training the student MLP by matching\nthe output probability distributions between the teacher and student models\nduring distillation, it has not been systematically studied how to inject the\nstructural information in an explicit and interpretable manner. Inspired by\nGNNs that separate feature transformation $T$ and propagation $\\Pi$, we\nre-frame the distillation process as making the student MLP learn both $T$ and\n$\\Pi$. Although this can be achieved by applying the inverse propagation\n$\\Pi^{-1}$ before distillation from the teacher, it still comes with a high\ncomputational cost from large matrix multiplications during training. To solve\nthis problem, we propose Propagate & Distill (P&D), which propagates the output\nof the teacher before distillation, which can be interpreted as an approximate\nprocess of the inverse propagation. We demonstrate that P&D can readily improve\nthe performance of the student MLP.",
            "author": [
                "Yong-Min Shin",
                "Won-Yong Shin"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17781v1",
                "http://arxiv.org/pdf/2311.17781v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.IT",
                "cs.NE",
                "cs.SI",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17778v1",
            "title": "Unified Binary and Multiclass Margin-Based Classification",
            "updated": "2023-11-29T16:24:32Z",
            "published": "2023-11-29T16:24:32Z",
            "summary": "The notion of margin loss has been central to the development and analysis of\nalgorithms for binary classification. To date, however, there remains no\nconsensus as to the analogue of the margin loss for multiclass classification.\nIn this work, we show that a broad range of multiclass loss functions,\nincluding many popular ones, can be expressed in the relative margin form, a\ngeneralization of the margin form of binary losses. The relative margin form is\nbroadly useful for understanding and analyzing multiclass losses as shown by\nour prior work (Wang and Scott, 2020, 2021). To further demonstrate the utility\nof this way of expressing multiclass losses, we use it to extend the seminal\nresult of Bartlett et al. (2006) on classification-calibration of binary margin\nlosses to multiclass. We then analyze the class of Fenchel-Young losses, and\nexpand the set of these losses that are known to be classification-calibrated.",
            "author": [
                "Yutong Wang",
                "Clayton Scott"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17778v1",
                "http://arxiv.org/pdf/2311.17778v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17776v1",
            "title": "One-Shot Open Affordance Learning with Foundation Models",
            "updated": "2023-11-29T16:23:06Z",
            "published": "2023-11-29T16:23:06Z",
            "summary": "We introduce One-shot Open Affordance Learning (OOAL), where a model is\ntrained with just one example per base object category, but is expected to\nidentify novel objects and affordances. While vision-language models excel at\nrecognizing novel objects and scenes, they often struggle to understand finer\nlevels of granularity such as affordances. To handle this issue, we conduct a\ncomprehensive analysis of existing foundation models, to explore their inherent\nunderstanding of affordances and assess the potential for data-limited\naffordance learning. We then propose a vision-language framework with simple\nand effective designs that boost the alignment between visual features and\naffordance text embeddings. Experiments on two affordance segmentation\nbenchmarks show that the proposed method outperforms state-of-the-art models\nwith less than 1% of the full training data, and exhibits reasonable\ngeneralization capability on unseen objects and affordances.",
            "author": [
                "Gen Li",
                "Deqing Sun",
                "Laura Sevilla-Lara",
                "Varun Jampani"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17776v1",
                "http://arxiv.org/pdf/2311.17776v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17973v1",
            "title": "Homogeneous Artificial Neural Network",
            "updated": "2023-11-29T16:16:32Z",
            "published": "2023-11-29T16:16:32Z",
            "summary": "The paper proposes an artificial neural network (ANN) being a global\napproximator for a special class of functions, which are known as generalized\nhomogeneous. The homogeneity means a symmetry of a function with respect to a\ngroup of transformations having topological characterization of a dilation. In\nthis paper, a class of the so-called linear dilations is considered. A\nhomogeneous universal approximation theorem is proven. Procedures for an\nupgrade of an existing ANN to a homogeneous one are developed. Theoretical\nresults are supported by examples from the various domains (computer science,\nsystems theory and automatic control).",
            "author": [
                "Andrey Polyakov"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17973v1",
                "http://arxiv.org/pdf/2311.17973v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.NA",
                "cs.NE",
                "cs.SY",
                "eess.SY",
                "math.NA",
                "math.OC",
                "68T07, 93C10, 93D15"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17972v1",
            "title": "Self-Infilling Code Generation",
            "updated": "2023-11-29T16:02:06Z",
            "published": "2023-11-29T16:02:06Z",
            "summary": "This work introduces a general code generation framework that incorporates\ninfilling operations into auto-regressive decoding. Our approach capitalizes on\nthe observation that recent code language models with infilling capabilities\ncan perform \\emph{self-infilling}: whereas infilling operations aim to fill in\nthe middle based on a predefined prefix and suffix, self-infilling sequentially\ngenerates both such surrounding context and the infilled content. We utilize\nthis feature to develop an infilling-augmented decoding process that\nfacilitates non-monotonic generation. This approach allows for postponing the\ngeneration of uncertain code snippets until a definitive suffix is established,\nleading to improved control over the generation sequence. In addition, it\nfacilitates a looping mechanism, which can iteratively update and synchronize\neach piece of generation in a cyclic manner. Extensive experiments are\nconducted to demonstrate that our proposed decoding process is effective in\nenhancing regularity and quality across several code generation benchmarks.",
            "author": [
                "Lin Zheng",
                "Jianbo Yuan",
                "Zhi Zhang",
                "Hongxia Yang",
                "Lingpeng Kong"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17972v1",
                "http://arxiv.org/pdf/2311.17972v1"
            ],
            "primary_category": "cs.PL",
            "category": [
                "cs.PL",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17752v1",
            "title": "BAND-2k: Banding Artifact Noticeable Database for Banding Detection and\n  Quality Assessment",
            "updated": "2023-11-29T15:56:31Z",
            "published": "2023-11-29T15:56:31Z",
            "summary": "Banding, also known as staircase-like contours, frequently occurs in flat\nareas of images/videos processed by the compression or quantization algorithms.\nAs undesirable artifacts, banding destroys the original image structure, thus\ndegrading users' quality of experience (QoE). In this paper, we systematically\ninvestigate the banding image quality assessment (IQA) problem, aiming to\ndetect the image banding artifacts and evaluate their perceptual visual\nquality. Considering that the existing image banding databases only contain\nlimited content sources and banding generation methods, and lack perceptual\nquality labels (i.e. mean opinion scores), we first build the largest banding\nIQA database so far, named Banding Artifact Noticeable Database (BAND-2k),\nwhich consists of 2,000 banding images generated by 15 compression and\nquantization schemes. A total of 23 workers participated in the subjective IQA\nexperiment, yielding over 214,000 patch-level banding class labels and 44,371\nreliable image-level quality ratings. Subsequently, we develop an effective\nno-reference (NR) banding evaluator for banding detection and quality\nassessment by leveraging frequency characteristics of banding artifacts. A dual\nconvolutional neural network is employed to concurrently learn the feature\nrepresentation from the high-frequency and low-frequency maps, thereby\nenhancing the ability to discern banding artifacts. The quality score of a\nbanding image is generated by pooling the banding detection maps masked by the\nspatial frequency filters. Experiments demonstrate that our banding evaluator\nachieves a remarkably high accuracy in banding detection and also exhibits high\nSRCC and PLCC results with the perceptual quality labels. These findings unveil\nthe strong correlations between the intensity of banding artifacts and the\nperceptual visual quality, thus validating the necessity of banding quality\nassessment.",
            "author": [
                "Zijian Chen",
                "Wei Sun",
                "Jun Jia",
                "Fangfang Lu",
                "Zicheng Zhang",
                "Jing Liu",
                "Ru Huang",
                "Xiongkuo Min",
                "Guangtao Zhai"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17752v1",
                "http://arxiv.org/pdf/2311.17752v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.DB",
                "cs.MM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17750v1",
            "title": "Addressing Membership Inference Attack in Federated Learning with Model\n  Compression",
            "updated": "2023-11-29T15:54:15Z",
            "published": "2023-11-29T15:54:15Z",
            "summary": "Federated Learning (FL) has been proposed as a privacy-preserving solution\nfor machine learning. However, recent works have shown that Federated Learning\ncan leak private client data through membership attacks. In this paper, we show\nthat the effectiveness of these attacks on the clients negatively correlates\nwith the size of the client datasets and model complexity. Based on this\nfinding, we propose model-agnostic Federated Learning as a privacy-enhancing\nsolution because it enables the use of models of varying complexity in the\nclients. To this end, we present $\\texttt{MaPP-FL}$, a novel privacy-aware FL\napproach that leverages model compression on the clients while keeping a full\nmodel on the server. We compare the performance of $\\texttt{MaPP-FL}$ against\nstate-of-the-art model-agnostic FL methods on the CIFAR-10, CIFAR-100, and\nFEMNIST vision datasets. Our experiments show the effectiveness of\n$\\texttt{MaPP-FL}$ in preserving the clients' and the server's privacy while\nachieving competitive classification accuracies.",
            "author": [
                "Gergely D\u00e1niel N\u00e9meth",
                "Miguel \u00c1ngel Lozano",
                "Novi Quadrianto",
                "Nuria Oliver"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17750v1",
                "http://arxiv.org/pdf/2311.17750v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17749v1",
            "title": "Learning Free Terminal Time Optimal Closed-loop Control of Manipulators",
            "updated": "2023-11-29T15:53:43Z",
            "published": "2023-11-29T15:53:43Z",
            "summary": "This paper presents a novel approach to learning free terminal time\nclosed-loop control for robotic manipulation tasks, enabling dynamic adjustment\nof task duration and control inputs to enhance performance. We extend the\nsupervised learning approach, namely solving selected optimal open-loop\nproblems and utilizing them as training data for a policy network, to the free\nterminal time scenario. Three main challenges are addressed in this extension.\nFirst, we introduce a marching scheme that enhances the solution quality and\nincreases the success rate of the open-loop solver by gradually refining time\ndiscretization. Second, we extend the QRnet in Nakamura-Zimmerer et al. (2021b)\nto the free terminal time setting to address discontinuity and improve\nstability at the terminal state. Third, we present a more automated version of\nthe initial value problem (IVP) enhanced sampling method from previous work\n(Zhang et al., 2022) to adaptively update the training dataset, significantly\nimproving its quality. By integrating these techniques, we develop a\nclosed-loop policy that operates effectively over a broad domain with varying\noptimal time durations, achieving near globally optimal total costs.",
            "author": [
                "Wei Hu",
                "Yue Zhao",
                "Weinan E",
                "Jiequn Han",
                "Jihao Long"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17749v1",
                "http://arxiv.org/pdf/2311.17749v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01520v1",
            "title": "Entropy and the Kullback-Leibler Divergence for Bayesian Networks:\n  Computational Complexity and Efficient Implementation",
            "updated": "2023-11-29T15:51:04Z",
            "published": "2023-11-29T15:51:04Z",
            "summary": "Bayesian networks (BNs) are a foundational model in machine learning and\ncausal inference. Their graphical structure can handle high-dimensional\nproblems, divide-and-conquering them into a sparse collection of smaller ones;\nunderlies Judea Pearl's causality; and determines their explainability and\ninterpretability. Despite their popularity, there are few resources in the\nliterature on how to compute Shannon's entropy and the Kullback-Leibler (KL)\ndivergence for BNs under their most common distributional assumptions. In this\npaper, we provide computationally efficient algorithms for both by leveraging\nBNs' graphical structure, and we illustrate them with a complete set of\nnumerical examples. In the process, we show it is possible to reduce the\ncomputational complexity of KL from cubic to quadratic for Gaussian BNs.",
            "author": [
                "Marco Scutari"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01520v1",
                "http://arxiv.org/pdf/2312.01520v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.LG",
                "stat.CO",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17744v1",
            "title": "Variational Bayes image restoration with compressive autoencoders",
            "updated": "2023-11-29T15:49:31Z",
            "published": "2023-11-29T15:49:31Z",
            "summary": "Regularization of inverse problems is of paramount importance in\ncomputational imaging. The ability of neural networks to learn efficient image\nrepresentations has been recently exploited to design powerful data-driven\nregularizers. While state-of-the-art plug-and-play methods rely on an implicit\nregularization provided by neural denoisers, alternative Bayesian approaches\nconsider Maximum A Posteriori (MAP) estimation in the latent space of a\ngenerative model, thus with an explicit regularization. However,\nstate-of-the-art deep generative models require a huge amount of training data\ncompared to denoisers. Besides, their complexity hampers the optimization of\nthe latent MAP. In this work, we propose to use compressive autoencoders for\nlatent estimation. These networks, which can be seen as variational\nautoencoders with a flexible latent prior, are smaller and easier to train than\nstate-of-the-art generative models. We then introduce the Variational Bayes\nLatent Estimation (VBLE) algorithm, which performs this estimation within the\nframework of variational inference. This allows for fast and easy (approximate)\nposterior sampling. Experimental results on image datasets BSD and FFHQ\ndemonstrate that VBLE reaches similar performance than state-of-the-art\nplug-and-play methods, while being able to quantify uncertainties faster than\nother existing posterior sampling techniques.",
            "author": [
                "Maud Biquard",
                "Marie Chabert",
                "Thomas Oberlin"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17744v1",
                "http://arxiv.org/pdf/2311.17744v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17740v1",
            "title": "A transductive few-shot learning approach for classification of digital\n  histopathological slides from liver cancer",
            "updated": "2023-11-29T15:44:00Z",
            "published": "2023-11-29T15:44:00Z",
            "summary": "This paper presents a new approach for classifying 2D histopathology patches\nusing few-shot learning. The method is designed to tackle a significant\nchallenge in histopathology, which is the limited availability of labeled data.\nBy applying a sliding window technique to histopathology slides, we illustrate\nthe practical benefits of transductive learning (i.e., making joint predictions\non patches) to achieve consistent and accurate classification. Our approach\ninvolves an optimization-based strategy that actively penalizes the prediction\nof a large number of distinct classes within each window. We conducted\nexperiments on histopathological data to classify tissue classes in digital\nslides of liver cancer, specifically hepatocellular carcinoma. The initial\nresults show the effectiveness of our method and its potential to enhance the\nprocess of automated cancer diagnosis and treatment, all while reducing the\ntime and effort required for expert annotation.",
            "author": [
                "Aymen Sadraoui",
                "S\u00e9gol\u00e8ne Martin",
                "Eliott Barbot",
                "Astrid Laurent-Bellue",
                "Jean-Christophe Pesquet",
                "Catherine Guettier",
                "Ismail Ben Ayed"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17740v1",
                "http://arxiv.org/pdf/2311.17740v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.LG",
                "q-bio.TO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17970v1",
            "title": "Description Generation using Variational Auto-Encoders for precursor\n  microRNA",
            "updated": "2023-11-29T15:41:45Z",
            "published": "2023-11-29T15:41:45Z",
            "summary": "Micro RNAs (miRNA) are a type of non-coding RNA, which are involved in gene\nregulation and can be associated with diseases such as cancer, cardiovascular\nand neurological diseases. As such, identifying the entire genome of miRNA can\nbe of great relevance. Since experimental methods for novel precursor miRNA\n(pre-miRNA) detection are complex and expensive, computational detection using\nML could be useful. Existing ML methods are often complex black boxes, which do\nnot create an interpretable structural description of pre-miRNA. In this paper,\nwe propose a novel framework, which makes use of generative modeling through\nVariational Auto-Encoders to uncover the generative factors of pre-miRNA. After\ntraining the VAE, the pre-miRNA description is developed using a decision tree\non the lower dimensional latent space. Applying the framework to miRNA\nclassification, we obtain a high reconstruction and classification performance,\nwhile also developing an accurate miRNA description.",
            "author": [
                "Marko Petkovi\u0107",
                "Vlado Menkovski"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17970v1",
                "http://arxiv.org/pdf/2311.17970v1"
            ],
            "primary_category": "q-bio.QM",
            "category": [
                "q-bio.QM",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17737v1",
            "title": "GenZI: Zero-Shot 3D Human-Scene Interaction Generation",
            "updated": "2023-11-29T15:40:11Z",
            "published": "2023-11-29T15:40:11Z",
            "summary": "Can we synthesize 3D humans interacting with scenes without learning from any\n3D human-scene interaction data? We propose GenZI, the first zero-shot approach\nto generating 3D human-scene interactions. Key to GenZI is our distillation of\ninteraction priors from large vision-language models (VLMs), which have learned\na rich semantic space of 2D human-scene compositions. Given a natural language\ndescription and a coarse point location of the desired interaction in a 3D\nscene, we first leverage VLMs to imagine plausible 2D human interactions\ninpainted into multiple rendered views of the scene. We then formulate a robust\niterative optimization to synthesize the pose and shape of a 3D human model in\nthe scene, guided by consistency with the 2D interaction hypotheses. In\ncontrast to existing learning-based approaches, GenZI circumvents the\nconventional need for captured 3D interaction data, and allows for flexible\ncontrol of the 3D interaction synthesis with easy-to-use text prompts.\nExtensive experiments show that our zero-shot approach has high flexibility and\ngenerality, making it applicable to diverse scene types, including both indoor\nand outdoor environments.",
            "author": [
                "Lei Li",
                "Angela Dai"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17737v1",
                "http://arxiv.org/pdf/2311.17737v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17733v1",
            "title": "Stable Invariants and Their Role in Word Measures on Groups",
            "updated": "2023-11-29T15:37:32Z",
            "published": "2023-11-29T15:37:32Z",
            "summary": "Every word in a free group induces a word measure -- a probability measure\ndefined via the word map -- on every compact group. This paper presents a\nconjectural picture about the role of a plethora of stable invariants of words\nin word measures on groups. These invariants generalize the stable commutator\nlength and include, among others, two invariants recently defined by Wilton:\nthe stable primitivity rank and a non-oriented analog of stable commutator\nlength we call stable square length. The conjectures say, roughly, that these\nstable invariants control the asymptotics of the expected values of stable\ncharacters, under word measures. We reinforce these conjectures by proving a\nversion for word measures on wreath products, and by introducing a related\nformula for stable irreducible characters of the symmetric group.",
            "author": [
                "Doron Puder",
                "Yotam Shomroni"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17733v1",
                "http://arxiv.org/pdf/2311.17733v1"
            ],
            "primary_category": "math.GR",
            "category": [
                "math.GR",
                "math.GT",
                "math.PR",
                "math.RT",
                "60B15, 20c15 (Primary) 60B20, 05E10, 20b30, 20c30, 20P05, 22c05,\n  20e05, 22e30, 28c10, 57m07, 20f65, 20f12 (Secondary)"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17727v1",
            "title": "Odor-Based Molecular Communications: State-of-the-Art, Vision,\n  Challenges, and Frontier Directions",
            "updated": "2023-11-29T15:33:45Z",
            "published": "2023-11-29T15:33:45Z",
            "summary": "Humankind mimics the processes and strategies that nature has perfected and\nuses them as a model to address its problems. That has recently found a new\ndirection, i.e., a novel communication technology called molecular\ncommunication (MC), using molecules to encode, transmit, and receive\ninformation. Despite extensive research, an innate MC method with plenty of\nnatural instances, i.e., olfactory or odor communication, has not yet been\nstudied with the tools of information and communication technologies (ICT).\nExisting studies focus on digitizing this sense and developing actuators\nwithout inspecting the principles of odor-based information coding and MC,\nwhich significantly limits its application potential. Hence, there is a need to\nfocus cross-disciplinary research efforts to reveal the fundamentals of this\nunconventional communication modality from an ICT perspective. The ways of\nnatural odor MC in nature need to be anatomized and engineered for end-to-end\ncommunication among humans and human-made things to enable several multi-sense\naugmented reality technologies reinforced with olfactory senses for novel\napplications and solutions in the Internet of Everything (IoE). This paper\nintroduces the concept of odor-based molecular communication (OMC) and provides\na comprehensive examination of olfactory systems. It explores odor\ncommunication in nature, including aspects of odor information, channels,\nreception, spatial perception, and cognitive functions. Additionally, a\ncomprehensive comparison of various communication systems sets the foundation\nfor further investigation. By highlighting the unique characteristics,\nadvantages, and potential applications of OMC through this comparative\nanalysis, the paper lays the groundwork for exploring the modeling of an\nend-to-end OMC channel, considering the design of OMC transmitters and\nreceivers, and developing innovative OMC techniques.",
            "author": [
                "Dilara Aktas",
                "Beyza Ezgi Ortlek",
                "Meltem Civas",
                "Elham Baradari",
                "Ayse Sila Okcu",
                "Melanie Whitfield",
                "Oktay Cetinkaya",
                "Ozgur Baris Akan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17727v1",
                "http://arxiv.org/pdf/2311.17727v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17969v1",
            "title": "Generation of a Compendium of Transcription Factor Cascades and\n  Identification of Potential Therapeutic Targets using Graph Machine Learning",
            "updated": "2023-11-29T15:31:58Z",
            "published": "2023-11-29T15:31:58Z",
            "summary": "Transcription factors (TFs) play a vital role in the regulation of gene\nexpression thereby making them critical to many cellular processes. In this\nstudy, we used graph machine learning methods to create a compendium of TF\ncascades using data extracted from the STRING database. A TF cascade is a\nsequence of TFs that regulate each other, forming a directed path in the TF\nnetwork. We constructed a knowledge graph of 81,488 unique TF cascades, with\nthe longest cascade consisting of 62 TFs. Our results highlight the complex and\nintricate nature of TF interactions, where multiple TFs work together to\nregulate gene expression. We also identified 10 TFs with the highest regulatory\ninfluence based on centrality measurements, providing valuable information for\nresearchers interested in studying specific TFs. Furthermore, our pathway\nenrichment analysis revealed significant enrichment of various pathways and\nfunctional categories, including those involved in cancer and other diseases,\nas well as those involved in development, differentiation, and cell signaling.\nThe enriched pathways identified in this study may have potential as targets\nfor therapeutic intervention in diseases associated with dysregulation of\ntranscription factors. We have released the dataset, knowledge graph, and\ngraphML methods for the TF cascades, and created a website to display the\nresults, which can be accessed by researchers interested in using this dataset.\nOur study provides a valuable resource for understanding the complex network of\ninteractions between TFs and their regulatory roles in cellular processes.",
            "author": [
                "Sonish Sivarajkumar",
                "Pratyush Tandale",
                "Ankit Bhardwaj",
                "Kipp W. Johnson",
                "Anoop Titus",
                "Benjamin S. Glicksberg",
                "Shameer Khader",
                "Kamlesh K. Yadav",
                "Lakshminarayanan Subramanian"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17969v1",
                "http://arxiv.org/pdf/2311.17969v1"
            ],
            "primary_category": "q-bio.MN",
            "category": [
                "q-bio.MN",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17722v1",
            "title": "SenTest: Evaluating Robustness of Sentence Encoders",
            "updated": "2023-11-29T15:21:35Z",
            "published": "2023-11-29T15:21:35Z",
            "summary": "Contrastive learning has proven to be an effective method for pre-training\nmodels using weakly labeled data in the vision domain. Sentence transformers\nare the NLP counterparts to this architecture, and have been growing in\npopularity due to their rich and effective sentence representations. Having\neffective sentence representations is paramount in multiple tasks, such as\ninformation retrieval, retrieval augmented generation (RAG), and sentence\ncomparison. Keeping in mind the deployability factor of transformers,\nevaluating the robustness of sentence transformers is of utmost importance.\nThis work focuses on evaluating the robustness of the sentence encoders. We\nemploy several adversarial attacks to evaluate its robustness. This system uses\ncharacter-level attacks in the form of random character substitution,\nword-level attacks in the form of synonym replacement, and sentence-level\nattacks in the form of intra-sentence word order shuffling. The results of the\nexperiments strongly undermine the robustness of sentence encoders. The models\nproduce significantly different predictions as well as embeddings on perturbed\ndatasets. The accuracy of the models can fall up to 15 percent on perturbed\ndatasets as compared to unperturbed datasets. Furthermore, the experiments\ndemonstrate that these embeddings does capture the semantic and syntactic\nstructure (sentence order) of sentences. However, existing supervised\nclassification strategies fail to leverage this information, and merely\nfunction as n-gram detectors.",
            "author": [
                "Tanmay Chavan",
                "Shantanu Patankar",
                "Aditya Kane",
                "Omkar Gokhale",
                "Geetanjali Kale",
                "Raviraj Joshi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17722v1",
                "http://arxiv.org/pdf/2311.17722v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17721v1",
            "title": "Beyond FRiM, ASAP: a family of sparse approximation for covariance\n  matrices and preconditioners",
            "updated": "2023-11-29T15:21:33Z",
            "published": "2023-11-29T15:21:33Z",
            "summary": "The FRiM fractal operator belongs to a family of operators, called ASAP,\ndefined by an ordered selection of nearest neighbors. This generalization\nprovides means to improve upon the good properties of FRiM. We propose a fast\nalgorithm to build an ASAP operator mimicking the fractal structure of FRiM for\npupils of any size and geometry and to learn the sparse coefficients from\nempirical data. We empirically show the good approximation by ASAP of\ncorrelated statistics and the benefits of ASAP for solving phase restoration\nproblems.",
            "author": [
                "\u00c9ric Thi\u00e9baut",
                "Michel Tallon",
                "Samuel Th\u00e9",
                "Lo\u00efc Denis"
            ],
            "link": [
                "http://dx.doi.org/10.1117/12.2630192",
                "http://arxiv.org/abs/2311.17721v1",
                "http://arxiv.org/pdf/2311.17721v1"
            ],
            "primary_category": "astro-ph.IM",
            "category": [
                "astro-ph.IM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17717v1",
            "title": "Receler: Reliable Concept Erasing of Text-to-Image Diffusion Models via\n  Lightweight Erasers",
            "updated": "2023-11-29T15:19:49Z",
            "published": "2023-11-29T15:19:49Z",
            "summary": "Concept erasure in text-to-image diffusion models aims to disable pre-trained\ndiffusion models from generating images related to a target concept. To perform\nreliable concept erasure, the properties of robustness and locality are\ndesirable. The former refrains the model from producing images associated with\nthe target concept for any paraphrased or learned prompts, while the latter\npreserves the model ability in generating images for non-target concepts. In\nthis paper, we propose Reliable Concept Erasing via Lightweight Erasers\n(Receler), which learns a lightweight Eraser to perform concept erasing and\nenhances locality and robustness with the proposed concept-localized\nregularization and adversarial prompt learning, respectively. Comprehensive\nquantitative and qualitative experiments with various concept prompts verify\nthe superiority of Receler over the previous erasing methods on the above two\ndesirable properties.",
            "author": [
                "Chi-Pin Huang",
                "Kai-Po Chang",
                "Chung-Ting Tsai",
                "Yung-Hsuan Lai",
                "Yu-Chiang Frank Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17717v1",
                "http://arxiv.org/pdf/2311.17717v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17705v1",
            "title": "Q-PAC: Automated Detection of Quantum Bug-Fix Patterns",
            "updated": "2023-11-29T15:09:32Z",
            "published": "2023-11-29T15:09:32Z",
            "summary": "Context: Bug-fix pattern detection has been investigated in the past in the\ncontext of classical software. However, while quantum software is developing\nrapidly, the literature still lacks automated methods and tools to identify,\nanalyze, and detect bug-fix patterns. To the best of our knowledge, our work\npreviously published in SEKE'23 was the first to leverage classical techniques\nto detect bug-fix patterns in quantum code.\n  Objective: To extend our previous effort, we present a research agenda\n(Q-Repair), including a series of testing and debugging methodologies, to\nimprove the quality of quantum software. The ultimate goal is to utilize\nmachine learning techniques to automatically predict fix patterns for existing\nquantum bugs.\n  Method: As part of the first stage of the agenda, we extend our initial study\nand propose a more comprehensive automated framework, called Q-PAC, for\ndetecting bug-fix patterns in IBM Qiskit quantum code. In the framework, we\ndevelop seven bug-fix pattern detectors using abstract syntax trees, syntactic\nfilters, and semantic checks.\n  Results: To demonstrate our method, we run Q-PAC on a variety of quantum\nbug-fix patterns using both real-world and handcrafted examples of bugs and\nfixes. The experimental results show that Q-PAC can effectively identify\nbug-fix patterns in IBM Qiskit.\n  Conclusion: We hope our initial study on quantum bug-fix detection can bring\nawareness of quantum software engineering to both researchers and\npractitioners. Thus, we also publish Q-PAC as an open-source software on\nGitHub. We would like to encourage other researchers to work on research\ndirections (such as Q-Repair) to improve the quality of the quantum\nprogramming.",
            "author": [
                "Pranav K. Nayak",
                "Krishn V. Kher",
                "M. Bharat Chandra",
                "M. V. Panduranga Rao",
                "Lei Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17705v1",
                "http://arxiv.org/pdf/2311.17705v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17704v1",
            "title": "An Efficient Algorithm for Unbalanced 1D Transportation",
            "updated": "2023-11-29T15:09:24Z",
            "published": "2023-11-29T15:09:24Z",
            "summary": "Optimal transport (OT) and unbalanced optimal transport (UOT) are central in\nmany machine learning, statistics and engineering applications. 1D OT is easily\nsolved, with complexity O(n log n), but no efficient algorithm was known for 1D\nUOT. We present a new approach that leverages the successive shortest path\nalgorithm for the corresponding network flow problem. By employing a suitable\nrepresentation, we bundle together multiple steps that do not change the cost\nof the shortest path. We prove that our algorithm solves 1D UOT in O(n log n),\nclosing the gap.",
            "author": [
                "Gabriel Gouvine"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17704v1",
                "http://arxiv.org/pdf/2311.17704v1"
            ],
            "primary_category": "cs.PF",
            "category": [
                "cs.PF",
                "cs.CC",
                "cs.DS",
                "68",
                "E.1; F.2.2"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17696v2",
            "title": "How to Build an AI Tutor that Can Adapt to Any Course and Provide\n  Accurate Answers Using Large Language Model and Retrieval-Augmented\n  Generation",
            "updated": "2023-11-30T06:28:22Z",
            "published": "2023-11-29T15:02:46Z",
            "summary": "Artificial intelligence is transforming education through data-driven,\npersonalized learning solutions. This paper introduces AI Tutor, an innovative\nweb application that provides personalized tutoring in any subject using\nstate-of-the-art Large Language Model (LLM). AI Tutor ingests course materials\nto construct an adaptive knowledge base tailored to the course. When students\npose questions, it retrieves the most relevant information and generates\ndetailed, conversational responses citing supporting evidence. The system is\npowered by advanced large language models and Retrieval-Augmented Generation\n(RAG) techniques for accurate, natural question answering. We present a\nfully-functional web interface and video demonstration that showcase AI Tutor's\nversatility across diverse subjects and its ability to produce pedagogically\ncogent responses. While an initial prototype, this work represents a pioneering\nstep toward AI-enabled tutoring systems that can democratize access to\nhigh-quality, customized educational support.",
            "author": [
                "Chenxi Dong"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17696v2",
                "http://arxiv.org/pdf/2311.17696v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17695v1",
            "title": "Fair Text-to-Image Diffusion via Fair Mapping",
            "updated": "2023-11-29T15:02:01Z",
            "published": "2023-11-29T15:02:01Z",
            "summary": "In this paper, we address the limitations of existing text-to-image diffusion\nmodels in generating demographically fair results when given human-related\ndescriptions. These models often struggle to disentangle the target language\ncontext from sociocultural biases, resulting in biased image generation. To\novercome this challenge, we propose Fair Mapping, a general, model-agnostic,\nand lightweight approach that modifies a pre-trained text-to-image model by\ncontrolling the prompt to achieve fair image generation. One key advantage of\nour approach is its high efficiency. The training process only requires\nupdating a small number of parameters in an additional linear mapping network.\nThis not only reduces the computational cost but also accelerates the\noptimization process. We first demonstrate the issue of bias in generated\nresults caused by language biases in text-guided diffusion models. By\ndeveloping a mapping network that projects language embeddings into an unbiased\nspace, we enable the generation of relatively balanced demographic results\nbased on a keyword specified in the prompt. With comprehensive experiments on\nface image generation, we show that our method significantly improves image\ngeneration performance when prompted with descriptions related to human faces.\nBy effectively addressing the issue of bias, we produce more fair and diverse\nimage outputs. This work contributes to the field of text-to-image generation\nby enhancing the ability to generate images that accurately reflect the\nintended demographic characteristics specified in the text.",
            "author": [
                "Jia Li",
                "Lijie Hu",
                "Jingfeng Zhang",
                "Tianhang Zheng",
                "Hua Zhang",
                "Di Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17695v1",
                "http://arxiv.org/pdf/2311.17695v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.CY",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17693v1",
            "title": "Toward a Surgeon-in-the-Loop Ophthalmic Robotic Apprentice using\n  Reinforcement and Imitation Learning",
            "updated": "2023-11-29T15:00:06Z",
            "published": "2023-11-29T15:00:06Z",
            "summary": "Robotic-assisted surgical systems have demonstrated significant potential in\nenhancing surgical precision and minimizing human errors. However, existing\nsystems lack the ability to accommodate the unique preferences and requirements\nof individual surgeons. Additionally, they primarily focus on general surgeries\n(e.g., laparoscopy) and are not suitable for highly precise microsurgeries,\nsuch as ophthalmic procedures. Thus, we propose a simulation-based image-guided\napproach for surgeon-centered autonomous agents that can adapt to the\nindividual surgeon's skill level and preferred surgical techniques during\nophthalmic cataract surgery. Our approach utilizes a simulated environment to\ntrain reinforcement and imitation learning agents guided by image data to\nperform all tasks of the incision phase of cataract surgery. By integrating the\nsurgeon's actions and preferences into the training process with the\nsurgeon-in-the-loop, our approach enables the robot to implicitly learn and\nadapt to the individual surgeon's unique approach through demonstrations. This\nresults in a more intuitive and personalized surgical experience for the\nsurgeon. Simultaneously, it ensures consistent performance for the autonomous\nrobotic apprentice. We define and evaluate the effectiveness of our approach\nusing our proposed metrics; and highlight the trade-off between a generic agent\nand a surgeon-centered adapted agent. Moreover, our approach has the potential\nto extend to other ophthalmic surgical procedures, opening the door to a new\ngeneration of surgeon-in-the-loop autonomous surgical robots. We provide an\nopen-source simulation framework for future development and reproducibility.",
            "author": [
                "Amr Gomaa",
                "Bilal Mahdy",
                "Niko Kleer",
                "Antonio Kr\u00fcger"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17693v1",
                "http://arxiv.org/pdf/2311.17693v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.CV",
                "cs.HC",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17685v1",
            "title": "Enhancing efficiency and robustness in high-dimensional linear\n  regression with additional unlabeled data",
            "updated": "2023-11-29T14:47:16Z",
            "published": "2023-11-29T14:47:16Z",
            "summary": "In semi-supervised learning, the prevailing understanding suggests that\nobserving additional unlabeled samples improves estimation accuracy for linear\nparameters only in the case of model misspecification. This paper challenges\nthis notion, demonstrating its inaccuracy in high dimensions. Initially\nfocusing on a dense scenario, we introduce robust semi-supervised estimators\nfor the regression coefficient without relying on sparse structures in the\npopulation slope. Even when the true underlying model is linear, we show that\nleveraging information from large-scale unlabeled data improves both estimation\naccuracy and inference robustness. Moreover, we propose semi-supervised methods\nwith further enhanced efficiency in scenarios with a sparse linear slope.\nDiverging from the standard semi-supervised literature, we also allow for\ncovariate shift. The performance of the proposed methods is illustrated through\nextensive numerical studies, including simulations and a real-data application\nto the AIDS Clinical Trials Group Protocol 175 (ACTG175).",
            "author": [
                "Kai Chen",
                "Yuqian Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17685v1",
                "http://arxiv.org/pdf/2311.17685v1"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17677v1",
            "title": "COVIDx CXR-4: An Expanded Multi-Institutional Open-Source Benchmark\n  Dataset for Chest X-ray Image-Based Computer-Aided COVID-19 Diagnostics",
            "updated": "2023-11-29T14:40:31Z",
            "published": "2023-11-29T14:40:31Z",
            "summary": "The global ramifications of the COVID-19 pandemic remain significant,\nexerting persistent pressure on nations even three years after its initial\noutbreak. Deep learning models have shown promise in improving COVID-19\ndiagnostics but require diverse and larger-scale datasets to improve\nperformance. In this paper, we introduce COVIDx CXR-4, an expanded\nmulti-institutional open-source benchmark dataset for chest X-ray image-based\ncomputer-aided COVID-19 diagnostics. COVIDx CXR-4 expands significantly on the\nprevious COVIDx CXR-3 dataset by increasing the total patient cohort size by\ngreater than 2.66 times, resulting in 84,818 images from 45,342 patients across\nmultiple institutions. We provide extensive analysis on the diversity of the\npatient demographic, imaging metadata, and disease distributions to highlight\npotential dataset biases. To the best of the authors' knowledge, COVIDx CXR-4\nis the largest and most diverse open-source COVID-19 CXR dataset and is made\npublicly available as part of an open initiative to advance research to aid\nclinicians against the COVID-19 disease.",
            "author": [
                "Yifan Wu",
                "Hayden Gunraj",
                "Chi-en Amy Tai",
                "Alexander Wong"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17677v1",
                "http://arxiv.org/pdf/2311.17677v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02182v1",
            "title": "Adam-like Algorithm with Smooth Clipping Attains Global Minima: Analysis\n  Based on Ergodicity of Functional SDEs",
            "updated": "2023-11-29T14:38:59Z",
            "published": "2023-11-29T14:38:59Z",
            "summary": "In this paper, we prove that an Adam-type algorithm with smooth clipping\napproaches the global minimizer of the regularized non-convex loss function.\nAdding smooth clipping and taking the state space as the set of all\ntrajectories, we can apply the ergodic theory of Markov semigroups for this\nalgorithm and investigate its asymptotic behavior. The ergodic theory we\nestablish in this paper reduces the problem of evaluating the convergence,\ngeneralization error and discretization error of this algorithm to the problem\nof evaluating the difference between two functional stochastic differential\nequations (SDEs) with different drift coefficients. As a result of our\nanalysis, we have shown that this algorithm minimizes the the regularized\nnon-convex loss function with errors of the form $n^{-1/2}$, $\\eta^{1/4}$,\n$\\beta^{-1} \\log (\\beta + 1)$ and $e^{- c t}$. Here, $c$ is a constant and $n$,\n$\\eta$, $\\beta$ and $t$ denote the size of the training dataset, learning rate,\ninverse temperature and time, respectively.",
            "author": [
                "Keisuke Suzuki"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02182v1",
                "http://arxiv.org/pdf/2312.02182v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "math.OC",
                "math.PR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17673v1",
            "title": "Using Ornstein-Uhlenbeck Process to understand Denoising Diffusion\n  Probabilistic Model and its Noise Schedules",
            "updated": "2023-11-29T14:36:33Z",
            "published": "2023-11-29T14:36:33Z",
            "summary": "The aim of this short note is to show that Denoising Diffusion Probabilistic\nModel DDPM, a non-homogeneous discrete-time Markov process, can be represented\nby a time-homogeneous continuous-time Markov process observed at non-uniformly\nsampled discrete times. Surprisingly, this continuous-time Markov process is\nthe well-known and well-studied Ornstein-Ohlenbeck (OU) process, which was\ndeveloped in 1930's for studying Brownian particles in Harmonic potentials. We\nestablish the formal equivalence between DDPM and the OU process using its\nanalytical solution. We further demonstrate that the design problem of the\nnoise scheduler for non-homogeneous DDPM is equivalent to designing observation\ntimes for the OU process. We present several heuristic designs for observation\ntimes based on principled quantities such as auto-variance and Fisher\nInformation and connect them to ad hoc noise schedules for DDPM. Interestingly,\nwe show that the Fisher-Information-motivated schedule corresponds exactly the\ncosine schedule, which was developed without any theoretical foundation but is\nthe current state-of-the-art noise schedule.",
            "author": [
                "Javier E. Santos",
                "Yen Ting Lin"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17673v1",
                "http://arxiv.org/pdf/2311.17673v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cond-mat.stat-mech",
                "cs.AI",
                "cs.LG",
                "math-ph",
                "math.MP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17662v1",
            "title": "Issue Report Validation in an Industrial Context",
            "updated": "2023-11-29T14:24:13Z",
            "published": "2023-11-29T14:24:13Z",
            "summary": "Effective issue triaging is crucial for software development teams to improve\nsoftware quality, and thus customer satisfaction. Validating issue reports\nmanually can be time-consuming, hindering the overall efficiency of the\ntriaging process. This paper presents an approach on automating the validation\nof issue reports to accelerate the issue triaging process in an industrial\nset-up. We work on 1,200 randomly selected issue reports in banking domain,\nwritten in Turkish, an agglutinative language, meaning that new words can be\nformed with linear concatenation of suffixes to express entire sentences. We\nmanually label these reports for validity, and extract the relevant patterns\nindicating that they are invalid. Since the issue reports we work on are\nwritten in an agglutinative language, we use morphological analysis to extract\nthe features. Using the proposed feature extractors, we utilize a machine\nlearning based approach to predict the issue reports' validity, performing a\n0.77 F1-score.",
            "author": [
                "Ethem Utku Aktas",
                "Ebru Cakmak",
                "Mete Cihad Inan",
                "Cemal Yilmaz"
            ],
            "link": [
                "http://dx.doi.org/10.1145/3611643.3613887",
                "http://arxiv.org/abs/2311.17662v1",
                "http://arxiv.org/pdf/2311.17662v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17657v1",
            "title": "Volumetric Cloud Field Reconstruction",
            "updated": "2023-11-29T14:19:40Z",
            "published": "2023-11-29T14:19:40Z",
            "summary": "Volumetric phenomena, such as clouds and fog, present a significant challenge\nfor 3D reconstruction systems due to their translucent nature and their complex\ninteractions with light. Conventional techniques for reconstructing scattering\nvolumes rely on controlled setups, limiting practical applications. This paper\nintroduces an approach to reconstructing volumes from a few input stereo pairs.\nWe propose a novel deep learning framework that integrates a deep stereo model\nwith a 3D Convolutional Neural Network (3D CNN) and an advection module,\ncapable of capturing the shape and dynamics of volumes. The stereo depths are\nused to carve empty space around volumes, providing the 3D CNN with a prior for\ncoping with the lack of input views. Refining our output, the advection module\nleverages the temporal evolution of the medium, providing a mechanism to infer\nmotion and improve temporal consistency. The efficacy of our system is\ndemonstrated through its ability to estimate density and velocity fields of\nlarge-scale volumes, in this case, clouds, from a sparse set of stereo image\npairs.",
            "author": [
                "Jacob Lin",
                "Miguel Farinha",
                "Edward Gryspeerdt",
                "Ronald Clark"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17657v1",
                "http://arxiv.org/pdf/2311.17657v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17647v1",
            "title": "VIM: Probing Multimodal Large Language Models for Visual Embedded\n  Instruction Following",
            "updated": "2023-11-29T14:08:53Z",
            "published": "2023-11-29T14:08:53Z",
            "summary": "We introduce VISUAL EMBEDDED INSTRUCTION (VIM), a new framework designed to\nevaluate the visual instruction following capability of Multimodal Large\nLanguage Models (MLLMs). As illustrated in Figure 2, VIM challenges the MLLMs\nby embedding the instructions into the visual scenes, demanding strong visual\ninterpretative skills for instruction following. We adapt VIM to various\nbenchmarks, including VQAv2, MME, MM-Vet, and RefCOCO series, compose a VIM\nbench, and probe diverse MLLMs across three distinct in-context learning\nsettings: Zero Shot, One Shot, and Pair Shot. We observe that there is a\nsignificant performance disparity between the open-source MLLMs and GPT-4V,\nimplying that their proficiency in visual instruction comprehension is not up\nto par. Our results highlight a promising direction for the enhancement of\nMLLMs capabilities on instruction following. We aim VIM to serve as a useful\nnorm for advancing the state of the art and driving further progress in the\nfield.",
            "author": [
                "Yujie Lu",
                "Xiujun Li",
                "William Yang Wang",
                "Yejin Choi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17647v1",
                "http://arxiv.org/pdf/2311.17647v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17646v1",
            "title": "A novel feature selection method based on quantum support vector machine",
            "updated": "2023-11-29T14:08:26Z",
            "published": "2023-11-29T14:08:26Z",
            "summary": "Feature selection is critical in machine learning to reduce dimensionality\nand improve model accuracy and efficiency. The exponential growth in feature\nspace dimensionality for modern datasets directly results in ambiguous samples\nand redundant features, which can severely degrade classification accuracy.\nQuantum machine learning offers potential advantages for addressing this\nchallenge. In this paper, we propose a novel method, quantum support vector\nmachine feature selection (QSVMF), integrating quantum support vector machines\nwith multi-objective genetic algorithm. QSVMF optimizes multiple simultaneous\nobjectives: maximizing classification accuracy, minimizing selected features\nand quantum circuit costs, and reducing feature covariance. We apply QSVMF for\nfeature selection on a breast cancer dataset, comparing the performance of\nQSVMF against classical approaches with the selected features. Experimental\nresults show that QSVMF achieves superior performance. Furthermore, The Pareto\nfront solutions of QSVMF enable analysis of accuracy versus feature set size\ntrade-offs, identifying extremely sparse yet accurate feature subsets. We\ncontextualize the biological relevance of the selected features in terms of\nknown breast cancer biomarkers. This work highlights the potential of\nquantum-based feature selection to enhance machine learning efficiency and\nperformance on complex real-world data.",
            "author": [
                "Haiyan Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17646v1",
                "http://arxiv.org/pdf/2311.17646v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17633v1",
            "title": "Introduction to Transformers: an NLP Perspective",
            "updated": "2023-11-29T13:51:04Z",
            "published": "2023-11-29T13:51:04Z",
            "summary": "Transformers have dominated empirical machine learning models of natural\nlanguage processing. In this paper, we introduce basic concepts of Transformers\nand present key techniques that form the recent advances of these models. This\nincludes a description of the standard Transformer architecture, a series of\nmodel refinements, and common applications. Given that Transformers and related\ndeep learning techniques might be evolving in ways we have never seen, we\ncannot dive into all the model details or cover all the technical areas.\nInstead, we focus on just those concepts that are helpful for gaining a good\nunderstanding of Transformers and their variants. We also summarize the key\nideas that impact this field, thereby yielding some insights into the strengths\nand limitations of these models.",
            "author": [
                "Tong Xiao",
                "Jingbo Zhu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17633v1",
                "http://arxiv.org/pdf/2311.17633v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17631v1",
            "title": "Q-learning Based Optimal False Data Injection Attack on Probabilistic\n  Boolean Control Networks",
            "updated": "2023-11-29T13:45:07Z",
            "published": "2023-11-29T13:45:07Z",
            "summary": "In this paper, we present a reinforcement learning (RL) method for solving\noptimal false data injection attack problems in probabilistic Boolean control\nnetworks (PBCNs) where the attacker lacks knowledge of the system model.\nSpecifically, we employ a Q-learning (QL) algorithm to address this problem. We\nthen propose an improved QL algorithm that not only enhances learning\nefficiency but also obtains optimal attack strategies for large-scale PBCNs\nthat the standard QL algorithm cannot handle. Finally, we verify the\neffectiveness of our proposed approach by considering two attacked PBCNs,\nincluding a 10-node network and a 28-node network.",
            "author": [
                "Xianlun Peng",
                "Yang Tang",
                "Fangfei Li",
                "Yang Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17631v1",
                "http://arxiv.org/pdf/2311.17631v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.CR",
                "cs.LG",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17621v1",
            "title": "The AutoSPADA Platform: User-Friendly Edge Computing for Distributed\n  Learning and Data Analytics in Connected Vehicles",
            "updated": "2023-11-29T13:30:26Z",
            "published": "2023-11-29T13:30:26Z",
            "summary": "Contemporary connected vehicles host numerous applications, such as\ndiagnostics and navigation, and new software is continuously being developed.\nHowever, the development process typically requires offline batch processing of\nlarge data volumes. In an edge computing approach, data analysts and developers\ncan instead process sensor data directly on computational resources inside\nvehicles. This enables rapid prototyping to shorten development cycles and\nreduce the time to create new business values or insights. This paper presents\nthe design, implementation, and operation of the AutoSPADA edge computing\nplatform for distributed data analytics. The platform's design follows\nscalability, reliability, resource efficiency, privacy, and security principles\npromoted through mature and industrially proven technologies. In AutoSPADA,\ncomputational tasks are general Python scripts, and we provide a library to,\nfor example, read signals from the vehicle and publish results to the cloud.\nHence, users only need Python knowledge to use the platform. Moreover, the\nplatform is designed to be extended to support additional programming\nlanguages.",
            "author": [
                "Adrian Nilsson",
                "Simon Smith",
                "Jonas Hagmar",
                "Magnus \u00d6nnheim",
                "Mats Jirstrand"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17621v1",
                "http://arxiv.org/pdf/2311.17621v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC",
                "cs.NI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17618v3",
            "title": "ShapeGPT: 3D Shape Generation with A Unified Multi-modal Language Model",
            "updated": "2023-12-01T12:46:13Z",
            "published": "2023-11-29T13:26:29Z",
            "summary": "The advent of large language models, enabling flexibility through\ninstruction-driven approaches, has revolutionized many traditional generative\ntasks, but large models for 3D data, particularly in comprehensively handling\n3D shapes with other modalities, are still under-explored. By achieving\ninstruction-based shape generations, versatile multimodal generative shape\nmodels can significantly benefit various fields like 3D virtual construction\nand network-aided design. In this work, we present ShapeGPT, a shape-included\nmulti-modal framework to leverage strong pre-trained language models to address\nmultiple shape-relevant tasks. Specifically, ShapeGPT employs a\nword-sentence-paragraph framework to discretize continuous shapes into shape\nwords, further assembles these words for shape sentences, as well as integrates\nshape with instructional text for multi-modal paragraphs. To learn this\nshape-language model, we use a three-stage training scheme, including shape\nrepresentation, multimodal alignment, and instruction-based generation, to\nalign shape-language codebooks and learn the intricate correlations among these\nmodalities. Extensive experiments demonstrate that ShapeGPT achieves comparable\nperformance across shape-relevant tasks, including text-to-shape,\nshape-to-text, shape completion, and shape editing.",
            "author": [
                "Fukun Yin",
                "Xin Chen",
                "Chi Zhang",
                "Biao Jiang",
                "Zibo Zhao",
                "Jiayuan Fan",
                "Gang Yu",
                "Taihao Li",
                "Tao Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17618v3",
                "http://arxiv.org/pdf/2311.17618v3"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17609v1",
            "title": "AnyLens: A Generative Diffusion Model with Any Rendering Lens",
            "updated": "2023-11-29T13:06:48Z",
            "published": "2023-11-29T13:06:48Z",
            "summary": "State-of-the-art diffusion models can generate highly realistic images based\non various conditioning like text, segmentation, and depth. However, an\nessential aspect often overlooked is the specific camera geometry used during\nimage capture. The influence of different optical systems on the final scene\nappearance is frequently overlooked. This study introduces a framework that\nintimately integrates a text-to-image diffusion model with the particular lens\ngeometry used in image rendering. Our method is based on a per-pixel coordinate\nconditioning method, enabling the control over the rendering geometry. Notably,\nwe demonstrate the manipulation of curvature properties, achieving diverse\nvisual effects, such as fish-eye, panoramic views, and spherical texturing\nusing a single diffusion model.",
            "author": [
                "Andrey Voynov",
                "Amir Hertz",
                "Moab Arar",
                "Shlomi Fruchter",
                "Daniel Cohen-Or"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17609v1",
                "http://arxiv.org/pdf/2311.17609v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17608v1",
            "title": "Adversarial Robust Memory-Based Continual Learner",
            "updated": "2023-11-29T13:05:20Z",
            "published": "2023-11-29T13:05:20Z",
            "summary": "Despite the remarkable advances that have been made in continual learning,\nthe adversarial vulnerability of such methods has not been fully discussed. We\ndelve into the adversarial robustness of memory-based continual learning\nalgorithms and observe limited robustness improvement by directly applying\nadversarial training techniques. Preliminary studies reveal the twin challenges\nfor building adversarial robust continual learners: accelerated forgetting in\ncontinual learning and gradient obfuscation in adversarial robustness. In this\nstudy, we put forward a novel adversarial robust memory-based continual learner\nthat adjusts data logits to mitigate the forgetting of pasts caused by\nadversarial samples. Furthermore, we devise a gradient-based data selection\nmechanism to overcome the gradient obfuscation caused by limited stored data.\nThe proposed approach can widely integrate with existing memory-based continual\nlearning as well as adversarial training algorithms in a plug-and-play way.\nExtensive experiments on Split-CIFAR10/100 and Split-Tiny-ImageNet demonstrate\nthe effectiveness of our approach, achieving up to 8.13% higher accuracy for\nadversarial data.",
            "author": [
                "Xiaoyue Mi",
                "Fan Tang",
                "Zonghan Yang",
                "Danding Wang",
                "Juan Cao",
                "Peng Li",
                "Yang Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17608v1",
                "http://arxiv.org/pdf/2311.17608v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17607v1",
            "title": "Topology-Preserving Adversarial Training",
            "updated": "2023-11-29T13:05:06Z",
            "published": "2023-11-29T13:05:06Z",
            "summary": "Despite the effectiveness in improving the robustness of neural networks,\nadversarial training has suffered from the natural accuracy degradation\nproblem, i.e., accuracy on natural samples has reduced significantly. In this\nstudy, we reveal that natural accuracy degradation is highly related to the\ndisruption of the natural sample topology in the representation space by\nquantitative and qualitative experiments. Based on this observation, we propose\nTopology-pReserving Adversarial traINing (TRAIN) to alleviate the problem by\npreserving the topology structure of natural samples from a standard model\ntrained only on natural samples during adversarial training. As an additional\nregularization, our method can easily be combined with various popular\nadversarial training algorithms in a plug-and-play manner, taking advantage of\nboth sides. Extensive experiments on CIFAR-10, CIFAR-100, and Tiny ImageNet\nshow that our proposed method achieves consistent and significant improvements\nover various strong baselines in most cases. Specifically, without additional\ndata, our proposed method achieves up to 8.78% improvement in natural accuracy\nand 4.50% improvement in robust accuracy.",
            "author": [
                "Xiaoyue Mi",
                "Fan Tang",
                "Yepeng Weng",
                "Danding Wang",
                "Juan Cao",
                "Sheng Tang",
                "Peng Li",
                "Yang Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17607v1",
                "http://arxiv.org/pdf/2311.17607v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17603v1",
            "title": "sec-certs: Examining the security certification practice for better\n  vulnerability mitigation",
            "updated": "2023-11-29T12:55:16Z",
            "published": "2023-11-29T12:55:16Z",
            "summary": "Products certified under security certification frameworks such as Common\nCriteria undergo significant scrutiny during the costly certification process.\nYet, critical vulnerabilities, including private key recovery (ROCA, Minerva,\nTPM-Fail...), get discovered in certified products with high assurance levels.\nFurthermore, assessing which certified products are impacted by such\nvulnerabilities is complicated due to the large amount of unstructured\ncertification-related data and unclear relationships between the certificates.\nTo address these problems, we conducted a large-scale automated analysis of\nCommon Criteria and FIPS 140 certificates. We trained unsupervised models to\nlearn which vulnerabilities from NIST's National Vulnerability Database impact\nexisting certified products and how certified products reference each other.\nOur tooling automates the analysis of tens of thousands of\ncertification-related documents, extracting machine-readable features where\nmanual analysis is unattainable. Further, we identify the security requirements\nthat are associated with products being affected by fewer and less severe\nvulnerabilities (on average). This indicates which aspects of certification\ncorrelate with higher security. We demonstrate how our tool can be used for\nbetter vulnerability mitigation on four case studies of known, high-profile\nvulnerabilities. All tools and continuously updated results are available at\nhttps://seccerts.org.",
            "author": [
                "Adam Janovsky",
                "Jan Jancar",
                "Petr Svenda",
                "\u0141ukasz Chmielewski",
                "Jiri Michalik",
                "Vashek Matyas"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17603v1",
                "http://arxiv.org/pdf/2311.17603v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17601v1",
            "title": "Continual Learning with Low Rank Adaptation",
            "updated": "2023-11-29T12:53:32Z",
            "published": "2023-11-29T12:53:32Z",
            "summary": "Recent work using pretrained transformers has shown impressive performance\nwhen fine-tuned with data from the downstream problem of interest. However,\nthey struggle to retain that performance when the data characteristics changes.\nIn this paper, we focus on continual learning, where a pre-trained transformer\nis updated to perform well on new data, while retaining its performance on data\nit was previously trained on. Earlier works have tackled this primarily through\nmethods inspired from prompt tuning. We question this choice, and investigate\nthe applicability of Low Rank Adaptation (LoRA) to continual learning. On a\nrange of domain-incremental learning benchmarks, our LoRA-based solution,\nCoLoR, yields state-of-the-art performance, while still being as parameter\nefficient as the prompt tuning based methods.",
            "author": [
                "Martin Wistuba",
                "Prabhu Teja Sivaprasad",
                "Lukas Balles",
                "Giovanni Zappella"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17601v1",
                "http://arxiv.org/pdf/2311.17601v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17598v1",
            "title": "Improving embedding of graphs with missing data by soft manifolds",
            "updated": "2023-11-29T12:48:33Z",
            "published": "2023-11-29T12:48:33Z",
            "summary": "Embedding graphs in continous spaces is a key factor in designing and\ndeveloping algorithms for automatic information extraction to be applied in\ndiverse tasks (e.g., learning, inferring, predicting). The reliability of graph\nembeddings directly depends on how much the geometry of the continuous space\nmatches the graph structure. Manifolds are mathematical structure that can\nenable to incorporate in their topological spaces the graph characteristics,\nand in particular nodes distances. State-of-the-art of manifold-based graph\nembedding algorithms take advantage of the assumption that the projection on a\ntangential space of each point in the manifold (corresponding to a node in the\ngraph) would locally resemble a Euclidean space. Although this condition helps\nin achieving efficient analytical solutions to the embedding problem, it does\nnot represent an adequate set-up to work with modern real life graphs, that are\ncharacterized by weighted connections across nodes often computed over sparse\ndatasets with missing records. In this work, we introduce a new class of\nmanifold, named soft manifold, that can solve this situation. In particular,\nsoft manifolds are mathematical structures with spherical symmetry where the\ntangent spaces to each point are hypocycloids whose shape is defined according\nto the velocity of information propagation across the data points. Using soft\nmanifolds for graph embedding, we can provide continuous spaces to pursue any\ntask in data analysis over complex datasets. Experimental results on\nreconstruction tasks on synthetic and real datasets show how the proposed\napproach enable more accurate and reliable characterization of graphs in\ncontinuous spaces with respect to the state-of-the-art.",
            "author": [
                "Andrea Marinoni",
                "Pietro Lio'",
                "Alessandro Barp",
                "Christian Jutten",
                "Mark Girolami"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17598v1",
                "http://arxiv.org/pdf/2311.17598v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17597v2",
            "title": "Continual Self-supervised Learning: Towards Universal Multi-modal\n  Medical Data Representation Learning",
            "updated": "2023-11-30T02:06:13Z",
            "published": "2023-11-29T12:47:42Z",
            "summary": "Self-supervised learning is an efficient pre-training method for medical\nimage analysis. However, current research is mostly confined to\nspecific-modality data pre-training, consuming considerable time and resources\nwithout achieving universality across different modalities. A straightforward\nsolution is combining all modality data for joint self-supervised pre-training,\nwhich poses practical challenges. Firstly, our experiments reveal conflicts in\nrepresentation learning as the number of modalities increases. Secondly,\nmulti-modal data collected in advance cannot cover all real-world scenarios. In\nthis paper, we reconsider versatile self-supervised learning from the\nperspective of continual learning and propose MedCoSS, a continuous\nself-supervised learning approach for multi-modal medical data. Unlike joint\nself-supervised learning, MedCoSS assigns different modality data to different\ntraining stages, forming a multi-stage pre-training process. To balance modal\nconflicts and prevent catastrophic forgetting, we propose a rehearsal-based\ncontinual learning method. We introduce the k-means sampling strategy to retain\ndata from previous modalities and rehearse it when learning new modalities.\nInstead of executing the pretext task on buffer data, a feature distillation\nstrategy and an intra-modal mixup strategy are applied to these data for\nknowledge retention. We conduct continuous self-supervised pre-training on a\nlarge-scale multi-modal unlabeled dataset, including clinical reports, X-rays,\nCT scans, MRI scans, and pathological images. Experimental results demonstrate\nMedCoSS's exceptional generalization ability across nine downstream datasets\nand its significant scalability in integrating new modality data. Code and\npre-trained weight are available at https://github.com/yeerwen/MedCoSS.",
            "author": [
                "Yiwen Ye",
                "Yutong Xie",
                "Jianpeng Zhang",
                "Ziyang Chen",
                "Qi Wu",
                "Yong Xia"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17597v2",
                "http://arxiv.org/pdf/2311.17597v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17593v1",
            "title": "LanGWM: Language Grounded World Model",
            "updated": "2023-11-29T12:41:55Z",
            "published": "2023-11-29T12:41:55Z",
            "summary": "Recent advances in deep reinforcement learning have showcased its potential\nin tackling complex tasks. However, experiments on visual control tasks have\nrevealed that state-of-the-art reinforcement learning models struggle with\nout-of-distribution generalization. Conversely, expressing higher-level\nconcepts and global contexts is relatively easy using language.\n  Building upon recent success of the large language models, our main objective\nis to improve the state abstraction technique in reinforcement learning by\nleveraging language for robust action selection. Specifically, we focus on\nlearning language-grounded visual features to enhance the world model learning,\na model-based reinforcement learning technique.\n  To enforce our hypothesis explicitly, we mask out the bounding boxes of a few\nobjects in the image observation and provide the text prompt as descriptions\nfor these masked objects. Subsequently, we predict the masked objects along\nwith the surrounding regions as pixel reconstruction, similar to the\ntransformer-based masked autoencoder approach.\n  Our proposed LanGWM: Language Grounded World Model achieves state-of-the-art\nperformance in out-of-distribution test at the 100K interaction steps\nbenchmarks of iGibson point navigation tasks. Furthermore, our proposed\ntechnique of explicit language-grounded visual representation learning has the\npotential to improve models for human-robot interaction because our extracted\nvisual features are language grounded.",
            "author": [
                "Rudra P. K. Poudel",
                "Harit Pandya",
                "Chao Zhang",
                "Roberto Cipolla"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17593v1",
                "http://arxiv.org/pdf/2311.17593v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CL",
                "cs.CV",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17592v1",
            "title": "Robust Correlated Equilibrium: Definition and Computation",
            "updated": "2023-11-29T12:41:17Z",
            "published": "2023-11-29T12:41:17Z",
            "summary": "We study N-player finite games with costs perturbed due to time-varying\ndisturbances in the underlying system and to that end we propose the concept of\nRobust Correlated Equilibrium that generalizes the definition of Correlated\nEquilibrium. Conditions under which the Robust Correlated Equilibrium exists\nare specified and a decentralized algorithm for learning strategies that are\noptimal in the sense of Robust Correlated Equilibrium is proposed. The primary\ncontribution of the paper is the convergence analysis of the algorithm and to\nthat end, we propose an extension of the celebrated Blackwell's Approachability\ntheorem to games with costs that are not just time-average as in the original\nBlackwell's Approachability Theorem but also include time-average of previous\nalgorithm iterates. The designed algorithm is applied to a practical water\ndistribution network with pumps being the controllers and their costs being\nperturbed by uncertain consumption by consumers. Simulation results show that\neach controller achieves no regret and empirical distributions converge to the\nRobust Correlated Equilibrium.",
            "author": [
                "Rahul Misra",
                "Rafa\u0142 Wisniewski",
                "Carsten Skovmose Kalles\u00f8e",
                "Manuela L. Bujorianu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17592v1",
                "http://arxiv.org/pdf/2311.17592v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.MA",
                "cs.SY",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17968v1",
            "title": "Latent Alignment with Deep Set EEG Decoders",
            "updated": "2023-11-29T12:40:45Z",
            "published": "2023-11-29T12:40:45Z",
            "summary": "The variability in EEG signals between different individuals poses a\nsignificant challenge when implementing brain-computer interfaces (BCI).\nCommonly proposed solutions to this problem include deep learning models, due\nto their increased capacity and generalization, as well as explicit domain\nadaptation techniques. Here, we introduce the Latent Alignment method that won\nthe Benchmarks for EEG Transfer Learning (BEETL) competition and present its\nformulation as a deep set applied on the set of trials from a given subject.\nIts performance is compared to recent statistical domain adaptation techniques\nunder various conditions. The experimental paradigms include motor imagery\n(MI), oddball event-related potentials (ERP) and sleep stage classification,\nwhere different well-established deep learning models are applied on each task.\nOur experimental results show that performing statistical distribution\nalignment at later stages in a deep learning model is beneficial to the\nclassification accuracy, yielding the highest performance for our proposed\nmethod. We further investigate practical considerations that arise in the\ncontext of using deep learning and statistical alignment for EEG decoding. In\nthis regard, we study class-discriminative artifacts that can spuriously\nimprove results for deep learning models, as well as the impact of\nclass-imbalance on alignment. We delineate a trade-off relationship between\nincreased classification accuracy when alignment is performed at later modeling\nstages, and susceptibility to class-imbalance in the set of trials that the\nstatistics are computed on.",
            "author": [
                "Stylianos Bakas",
                "Siegfried Ludwig",
                "Dimitrios A. Adamos",
                "Nikolaos Laskaris",
                "Yannis Panagakis",
                "Stefanos Zafeiriou"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17968v1",
                "http://arxiv.org/pdf/2311.17968v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP",
                "cs.AI",
                "cs.HC",
                "cs.LG",
                "I.2.6"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17967v1",
            "title": "Discovering Galaxy Features via Dataset Distillation",
            "updated": "2023-11-29T12:39:31Z",
            "published": "2023-11-29T12:39:31Z",
            "summary": "In many applications, Neural Nets (NNs) have classification performance on\npar or even exceeding human capacity. Moreover, it is likely that NNs leverage\nunderlying features that might differ from those humans perceive to classify.\nCan we \"reverse-engineer\" pertinent features to enhance our scientific\nunderstanding? Here, we apply this idea to the notoriously difficult task of\ngalaxy classification: NNs have reached high performance for this task, but\nwhat does a neural net (NN) \"see\" when it classifies galaxies? Are there\nmorphological features that the human eye might overlook that could help with\nthe task and provide new insights? Can we visualize tracers of early evolution,\nor additionally incorporated spectral data? We present a novel way to summarize\nand visualize galaxy morphology through the lens of neural networks, leveraging\nDataset Distillation, a recent deep-learning methodology with the primary\nobjective to distill knowledge from a large dataset and condense it into a\ncompact synthetic dataset, such that a model trained on this synthetic dataset\nachieves performance comparable to a model trained on the full dataset. We\ncurate a class-balanced, medium-size high-confidence version of the Galaxy Zoo\n2 dataset, and proceed with dataset distillation from our accurate\nNN-classifier to create synthesized prototypical images of galaxy morphological\nfeatures, demonstrating its effectiveness. Of independent interest, we\nintroduce a self-adaptive version of the state-of-the-art Matching Trajectory\nalgorithm to automate the distillation process, and show enhanced performance\non computer vision benchmarks.",
            "author": [
                "Haowen Guan",
                "Xuan Zhao",
                "Zishi Wang",
                "Zhiyang Li",
                "Julia Kempe"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17967v1",
                "http://arxiv.org/pdf/2311.17967v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "astro-ph.IM",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17587v1",
            "title": "Deep Reinforcement Learning Graphs: Feedback Motion Planning via Neural\n  Lyapunov Verification",
            "updated": "2023-11-29T12:31:06Z",
            "published": "2023-11-29T12:31:06Z",
            "summary": "Recent advancements in model-free deep reinforcement learning have enabled\nefficient agent training. However, challenges arise when determining the region\nof attraction for these controllers, especially if the region does not fully\ncover the desired area. This paper addresses this issue by introducing a\nfeedback motion control algorithm that utilizes data-driven techniques and\nneural networks. The algorithm constructs a graph of connected\nreinforcement-learning based controllers, each with its own defined region of\nattraction. This incremental approach effectively covers a bounded region of\ninterest, creating a trajectory of interconnected nodes that guide the system\nfrom an initial state to the goal. Two approaches are presented for connecting\nnodes within the algorithm. The first is a tree-structured method, facilitating\n\"point-to-point\" control by constructing a tree connecting the initial state to\nthe goal state. The second is a graph-structured method, enabling\n\"space-to-space\" control by building a graph within a bounded region. This\napproach allows for control from arbitrary initial and goal states. The\nproposed method's performance is evaluated on a first-order dynamic system,\nconsidering scenarios both with and without obstacles. The results demonstrate\nthe effectiveness of the proposed algorithm in achieving the desired control\nobjectives.",
            "author": [
                "Armin Ghanbarzadeh",
                "Esmaeil Najafi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17587v1",
                "http://arxiv.org/pdf/2311.17587v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.RO",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17586v1",
            "title": "Federated Online and Bandit Convex Optimization",
            "updated": "2023-11-29T12:29:54Z",
            "published": "2023-11-29T12:29:54Z",
            "summary": "We study the problems of distributed online and bandit convex optimization\nagainst an adaptive adversary. We aim to minimize the average regret on $M$\nmachines working in parallel over $T$ rounds with $R$ intermittent\ncommunications. Assuming the underlying cost functions are convex and can be\ngenerated adaptively, our results show that collaboration is not beneficial\nwhen the machines have access to the first-order gradient information at the\nqueried points. This is in contrast to the case for stochastic functions, where\neach machine samples the cost functions from a fixed distribution. Furthermore,\nwe delve into the more challenging setting of federated online optimization\nwith bandit (zeroth-order) feedback, where the machines can only access values\nof the cost functions at the queried points. The key finding here is\nidentifying the high-dimensional regime where collaboration is beneficial and\nmay even lead to a linear speedup in the number of machines. We further\nillustrate our findings through federated adversarial linear bandits by\ndeveloping novel distributed single and two-point feedback algorithms. Our work\nis the first attempt towards a systematic understanding of federated online\noptimization with limited feedback, and it attains tight regret bounds in the\nintermittent communication setting for both first and zeroth-order feedback.\nOur results thus bridge the gap between stochastic and adaptive settings in\nfederated online optimization.",
            "author": [
                "Kumar Kshitij Patel",
                "Lingxiao Wang",
                "Aadirupa Saha",
                "Nati Sebro"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17586v1",
                "http://arxiv.org/pdf/2311.17586v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "math.OC",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17583v1",
            "title": "CLIPC8: Face liveness detection algorithm based on image-text pairs and\n  contrastive learning",
            "updated": "2023-11-29T12:21:42Z",
            "published": "2023-11-29T12:21:42Z",
            "summary": "Face recognition technology is widely used in the financial field, and\nvarious types of liveness attack behaviors need to be addressed. Existing\nliveness detection algorithms are trained on specific training datasets and\ntested on testing datasets, but their performance and robustness in\ntransferring to unseen datasets are relatively poor. To tackle this issue, we\npropose a face liveness detection method based on image-text pairs and\ncontrastive learning, dividing liveness attack problems in the financial field\ninto eight categories and using text information to describe the images of\nthese eight types of attacks. The text encoder and image encoder are used to\nextract feature vector representations for the classification description text\nand face images, respectively. By maximizing the similarity of positive samples\nand minimizing the similarity of negative samples, the model learns shared\nrepresentations between images and texts. The proposed method is capable of\neffectively detecting specific liveness attack behaviors in certain scenarios,\nsuch as those occurring in dark environments or involving the tampering of ID\ncard photos. Additionally, it is also effective in detecting traditional\nliveness attack methods, such as printing photo attacks and screen remake\nattacks. The zero-shot capabilities of face liveness detection on five public\ndatasets, including NUAA, CASIA-FASD, Replay-Attack, OULU-NPU and MSU-MFSD also\nreaches the level of commercial algorithms. The detection capability of\nproposed algorithm was verified on 5 types of testing datasets, and the results\nshow that the method outperformed commercial algorithms, and the detection\nrates reached 100% on multiple datasets. Demonstrating the effectiveness and\nrobustness of introducing image-text pairs and contrastive learning into\nliveness detection tasks as proposed in this paper.",
            "author": [
                "Xu Liu",
                "Shu Zhou",
                "Yurong Song",
                "Wenzhe Luo",
                "Xin Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17583v1",
                "http://arxiv.org/pdf/2311.17583v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17582v1",
            "title": "LoCoMotif: Discovering time-warped motifs in time series",
            "updated": "2023-11-29T12:18:46Z",
            "published": "2023-11-29T12:18:46Z",
            "summary": "Time Series Motif Discovery (TSMD) refers to the task of identifying patterns\nthat occur multiple times (possibly with minor variations) in a time series.\nAll existing methods for TSMD have one or more of the following limitations:\nthey only look for the two most similar occurrences of a pattern; they only\nlook for patterns of a pre-specified, fixed length; they cannot handle\nvariability along the time axis; and they only handle univariate time series.\nIn this paper, we present a new method, LoCoMotif, that has none of these\nlimitations. The method is motivated by a concrete use case from physiotherapy.\nWe demonstrate the value of the proposed method on this use case. We also\nintroduce a new quantitative evaluation metric for motif discovery, and\nbenchmark data for comparing TSMD methods. LoCoMotif substantially outperforms\nthe existing methods, on top of being more broadly applicable.",
            "author": [
                "Daan Van Wesenbeeck",
                "Aras Yurtman",
                "Wannes Meert",
                "Hendrik Blockeel"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17582v1",
                "http://arxiv.org/pdf/2311.17582v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17575v1",
            "title": "Identifying Causal Effects of Nonbinary, Ordered Treatments using\n  Multiple Instrumental Variables",
            "updated": "2023-11-29T12:11:44Z",
            "published": "2023-11-29T12:11:44Z",
            "summary": "This paper addresses the challenge of identifying causal effects of\nnonbinary, ordered treatments with multiple binary instruments. Next to\npresenting novel insights into the widely-applied two-stage least squares\nestimand, I show that a weighted average of local average treatment effects for\ncombined complier populations is identified under the limited monotonicity\nassumption. This novel causal parameter has an intuitive interpretation,\noffering an appealing alternative to two-stage least squares. I employ recent\nadvances in causal machine learning for estimation. I further demonstrate how\ncausal forests can be used to detect local violations of the underlying limited\nmonotonicity assumption. The methodology is applied to study the impact of\ncommunity nurseries on child health outcomes.",
            "author": [
                "Nadja van 't Hoff"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17575v1",
                "http://arxiv.org/pdf/2311.17575v1"
            ],
            "primary_category": "econ.EM",
            "category": [
                "econ.EM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17965v1",
            "title": "Defining Reference Sequences for Nocardia Species by Similarity and\n  Clustering Analyses of 16S rRNA Gene Sequence Data",
            "updated": "2023-11-29T12:09:02Z",
            "published": "2023-11-29T12:09:02Z",
            "summary": "The intra- and inter-species genetic diversity of bacteria and the absence of\n'reference', or the most representative, sequences of individual species\npresent a significant challenge for sequence-based identification. The aims of\nthis study were to determine the utility, and compare the performance of\nseveral clustering and classification algorithms to identify the species of 364\nsequences of 16S rRNA gene with a defined species in GenBank, and 110 sequences\nof 16S rRNA gene with no defined species, all within the genus Nocardia. A\ntotal of 364 16S rRNA gene sequences of Nocardia species were studied. In\naddition, 110 16S rRNA gene sequences assigned only to the Nocardia genus level\nat the time of submission to GenBank were used for machine learning\nclassification experiments. Different clustering algorithms were compared with\na novel algorithm or the linear mapping (LM) of the distance matrix. Principal\nComponents Analysis was used for the dimensionality reduction and\nvisualization. Results: The LM algorithm achieved the highest performance and\nclassified the set of 364 16S rRNA sequences into 80 clusters, the majority of\nwhich (83.52%) corresponded with the original species. The most representative\n16S rRNA sequences for individual Nocardia species have been identified as\n'centroids' in respective clusters from which the distances to all other\nsequences were minimized; 110 16S rRNA gene sequences with identifications\nrecorded only at the genus level were classified using machine learning\nmethods. Simple kNN machine learning demonstrated the highest performance and\nclassified Nocardia species sequences with an accuracy of 92.7% and a mean\nfrequency of 0.578.",
            "author": [
                "Manal Helal",
                "Fanrong Kong",
                "Sharon C. A. Chen",
                "Michael Bain",
                "Richard Christen",
                "Vitali Sintchenko"
            ],
            "link": [
                "http://dx.doi.org/10.1371/journal.pone.0019517",
                "http://arxiv.org/abs/2311.17965v1",
                "http://arxiv.org/pdf/2311.17965v1"
            ],
            "primary_category": "q-bio.GN",
            "category": [
                "q-bio.GN",
                "cs.LG",
                "I.2.6"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17565v1",
            "title": "Bias Resilient Multi-Step Off-Policy Goal-Conditioned Reinforcement\n  Learning",
            "updated": "2023-11-29T11:59:03Z",
            "published": "2023-11-29T11:59:03Z",
            "summary": "In goal-conditioned reinforcement learning (GCRL), sparse rewards present\nsignificant challenges, often obstructing efficient learning. Although\nmulti-step GCRL can boost this efficiency, it can also lead to off-policy\nbiases in target values. This paper dives deep into these biases, categorizing\nthem into two distinct categories: \"shooting\" and \"shifting\". Recognizing that\ncertain behavior policies can hasten policy refinement, we present solutions\ndesigned to capitalize on the positive aspects of these biases while minimizing\ntheir drawbacks, enabling the use of larger step sizes to speed up GCRL. An\nempirical study demonstrates that our approach ensures a resilient and robust\nimprovement, even in ten-step learning scenarios, leading to superior learning\nefficiency and performance that generally surpass the baseline and several\nstate-of-the-art multi-step GCRL benchmarks.",
            "author": [
                "Lisheng Wu",
                "Ke Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17565v1",
                "http://arxiv.org/pdf/2311.17565v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17563v1",
            "title": "Efficient Computation of Sparse and Robust Maximum Association\n  Estimators",
            "updated": "2023-11-29T11:57:50Z",
            "published": "2023-11-29T11:57:50Z",
            "summary": "Although robust statistical estimators are less affected by outlying\nobservations, their computation is usually more challenging. This is\nparticularly the case in high-dimensional sparse settings. The availability of\nnew optimization procedures, mainly developed in the computer science domain,\noffers new possibilities for the field of robust statistics. This paper\ninvestigates how such procedures can be used for robust sparse association\nestimators. The problem can be split into a robust estimation step followed by\nan optimization for the remaining decoupled, (bi-)convex problem. A combination\nof the augmented Lagrangian algorithm and adaptive gradient descent is\nimplemented to also include suitable constraints for inducing sparsity. We\nprovide results concerning the precision of the algorithm and show the\nadvantages over existing algorithms in this context. High-dimensional empirical\nexamples underline the usefulness of this procedure. Extensions to other robust\nsparse estimators are possible.",
            "author": [
                "Pia Pfeiffer",
                "Andreas Alfons",
                "Peter Filzmoser"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17563v1",
                "http://arxiv.org/pdf/2311.17563v1"
            ],
            "primary_category": "stat.CO",
            "category": [
                "stat.CO",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17964v1",
            "title": "Linear normalised hash function for clustering gene sequences and\n  identifying reference sequences from multiple sequence alignments",
            "updated": "2023-11-29T11:51:05Z",
            "published": "2023-11-29T11:51:05Z",
            "summary": "The aim of this study was to develop a method that would identify the cluster\ncentroids and the optimal number of clusters for a given sensitivity level and\ncould work equally well for the different sequence datasets. A novel method\nthat combines the linear mapping hash function and multiple sequence alignment\n(MSA) was developed. This method takes advantage of the already sorted by\nsimilarity sequences from the MSA output, and identifies the optimal number of\nclusters, clusters cut-offs, and clusters centroids that can represent\nreference gene vouchers for the different species. The linear mapping hash\nfunction can map an already ordered by similarity distance matrix to indices to\nreveal gaps in the values around which the optimal cut-offs of the different\nclusters can be identified. The method was evaluated using sets of closely\nrelated (16S rRNA gene sequences of Nocardia species) and highly variable (VP1\ngenomic region of Enterovirus 71) sequences and outperformed existing\nunsupervised machine learning clustering methods and dimensionality reduction\nmethods. This method does not require prior knowledge of the number of clusters\nor the distance between clusters, handles clusters of different sizes and\nshapes, and scales linearly with the dataset. The combination of MSA with the\nlinear mapping hash function is a computationally efficient way of gene\nsequence clustering and can be a valuable tool for the assessment of\nsimilarity, clustering of different microbial genomes, identifying reference\nsequences, and for the study of evolution of bacteria and viruses.",
            "author": [
                "Manal Helal",
                "Fanrong Kong",
                "Sharon C-A Chen",
                "Fei Zhou",
                "Dominic E Dwyer",
                "John Potter",
                "Vitali Sintchenko"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17964v1",
                "http://arxiv.org/pdf/2311.17964v1"
            ],
            "primary_category": "q-bio.GN",
            "category": [
                "q-bio.GN",
                "cs.LG",
                "I.2.6"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17560v1",
            "title": "Interpreting Differentiable Latent States for Healthcare Time-series\n  Data",
            "updated": "2023-11-29T11:48:16Z",
            "published": "2023-11-29T11:48:16Z",
            "summary": "Machine learning enables extracting clinical insights from large temporal\ndatasets. The applications of such machine learning models include identifying\ndisease patterns and predicting patient outcomes. However, limited\ninterpretability poses challenges for deploying advanced machine learning in\ndigital healthcare. Understanding the meaning of latent states is crucial for\ninterpreting machine learning models, assuming they capture underlying\npatterns. In this paper, we present a concise algorithm that allows for i)\ninterpreting latent states using highly related input features; ii)\ninterpreting predictions using subsets of input features via latent states; and\niii) interpreting changes in latent states over time. The proposed algorithm is\nfeasible for any model that is differentiable. We demonstrate that this\napproach enables the identification of a daytime behavioral pattern for\npredicting nocturnal behavior in a real-world healthcare dataset.",
            "author": [
                "Yu Chen",
                "Nivedita Bijlani",
                "Samaneh Kouchaki",
                "Payam Barnaghi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17560v1",
                "http://arxiv.org/pdf/2311.17560v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17553v1",
            "title": "Deep Learning 21cm Lightcones in 3D",
            "updated": "2023-11-29T11:36:15Z",
            "published": "2023-11-29T11:36:15Z",
            "summary": "Interferometric measurements of the 21cm signal are a prime example of the\ndata-driven era in astrophysics we are entering with current and upcoming\nexperiments. We showcase the use of deep networks that are tailored for the\nstructure of 3D tomographic 21cm light-cones to firstly detect and characterise\nHI sources and to secondly directly infer global astrophysical and cosmological\nmodel parameters. We compare different architectures and highlight how 3D CNN\narchitectures that mirror the data structure are the best-performing model.",
            "author": [
                "Caroline Heneka"
            ],
            "link": [
                "http://dx.doi.org/10.1007/978-3-031-34167-0_34",
                "http://arxiv.org/abs/2311.17553v1",
                "http://arxiv.org/pdf/2311.17553v1"
            ],
            "primary_category": "astro-ph.CO",
            "category": [
                "astro-ph.CO",
                "astro-ph.IM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17552v1",
            "title": "An Efficient Illumination Invariant Tiger Detection Framework for\n  Wildlife Surveillance",
            "updated": "2023-11-29T11:35:54Z",
            "published": "2023-11-29T11:35:54Z",
            "summary": "Tiger conservation necessitates the strategic deployment of multifaceted\ninitiatives encompassing the preservation of ecological habitats, anti-poaching\nmeasures, and community involvement for sustainable growth in the tiger\npopulation. With the advent of artificial intelligence, tiger surveillance can\nbe automated using object detection. In this paper, an accurate illumination\ninvariant framework is proposed based on EnlightenGAN and YOLOv8 for tiger\ndetection. The fine-tuned YOLOv8 model achieves a mAP score of 61% without\nillumination enhancement. The illumination enhancement improves the mAP by\n0.7%. The approaches elevate the state-of-the-art performance on the ATRW\ndataset by approximately 6% to 7%.",
            "author": [
                "Gaurav Pendharkar",
                "A. Ancy Micheal",
                "Jason Misquitta",
                "Ranjeesh Kaippada"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17552v1",
                "http://arxiv.org/pdf/2311.17552v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00258v1",
            "title": "Precipitation Nowcasting With Spatial And Temporal Transfer Learning\n  Using Swin-UNETR",
            "updated": "2023-11-29T11:35:50Z",
            "published": "2023-11-29T11:35:50Z",
            "summary": "Climate change has led to an increase in frequency of extreme weather events.\nEarly warning systems can prevent disasters and loss of life. Managing such\nevents remain a challenge for both public and private institutions.\nPrecipitation nowcasting can help relevant institutions to better prepare for\nsuch events. Numerical weather prediction (NWP) has traditionally been used to\nmake physics based forecasting, and recently deep learning based approaches\nhave been used to reduce turn-around time for nowcasting. In this work,\nrecently proposed Swin-UNETR (Swin UNEt TRansformer) is used for precipitation\nnowcasting for ten different regions of Europe. Swin-UNETR utilizes a U-shaped\nnetwork within which a swin transformer-based encoder extracts multi-scale\nfeatures from multiple input channels of satellite image, while CNN-based\ndecoder makes the prediction. Trained model is capable of nowcasting not only\nfor the regions for which data is available, but can also be used for new\nregions for which data is not available.",
            "author": [
                "Ajitabh Kumar"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00258v1",
                "http://arxiv.org/pdf/2312.00258v1"
            ],
            "primary_category": "physics.ao-ph",
            "category": [
                "physics.ao-ph",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17548v1",
            "title": "Detecting genuine multipartite entanglement via machine learning",
            "updated": "2023-11-29T11:31:22Z",
            "published": "2023-11-29T11:31:22Z",
            "summary": "In recent years, supervised and semi-supervised machine learning methods such\nas neural networks, support vector machines (SVM), and semi-supervised support\nvector machines (S4VM) have been widely used in quantum entanglement and\nquantum steering verification problems. However, few studies have focused on\ndetecting genuine multipartite entanglement based on machine learning. Here, we\ninvestigate supervised and semi-supervised machine learning for detecting\ngenuine multipartite entanglement of three-qubit states. We randomly generate\nthree-qubit density matrices, and train an SVM for the detection of genuine\nmultipartite entangled states. Moreover, we improve the training method of\nS4VM, which optimizes the grouping of prediction samples and then performs\niterative predictions. Through numerical simulation, it is confirmed that this\nmethod can significantly improve the prediction accuracy.",
            "author": [
                "Yi-Jun Luo",
                "Jin-Ming Liu",
                "Chengjie Zhang"
            ],
            "link": [
                "http://dx.doi.org/10.1103/PhysRevA.108.052424",
                "http://arxiv.org/abs/2311.17548v1",
                "http://arxiv.org/pdf/2311.17548v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17546v1",
            "title": "VINNA for Neonates -- Orientation Independence through Latent\n  Augmentations",
            "updated": "2023-11-29T11:28:26Z",
            "published": "2023-11-29T11:28:26Z",
            "summary": "Fast and accurate segmentation of neonatal brain images is highly desired to\nbetter understand and detect changes during development and disease. Yet, the\nlimited availability of ground truth datasets, lack of standardized acquisition\nprotocols, and wide variations of head positioning pose challenges for method\ndevelopment. A few automated image analysis pipelines exist for newborn brain\nMRI segmentation, but they often rely on time-consuming procedures and require\nresampling to a common resolution, subject to loss of information due to\ninterpolation and down-sampling. Without registration and image resampling,\nvariations with respect to head positions and voxel resolutions have to be\naddressed differently. In deep-learning, external augmentations are\ntraditionally used to artificially expand the representation of spatial\nvariability, increasing the training dataset size and robustness. However,\nthese transformations in the image space still require resampling, reducing\naccuracy specifically in the context of label interpolation. We recently\nintroduced the concept of resolution-independence with the Voxel-size\nIndependent Neural Network framework, VINN. Here, we extend this concept by\nadditionally shifting all rigid-transforms into the network architecture with a\nfour degree of freedom (4-DOF) transform module, enabling resolution-aware\ninternal augmentations (VINNA). In this work we show that VINNA (i)\nsignificantly outperforms state-of-the-art external augmentation approaches,\n(ii) effectively addresses the head variations present specifically in newborn\ndatasets, and (iii) retains high segmentation accuracy across a range of\nresolutions (0.5-1.0 mm). The 4-DOF transform module is a powerful, general\napproach to implement spatial augmentation without requiring image or label\ninterpolation. The specific network application to newborns will be made\npublicly available as VINNA4neonates.",
            "author": [
                "Leonie Henschel",
                "David K\u00fcgler",
                "Lilla Z\u00f6llei",
                "Martin Reuter"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17546v1",
                "http://arxiv.org/pdf/2311.17546v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17961v1",
            "title": "Skilful Precipitation Nowcasting Using NowcastNet",
            "updated": "2023-11-29T11:24:52Z",
            "published": "2023-11-29T11:24:52Z",
            "summary": "Designing early warning system for precipitation requires accurate short-term\nforecasting system. Climate change has led to an increase in frequency of\nextreme weather events, and hence such systems can prevent disasters and loss\nof life. Managing such events remain a challenge for both public and private\ninstitutions. Precipitation nowcasting can help relevant institutions to better\nprepare for such events as they impact agriculture, transport, public health\nand safety, etc. Physics-based numerical weather prediction (NWP) is unable to\nperform well for nowcasting because of large computational turn-around time.\nDeep-learning based models on the other hand are able to give predictions\nwithin seconds. We use recently proposed NowcastNet, a physics-conditioned deep\ngenerative network, to forecast precipitation for different regions of Europe\nusing satellite images. Both spatial and temporal transfer learning is done by\nforecasting for the unseen regions and year. Model makes realistic predictions\nand is able to outperform baseline for such a prediction task.",
            "author": [
                "Ajitabh Kumar"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17961v1",
                "http://arxiv.org/pdf/2311.17961v1"
            ],
            "primary_category": "physics.ao-ph",
            "category": [
                "physics.ao-ph",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17539v1",
            "title": "The Effects of Overparameterization on Sharpness-aware Minimization: An\n  Empirical and Theoretical Analysis",
            "updated": "2023-11-29T11:19:50Z",
            "published": "2023-11-29T11:19:50Z",
            "summary": "Training an overparameterized neural network can yield minimizers of the same\nlevel of training loss and yet different generalization capabilities. With\nevidence that indicates a correlation between sharpness of minima and their\ngeneralization errors, increasing efforts have been made to develop an\noptimization method to explicitly find flat minima as more generalizable\nsolutions. This sharpness-aware minimization (SAM) strategy, however, has not\nbeen studied much yet as to how overparameterization can actually affect its\nbehavior. In this work, we analyze SAM under varying degrees of\noverparameterization and present both empirical and theoretical results that\nsuggest a critical influence of overparameterization on SAM. Specifically, we\nfirst use standard techniques in optimization to prove that SAM can achieve a\nlinear convergence rate under overparameterization in a stochastic setting. We\nalso show that the linearly stable minima found by SAM are indeed flatter and\nhave more uniformly distributed Hessian moments compared to those of SGD. These\nresults are corroborated with our experiments that reveal a consistent trend\nthat the generalization improvement made by SAM continues to increase as the\nmodel becomes more overparameterized. We further present that sparsity can open\nup an avenue for effective overparameterization in practice.",
            "author": [
                "Sungbin Shin",
                "Dongyeop Lee",
                "Maksym Andriushchenko",
                "Namhoon Lee"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17539v1",
                "http://arxiv.org/pdf/2311.17539v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "math.OC",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17532v1",
            "title": "Weakly-Supervised Emotion Transition Learning for Diverse 3D Co-speech\n  Gesture Generation",
            "updated": "2023-11-29T11:10:40Z",
            "published": "2023-11-29T11:10:40Z",
            "summary": "Generating vivid and emotional 3D co-speech gestures is crucial for virtual\navatar animation in human-machine interaction applications. While the existing\nmethods enable generating the gestures to follow a single emotion label, they\noverlook that long gesture sequence modeling with emotion transition is more\npractical in real scenes. In addition, the lack of large-scale available\ndatasets with emotional transition speech and corresponding 3D human gestures\nalso limits the addressing of this task. To fulfill this goal, we first\nincorporate the ChatGPT-4 and an audio inpainting approach to construct the\nhigh-fidelity emotion transition human speeches. Considering obtaining the\nrealistic 3D pose annotations corresponding to the dynamically inpainted\nemotion transition audio is extremely difficult, we propose a novel weakly\nsupervised training strategy to encourage authority gesture transitions.\nSpecifically, to enhance the coordination of transition gestures w.r.t\ndifferent emotional ones, we model the temporal association representation\nbetween two different emotional gesture sequences as style guidance and infuse\nit into the transition generation. We further devise an emotion mixture\nmechanism that provides weak supervision based on a learnable mixed emotion\nlabel for transition gestures. Last, we present a keyframe sampler to supply\neffective initial posture cues in long sequences, enabling us to generate\ndiverse gestures. Extensive experiments demonstrate that our method outperforms\nthe state-of-the-art models constructed by adapting single emotion-conditioned\ncounterparts on our newly defined emotion transition task and datasets.",
            "author": [
                "Xingqun Qi",
                "Jiahao Pan",
                "Peng Li",
                "Ruibin Yuan",
                "Xiaowei Chi",
                "Mengfei Li",
                "Wenhan Luo",
                "Wei Xue",
                "Shanghang Zhang",
                "Qifeng Liu",
                "Yike Guo"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17532v1",
                "http://arxiv.org/pdf/2311.17532v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17959v1",
            "title": "Transformer Based Model for Predicting Rapid Impact Compaction Outcomes:\n  A Case Study of Utapao International Airport",
            "updated": "2023-11-29T10:56:02Z",
            "published": "2023-11-29T10:56:02Z",
            "summary": "This paper introduces a novel deep learning approach to predict the\nengineering properties of the ground improved by Rapid Impact Compaction (RIC),\nwhich is a ground improvement technique that uses a drop hammer to compact the\nsoil and fill layers. The proposed approach uses transformer-based neural\nnetworks to capture the complex nonlinear relationships between the input\nfeatures, such as the hammer energy, drop height, and number of blows, and the\noutput variables, such as the cone resistance. The approach is applied to a\nreal-world dataset from a trial test section for the new apron construction of\nthe Utapao International Airport in Thailand. The results show that the\nproposed approach outperforms the existing methods in terms of prediction\naccuracy and efficiency and provides interpretable attention maps that reveal\nthe importance of different features for RIC prediction. The paper also\ndiscusses the limitations and future directions of applying deep learning\nmethods to RIC prediction.",
            "author": [
                "Sompote Youwai",
                "Sirasak Detcheewa"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17959v1",
                "http://arxiv.org/pdf/2311.17959v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17519v1",
            "title": "Reinforcement Learning with thermal fluctuations at the nano-scale",
            "updated": "2023-11-29T10:41:11Z",
            "published": "2023-11-29T10:41:11Z",
            "summary": "Reinforcement Learning offers a framework to learn to choose actions in order\nto achieve some goal. However, at the nano-scale, thermal fluctuations hamper\nthe learning process. We show that in this regime, while optimal actions should\nbring an improvement proportional to the small ratio of the applied force times\na length-scale over the temperature, the learned improvement is smaller and\nproportional to the square of this small ratio. Consequently, the efficiency of\nlearning, which compares the learning improvement to the theoretical optimal\nimprovement, drops to zero. Nevertheless, we show how to circumvent these\nlimitations by using actions learned at a lower temperature. Our results are\nillustrated with simulations of the control of small particle clusters, and\nshould apply to a wide class of other problems that can also be formulated as a\nMarkov Decision Processes such as nano-navigation and nano-machine actuation.",
            "author": [
                "Francesco Boccardo",
                "Olivier Pierre-Louis"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17519v1",
                "http://arxiv.org/pdf/2311.17519v1"
            ],
            "primary_category": "cond-mat.stat-mech",
            "category": [
                "cond-mat.stat-mech",
                "physics.comp-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17518v1",
            "title": "The devil is in the fine-grained details: Evaluating open-vocabulary\n  object detectors for fine-grained understanding",
            "updated": "2023-11-29T10:40:52Z",
            "published": "2023-11-29T10:40:52Z",
            "summary": "Recent advancements in large vision-language models enabled visual object\ndetection in open-vocabulary scenarios, where object classes are defined in\nfree-text formats during inference. In this paper, we aim to probe the\nstate-of-the-art methods for open-vocabulary object detection to determine to\nwhat extent they understand fine-grained properties of objects and their parts.\nTo this end, we introduce an evaluation protocol based on dynamic vocabulary\ngeneration to test whether models detect, discern, and assign the correct\nfine-grained description to objects in the presence of hard-negative classes.\nWe contribute with a benchmark suite of increasing difficulty and probing\ndifferent properties like color, pattern, and material. We further enhance our\ninvestigation by evaluating several state-of-the-art open-vocabulary object\ndetectors using the proposed protocol and find that most existing solutions,\nwhich shine in standard open-vocabulary benchmarks, struggle to accurately\ncapture and distinguish finer object details. We conclude the paper by\nhighlighting the limitations of current methodologies and exploring promising\nresearch directions to overcome the discovered drawbacks. Data and code are\navailable at https://github.com/lorebianchi98/FG-OVD.",
            "author": [
                "Lorenzo Bianchi",
                "Fabio Carrara",
                "Nicola Messina",
                "Claudio Gennaro",
                "Fabrizio Falchi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17518v1",
                "http://arxiv.org/pdf/2311.17518v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17515v1",
            "title": "Fusion of Single and Integral Multispectral Aerial Images",
            "updated": "2023-11-29T10:38:42Z",
            "published": "2023-11-29T10:38:42Z",
            "summary": "We present a novel hybrid (model- and learning-based) architecture for fusing\nthe most significant features from conventional aerial images and integral\naerial images that result from synthetic aperture sensing for removing\nocclusion caused by dense vegetation. It combines the environment's spatial\nreferences with features of unoccluded targets. Our method out-beats the\nstate-of-the-art, does not require manually tuned parameters, can be extended\nto an arbitrary number and combinations of spectral channels, and is\nreconfigurable to address different use-cases.",
            "author": [
                "Mohamed Youssef",
                "Oliver Bimber"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17515v1",
                "http://arxiv.org/pdf/2311.17515v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17514v1",
            "title": "Reinforcement Replaces Supervision: Query focused Summarization using\n  Deep Reinforcement Learning",
            "updated": "2023-11-29T10:38:16Z",
            "published": "2023-11-29T10:38:16Z",
            "summary": "Query-focused Summarization (QfS) deals with systems that generate summaries\nfrom document(s) based on a query. Motivated by the insight that Reinforcement\nLearning (RL) provides a generalization to Supervised Learning (SL) for Natural\nLanguage Generation, and thereby performs better (empirically) than SL, we use\nan RL-based approach for this task of QfS. Additionally, we also resolve the\nconflict of employing RL in Transformers with Teacher Forcing. We develop\nmultiple Policy Gradient networks, trained on various reward signals: ROUGE,\nBLEU, and Semantic Similarity, which lead to a 10-point improvement over the\nState-of-the-Art approach on the ROUGE-L metric for a benchmark dataset (ELI5).\nWe also show performance of our approach in zero-shot setting for another\nbenchmark dataset (DebatePedia) -- our approach leads to results comparable to\nbaselines, which were specifically trained on DebatePedia. To aid the RL\ntraining, we propose a better semantic similarity reward, enabled by a novel\nPassage Embedding scheme developed using Cluster Hypothesis. Lastly, we\ncontribute a gold-standard test dataset to further research in QfS and\nLong-form Question Answering (LfQA).",
            "author": [
                "Swaroop Nath",
                "Harshad Khadilkar",
                "Pushpak Bhattacharyya"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17514v1",
                "http://arxiv.org/pdf/2311.17514v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17508v1",
            "title": "Model Performance Prediction for Hyperparameter Optimization of Deep\n  Learning Models Using High Performance Computing and Quantum Annealing",
            "updated": "2023-11-29T10:32:40Z",
            "published": "2023-11-29T10:32:40Z",
            "summary": "Hyperparameter Optimization (HPO) of Deep Learning-based models tends to be a\ncompute resource intensive process as it usually requires to train the target\nmodel with many different hyperparameter configurations. We show that\nintegrating model performance prediction with early stopping methods holds\ngreat potential to speed up the HPO process of deep learning models. Moreover,\nwe propose a novel algorithm called Swift-Hyperband that can use either\nclassical or quantum support vector regression for performance prediction and\nbenefit from distributed High Performance Computing environments. This\nalgorithm is tested not only for the Machine-Learned Particle Flow model used\nin High Energy Physics, but also for a wider range of target models from\ndomains such as computer vision and natural language processing.\nSwift-Hyperband is shown to find comparable (or better) hyperparameters as well\nas using less computational resources in all test cases.",
            "author": [
                "Juan Pablo Garc\u00eda Amboage",
                "Eric Wulff",
                "Maria Girone",
                "Tom\u00e1s F. Pena"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17508v1",
                "http://arxiv.org/pdf/2311.17508v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "physics.data-an"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03743v1",
            "title": "Easy Data Augmentation in Sentiment Analysis of Cyberbullying",
            "updated": "2023-11-29T10:05:58Z",
            "published": "2023-11-29T10:05:58Z",
            "summary": "Instagram, a social media platform, has in the vicinity of 2 billion active\nusers in 2023. The platform allows users to post photos and videos with one\nanother. However, cyberbullying remains a significant problem for about 50% of\nyoung Indonesians. To address this issue, sentiment analysis for comment\nfiltering uses a Support Vector Machine (SVM) and Easy Data Augmentation (EDA).\nEDA will augment the dataset, enabling robust prediction and analysis of\ncyberbullying by introducing more variation. Based on the tests, SVM\ncombination with EDA results in a 2.52% increase in the k-Fold Cross Validation\nscore. Our proposed approach shows an improved accuracy of 92.5%, 2.5% higher\nthan that of the existing state-of-the-art method. To maintain the\nreproducibility and replicability of this research, the source code can be\naccessed at uns.id/eda_svm.",
            "author": [
                "Alwan Wirawan",
                "Hasan Dwi Cahyono",
                "Winarno"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03743v1",
                "http://arxiv.org/pdf/2312.03743v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17483v2",
            "title": "Classification, Challenges, and Automated Approaches to Handle\n  Non-Functional Requirements in ML-Enabled Systems: A Systematic Literature\n  Review",
            "updated": "2023-11-30T14:34:05Z",
            "published": "2023-11-29T09:45:41Z",
            "summary": "Machine learning (ML) is nowadays so pervasive and diffused that virtually no\napplication can avoid its use. Nonetheless, its enormous potential is\nconstantly threatened by non-functional requirements, such as sustainability.\nIn particular, we noticed the lack of a comprehensive synthesis of the research\nefforts done so far and how these may drive further research. In this paper, we\npropose a systematic literature review targeting three key aspects such as (1)\nthe classification of the non-functional requirements investigated so far, (2)\nthe challenges to face when dealing with them, and (3) the automated approaches\nproposed in literature to support practitioners when optimizing them in\npractice. Through the combination of well-established guidelines for conducting\nsystematic literature reviews and additional search criteria, we survey a total\namount of 69 research articles. Our findings report that current research\nidentified 30 different non-functional requirements, which can be grouped into\nsix main classes. We also deliver a catalog of over 23 software engineering\nchallenges that further research should consider, besides an overview of the\nautomated approaches researchers proposed to support practitioners when\noptimizing non-functional requirements of machine learning-enabled systems. We\nconclude our work by distilling implications and a future outlook on the topic.",
            "author": [
                "Vincenzo De Martino",
                "Fabio Palomba"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17483v2",
                "http://arxiv.org/pdf/2311.17483v2"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17958v1",
            "title": "CommunityAI: Towards Community-based Federated Learning",
            "updated": "2023-11-29T09:31:52Z",
            "published": "2023-11-29T09:31:52Z",
            "summary": "Federated Learning (FL) has emerged as a promising paradigm to train machine\nlearning models collaboratively while preserving data privacy. However, its\nwidespread adoption faces several challenges, including scalability,\nheterogeneous data and devices, resource constraints, and security concerns.\nDespite its promise, FL has not been specifically adapted for community\ndomains, primarily due to the wide-ranging differences in data types and\ncontext, devices and operational conditions, environmental factors, and\nstakeholders. In response to these challenges, we present a novel framework for\nCommunity-based Federated Learning called CommunityAI. CommunityAI enables\nparticipants to be organized into communities based on their shared interests,\nexpertise, or data characteristics. Community participants collectively\ncontribute to training and refining learning models while maintaining data and\nparticipant privacy within their respective groups. Within this paper, we\ndiscuss the conceptual architecture, system requirements, processes, and future\nchallenges that must be solved. Finally, our goal within this paper is to\npresent our vision regarding enabling a collaborative learning process within\nvarious communities.",
            "author": [
                "Ilir Murturi",
                "Praveen Kumar Donta",
                "Schahram Dustdar"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17958v1",
                "http://arxiv.org/pdf/2311.17958v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17475v2",
            "title": "CLiSA: A Hierarchical Hybrid Transformer Model using Orthogonal Cross\n  Attention for Satellite Image Cloud Segmentation",
            "updated": "2023-12-01T10:43:18Z",
            "published": "2023-11-29T09:31:31Z",
            "summary": "Clouds in optical satellite images are a major concern since their presence\nhinders the ability to carry accurate analysis as well as processing. Presence\nof clouds also affects the image tasking schedule and results in wastage of\nvaluable storage space on ground as well as space-based systems. Due to these\nreasons, deriving accurate cloud masks from optical remote-sensing images is an\nimportant task. Traditional methods such as threshold-based, spatial filtering\nfor cloud detection in satellite images suffer from lack of accuracy. In recent\nyears, deep learning algorithms have emerged as a promising approach to solve\nimage segmentation problems as it allows pixel-level classification and\nsemantic-level segmentation. In this paper, we introduce a deep-learning model\nbased on hybrid transformer architecture for effective cloud mask generation\nnamed CLiSA - Cloud segmentation via Lipschitz Stable Attention network. In\nthis context, we propose an concept of orthogonal self-attention combined with\nhierarchical cross attention model, and we validate its Lipschitz stability\ntheoretically and empirically. We design the whole setup under adversarial\nsetting in presence of Lov\\'asz-Softmax loss. We demonstrate both qualitative\nand quantitative outcomes for multiple satellite image datasets including\nLandsat-8, Sentinel-2, and Cartosat-2s. Performing comparative study we show\nthat our model performs preferably against other state-of-the-art methods and\nalso provides better generalization in precise cloud extraction from satellite\nmulti-spectral (MX) images. We also showcase different ablation studies to\nendorse our choices corresponding to different architectural elements and\nobjective functions.",
            "author": [
                "Subhajit Paul",
                "Ashutosh Gupta"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17475v2",
                "http://arxiv.org/pdf/2311.17475v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17466v1",
            "title": "Slot-Mixup with Subsampling: A Simple Regularization for WSI\n  Classification",
            "updated": "2023-11-29T09:18:39Z",
            "published": "2023-11-29T09:18:39Z",
            "summary": "Whole slide image (WSI) classification requires repetitive zoom-in and out\nfor pathologists, as only small portions of the slide may be relevant to\ndetecting cancer. Due to the lack of patch-level labels, multiple instance\nlearning (MIL) is a common practice for training a WSI classifier. One of the\nchallenges in MIL for WSIs is the weak supervision coming only from the\nslide-level labels, often resulting in severe overfitting. In response,\nresearchers have considered adopting patch-level augmentation or applying mixup\naugmentation, but their applicability remains unverified. Our approach augments\nthe training dataset by sampling a subset of patches in the WSI without\nsignificantly altering the underlying semantics of the original slides.\nAdditionally, we introduce an efficient model (Slot-MIL) that organizes patches\ninto a fixed number of slots, the abstract representation of patches, using an\nattention mechanism. We empirically demonstrate that the subsampling\naugmentation helps to make more informative slots by restricting the\nover-concentration of attention and to improve interpretability. Finally, we\nillustrate that combining our attention-based aggregation model with\nsubsampling and mixup, which has shown limited compatibility in existing MIL\nmethods, can enhance both generalization and calibration. Our proposed methods\nachieve the state-of-the-art performance across various benchmark datasets\nincluding class imbalance and distribution shifts.",
            "author": [
                "Seongho Keum",
                "Sanghyun Kim",
                "Soojeong Lee",
                "Juho Lee"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17466v1",
                "http://arxiv.org/pdf/2311.17466v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17458v1",
            "title": "Quantum Neural Networks under Depolarization Noise: Exploring White-Box\n  Attacks and Defenses",
            "updated": "2023-11-29T09:00:19Z",
            "published": "2023-11-29T09:00:19Z",
            "summary": "Leveraging the unique properties of quantum mechanics, Quantum Machine\nLearning (QML) promises computational breakthroughs and enriched perspectives\nwhere traditional systems reach their boundaries. However, similarly to\nclassical machine learning, QML is not immune to adversarial attacks. Quantum\nadversarial machine learning has become instrumental in highlighting the weak\npoints of QML models when faced with adversarial crafted feature vectors.\nDiving deep into this domain, our exploration shines light on the interplay\nbetween depolarization noise and adversarial robustness. While previous results\nenhanced robustness from adversarial threats through depolarization noise, our\nfindings paint a different picture. Interestingly, adding depolarization noise\ndiscontinued the effect of providing further robustness for a multi-class\nclassification scenario. Consolidating our findings, we conducted experiments\nwith a multi-class classifier adversarially trained on gate-based quantum\nsimulators, further elucidating this unexpected behavior.",
            "author": [
                "David Winderl",
                "Nicola Franco",
                "Jeanette Miriam Lorenz"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17458v1",
                "http://arxiv.org/pdf/2311.17458v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17957v1",
            "title": "HandRefiner: Refining Malformed Hands in Generated Images by\n  Diffusion-based Conditional Inpainting",
            "updated": "2023-11-29T08:52:08Z",
            "published": "2023-11-29T08:52:08Z",
            "summary": "Diffusion models have achieved remarkable success in generating realistic\nimages but suffer from generating accurate human hands, such as incorrect\nfinger counts or irregular shapes. This difficulty arises from the complex task\nof learning the physical structure and pose of hands from training images,\nwhich involves extensive deformations and occlusions. For correct hand\ngeneration, our paper introduces a lightweight post-processing solution called\n$\\textbf{HandRefiner}$. HandRefiner employs a conditional inpainting approach\nto rectify malformed hands while leaving other parts of the image untouched. We\nleverage the hand mesh reconstruction model that consistently adheres to the\ncorrect number of fingers and hand shape, while also being capable of fitting\nthe desired hand pose in the generated image. Given a generated failed image\ndue to malformed hands, we utilize ControlNet modules to re-inject such correct\nhand information. Additionally, we uncover a phase transition phenomenon within\nControlNet as we vary the control strength. It enables us to take advantage of\nmore readily available synthetic data without suffering from the domain gap\nbetween realistic and synthetic hands. Experiments demonstrate that HandRefiner\ncan significantly improve the generation quality quantitatively and\nqualitatively. The code is available at\nhttps://github.com/wenquanlu/HandRefiner .",
            "author": [
                "Wenquan Lu",
                "Yufei Xu",
                "Jing Zhang",
                "Chaoyue Wang",
                "Dacheng Tao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17957v1",
                "http://arxiv.org/pdf/2311.17957v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17453v1",
            "title": "Privacy Measurement in Tabular Synthetic Data: State of the Art and\n  Future Research Directions",
            "updated": "2023-11-29T08:51:40Z",
            "published": "2023-11-29T08:51:40Z",
            "summary": "Synthetic data (SD) have garnered attention as a privacy enhancing\ntechnology. Unfortunately, there is no standard for quantifying their degree of\nprivacy protection. In this paper, we discuss proposed quantification\napproaches. This contributes to the development of SD privacy standards;\nstimulates multi-disciplinary discussion; and helps SD researchers make\ninformed modeling and evaluation decisions.",
            "author": [
                "Alexander Boudewijn",
                "Andrea Filippo Ferraris",
                "Daniele Panfilo",
                "Vanessa Cocca",
                "Sabrina Zinutti",
                "Karel De Schepper",
                "Carlo Rossi Chauvenet"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17453v1",
                "http://arxiv.org/pdf/2311.17453v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CR",
                "cs.DB",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17451v1",
            "title": "Wireless Network Digital Twin for 6G: Generative AI as A Key Enabler",
            "updated": "2023-11-29T08:48:26Z",
            "published": "2023-11-29T08:48:26Z",
            "summary": "Digital twin, which enables emulation, evaluation, and optimization of\nphysical entities through synchronized digital replicas, has gained\nincreasingly attention as a promising technology for intricate wireless\nnetworks. For 6G, numerous innovative wireless technologies and network\narchitectures have posed new challenges in establishing wireless network\ndigital twins. To tackle these challenges, artificial intelligence (AI),\nparticularly the flourishing generative AI, emerges as a potential solution. In\nthis article, we discuss emerging prerequisites for wireless network digital\ntwins considering the complicated network architecture, tremendous network\nscale, extensive coverage, and diversified application scenarios in the 6G era.\nWe further explore the applications of generative AI, such as transformer and\ndiffusion model, to empower the 6G digital twin from multiple perspectives\nincluding implementation, physical-digital synchronization, and slicing\ncapability. Subsequently, we propose a hierarchical generative AI-enabled\nwireless network digital twin at both the message-level and policy-level, and\nprovide a typical use case with numerical results to validate the effectiveness\nand efficiency. Finally, open research issues for wireless network digital\ntwins in the 6G era are discussed.",
            "author": [
                "Zhenyu Tao",
                "Wei Xu",
                "Yongming Huang",
                "Xiaoyun Wang",
                "Xiaohu You"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17451v1",
                "http://arxiv.org/pdf/2311.17451v1"
            ],
            "primary_category": "cs.NI",
            "category": [
                "cs.NI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17450v1",
            "title": "Continual Learning for Image Segmentation with Dynamic Query",
            "updated": "2023-11-29T08:46:46Z",
            "published": "2023-11-29T08:46:46Z",
            "summary": "Image segmentation based on continual learning exhibits a critical drop of\nperformance, mainly due to catastrophic forgetting and background shift, as\nthey are required to incorporate new classes continually. In this paper, we\npropose a simple, yet effective Continual Image Segmentation method with\nincremental Dynamic Query (CISDQ), which decouples the representation learning\nof both old and new knowledge with lightweight query embedding. CISDQ mainly\nincludes three contributions: 1) We define dynamic queries with adaptive\nbackground class to exploit past knowledge and learn future classes naturally.\n2) CISDQ proposes a class/instance-aware Query Guided Knowledge Distillation\nstrategy to overcome catastrophic forgetting by capturing the inter-class\ndiversity and intra-class identity. 3) Apart from semantic segmentation, CISDQ\nintroduce the continual learning for instance segmentation in which\ninstance-wise labeling and supervision are considered. Extensive experiments on\nthree datasets for two tasks (i.e., continual semantic and instance\nsegmentation are conducted to demonstrate that CISDQ achieves the\nstate-of-the-art performance, specifically, obtaining 4.4% and 2.9% mIoU\nimprovements for the ADE 100-10 (6 steps) setting and ADE 100-5 (11 steps)\nsetting.",
            "author": [
                "Weijia Wu",
                "Yuzhong Zhao",
                "Zhuang Li",
                "Lianlei Shan",
                "Hong Zhou",
                "Mike Zheng Shou"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17450v1",
                "http://arxiv.org/pdf/2311.17450v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17956v1",
            "title": "QuadraNet: Improving High-Order Neural Interaction Efficiency with\n  Hardware-Aware Quadratic Neural Networks",
            "updated": "2023-11-29T08:45:27Z",
            "published": "2023-11-29T08:45:27Z",
            "summary": "Recent progress in computer vision-oriented neural network designs is mostly\ndriven by capturing high-order neural interactions among inputs and features.\nAnd there emerged a variety of approaches to accomplish this, such as\nTransformers and its variants. However, these interactions generate a large\namount of intermediate state and/or strong data dependency, leading to\nconsiderable memory consumption and computing cost, and therefore compromising\nthe overall runtime performance. To address this challenge, we rethink the\nhigh-order interactive neural network design with a quadratic computing\napproach. Specifically, we propose QuadraNet -- a comprehensive model design\nmethodology from neuron reconstruction to structural block and eventually to\nthe overall neural network implementation. Leveraging quadratic neurons'\nintrinsic high-order advantages and dedicated computation optimization schemes,\nQuadraNet could effectively achieve optimal cognition and computation\nperformance. Incorporating state-of-the-art hardware-aware neural architecture\nsearch and system integration techniques, QuadraNet could also be well\ngeneralized in different hardware constraint settings and deployment scenarios.\nThe experiment shows thatQuadraNet achieves up to 1.5$\\times$ throughput, 30%\nless memory footprint, and similar cognition performance, compared with the\nstate-of-the-art high-order approaches.",
            "author": [
                "Chenhui Xu",
                "Fuxun Yu",
                "Zirui Xu",
                "Chenchen Liu",
                "Jinjun Xiong",
                "Xiang Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17956v1",
                "http://arxiv.org/pdf/2311.17956v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV",
                "cs.NE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17449v1",
            "title": "Weakly-semi-supervised object detection in remotely sensed imagery",
            "updated": "2023-11-29T08:43:04Z",
            "published": "2023-11-29T08:43:04Z",
            "summary": "Deep learning for detecting objects in remotely sensed imagery can enable new\ntechnologies for important applications including mitigating climate change.\nHowever, these models often require large datasets labeled with bounding box\nannotations which are expensive to curate, prohibiting the development of\nmodels for new tasks and geographies. To address this challenge, we develop\nweakly-semi-supervised object detection (WSSOD) models on remotely sensed\nimagery which can leverage a small amount of bounding boxes together with a\nlarge amount of point labels that are easy to acquire at scale in geospatial\ndata. We train WSSOD models which use large amounts of point-labeled images\nwith varying fractions of bounding box labeled images in FAIR1M and a wind\nturbine detection dataset, and demonstrate that they substantially outperform\nfully supervised models trained with the same amount of bounding box labeled\nimages on both datasets. Furthermore, we find that the WSSOD models trained\nwith 2-10x fewer bounding box labeled images can perform similarly to or\noutperform fully supervised models trained on the full set of bounding-box\nlabeled images. We believe that the approach can be extended to other remote\nsensing tasks to reduce reliance on bounding box labels and increase\ndevelopment of models for impactful applications.",
            "author": [
                "Ji Hun Wang",
                "Jeremy Irvin",
                "Beri Kohen Behar",
                "Ha Tran",
                "Raghav Samavedam",
                "Quentin Hsu",
                "Andrew Y. Ng"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17449v1",
                "http://arxiv.org/pdf/2311.17449v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17447v1",
            "title": "Learning-driven Zero Trust in Distributed Computing Continuum Systems",
            "updated": "2023-11-29T08:41:06Z",
            "published": "2023-11-29T08:41:06Z",
            "summary": "Converging Zero Trust (ZT) with learning techniques can solve various\noperational and security challenges in Distributed Computing Continuum Systems\n(DCCS). Implementing centralized ZT architecture is seen as unsuitable for the\ncomputing continuum (e.g., computing entities with limited connectivity and\nvisibility, etc.). At the same time, implementing decentralized ZT in the\ncomputing continuum requires understanding infrastructure limitations and novel\napproaches to enhance resource access management decisions. To overcome such\nchallenges, we present a novel learning-driven ZT conceptual architecture\ndesigned for DCCS. We aim to enhance ZT architecture service quality by\nincorporating lightweight learning strategies such as Representation Learning\n(ReL) and distributing ZT components across the computing continuum. The ReL\nhelps to improve the decision-making process by predicting threats or untrusted\nrequests. Through an illustrative example, we show how the learning process\ndetects and blocks the requests, enhances resource access control, and reduces\nnetwork and computation overheads. Lastly, we discuss the conceptual\narchitecture, processes, and provide a research agenda.",
            "author": [
                "Ilir Murturi",
                "Praveen Kumar Donta",
                "Victor Casamayor Pujol",
                "Andrea Morichetta",
                "Schahram Dustdar"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17447v1",
                "http://arxiv.org/pdf/2311.17447v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.AI",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17446v1",
            "title": "Uncertainty in Additive Feature Attribution methods",
            "updated": "2023-11-29T08:40:46Z",
            "published": "2023-11-29T08:40:46Z",
            "summary": "In this work, we explore various topics that fall under the umbrella of\nUncertainty in post-hoc Explainable AI (XAI) methods. We in particular focus on\nthe class of additive feature attribution explanation methods. We first\ndescribe our specifications of uncertainty and compare various statistical and\nrecent methods to quantify the same. Next, for a particular instance, we study\nthe relationship between a feature's attribution and its uncertainty and\nobserve little correlation. As a result, we propose a modification in the\ndistribution from which perturbations are sampled in LIME-based algorithms such\nthat the important features have minimal uncertainty without an increase in\ncomputational cost. Next, while studying how the uncertainty in explanations\nvaries across the feature space of a classifier, we observe that a fraction of\ninstances show near-zero uncertainty. We coin the term \"stable instances\" for\nsuch instances and diagnose factors that make an instance stable. Next, we\nstudy how an XAI algorithm's uncertainty varies with the size and complexity of\nthe underlying model. We observe that the more complex the model, the more\ninherent uncertainty is exhibited by it. As a result, we propose a measure to\nquantify the relative complexity of a blackbox classifier. This could be\nincorporated, for example, in LIME-based algorithms' sampling densities, to\nhelp different explanation algorithms achieve tighter confidence levels.\nTogether, the above measures would have a strong impact on making XAI models\nrelatively trustworthy for the end-user as well as aiding scientific discovery.",
            "author": [
                "Abhishek Madaan",
                "Tanya Chowdhury",
                "Neha Rana",
                "James Allan",
                "Tanmoy Chakraborty"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17446v1",
                "http://arxiv.org/pdf/2311.17446v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "I.2.6"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17443v1",
            "title": "Power system investment optimization to identify carbon neutrality\n  scenarios for Italy",
            "updated": "2023-11-29T08:35:48Z",
            "published": "2023-11-29T08:35:48Z",
            "summary": "In 2021, the European Commission has adopted the Fit-for-55 policy package,\nlegally binding European countries to reduce their CO2 emissions by 55% with\nrespect to 1990, a first step to achieve carbon neutrality in 2050. In this\ncontext, it is crucial to help national policymakers to choose the most\nappropriate technologies to achieve these goals and energy system modelling can\nbe a valuable tool. This article presents a model of the Italian power system\nrealized employing the open energy modelling framework Oemof. A Linear\nProgramming Optimization is implemented to evaluate how to minimise system\ncosts at decreasing CO2 emissions in 2030. The developed tool is applied to\nevaluate different research questions: i) pathway towards full decarbonization\nand power self-sufficiency of the electricity sector in Italy, ii) relevance of\nflexibility assets in power grids: li-ion batteries, hydrogen storage and\ntransmission lines reinforcement. A 55% CO2 emissions reduction for the actual\nItalian power sector can be achieved through an increase of 30% of the total\nannual system cost. Full decarbonization can be reached with four times today's\nannual costs, which could be lowered with sector coupling and considering more\ntechnologies.",
            "author": [
                "Alice Di Bella",
                "Federico Canti",
                "Matteo Giacomo Prina",
                "Valeria Casalicchio",
                "Giampaolo Manzolini",
                "Wolfram Sparber"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17443v1",
                "http://arxiv.org/pdf/2311.17443v1"
            ],
            "primary_category": "physics.soc-ph",
            "category": [
                "physics.soc-ph",
                "econ.GN",
                "q-fin.EC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17435v1",
            "title": "MM-Narrator: Narrating Long-form Videos with Multimodal In-Context\n  Learning",
            "updated": "2023-11-29T08:27:00Z",
            "published": "2023-11-29T08:27:00Z",
            "summary": "We present MM-Narrator, a novel system leveraging GPT-4 with multimodal\nin-context learning for the generation of audio descriptions (AD). Unlike\nprevious methods that primarily focused on downstream fine-tuning with short\nvideo clips, MM-Narrator excels in generating precise audio descriptions for\nvideos of extensive lengths, even beyond hours, in an autoregressive manner.\nThis capability is made possible by the proposed memory-augmented generation\nprocess, which effectively utilizes both the short-term textual context and\nlong-term visual memory through an efficient register-and-recall mechanism.\nThese contextual memories compile pertinent past information, including\nstorylines and character identities, ensuring an accurate tracking and\ndepicting of story-coherent and character-centric audio descriptions.\nMaintaining the training-free design of MM-Narrator, we further propose a\ncomplexity-based demonstration selection strategy to largely enhance its\nmulti-step reasoning capability via few-shot multimodal in-context learning\n(MM-ICL). Experimental results on MAD-eval dataset demonstrate that MM-Narrator\nconsistently outperforms both the existing fine-tuning-based approaches and\nLLM-based approaches in most scenarios, as measured by standard evaluation\nmetrics. Additionally, we introduce the first segment-based evaluator for\nrecurrent text generation. Empowered by GPT-4, this evaluator comprehensively\nreasons and marks AD generation performance in various extendable dimensions.",
            "author": [
                "Chaoyi Zhang",
                "Kevin Lin",
                "Zhengyuan Yang",
                "Jianfeng Wang",
                "Linjie Li",
                "Chung-Ching Lin",
                "Zicheng Liu",
                "Lijuan Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17435v1",
                "http://arxiv.org/pdf/2311.17435v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17434v1",
            "title": "Group-wise Sparse and Explainable Adversarial Attacks",
            "updated": "2023-11-29T08:26:18Z",
            "published": "2023-11-29T08:26:18Z",
            "summary": "Sparse adversarial attacks fool deep neural networks (DNNs) through minimal\npixel perturbations, typically regularized by the $\\ell_0$ norm. Recent efforts\nhave replaced this norm with a structural sparsity regularizer, such as the\nnuclear group norm, to craft group-wise sparse adversarial attacks. The\nresulting perturbations are thus explainable and hold significant practical\nrelevance, shedding light on an even greater vulnerability of DNNs than\npreviously anticipated. However, crafting such attacks poses an optimization\nchallenge, as it involves computing norms for groups of pixels within a\nnon-convex objective. In this paper, we tackle this challenge by presenting an\nalgorithm that simultaneously generates group-wise sparse attacks within\nsemantically meaningful areas of an image. In each iteration, the core\noperation of our algorithm involves the optimization of a quasinorm adversarial\nloss. This optimization is achieved by employing the $1/2$-quasinorm proximal\noperator for some iterations, a method tailored for nonconvex programming.\nSubsequently, the algorithm transitions to a projected Nesterov's accelerated\ngradient descent with $2$-norm regularization applied to perturbation\nmagnitudes. We rigorously evaluate the efficacy of our novel attack in both\ntargeted and non-targeted attack scenarios, on CIFAR-10 and ImageNet datasets.\nWhen compared to state-of-the-art methods, our attack consistently results in a\nremarkable increase in group-wise sparsity, e.g., an increase of $48.12\\%$ on\nCIFAR-10 and $40.78\\%$ on ImageNet (average case, targeted attack), all while\nmaintaining lower perturbation magnitudes. Notably, this performance is\ncomplemented by a significantly faster computation time and a $100\\%$ attack\nsuccess rate.",
            "author": [
                "Shpresim Sadiku",
                "Moritz Wagner",
                "Sebastian Pokutta"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17434v1",
                "http://arxiv.org/pdf/2311.17434v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.CR",
                "cs.LG",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17431v4",
            "title": "Grounding Foundation Models through Federated Transfer Learning: A\n  General Framework",
            "updated": "2023-12-05T09:35:03Z",
            "published": "2023-11-29T08:21:42Z",
            "summary": "Foundation Models (FMs) such as GPT-4 encoded with vast knowledge and\npowerful emergent abilities have achieved remarkable success in various natural\nlanguage processing and computer vision tasks. Grounding FMs by adapting them\nto domain-specific tasks or augmenting them with domain-specific knowledge\nenables us to exploit the full potential of FMs. However, grounding FMs faces\nseveral challenges, stemming primarily from constrained computing resources,\ndata privacy, model heterogeneity, and model ownership. Federated Transfer\nLearning (FTL), the combination of federated learning and transfer learning,\nprovides promising solutions to address these challenges. In recent years, the\nneed for grounding FMs leveraging FTL, coined FTL-FM, has arisen strongly in\nboth academia and industry. Motivated by the strong growth in FTL-FM research\nand the potential impact of FTL-FM on industrial applications, we propose an\nFTL-FM framework that formulates problems of grounding FMs in the federated\nlearning setting, construct a detailed taxonomy based on the FTL-FM framework\nto categorize state-of-the-art FTL-FM works, and comprehensively overview\nFTL-FM works based on the proposed taxonomy. We also establish correspondences\nbetween FTL-FM and conventional phases of adapting FM so that FM practitioners\ncan align their research works with FTL-FM. In addition, we overview advanced\nefficiency-improving and privacy-preserving techniques because efficiency and\nprivacy are critical concerns in FTL-FM. Last, we discuss opportunities and\nfuture research directions of FTL-FM.",
            "author": [
                "Yan Kang",
                "Tao Fan",
                "Hanlin Gu",
                "Lixin Fan",
                "Qiang Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17431v4",
                "http://arxiv.org/pdf/2311.17431v4"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17429v1",
            "title": "TARGET: Template-Transferable Backdoor Attack Against Prompt-based NLP\n  Models via GPT4",
            "updated": "2023-11-29T08:12:09Z",
            "published": "2023-11-29T08:12:09Z",
            "summary": "Prompt-based learning has been widely applied in many low-resource NLP tasks\nsuch as few-shot scenarios. However, this paradigm has been shown to be\nvulnerable to backdoor attacks. Most of the existing attack methods focus on\ninserting manually predefined templates as triggers in the pre-training phase\nto train the victim model and utilize the same triggers in the downstream task\nto perform inference, which tends to ignore the transferability and\nstealthiness of the templates. In this work, we propose a novel approach of\nTARGET (Template-trAnsfeRable backdoor attack aGainst prompt-basEd NLP models\nvia GPT4), which is a data-independent attack method. Specifically, we first\nutilize GPT4 to reformulate manual templates to generate tone-strong and normal\ntemplates, and the former are injected into the model as a backdoor trigger in\nthe pre-training phase. Then, we not only directly employ the above templates\nin the downstream task, but also use GPT4 to generate templates with similar\ntone to the above templates to carry out transferable attacks. Finally we have\nconducted extensive experiments on five NLP datasets and three BERT series\nmodels, with experimental results justifying that our TARGET method has better\nattack performance and stealthiness compared to the two-external baseline\nmethods on direct attacks, and in addition achieves satisfactory attack\ncapability in the unseen tone-similar templates.",
            "author": [
                "Zihao Tan",
                "Qingliang Chen",
                "Yongjian Huang",
                "Chen Liang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17429v1",
                "http://arxiv.org/pdf/2311.17429v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17955v1",
            "title": "PEAN: A Diffusion-based Prior-Enhanced Attention Network for Scene Text\n  Image Super-Resolution",
            "updated": "2023-11-29T08:11:20Z",
            "published": "2023-11-29T08:11:20Z",
            "summary": "Scene text image super-resolution (STISR) aims at simultaneously increasing\nthe resolution and readability of low-resolution scene text images, thus\nboosting the performance of the downstream recognition task. Two factors in\nscene text images, semantic information and visual structure, affect the\nrecognition performance significantly. To mitigate the effects from these\nfactors, this paper proposes a Prior-Enhanced Attention Network (PEAN).\nSpecifically, a diffusion-based module is developed to enhance the text prior,\nhence offering better guidance for the SR network to generate SR images with\nhigher semantic accuracy. Meanwhile, the proposed PEAN leverages an\nattention-based modulation module to understand scene text images by neatly\nperceiving the local and global dependence of images, despite the shape of the\ntext. A multi-task learning paradigm is employed to optimize the network,\nenabling the model to generate legible SR images. As a result, PEAN establishes\nnew SOTA results on the TextZoom benchmark. Experiments are also conducted to\nanalyze the importance of the enhanced text prior as a means of improving the\nperformance of the SR network. Code will be made available at\nhttps://github.com/jdfxzzy/PEAN.",
            "author": [
                "Zuoyan Zhao",
                "Shipeng Zhu",
                "Pengfei Fang",
                "Hui Xue"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17955v1",
                "http://arxiv.org/pdf/2311.17955v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17428v1",
            "title": "SigFormer: Sparse Signal-Guided Transformer for Multi-Modal Human Action\n  Segmentation",
            "updated": "2023-11-29T08:09:01Z",
            "published": "2023-11-29T08:09:01Z",
            "summary": "Multi-modal human action segmentation is a critical and challenging task with\na wide range of applications. Nowadays, the majority of approaches concentrate\non the fusion of dense signals (i.e., RGB, optical flow, and depth maps).\nHowever, the potential contributions of sparse IoT sensor signals, which can be\ncrucial for achieving accurate recognition, have not been fully explored. To\nmake up for this, we introduce a Sparse signalguided Transformer (SigFormer) to\ncombine both dense and sparse signals. We employ mask attention to fuse\nlocalized features by constraining cross-attention within the regions where\nsparse signals are valid. However, since sparse signals are discrete, they lack\nsufficient information about the temporal action boundaries. Therefore, in\nSigFormer, we propose to emphasize the boundary information at two stages to\nalleviate this problem. In the first feature extraction stage, we introduce an\nintermediate bottleneck module to jointly learn both category and boundary\nfeatures of each dense modality through the inner loss functions. After the\nfusion of dense modalities and sparse signals, we then devise a two-branch\narchitecture that explicitly models the interrelationship between action\ncategory and temporal boundary. Experimental results demonstrate that SigFormer\noutperforms the state-of-the-art approaches on a multi-modal action\nsegmentation dataset from real industrial environments, reaching an outstanding\nF1 score of 0.958. The codes and pre-trained models have been available at\nhttps://github.com/LIUQI-creat/SigFormer.",
            "author": [
                "Qi Liu",
                "Xinchen Liu",
                "Kun Liu",
                "Xiaoyan Gu",
                "Wu Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17428v1",
                "http://arxiv.org/pdf/2311.17428v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17425v1",
            "title": "SpeechAct: Towards Generating Whole-body Motion from Speech",
            "updated": "2023-11-29T07:57:30Z",
            "published": "2023-11-29T07:57:30Z",
            "summary": "This paper addresses the problem of generating whole-body motion from speech.\nDespite great successes, prior methods still struggle to produce reasonable and\ndiverse whole-body motions from speech. This is due to their reliance on\nsuboptimal representations and a lack of strategies for generating diverse\nresults. To address these challenges, we present a novel hybrid point\nrepresentation to achieve accurate and continuous motion generation, e.g.,\navoiding foot skating, and this representation can be transformed into an\neasy-to-use representation, i.e., SMPL-X body mesh, for many applications. To\ngenerate whole-body motion from speech, for facial motion, closely tied to the\naudio signal, we introduce an encoder-decoder architecture to achieve\ndeterministic outcomes. However, for the body and hands, which have weaker\nconnections to the audio signal, we aim to generate diverse yet reasonable\nmotions. To boost diversity in motion generation, we propose a contrastive\nmotion learning method to encourage the model to produce more distinctive\nrepresentations. Specifically, we design a robust VQ-VAE to learn a quantized\nmotion codebook using our hybrid representation. Then, we regress the motion\nrepresentation from the audio signal by a translation model employing our\ncontrastive motion learning method. Experimental results validate the superior\nperformance and the correctness of our model. The project page is available for\nresearch purposes at http://cic.tju.edu.cn/faculty/likun/projects/SpeechAct.",
            "author": [
                "Jinsong Zhang",
                "Minjie Zhu",
                "Yuxiang Zhang",
                "Yebin Liu",
                "Kun Li"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17425v1",
                "http://arxiv.org/pdf/2311.17425v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17952v1",
            "title": "Synchronizing Vision and Language: Bidirectional Token-Masking\n  AutoEncoder for Referring Image Segmentation",
            "updated": "2023-11-29T07:33:38Z",
            "published": "2023-11-29T07:33:38Z",
            "summary": "Referring Image Segmentation (RIS) aims to segment target objects expressed\nin natural language within a scene at the pixel level. Various recent RIS\nmodels have achieved state-of-the-art performance by generating contextual\ntokens to model multimodal features from pretrained encoders and effectively\nfusing them using transformer-based cross-modal attention. While these methods\nmatch language features with image features to effectively identify likely\ntarget objects, they often struggle to correctly understand contextual\ninformation in complex and ambiguous sentences and scenes. To address this\nissue, we propose a novel bidirectional token-masking autoencoder (BTMAE)\ninspired by the masked autoencoder (MAE). The proposed model learns the context\nof image-to-language and language-to-image by reconstructing missing features\nin both image and language features at the token level. In other words, this\napproach involves mutually complementing across the features of images and\nlanguage, with a focus on enabling the network to understand interconnected\ndeep contextual information between the two modalities. This learning method\nenhances the robustness of RIS performance in complex sentences and scenes. Our\nBTMAE achieves state-of-the-art performance on three popular datasets, and we\ndemonstrate the effectiveness of the proposed method through various ablation\nstudies.",
            "author": [
                "Minhyeok Lee",
                "Dogyoon Lee",
                "Jungho Lee",
                "Suhwan Cho",
                "Heeseung Choi",
                "Ig-Jae Kim",
                "Sangyoun Lee"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17952v1",
                "http://arxiv.org/pdf/2311.17952v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17410v2",
            "title": "GNNFlow: A Distributed Framework for Continuous Temporal GNN Learning on\n  Dynamic Graphs",
            "updated": "2023-11-30T03:48:24Z",
            "published": "2023-11-29T07:30:32Z",
            "summary": "Graph Neural Networks (GNNs) play a crucial role in various fields. However,\nmost existing deep graph learning frameworks assume pre-stored static graphs\nand do not support training on graph streams. In contrast, many real-world\ngraphs are dynamic and contain time domain information. We introduce GNNFlow, a\ndistributed framework that enables efficient continuous temporal graph\nrepresentation learning on dynamic graphs on multi-GPU machines. GNNFlow\nintroduces an adaptive time-indexed block-based data structure that effectively\nbalances memory usage with graph update and sampling operation efficiency. It\nfeatures a hybrid GPU-CPU graph data placement for rapid GPU-based temporal\nneighborhood sampling and kernel optimizations for enhanced sampling processes.\nA dynamic GPU cache for node and edge features is developed to maximize cache\nhit rates through reuse and restoration strategies. GNNFlow supports\ndistributed training across multiple machines with static scheduling to ensure\nload balance. We implement GNNFlow based on DGL and PyTorch. Our experimental\nresults show that GNNFlow provides up to 21.1x faster continuous learning than\nexisting systems.",
            "author": [
                "Yuchen Zhong",
                "Guangming Sheng",
                "Tianzuo Qin",
                "Minjie Wang",
                "Quan Gan",
                "Chuan Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17410v2",
                "http://arxiv.org/pdf/2311.17410v2"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02181v1",
            "title": "How Generative-AI can be Effectively used in Government Chatbots",
            "updated": "2023-11-29T07:27:15Z",
            "published": "2023-11-29T07:27:15Z",
            "summary": "With the rapid development of artificial intelligence and breakthroughs in\nmachine learning and natural language processing, intelligent\nquestion-answering robots have become widely used in government affairs. This\npaper conducts a horizontal comparison between Guangdong Province's government\nchatbots, ChatGPT, and Wenxin Ernie, two large language models, to analyze the\nstrengths and weaknesses of existing government chatbots and AIGC technology.\nThe study finds significant differences between government chatbots and large\nlanguage models. China's government chatbots are still in an exploratory stage\nand have a gap to close to achieve \"intelligence.\" To explore the future\ndirection of government chatbots more deeply, this research proposes targeted\noptimization paths to help generative AI be effectively applied in government\nchatbot conversations.",
            "author": [
                "Zeteng Lin"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02181v1",
                "http://arxiv.org/pdf/2312.02181v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG",
                "econ.GN",
                "q-fin.EC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17408v1",
            "title": "Dynamic Dense Graph Convolutional Network for Skeleton-based Human\n  Motion Prediction",
            "updated": "2023-11-29T07:25:49Z",
            "published": "2023-11-29T07:25:49Z",
            "summary": "Graph Convolutional Networks (GCN) which typically follows a neural message\npassing framework to model dependencies among skeletal joints has achieved high\nsuccess in skeleton-based human motion prediction task. Nevertheless, how to\nconstruct a graph from a skeleton sequence and how to perform message passing\non the graph are still open problems, which severely affect the performance of\nGCN. To solve both problems, this paper presents a Dynamic Dense Graph\nConvolutional Network (DD-GCN), which constructs a dense graph and implements\nan integrated dynamic message passing. More specifically, we construct a dense\ngraph with 4D adjacency modeling as a comprehensive representation of motion\nsequence at different levels of abstraction. Based on the dense graph, we\npropose a dynamic message passing framework that learns dynamically from data\nto generate distinctive messages reflecting sample-specific relevance among\nnodes in the graph. Extensive experiments on benchmark Human 3.6M and CMU Mocap\ndatasets verify the effectiveness of our DD-GCN which obviously outperforms\nstate-of-the-art GCN-based methods, especially when using long-term and our\nproposed extremely long-term protocol.",
            "author": [
                "Xinshun Wang",
                "Wanying Zhang",
                "Can Wang",
                "Yuan Gao",
                "Mengyuan Liu"
            ],
            "link": [
                "http://dx.doi.org/10.1109/TIP.2023.3334954",
                "http://arxiv.org/abs/2311.17408v1",
                "http://arxiv.org/pdf/2311.17408v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17405v2",
            "title": "Learning and Autonomy for Extraterrestrial Terrain Sampling: An\n  Experience Report from OWLAT Deployment",
            "updated": "2023-12-04T17:25:15Z",
            "published": "2023-11-29T07:16:34Z",
            "summary": "Extraterrestrial autonomous lander missions increasingly demand adaptive\ncapabilities to handle the unpredictable and diverse nature of the terrain.\nThis paper discusses the deployment of a Deep Meta-Learning with Controlled\nDeployment Gaps (CoDeGa) trained model for terrain scooping tasks in Ocean\nWorlds Lander Autonomy Testbed (OWLAT) at NASA Jet Propulsion Laboratory. The\nCoDeGa-powered scooping strategy is designed to adapt to novel terrains,\nselecting scooping actions based on the available RGB-D image data and limited\nexperience. The paper presents our experiences with transferring the scooping\nframework with CoDeGa-trained model from a low-fidelity testbed to the\nhigh-fidelity OWLAT testbed. Additionally, it validates the method's\nperformance in novel, realistic environments, and shares the lessons learned\nfrom deploying learning-based autonomy algorithms for space exploration.\nExperimental results from OWLAT substantiate the efficacy of CoDeGa in rapidly\nadapting to unfamiliar terrains and effectively making autonomous decisions\nunder considerable domain shifts, thereby endorsing its potential utility in\nfuture extraterrestrial missions.",
            "author": [
                "Pranay Thangeda",
                "Ashish Goel",
                "Erica Tevere",
                "Yifan Zhu",
                "Erik Kramer",
                "Adriana Daca",
                "Hari Nayar",
                "Kris Hauser",
                "Melkior Ornik"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17405v2",
                "http://arxiv.org/pdf/2311.17405v2"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17951v1",
            "title": "C3Net: Compound Conditioned ControlNet for Multimodal Content Generation",
            "updated": "2023-11-29T07:11:56Z",
            "published": "2023-11-29T07:11:56Z",
            "summary": "We present Compound Conditioned ControlNet, C3Net, a novel generative neural\narchitecture taking conditions from multiple modalities and synthesizing\nmultimodal contents simultaneously (e.g., image, text, audio). C3Net adapts the\nControlNet architecture to jointly train and make inferences on a\nproduction-ready diffusion model and its trainable copies. Specifically, C3Net\nfirst aligns the conditions from multi-modalities to the same semantic latent\nspace using modality-specific encoders based on contrastive training. Then, it\ngenerates multimodal outputs based on the aligned latent space, whose semantic\ninformation is combined using a ControlNet-like architecture called Control\nC3-UNet. Correspondingly, with this system design, our model offers an improved\nsolution for joint-modality generation through learning and explaining\nmultimodal conditions instead of simply taking linear interpolations on the\nlatent space. Meanwhile, as we align conditions to a unified latent space,\nC3Net only requires one trainable Control C3-UNet to work on multimodal\nsemantic information. Furthermore, our model employs unimodal pretraining on\nthe condition alignment stage, outperforming the non-pretrained alignment even\non relatively scarce training data and thus demonstrating high-quality compound\ncondition generation. We contribute the first high-quality tri-modal validation\nset to validate quantitatively that C3Net outperforms or is on par with first\nand contemporary state-of-the-art multimodal generation. Our codes and\ntri-modal dataset will be released.",
            "author": [
                "Juntao Zhang",
                "Yuehuai Liu",
                "Yu-Wing Tai",
                "Chi-Keung Tang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17951v1",
                "http://arxiv.org/pdf/2311.17951v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17401v1",
            "title": "Gene-MOE: A Sparsely-gated Framework for Pan-Cancer Genomic Analysis",
            "updated": "2023-11-29T07:09:25Z",
            "published": "2023-11-29T07:09:25Z",
            "summary": "Analyzing the genomic information from the Pan-Cancer database can help us\nunderstand cancer-related factors and contribute to the cancer diagnosis and\nprognosis. However, existing computational methods and deep learning methods\ncan not effectively find the deep correlations between tens of thousands of\ngenes, which leads to precision loss. In this paper, we proposed a novel\npretrained model called Gene-MOE to learn the general feature representations\nof the Pan-Cancer dataset and transfer the pretrained weights to the downstream\ntasks. The Gene-MOE fully exploits the mixture of expert (MOE) layers to learn\nrich feature representations of high-dimensional genes. At the same time, we\nbuild a mixture of attention expert (MOAE) model to learn the deep semantic\nrelationships within genetic features. Finally, we proposed a new\nself-supervised pretraining strategy including loss function design, data\nenhancement, and optimization strategy to train the Gene-MOE and further\nimprove the performance for the downstream analysis. We carried out cancer\nclassification and survival analysis experiments based on the Gene-MOE.\nAccording to the survival analysis results on 14 cancer types, using Gene-MOE\noutperformed state-of-the-art models on 12 cancer types. According to the\nclassification results, the total accuracy of the classification model for 33\ncancer classifications reached 95.2\\%. Through detailed feature analysis, we\nfound the Gene-MOE model can learn rich feature representations of\nhigh-dimensional genes.",
            "author": [
                "Xiangyu Meng",
                "Tao Song",
                "Qing Yang",
                "Huanhuan Dai",
                "Lian Qiao",
                "Hongzhen Ding",
                "Long Hao",
                "Xun Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17401v1",
                "http://arxiv.org/pdf/2311.17401v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17400v2",
            "title": "Improving the Robustness of Transformer-based Large Language Models with\n  Dynamic Attention",
            "updated": "2023-11-30T02:08:24Z",
            "published": "2023-11-29T07:09:13Z",
            "summary": "Transformer-based models, such as BERT and GPT, have been widely adopted in\nnatural language processing (NLP) due to their exceptional performance.\nHowever, recent studies show their vulnerability to textual adversarial attacks\nwhere the model's output can be misled by intentionally manipulating the text\ninputs. Despite various methods that have been proposed to enhance the model's\nrobustness and mitigate this vulnerability, many require heavy consumption\nresources (e.g., adversarial training) or only provide limited protection\n(e.g., defensive dropout). In this paper, we propose a novel method called\ndynamic attention, tailored for the transformer architecture, to enhance the\ninherent robustness of the model itself against various adversarial attacks.\nOur method requires no downstream task knowledge and does not incur additional\ncosts. The proposed dynamic attention consists of two modules: (I) attention\nrectification, which masks or weakens the attention value of the chosen tokens,\nand (ii) dynamic modeling, which dynamically builds the set of candidate\ntokens. Extensive experiments demonstrate that dynamic attention significantly\nmitigates the impact of adversarial attacks, improving up to 33\\% better\nperformance than previous methods against widely-used adversarial attacks. The\nmodel-level design of dynamic attention enables it to be easily combined with\nother defense methods (e.g., adversarial training) to further enhance the\nmodel's robustness. Furthermore, we demonstrate that dynamic attention\npreserves the state-of-the-art robustness space of the original model compared\nto other dynamic modeling methods.",
            "author": [
                "Lujia Shen",
                "Yuwen Pu",
                "Shouling Ji",
                "Changjiang Li",
                "Xuhong Zhang",
                "Chunpeng Ge",
                "Ting Wang"
            ],
            "link": [
                "http://dx.doi.org/10.14722/ndss.2024.24115",
                "http://arxiv.org/abs/2311.17400v2",
                "http://arxiv.org/pdf/2311.17400v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.CR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17398v1",
            "title": "Numerical Accuracy Matters: Applications of Machine Learned Potential\n  Energy Surfaces",
            "updated": "2023-11-29T06:55:34Z",
            "published": "2023-11-29T06:55:34Z",
            "summary": "The role of numerical accuracy in training and evaluating neural\nnetwork-based potential energy surfaces is examined for different experimental\nobservables. For observables that require third- and fourth-order derivatives\nof the total energy with respect to Cartesian coordinates single-precision\narithmetics as is typically used in ML-based approaches is insufficient and\nleads to roughness of the underlying PES as is explicitly demonstrated.\nIncreasing the numerical accuracy to double-precision yields a smooth PES with\nhigher-order derivatives that are numerically stable and yield meaningful\nanharmonic frequencies and tunneling splitting as is demonstrated for H$_2$CO\nand malonaldehyde. For molecular dynamics simulations, which only require\nfirst-order derivatives, single-precision arithmetics appears to be sufficient,\nthough.",
            "author": [
                "Silvan K\u00e4ser",
                "Markus Meuwly"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17398v1",
                "http://arxiv.org/pdf/2311.17398v1"
            ],
            "primary_category": "physics.chem-ph",
            "category": [
                "physics.chem-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17394v1",
            "title": "Deepfakes, Misinformation, and Disinformation in the Era of Frontier AI,\n  Generative AI, and Large AI Models",
            "updated": "2023-11-29T06:47:58Z",
            "published": "2023-11-29T06:47:58Z",
            "summary": "With the advent of sophisticated artificial intelligence (AI) technologies,\nthe proliferation of deepfakes and the spread of m/disinformation have emerged\nas formidable threats to the integrity of information ecosystems worldwide.\nThis paper provides an overview of the current literature. Within the frontier\nAI's crucial application in developing defense mechanisms for detecting\ndeepfakes, we highlight the mechanisms through which generative AI based on\nlarge models (LM-based GenAI) craft seemingly convincing yet fabricated\ncontents. We explore the multifaceted implications of LM-based GenAI on\nsociety, politics, and individual privacy violations, underscoring the urgent\nneed for robust defense strategies. To address these challenges, in this study,\nwe introduce an integrated framework that combines advanced detection\nalgorithms, cross-platform collaboration, and policy-driven initiatives to\nmitigate the risks associated with AI-Generated Content (AIGC). By leveraging\nmulti-modal analysis, digital watermarking, and machine learning-based\nauthentication techniques, we propose a defense mechanism adaptable to AI\ncapabilities of ever-evolving nature. Furthermore, the paper advocates for a\nglobal consensus on the ethical usage of GenAI and implementing cyber-wellness\neducational programs to enhance public awareness and resilience against\nm/disinformation. Our findings suggest that a proactive and collaborative\napproach involving technological innovation and regulatory oversight is\nessential for safeguarding netizens while interacting with cyberspace against\nthe insidious effects of deepfakes and GenAI-enabled m/disinformation\ncampaigns.",
            "author": [
                "Mohamed R. Shoaib",
                "Zefan Wang",
                "Milad Taleby Ahvanooey",
                "Jun Zhao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17394v1",
                "http://arxiv.org/pdf/2311.17394v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17391v1",
            "title": "Unveiling the Implicit Toxicity in Large Language Models",
            "updated": "2023-11-29T06:42:36Z",
            "published": "2023-11-29T06:42:36Z",
            "summary": "The open-endedness of large language models (LLMs) combined with their\nimpressive capabilities may lead to new safety issues when being exploited for\nmalicious use. While recent studies primarily focus on probing toxic outputs\nthat can be easily detected with existing toxicity classifiers, we show that\nLLMs can generate diverse implicit toxic outputs that are exceptionally\ndifficult to detect via simply zero-shot prompting. Moreover, we propose a\nreinforcement learning (RL) based attacking method to further induce the\nimplicit toxicity in LLMs. Specifically, we optimize the language model with a\nreward that prefers implicit toxic outputs to explicit toxic and non-toxic\nones. Experiments on five widely-adopted toxicity classifiers demonstrate that\nthe attack success rate can be significantly improved through RL fine-tuning.\nFor instance, the RL-finetuned LLaMA-13B model achieves an attack success rate\nof 90.04% on BAD and 62.85% on Davinci003. Our findings suggest that LLMs pose\na significant threat in generating undetectable implicit toxic outputs. We\nfurther show that fine-tuning toxicity classifiers on the annotated examples\nfrom our attacking method can effectively enhance their ability to detect\nLLM-generated implicit toxic language. The code is publicly available at\nhttps://github.com/thu-coai/Implicit-Toxicity.",
            "author": [
                "Jiaxin Wen",
                "Pei Ke",
                "Hao Sun",
                "Zhexin Zhang",
                "Chengfei Li",
                "Jinfeng Bai",
                "Minlie Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17391v1",
                "http://arxiv.org/pdf/2311.17391v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17373v1",
            "title": "The Devil is in the Data: Learning Fair Graph Neural Networks via\n  Partial Knowledge Distillation",
            "updated": "2023-11-29T05:54:58Z",
            "published": "2023-11-29T05:54:58Z",
            "summary": "Graph neural networks (GNNs) are being increasingly used in many high-stakes\ntasks, and as a result, there is growing attention on their fairness recently.\nGNNs have been shown to be unfair as they tend to make discriminatory decisions\ntoward certain demographic groups, divided by sensitive attributes such as\ngender and race. While recent works have been devoted to improving their\nfairness performance, they often require accessible demographic information.\nThis greatly limits their applicability in real-world scenarios due to legal\nrestrictions. To address this problem, we present a demographic-agnostic method\nto learn fair GNNs via knowledge distillation, namely FairGKD. Our work is\nmotivated by the empirical observation that training GNNs on partial data\n(i.e., only node attributes or topology data) can improve their fairness,\nalbeit at the cost of utility. To make a balanced trade-off between fairness\nand utility performance, we employ a set of fairness experts (i.e., GNNs\ntrained on different partial data) to construct the synthetic teacher, which\ndistills fairer and informative knowledge to guide the learning of the GNN\nstudent. Experiments on several benchmark datasets demonstrate that FairGKD,\nwhich does not require access to demographic information, significantly\nimproves the fairness of GNNs by a large margin while maintaining their\nutility.",
            "author": [
                "Yuchang Zhu",
                "Jintang Li",
                "Liang Chen",
                "Zibin Zheng"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17373v1",
                "http://arxiv.org/pdf/2311.17373v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CY",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17949v1",
            "title": "Zero-shot Retrieval: Augmenting Pre-trained Models with Search Engines",
            "updated": "2023-11-29T05:33:28Z",
            "published": "2023-11-29T05:33:28Z",
            "summary": "Large pre-trained models can dramatically reduce the amount of task-specific\ndata required to solve a problem, but they often fail to capture\ndomain-specific nuances out of the box. The Web likely contains the information\nnecessary to excel on any specific application, but identifying the right data\na priori is challenging. This paper shows how to leverage recent advances in\nNLP and multi-modal learning to augment a pre-trained model with search engine\nretrieval. We propose to retrieve useful data from the Web at test time based\non test cases that the model is uncertain about. Different from existing\nretrieval-augmented approaches, we then update the model to address this\nunderlying uncertainty. We demonstrate substantial improvements in zero-shot\nperformance, e.g. a remarkable increase of 15 percentage points in accuracy on\nthe Stanford Cars and Flowers datasets. We also present extensive experiments\nthat explore the impact of noisy retrieval and different learning strategies.",
            "author": [
                "Hamed Damirchi",
                "Cristian Rodr\u00edguez-Opazo",
                "Ehsan Abbasnejad",
                "Damien Teney",
                "Javen Qinfeng Shi",
                "Stephen Gould",
                "Anton van den Hengel"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17949v1",
                "http://arxiv.org/pdf/2311.17949v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17948v1",
            "title": "Action-slot: Visual Action-centric Representations for Multi-label\n  Atomic Activity Recognition in Traffic Scenes",
            "updated": "2023-11-29T05:28:05Z",
            "published": "2023-11-29T05:28:05Z",
            "summary": "In this paper, we study multi-label atomic activity recognition. Despite the\nnotable progress in action recognition, it is still challenging to recognize\natomic activities due to a deficiency in a holistic understanding of both\nmultiple road users' motions and their contextual information. In this paper,\nwe introduce Action-slot, a slot attention-based approach that learns visual\naction-centric representations, capturing both motion and contextual\ninformation. Our key idea is to design action slots that are capable of paying\nattention to regions where atomic activities occur, without the need for\nexplicit perception guidance. To further enhance slot attention, we introduce a\nbackground slot that competes with action slots, aiding the training process in\navoiding unnecessary focus on background regions devoid of activities. Yet, the\nimbalanced class distribution in the existing dataset hampers the assessment of\nrare activities. To address the limitation, we collect a synthetic dataset\ncalled TACO, which is four times larger than OATS and features a balanced\ndistribution of atomic activities. To validate the effectiveness of our method,\nwe conduct comprehensive experiments and ablation studies against various\naction recognition baselines. We also show that the performance of multi-label\natomic activity recognition on real-world datasets can be improved by\npretraining representations on TACO. We will release our source code and\ndataset. See the videos of visualization on the project page:\nhttps://hcis-lab.github.io/Action-slot/",
            "author": [
                "Chi-Hsi Kung",
                "Shu-Wei Lu",
                "Yi-Hsuan Tsai",
                "Yi-Ting Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17948v1",
                "http://arxiv.org/pdf/2311.17948v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17358v1",
            "title": "OpenSense: An Open-World Sensing Framework for Incremental Learning and\n  Dynamic Sensor Scheduling on Embedded Edge Devices",
            "updated": "2023-11-29T05:11:29Z",
            "published": "2023-11-29T05:11:29Z",
            "summary": "Recent advances in Internet-of-Things (IoT) technologies have sparked\nsignificant interest towards developing learning-based sensing applications on\nembedded edge devices. These efforts, however, are being challenged by the\ncomplexities of adapting to unforeseen conditions in an open-world environment,\nmainly due to the intensive computational and energy demands exceeding the\ncapabilities of edge devices. In this paper, we propose OpenSense, an\nopen-world time-series sensing framework for making inferences from time-series\nsensor data and achieving incremental learning on an embedded edge device with\nlimited resources. The proposed framework is able to achieve two essential\ntasks, inference and incremental learning, eliminating the necessity for\npowerful cloud servers. In addition, to secure enough time for incremental\nlearning and reduce energy consumption, we need to schedule sensing activities\nwithout missing any events in the environment. Therefore, we propose two\ndynamic sensor scheduling techniques: (i) a class-level period assignment\nscheduler that finds an appropriate sensing period for each inferred class, and\n(ii) a Q-learning-based scheduler that dynamically determines the sensing\ninterval for each classification moment by learning the patterns of event\nclasses. With this framework, we discuss the design choices made to ensure\nsatisfactory learning performance and efficient resource usage. Experimental\nresults demonstrate the ability of the system to incrementally adapt to\nunforeseen conditions and to efficiently schedule to run on a\nresource-constrained device.",
            "author": [
                "Abdulrahman Bukhari",
                "Seyedmehdi Hosseinimotlagh",
                "Hyoseung Kim"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17358v1",
                "http://arxiv.org/pdf/2311.17358v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17354v1",
            "title": "A natural language processing-based approach: mapping human perception\n  by understanding deep semantic features in street view images",
            "updated": "2023-11-29T05:00:43Z",
            "published": "2023-11-29T05:00:43Z",
            "summary": "In the past decade, using Street View images and machine learning to measure\nhuman perception has become a mainstream research approach in urban science.\nHowever, this approach using only image-shallow information makes it difficult\nto comprehensively understand the deep semantic features of human perception of\na scene. In this study, we proposed a new framework based on a pre-train\nnatural language model to understand the relationship between human perception\nand the sense of a scene. Firstly, Place Pulse 2.0 was used as our base\ndataset, which contains a variety of human-perceived labels, namely, beautiful,\nsafe, wealthy, depressing, boring, and lively. An image captioning network was\nused to extract the description information of each street view image.\nSecondly, a pre-trained BERT model was finetuning and added a regression\nfunction for six human perceptual dimensions. Furthermore, we compared the\nperformance of five traditional regression methods with our approach and\nconducted a migration experiment in Hong Kong. Our results show that human\nperception scoring by deep semantic features performed better than previous\nstudies by machine learning methods with shallow features. The use of deep\nscene semantic features provides new ideas for subsequent human perception\nresearch, as well as better explanatory power in the face of spatial\nheterogeneity.",
            "author": [
                "Haoran Ma",
                "Dongdong Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17354v1",
                "http://arxiv.org/pdf/2311.17354v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17353v1",
            "title": "Continuous optimization by quantum adaptive distribution search",
            "updated": "2023-11-29T04:48:09Z",
            "published": "2023-11-29T04:48:09Z",
            "summary": "In this paper, we introduce the quantum adaptive distribution search (QuADS),\na quantum continuous optimization algorithm that integrates Grover adaptive\nsearch (GAS) with the covariance matrix adaptation - evolution strategy\n(CMA-ES), a classical technique for continuous optimization. QuADS utilizes the\nquantum-based search capabilities of GAS and enhances them with the principles\nof CMA-ES for more efficient optimization. It employs a multivariate normal\ndistribution for the initial state of the quantum search and repeatedly updates\nit throughout the optimization process. Our numerical experiments show that\nQuADS outperforms both GAS and CMA-ES. This is achieved through adaptive\nrefinement of the initial state distribution rather than consistently using a\nuniform state, resulting in fewer oracle calls. This study presents an\nimportant step toward exploiting the potential of quantum computing for\ncontinuous optimization.",
            "author": [
                "Kohei Morimoto",
                "Yusuke Takase",
                "Kosuke Mitarai",
                "Keisuke Fujii"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17353v1",
                "http://arxiv.org/pdf/2311.17353v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03742v1",
            "title": "Clinical Risk Prediction Using Language Models: Benefits And\n  Considerations",
            "updated": "2023-11-29T04:32:19Z",
            "published": "2023-11-29T04:32:19Z",
            "summary": "The utilization of Electronic Health Records (EHRs) for clinical risk\nprediction is on the rise. However, strict privacy regulations limit access to\ncomprehensive health records, making it challenging to apply standard machine\nlearning algorithms in practical real-world scenarios. Previous research has\naddressed this data limitation by incorporating medical ontologies and\nemploying transfer learning methods. In this study, we investigate the\npotential of leveraging language models (LMs) as a means to incorporate\nsupplementary domain knowledge for improving the performance of various\nEHR-based risk prediction tasks. Unlike applying LMs to unstructured EHR data\nsuch as clinical notes, this study focuses on using textual descriptions within\nstructured EHR to make predictions exclusively based on that information. We\nextensively compare against previous approaches across various data types and\nsizes. We find that employing LMs to represent structured EHRs, such as\ndiagnostic histories, leads to improved or at least comparable performance in\ndiverse risk prediction tasks. Furthermore, LM-based approaches offer numerous\nadvantages, including few-shot learning, the capability to handle previously\nunseen medical concepts, and adaptability to various medical vocabularies.\nNevertheless, we underscore, through various experiments, the importance of\nbeing cautious when employing such models, as concerns regarding the\nreliability of LMs persist.",
            "author": [
                "Angeela Acharya",
                "Sulabh Shrestha",
                "Anyi Chen",
                "Joseph Conte",
                "Sanja Avramovic",
                "Siddhartha Sikdar",
                "Antonios Anastasopoulos",
                "Sanmay Das"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03742v1",
                "http://arxiv.org/pdf/2312.03742v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17352v1",
            "title": "Efficient Stitchable Task Adaptation",
            "updated": "2023-11-29T04:31:35Z",
            "published": "2023-11-29T04:31:35Z",
            "summary": "The paradigm of pre-training and fine-tuning has laid the foundation for\ndeploying deep learning models. However, most fine-tuning methods are designed\nto meet a specific resource budget. Recently, considering diverse deployment\nscenarios with various resource budgets, stitchable neural network (SN-Net) is\nintroduced to quickly obtain numerous new networks (stitches) from the\npre-trained models (anchors) in a model family via model stitching. Although\npromising, SN-Net confronts new challenges when adapting it to new target\ndomains, including huge memory and storage requirements and a long and\nsub-optimal multistage adaptation process. In this work, we present a novel\nframework, Efficient Stitchable Task Adaptation (ESTA), to efficiently produce\na palette of fine-tuned models that adhere to diverse resource constraints.\nSpecifically, we first tailor parameter-efficient fine-tuning to share low-rank\nupdates among the stitches while maintaining independent bias terms. In this\nway, we largely reduce fine-tuning memory burdens and mitigate the interference\namong stitches that arises in task adaptation. Furthermore, we streamline a\nsimple yet effective one-stage deployment pipeline, which estimates the\nimportant stitches to deploy with training-time gradient statistics. By\nassigning higher sampling probabilities to important stitches, we also get a\nboosted Pareto frontier. Extensive experiments on 25 downstream visual\nrecognition tasks demonstrate that our ESTA is capable of generating stitches\nwith smooth accuracy-efficiency trade-offs and surpasses the direct SN-Net\nadaptation by remarkable margins with significantly lower training time and\nfewer trainable parameters. Furthermore, we demonstrate the flexibility and\nscalability of our ESTA framework by stitching LLMs from LLaMA family,\nobtaining chatbot stitches of assorted sizes.",
            "author": [
                "Haoyu He",
                "Zizheng Pan",
                "Jing Liu",
                "Jianfei Cai",
                "Bohan Zhuang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17352v1",
                "http://arxiv.org/pdf/2311.17352v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00055v1",
            "title": "LEAP: LLM-Generation of Egocentric Action Programs",
            "updated": "2023-11-29T04:25:52Z",
            "published": "2023-11-29T04:25:52Z",
            "summary": "We introduce LEAP (illustrated in Figure 1), a novel method for generating\nvideo-grounded action programs through use of a Large Language Model (LLM).\nThese action programs represent the motoric, perceptual, and structural aspects\nof action, and consist of sub-actions, pre- and post-conditions, and control\nflows. LEAP's action programs are centered on egocentric video and employ\nrecent developments in LLMs both as a source for program knowledge and as an\naggregator and assessor of multimodal video information. We apply LEAP over a\nmajority (87\\%) of the training set of the EPIC Kitchens dataset, and release\nthe resulting action programs as a publicly available dataset here\n(https://drive.google.com/drive/folders/1Cpkw_TI1IIxXdzor0pOXG3rWJWuKU5Ex?usp=drive_link).\nWe employ LEAP as a secondary source of supervision, using its action programs\nin a loss term applied to action recognition and anticipation networks. We\ndemonstrate sizable improvements in performance in both tasks due to training\nwith the LEAP dataset. Our method achieves 1st place on the EPIC Kitchens\nAction Recognition leaderboard as of November 17 among the networks restricted\nto RGB-input (see Supplementary Materials).",
            "author": [
                "Eadom Dessalene",
                "Michael Maynord",
                "Cornelia Ferm\u00fcller",
                "Yiannis Aloimonos"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00055v1",
                "http://arxiv.org/pdf/2312.00055v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17351v1",
            "title": "Exploring Large Language Models for Human Mobility Prediction under\n  Public Events",
            "updated": "2023-11-29T04:25:15Z",
            "published": "2023-11-29T04:25:15Z",
            "summary": "Public events, such as concerts and sports games, can be major attractors for\nlarge crowds, leading to irregular surges in travel demand. Accurate human\nmobility prediction for public events is thus crucial for event planning as\nwell as traffic or crowd management. While rich textual descriptions about\npublic events are commonly available from online sources, it is challenging to\nencode such information in statistical or machine learning models. Existing\nmethods are generally limited in incorporating textual information, handling\ndata sparsity, or providing rationales for their predictions. To address these\nchallenges, we introduce a framework for human mobility prediction under public\nevents (LLM-MPE) based on Large Language Models (LLMs), leveraging their\nunprecedented ability to process textual data, learn from minimal examples, and\ngenerate human-readable explanations. Specifically, LLM-MPE first transforms\nraw, unstructured event descriptions from online sources into a standardized\nformat, and then segments historical mobility data into regular and\nevent-related components. A prompting strategy is designed to direct LLMs in\nmaking and rationalizing demand predictions considering historical mobility and\nevent features. A case study is conducted for Barclays Center in New York City,\nbased on publicly available event information and taxi trip data. Results show\nthat LLM-MPE surpasses traditional models, particularly on event days, with\ntextual data significantly enhancing its accuracy. Furthermore, LLM-MPE offers\ninterpretable insights into its predictions. Despite the great potential of\nLLMs, we also identify key challenges including misinformation and high costs\nthat remain barriers to their broader adoption in large-scale human mobility\nanalysis.",
            "author": [
                "Yuebing Liang",
                "Yichao Liu",
                "Xiaohan Wang",
                "Zhan Zhao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17351v1",
                "http://arxiv.org/pdf/2311.17351v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17347v1",
            "title": "Data-driven Bandwidth Adaptation for Radio Access Network Slices",
            "updated": "2023-11-29T04:14:02Z",
            "published": "2023-11-29T04:14:02Z",
            "summary": "We develop a Bandwidth Demand Estimator (BDE); a network function that\nperiodically monitors the traffic of a Network Slice (NS) and adapts the\nbandwidth at the base station to efficiently meet its packet delay\nrequirements. We design the BDE based on a data-driven approach that utilizes\nQoS feedback. Given the traffic of the NS, the BDE needs to learn the bandwidth\nrequired to satisfy the QoS. However, it also needs to consider the future\neffects of its actions since low bandwidths may create large packet queues that\nhinder the allocation process later on. For this reason, we propose a\nreinforcement learning approach. The action is the allocated bandwidth. The\nstate describes the traffic, the wireless channel and the packet queue of the\nNS. The cost is a weighted sum of the bandwidth and of a binary variable that\nequals 1 if the QoS is violated. We periodically estimate the transition matrix\nof the system and perform value iteration to find the optimal policy. To speed\nup the estimation process, we initialize our algorithm with multi-armed bandits\nand exploit the monotonicity of the cost w.r.t. the action. The overall\napproach can be viewed as a data-driven version of receding horizon control. We\nimplement our BDE on a 3GPP compliant testbed developed by Amarisoft.\nExperimental results show that the BDE reduces both the average allocated\nbandwidth and the QoS violations in the NS when compared to baseline schemes.\nThe BDE can also satisfy per user tail packet delay requirements.",
            "author": [
                "Panagiotis Nikolaidis",
                "Asim Zoulkarni",
                "John Baras"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17347v1",
                "http://arxiv.org/pdf/2311.17347v1"
            ],
            "primary_category": "cs.NI",
            "category": [
                "cs.NI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17946v1",
            "title": "DreamSync: Aligning Text-to-Image Generation with Image Understanding\n  Feedback",
            "updated": "2023-11-29T03:42:16Z",
            "published": "2023-11-29T03:42:16Z",
            "summary": "Despite their wide-spread success, Text-to-Image models (T2I) still struggle\nto produce images that are both aesthetically pleasing and faithful to the\nuser's input text. We introduce DreamSync, a model-agnostic training algorithm\nby design that improves T2I models to be faithful to the text input. DreamSync\nbuilds off a recent insight from TIFA's evaluation framework -- that large\nvision-language models (VLMs) can effectively identify the fine-grained\ndiscrepancies between generated images and the text inputs. DreamSync uses this\ninsight to train T2I models without any labeled data; it improves T2I models\nusing its own generations. First, it prompts the model to generate several\ncandidate images for a given input text. Then, it uses two VLMs to select the\nbest generation: a Visual Question Answering model that measures the alignment\nof generated images to the text, and another that measures the generation's\naesthetic quality. After selection, we use LoRA to iteratively finetune the T2I\nmodel to guide its generation towards the selected best generations. DreamSync\ndoes not need any additional human annotation. model architecture changes, or\nreinforcement learning. Despite its simplicity, DreamSync improves both the\nsemantic alignment and aesthetic appeal of two diffusion-based T2I models,\nevidenced by multiple benchmarks (+1.7% on TIFA, +2.9% on DSG1K, +3.4% on VILA\naesthetic) and human evaluation.",
            "author": [
                "Jiao Sun",
                "Deqing Fu",
                "Yushi Hu",
                "Su Wang",
                "Royi Rassin",
                "Da-Cheng Juan",
                "Dana Alon",
                "Charles Herrmann",
                "Sjoerd van Steenkiste",
                "Ranjay Krishna",
                "Cyrus Rashtchian"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17946v1",
                "http://arxiv.org/pdf/2311.17946v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17341v1",
            "title": "Improving Photometric Redshifts by Merging Probability Density Functions\n  from Template-Based and Machine Learning Algorithms",
            "updated": "2023-11-29T03:39:50Z",
            "published": "2023-11-29T03:39:50Z",
            "summary": "This study aims to improve the photometric redshifts (photo-$z$s) of galaxies\nby integrating two contemporary methods: template-fitting and machine learning.\nFinding the synergy between these two methods was not a high priority in the\npast, but now that our computer processing power and observational accuracy\nhave increased, we deem it worth investigating. We compared two methods to\nimprove galaxy photometric redshift estimations by using the algorithms ANNz2\nand BPz on different photometric and spectroscopic samples from the Sloan\nDigital Sky Survey (SDSS). We find that the photometric redshift performance of\nANNz2 (machine learning) is better than that of BPz (galactic templates), and\nwith the utilisation of the merging technique we introduced, we see that there\nis an improvement in photo-$z$ when the two strategies are consolidated,\nproviding improvements in $\\sigma_{RMS}$ and $\\sigma_{68}$ up to [0.0265,\n0.0222] in the LRG sample and [0.0471, 0.0471] in the Stripe-82 Sample. This\nsimple demonstration can be used for photo-$z$s of galaxies in fainter and\ndeeper sky surveys, and future work is required to prove its viability in these\nsamples.",
            "author": [
                "Ishaq Y. K. Alshuaili",
                "John Y. H. Soo",
                "Mohd Zubir Mat Jafri",
                "Yasmin Rafid"
            ],
            "link": [
                "http://dx.doi.org/10.1134/S1063773722110019",
                "http://arxiv.org/abs/2311.17341v1",
                "http://arxiv.org/pdf/2311.17341v1"
            ],
            "primary_category": "astro-ph.CO",
            "category": [
                "astro-ph.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17339v1",
            "title": "RADAP: A Robust and Adaptive Defense Against Diverse Adversarial Patches\n  on Face Recognition",
            "updated": "2023-11-29T03:37:14Z",
            "published": "2023-11-29T03:37:14Z",
            "summary": "Face recognition (FR) systems powered by deep learning have become widely\nused in various applications. However, they are vulnerable to adversarial\nattacks, especially those based on local adversarial patches that can be\nphysically applied to real-world objects. In this paper, we propose RADAP, a\nrobust and adaptive defense mechanism against diverse adversarial patches in\nboth closed-set and open-set FR systems. RADAP employs innovative techniques,\nsuch as FCutout and F-patch, which use Fourier space sampling masks to improve\nthe occlusion robustness of the FR model and the performance of the patch\nsegmenter. Moreover, we introduce an edge-aware binary cross-entropy (EBCE)\nloss function to enhance the accuracy of patch detection. We also present the\nsplit and fill (SAF) strategy, which is designed to counter the vulnerability\nof the patch segmenter to complete white-box adaptive attacks. We conduct\ncomprehensive experiments to validate the effectiveness of RADAP, which shows\nsignificant improvements in defense performance against various adversarial\npatches, while maintaining clean accuracy higher than that of the undefended\nVanilla model.",
            "author": [
                "Xiaoliang Liu",
                "Furao Shen",
                "Jian Zhao",
                "Changhai Nie"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17339v1",
                "http://arxiv.org/pdf/2311.17339v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17945v1",
            "title": "Contrastive Vision-Language Alignment Makes Efficient Instruction\n  Learner",
            "updated": "2023-11-29T03:29:46Z",
            "published": "2023-11-29T03:29:46Z",
            "summary": "We study the task of extending the large language model (LLM) into a\nvision-language instruction-following model. This task is crucial but\nchallenging since the LLM is trained on text modality only, making it hard to\neffectively digest the visual modality. To address this, existing methods\ntypically train a visual adapter to align the representation between a\npre-trained vision transformer (ViT) and the LLM by a generative image\ncaptioning loss. However, we find that the generative objective can only\nproduce weak alignment for vision and language, making the aligned\nvision-language model very hungry for the instruction fine-tuning data. In this\npaper, we propose CG-VLM that applies both Contrastive and Generative alignment\nobjectives to effectively align the representation of ViT and LLM. Different\nfrom image level and sentence level alignment in common contrastive learning\nsettings, CG-VLM aligns the image-patch level features and text-token level\nembeddings, which, however, is very hard to achieve as no explicit grounding\npatch-token relation provided in standard image captioning datasets. To address\nthis issue, we propose to maximize the averaged similarity between pooled\nimage-patch features and text-token embeddings. Extensive experiments\ndemonstrate that the proposed CG-VLM produces strong vision-language alignment\nand is an efficient instruction learner. For example, using only 10%\ninstruction tuning data, we reach 95% performance of state-of-the-art method\nLLaVA [29] on the zero-shot ScienceQA-Image benchmark.",
            "author": [
                "Lizhao Liu",
                "Xinyu Sun",
                "Tianhang Xiang",
                "Zhuangwei Zhuang",
                "Liuren Yin",
                "Mingkui Tan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17945v1",
                "http://arxiv.org/pdf/2311.17945v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17335v1",
            "title": "eMotions: A Large-Scale Dataset for Emotion Recognition in Short Videos",
            "updated": "2023-11-29T03:24:30Z",
            "published": "2023-11-29T03:24:30Z",
            "summary": "Nowadays, short videos (SVs) are essential to information acquisition and\nsharing in our life. The prevailing use of SVs to spread emotions leads to the\nnecessity of emotion recognition in SVs. Considering the lack of SVs emotion\ndata, we introduce a large-scale dataset named eMotions, comprising 27,996\nvideos. Meanwhile, we alleviate the impact of subjectivities on labeling\nquality by emphasizing better personnel allocations and multi-stage\nannotations. In addition, we provide the category-balanced and test-oriented\nvariants through targeted data sampling. Some commonly used videos (e.g.,\nfacial expressions and postures) have been well studied. However, it is still\nchallenging to understand the emotions in SVs. Since the enhanced content\ndiversity brings more distinct semantic gaps and difficulties in learning\nemotion-related features, and there exists information gaps caused by the\nemotion incompleteness under the prevalently audio-visual co-expressions. To\ntackle these problems, we present an end-to-end baseline method AV-CPNet that\nemploys the video transformer to better learn semantically relevant\nrepresentations. We further design the two-stage cross-modal fusion module to\ncomplementarily model the correlations of audio-visual features. The EP-CE\nLoss, incorporating three emotion polarities, is then applied to guide model\noptimization. Extensive experimental results on nine datasets verify the\neffectiveness of AV-CPNet. Datasets and code will be open on\nhttps://github.com/XuecWu/eMotions.",
            "author": [
                "Xuecheng Wu",
                "Heli Sun",
                "Junxiao Xue",
                "Ruofan Zhai",
                "Xiangyan Kong",
                "Jiayu Nie",
                "Liang He"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17335v1",
                "http://arxiv.org/pdf/2311.17335v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.MM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17327v1",
            "title": "Improving Self-supervised Molecular Representation Learning using\n  Persistent Homology",
            "updated": "2023-11-29T02:58:30Z",
            "published": "2023-11-29T02:58:30Z",
            "summary": "Self-supervised learning (SSL) has great potential for molecular\nrepresentation learning given the complexity of molecular graphs, the large\namounts of unlabelled data available, the considerable cost of obtaining labels\nexperimentally, and the hence often only small training datasets. The\nimportance of the topic is reflected in the variety of paradigms and\narchitectures that have been investigated recently. Yet the differences in\nperformance seem often minor and are barely understood to date. In this paper,\nwe study SSL based on persistent homology (PH), a mathematical tool for\nmodeling topological features of data that persist across multiple scales. It\nhas several unique features which particularly suit SSL, naturally offering:\ndifferent views of the data, stability in terms of distance preservation, and\nthe opportunity to flexibly incorporate domain knowledge. We (1) investigate an\nautoencoder, which shows the general representational power of PH, and (2)\npropose a contrastive loss that complements existing approaches. We rigorously\nevaluate our approach for molecular property prediction and demonstrate its\nparticular features in improving the embedding space: after SSL, the\nrepresentations are better and offer considerably more predictive power than\nthe baselines over different probing tasks; our loss increases baseline\nperformance, sometimes largely; and we often obtain substantial improvements\nover very small datasets, a common scenario in practice.",
            "author": [
                "Yuankai Luo",
                "Lei Shi",
                "Veronika Thost"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17327v1",
                "http://arxiv.org/pdf/2311.17327v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17326v1",
            "title": "Mostly Beneficial Clustering: Aggregating Data for Operational Decision\n  Making",
            "updated": "2023-11-29T02:53:32Z",
            "published": "2023-11-29T02:53:32Z",
            "summary": "With increasingly volatile market conditions and rapid product innovations,\noperational decision-making for large-scale systems entails solving thousands\nof problems with limited data. Data aggregation is proposed to combine the data\nacross problems to improve the decisions obtained by solving those problems\nindividually. We propose a novel cluster-based shrunken-SAA approach that can\nexploit the cluster structure among problems when implementing the data\naggregation approaches. We prove that, as the number of problems grows,\nleveraging the known cluster structure among problems yields additional\nbenefits over the data aggregation approaches that neglect such structure. When\nthe cluster structure is unknown, we show that unveiling the cluster structure,\neven at the cost of a few data points, can be beneficial, especially when the\ndistance between clusters of problems is substantial. Our proposed approach can\nbe extended to general cost functions under mild conditions. When the number of\nproblems gets large, the optimality gap of our proposed approach decreases\nexponentially in the distance between the clusters. We explore the performance\nof the proposed approach through the application of managing newsvendor systems\nvia numerical experiments. We investigate the impacts of distance metrics\nbetween problem instances on the performance of the cluster-based Shrunken-SAA\napproach with synthetic data. We further validate our proposed approach with\nreal data and highlight the advantages of cluster-based data aggregation,\nespecially in the small-data large-scale regime, compared to the existing\napproaches.",
            "author": [
                "Chengzhang Li",
                "Zhenkang Peng",
                "Ying Rong"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17326v1",
                "http://arxiv.org/pdf/2311.17326v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17325v1",
            "title": "Alternate Diverse Teaching for Semi-supervised Medical Image\n  Segmentation",
            "updated": "2023-11-29T02:44:54Z",
            "published": "2023-11-29T02:44:54Z",
            "summary": "Semi-supervised medical image segmentation studies have shown promise in\ntraining models with limited labeled data. However, current dominant\nteacher-student based approaches can suffer from the confirmation bias. To\naddress this challenge, we propose AD-MT, an alternate diverse teaching\napproach in a teacher-student framework. It involves a single student model and\ntwo non-trainable teacher models that are momentum-updated periodically and\nrandomly in an alternate fashion. To mitigate the confirmation bias from the\ndiverse supervision, the core of AD-MT lies in two proposed modules: the Random\nPeriodic Alternate (RPA) Updating Module and the Conflict-Combating Module\n(CCM). The RPA schedules the alternating diverse updating process with\ncomplementary data batches, distinct data augmentation, and random switching\nperiods to encourage diverse reasoning from different teaching perspectives.\nThe CCM employs an entropy-based ensembling strategy to encourage the model to\nlearn from both the consistent and conflicting predictions between the\nteachers. Experimental results demonstrate the effectiveness and superiority of\nour AD-MT on the 2D and 3D medical segmentation benchmarks across various\nsemi-supervised settings.",
            "author": [
                "Zhen Zhao",
                "Zicheng Wang",
                "Longyue Wang",
                "Yixuan Yuan",
                "Luping Zhou"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17325v1",
                "http://arxiv.org/pdf/2311.17325v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17323v1",
            "title": "Accelerating DNN Training With Photonics: A Residue Number System-Based\n  Design",
            "updated": "2023-11-29T02:40:12Z",
            "published": "2023-11-29T02:40:12Z",
            "summary": "Photonic computing is a compelling avenue for performing highly efficient\nmatrix multiplication, a crucial operation in Deep Neural Networks (DNNs).\nWhile this method has shown great success in DNN inference, meeting the high\nprecision demands of DNN training proves challenging due to the precision\nlimitations imposed by costly data converters and the analog noise inherent in\nphotonic hardware. This paper proposes Mirage, a photonic DNN training\naccelerator that overcomes the precision challenges in photonic hardware using\nthe Residue Number System (RNS). RNS is a numeral system based on modular\narithmetic$\\unicode{x2014}$allowing us to perform high-precision operations via\nmultiple low-precision modular operations. In this work, we present a novel\nmicro-architecture and dataflow for an RNS-based photonic tensor core\nperforming modular arithmetic in the analog domain. By combining RNS and\nphotonics, Mirage provides high energy efficiency without compromising\nprecision and can successfully train state-of-the-art DNNs achieving accuracy\ncomparable to FP32 training. Our study shows that on average across several\nDNNs when compared to systolic arrays, Mirage achieves more than $23.8\\times$\nfaster training and $32.1\\times$ lower EDP in an iso-energy scenario and\nconsumes $42.8\\times$ lower power with comparable or better EDP in an iso-area\nscenario.",
            "author": [
                "Cansu Demirkiran",
                "Guowei Yang",
                "Darius Bunandar",
                "Ajay Joshi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17323v1",
                "http://arxiv.org/pdf/2311.17323v1"
            ],
            "primary_category": "cs.AR",
            "category": [
                "cs.AR",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17319v1",
            "title": "Microstructure reconstruction of 2D/3D random materials via\n  diffusion-based deep generative models",
            "updated": "2023-11-29T02:20:47Z",
            "published": "2023-11-29T02:20:47Z",
            "summary": "Microstructure reconstruction serves as a crucial foundation for establishing\nProcess-Structure-Property (PSP) relationship in material design. Confronting\nthe limitations of variational autoencoder and generative adversarial network\nwithin generative modeling, this study adopted the denoising diffusion\nprobability model (DDPM) to learn the probability distribution of\nhigh-dimensional raw data and successfully reconstructed the microstructures of\nvarious composite materials, such as inclusion materials, spinodal\ndecomposition materials, chessboard materials, fractal noise materials, and so\non. The quality of generated microstructure was evaluated using quantitative\nmeasures like spatial correlation functions and Fourier descriptor. On this\nbasis, this study also successfully achieved the regulation of microstructure\nrandomness and the generation of gradient materials through continuous\ninterpolation in latent space using denoising diffusion implicit model (DDIM).\nFurthermore, the two-dimensional microstructure reconstruction is extended to\nthree-dimensional framework and integrates permeability as a feature encoding\nembedding. This enables the conditional generation of three-dimensional\nmicrostructures for random porous materials within a defined permeability\nrange. The permeabilities of these generated microstructures were further\nvalidated through the application of the Boltzmann method.",
            "author": [
                "Xianrui Lyu",
                "Xiaodan Ren"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17319v1",
                "http://arxiv.org/pdf/2311.17319v1"
            ],
            "primary_category": "cs.CE",
            "category": [
                "cs.CE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17944v1",
            "title": "LALM: Long-Term Action Anticipation with Language Models",
            "updated": "2023-11-29T02:17:27Z",
            "published": "2023-11-29T02:17:27Z",
            "summary": "Understanding human activity is a crucial yet intricate task in egocentric\nvision, a field that focuses on capturing visual perspectives from the camera\nwearer's viewpoint. While traditional methods heavily rely on representation\nlearning trained on extensive video data, there exists a significant\nlimitation: obtaining effective video representations proves challenging due to\nthe inherent complexity and variability in human activities.Furthermore,\nexclusive dependence on video-based learning may constrain a model's capability\nto generalize across long-tail classes and out-of-distribution scenarios.\n  In this study, we introduce a novel approach for long-term action\nanticipation using language models (LALM), adept at addressing the complex\nchallenges of long-term activity understanding without the need for extensive\ntraining. Our method incorporates an action recognition model to track previous\naction sequences and a vision-language model to articulate relevant\nenvironmental details. By leveraging the context provided by these past events,\nwe devise a prompting strategy for action anticipation using large language\nmodels (LLMs). Moreover, we implement Maximal Marginal Relevance for example\nselection to facilitate in-context learning of the LLMs. Our experimental\nresults demonstrate that LALM surpasses the state-of-the-art methods in the\ntask of long-term action anticipation on the Ego4D benchmark. We further\nvalidate LALM on two additional benchmarks, affirming its capacity for\ngeneralization across intricate activities with different sets of taxonomies.\nThese are achieved without specific fine-tuning.",
            "author": [
                "Sanghwan Kim",
                "Daoji Huang",
                "Yongqin Xian",
                "Otmar Hilliges",
                "Luc Van Gool",
                "Xi Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17944v1",
                "http://arxiv.org/pdf/2311.17944v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17315v2",
            "title": "Explaining CLIP's performance disparities on data from blind/low vision\n  users",
            "updated": "2023-12-01T02:25:33Z",
            "published": "2023-11-29T02:10:31Z",
            "summary": "Large multi-modal models (LMMs) hold the potential to usher in a new era of\nautomated visual assistance for people who are blind or low vision (BLV). Yet,\nthese models have not been systematically evaluated on data captured by BLV\nusers. We address this by empirically assessing CLIP, a widely-used LMM likely\nto underpin many assistive technologies. Testing 25 CLIP variants in a\nzero-shot classification task, we find that their accuracy is 15 percentage\npoints lower on average for images captured by BLV users than web-crawled\nimages. This disparity stems from CLIP's sensitivities to 1) image content\n(e.g. not recognizing disability objects as well as other objects); 2) image\nquality (e.g. not being robust to lighting variation); and 3) text content\n(e.g. not recognizing objects described by tactile adjectives as well as visual\nones). We delve deeper with a textual analysis of three common pre-training\ndatasets: LAION-400M, LAION-2B and DataComp-1B, showing that disability content\nis rarely mentioned. We then provide three examples that illustrate how the\nperformance disparities extend to three downstream models underpinned by CLIP:\nOWL-ViT, CLIPSeg and DALL-E2. We find that few-shot learning with as few as 5\nimages can mitigate CLIP's quality-of-service disparities for BLV users in some\nscenarios, which we discuss alongside a set of other possible mitigations.",
            "author": [
                "Daniela Massiceti",
                "Camilla Longden",
                "Agnieszka S\u0142owik",
                "Samuel Wills",
                "Martin Grayson",
                "Cecily Morrison"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17315v2",
                "http://arxiv.org/pdf/2311.17315v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17305v1",
            "title": "Two-Step Reinforcement Learning for Multistage Strategy Card Game",
            "updated": "2023-11-29T01:31:21Z",
            "published": "2023-11-29T01:31:21Z",
            "summary": "In the realm of artificial intelligence and card games, this study introduces\na two-step reinforcement learning (RL) strategy tailored for \"The Lord of the\nRings: The Card Game (LOTRCG),\" a complex multistage strategy card game. This\nresearch diverges from conventional RL methods by adopting a phased learning\napproach, beginning with a foundational learning stage in a simplified version\nof the game and subsequently progressing to the complete, intricate game\nenvironment. This methodology notably enhances the AI agent's adaptability and\nperformance in the face of LOTRCG's unpredictable and challenging nature. The\npaper also explores a multi-agent system, where distinct RL agents are employed\nfor various decision-making aspects of the game. This approach has demonstrated\na remarkable improvement in game outcomes, with the RL agents achieving a\nwinrate of 78.5% across a set of 10,000 random games.",
            "author": [
                "Konrad Godlewski",
                "Bartosz Sawicki"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17305v1",
                "http://arxiv.org/pdf/2311.17305v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.GT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17303v2",
            "title": "Enhancing the Performance of Neural Networks Through Causal Discovery\n  and Integration of Domain Knowledge",
            "updated": "2023-12-01T01:34:47Z",
            "published": "2023-11-29T01:25:00Z",
            "summary": "In this paper, we develop a generic methodology to encode hierarchical\ncausality structure among observed variables into a neural network in order to\nimprove its predictive performance. The proposed methodology, called\ncausality-informed neural network (CINN), leverages three coherent steps to\nsystematically map the structural causal knowledge into the layer-to-layer\ndesign of neural network while strictly preserving the orientation of every\ncausal relationship. In the first step, CINN discovers causal relationships\nfrom observational data via directed acyclic graph (DAG) learning, where causal\ndiscovery is recast as a continuous optimization problem to avoid the\ncombinatorial nature. In the second step, the discovered hierarchical causality\nstructure among observed variables is systematically encoded into neural\nnetwork through a dedicated architecture and customized loss function. By\ncategorizing variables in the causal DAG as root, intermediate, and leaf nodes,\nthe hierarchical causal DAG is translated into CINN with a one-to-one\ncorrespondence between nodes in the causal DAG and units in the CINN while\nmaintaining the relative order among these nodes. Regarding the loss function,\nboth intermediate and leaf nodes in the DAG graph are treated as target outputs\nduring CINN training so as to drive co-learning of causal relationships among\ndifferent types of nodes. As multiple loss components emerge in CINN, we\nleverage the projection of conflicting gradients to mitigate gradient\ninterference among the multiple learning tasks. Computational experiments\nacross a broad spectrum of UCI data sets demonstrate substantial advantages of\nCINN in predictive performance over other state-of-the-art methods. In\naddition, an ablation study underscores the value of integrating structural and\nquantitative causal knowledge in enhancing the neural network's predictive\nperformance incrementally.",
            "author": [
                "Xiaoge Zhang",
                "Xiao-Lin Wang",
                "Fenglei Fan",
                "Yiu-Ming Cheung",
                "Indranil Bose"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17303v2",
                "http://arxiv.org/pdf/2311.17303v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "stat.ME"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17943v1",
            "title": "LayerCollapse: Adaptive compression of neural networks",
            "updated": "2023-11-29T01:23:41Z",
            "published": "2023-11-29T01:23:41Z",
            "summary": "Handling the ever-increasing scale of contemporary deep learning and\ntransformer-based models poses a significant challenge. Although great strides\nhave been made in optimizing model compression techniques such as model\narchitecture search and knowledge distillation, the availability of data and\ncomputational resources remains a considerable hurdle for these optimizations.\nThis paper introduces LayerCollapse, a novel alternative adaptive model\ncompression methodology. LayerCollapse works by eliminating non-linearities\nwithin the network and collapsing two consecutive fully connected layers into a\nsingle linear transformation. This approach simultaneously reduces both the\nnumber of layers and the parameter count, thereby enhancing model efficiency.\nWe also introduce a compression aware regularizer, which compresses the model\nin alignment with the dataset quality and model expressiveness, consequently\nreducing overfitting across tasks. Our results demonstrate LayerCollapse's\neffective compression and regularization capabilities in multiple fine-grained\nclassification benchmarks, achieving up to 74% post training compression with\nminimal accuracy loss. We compare this method with knowledge distillation on\nthe same target network, showcasing a five-fold increase in computational\nefficiency and 8% improvement in overall accuracy on the ImageNet dataset.",
            "author": [
                "Soheil Zibakhsh Shabgahi",
                "Mohammad Soheil Shariff",
                "Farinaz Koushanfar"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17943v1",
                "http://arxiv.org/pdf/2311.17943v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17302v1",
            "title": "Peer interaction facilitates co-construction of knowledge in quantum\n  mechanics",
            "updated": "2023-11-29T01:19:50Z",
            "published": "2023-11-29T01:19:50Z",
            "summary": "Collaborative learning with peers can lead to students learning from each\nother and solving physics problems correctly not only in situations in which\none student knows how to solve the problems but also when none of the students\ncan solve the problems alone. We define the rate of construction as the\npercentage of groups collaborating on problem-solving that solve the problem\ncorrectly out of all groups having at least one member who answered correctly\nand one incorrectly while solving the same problem individually first. We\ndefine the rate of co-construction on each problem as the percentage of\ncollaborating groups that answered it correctly if no student in the group\nindividually answered it correctly before the collaborative work. In this\nstudy, we investigated student learning measured by student performance on a\nvalidated quantum mechanics survey and rates of construction and\nco-construction of knowledge when students first worked individually after\nlecture-based instruction in relevant concepts and then worked with peers\nduring class without receiving any feedback from the course instructor. We find\nthat construction of knowledge consistently occurred at a high rate during peer\ncollaboration. However, rates of co-construction were more varied. High rates\nof co-construction were generally achieved when approximately half of the\nstudents knew the correct answers initially. We also conducted an analysis of\nsome of the survey questions that correlate with high rates of co-construction\nto gain some insight into what students converged on after peer interaction and\nwhat types of difficulties were reduced. Our findings can be valuable for\ninstructors who want to provide in-class and out-of-class opportunities for\npeer collaboration in their physics courses.",
            "author": [
                "Mary Jane Brundage",
                "Alysa Malespina",
                "Chandralekha Singh"
            ],
            "link": [
                "http://dx.doi.org/10.1103/PhysRevPhysEducRes.19.020133",
                "http://arxiv.org/abs/2311.17302v1",
                "http://arxiv.org/pdf/2311.17302v1"
            ],
            "primary_category": "physics.ed-ph",
            "category": [
                "physics.ed-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17301v1",
            "title": "Language Models: A Guide for the Perplexed",
            "updated": "2023-11-29T01:19:02Z",
            "published": "2023-11-29T01:19:02Z",
            "summary": "Given the growing importance of AI literacy, we decided to write this\ntutorial to help narrow the gap between the discourse among those who study\nlanguage models -- the core technology underlying ChatGPT and similar products\n-- and those who are intrigued and want to learn more about them. In short, we\nbelieve the perspective of researchers and educators can add some clarity to\nthe public's understanding of the technologies beyond what's currently\navailable, which tends to be either extremely technical or promotional material\ngenerated about products by their purveyors.\n  Our approach teases apart the concept of a language model from products built\non them, from the behaviors attributed to or desired from those products, and\nfrom claims about similarity to human cognition. As a starting point, we (1)\noffer a scientific viewpoint that focuses on questions amenable to study\nthrough experimentation; (2) situate language models as they are today in the\ncontext of the research that led to their development; and (3) describe the\nboundaries of what is known about the models at this writing.",
            "author": [
                "Sofia Serrano",
                "Zander Brumbaugh",
                "Noah A. Smith"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17301v1",
                "http://arxiv.org/pdf/2311.17301v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.CY",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17942v1",
            "title": "Object-based (yet Class-agnostic) Video Domain Adaptation",
            "updated": "2023-11-29T01:17:38Z",
            "published": "2023-11-29T01:17:38Z",
            "summary": "Existing video-based action recognition systems typically require dense\nannotation and struggle in environments when there is significant distribution\nshift relative to the training data. Current methods for video domain\nadaptation typically fine-tune the model using fully annotated data on a subset\nof target domain data or align the representation of the two domains using\nbootstrapping or adversarial learning. Inspired by the pivotal role of objects\nin recent supervised object-centric action recognition models, we present\nObject-based (yet Class-agnostic) Video Domain Adaptation (ODAPT), a simple yet\neffective framework for adapting the existing action recognition systems to new\ndomains by utilizing a sparse set of frames with class-agnostic object\nannotations in a target domain. Our model achieves a +6.5 increase when\nadapting across kitchens in Epic-Kitchens and a +3.1 increase adapting between\nEpic-Kitchens and the EGTEA dataset. ODAPT is a general framework that can also\nbe combined with previous unsupervised methods, offering a +5.0 boost when\ncombined with the self-supervised multi-modal method MMSADA and a +1.7 boost\nwhen added to the adversarial-based method TA$^3$N on Epic-Kitchens.",
            "author": [
                "Dantong Niu",
                "Amir Bar",
                "Roei Herzig",
                "Trevor Darrell",
                "Anna Rohrbach"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17942v1",
                "http://arxiv.org/pdf/2311.17942v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17299v1",
            "title": "Federated Fine-Tuning of Foundation Models via Probabilistic Masking",
            "updated": "2023-11-29T01:10:39Z",
            "published": "2023-11-29T01:10:39Z",
            "summary": "Foundation Models (FMs) have revolutionized machine learning with their\nadaptability and high performance across tasks; yet, their integration into\nFederated Learning (FL) is challenging due to substantial communication\noverhead from their extensive parameterization. Current communication-efficient\nFL strategies, such as gradient compression, reduce bitrates to around $1$\nbit-per-parameter (bpp). However, these approaches fail to harness the\ncharacteristics of FMs, with their large number of parameters still posing a\nchallenge to communication efficiency, even at these bitrate regimes. In this\nwork, we present DeltaMask, a novel method that efficiently fine-tunes FMs in\nFL at an ultra-low bitrate, well below 1 bpp. DeltaMask employs stochastic\nmasking to detect highly effective subnetworks within FMs and leverage\nstochasticity and sparsity in client masks to compress updates into a compact\ngrayscale image using probabilistic filters, deviating from traditional weight\ntraining approaches. Our comprehensive evaluations across various datasets and\narchitectures demonstrate DeltaMask efficiently achieves bitrates as low as\n0.09 bpp, enhancing communication efficiency while maintaining FMs performance,\nas measured on 8 datasets and 5 pre-trained models of various network\narchitectures.",
            "author": [
                "Vasileios Tsouvalas",
                "Yuki Asano",
                "Aaqib Saeed"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17299v1",
                "http://arxiv.org/pdf/2311.17299v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17297v1",
            "title": "Stability control for USVs with SINDY-based online dynamic model update",
            "updated": "2023-11-29T00:48:25Z",
            "published": "2023-11-29T00:48:25Z",
            "summary": "Unmanned Surface Vehicles (USVs) play a pivotal role in various applications,\nincluding surface rescue, commercial transactions, scientific exploration,\nwater rescue, and military operations. The effective control of high-speed\nunmanned surface boats stands as a critical aspect within the overall USV\nsystem, particularly in challenging environments marked by complex surface\nobstacles and dynamic conditions, such as time-varying surges, non-directional\nforces, and unpredictable winds. In this paper, we propose a data-driven\ncontrol method based on Koopman theory. This involves constructing a\nhigh-dimensional linear model by mapping a low-dimensional nonlinear model to a\nhigher-dimensional linear space through data identification. The observable\nUSVs dynamical system is dynamically reconstructed using online error learning.\nTo enhance tracking control accuracy, we utilize a Constructive Lyapunov\nFunction (CLF)-Control Barrier Function (CBF)-Quadratic Programming (QP)\napproach to regulate the high-dimensional linear dynamical system obtained\nthrough identification. This approach facilitates error compensation, thereby\nachieving more precise tracking control.",
            "author": [
                "Zong Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17297v1",
                "http://arxiv.org/pdf/2311.17297v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17287v1",
            "title": "Utilizing Model Residuals to Identify Rental Properties of Interest: The\n  Price Anomaly Score (PAS) and Its Application to Real-time Data in Manhattan",
            "updated": "2023-11-29T00:14:30Z",
            "published": "2023-11-29T00:14:30Z",
            "summary": "Understanding whether a property is priced fairly hinders buyers and sellers\nsince they usually do not have an objective viewpoint of the price distribution\nfor the overall market of their interest. Drawing from data collected of all\npossible available properties for rent in Manhattan as of September 2023, this\npaper aims to strengthen our understanding of model residuals; specifically on\nmachine learning models which generalize for a majority of the distribution of\na well-proportioned dataset. Most models generally perceive deviations from\npredicted values as mere inaccuracies, however this paper proposes a different\nvantage point: when generalizing to at least 75\\% of the data-set, the\nremaining deviations reveal significant insights. To harness these insights, we\nintroduce the Price Anomaly Score (PAS), a metric capable of capturing\nboundaries between irregularly predicted prices. By combining relative pricing\ndiscrepancies with statistical significance, the Price Anomaly Score (PAS)\noffers a multifaceted view of rental valuations. This metric allows experts to\nidentify overpriced or underpriced properties within a dataset by aggregating\nPAS values, then fine-tuning upper and lower boundaries to any threshold to set\nindicators of choice.",
            "author": [
                "Youssef Sultan",
                "Jackson C. Rafter",
                "Huyen T. Nguyen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17287v1",
                "http://arxiv.org/pdf/2311.17287v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00054v1",
            "title": "Is Inverse Reinforcement Learning Harder than Standard Reinforcement\n  Learning?",
            "updated": "2023-11-29T00:09:01Z",
            "published": "2023-11-29T00:09:01Z",
            "summary": "Inverse Reinforcement Learning (IRL) -- the problem of learning reward\nfunctions from demonstrations of an \\emph{expert policy} -- plays a critical\nrole in developing intelligent systems, such as those that understand and\nimitate human behavior. While widely used in applications, theoretical\nunderstandings of IRL admit unique challenges and remain less developed\ncompared with standard RL theory. For example, it remains open how to do IRL\nefficiently in standard \\emph{offline} settings with pre-collected data, where\nstates are obtained from a \\emph{behavior policy} (which could be the expert\npolicy itself), and actions are sampled from the expert policy.\n  This paper provides the first line of results for efficient IRL in vanilla\noffline and online settings using polynomial samples and runtime. We first\ndesign a new IRL algorithm for the offline setting, Reward Learning with\nPessimism (RLP), and show that it achieves polynomial sample complexity in\nterms of the size of the MDP, a concentrability coefficient between the\nbehavior policy and the expert policy, and the desired accuracy. Building on\nRLP, we further design an algorithm Reward Learning with Exploration (RLE),\nwhich operates in a natural online setting where the learner can both actively\nexplore the environment and query the expert policy, and obtain a stronger\nnotion of IRL guarantee from polynomial samples. We establish sample complexity\nlower bounds for both settings showing that RLP and RLE are nearly optimal.\nFinally, as an application, we show that the learned reward functions can\n\\emph{transfer} to another target MDP with suitable guarantees when the target\nMDP satisfies certain similarity assumptions with the original (source) MDP.",
            "author": [
                "Lei Zhao",
                "Mengdi Wang",
                "Yu Bai"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00054v1",
                "http://arxiv.org/pdf/2312.00054v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17279v1",
            "title": "LiveTune: Dynamic Parameter Tuning for Training Deep Neural Networks",
            "updated": "2023-11-28T23:38:42Z",
            "published": "2023-11-28T23:38:42Z",
            "summary": "Traditional machine learning training is a static process that lacks\nreal-time adaptability of hyperparameters. Popular tuning solutions during\nruntime involve checkpoints and schedulers. Adjusting hyper-parameters usually\nrequire the program to be restarted, wasting utilization and time, while\nplacing unnecessary strain on memory and processors. We present LiveTune, a new\nframework allowing real-time parameter tuning during training through\nLiveVariables. Live Variables allow for a continuous training session by\nstoring parameters on designated ports on the system, allowing them to be\ndynamically adjusted. Extensive evaluations of our framework show saving up to\n60 seconds and 5.4 Kilojoules of energy per hyperparameter change.",
            "author": [
                "Soheil Zibakhsh Shabgahi",
                "Nojan Sheybani",
                "Aiden Tabrizi",
                "Farinaz Koushanfar"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17279v1",
                "http://arxiv.org/pdf/2311.17279v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.PF"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17277v1",
            "title": "An Online Optimization-Based Decision Support Tool for Small Farmers in\n  India: Learning in Non-stationary Environments",
            "updated": "2023-11-28T23:33:16Z",
            "published": "2023-11-28T23:33:16Z",
            "summary": "Crop management decision support systems are specialized tools for farmers\nthat reduce the riskiness of revenue streams, especially valuable for use under\nthe current climate changes that impact agricultural productivity.\nUnfortunately, small farmers in India, who could greatly benefit from these\ntools, do not have access to them. In this paper, we model an individual\ngreenhouse as a Markov Decision Process (MDP) and adapt Li and Li (2019)'s\nFollow the Weighted Leader (FWL) online learning algorithm to offer crop\nplanning advice. We successfully produce utility-preserving cropping pattern\nsuggestions in simulations. When we compare against an offline planning\nalgorithm, we achieve the same cumulative revenue with greatly reduced runtime.",
            "author": [
                "Tuxun Lu",
                "Aviva Prins"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17277v1",
                "http://arxiv.org/pdf/2311.17277v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17276v1",
            "title": "Machine Unlearning in Learned Databases: An Experimental Analysis",
            "updated": "2023-11-28T23:32:42Z",
            "published": "2023-11-28T23:32:42Z",
            "summary": "Machine learning models based on neural networks (NNs) are enjoying\never-increasing attention in the DB community. However, an important issue has\nbeen largely overlooked, namely the challenge of dealing with the highly\ndynamic nature of DBs, where data updates are fundamental, highly-frequent\noperations. Although some recent research has addressed the issues of\nmaintaining updated NN models in the presence of new data insertions, the\neffects of data deletions (a.k.a., \"machine unlearning\") remain a blind spot.\nWith this work, for the first time to our knowledge, we pose and answer the\nfollowing key questions: What is the effect of unlearning algorithms on\nNN-based DB models? How do these effects translate to effects on downstream DB\ntasks, such as selectivity estimation (SE), approximate query processing (AQP),\ndata generation (DG), and upstream tasks like data classification (DC)? What\nmetrics should we use to assess the impact and efficacy of unlearning\nalgorithms in learned DBs? Is the problem of machine unlearning in DBs\ndifferent from that of machine learning in DBs in the face of data insertions?\nIs the problem of machine unlearning for DBs different from unlearning in the\nML literature? what are the overhead and efficiency of unlearning algorithms?\nWhat is the sensitivity of unlearning on batching delete operations? If we have\na suitable unlearning algorithm, can we combine it with an algorithm handling\ndata insertions en route to solving the general adaptability/updatability\nrequirement in learned DBs in the face of both data inserts and deletes? We\nanswer these questions using a comprehensive set of experiments, various\nunlearning algorithms, a variety of downstream DB tasks, and an upstream task\n(DC), each with different NNs, and using a variety of metrics on a variety of\nreal datasets, making this also a first key step towards a benchmark for\nlearned DB unlearning.",
            "author": [
                "Meghdad Kurmanji",
                "Eleni Triantafillou",
                "Peter Triantafillou"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17276v1",
                "http://arxiv.org/pdf/2311.17276v1"
            ],
            "primary_category": "cs.DB",
            "category": [
                "cs.DB"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17941v1",
            "title": "Advancing Attack-Resilient Scheduling of Integrated Energy Systems with\n  Demand Response via Deep Reinforcement Learning",
            "updated": "2023-11-28T23:29:36Z",
            "published": "2023-11-28T23:29:36Z",
            "summary": "Optimally scheduling multi-energy flow is an effective method to utilize\nrenewable energy sources (RES) and improve the stability and economy of\nintegrated energy systems (IES). However, the stable demand-supply of IES faces\nchallenges from uncertainties that arise from RES and loads, as well as the\nincreasing impact of cyber-attacks with advanced information and communication\ntechnologies adoption. To address these challenges, this paper proposes an\ninnovative model-free resilience scheduling method based on state-adversarial\ndeep reinforcement learning (DRL) for integrated demand response (IDR)-enabled\nIES. The proposed method designs an IDR program to explore the interaction\nability of electricity-gas-heat flexible loads. Additionally, a\nstate-adversarial Markov decision process (SA-MDP) model characterizes the\nenergy scheduling problem of IES under cyber-attack. The state-adversarial soft\nactor-critic (SA-SAC) algorithm is proposed to mitigate the impact of\ncyber-attacks on the scheduling strategy. Simulation results demonstrate that\nour method is capable of adequately addressing the uncertainties resulting from\nRES and loads, mitigating the impact of cyber-attacks on the scheduling\nstrategy, and ensuring a stable demand supply for various energy sources.\nMoreover, the proposed method demonstrates resilience against cyber-attacks.\nCompared to the original soft actor-critic (SAC) algorithm, it achieves a 10\\%\nimprovement in economic performance under cyber-attack scenarios.",
            "author": [
                "Yang Li",
                "Wenjie Ma",
                "Yuanzheng Li",
                "Sen Li",
                "Zhe Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17941v1",
                "http://arxiv.org/pdf/2311.17941v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.LG",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17267v1",
            "title": "E-ViLM: Efficient Video-Language Model via Masked Video Modeling with\n  Semantic Vector-Quantized Tokenizer",
            "updated": "2023-11-28T22:57:17Z",
            "published": "2023-11-28T22:57:17Z",
            "summary": "To build scalable models for challenging real-world tasks, it is important to\nlearn from diverse, multi-modal data in various forms (e.g., videos, text, and\nimages). Among the existing works, a plethora of them have focused on\nleveraging large but cumbersome cross-modal architectures. Regardless of their\neffectiveness, larger architectures unavoidably prevent the models from being\nextended to real-world applications, so building a lightweight VL architecture\nand an efficient learning schema is of great practical value. In this paper, we\npropose an Efficient Video-Language Model (dubbed as E-ViLM) and a masked video\nmodeling (MVM) schema, assisted with a semantic vector-quantized tokenizer. In\nparticular, our E-ViLM learns to reconstruct the semantic labels of masked\nvideo regions, produced by the pre-trained vector-quantized tokenizer, which\ndiscretizes the continuous visual signals into labels. We show that with our\nsimple MVM task and regular VL pre-training modelings, our E-ViLM, despite its\ncompactness, is able to learn expressive representations from Video-Language\ncorpus and generalize well to extensive Video-Language tasks including video\nquestion answering, text-to-video retrieval, etc. In particular, our E-ViLM\nobtains obvious efficiency improvements by reaching competing performances with\nfaster inference speed, i.e., our model reaches $39.3$% Top-$1$ accuracy on the\nMSRVTT benchmark, retaining $91.4$% of the accuracy of state-of-the-art larger\nVL architecture with only $15%$ parameters and $94.8%$ fewer GFLOPs. We also\nprovide extensive ablative studies that validate the effectiveness of our\nproposed learning schema for E-ViLM.",
            "author": [
                "Jacob Zhiyuan Fang",
                "Skyler Zheng",
                "Vasu Sharma",
                "Robinson Piramuthu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17267v1",
                "http://arxiv.org/pdf/2311.17267v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17265v1",
            "title": "Exceptional Mechanical Performance by Spatial Printing with Continuous\n  Fiber",
            "updated": "2023-11-28T22:54:54Z",
            "published": "2023-11-28T22:54:54Z",
            "summary": "This work explores a spatial printing method to fabricate continuous\nfiber-reinforced thermoplastic composites (CFRTPCs), which can achieve\nexceptional mechanical performance. For models giving complex 3D stress\ndistribution under loads, typical planar-layer based fiber placement usually\nfails to provide sufficient reinforcement due to their orientations being\nconstrained to planes. The effectiveness of fiber reinforcement could be\nmaximized by using multi-axis additive manufacturing (MAAM) to better control\nthe orientation of continuous fibers in 3D-printed composites. Here, we propose\na computational approach to generate 3D toolpaths that satisfy two major\nreinforcement objectives: 1) following the maximal stress directions in\ncritical regions and 2) connecting multiple load-bearing regions by continuous\nfibers. Principal stress lines are first extracted in an input solid model to\nidentify critical regions. Curved layers aligned with maximal stresses in these\ncritical regions are generated by computing an optimized scalar field and\nextracting its iso-surfaces. Then, topological analysis and operations are\napplied to each curved layer to generate a computational domain that preserves\nfiber continuity between load-bearing regions. Lastly, continuous fiber\ntoolpaths aligned with maximal stresses are generated on each surface layer by\ncomputing an optimized scalar field and extracting its iso-curves. A hardware\nsystem with dual robotic arms is employed to conduct the physical MAAM tasks\ndepositing polymer or fiber reinforced polymer composite materials by applying\na force normal to the extrusion plane to aid consolidation. When comparing to\nplanar-layer based printing results in tension, up to 644% breaking forces and\n240% stiffness are observed on shapes fabricated by our spatial printing\nmethod.",
            "author": [
                "Guoxin Fang",
                "Tianyu Zhang",
                "Yuming Huang",
                "Zhizhou Zhang",
                "Kunal Masania",
                "Charlie C. L. Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17265v1",
                "http://arxiv.org/pdf/2311.17265v1"
            ],
            "primary_category": "cs.CG",
            "category": [
                "cs.CG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17264v1",
            "title": "RETSim: Resilient and Efficient Text Similarity",
            "updated": "2023-11-28T22:54:33Z",
            "published": "2023-11-28T22:54:33Z",
            "summary": "This paper introduces RETSim (Resilient and Efficient Text Similarity), a\nlightweight, multilingual deep learning model trained to produce robust metric\nembeddings for near-duplicate text retrieval, clustering, and dataset\ndeduplication tasks. We demonstrate that RETSim is significantly more robust\nand accurate than MinHash and neural text embeddings, achieving new\nstate-of-the-art performance on dataset deduplication, adversarial text\nretrieval benchmarks, and spam clustering tasks. We also introduce the W4NT3D\nbenchmark (Wiki-40B 4dversarial Near-T3xt Dataset) for evaluating multilingual,\nnear-duplicate text retrieval capabilities under adversarial settings. RETSim\nand the W4NT3D benchmark are open-sourced under the MIT License at\nhttps://github.com/google/unisim.",
            "author": [
                "Marina Zhang",
                "Owen Vallis",
                "Aysegul Bumin",
                "Tanay Vakharia",
                "Elie Bursztein"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17264v1",
                "http://arxiv.org/pdf/2311.17264v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17259v2",
            "title": "SoUnD Framework: Analyzing (So)cial Representation in (Un)structured\n  (D)ata",
            "updated": "2023-12-01T18:41:59Z",
            "published": "2023-11-28T22:48:00Z",
            "summary": "The unstructured nature of data used in foundation model development is a\nchallenge to systematic analyses for making data use and documentation\ndecisions. From a Responsible AI perspective, these decisions often rely upon\nunderstanding how people are represented in data. We propose a framework\ndesigned to guide analysis of human representation in unstructured data and\nidentify downstream risks. We apply the framework in two toy examples using the\nCommon Crawl web text corpus (C4) and LAION-400M. We also propose a set of\nhypothetical action steps in service of dataset use, development, and\ndocumentation.",
            "author": [
                "Mark D\u00edaz",
                "Sunipa Dev",
                "Emily Reif",
                "Emily Denton",
                "Vinodkumar Prabhakaran"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17259v2",
                "http://arxiv.org/pdf/2311.17259v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17252v1",
            "title": "Analyzing the Impact of Tax Credits on Households in Simulated Economic\n  Systems with Learning Agents",
            "updated": "2023-11-28T22:25:02Z",
            "published": "2023-11-28T22:25:02Z",
            "summary": "In economic modeling, there has been an increasing investigation into\nmulti-agent simulators. Nevertheless, state-of-the-art studies establish the\nmodel based on reinforcement learning (RL) exclusively for specific agent\ncategories, e.g., households, firms, or the government. It lacks concerns over\nthe resulting adaptation of other pivotal agents, thereby disregarding the\ncomplex interactions within a real-world economic system. Furthermore, we pay\nattention to the vital role of the government policy in distributing tax\ncredits. Instead of uniform distribution considered in state-of-the-art, it\nrequires a well-designed strategy to reduce disparities among households and\nimprove social welfare. To address these limitations, we propose an expansive\nmulti-agent economic model comprising reinforcement learning agents of numerous\ntypes. Additionally, our research comprehensively explores the impact of tax\ncredit allocation on household behavior and captures the spectrum of spending\npatterns that can be observed across diverse households. Further, we propose an\ninnovative government policy to distribute tax credits, strategically\nleveraging insights from tax credit spending patterns. Simulation results\nillustrate the efficacy of the proposed government strategy in ameliorating\ninequalities across households.",
            "author": [
                "Jialin Dong",
                "Kshama Dwarakanath",
                "Svitlana Vyetrenko"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17252v1",
                "http://arxiv.org/pdf/2311.17252v1"
            ],
            "primary_category": "cs.MA",
            "category": [
                "cs.MA",
                "econ.GN",
                "q-fin.EC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17251v1",
            "title": "SubZero: Subspace Zero-Shot MRI Reconstruction",
            "updated": "2023-11-28T22:16:51Z",
            "published": "2023-11-28T22:16:51Z",
            "summary": "Recently introduced zero-shot self-supervised learning (ZS-SSL) has shown\npotential in accelerated MRI in a scan-specific scenario, which enabled\nhigh-quality reconstructions without access to a large training dataset. ZS-SSL\nhas been further combined with the subspace model to accelerate 2D T2-shuffling\nacquisitions. In this work, we propose a parallel network framework and\nintroduce an attention mechanism to improve subspace-based zero-shot\nself-supervised learning and enable higher acceleration factors. We name our\nmethod SubZero and demonstrate that it can achieve improved performance\ncompared with current methods in T1 and T2 mapping acquisitions.",
            "author": [
                "Heng Yu",
                "Yamin Arefeen",
                "Berkin Bilgic"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17251v1",
                "http://arxiv.org/pdf/2311.17251v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02993v1",
            "title": "ZTCloudGuard: Zero Trust Context-Aware Access Management Framework to\n  Avoid Misuse Cases in the Era of Generative AI and Cloud-based Health\n  Information Ecosystem",
            "updated": "2023-11-28T22:12:07Z",
            "published": "2023-11-28T22:12:07Z",
            "summary": "Managing access between large numbers of distributed medical devices has\nbecome a crucial aspect of modern healthcare systems, enabling the\nestablishment of smart hospitals and telehealth infrastructure. However, as\ntelehealth technology continues to evolve and Internet of Things (IoT) devices\nbecome more widely used, they are also becoming increasingly exposed to various\ntypes of vulnerabilities and medical errors. In healthcare information systems,\nabout 90\\% of vulnerabilities emerged from misuse cases and human errors. As a\nresult, there is a need for additional research and development of security\ntools to prevent such attacks. This article proposes a zero-trust-based\ncontext-aware framework for managing access to the main components of the cloud\necosystem, including users, devices and output data. The main goal and benefit\nof the proposed framework is to build a scoring system to prevent or alleviate\nmisuse cases while using distributed medical devices in cloud-based healthcare\ninformation systems. The framework has two main scoring schemas to maintain the\nchain of trust. First, it proposes a critical trust score based on cloud-native\nmicro-services of authentication, encryption, logging, and authorizations.\nSecond, creating a bond trust scoring to assess the real-time semantic and\nsyntactic analysis of attributes stored in a healthcare information system. The\nanalysis is based on a pre-trained machine learning model to generate the\nsemantic and syntactic scores. The framework also takes into account regulatory\ncompliance and user consent to create a scoring system. The advantage of this\nmethod is that it is applicable to any language and adapts to all attributes as\nit relies on a language model, not just a set of predefined and limited\nattributes. The results show a high F1 score of 93.5%, which proves that it is\nvalid for detecting misuse cases.",
            "author": [
                "Khalid Al-hammuri",
                "Fayez Gebali",
                "Awos Kanan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02993v1",
                "http://arxiv.org/pdf/2312.02993v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17250v1",
            "title": "Fourier Neural Differential Equations for learning Quantum Field\n  Theories",
            "updated": "2023-11-28T22:11:15Z",
            "published": "2023-11-28T22:11:15Z",
            "summary": "A Quantum Field Theory is defined by its interaction Hamiltonian, and linked\nto experimental data by the scattering matrix. The scattering matrix is\ncalculated as a perturbative series, and represented succinctly as a first\norder differential equation in time. Neural Differential Equations (NDEs) learn\nthe time derivative of a residual network's hidden state, and have proven\nefficacy in learning differential equations with physical constraints. Hence\nusing an NDE to learn particle scattering matrices presents a possible\nexperiment-theory phenomenological connection. In this paper, NDE models are\nused to learn $\\phi^4$ theory, Scalar-Yukawa theory and Scalar Quantum\nElectrodynamics. A new NDE architecture is also introduced, the Fourier Neural\nDifferential Equation (FNDE), which combines NDE integration and Fourier\nnetwork convolution. The FNDE model demonstrates better generalisability than\nthe non-integrated equivalent FNO model. It is also shown that by training on\nscattering data, the interaction Hamiltonian of a theory can be extracted from\nnetwork parameters.",
            "author": [
                "Isaac Brant",
                "Alexander Norcliffe",
                "Pietro Li\u00f2"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17250v1",
                "http://arxiv.org/pdf/2311.17250v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "hep-ph",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17248v1",
            "title": "Deep Regularized Compound Gaussian Network for Solving Linear Inverse\n  Problems",
            "updated": "2023-11-28T21:53:57Z",
            "published": "2023-11-28T21:53:57Z",
            "summary": "Incorporating prior information into inverse problems, e.g. via\nmaximum-a-posteriori estimation, is an important technique for facilitating\nrobust inverse problem solutions. In this paper, we devise two novel approaches\nfor linear inverse problems that permit problem-specific statistical prior\nselections within the compound Gaussian (CG) class of distributions. The CG\nclass subsumes many commonly used priors in signal and image reconstruction\nmethods including those of sparsity-based approaches. The first method\ndeveloped is an iterative algorithm, called generalized compound Gaussian least\nsquares (G-CG-LS), that minimizes a regularized least squares objective\nfunction where the regularization enforces a CG prior. G-CG-LS is then\nunrolled, or unfolded, to furnish our second method, which is a novel deep\nregularized (DR) neural network, called DR-CG-Net, that learns the prior\ninformation. A detailed computational theory on convergence properties of\nG-CG-LS and thorough numerical experiments for DR-CG-Net are provided. Due to\nthe comprehensive nature of the CG prior, these experiments show that our\nunrolled DR-CG-Net outperforms competitive prior art methods in tomographic\nimaging and compressive sensing, especially in challenging low-training\nscenarios.",
            "author": [
                "Carter Lyons",
                "Raghu G. Raj",
                "Margaret Cheney"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17248v1",
                "http://arxiv.org/pdf/2311.17248v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP",
                "cs.AI",
                "cs.NA",
                "math.NA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17241v1",
            "title": "End-to-End Temporal Action Detection with 1B Parameters Across 1000\n  Frames",
            "updated": "2023-11-28T21:31:04Z",
            "published": "2023-11-28T21:31:04Z",
            "summary": "Recently, temporal action detection (TAD) has seen significant performance\nimprovement with end-to-end training. However, due to the memory bottleneck,\nonly models with limited scales and limited data volumes can afford end-to-end\ntraining, which inevitably restricts TAD performance. In this paper, we reduce\nthe memory consumption for end-to-end training, and manage to scale up the TAD\nbackbone to 1 billion parameters and the input video to 1,536 frames, leading\nto significant detection performance. The key to our approach lies in our\nproposed temporal-informative adapter (TIA), which is a novel lightweight\nmodule that reduces training memory. Using TIA, we free the humongous backbone\nfrom learning to adapt to the TAD task by only updating the parameters in TIA.\nTIA also leads to better TAD representation by temporally aggregating context\nfrom adjacent frames throughout the backbone. We evaluate our model across four\nrepresentative datasets. Owing to our efficient design, we are able to train\nend-to-end on VideoMAEv2-giant and achieve 75.4% mAP on THUMOS14, being the\nfirst end-to-end model to outperform the best feature-based methods.",
            "author": [
                "Shuming Liu",
                "Chen-Lin Zhang",
                "Chen Zhao",
                "Bernard Ghanem"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17241v1",
                "http://arxiv.org/pdf/2311.17241v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17238v1",
            "title": "Domain Adaptation for Measurements of Strong Gravitational Lenses",
            "updated": "2023-11-28T21:21:28Z",
            "published": "2023-11-28T21:21:28Z",
            "summary": "Upcoming surveys are predicted to discover galaxy-scale strong lenses on the\norder of $10^5$, making deep learning methods necessary in lensing data\nanalysis. Currently, there is insufficient real lensing data to train deep\nlearning algorithms, but the alternative of training only on simulated data\nresults in poor performance on real data. Domain Adaptation may be able to\nbridge the gap between simulated and real datasets. We utilize domain\nadaptation for the estimation of Einstein radius ($\\Theta_E$) in simulated\ngalaxy-scale gravitational lensing images with different levels of\nobservational realism. We evaluate two domain adaptation techniques - Domain\nAdversarial Neural Networks (DANN) and Maximum Mean Discrepancy (MMD). We train\non a source domain of simulated lenses and apply it to a target domain of\nlenses simulated to emulate noise conditions in the Dark Energy Survey (DES).\nWe show that both domain adaptation techniques can significantly improve the\nmodel performance on the more complex target domain dataset. This work is the\nfirst application of domain adaptation for a regression task in strong lensing\nimaging analysis. Our results show the potential of using domain adaptation to\nperform analysis of future survey data with a deep neural network trained on\nsimulated data.",
            "author": [
                "Paxson Swierc",
                "Megan Zhao",
                "Aleksandra \u0106iprijanovi\u0107",
                "Brian Nord"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17238v1",
                "http://arxiv.org/pdf/2311.17238v1"
            ],
            "primary_category": "astro-ph.IM",
            "category": [
                "astro-ph.IM",
                "astro-ph.CO",
                "astro-ph.GA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17233v1",
            "title": "Quantifying the redundancy between prosody and text",
            "updated": "2023-11-28T21:15:24Z",
            "published": "2023-11-28T21:15:24Z",
            "summary": "Prosody -- the suprasegmental component of speech, including pitch, loudness,\nand tempo -- carries critical aspects of meaning. However, the relationship\nbetween the information conveyed by prosody vs. by the words themselves remains\npoorly understood. We use large language models (LLMs) to estimate how much\ninformation is redundant between prosody and the words themselves. Using a\nlarge spoken corpus of English audiobooks, we extract prosodic features aligned\nto individual words and test how well they can be predicted from LLM\nembeddings, compared to non-contextual word embeddings. We find a high degree\nof redundancy between the information carried by the words and prosodic\ninformation across several prosodic features, including intensity, duration,\npauses, and pitch contours. Furthermore, a word's prosodic information is\nredundant with both the word itself and the context preceding as well as\nfollowing it. Still, we observe that prosodic features can not be fully\npredicted from text, suggesting that prosody carries information above and\nbeyond the words. Along with this paper, we release a general-purpose data\nprocessing pipeline for quantifying the relationship between linguistic\ninformation and extra-linguistic features.",
            "author": [
                "Lukas Wolf",
                "Tiago Pimentel",
                "Evelina Fedorenko",
                "Ryan Cotterell",
                "Alex Warstadt",
                "Ethan Wilcox",
                "Tamar Regev"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17233v1",
                "http://arxiv.org/pdf/2311.17233v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.IT",
                "cs.LG",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17232v1",
            "title": "ReWaRD: Retinal Waves for Pre-Training Artificial Neural Networks\n  Mimicking Real Prenatal Development",
            "updated": "2023-11-28T21:14:05Z",
            "published": "2023-11-28T21:14:05Z",
            "summary": "Computational models trained on a large amount of natural images are the\nstate-of-the-art to study human vision - usually adult vision. Computational\nmodels of infant vision and its further development are gaining more and more\nattention in the community. In this work we aim at the very beginning of our\nvisual experience - pre- and post-natal retinal waves which suggest to be a\npre-training mechanism for the primate visual system at a very early stage of\ndevelopment. We see this approach as an instance of biologically plausible data\ndriven inductive bias through pre-training. We built a computational model that\nmimics this development mechanism by pre-training different artificial\nconvolutional neural networks with simulated retinal wave images. The resulting\nfeatures of this biologically plausible pre-training closely match the V1\nfeatures of the primate visual system. We show that the performance gain by\npre-training with retinal waves is similar to a state-of-the art pre-training\npipeline. Our framework contains the retinal wave generator, as well as a\ntraining strategy, which can be a first step in a curriculum learning based\ntraining diet for various models of development. We release code, data and\ntrained networks to build the basis for future work on visual development and\nbased on a curriculum learning approach including prenatal development to\nsupport studies of innate vs. learned properties of the primate visual system.\nAn additional benefit of our pre-trained networks for neuroscience or computer\nvision applications is the absence of biases inherited from datasets like\nImageNet.",
            "author": [
                "Benjamin Cappell",
                "Andreas Stoll",
                "Williams Chukwudi Umah",
                "Bernhard Egger"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17232v1",
                "http://arxiv.org/pdf/2311.17232v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17228v1",
            "title": "Survey on AI Ethics: A Socio-technical Perspective",
            "updated": "2023-11-28T21:00:56Z",
            "published": "2023-11-28T21:00:56Z",
            "summary": "The past decade has observed a great advancement in AI with deep\nlearning-based models being deployed in diverse scenarios including\nsafety-critical applications. As these AI systems become deeply embedded in our\nsocietal infrastructure, the repercussions of their decisions and actions have\nsignificant consequences, making the ethical implications of AI deployment\nhighly relevant and important. The ethical concerns associated with AI are\nmultifaceted, including challenging issues of fairness, privacy and data\nprotection, responsibility and accountability, safety and robustness,\ntransparency and explainability, and environmental impact. These principles\ntogether form the foundations of ethical AI considerations that concern every\nstakeholder in the AI system lifecycle. In light of the present ethical and\nfuture x-risk concerns, governments have shown increasing interest in\nestablishing guidelines for the ethical deployment of AI. This work unifies the\ncurrent and future ethical concerns of deploying AI into society. While we\nacknowledge and appreciate the technical surveys for each of the ethical\nprinciples concerned, in this paper, we aim to provide a comprehensive overview\nthat not only addresses each principle from a technical point of view but also\ndiscusses them from a social perspective.",
            "author": [
                "Dave Mbiazi",
                "Meghana Bhange",
                "Maryam Babaei",
                "Ivaxi Sheth",
                "Patrik Joslin Kenfack"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17228v1",
                "http://arxiv.org/pdf/2311.17228v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17225v1",
            "title": "Invariance assumptions for class distribution estimation",
            "updated": "2023-11-28T20:57:10Z",
            "published": "2023-11-28T20:57:10Z",
            "summary": "We study the problem of class distribution estimation under dataset shift. On\nthe training dataset, both features and class labels are observed while on the\ntest dataset only the features can be observed. The task then is the estimation\nof the distribution of the class labels, i.e. the estimation of the class prior\nprobabilities, in the test dataset. Assumptions of invariance between the\ntraining joint distribution of features and labels and the test distribution\ncan considerably facilitate this task. We discuss the assumptions of covariate\nshift, factorizable joint shift, and sparse joint shift and their implications\nfor class distribution estimation.",
            "author": [
                "Dirk Tasche"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17225v1",
                "http://arxiv.org/pdf/2311.17225v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "68P99, 62B05",
                "G.3; I.5.1"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17218v1",
            "title": "BIM: Block-Wise Self-Supervised Learning with Masked Image Modeling",
            "updated": "2023-11-28T20:42:30Z",
            "published": "2023-11-28T20:42:30Z",
            "summary": "Like masked language modeling (MLM) in natural language processing, masked\nimage modeling (MIM) aims to extract valuable insights from image patches to\nenhance the feature extraction capabilities of the underlying deep neural\nnetwork (DNN). Contrasted with other training paradigms like supervised\nlearning and unsupervised contrastive learning, masked image modeling (MIM)\npretraining typically demands significant computational resources in order to\nmanage large training data batches (e.g., 4096). The significant memory and\ncomputation requirements pose a considerable challenge to its broad adoption.\nTo mitigate this, we introduce a novel learning framework,\ntermed~\\textit{Block-Wise Masked Image Modeling} (BIM). This framework involves\ndecomposing the MIM tasks into several sub-tasks with independent computation\npatterns, resulting in block-wise back-propagation operations instead of the\ntraditional end-to-end approach. Our proposed BIM maintains superior\nperformance compared to conventional MIM while greatly reducing peak memory\nconsumption. Moreover, BIM naturally enables the concurrent training of\nnumerous DNN backbones of varying depths. This leads to the creation of\nmultiple trained DNN backbones, each tailored to different hardware platforms\nwith distinct computing capabilities. This approach significantly reduces\ncomputational costs in comparison with training each DNN backbone individually.\nOur framework offers a promising solution for resource constrained training of\nMIM.",
            "author": [
                "Yixuan Luo",
                "Mengye Ren",
                "Sai Qian Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17218v1",
                "http://arxiv.org/pdf/2311.17218v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17215v1",
            "title": "Applications of Moments of Dirichlet Coefficients in Elliptic Curve\n  Families",
            "updated": "2023-11-28T20:39:19Z",
            "published": "2023-11-28T20:39:19Z",
            "summary": "The moments of the coefficients of elliptic curve L-functions are related to\nnumerous arithmetic problems. Rosen and Silverman proved a conjecture of Nagao\nrelating the first moment of one-parameter families satisfying Tate's\nconjecture to the rank of the corresponding elliptic surface over Q(T); one can\nalso construct families of moderate rank by finding families with large first\nmoments. Michel proved that if j(T) is not constant, then the second moment of\nthe family is of size p^2 + O(p^(3/2)); these two moments show that for\nsuitably small support the behavior of zeros near the central point agree with\nthat of eigenvalues from random matrix ensembles, with the higher moments\nimpacting the rate of convergence.\n  In his thesis, Miller noticed a negative bias in the second moment of every\none-parameter family of elliptic curves over the rationals whose second moment\nhad a calculable closed-form expression, specifically the first lower order\nterm which does not average to zero is on average negative. This Bias\nConjecture is confirmed for many families; however, these are highly\nnon-generic families whose resulting Legendre sums can be determined. Inspired\nby the recent successes by Yang-Hui He, Kyu-Hwan Lee, Thomas Oliver, Alexey\nPozdnyakov and others in investigations of murmurations of elliptic curve\ncoefficients with machine learning techniques, we pose a similar problem for\ntrying to understand the Bias Conjecture. As a start to this program, we\nnumerically investigate the Bias Conjecture for a family whose bias is positive\nfor half the primes. Since the numerics do not offer conclusive evidence that\nnegative bias for the other half is enough to overwhelm the positive bias, the\nBias Conjecture cannot be verified for the family.",
            "author": [
                "Zo\u00eb Batterman",
                "Aditya Jambhale",
                "Steven J. Miller",
                "Akash L. Narayanan",
                "Kishan Sharma",
                "Andrew Yang",
                "Chris Yao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17215v1",
                "http://arxiv.org/pdf/2311.17215v1"
            ],
            "primary_category": "math.NT",
            "category": [
                "math.NT",
                "cs.NA",
                "math.NA",
                "11G05, 11G40"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17212v1",
            "title": "Failure Artifact Scenarios to Understand High School Students' Growth in\n  Troubleshooting Physical Computing Projects",
            "updated": "2023-11-28T20:34:09Z",
            "published": "2023-11-28T20:34:09Z",
            "summary": "Debugging physical computing projects provides a rich context to understand\ncross-disciplinary problem solving that integrates multiple domains of\ncomputing and engineering. Yet understanding and assessing students' learning\nof debugging remains a challenge, particularly in understudied areas such as\nphysical computing, since finding and fixing hardware and software bugs is a\ndeeply contextual practice. In this paper we draw on the rich history of\nclinical interviews to develop and pilot \"failure artifact scenarios\" in order\nto study changes in students' approaches to debugging and troubleshooting\nelectronic textiles (e-textiles). We applied this clinical interview protocol\nbefore and after an eight-week-long e-textiles unit. We analyzed pre/post\nclinical interviews from 18 students at four different schools. The analysis\nrevealed that students improved in identifying bugs with greater specificity,\nand across domains, and in considering multiple causes for bugs. We discuss\nimplications for developing tools to assess students' debugging abilities\nthrough contextualized debugging scenarios in physical computing.",
            "author": [
                "L. Morales-Navarro",
                "D. A. Fields",
                "D. Barapatre",
                "Y. B. Kafai"
            ],
            "link": [
                "http://dx.doi.org/10.1145/3626252.3630855",
                "http://arxiv.org/abs/2311.17212v1",
                "http://arxiv.org/pdf/2311.17212v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY",
                "cs.HC",
                "K.3.2"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17211v1",
            "title": "LSTM model predicting outcome of strategic thinking task exhibits\n  representations of level-k thinking",
            "updated": "2023-11-28T20:33:55Z",
            "published": "2023-11-28T20:33:55Z",
            "summary": "Which neural mechanisms underlie strategic thinking in the human brain?\nNeuroeconomic research has not yet bridged the gap between theoretical models\nof higher-order reasoning and the precise mechanisms implemented in neural\nnetworks in the human brain. In this paper, I demonstrate that a recurrent\nneural network model can learn to perform strongly in the simple strategic game\nRock-Paper-Scissors. In doing so, it develops implicit representations of\nstrategically important variables (the levels $k$ of reasoning) which\neconomists have postulated in theoretical models. These representations can be\nextracted from the hidden activations of the neural network. These findings\nhint at a connection between the mechanisms implicit in recurrent neural\nnetworks and models of strategic thinking in economic theory. Future empirical\nbrain research can investigate whether these mechanisms correspond to\nmechanisms implicit in biological neural networks.",
            "author": [
                "Mario Stepanik"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17211v1",
                "http://arxiv.org/pdf/2311.17211v1"
            ],
            "primary_category": "cs.GT",
            "category": [
                "cs.GT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17204v1",
            "title": "Optimal EEG Electrode Set for Emotion Recognition From Brain Signals: An\n  Empirical Quest",
            "updated": "2023-11-28T20:18:42Z",
            "published": "2023-11-28T20:18:42Z",
            "summary": "The human brain is a complex organ, still completely undiscovered, that\ncontrols almost all the parts of the body. Apart from survival, the human brain\nstimulates emotions. Recent research indicates that brain signals can be very\neffective for emotion recognition. However, which parts of the brain exhibit\nmost of the emotions is still under-explored. In this study, we empirically\nanalyze the contribution of each part of the brain in exhibiting emotions. We\nuse the DEAP dataset to find the most optimal electrode set which eventually\nleads to the effective brain part associated with emotions. We use Fast Fourier\nTransformation for effective feature extraction and a 1D-CNN with residual\nconnection for classification. Though 32 electrodes from the DEAP dataset got\nan accuracy of 97.34%, only 12 electrodes (F7, P8, O1, F8, C4, T7, PO3, Fp1,\nFp2, O2, P3, and Fz) achieve 95.81% accuracy. This study also shows that adding\nmore than 10 electrodes does not improve performance significantly. Moreover,\nthe frontal lobe is the most important for recognizing emotion.",
            "author": [
                "Rumman Ahmed Prodhan",
                "Sumya Akter",
                "Tanmoy Sarkar Pias",
                "Md. Akhtaruzzaman Adnan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17204v1",
                "http://arxiv.org/pdf/2311.17204v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17198v1",
            "title": "Amplifying the Chirp: Using Deep Learning (U-Nets) to filter signal from\n  noise in LIGO data",
            "updated": "2023-11-28T20:04:27Z",
            "published": "2023-11-28T20:04:27Z",
            "summary": "The direct detection of gravitational waves by LIGO has heralded a new era\nfor astronomy and physics. Typically the gravitational waves observed by LIGO\nare dominated by noise. In this work we use Deep Convolutional Neural Networks\n(specifically U-Nets) to filter a clean signal from noisy data. We present two\nrealizations of U-Net filters, the Noise2Clean U-Net filter which is trained\nusing noisy and clean realizations of the same signal, as well as Noise2Noise\nU-Net which is trained on two separate noisy realization of the same signal. We\nfind that the U-Nets successfully filter signal from noise. We also benchmark\nthe performance of U-Nets by using them to detect the binary presence or\nabsence of gravitational wave signals in data.",
            "author": [
                "Akshay Ghalsasi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17198v1",
                "http://arxiv.org/pdf/2311.17198v1"
            ],
            "primary_category": "gr-qc",
            "category": [
                "gr-qc",
                "astro-ph.CO",
                "astro-ph.IM",
                "hep-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00053v1",
            "title": "Anti-Sexism Alert System: Identification of Sexist Comments on Social\n  Media Using AI Techniques",
            "updated": "2023-11-28T19:48:46Z",
            "published": "2023-11-28T19:48:46Z",
            "summary": "Social relationships in the digital sphere are becoming more usual and\nfrequent, and they constitute a very important aspect for all of us. {Violent\ninteractions in this sphere are very frequent, and have serious effects on the\nvictims}. Within this global scenario, there is one kind of digital violence\nthat is becoming really worrying: sexism against women. Sexist comments that\nare publicly posted in social media (newspaper comments, social networks,\netc.), usually obtain a lot of attention and become viral, with consequent\ndamage to the persons involved. In this paper, we introduce an anti-sexism\nalert system, based on natural language processing (NLP) and artificial\nintelligence (AI), that analyzes any public post, and decides if it could be\nconsidered a sexist comment or not. Additionally, this system also works on\nanalyzing all the public comments linked to any multimedia content (piece of\nnews, video, tweet, etc.) and decides, using a color-based system similar to\ntraffic lights, if there is sexism in the global set of posts. We have created\na labeled data set in Spanish, since the majority of studies focus on English,\nto train our system, which offers a very good performance after the validation\nexperiments.",
            "author": [
                "Rebeca P. D\u00edaz Redondo",
                "Ana Fern\u00e1ndez Vilas",
                "Mateo Ramos Merino",
                "Sonia Valladares",
                "Soledad Torres Guijarro",
                "Manar Mohamed Hafez"
            ],
            "link": [
                "http://dx.doi.org/10.3390/app13074341",
                "http://arxiv.org/abs/2312.00053v1",
                "http://arxiv.org/pdf/2312.00053v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17190v1",
            "title": "Minimax Exploiter: A Data Efficient Approach for Competitive Self-Play",
            "updated": "2023-11-28T19:34:40Z",
            "published": "2023-11-28T19:34:40Z",
            "summary": "Recent advances in Competitive Self-Play (CSP) have achieved, or even\nsurpassed, human level performance in complex game environments such as Dota 2\nand StarCraft II using Distributed Multi-Agent Reinforcement Learning (MARL).\nOne core component of these methods relies on creating a pool of learning\nagents -- consisting of the Main Agent, past versions of this agent, and\nExploiter Agents -- where Exploiter Agents learn counter-strategies to the Main\nAgents. A key drawback of these approaches is the large computational cost and\nphysical time that is required to train the system, making them impractical to\ndeploy in highly iterative real-life settings such as video game productions.\nIn this paper, we propose the Minimax Exploiter, a game theoretic approach to\nexploiting Main Agents that leverages knowledge of its opponents, leading to\nsignificant increases in data efficiency. We validate our approach in a\ndiversity of settings, including simple turn based games, the arcade learning\nenvironment, and For Honor, a modern video game. The Minimax Exploiter\nconsistently outperforms strong baselines, demonstrating improved stability and\ndata efficiency, leading to a robust CSP-MARL method that is both flexible and\neasy to deploy.",
            "author": [
                "Daniel Bairamian",
                "Philippe Marcotte",
                "Joshua Romoff",
                "Gabriel Robert",
                "Derek Nowrouzezahrai"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17190v1",
                "http://arxiv.org/pdf/2311.17190v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.MA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17179v2",
            "title": "SatCLIP: Global, General-Purpose Location Embeddings with Satellite\n  Imagery",
            "updated": "2023-11-30T20:25:24Z",
            "published": "2023-11-28T19:14:40Z",
            "summary": "Geographic location is essential for modeling tasks in fields ranging from\necology to epidemiology to the Earth system sciences. However, extracting\nrelevant and meaningful characteristics of a location can be challenging, often\nentailing expensive data fusion or data distillation from global imagery\ndatasets. To address this challenge, we introduce Satellite Contrastive\nLocation-Image Pretraining (SatCLIP), a global, general-purpose geographic\nlocation encoder that learns an implicit representation of locations from\nopenly available satellite imagery. Trained location encoders provide vector\nembeddings summarizing the characteristics of any given location for convenient\nusage in diverse downstream tasks. We show that SatCLIP embeddings, pretrained\non globally sampled multi-spectral Sentinel-2 satellite data, can be used in\nvarious predictive tasks that depend on location information but not\nnecessarily satellite imagery, including temperature prediction, animal\nrecognition in imagery, and population density estimation. Across tasks,\nSatCLIP embeddings consistently outperform embeddings from existing pretrained\nlocation encoders, ranging from models trained on natural images to models\ntrained on semantic context. SatCLIP embeddings also help to improve geographic\ngeneralization. This demonstrates the potential of general-purpose location\nencoders and opens the door to learning meaningful representations of our\nplanet from the vast, varied, and largely untapped modalities of geospatial\ndata.",
            "author": [
                "Konstantin Klemmer",
                "Esther Rolf",
                "Caleb Robinson",
                "Lester Mackey",
                "Marc Ru\u00dfwurm"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17179v2",
                "http://arxiv.org/pdf/2311.17179v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.CY",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17175v1",
            "title": "Kicking it Off(-shell) with Direct Diffusion",
            "updated": "2023-11-28T19:08:04Z",
            "published": "2023-11-28T19:08:04Z",
            "summary": "Off-shell effects in large LHC backgrounds are crucial for precision\npredictions and, at the same time, challenging to simulate. We show how a\ngenerative diffusion network learns off-shell kinematics given the much simpler\non-shell process. It generates off-shell configurations fast and precisely,\nwhile reproducing even challenging on-shell features.",
            "author": [
                "Anja Butter",
                "Tomas Jezo",
                "Michael Klasen",
                "Mathias Kuschick",
                "Sofia Palacios Schweitzer",
                "Tilman Plehn"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17175v1",
                "http://arxiv.org/pdf/2311.17175v1"
            ],
            "primary_category": "hep-ph",
            "category": [
                "hep-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17173v1",
            "title": "A personalized Uncertainty Quantification framework for patient survival\n  models: estimating individual uncertainty of patients with metastatic brain\n  tumors in the absence of ground truth",
            "updated": "2023-11-28T19:07:30Z",
            "published": "2023-11-28T19:07:30Z",
            "summary": "TodevelopanovelUncertaintyQuantification (UQ) framework to estimate the\nuncertainty of patient survival models in the absence of ground truth, we\ndeveloped and evaluated our approach based on a dataset of 1383 patients\ntreated with stereotactic radiosurgery (SRS) for brain metastases between\nJanuary 2015 and December 2020. Our motivating hypothesis is that a\ntime-to-event prediction of a test patient on inference is more certain given a\nhigher feature-space-similarity to patients in the training set. Therefore, the\nuncertainty for a particular patient-of-interest is represented by the\nconcordance index between a patient similarity rank and a prediction similarity\nrank. Model uncertainty was defined as the increased percentage of the max\nuncertainty-constrained-AUC compared to the model AUC. We evaluated our method\non multiple clinically-relevant endpoints, including time to intracranial\nprogression (ICP), progression-free survival (PFS) after SRS, overall survival\n(OS), and time to ICP and/or death (ICPD), on a variety of both statistical and\nnon-statistical models, including CoxPH, conditional survival forest (CSF), and\nneural multi-task linear regression (NMTLR). Our results show that all models\nhad the lowest uncertainty on ICP (2.21%) and the highest uncertainty (17.28%)\non ICPD. OS models demonstrated high variation in uncertainty performance,\nwhere NMTLR had the lowest uncertainty(1.96%)and CSF had the highest\nuncertainty (14.29%). In conclusion, our method can estimate the uncertainty of\nindividual patient survival modeling results. As expected, our data empirically\ndemonstrate that as model uncertainty measured via our technique increases, the\nsimilarity between a feature-space and its predicted outcome decreases.",
            "author": [
                "Yuqi Wang",
                "Aarzu Gupta",
                "David Carpenter",
                "Trey Mullikin",
                "Zachary J. Reitman",
                "Scott Floyd",
                "John Kirkpatrick",
                "Joseph K. Salama",
                "Paul W. Sperduto",
                "Jian-Guo Liu",
                "Mustafa R. Bashir",
                "Kyle J. Lafata"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17173v1",
                "http://arxiv.org/pdf/2311.17173v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "q-bio.QM",
                "stat.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17170v1",
            "title": "Using the Conceptual Survey of Electricity and Magnetism to investigate\n  progression in student understanding from introductory to advanced levels",
            "updated": "2023-11-28T19:04:17Z",
            "published": "2023-11-28T19:04:17Z",
            "summary": "The Conceptual Survey of Electricity and Magnetism (CSEM) is a\nmultiple-choice survey that contains a variety of electricity and magnetism\nconcepts from Coulomb's law to Faraday's law at the level of introductory\nphysics used to help inform instructors of student mastery of those concepts.\nPrior studies suggest that many concepts on the survey are challenging for\nintroductory physics students and the average student scores after traditional\ninstruction are low. The research presented here investigates the progression\nin student understanding on the CSEM. We compare the performance of students in\nintroductory and advanced level physics courses to understand the evolution of\nstudent understanding of concepts covered in the CSEM after traditional\nlecture-based instruction. We find that on all CSEM questions on which less\nthan 50% of the introductory physics students answered a question correctly\nafter instruction, less than two thirds of the upper-level undergraduate\nstudents provided the correct response after traditional instruction. We also\nanalyzed the CSEM data from graduate students for benchmarking purposes. We\ndiscuss the CSEM questions that remain challenging and the common alternative\nconceptions among upper-level students. The findings presented here at least\npartly point to the fact that traditional instruction in upper-level courses\nwhich typically focuses primarily on quantitative problem solving and\nincentivizes use of algorithmic approaches is not effective for helping\nstudents develop a solid understanding of these concepts. However, it is\nimportant for helping students integrate conceptual and quantitative aspects of\nlearning in order to build a robust knowledge structure of basic concepts in\nelectricity and magnetism.",
            "author": [
                "Alexandru Maries",
                "Mary Jane Brundage",
                "Chandralekha Singh"
            ],
            "link": [
                "http://dx.doi.org/10.1103/PhysRevPhysEducRes.18.020114",
                "http://arxiv.org/abs/2311.17170v1",
                "http://arxiv.org/pdf/2311.17170v1"
            ],
            "primary_category": "physics.ed-ph",
            "category": [
                "physics.ed-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17165v1",
            "title": "(Ir)rationality in AI: State of the Art, Research Challenges and Open\n  Questions",
            "updated": "2023-11-28T19:01:09Z",
            "published": "2023-11-28T19:01:09Z",
            "summary": "The concept of rationality is central to the field of artificial\nintelligence. Whether we are seeking to simulate human reasoning, or the goal\nis to achieve bounded optimality, we generally seek to make artificial agents\nas rational as possible. Despite the centrality of the concept within AI, there\nis no unified definition of what constitutes a rational agent. This article\nprovides a survey of rationality and irrationality in artificial intelligence,\nand sets out the open questions in this area. The understanding of rationality\nin other fields has influenced its conception within artificial intelligence,\nin particular work in economics, philosophy and psychology. Focusing on the\nbehaviour of artificial agents, we consider irrational behaviours that can\nprove to be optimal in certain scenarios. Some methods have been developed to\ndeal with irrational agents, both in terms of identification and interaction,\nhowever work in this area remains limited. Methods that have up to now been\ndeveloped for other purposes, namely adversarial scenarios, may be adapted to\nsuit interactions with artificial agents. We further discuss the interplay\nbetween human and artificial agents, and the role that rationality plays within\nthis interaction; many questions remain in this area, relating to potentially\nirrational behaviour of both humans and artificial agents.",
            "author": [
                "Olivia Macmillan-Scott",
                "Mirco Musolesi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17165v1",
                "http://arxiv.org/pdf/2311.17165v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CY",
                "cs.HC",
                "cs.LG",
                "cs.MA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17162v1",
            "title": "Fast Particle-based Anomaly Detection Algorithm with Variational\n  Autoencoder",
            "updated": "2023-11-28T19:00:29Z",
            "published": "2023-11-28T19:00:29Z",
            "summary": "Model-agnostic anomaly detection is one of the promising approaches in the\nsearch for new beyond the standard model physics. In this paper, we present\nSet-VAE, a particle-based variational autoencoder (VAE) anomaly detection\nalgorithm. We demonstrate a 2x signal efficiency gain compared with traditional\nsubjettiness-based jet selection. Furthermore, with an eye to the future\ndeployment to trigger systems, we propose the CLIP-VAE, which reduces the\ninference-time cost of anomaly detection by using the KL-divergence loss as the\nanomaly score, resulting in a 2x acceleration in latency and reducing the\ncaching requirement.",
            "author": [
                "Ryan Liu",
                "Abhijith Gandrakota",
                "Jennifer Ngadiuba",
                "Maria Spiropulu",
                "Jean-Roch Vlimant"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17162v1",
                "http://arxiv.org/pdf/2311.17162v1"
            ],
            "primary_category": "hep-ex",
            "category": [
                "hep-ex",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17154v1",
            "title": "Pragmatic Radiology Report Generation",
            "updated": "2023-11-28T19:00:03Z",
            "published": "2023-11-28T19:00:03Z",
            "summary": "When pneumonia is not found on a chest X-ray, should the report describe this\nnegative observation or omit it? We argue that this question cannot be answered\nfrom the X-ray alone and requires a pragmatic perspective, which captures the\ncommunicative goal that radiology reports serve between radiologists and\npatients. However, the standard image-to-text formulation for radiology report\ngeneration fails to incorporate such pragmatic intents. Following this\npragmatic perspective, we demonstrate that the indication, which describes why\na patient comes for an X-ray, drives the mentions of negative observations and\nintroduce indications as additional input to report generation. With respect to\nthe output, we develop a framework to identify uninferable information from the\nimage as a source of model hallucinations, and limit them by cleaning\ngroundtruth reports. Finally, we use indications and cleaned groundtruth\nreports to develop pragmatic models, and show that they outperform existing\nmethods not only in new pragmatics-inspired metrics (+4.3 Negative F1) but also\nin standard metrics (+6.3 Positive F1 and +11.0 BLEU-2).",
            "author": [
                "Dang Nguyen",
                "Chacha Chen",
                "He He",
                "Chenhao Tan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17154v1",
                "http://arxiv.org/pdf/2311.17154v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.CY",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17141v1",
            "title": "A point cloud approach to generative modeling for galaxy surveys at the\n  field level",
            "updated": "2023-11-28T19:00:00Z",
            "published": "2023-11-28T19:00:00Z",
            "summary": "We introduce a diffusion-based generative model to describe the distribution\nof galaxies in our Universe directly as a collection of points in 3-D space\n(coordinates) optionally with associated attributes (e.g., velocities and\nmasses), without resorting to binning or voxelization. The custom diffusion\nmodel can be used both for emulation, reproducing essential summary statistics\nof the galaxy distribution, as well as inference, by computing the conditional\nlikelihood of a galaxy field. We demonstrate a first application to massive\ndark matter haloes in the Quijote simulation suite. This approach can be\nextended to enable a comprehensive analysis of cosmological data, circumventing\nlimitations inherent to summary statistic -- as well as neural simulation-based\ninference methods.",
            "author": [
                "Carolina Cuesta-Lazaro",
                "Siddharth Mishra-Sharma"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17141v1",
                "http://arxiv.org/pdf/2311.17141v1"
            ],
            "primary_category": "astro-ph.CO",
            "category": [
                "astro-ph.CO",
                "astro-ph.IM",
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17143v1",
            "title": "Predicting the Age of Astronomical Transients from Real-Time\n  Multivariate Time Series",
            "updated": "2023-11-28T19:00:00Z",
            "published": "2023-11-28T19:00:00Z",
            "summary": "Astronomical transients, such as supernovae and other rare stellar\nexplosions, have been instrumental in some of the most significant discoveries\nin astronomy. New astronomical sky surveys will soon record unprecedented\nnumbers of transients as sparsely and irregularly sampled multivariate time\nseries. To improve our understanding of the physical mechanisms of transients\nand their progenitor systems, early-time measurements are necessary.\nPrioritizing the follow-up of transients based on their age along with their\nclass is crucial for new surveys. To meet this demand, we present the first\nmethod of predicting the age of transients in real-time from multi-wavelength\ntime-series observations. We build a Bayesian probabilistic recurrent neural\nnetwork. Our method can accurately predict the age of a transient with robust\nuncertainties as soon as it is initially triggered by a survey telescope. This\nwork will be essential for the advancement of our understanding of the numerous\nyoung transients being detected by ongoing and upcoming astronomical surveys.",
            "author": [
                "Hali Huang",
                "Daniel Muthukrishna",
                "Prajna Nair",
                "Zimi Zhang",
                "Michael Fausnaugh",
                "Torsha Majumder",
                "Ryan J. Foley",
                "George R. Ricker"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17143v1",
                "http://arxiv.org/pdf/2311.17143v1"
            ],
            "primary_category": "astro-ph.IM",
            "category": [
                "astro-ph.IM",
                "astro-ph.HE",
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17146v1",
            "title": "Calabi-Yau Four/Five/Six-folds as $\\mathbb{P}^n_\\textbf{w}$\n  Hypersurfaces: Machine Learning, Approximation, and Generation",
            "updated": "2023-11-28T19:00:00Z",
            "published": "2023-11-28T19:00:00Z",
            "summary": "Calabi-Yau four-folds may be constructed as hypersurfaces in weighted\nprojective spaces of complex dimension 5 defined via weight systems of 6\nweights. In this work, neural networks were implemented to learn the Calabi-Yau\nHodge numbers from the weight systems, where gradient saliency and symbolic\nregression then inspired a truncation of the Landau-Ginzburg model formula for\nthe Hodge numbers of any dimensional Calabi-Yau constructed in this way. The\napproximation always provides a tight lower bound, is shown to be dramatically\nquicker to compute (with compute times reduced by up to four orders of\nmagnitude), and gives remarkably accurate results for systems with large\nweights. Additionally, complementary datasets of weight systems satisfying the\nnecessary but insufficient conditions for transversality were constructed,\nincluding considerations of the IP, reflexivity, and intradivisibility\nproperties. Overall producing a classification of this weight system landscape,\nfurther confirmed with machine learning methods. Using the knowledge of this\nclassification, and the properties of the presented approximation, a novel\ndataset of transverse weight systems consisting of 7 weights was generated for\na sum of weights $\\leq 200$; producing a new database of Calabi-Yau five-folds,\nwith their respective topological properties computed. Further to this an\nequivalent database of candidate Calabi-Yau six-folds was generated with\napproximated Hodge numbers.",
            "author": [
                "Edward Hirst",
                "Tancredi Schettini Gherardini"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17146v1",
                "http://arxiv.org/pdf/2311.17146v1"
            ],
            "primary_category": "hep-th",
            "category": [
                "hep-th",
                "math.AG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17059v1",
            "title": "Mission-driven Exploration for Accelerated Deep Reinforcement Learning\n  with Temporal Logic Task Specifications",
            "updated": "2023-11-28T18:59:58Z",
            "published": "2023-11-28T18:59:58Z",
            "summary": "This paper addresses the problem of designing optimal control policies for\nmobile robots with mission and safety requirements specified using Linear\nTemporal Logic (LTL). We consider robots with unknown stochastic dynamics\noperating in environments with unknown geometric structure. The robots are\nequipped with sensors allowing them to detect obstacles. Our goal is to\nsynthesize a control policy that maximizes the probability of satisfying an\nLTL-encoded task in the presence of motion and environmental uncertainty.\nSeveral deep reinforcement learning (DRL) algorithms have been proposed\nrecently to address similar problems. A common limitation in related works is\nthat of slow learning performance. In order to address this issue, we propose a\nnovel DRL algorithm, which has the capability to learn control policies at a\nnotably faster rate compared to similar methods. Its sample efficiency is due\nto a mission-driven exploration strategy that prioritizes exploration towards\ndirections that may contribute to mission accomplishment. Identifying these\ndirections relies on an automaton representation of the LTL task as well as a\nlearned neural network that (partially) models the unknown system dynamics. We\nprovide comparative experiments demonstrating the efficiency of our algorithm\non robot navigation tasks in unknown environments.",
            "author": [
                "Jun Wang",
                "Hosein Hasanbeig",
                "Kaiyuan Tan",
                "Zihe Sun",
                "Yiannis Kantaros"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17059v1",
                "http://arxiv.org/pdf/2311.17059v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17056v1",
            "title": "Self-Supervised Motion Magnification by Backpropagating Through Optical\n  Flow",
            "updated": "2023-11-28T18:59:51Z",
            "published": "2023-11-28T18:59:51Z",
            "summary": "This paper presents a simple, self-supervised method for magnifying subtle\nmotions in video: given an input video and a magnification factor, we\nmanipulate the video such that its new optical flow is scaled by the desired\namount. To train our model, we propose a loss function that estimates the\noptical flow of the generated video and penalizes how far if deviates from the\ngiven magnification factor. Thus, training involves differentiating through a\npretrained optical flow network. Since our model is self-supervised, we can\nfurther improve its performance through test-time adaptation, by finetuning it\non the input video. It can also be easily extended to magnify the motions of\nonly user-selected objects. Our approach avoids the need for synthetic\nmagnification datasets that have been used to train prior learning-based\napproaches. Instead, it leverages the existing capabilities of off-the-shelf\nmotion estimators. We demonstrate the effectiveness of our method through\nevaluations of both visual quality and quantitative metrics on a range of\nreal-world and synthetic videos, and we show our method works for both\nsupervised and unsupervised optical flow methods.",
            "author": [
                "Zhaoying Pan",
                "Daniel Geng",
                "Andrew Owens"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17056v1",
                "http://arxiv.org/pdf/2311.17056v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16504v1",
            "title": "Rethinking Directional Integration in Neural Radiance Fields",
            "updated": "2023-11-28T18:59:50Z",
            "published": "2023-11-28T18:59:50Z",
            "summary": "Recent works use the Neural radiance field (NeRF) to perform multi-view 3D\nreconstruction, providing a significant leap in rendering photorealistic\nscenes. However, despite its efficacy, NeRF exhibits limited capability of\nlearning view-dependent effects compared to light field rendering or\nimage-based view synthesis. To that end, we introduce a modification to the\nNeRF rendering equation which is as simple as a few lines of code change for\nany NeRF variations, while greatly improving the rendering quality of\nview-dependent effects. By swapping the integration operator and the direction\ndecoder network, we only integrate the positional features along the ray and\nmove the directional terms out of the integration, resulting in a\ndisentanglement of the view-dependent and independent components. The modified\nequation is equivalent to the classical volumetric rendering in ideal cases on\nobject surfaces with Dirac densities. Furthermore, we prove that with the\nerrors caused by network approximation and numerical integration, our rendering\nequation exhibits better convergence properties with lower error accumulations\ncompared to the classical NeRF. We also show that the modified equation can be\ninterpreted as light field rendering with learned ray embeddings. Experiments\non different NeRF variations show consistent improvements in the quality of\nview-dependent effects with our simple modification.",
            "author": [
                "Congyue Deng",
                "Jiawei Yang",
                "Leonidas Guibas",
                "Yue Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16504v1",
                "http://arxiv.org/pdf/2311.16504v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17055v1",
            "title": "No Representation Rules Them All in Category Discovery",
            "updated": "2023-11-28T18:59:46Z",
            "published": "2023-11-28T18:59:46Z",
            "summary": "In this paper we tackle the problem of Generalized Category Discovery (GCD).\nSpecifically, given a dataset with labelled and unlabelled images, the task is\nto cluster all images in the unlabelled subset, whether or not they belong to\nthe labelled categories. Our first contribution is to recognize that most\nexisting GCD benchmarks only contain labels for a single clustering of the\ndata, making it difficult to ascertain whether models are using the available\nlabels to solve the GCD task, or simply solving an unsupervised clustering\nproblem. As such, we present a synthetic dataset, named 'Clevr-4', for category\ndiscovery. Clevr-4 contains four equally valid partitions of the data, i.e\nbased on object shape, texture, color or count. To solve the task, models are\nrequired to extrapolate the taxonomy specified by the labelled set, rather than\nsimply latching onto a single natural grouping of the data. We use this dataset\nto demonstrate the limitations of unsupervised clustering in the GCD setting,\nshowing that even very strong unsupervised models fail on Clevr-4. We further\nuse Clevr-4 to examine the weaknesses of existing GCD algorithms, and propose a\nnew method which addresses these shortcomings, leveraging consistent findings\nfrom the representation learning literature to do so. Our simple solution,\nwhich is based on 'mean teachers' and termed $\\mu$GCD, substantially\noutperforms implemented baselines on Clevr-4. Finally, when we transfer these\nfindings to real data on the challenging Semantic Shift Benchmark (SSB), we\nfind that $\\mu$GCD outperforms all prior work, setting a new state-of-the-art.\nFor the project webpage, see https://www.robots.ox.ac.uk/~vgg/data/clevr4/",
            "author": [
                "Sagar Vaze",
                "Andrea Vedaldi",
                "Andrew Zisserman"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17055v1",
                "http://arxiv.org/pdf/2311.17055v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.IT",
                "cs.LG",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17138v1",
            "title": "Shadows Don't Lie and Lines Can't Bend! Generative Models don't know\n  Projective Geometry...for now",
            "updated": "2023-11-28T18:59:06Z",
            "published": "2023-11-28T18:59:06Z",
            "summary": "Generative models can produce impressively realistic images. This paper\ndemonstrates that generated images have geometric features different from those\nof real images. We build a set of collections of generated images, prequalified\nto fool simple, signal-based classifiers into believing they are real. We then\nshow that prequalified generated images can be identified reliably by\nclassifiers that only look at geometric properties. We use three such\nclassifiers. All three classifiers are denied access to image pixels, and look\nonly at derived geometric features. The first classifier looks at the\nperspective field of the image, the second looks at lines detected in the\nimage, and the third looks at relations between detected objects and shadows.\nOur procedure detects generated images more reliably than SOTA local signal\nbased detectors, for images from a number of distinct generators. Saliency maps\nsuggest that the classifiers can identify geometric problems reliably. We\nconclude that current generators cannot reliably reproduce geometric properties\nof real images.",
            "author": [
                "Ayush Sarkar",
                "Hanlin Mai",
                "Amitabh Mahapatra",
                "Svetlana Lazebnik",
                "D. A. Forsyth",
                "Anand Bhattad"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17138v1",
                "http://arxiv.org/pdf/2311.17138v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.GR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17137v1",
            "title": "Generative Models: What do they know? Do they know things? Let's find\n  out!",
            "updated": "2023-11-28T18:59:02Z",
            "published": "2023-11-28T18:59:02Z",
            "summary": "Generative models have been shown to be capable of synthesizing highly\ndetailed and realistic images. It is natural to suspect that they implicitly\nlearn to model some image intrinsics such as surface normals, depth, or\nshadows. In this paper, we present compelling evidence that generative models\nindeed internally produce high-quality scene intrinsic maps. We introduce\nIntrinsic LoRA (I LoRA), a universal, plug-and-play approach that transforms\nany generative model into a scene intrinsic predictor, capable of extracting\nintrinsic scene maps directly from the original generator network without\nneeding additional decoders or fully fine-tuning the original network. Our\nmethod employs a Low-Rank Adaptation (LoRA) of key feature maps, with newly\nlearned parameters that make up less than 0.6% of the total parameters in the\ngenerative model. Optimized with a small set of labeled images, our\nmodel-agnostic approach adapts to various generative architectures, including\nDiffusion models, GANs, and Autoregressive models. We show that the scene\nintrinsic maps produced by our method compare well with, and in some cases\nsurpass those generated by leading supervised techniques.",
            "author": [
                "Xiaodan Du",
                "Nicholas Kolkin",
                "Greg Shakhnarovich",
                "Anand Bhattad"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17137v1",
                "http://arxiv.org/pdf/2311.17137v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.GR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17053v1",
            "title": "DiffuseBot: Breeding Soft Robots With Physics-Augmented Generative\n  Diffusion Models",
            "updated": "2023-11-28T18:58:48Z",
            "published": "2023-11-28T18:58:48Z",
            "summary": "Nature evolves creatures with a high complexity of morphological and\nbehavioral intelligence, meanwhile computational methods lag in approaching\nthat diversity and efficacy. Co-optimization of artificial creatures'\nmorphology and control in silico shows promise for applications in physical\nsoft robotics and virtual character creation; such approaches, however, require\ndeveloping new learning algorithms that can reason about function atop pure\nstructure. In this paper, we present DiffuseBot, a physics-augmented diffusion\nmodel that generates soft robot morphologies capable of excelling in a wide\nspectrum of tasks. DiffuseBot bridges the gap between virtually generated\ncontent and physical utility by (i) augmenting the diffusion process with a\nphysical dynamical simulation which provides a certificate of performance, and\n(ii) introducing a co-design procedure that jointly optimizes physical design\nand control by leveraging information about physical sensitivities from\ndifferentiable simulation. We showcase a range of simulated and fabricated\nrobots along with their capabilities. Check our website at\nhttps://diffusebot.github.io/",
            "author": [
                "Tsun-Hsuan Wang",
                "Juntian Zheng",
                "Pingchuan Ma",
                "Yilun Du",
                "Byungchul Kim",
                "Andrew Spielberg",
                "Joshua Tenenbaum",
                "Chuang Gan",
                "Daniela Rus"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17053v1",
                "http://arxiv.org/pdf/2311.17053v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17050v1",
            "title": "Surf-D: High-Quality Surface Generation for Arbitrary Topologies using\n  Diffusion Models",
            "updated": "2023-11-28T18:56:01Z",
            "published": "2023-11-28T18:56:01Z",
            "summary": "In this paper, we present Surf-D, a novel method for generating high-quality\n3D shapes as Surfaces with arbitrary topologies using Diffusion models.\nSpecifically, we adopt Unsigned Distance Field (UDF) as the surface\nrepresentation, as it excels in handling arbitrary topologies, enabling the\ngeneration of complex shapes. While the prior methods explored shape generation\nwith different representations, they suffer from limited topologies and\ngeometry details. Moreover, it's non-trivial to directly extend prior diffusion\nmodels to UDF because they lack spatial continuity due to the discrete volume\nstructure. However, UDF requires accurate gradients for mesh extraction and\nlearning. To tackle the issues, we first leverage a point-based auto-encoder to\nlearn a compact latent space, which supports gradient querying for any input\npoint through differentiation to effectively capture intricate geometry at a\nhigh resolution. Since the learning difficulty for various shapes can differ, a\ncurriculum learning strategy is employed to efficiently embed various surfaces,\nenhancing the whole embedding process. With pretrained shape latent space, we\nemploy a latent diffusion model to acquire the distribution of various shapes.\nOur approach demonstrates superior performance in shape generation across\nmultiple modalities and conducts extensive experiments in unconditional\ngeneration, category conditional generation, 3D reconstruction from images, and\ntext-to-shape tasks.",
            "author": [
                "Zhengming Yu",
                "Zhiyang Dou",
                "Xiaoxiao Long",
                "Cheng Lin",
                "Zekun Li",
                "Yuan Liu",
                "Norman M\u00fcller",
                "Taku Komura",
                "Marc Habermann",
                "Christian Theobalt",
                "Xin Li",
                "Wenping Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17050v1",
                "http://arxiv.org/pdf/2311.17050v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17049v1",
            "title": "MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced\n  Training",
            "updated": "2023-11-28T18:55:42Z",
            "published": "2023-11-28T18:55:42Z",
            "summary": "Contrastive pretraining of image-text foundation models, such as CLIP,\ndemonstrated excellent zero-shot performance and improved robustness on a wide\nrange of downstream tasks. However, these models utilize large\ntransformer-based encoders with significant memory and latency overhead which\npose challenges for deployment on mobile devices. In this work, we introduce\nMobileCLIP -- a new family of efficient image-text models optimized for runtime\nperformance along with a novel and efficient training approach, namely\nmulti-modal reinforced training. The proposed training approach leverages\nknowledge transfer from an image captioning model and an ensemble of strong\nCLIP encoders to improve the accuracy of efficient models. Our approach avoids\ntrain-time compute overhead by storing the additional knowledge in a reinforced\ndataset. MobileCLIP sets a new state-of-the-art latency-accuracy tradeoff for\nzero-shot classification and retrieval tasks on several datasets. Our\nMobileCLIP-S2 variant is 2.3$\\times$ faster while more accurate compared to\nprevious best CLIP model based on ViT-B/16. We further demonstrate the\neffectiveness of our multi-modal reinforced training by training a CLIP model\nbased on ViT-B/16 image backbone and achieving +2.9% average performance\nimprovement on 38 evaluation benchmarks compared to the previous best.\nMoreover, we show that the proposed approach achieves 10$\\times$-1000$\\times$\nimproved learning efficiency when compared with non-reinforced CLIP training.",
            "author": [
                "Pavan Kumar Anasosalu Vasu",
                "Hadi Pouransari",
                "Fartash Faghri",
                "Raviteja Vemulapalli",
                "Oncel Tuzel"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17049v1",
                "http://arxiv.org/pdf/2311.17049v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17135v2",
            "title": "TLControl: Trajectory and Language Control for Human Motion Synthesis",
            "updated": "2023-11-30T20:36:16Z",
            "published": "2023-11-28T18:54:16Z",
            "summary": "Controllable human motion synthesis is essential for applications in AR/VR,\ngaming, movies, and embodied AI. Existing methods often focus solely on either\nlanguage or full trajectory control, lacking precision in synthesizing motions\naligned with user-specified trajectories, especially for multi-joint control.\nTo address these issues, we present TLControl, a new method for realistic human\nmotion synthesis, incorporating both low-level trajectory and high-level\nlanguage semantics controls. Specifically, we first train a VQ-VAE to learn a\ncompact latent motion space organized by body parts. We then propose a Masked\nTrajectories Transformer to make coarse initial predictions of full\ntrajectories of joints based on the learned latent motion space, with\nuser-specified partial trajectories and text descriptions as conditioning.\nFinally, we introduce an efficient test-time optimization to refine these\ncoarse predictions for accurate trajectory control. Experiments demonstrate\nthat TLControl outperforms the state-of-the-art in trajectory accuracy and time\nefficiency, making it practical for interactive and high-quality animation\ngeneration.",
            "author": [
                "Weilin Wan",
                "Zhiyang Dou",
                "Taku Komura",
                "Wenping Wang",
                "Dinesh Jayaraman",
                "Lingjie Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17135v2",
                "http://arxiv.org/pdf/2311.17135v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17041v2",
            "title": "Efficient In-Context Learning in Vision-Language Models for Egocentric\n  Videos",
            "updated": "2023-11-29T15:52:55Z",
            "published": "2023-11-28T18:53:06Z",
            "summary": "Recent advancements in text-only large language models (LLMs) have\nhighlighted the benefit of in-context learning for adapting to new tasks with a\nfew demonstrations. However, extending in-context learning to large\nvision-language models (VLMs) using a huge amount of naturalistic\nvision-language data has shown limited success, particularly for egocentric\nvideos, due to high data collection costs. We propose a novel training method\n$\\mathbb{E}$fficient $\\mathbb{I}$n-context $\\mathbb{L}$earning on\n$\\mathbb{E}$gocentric $\\mathbb{V}$ideos ($\\mathbb{EILEV}$), which elicits\nin-context learning in VLMs for egocentric videos without requiring massive,\nnaturalistic egocentric video datasets. $\\mathbb{EILEV}$ involves architectural\nand training data adaptations to allow the model to process contexts\ninterleaved with video clips and narrations, sampling of in-context examples\nwith clusters of similar verbs and nouns, use of data with skewed marginal\ndistributions with a long tail of infrequent verbs and nouns, as well as\nhomonyms and synonyms. Our evaluations show that $\\mathbb{EILEV}$-trained\nmodels outperform larger VLMs trained on a huge amount of naturalistic data in\nin-context learning. Furthermore, they can generalize to not only\nout-of-distribution, but also novel, rare egocentric videos and texts via\nin-context learning, demonstrating potential for applications requiring\ncost-effective training, and rapid post-deployment adaptability. Our code and\ndemo are available at \\url{https://github.com/yukw777/EILEV}.",
            "author": [
                "Keunwoo Peter Yu",
                "Zheyuan Zhang",
                "Fengyuan Hu",
                "Joyce Chai"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17041v2",
                "http://arxiv.org/pdf/2311.17041v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17134v2",
            "title": "GlycoNMR: Dataset and benchmarks for NMR chemical shift prediction of\n  carbohydrates with graph neural networks",
            "updated": "2023-11-30T02:06:29Z",
            "published": "2023-11-28T18:51:19Z",
            "summary": "Molecular representation learning (MRL) is a powerful tool for bridging the\ngap between machine learning and chemical sciences, as it converts molecules\ninto numerical representations while preserving their chemical features. These\nencoded representations serve as a foundation for various downstream\nbiochemical studies, including property prediction and drug design. MRL has had\ngreat success with proteins and general biomolecule datasets. Yet, in the\ngrowing sub-field of glycoscience (the study of carbohydrates, where longer\ncarbohydrates are also called glycans), MRL methods have been barely explored.\nThis under-exploration can be primarily attributed to the limited availability\nof comprehensive and well-curated carbohydrate-specific datasets and a lack of\nMachine learning (ML) pipelines specifically tailored to meet the unique\nproblems presented by carbohydrate data. Since interpreting and annotating\ncarbohydrate-specific data is generally more complicated than protein data,\ndomain experts are usually required to get involved. The existing MRL methods,\npredominately optimized for proteins and small biomolecules, also cannot be\ndirectly used in carbohydrate applications without special modifications. To\naddress this challenge, accelerate progress in glycoscience, and enrich the\ndata resources of the MRL community, we introduce GlycoNMR. GlycoNMR contains\ntwo laboriously curated datasets with 2,609 carbohydrate structures and 211,543\nannotated nuclear magnetic resonance (NMR) chemical shifts for precise\natomic-level prediction. We tailored carbohydrate-specific features and adapted\nexisting MRL models to tackle this problem effectively. For illustration, we\nbenchmark four modified MRL models on our new datasets.",
            "author": [
                "Zizhang Chen",
                "Ryan Paul Badman",
                "Lachele Foley",
                "Robert Woods",
                "Pengyu Hong"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17134v2",
                "http://arxiv.org/pdf/2311.17134v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "q-bio.QM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17035v1",
            "title": "Scalable Extraction of Training Data from (Production) Language Models",
            "updated": "2023-11-28T18:47:03Z",
            "published": "2023-11-28T18:47:03Z",
            "summary": "This paper studies extractable memorization: training data that an adversary\ncan efficiently extract by querying a machine learning model without prior\nknowledge of the training dataset. We show an adversary can extract gigabytes\nof training data from open-source language models like Pythia or GPT-Neo,\nsemi-open models like LLaMA or Falcon, and closed models like ChatGPT. Existing\ntechniques from the literature suffice to attack unaligned models; in order to\nattack the aligned ChatGPT, we develop a new divergence attack that causes the\nmodel to diverge from its chatbot-style generations and emit training data at a\nrate 150x higher than when behaving properly. Our methods show practical\nattacks can recover far more data than previously thought, and reveal that\ncurrent alignment techniques do not eliminate memorization.",
            "author": [
                "Milad Nasr",
                "Nicholas Carlini",
                "Jonathan Hayase",
                "Matthew Jagielski",
                "A. Feder Cooper",
                "Daphne Ippolito",
                "Christopher A. Choquette-Choo",
                "Eric Wallace",
                "Florian Tram\u00e8r",
                "Katherine Lee"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17035v1",
                "http://arxiv.org/pdf/2311.17035v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17030v2",
            "title": "Is This the Subspace You Are Looking for? An Interpretability Illusion\n  for Subspace Activation Patching",
            "updated": "2023-12-06T14:28:46Z",
            "published": "2023-11-28T18:32:19Z",
            "summary": "Mechanistic interpretability aims to understand model behaviors in terms of\nspecific, interpretable features, often hypothesized to manifest as\nlow-dimensional subspaces of activations. Specifically, recent studies have\nexplored subspace interventions (such as activation patching) as a way to\nsimultaneously manipulate model behavior and attribute the features behind it\nto given subspaces.\n  In this work, we demonstrate that these two aims diverge, potentially leading\nto an illusory sense of interpretability. Counterintuitively, even if a\nsubspace intervention makes the model's output behave as if the value of a\nfeature was changed, this effect may be achieved by activating a dormant\nparallel pathway leveraging another subspace that is causally disconnected from\nmodel outputs. We demonstrate this phenomenon in a distilled mathematical\nexample, in two real-world domains (the indirect object identification task and\nfactual recall), and present evidence for its prevalence in practice. In the\ncontext of factual recall, we further show a link to rank-1 fact editing,\nproviding a mechanistic explanation for previous work observing an\ninconsistency between fact editing performance and fact localization.\n  However, this does not imply that activation patching of subspaces is\nintrinsically unfit for interpretability. To contextualize our findings, we\nalso show what a success case looks like in a task (indirect object\nidentification) where prior manual circuit analysis informs an understanding of\nthe location of a feature. We explore the additional evidence needed to argue\nthat a patched subspace is faithful.",
            "author": [
                "Aleksandar Makelov",
                "Georg Lange",
                "Neel Nanda"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17030v2",
                "http://arxiv.org/pdf/2311.17030v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17026v1",
            "title": "When the Few Outweigh the Many: Illicit Content Recognition with\n  Few-Shot Learning",
            "updated": "2023-11-28T18:28:03Z",
            "published": "2023-11-28T18:28:03Z",
            "summary": "The anonymity and untraceability benefits of the Dark web account for the\nexponentially-increased potential of its popularity while creating a suitable\nwomb for many illicit activities, to date. Hence, in collaboration with\ncybersecurity and law enforcement agencies, research has provided approaches\nfor recognizing and classifying illicit activities with most exploiting textual\ndark web markets' content recognition; few such approaches use images that\noriginated from dark web content. This paper investigates this alternative\ntechnique for recognizing illegal activities from images. In particular, we\ninvestigate label-agnostic learning techniques like One-Shot and Few-Shot\nlearning featuring the use Siamese neural networks, a state-of-the-art approach\nin the field. Our solution manages to handle small-scale datasets with\npromising accuracy. In particular, Siamese neural networks reach 90.9% on\n20-Shot experiments over a 10-class dataset; this leads us to conclude that\nsuch models are a promising and cheaper alternative to the definition of\nautomated law-enforcing machinery over the dark web.",
            "author": [
                "G. Cascavilla",
                "G. Catolino",
                "M. Conti",
                "D. Mellios",
                "D. A. Tamburri"
            ],
            "link": [
                "http://dx.doi.org/10.5220/0012049400003555",
                "http://arxiv.org/abs/2311.17026v1",
                "http://arxiv.org/pdf/2311.17026v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.CR",
                "cs.CY",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17021v1",
            "title": "Optimal Categorical Instrumental Variables",
            "updated": "2023-11-28T18:20:05Z",
            "published": "2023-11-28T18:20:05Z",
            "summary": "This paper discusses estimation with a categorical instrumental variable in\nsettings with potentially few observations per category. The proposed\ncategorical instrumental variable estimator (CIV) leverages a regularization\nassumption that implies existence of a latent categorical variable with fixed\nfinite support achieving the same first stage fit as the observed instrument.\nIn asymptotic regimes that allow the number of observations per category to\ngrow at arbitrary small polynomial rate with the sample size, I show that when\nthe cardinality of the support of the optimal instrument is known, CIV is\nroot-n asymptotically normal, achieves the same asymptotic variance as the\noracle IV estimator that presumes knowledge of the optimal instrument, and is\nsemiparametrically efficient under homoskedasticity. Under-specifying the\nnumber of support points reduces efficiency but maintains asymptotic normality.",
            "author": [
                "Thomas Wiemann"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17021v1",
                "http://arxiv.org/pdf/2311.17021v1"
            ],
            "primary_category": "econ.EM",
            "category": [
                "econ.EM",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17133v1",
            "title": "Deployment of a Robust and Explainable Mortality Prediction Model: The\n  COVID-19 Pandemic and Beyond",
            "updated": "2023-11-28T18:15:53Z",
            "published": "2023-11-28T18:15:53Z",
            "summary": "This study investigated the performance, explainability, and robustness of\ndeployed artificial intelligence (AI) models in predicting mortality during the\nCOVID-19 pandemic and beyond. The first study of its kind, we found that\nBayesian Neural Networks (BNNs) and intelligent training techniques allowed our\nmodels to maintain performance amidst significant data shifts. Our results\nemphasize the importance of developing robust AI models capable of matching or\nsurpassing clinician predictions, even under challenging conditions. Our\nexploration of model explainability revealed that stochastic models generate\nmore diverse and personalized explanations thereby highlighting the need for AI\nmodels that provide detailed and individualized insights in real-world clinical\nsettings. Furthermore, we underscored the importance of quantifying uncertainty\nin AI models which enables clinicians to make better-informed decisions based\non reliable predictions. Our study advocates for prioritizing implementation\nscience in AI research for healthcare and ensuring that AI solutions are\npractical, beneficial, and sustainable in real-world clinical environments. By\naddressing unique challenges and complexities in healthcare settings,\nresearchers can develop AI models that effectively improve clinical practice\nand patient outcomes.",
            "author": [
                "Jacob R. Epifano",
                "Stephen Glass",
                "Ravi P. Ramachandran",
                "Sharad Patel",
                "Aaron J. Masino",
                "Ghulam Rasool"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17133v1",
                "http://arxiv.org/pdf/2311.17133v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17007v1",
            "title": "Computational Hypergraph Discovery, a Gaussian Process framework for\n  connecting the dots",
            "updated": "2023-11-28T18:02:06Z",
            "published": "2023-11-28T18:02:06Z",
            "summary": "Most scientific challenges can be framed into one of the following three\nlevels of complexity of function approximation. Type 1: Approximate an unknown\nfunction given input/output data. Type 2: Consider a collection of variables\nand functions, some of which are unknown, indexed by the nodes and hyperedges\nof a hypergraph (a generalized graph where edges can connect more than two\nvertices). Given partial observations of the variables of the hypergraph\n(satisfying the functional dependencies imposed by its structure), approximate\nall the unobserved variables and unknown functions. Type 3: Expanding on Type\n2, if the hypergraph structure itself is unknown, use partial observations of\nthe variables of the hypergraph to discover its structure and approximate its\nunknown functions. While most Computational Science and Engineering and\nScientific Machine Learning challenges can be framed as Type 1 and Type 2\nproblems, many scientific problems can only be categorized as Type 3. Despite\ntheir prevalence, these Type 3 challenges have been largely overlooked due to\ntheir inherent complexity. Although Gaussian Process (GP) methods are sometimes\nperceived as well-founded but old technology limited to Type 1 curve fitting,\ntheir scope has recently been expanded to Type 2 problems. In this paper, we\nintroduce an interpretable GP framework for Type 3 problems, targeting the\ndata-driven discovery and completion of computational hypergraphs. Our approach\nis based on a kernel generalization of Row Echelon Form reduction from linear\nsystems to nonlinear ones and variance-based analysis. Here, variables are\nlinked via GPs and those contributing to the highest data variance unveil the\nhypergraph's structure. We illustrate the scope and efficiency of the proposed\napproach with applications to (algebraic) equation discovery, network discovery\n(gene pathways, chemical, and mechanical) and raw data analysis.",
            "author": [
                "Th\u00e9o Bourdais",
                "Pau Batlle",
                "Xianjin Yang",
                "Ricardo Baptista",
                "Nicolas Rouquette",
                "Houman Owhadi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17007v1",
                "http://arxiv.org/pdf/2311.17007v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.NA",
                "cs.SI",
                "math.NA",
                "stat.ML",
                "62A09, 62H22, 65S05, 90C35, 94C15, 46E22, 62J02, 15A83, 62D20, 68R10"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17008v1",
            "title": "An Investigation of Time Reversal Symmetry in Reinforcement Learning",
            "updated": "2023-11-28T18:02:06Z",
            "published": "2023-11-28T18:02:06Z",
            "summary": "One of the fundamental challenges associated with reinforcement learning (RL)\nis that collecting sufficient data can be both time-consuming and expensive. In\nthis paper, we formalize a concept of time reversal symmetry in a Markov\ndecision process (MDP), which builds upon the established structure of\ndynamically reversible Markov chains (DRMCs) and time-reversibility in\nclassical physics. Specifically, we investigate the utility of this concept in\nreducing the sample complexity of reinforcement learning. We observe that\nutilizing the structure of time reversal in an MDP allows every environment\ntransition experienced by an agent to be transformed into a feasible\nreverse-time transition, effectively doubling the number of experiences in the\nenvironment. To test the usefulness of this newly synthesized data, we develop\na novel approach called time symmetric data augmentation (TSDA) and investigate\nits application in both proprioceptive and pixel-based state within the realm\nof off-policy, model-free RL. Empirical evaluations showcase how these\nsynthetic transitions can enhance the sample efficiency of RL agents in time\nreversible scenarios without friction or contact. We also test this method in\nmore realistic environments where these assumptions are not globally satisfied.\nWe find that TSDA can significantly degrade sample efficiency and policy\nperformance, but can also improve sample efficiency under the right conditions.\nUltimately we conclude that time symmetry shows promise in enhancing the sample\nefficiency of reinforcement learning and provide guidance when the environment\nand reward structures are of an appropriate form for TSDA to be employed\neffectively.",
            "author": [
                "Brett Barkley",
                "Amy Zhang",
                "David Fridovich-Keil"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17008v1",
                "http://arxiv.org/pdf/2311.17008v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17006v1",
            "title": "On the Impact of Sampling on Deep Sequential State Estimation",
            "updated": "2023-11-28T17:59:49Z",
            "published": "2023-11-28T17:59:49Z",
            "summary": "State inference and parameter learning in sequential models can be\nsuccessfully performed with approximation techniques that maximize the evidence\nlower bound to the marginal log-likelihood of the data distribution. These\nmethods may be referred to as Dynamical Variational Autoencoders, and our\nspecific focus lies on the deep Kalman filter. It has been shown that the ELBO\nobjective can oversimplify data representations, potentially compromising\nestimation quality. Tighter Monte Carlo objectives have been proposed in the\nliterature to enhance generative modeling performance. For instance, the IWAE\nobjective uses importance weights to reduce the variance of marginal\nlog-likelihood estimates. In this paper, importance sampling is applied to the\nDKF framework for learning deep Markov models, resulting in the IW-DKF, which\nshows an improvement in terms of log-likelihood estimates and KL divergence\nbetween the variational distribution and the transition model. The framework\nusing the sampled DKF update rule is also accommodated to address sequential\nstate and parameter estimation when working with highly non-linear\nphysics-based models. An experiment with the 3-space Lorenz attractor shows an\nenhanced generative modeling performance and also a decrease in RMSE when\nestimating the model parameters and latent states, indicating that tighter MCOs\nlead to improved state inference performance.",
            "author": [
                "Helena Calatrava",
                "Ricardo Augusto Borsoi",
                "Tales Imbiriba",
                "Pau Closas"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17006v1",
                "http://arxiv.org/pdf/2311.17006v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17002v2",
            "title": "Ranni: Taming Text-to-Image Diffusion for Accurate Instruction Following",
            "updated": "2023-11-30T09:30:19Z",
            "published": "2023-11-28T17:57:44Z",
            "summary": "Existing text-to-image (T2I) diffusion models usually struggle in\ninterpreting complex prompts, especially those with quantity, object-attribute\nbinding, and multi-subject descriptions. In this work, we introduce a semantic\npanel as the middleware in decoding texts to images, supporting the generator\nto better follow instructions. The panel is obtained through arranging the\nvisual concepts parsed from the input text by the aid of large language models,\nand then injected into the denoising network as a detailed control signal to\ncomplement the text condition. To facilitate text-to-panel learning, we come up\nwith a carefully designed semantic formatting protocol, accompanied by a\nfully-automatic data preparation pipeline. Thanks to such a design, our\napproach, which we call Ranni, manages to enhance a pre-trained T2I generator\nregarding its textual controllability. More importantly, the introduction of\nthe generative middleware brings a more convenient form of interaction (i.e.,\ndirectly adjusting the elements in the panel or using language instructions)\nand further allows users to finely customize their generation, based on which\nwe develop a practical system and showcase its potential in continuous\ngeneration and chatting-based editing. Our project page is at\nhttps://ranni-t2i.github.io/Ranni.",
            "author": [
                "Yutong Feng",
                "Biao Gong",
                "Di Chen",
                "Yujun Shen",
                "Yu Liu",
                "Jingren Zhou"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17002v2",
                "http://arxiv.org/pdf/2311.17002v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16996v1",
            "title": "Goal-conditioned Offline Planning from Curious Exploration",
            "updated": "2023-11-28T17:48:18Z",
            "published": "2023-11-28T17:48:18Z",
            "summary": "Curiosity has established itself as a powerful exploration strategy in deep\nreinforcement learning. Notably, leveraging expected future novelty as\nintrinsic motivation has been shown to efficiently generate exploratory\ntrajectories, as well as a robust dynamics model. We consider the challenge of\nextracting goal-conditioned behavior from the products of such unsupervised\nexploration techniques, without any additional environment interaction. We find\nthat conventional goal-conditioned reinforcement learning approaches for\nextracting a value function and policy fall short in this difficult offline\nsetting. By analyzing the geometry of optimal goal-conditioned value functions,\nwe relate this issue to a specific class of estimation artifacts in learned\nvalues. In order to mitigate their occurrence, we propose to combine\nmodel-based planning over learned value landscapes with a graph-based value\naggregation scheme. We show how this combination can correct both local and\nglobal artifacts, obtaining significant improvements in zero-shot goal-reaching\nperformance across diverse simulated environments.",
            "author": [
                "Marco Bagatella",
                "Georg Martius"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16996v1",
                "http://arxiv.org/pdf/2311.16996v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02179v1",
            "title": "Training Chain-of-Thought via Latent-Variable Inference",
            "updated": "2023-11-28T17:47:32Z",
            "published": "2023-11-28T17:47:32Z",
            "summary": "Large language models (LLMs) solve problems more accurately and interpretably\nwhen instructed to work out the answer step by step using a\n``chain-of-thought'' (CoT) prompt. One can also improve LLMs' performance on a\nspecific task by supervised fine-tuning, i.e., by using gradient ascent on some\ntunable parameters to maximize the average log-likelihood of correct answers\nfrom a labeled training set. Naively combining CoT with supervised tuning\nrequires supervision not just of the correct answers, but also of detailed\nrationales that lead to those answers; these rationales are expensive to\nproduce by hand. Instead, we propose a fine-tuning strategy that tries to\nmaximize the \\emph{marginal} log-likelihood of generating a correct answer\nusing CoT prompting, approximately averaging over all possible rationales. The\ncore challenge is sampling from the posterior over rationales conditioned on\nthe correct answer; we address it using a simple Markov-chain Monte Carlo\n(MCMC) expectation-maximization (EM) algorithm inspired by the self-taught\nreasoner (STaR), memoized wake-sleep, Markovian score climbing, and persistent\ncontrastive divergence. This algorithm also admits a novel control-variate\ntechnique that drives the variance of our gradient estimates to zero as the\nmodel improves. Applying our technique to GSM8K and the tasks in BIG-Bench\nHard, we find that this MCMC-EM fine-tuning technique typically improves the\nmodel's accuracy on held-out examples more than STaR or prompt-tuning with or\nwithout CoT.",
            "author": [
                "Du Phan",
                "Matthew D. Hoffman",
                "David Dohan",
                "Sholto Douglas",
                "Tuan Anh Le",
                "Aaron Parisi",
                "Pavel Sountsov",
                "Charles Sutton",
                "Sharad Vikram",
                "Rif A. Saurous"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02179v1",
                "http://arxiv.org/pdf/2312.02179v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16989v3",
            "title": "ChatGPT's One-year Anniversary: Are Open-Source Large Language Models\n  Catching up?",
            "updated": "2023-12-05T16:58:46Z",
            "published": "2023-11-28T17:44:51Z",
            "summary": "Upon its release in late 2022, ChatGPT has brought a seismic shift in the\nentire landscape of AI, both in research and commerce. Through\ninstruction-tuning a large language model (LLM) with supervised fine-tuning and\nreinforcement learning from human feedback, it showed that a model could answer\nhuman questions and follow instructions on a broad panel of tasks. Following\nthis success, interests in LLMs have intensified, with new LLMs flourishing at\nfrequent interval across academia and industry, including many start-ups\nfocused on LLMs. While closed-source LLMs (e.g., OpenAI's GPT, Anthropic's\nClaude) generally outperform their open-source counterparts, the progress on\nthe latter has been rapid with claims of achieving parity or even better on\ncertain tasks. This has crucial implications not only on research but also on\nbusiness. In this work, on the first anniversary of ChatGPT, we provide an\nexhaustive overview of this success, surveying all tasks where an open-source\nLLM has claimed to be on par or better than ChatGPT.",
            "author": [
                "Hailin Chen",
                "Fangkai Jiao",
                "Xingxuan Li",
                "Chengwei Qin",
                "Mathieu Ravaut",
                "Ruochen Zhao",
                "Caiming Xiong",
                "Shafiq Joty"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16989v3",
                "http://arxiv.org/pdf/2311.16989v3"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16984v1",
            "title": "FedECA: A Federated External Control Arm Method for Causal Inference\n  with Time-To-Event Data in Distributed Settings",
            "updated": "2023-11-28T17:35:38Z",
            "published": "2023-11-28T17:35:38Z",
            "summary": "External control arms (ECA) can inform the early clinical development of\nexperimental drugs and provide efficacy evidence for regulatory approval in\nnon-randomized settings. However, the main challenge of implementing ECA lies\nin accessing real-world data or historical clinical trials. Indeed, data\nsharing is often not feasible due to privacy considerations related to data\nleaving the original collection centers, along with pharmaceutical companies'\ncompetitive motives. In this paper, we leverage a privacy-enhancing technology\ncalled federated learning (FL) to remove some of the barriers to data sharing.\nWe introduce a federated learning inverse probability of treatment weighted\n(IPTW) method for time-to-event outcomes called FedECA which eases the\nimplementation of ECA by limiting patients' data exposure. We show with\nextensive experiments that FedECA outperforms its closest competitor,\nmatching-adjusted indirect comparison (MAIC), in terms of statistical power and\nability to balance the treatment and control groups. To encourage the use of\nsuch methods, we publicly release our code which relies on Substra, an\nopen-source FL software with proven experience in privacy-sensitive contexts.",
            "author": [
                "Jean Ogier du Terrail",
                "Quentin Klopfenstein",
                "Honghao Li",
                "Imke Mayer",
                "Nicolas Loiseau",
                "Mohammad Hallal",
                "F\u00e9lix Balazard",
                "Mathieu Andreux"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16984v1",
                "http://arxiv.org/pdf/2311.16984v1"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME",
                "cs.DC",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16977v1",
            "title": "Bidirectional Reactive Programming for Machine Learning",
            "updated": "2023-11-28T17:25:16Z",
            "published": "2023-11-28T17:25:16Z",
            "summary": "Reactive languages are dedicated to the programming of systems which interact\ncontinuously and concurrently with their environment. Values take the form of\nunbounded streams modeling the (discrete) passing of time or the sequence of\nconcurrent interactions. While conventional reactivity models recurrences\nforward in time, we introduce a symmetric reactive construct enabling backward\nrecurrences. Constraints on the latter allow to make the implementation\npractical. Machine Learning (ML) systems provide numerous motivations for all\nof this: we demonstrate that reverse-mode automatic differentiation,\nbackpropagation, batch normalization, bidirectional recurrent neural networks,\ntraining and reinforcement learning algorithms, are all naturally captured as\nbidirectional reactive programs.",
            "author": [
                "Dumitru Potop Butucaru",
                "Albert Cohen",
                "Gordon Plotkin",
                "Hugo Pompougnac"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16977v1",
                "http://arxiv.org/pdf/2311.16977v1"
            ],
            "primary_category": "cs.PL",
            "category": [
                "cs.PL",
                "cs.LG",
                "D.3; D.3.1; I.2; I.2.5"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16965v1",
            "title": "Natural Language Processing Through Transfer Learning: A Case Study on\n  Sentiment Analysis",
            "updated": "2023-11-28T17:12:06Z",
            "published": "2023-11-28T17:12:06Z",
            "summary": "Artificial intelligence and machine learning have significantly bolstered the\ntechnological world. This paper explores the potential of transfer learning in\nnatural language processing focusing mainly on sentiment analysis. The models\ntrained on the big data can also be used where data are scarce. The claim is\nthat, compared to training models from scratch, transfer learning, using\npre-trained BERT models, can increase sentiment classification accuracy. The\nstudy adopts a sophisticated experimental design that uses the IMDb dataset of\nsentimentally labelled movie reviews. Pre-processing includes tokenization and\nencoding of text data, making it suitable for NLP models. The dataset is used\non a BERT based model, measuring its performance using accuracy. The result\ncomes out to be 100 per cent accurate. Although the complete accuracy could\nappear impressive, it might be the result of overfitting or a lack of\ngeneralization. Further analysis is required to ensure the model's ability to\nhandle diverse and unseen data. The findings underscore the effectiveness of\ntransfer learning in NLP, showcasing its potential to excel in sentiment\nanalysis tasks. However, the research calls for a cautious interpretation of\nperfect accuracy and emphasizes the need for additional measures to validate\nthe model's generalization.",
            "author": [
                "Aman Yadav",
                "Abhishek Vichare"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16965v1",
                "http://arxiv.org/pdf/2311.16965v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16964v1",
            "title": "Machine learning force-field models for metallic spin glass",
            "updated": "2023-11-28T17:12:03Z",
            "published": "2023-11-28T17:12:03Z",
            "summary": "Metallic spin glass systems, such as dilute magnetic alloys, are\ncharacterized by randomly distributed local moments coupled to each other\nthrough a long-range electron-mediated effective interaction. We present a\nscalable machine learning (ML) framework for dynamical simulations of metallic\nspin glasses. A Behler-Parrinello type neural-network model, based on the\nprinciple of locality, is developed to accurately and efficiently predict\nelectron-induced local magnetic fields that drive the spin dynamics. A crucial\ncomponent of the ML model is a proper symmetry-invariant representation of\nlocal magnetic environment which is direct input to the neural net. We develop\nsuch a magnetic descriptor by incorporating the spin degrees of freedom into\nthe atom-centered symmetry function methods which are widely used in ML\nforce-field models for quantum molecular dynamics. We apply our approach to\nstudy the relaxation dynamics of an amorphous generalization of the s-d model.\nOur work highlights the promising potential of ML models for large-scale\ndynamical modeling of itinerant magnets with quenched disorder.",
            "author": [
                "Menglin Shi",
                "Sheng Zhang",
                "Gia-Wei Chern"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16964v1",
                "http://arxiv.org/pdf/2311.16964v1"
            ],
            "primary_category": "cond-mat.dis-nn",
            "category": [
                "cond-mat.dis-nn",
                "cond-mat.mtrl-sci",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16956v1",
            "title": "Adaptive Step Sizes for Preconditioned Stochastic Gradient Descent",
            "updated": "2023-11-28T17:03:56Z",
            "published": "2023-11-28T17:03:56Z",
            "summary": "This paper proposes a novel approach to adaptive step sizes in stochastic\ngradient descent (SGD) by utilizing quantities that we have identified as\nnumerically traceable -- the Lipschitz constant for gradients and a concept of\nthe local variance in search directions. Our findings yield a nearly\nhyperparameter-free algorithm for stochastic optimization, which has provable\nconvergence properties when applied to quadratic problems and exhibits truly\nproblem adaptive behavior on classical image classification tasks. Our\nframework enables the potential inclusion of a preconditioner, thereby enabling\nthe implementation of adaptive step sizes for stochastic second-order\noptimization methods.",
            "author": [
                "Frederik K\u00f6hne",
                "Leonie Kreis",
                "Anton Schiela",
                "Roland Herzog"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16956v1",
                "http://arxiv.org/pdf/2311.16956v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16948v1",
            "title": "End-to-end Reinforcement Learning for Time-Optimal Quadcopter Flight",
            "updated": "2023-11-28T16:50:57Z",
            "published": "2023-11-28T16:50:57Z",
            "summary": "Aggressive time-optimal control of quadcopters poses a significant challenge\nin the field of robotics. The state-of-the-art approach leverages reinforcement\nlearning (RL) to train optimal neural policies. However, a critical hurdle is\nthe sim-to-real gap, often addressed by employing a robust inner loop\ncontroller -an abstraction that, in theory, constrains the optimality of the\ntrained controller, necessitating margins to counter potential disturbances. In\ncontrast, our novel approach introduces high-speed quadcopter control using\nend-to-end RL (E2E) that gives direct motor commands. To bridge the reality\ngap, we incorporate a learned residual model and an adaptive method that can\ncompensate for modeling errors in thrust and moments. We compare our E2E\napproach against a state-of-the-art network that commands thrust and body rates\nto an INDI inner loop controller, both in simulated and real-world flight. E2E\nshowcases a significant 1.39-second advantage in simulation and a 0.17-second\nedge in real-world testing, highlighting end-to-end reinforcement learning's\npotential. The performance drop observed from simulation to reality shows\npotential for further improvement, including refining strategies to address the\nreality gap or exploring offline reinforcement learning with real flight data.",
            "author": [
                "Robin Ferede",
                "Christophe De Wagter",
                "Dario Izzo",
                "Guido C. H. E. de Croon"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16948v1",
                "http://arxiv.org/pdf/2311.16948v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16943v1",
            "title": "Image segmentation with traveling waves in an exactly solvable recurrent\n  neural network",
            "updated": "2023-11-28T16:46:44Z",
            "published": "2023-11-28T16:46:44Z",
            "summary": "We study image segmentation using spatiotemporal dynamics in a recurrent\nneural network where the state of each unit is given by a complex number. We\nshow that this network generates sophisticated spatiotemporal dynamics that can\neffectively divide an image into groups according to a scene's structural\ncharacteristics. Using an exact solution of the recurrent network's dynamics,\nwe present a precise description of the mechanism underlying object\nsegmentation in this network, providing a clear mathematical interpretation of\nhow the network performs this task. We then demonstrate a simple algorithm for\nobject segmentation that generalizes across inputs ranging from simple\ngeometric objects in grayscale images to natural images. Object segmentation\nacross all images is accomplished with one recurrent neural network that has a\nsingle, fixed set of weights. This demonstrates the expressive potential of\nrecurrent neural networks when constructed using a mathematical approach that\nbrings together their structure, dynamics, and computation.",
            "author": [
                "Luisa H. B. Liboni",
                "Roberto C. Budzinski",
                "Alexandra N. Busch",
                "Sindy L\u00f6we",
                "Thomas A. Keller",
                "Max Welling",
                "Lyle E. Muller"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16943v1",
                "http://arxiv.org/pdf/2311.16943v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG",
                "cs.NE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16941v1",
            "title": "Debiasing Multimodal Models via Causal Information Minimization",
            "updated": "2023-11-28T16:46:14Z",
            "published": "2023-11-28T16:46:14Z",
            "summary": "Most existing debiasing methods for multimodal models, including causal\nintervention and inference methods, utilize approximate heuristics to represent\nthe biases, such as shallow features from early stages of training or unimodal\nfeatures for multimodal tasks like VQA, etc., which may not be accurate. In\nthis paper, we study bias arising from confounders in a causal graph for\nmultimodal data and examine a novel approach that leverages causally-motivated\ninformation minimization to learn the confounder representations. Robust\npredictive features contain diverse information that helps a model generalize\nto out-of-distribution data. Hence, minimizing the information content of\nfeatures obtained from a pretrained biased model helps learn the simplest\npredictive features that capture the underlying data distribution. We treat\nthese features as confounder representations and use them via methods motivated\nby causal theory to remove bias from models. We find that the learned\nconfounder representations indeed capture dataset biases, and the proposed\ndebiasing methods improve out-of-distribution (OOD) performance on multiple\nmultimodal datasets without sacrificing in-distribution performance.\nAdditionally, we introduce a novel metric to quantify the sufficiency of\nspurious features in models' predictions that further demonstrates the\neffectiveness of our proposed methods. Our code is available at:\nhttps://github.com/Vaidehi99/CausalInfoMin",
            "author": [
                "Vaidehi Patil",
                "Adyasha Maharana",
                "Mohit Bansal"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16941v1",
                "http://arxiv.org/pdf/2311.16941v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CL",
                "cs.CV",
                "stat.ME"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16940v1",
            "title": "FP-Fed: Privacy-Preserving Federated Detection of Browser Fingerprinting",
            "updated": "2023-11-28T16:43:17Z",
            "published": "2023-11-28T16:43:17Z",
            "summary": "Browser fingerprinting often provides an attractive alternative to\nthird-party cookies for tracking users across the web. In fact, the increasing\nrestrictions on third-party cookies placed by common web browsers and recent\nregulations like the GDPR may accelerate the transition. To counter browser\nfingerprinting, previous work proposed several techniques to detect its\nprevalence and severity. However, these rely on 1) centralized web crawls\nand/or 2) computationally intensive operations to extract and process signals\n(e.g., information-flow and static analysis). To address these limitations, we\npresent FP-Fed, the first distributed system for browser fingerprinting\ndetection. Using FP-Fed, users can collaboratively train on-device models based\non their real browsing patterns, without sharing their training data with a\ncentral entity, by relying on Differentially Private Federated Learning\n(DP-FL). To demonstrate its feasibility and effectiveness, we evaluate FP-Fed's\nperformance on a set of 18.3k popular websites with different privacy levels,\nnumbers of participants, and features extracted from the scripts. Our\nexperiments show that FP-Fed achieves reasonably high detection performance and\ncan perform both training and inference efficiently, on-device, by only relying\non runtime signals extracted from the execution trace, without requiring any\nresource-intensive operation.",
            "author": [
                "Meenatchi Sundaram Muthu Selva Annamalai",
                "Igor Bilogrevic",
                "Emiliano De Cristofaro"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16940v1",
                "http://arxiv.org/pdf/2311.16940v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16926v3",
            "title": "LLaFS: When Large-Language Models Meet Few-Shot Segmentation",
            "updated": "2023-12-05T10:04:37Z",
            "published": "2023-11-28T16:31:27Z",
            "summary": "This paper proposes LLaFS, the first attempt to leverage large language\nmodels (LLMs) in few-shot segmentation. In contrast to the conventional\nfew-shot segmentation methods that only rely on the limited and biased\ninformation from the annotated support images, LLaFS leverages the vast prior\nknowledge gained by LLM as an effective supplement and directly uses the LLM to\nsegment images in a few-shot manner. To enable the text-based LLM to handle\nimage-related tasks, we carefully design an input instruction that allows the\nLLM to produce segmentation results represented as polygons, and propose a\nregion-attribute table to simulate the human visual mechanism and provide\nmulti-modal guidance. We also synthesize pseudo samples and use curriculum\nlearning for pretraining to augment data and achieve better optimization. LLaFS\nachieves state-of-the-art results on multiple datasets, showing the potential\nof using LLMs for few-shot computer vision tasks. Code will be available at\nhttps://github.com/lanyunzhu99/LLaFS.",
            "author": [
                "Lanyun Zhu",
                "Tianrun Chen",
                "Deyi Ji",
                "Jieping Ye",
                "Jun Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16926v3",
                "http://arxiv.org/pdf/2311.16926v3"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16918v1",
            "title": "RichDreamer: A Generalizable Normal-Depth Diffusion Model for Detail\n  Richness in Text-to-3D",
            "updated": "2023-11-28T16:22:33Z",
            "published": "2023-11-28T16:22:33Z",
            "summary": "Lifting 2D diffusion for 3D generation is a challenging problem due to the\nlack of geometric prior and the complex entanglement of materials and lighting\nin natural images. Existing methods have shown promise by first creating the\ngeometry through score-distillation sampling (SDS) applied to rendered surface\nnormals, followed by appearance modeling. However, relying on a 2D RGB\ndiffusion model to optimize surface normals is suboptimal due to the\ndistribution discrepancy between natural images and normals maps, leading to\ninstability in optimization. In this paper, recognizing that the normal and\ndepth information effectively describe scene geometry and be automatically\nestimated from images, we propose to learn a generalizable Normal-Depth\ndiffusion model for 3D generation. We achieve this by training on the\nlarge-scale LAION dataset together with the generalizable image-to-depth and\nnormal prior models. In an attempt to alleviate the mixed illumination effects\nin the generated materials, we introduce an albedo diffusion model to impose\ndata-driven constraints on the albedo component. Our experiments show that when\nintegrated into existing text-to-3D pipelines, our models significantly enhance\nthe detail richness, achieving state-of-the-art results. Our project page is\nhttps://lingtengqiu.github.io/RichDreamer/.",
            "author": [
                "Lingteng Qiu",
                "Guanying Chen",
                "Xiaodong Gu",
                "Qi Zuo",
                "Mutian Xu",
                "Yushuang Wu",
                "Weihao Yuan",
                "Zilong Dong",
                "Liefeng Bo",
                "Xiaoguang Han"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16918v1",
                "http://arxiv.org/pdf/2311.16918v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16914v1",
            "title": "Brain-ID: Learning Robust Feature Representations for Brain Imaging",
            "updated": "2023-11-28T16:16:10Z",
            "published": "2023-11-28T16:16:10Z",
            "summary": "Recent learning-based approaches have made astonishing advances in calibrated\nmedical imaging like computerized tomography, yet they struggle to generalize\nin uncalibrated modalities -- notoriously magnetic resonance imaging (MRI),\nwhere performance is highly sensitive to the differences in MR contrast,\nresolution, and orientation between the training and testing data. This\nprevents broad applicability to the diverse clinical acquisition protocols in\nthe real world. We introduce Brain-ID, a robust feature representation learning\nstrategy for brain imaging, which is contrast-agnostic, and robust to the brain\nanatomy of each subject regardless of the appearance of acquired images (i.e.,\ndeformation, contrast, resolution, orientation, artifacts, etc). Brain-ID is\ntrained entirely on synthetic data, and easily adapts to downstream tasks with\nour proposed simple one-layer solution. We validate the robustness of Brain-ID\nfeatures, and evaluate their performance in a variety of downstream\napplications, including both contrast-independent (anatomy\nreconstruction/contrast synthesis, brain segmentation), and contrast-dependent\n(super-resolution, bias field estimation) tasks. Extensive experiments on 6\npublic datasets demonstrate that Brain-ID achieves state-of-the-art performance\nin all tasks, and more importantly, preserves its performance when only limited\ntraining data is available.",
            "author": [
                "Peirong Liu",
                "Oula Puonti",
                "Xiaoling Hu",
                "Daniel C. Alexander",
                "Juan Eugenio Iglesias"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16914v1",
                "http://arxiv.org/pdf/2311.16914v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16909v1",
            "title": "Multinomial belief networks",
            "updated": "2023-11-28T16:12:50Z",
            "published": "2023-11-28T16:12:50Z",
            "summary": "A Bayesian approach to machine learning is attractive when we need to\nquantify uncertainty, deal with missing observations, when samples are scarce,\nor when the data is sparse. All of these commonly apply when analysing\nhealthcare data. To address these analytical requirements, we propose a deep\ngenerative model for multinomial count data where both the weights and hidden\nunits of the network are Dirichlet distributed. A Gibbs sampling procedure is\nformulated that takes advantage of a series of augmentation relations,\nanalogous to the Zhou-Cong-Chen model. We apply the model on small handwritten\ndigits, and a large experimental dataset of DNA mutations in cancer, and we\nshow how the model is able to extract biologically meaningful meta-signatures\nin a fully data-driven way.",
            "author": [
                "H. C. Donker",
                "D. Neijzen",
                "G. A. Lunter"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16909v1",
                "http://arxiv.org/pdf/2311.16909v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG",
                "stat.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17129v1",
            "title": "Feedback RoI Features Improve Aerial Object Detection",
            "updated": "2023-11-28T16:09:09Z",
            "published": "2023-11-28T16:09:09Z",
            "summary": "Neuroscience studies have shown that the human visual system utilizes\nhigh-level feedback information to guide lower-level perception, enabling\nadaptation to signals of different characteristics. In light of this, we\npropose Feedback multi-Level feature Extractor (Flex) to incorporate a similar\nmechanism for object detection. Flex refines feature selection based on\nimage-wise and instance-level feedback information in response to image quality\nvariation and classification uncertainty. Experimental results show that Flex\noffers consistent improvement to a range of existing SOTA methods on the\nchallenging aerial object detection datasets including DOTA-v1.0, DOTA-v1.5,\nand HRSC2016. Although the design originates in aerial image detection, further\nexperiments on MS COCO also reveal our module's efficacy in general detection\nmodels. Quantitative and qualitative analyses indicate that the improvements\nare closely related to image qualities, which match our motivation.",
            "author": [
                "Botao Ren",
                "Botian Xu",
                "Tengyu Liu",
                "Jingyi Wang",
                "Zhidong Deng"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17129v1",
                "http://arxiv.org/pdf/2311.17129v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03739v1",
            "title": "Syntax-Informed Interactive Model for Comprehensive Aspect-Based\n  Sentiment Analysis",
            "updated": "2023-11-28T16:03:22Z",
            "published": "2023-11-28T16:03:22Z",
            "summary": "Aspect-based sentiment analysis (ABSA), a nuanced task in text analysis,\nseeks to discern sentiment orientation linked to specific aspect terms in text.\nTraditional approaches often overlook or inadequately model the explicit\nsyntactic structures of sentences, crucial for effective aspect term\nidentification and sentiment determination. Addressing this gap, we introduce\nan innovative model: Syntactic Dependency Enhanced Multi-Task Interaction\nArchitecture (SDEMTIA) for comprehensive ABSA. Our approach innovatively\nexploits syntactic knowledge (dependency relations and types) using a\nspecialized Syntactic Dependency Embedded Interactive Network (SDEIN). We also\nincorporate a novel and efficient message-passing mechanism within a multi-task\nlearning framework to bolster learning efficacy. Our extensive experiments on\nbenchmark datasets showcase our model's superiority, significantly surpassing\nexisting methods. Additionally, incorporating BERT as an auxiliary feature\nextractor further enhances our model's performance.",
            "author": [
                "Ullman Galen",
                "Frey Lee",
                "Woods Ali"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03739v1",
                "http://arxiv.org/pdf/2312.03739v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16898v1",
            "title": "On the existence of optimal multi-valued decoders and their accuracy\n  bounds for undersampled inverse problems",
            "updated": "2023-11-28T15:51:00Z",
            "published": "2023-11-28T15:51:00Z",
            "summary": "Undersampled inverse problems occur everywhere in the sciences including\nmedical imaging, radar, astronomy etc., yielding underdetermined linear or\nnon-linear reconstruction problems. There are now a myriad of techniques to\ndesign decoders that can tackle such problems, ranging from optimization based\napproaches, such as compressed sensing, to deep learning (DL), and variants in\nbetween the two techniques. The variety of methods begs for a unifying approach\nto determine the existence of optimal decoders and fundamental accuracy bounds,\nin order to facilitate a theoretical and empirical understanding of the\nperformance of existing and future methods. Such a theory must allow for both\nsingle-valued and multi-valued decoders, as underdetermined inverse problems\ntypically have multiple solutions. Indeed, multi-valued decoders arise due to\nnon-uniqueness of minimizers in optimisation problems, such as in compressed\nsensing, and for DL based decoders in generative adversarial models, such as\ndiffusion models and ensemble models. In this work we provide a framework for\nassessing the lowest possible reconstruction accuracy in terms of worst- and\naverage-case errors. The universal bounds bounds only depend on the measurement\nmodel $F$, the model class $\\mathcal{M}_1 \\subseteq \\mathcal{X}$ and the noise\nmodel $\\mathcal{E}$. For linear $F$ these bounds depend on its kernel, and in\nthe non-linear case the concept of kernel is generalized for undersampled\nsettings. Additionally, we provide multi-valued variational solutions that\nobtain the lowest possible reconstruction error.",
            "author": [
                "Nina Maria Gottschling",
                "Paolo Campodonico",
                "Vegard Antun",
                "Anders C. Hansen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16898v1",
                "http://arxiv.org/pdf/2311.16898v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16896v2",
            "title": "65 GOPS/neuron Photonic Tensor Core with Thin-film Lithium Niobate\n  Photonics",
            "updated": "2023-12-01T03:06:18Z",
            "published": "2023-11-28T15:50:19Z",
            "summary": "Photonics offers a transformative approach to artificial intelligence (AI)\nand neuromorphic computing by providing low latency, high bandwidth, and\nenergy-efficient computations. Here, we introduce a photonic tensor core\nprocessor enabled by time-multiplexed inputs and charge-integrated outputs.\nThis fully integrated processor, comprising only two thin-film lithium niobate\n(TFLN) modulators, a III-V laser, and a charge-integration photoreceiver, can\nimplement an entire layer of a neural network. It can execute 65 billion\noperations per second (GOPS) per neuron, including simultaneous weight\nupdates-a hitherto unachieved speed. Our processor stands out from conventional\nphotonic processors, which have static weights set during training, as it\nsupports fast \"hardware-in-the-loop\" training, and can dynamically adjust the\ninputs (fan-in) and outputs (fan-out) within a layer, thereby enhancing its\nversatility. Our processor can perform large-scale dot-product operations with\nvector dimensions up to 131,072. Furthermore, it successfully classifies\n(supervised learning) and clusters (unsupervised learning) 112*112-pixel images\nafter \"hardware-in-the-loop\" training. To handle \"hardware-in-the-loop\"\ntraining for clustering AI tasks, we provide a solution for multiplications\ninvolving two negative numbers based on our processor.",
            "author": [
                "Zhongjin Lin",
                "Bhavin J. Shastri",
                "Shangxuan Yu",
                "Jingxiang Song",
                "Yuntao Zhu",
                "Arman Safarnejadian",
                "Wangning Cai",
                "Yanmei Lin",
                "Wei Ke",
                "Mustafa Hammood",
                "Tianye Wang",
                "Mengyue Xu",
                "Zibo Zheng",
                "Mohammed Al-Qadasi",
                "Omid Esmaeeli",
                "Mohamed Rahim",
                "Grzegorz Pakulski",
                "Jens Schmid",
                "Pedro Barrios",
                "Weihong Jiang",
                "Hugh Morison",
                "Matthew Mitchell",
                "Xiaogang Qiang",
                "Xun Guan",
                "Nicolas A. F. Jaeger",
                "Leslie A. n Rusch",
                "Sudip Shekhar",
                "Wei Shi",
                "Siyuan Yu",
                "Xinlun Cai",
                "Lukas Chrostowski"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16896v2",
                "http://arxiv.org/pdf/2311.16896v2"
            ],
            "primary_category": "physics.optics",
            "category": [
                "physics.optics",
                "cs.ET",
                "physics.app-ph",
                "78A05"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16895v1",
            "title": "Optimization Theory Based Deep Reinforcement Learning for Resource\n  Allocation in Ultra-Reliable Wireless Networked Control Systems",
            "updated": "2023-11-28T15:49:29Z",
            "published": "2023-11-28T15:49:29Z",
            "summary": "The design of Wireless Networked Control System (WNCS) requires addressing\ncritical interactions between control and communication systems with minimal\ncomplexity and communication overhead while providing ultra-high reliability.\nThis paper introduces a novel optimization theory based deep reinforcement\nlearning (DRL) framework for the joint design of controller and communication\nsystems. The objective of minimum power consumption is targeted while\nsatisfying the schedulability and rate constraints of the communication system\nin the finite blocklength regime and stability constraint of the control\nsystem. Decision variables include the sampling period in the control system,\nand blocklength and packet error probability in the communication system. The\nproposed framework contains two stages: optimization theory and DRL. In the\noptimization theory stage, following the formulation of the joint optimization\nproblem, optimality conditions are derived to find the mathematical relations\nbetween the optimal values of the decision variables. These relations allow the\ndecomposition of the problem into multiple building blocks. In the DRL stage,\nthe blocks that are simplified but not tractable are replaced by DRL. Via\nextensive simulations, the proposed optimization theory based DRL approach is\ndemonstrated to outperform the optimization theory and pure DRL based\napproaches, with close to optimal performance and much lower complexity.",
            "author": [
                "Hamida Qumber Ali",
                "Amirhassan Babazadeh Darabi",
                "Sinem Coleri"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16895v1",
                "http://arxiv.org/pdf/2311.16895v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.AI",
                "cs.IT",
                "cs.SY",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16894v1",
            "title": "Dendrogram distance: an evaluation metric for generative networks using\n  hierarchical clustering",
            "updated": "2023-11-28T15:46:12Z",
            "published": "2023-11-28T15:46:12Z",
            "summary": "We present a novel metric for generative modeling evaluation, focusing\nprimarily on generative networks. The method uses dendrograms to represent real\nand fake data, allowing for the divergence between training and generated\nsamples to be computed. This metric focus on mode collapse, targeting\ngenerators that are not able to capture all modes in the training set. To\nevaluate the proposed method it is introduced a validation scheme based on\nsampling from real datasets, therefore the metric is evaluated in a controlled\nenvironment and proves to be competitive with other state-of-the-art\napproaches.",
            "author": [
                "Gustavo Sutter Carvalho",
                "Moacir Antonelli Ponti"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16894v1",
                "http://arxiv.org/pdf/2311.16894v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16892v1",
            "title": "Enhancing Item-level Bundle Representation for Bundle Recommendation",
            "updated": "2023-11-28T15:43:46Z",
            "published": "2023-11-28T15:43:46Z",
            "summary": "Bundle recommendation approaches offer users a set of related items on a\nparticular topic. The current state-of-the-art (SOTA) method utilizes\ncontrastive learning to learn representations at both the bundle and item\nlevels. However, due to the inherent difference between the bundle-level and\nitem-level preferences, the item-level representations may not receive\nsufficient information from the bundle affiliations to make accurate\npredictions. In this paper, we propose a novel approach EBRec, short of\nEnhanced Bundle Recommendation, which incorporates two enhanced modules to\nexplore inherent item-level bundle representations. First, we propose to\nincorporate the bundle-user-item (B-U-I) high-order correlations to explore\nmore collaborative information, thus to enhance the previous bundle\nrepresentation that solely relies on the bundle-item affiliation information.\nSecond, we further enhance the B-U-I correlations by augmenting the observed\nuser-item interactions with interactions generated from pre-trained models,\nthus improving the item-level bundle representations. We conduct extensive\nexperiments on three public datasets, and the results justify the effectiveness\nof our approach as well as the two core modules. Codes and datasets are\navailable at https://github.com/answermycode/EBRec.",
            "author": [
                "Xiaoyu Du",
                "Kun Qian",
                "Yunshan Ma",
                "Xinguang Xiang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16892v1",
                "http://arxiv.org/pdf/2311.16892v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16888v1",
            "title": "Technological Challenges of Ambient Serious Games in Higher Education",
            "updated": "2023-11-28T15:38:47Z",
            "published": "2023-11-28T15:38:47Z",
            "summary": "Naturally, university courses should be designed to attract students,\nengaging them to achieve learning goals. Toward this end, the use of Serious\nGames has been proposed in the literature. To address positive effects, such as\ncontent memorability and attendance rates, we propose Ambient Serious Games as\ngames embedded in a computer-enriched environment, which is only partially\nperceived mentally by players. In this paper, we describe five technological\nkey challenges that must be overcome to seamlessly and beneficially integrate\nan Ambient Serious Game into teaching. These challenges, derived from a\nscenario, focus on the technological provision and conduct of such games based\non a software platform. They include (1) the integration of physical smart\nlearning objects in heterogeneous environments under dynamic constraints, (2)\nthe representation of abstract subject matter using smart learning objects, (3)\nthe guided or automatic connection of all involved components, (4) the\nexplanation of the components, their interaction, as well as the serious game\nitself, and (5) feedback on the game state.",
            "author": [
                "Lea C. Brandl",
                "B\u00f6rge Kordts",
                "Andreas Schrader"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16888v1",
                "http://arxiv.org/pdf/2311.16888v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16883v2",
            "title": "Compressing the Backward Pass of Large-Scale Neural Architectures by\n  Structured Activation Pruning",
            "updated": "2023-11-29T14:41:36Z",
            "published": "2023-11-28T15:31:31Z",
            "summary": "The rise of Deep Neural Networks (DNNs) has led to an increase in model size\nand complexity, straining the memory capacity of GPUs. Sparsity in DNNs,\ncharacterized as structural or ephemeral, has gained attention as a solution.\nThis work focuses on ephemeral sparsity, aiming to reduce memory consumption\nduring training. It emphasizes the significance of activations, an often\noverlooked component, and their role in memory usage. This work employs\nstructured pruning in Block Sparse Compressed Row (BSR) format in combination\nwith a magnitude-based criterion to efficiently prune activations. We\nfurthermore introduce efficient block-sparse operators for GPUs and showcase\ntheir effectiveness, as well as the superior compression offered by block\nsparsity. We report the effectiveness of activation pruning by evaluating\ntraining speed, accuracy, and memory usage of large-scale neural architectures\non the example of ResMLP on image classification tasks. As a result, we observe\na memory reduction of up to 32% while maintaining accuracy. Ultimately, our\napproach aims to democratize large-scale model training, reduce GPU\nrequirements, and address ecological concerns.",
            "author": [
                "Daniel Barley",
                "Holger Fr\u00f6ning"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16883v2",
                "http://arxiv.org/pdf/2311.16883v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.PF"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16882v1",
            "title": "Optimisation-Based Multi-Modal Semantic Image Editing",
            "updated": "2023-11-28T15:31:11Z",
            "published": "2023-11-28T15:31:11Z",
            "summary": "Image editing affords increased control over the aesthetics and content of\ngenerated images. Pre-existing works focus predominantly on text-based\ninstructions to achieve desired image modifications, which limit edit precision\nand accuracy. In this work, we propose an inference-time editing optimisation,\ndesigned to extend beyond textual edits to accommodate multiple editing\ninstruction types (e.g. spatial layout-based; pose, scribbles, edge maps). We\npropose to disentangle the editing task into two competing subtasks: successful\nlocal image modifications and global content consistency preservation, where\nsubtasks are guided through two dedicated loss functions. By allowing to adjust\nthe influence of each loss function, we build a flexible editing solution that\ncan be adjusted to user preferences. We evaluate our method using text, pose\nand scribble edit conditions, and highlight our ability to achieve complex\nedits, through both qualitative and quantitative experiments.",
            "author": [
                "Bowen Li",
                "Yongxin Yang",
                "Steven McDonagh",
                "Shifeng Zhang",
                "Petru-Daniel Tudosiu",
                "Sarah Parisot"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16882v1",
                "http://arxiv.org/pdf/2311.16882v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16877v1",
            "title": "Imputation using training labels and classification via label imputation",
            "updated": "2023-11-28T15:26:09Z",
            "published": "2023-11-28T15:26:09Z",
            "summary": "Missing data is a common problem in practical settings. Various imputation\nmethods have been developed to deal with missing data. However, even though the\nlabel is usually available in the training data, the common practice of\nimputation usually only relies on the input and ignores the label. In this\nwork, we illustrate how stacking the label into the input can significantly\nimprove the imputation of the input. In addition, we propose a classification\nstrategy that initializes the predicted test label with missing values and\nstacks the label with the input for imputation. This allows imputing the label\nand the input at the same time. Also, the technique is capable of handling data\ntraining with missing labels without any prior imputation and is applicable to\ncontinuous, categorical, or mixed-type data. Experiments show promising results\nin terms of accuracy.",
            "author": [
                "Thu Nguyen",
                "P\u00e5l Halvorsen",
                "Michael A. Riegler"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16877v1",
                "http://arxiv.org/pdf/2311.16877v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16876v1",
            "title": "Digital Twin-Enhanced Deep Reinforcement Learning for Resource\n  Management in Networks Slicing",
            "updated": "2023-11-28T15:25:14Z",
            "published": "2023-11-28T15:25:14Z",
            "summary": "Network slicing-based communication systems can dynamically and efficiently\nallocate resources for diversified services. However, due to the limitation of\nthe network interface on channel access and the complexity of the resource\nallocation, it is challenging to achieve an acceptable solution in the\npractical system without precise prior knowledge of the dynamics probability\nmodel of the service requests. Existing work attempts to solve this problem\nusing deep reinforcement learning (DRL), however, such methods usually require\na lot of interaction with the real environment in order to achieve good\nresults. In this paper, a framework consisting of a digital twin and\nreinforcement learning agents is present to handle the issue. Specifically, we\npropose to use the historical data and the neural networks to build a digital\ntwin model to simulate the state variation law of the real environment. Then,\nwe use the data generated by the network slicing environment to calibrate the\ndigital twin so that it is in sync with the real environment. Finally, DRL for\nslice optimization optimizes its own performance in this virtual\npre-verification environment. We conducted an exhaustive verification of the\nproposed digital twin framework to confirm its scalability. Specifically, we\npropose to use loss landscapes to visualize the generalization of DRL\nsolutions. We explore a distillation-based optimization scheme for lightweight\nslicing strategies. In addition, we also extend the framework to offline\nreinforcement learning, where solutions can be used to obtain intelligent\ndecisions based solely on historical data. Numerical simulation experiments\nshow that the proposed digital twin can significantly improve the performance\nof the slice optimization strategy.",
            "author": [
                "Zhengming Zhang",
                "Yongming Huang",
                "Cheng Zhang",
                "Qingbi Zheng",
                "Luxi Yang",
                "Xiaohu You"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16876v1",
                "http://arxiv.org/pdf/2311.16876v1"
            ],
            "primary_category": "cs.NI",
            "category": [
                "cs.NI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16872v1",
            "title": "A unified weighting framework for evaluating nearest neighbour\n  classification",
            "updated": "2023-11-28T15:24:02Z",
            "published": "2023-11-28T15:24:02Z",
            "summary": "We present the first comprehensive and large-scale evaluation of classical\n(NN), fuzzy (FNN) and fuzzy rough (FRNN) nearest neighbour classification. We\nshow that existing proposals for nearest neighbour weighting can be\nstandardised in the form of kernel functions, applied to the distance values\nand/or ranks of the nearest neighbours of a test instance. Furthermore, we\nidentify three commonly used distance functions and four scaling measures. We\nsystematically evaluate these choices on a collection of 85 real-life\nclassification datasets. We find that NN, FNN and FRNN all perform best with\nBoscovich distance. NN and FRNN perform best with a combination of Samworth\nrank- and distance weights and scaling by the mean absolute deviation around\nthe median ($r_1$), the standard deviaton ($r_2$) or the interquartile range\n($r_{\\infty}^*$), while FNN performs best with only Samworth distance-weights\nand $r_1$- or $r_2$-scaling. We also introduce a new kernel based on fuzzy\nYager negation, and show that NN achieves comparable performance with Yager\ndistance-weights, which are simpler to implement than a combination of Samworth\ndistance- and rank-weights. Finally, we demonstrate that FRNN generally\noutperforms NN, which in turns performs systematically better than FNN.",
            "author": [
                "Oliver Urs Lenz",
                "Henri Bollaert",
                "Chris Cornelis"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16872v1",
                "http://arxiv.org/pdf/2311.16872v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16866v1",
            "title": "Quantum Metric Learning for New Physics Searches at the LHC",
            "updated": "2023-11-28T15:12:43Z",
            "published": "2023-11-28T15:12:43Z",
            "summary": "In the NISQ (Noisy intermediate-scale quantum) area, Quantum computers can be\nutilized for deep learning by treating variational quantum circuits as neural\nnetwork models. This can be achieved by first encoding the input data onto\nquantum computers using nonparametric unitary gates. An alternative approach is\nto train the data encoding to map input data from different classes to\nseparated locations in the Hilbert space. The separation is achieved with\nmetric loss functions, hence the naming ``Quantum Metric Learning\". With the\nlimited number of qubits in the NISQ area, this approach works naturally as a\nhybrid classical-quantum computation enabling embedding of high-dimensional\nfeature data into a small number of qubits. Here, we consider an example of the\nglobal QCD color structure of hard b-jets emerging from color singlet scalar\ndecays to optimize the signal to background discrimination with a hybrid\nclassical-quantum metric learning. Due to the sparsity of data, self-supervised\nmethods with data augmentation have been utilized so far. Compared to the this\nclassical self-supervised approach, our hybrid method shows the better\nclassification performance without data augmentations. We emphasize that\nperformance enhancements independent of data augmentation techniques are devoid\nof the artificial risks introduced by data augmentation.",
            "author": [
                "A. Hammad",
                "Kyoungchul Kong",
                "Myeonghun Park",
                "Soyoung Shim"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16866v1",
                "http://arxiv.org/pdf/2311.16866v1"
            ],
            "primary_category": "hep-ph",
            "category": [
                "hep-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16863v1",
            "title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
            "updated": "2023-11-28T15:09:36Z",
            "published": "2023-11-28T15:09:36Z",
            "summary": "Recent years have seen a surge in the popularity of commercial AI products\nbased on generative, multi-purpose AI systems promising a unified approach to\nbuilding machine learning (ML) models into technology. However, this ambition\nof \"generality\" comes at a steep cost to the environment, given the amount of\nenergy these systems require and the amount of carbon that they emit. In this\nwork, we propose the first systematic comparison of the ongoing inference cost\nof various categories of ML systems, covering both task-specific (i.e.\nfinetuned models that carry out a single task) and `general-purpose' models,\n(i.e. those trained for multiple tasks). We measure deployment cost as the\namount of energy and carbon required to perform 1,000 inferences on\nrepresentative benchmark dataset using these models. We find that\nmulti-purpose, generative architectures are orders of magnitude more expensive\nthan task-specific systems for a variety of tasks, even when controlling for\nthe number of model parameters. We conclude with a discussion around the\ncurrent trend of deploying multi-purpose generative ML systems, and caution\nthat their utility should be more intentionally weighed against increased costs\nin terms of energy and emissions. All the data from our study can be accessed\nvia an interactive demo to carry out further exploration and analysis.",
            "author": [
                "Alexandra Sasha Luccioni",
                "Yacine Jernite",
                "Emma Strubell"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16863v1",
                "http://arxiv.org/pdf/2311.16863v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16860v2",
            "title": "Data-efficient operator learning for solving high Mach number fluid flow\n  problems",
            "updated": "2023-12-04T13:46:17Z",
            "published": "2023-11-28T15:07:25Z",
            "summary": "We consider the problem of using SciML to predict solutions of high Mach\nfluid flows over irregular geometries. In this setting, data is limited, and so\nit is desirable for models to perform well in the low-data setting. We show\nthat Neural Basis Functions (NBF), which learns a basis of behavior modes from\nthe data and then uses this basis to make predictions, is more effective than a\nbasis-unaware baseline model. In addition, we identify continuing challenges in\nthe space of predicting solutions for this type of problem.",
            "author": [
                "Noah Ford",
                "Victor J. Leon",
                "Honest Mrema",
                "Jeffrey Gilbert",
                "Alexander New"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16860v2",
                "http://arxiv.org/pdf/2311.16860v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.NA",
                "math.NA",
                "physics.flu-dyn"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16856v1",
            "title": "Attentional Graph Neural Networks for Robust Massive Network\n  Localization",
            "updated": "2023-11-28T15:05:13Z",
            "published": "2023-11-28T15:05:13Z",
            "summary": "Graph neural networks (GNNs) have gained significant popularity for\nclassification tasks in machine learning, yet their applications to regression\nproblems remain limited. Concurrently, attention mechanisms have emerged as\npowerful tools in sequential learning tasks. In this paper, we employ GNNs and\nattention mechanisms to address a classical but challenging nonlinear\nregression problem: network localization. We propose a novel GNN-based network\nlocalization method that achieves exceptional stability and accuracy in the\npresence of severe non-line-of-sight (NLOS) propagations, while eliminating the\nneed for laborious offline calibration or NLOS identification. Extensive\nexperimental results validate the effectiveness and high accuracy of our\nGNN-based localization model, particularly in challenging NLOS scenarios.\nHowever, the proposed GNN-based model exhibits limited flexibility, and its\naccuracy is highly sensitive to a specific hyperparameter that determines the\ngraph structure. To address the limitations and extend the applicability of the\nGNN-based model to real scenarios, we introduce two attentional graph neural\nnetworks (AGNNs) that offer enhanced flexibility and the ability to\nautomatically learn the optimal hyperparameter for each node. Experimental\nresults confirm that the AGNN models are able to enhance localization accuracy,\nproviding a promising solution for real-world applications. We also provide\nsome analyses of the improved performance achieved by the AGNN models from the\nperspectives of dynamic attention and signal denoising characteristics.",
            "author": [
                "Wenzhong Yan",
                "Juntao Wang",
                "Feng Yin",
                "Abdelhak M. Zoubir"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16856v1",
                "http://arxiv.org/pdf/2311.16856v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "eess.SP",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16854v2",
            "title": "A Unified Approach for Text- and Image-guided 4D Scene Generation",
            "updated": "2023-11-29T15:56:38Z",
            "published": "2023-11-28T15:03:53Z",
            "summary": "Large-scale diffusion generative models are greatly simplifying image, video\nand 3D asset creation from user-provided text prompts and images. However, the\nchallenging problem of text-to-4D dynamic 3D scene generation with diffusion\nguidance remains largely unexplored. We propose Dream-in-4D, which features a\nnovel two-stage approach for text-to-4D synthesis, leveraging (1) 3D and 2D\ndiffusion guidance to effectively learn a high-quality static 3D asset in the\nfirst stage; (2) a deformable neural radiance field that explicitly\ndisentangles the learned static asset from its deformation, preserving quality\nduring motion learning; and (3) a multi-resolution feature grid for the\ndeformation field with a displacement total variation loss to effectively learn\nmotion with video diffusion guidance in the second stage. Through a user\npreference study, we demonstrate that our approach significantly advances image\nand motion quality, 3D consistency and text fidelity for text-to-4D generation\ncompared to baseline approaches. Thanks to its motion-disentangled\nrepresentation, Dream-in-4D can also be easily adapted for controllable\ngeneration where appearance is defined by one or multiple images, without the\nneed to modify the motion learning stage. Thus, our method offers, for the\nfirst time, a unified approach for text-to-4D, image-to-4D and personalized 4D\ngeneration tasks.",
            "author": [
                "Yufeng Zheng",
                "Xueting Li",
                "Koki Nagano",
                "Sifei Liu",
                "Karsten Kreis",
                "Otmar Hilliges",
                "Shalini De Mello"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16854v2",
                "http://arxiv.org/pdf/2311.16854v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16852v1",
            "title": "Optimal minimax rate of learning interaction kernels",
            "updated": "2023-11-28T15:01:58Z",
            "published": "2023-11-28T15:01:58Z",
            "summary": "Nonparametric estimation of nonlocal interaction kernels is crucial in\nvarious applications involving interacting particle systems. The inference\nchallenge, situated at the nexus of statistical learning and inverse problems,\ncomes from the nonlocal dependency. A central question is whether the optimal\nminimax rate of convergence for this problem aligns with the rate of\n$M^{-\\frac{2\\beta}{2\\beta+1}}$ in classical nonparametric regression, where $M$\nis the sample size and $\\beta$ represents the smoothness exponent of the radial\nkernel. Our study confirms this alignment for systems with a finite number of\nparticles.\n  We introduce a tamed least squares estimator (tLSE) that attains the optimal\nconvergence rate for a broad class of exchangeable distributions. The tLSE\nbridges the smallest eigenvalue of random matrices and Sobolev embedding. This\nestimator relies on nonasymptotic estimates for the left tail probability of\nthe smallest eigenvalue of the normal matrix. The lower minimax rate is derived\nusing the Fano-Tsybakov hypothesis testing method. Our findings reveal that\nprovided the inverse problem in the large sample limit satisfies a coercivity\ncondition, the left tail probability does not alter the bias-variance tradeoff,\nand the optimal minimax rate remains intact. Our tLSE method offers a\nstraightforward approach for establishing the optimal minimax rate for models\nwith either local or nonlocal dependency.",
            "author": [
                "Xiong Wang",
                "Inbar Seroussi",
                "Fei Lu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16852v1",
                "http://arxiv.org/pdf/2311.16852v1"
            ],
            "primary_category": "math.ST",
            "category": [
                "math.ST",
                "math.PR",
                "stat.ML",
                "stat.TH",
                "62G08, 62G20, 60B20"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16851v1",
            "title": "Edge AI for Internet of Energy: Challenges and Perspectives",
            "updated": "2023-11-28T15:01:56Z",
            "published": "2023-11-28T15:01:56Z",
            "summary": "The digital landscape of the Internet of Energy (IoE) is on the brink of a\nrevolutionary transformation with the integration of edge Artificial\nIntelligence (AI). This comprehensive review elucidates the promise and\npotential that edge AI holds for reshaping the IoE ecosystem. Commencing with a\nmeticulously curated research methodology, the article delves into the myriad\nof edge AI techniques specifically tailored for IoE. The myriad benefits,\nspanning from reduced latency and real-time analytics to the pivotal aspects of\ninformation security, scalability, and cost-efficiency, underscore the\nindispensability of edge AI in modern IoE frameworks. As the narrative\nprogresses, readers are acquainted with pragmatic applications and techniques,\nhighlighting on-device computation, secure private inference methods, and the\navant-garde paradigms of AI training on the edge. A critical analysis follows,\noffering a deep dive into the present challenges including security concerns,\ncomputational hurdles, and standardization issues. However, as the horizon of\ntechnology ever expands, the review culminates in a forward-looking\nperspective, envisaging the future symbiosis of 5G networks, federated edge AI,\ndeep reinforcement learning, and more, painting a vibrant panorama of what the\nfuture beholds. For anyone vested in the domains of IoE and AI, this review\noffers both a foundation and a visionary lens, bridging the present realities\nwith future possibilities.",
            "author": [
                "Yassine Himeur",
                "Aya Nabil Sayed",
                "Abdullah Alsalemi",
                "Faycal Bensaali",
                "Abbes Amira"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16851v1",
                "http://arxiv.org/pdf/2311.16851v1"
            ],
            "primary_category": "cs.NI",
            "category": [
                "cs.NI",
                "cs.AI",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16849v1",
            "title": "Identifiable Feature Learning for Spatial Data with Nonlinear ICA",
            "updated": "2023-11-28T15:00:11Z",
            "published": "2023-11-28T15:00:11Z",
            "summary": "Recently, nonlinear ICA has surfaced as a popular alternative to the many\nheuristic models used in deep representation learning and disentanglement. An\nadvantage of nonlinear ICA is that a sophisticated identifiability theory has\nbeen developed; in particular, it has been proven that the original components\ncan be recovered under sufficiently strong latent dependencies. Despite this\ngeneral theory, practical nonlinear ICA algorithms have so far been mainly\nlimited to data with one-dimensional latent dependencies, especially\ntime-series data. In this paper, we introduce a new nonlinear ICA framework\nthat employs $t$-process (TP) latent components which apply naturally to data\nwith higher-dimensional dependency structures, such as spatial and\nspatio-temporal data. In particular, we develop a new learning and inference\nalgorithm that extends variational inference methods to handle the combination\nof a deep neural network mixing function with the TP prior, and employs the\nmethod of inducing points for computational efficacy. On the theoretical side,\nwe show that such TP independent components are identifiable under very general\nconditions. Further, Gaussian Process (GP) nonlinear ICA is established as a\nlimit of the TP Nonlinear ICA model, and we prove that the identifiability of\nthe latent components at this GP limit is more restricted. Namely, those\ncomponents are identifiable if and only if they have distinctly different\ncovariance kernels. Our algorithm and identifiability theorems are explored on\nsimulated spatial data and real world spatio-temporal data.",
            "author": [
                "Hermanni H\u00e4lv\u00e4",
                "Jonathan So",
                "Richard E. Turner",
                "Aapo Hyv\u00e4rinen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16849v1",
                "http://arxiv.org/pdf/2311.16849v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16842v1",
            "title": "RELIC: Investigating Large Language Model Responses using\n  Self-Consistency",
            "updated": "2023-11-28T14:55:52Z",
            "published": "2023-11-28T14:55:52Z",
            "summary": "Large Language Models (LLMs) are notorious for blending fact with fiction and\ngenerating non-factual content, known as hallucinations. To tackle this\nchallenge, we propose an interactive system that helps users obtain insights\ninto the reliability of the generated text. Our approach is based on the idea\nthat the self-consistency of multiple samples generated by the same LLM relates\nto its confidence in individual claims in the generated texts. Using this idea,\nwe design RELIC, an interactive system that enables users to investigate and\nverify semantic-level variations in multiple long-form responses. This allows\nusers to recognize potentially inaccurate information in the generated text and\nmake necessary corrections. From a user study with ten participants, we\ndemonstrate that our approach helps users better verify the reliability of the\ngenerated text. We further summarize the design implications and lessons\nlearned from this research for inspiring future studies on reliable human-LLM\ninteractions.",
            "author": [
                "Furui Cheng",
                "Vil\u00e9m Zouhar",
                "Simran Arora",
                "Mrinmaya Sachan",
                "Hendrik Strobelt",
                "Mennatallah El-Assady"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16842v1",
                "http://arxiv.org/pdf/2311.16842v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16841v1",
            "title": "Two-step dynamic obstacle avoidance",
            "updated": "2023-11-28T14:55:50Z",
            "published": "2023-11-28T14:55:50Z",
            "summary": "Dynamic obstacle avoidance (DOA) is a fundamental challenge for any\nautonomous vehicle, independent of whether it operates in sea, air, or land.\nThis paper proposes a two-step architecture for handling DOA tasks by combining\nsupervised and reinforcement learning (RL). In the first step, we introduce a\ndata-driven approach to estimate the collision risk of an obstacle using a\nrecurrent neural network, which is trained in a supervised fashion and offers\nrobustness to non-linear obstacle movements. In the second step, we include\nthese collision risk estimates into the observation space of an RL agent to\nincrease its situational awareness.~We illustrate the power of our two-step\napproach by training different RL agents in a challenging environment that\nrequires to navigate amid multiple obstacles. The non-linear movements of\nobstacles are exemplarily modeled based on stochastic processes and periodic\npatterns, although our architecture is suitable for any obstacle dynamics. The\nexperiments reveal that integrating our collision risk metrics into the\nobservation space doubles the performance in terms of reward, which is\nequivalent to halving the number of collisions in the considered environment.\nFurthermore, we show that the architecture's performance improvement is\nindependent of the applied RL algorithm.",
            "author": [
                "Fabian Hart",
                "Martin Waltz",
                "Ostap Okhrin"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16841v1",
                "http://arxiv.org/pdf/2311.16841v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16838v1",
            "title": "Increasing Transparency of Reinforcement Learning using Shielding for\n  Human Preferences and Explanations",
            "updated": "2023-11-28T14:53:47Z",
            "published": "2023-11-28T14:53:47Z",
            "summary": "The adoption of Reinforcement Learning (RL) in several human-centred\napplications provides robots with autonomous decision-making capabilities and\nadaptability based on the observations of the operating environment. In such\nscenarios, however, the learning process can make robots' behaviours unclear\nand unpredictable to humans, thus preventing a smooth and effective Human-Robot\nInteraction (HRI). As a consequence, it becomes crucial to avoid robots\nperforming actions that are unclear to the user. In this work, we investigate\nwhether including human preferences in RL (concerning the actions the robot\nperforms during learning) improves the transparency of a robot's behaviours.\nFor this purpose, a shielding mechanism is included in the RL algorithm to\ninclude human preferences and to monitor the learning agent's decisions. We\ncarried out a within-subjects study involving 26 participants to evaluate the\nrobot's transparency in terms of Legibility, Predictability, and Expectability\nin different settings. Results indicate that considering human preferences\nduring learning improves Legibility with respect to providing only\nExplanations, and combining human preferences with explanations elucidating the\nrationale behind the robot's decisions further amplifies transparency. Results\nalso confirm that an increase in transparency leads to an increase in the\nsafety, comfort, and reliability of the robot. These findings show the\nimportance of transparency during learning and suggest a paradigm for robotic\napplications with human in the loop.",
            "author": [
                "Georgios Angelopoulos",
                "Luigi Mangiacapra",
                "Alessandra Rossi",
                "Claudia Di Napoli",
                "Silvia Rossi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16838v1",
                "http://arxiv.org/pdf/2311.16838v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16835v2",
            "title": "Unified-modal Salient Object Detection via Adaptive Prompt Learning",
            "updated": "2023-11-29T13:14:58Z",
            "published": "2023-11-28T14:51:08Z",
            "summary": "Existing single-modal and multi-modal salient object detection (SOD) methods\nfocus on designing specific architectures tailored for their respective tasks.\nHowever, developing completely different models for different tasks leads to\nlabor and time consumption, as well as high computational and practical\ndeployment costs. In this paper, we make the first attempt to address both\nsingle-modal and multi-modal SOD in a unified framework called UniSOD.\nNevertheless, assigning appropriate strategies to modality variable inputs is\nchallenging. To this end, UniSOD learns modality-aware prompts with\ntask-specific hints through adaptive prompt learning, which are plugged into\nthe proposed pre-trained baseline SOD model to handle corresponding tasks,\nwhile only requiring few learnable parameters compared to training the entire\nmodel. Each modality-aware prompt is generated from a switchable prompt\ngeneration block, which performs structural switching solely relied on\nsingle-modal and multi-modal inputs. UniSOD achieves consistent performance\nimprovement on 14 benchmark datasets for RGB, RGB-D, and RGB-T SOD, which\ndemonstrates that our method effectively and efficiently unifies single-modal\nand multi-modal SOD tasks.",
            "author": [
                "Kunpeng Wang",
                "Chenglong Li",
                "Zhengzheng Tu",
                "Bin Luo"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16835v2",
                "http://arxiv.org/pdf/2311.16835v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16834v2",
            "title": "Modular Neural Networks for Time Series Forecasting: Interpretability\n  and Feature Selection using Attention",
            "updated": "2023-11-29T13:23:42Z",
            "published": "2023-11-28T14:51:06Z",
            "summary": "Multivariate time series have many applications, from healthcare and\nmeteorology to life science. Although deep learning models have shown excellent\npredictive performance for time series, they have been criticised for being\n\"black-boxes\" or non-interpretable. This paper proposes a novel modular neural\nnetwork model for multivariate time series prediction that is interpretable by\nconstruction. A recurrent neural network learns the temporal dependencies in\nthe data while an attention-based feature selection component selects the most\nrelevant features and suppresses redundant features used in the learning of the\ntemporal dependencies. A modular deep network is trained from the selected\nfeatures independently to show the users how features influence outcomes,\nmaking the model interpretable. Experimental results show that this approach\ncan outperform state-of-the-art interpretable Neural Additive Models (NAM) and\nvariations thereof in both regression and classification of time series tasks,\nachieving a predictive performance that is comparable to the top\nnon-interpretable methods for time series, LSTM and XGBoost.",
            "author": [
                "Qiqi Su",
                "Christos Kloukinas",
                "Artur d'Avila Garcez"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16834v2",
                "http://arxiv.org/pdf/2311.16834v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16833v1",
            "title": "1-Lipschitz Layers Compared: Memory, Speed, and Certifiable Robustness",
            "updated": "2023-11-28T14:50:50Z",
            "published": "2023-11-28T14:50:50Z",
            "summary": "The robustness of neural networks against input perturbations with bounded\nmagnitude represents a serious concern in the deployment of deep learning\nmodels in safety-critical systems. Recently, the scientific community has\nfocused on enhancing certifiable robustness guarantees by crafting 1-Lipschitz\nneural networks that leverage Lipschitz bounded dense and convolutional layers.\nAlthough different methods have been proposed in the literature to achieve this\ngoal, understanding the performance of such methods is not straightforward,\nsince different metrics can be relevant (e.g., training time, memory usage,\naccuracy, certifiable robustness) for different applications. For this reason,\nthis work provides a thorough theoretical and empirical comparison between\nmethods by evaluating them in terms of memory usage, speed, and certifiable\nrobust accuracy. The paper also provides some guidelines and recommendations to\nsupport the user in selecting the methods that work best depending on the\navailable resources. We provide code at\nhttps://github.com/berndprach/1LipschitzLayersCompared.",
            "author": [
                "Bernd Prach",
                "Fabio Brau",
                "Giorgio Buttazzo",
                "Christoph H. Lampert"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16833v1",
                "http://arxiv.org/pdf/2311.16833v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV",
                "cs.NE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16829v1",
            "title": "Decomposer: Semi-supervised Learning of Image Restoration and Image\n  Decomposition",
            "updated": "2023-11-28T14:48:22Z",
            "published": "2023-11-28T14:48:22Z",
            "summary": "We present Decomposer, a semi-supervised reconstruction model that decomposes\ndistorted image sequences into their fundamental building blocks - the original\nimage and the applied augmentations, i.e., shadow, light, and occlusions. To\nsolve this problem, we use the SIDAR dataset that provides a large number of\ndistorted image sequences: each sequence contains images with shadows,\nlighting, and occlusions applied to an undistorted version. Each distortion\nchanges the original signal in different ways, e.g., additive or multiplicative\nnoise. We propose a transformer-based model to explicitly learn this\ndecomposition. The sequential model uses 3D Swin-Transformers for\nspatio-temporal encoding and 3D U-Nets as prediction heads for individual parts\nof the decomposition. We demonstrate that by separately pre-training our model\non weakly supervised pseudo labels, we can steer our model to optimize for our\nambiguous problem definition and learn to differentiate between the different\nimage distortions.",
            "author": [
                "Boris Meinardus",
                "Mariusz Trzeciakiewicz",
                "Tim Herzig",
                "Monika Kwiatkowski",
                "Simon Matern",
                "Olaf Hellwich"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16829v1",
                "http://arxiv.org/pdf/2311.16829v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16828v1",
            "title": "SARA: Controllable Makeup Transfer with Spatial Alignment and\n  Region-Adaptive Normalization",
            "updated": "2023-11-28T14:46:51Z",
            "published": "2023-11-28T14:46:51Z",
            "summary": "Makeup transfer is a process of transferring the makeup style from a\nreference image to the source images, while preserving the source images'\nidentities. This technique is highly desirable and finds many applications.\nHowever, existing methods lack fine-level control of the makeup style, making\nit challenging to achieve high-quality results when dealing with large spatial\nmisalignments. To address this problem, we propose a novel Spatial Alignment\nand Region-Adaptive normalization method (SARA) in this paper. Our method\ngenerates detailed makeup transfer results that can handle large spatial\nmisalignments and achieve part-specific and shade-controllable makeup transfer.\nSpecifically, SARA comprises three modules: Firstly, a spatial alignment module\nthat preserves the spatial context of makeup and provides a target semantic map\nfor guiding the shape-independent style codes. Secondly, a region-adaptive\nnormalization module that decouples shape and makeup style using per-region\nencoding and normalization, which facilitates the elimination of spatial\nmisalignments. Lastly, a makeup fusion module blends identity features and\nmakeup style by injecting learned scale and bias parameters. Experimental\nresults show that our SARA method outperforms existing methods and achieves\nstate-of-the-art performance on two public datasets.",
            "author": [
                "Xiaojing Zhong",
                "Xinyi Huang",
                "Zhonghua Wu",
                "Guosheng Lin",
                "Qingyao Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16828v1",
                "http://arxiv.org/pdf/2311.16828v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16822v1",
            "title": "Large Language Models Suffer From Their Own Output: An Analysis of the\n  Self-Consuming Training Loop",
            "updated": "2023-11-28T14:36:43Z",
            "published": "2023-11-28T14:36:43Z",
            "summary": "Large language models (LLM) have become state of the art in many benchmarks\nand conversational LLM applications like ChatGPT are now widely used by the\npublic. Those LLMs can be used to generate large amounts of content which is\nposted on the internet to various platforms. As LLMs are trained on datasets\nusually collected from the internet, this LLM-generated content might be used\nto train the next generation of LLMs. Therefore, a self-consuming training loop\nemerges in which new LLM generations are trained on the output from the\nprevious generations. We empirically study this self-consuming training loop\nusing a novel dataset to analytically and accurately measure quality and\ndiversity of generated outputs. We find that this self-consuming training loop\ninitially improves both quality and diversity. However, after a few generations\nthe output inevitably degenerates in diversity. We find that the rate of\ndegeneration depends on the proportion of real and generated data.",
            "author": [
                "Martin Briesch",
                "Dominik Sobania",
                "Franz Rothlauf"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16822v1",
                "http://arxiv.org/pdf/2311.16822v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL",
                "cs.NE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17124v1",
            "title": "A knowledge-driven AutoML architecture",
            "updated": "2023-11-28T14:31:38Z",
            "published": "2023-11-28T14:31:38Z",
            "summary": "This paper proposes a knowledge-driven AutoML architecture for pipeline and\ndeep feature synthesis. The main goal is to render the AutoML process\nexplainable and to leverage domain knowledge in the synthesis of pipelines and\nfeatures. The architecture explores several novel ideas: first, the\nconstruction of pipelines and deep features is approached in an unified way.\nNext, synthesis is driven by a shared knowledge system, interactively queried\nas to what pipeline operations to use or features to compute. Lastly, the\nsynthesis processes takes decisions at runtime using partial solutions and\nresults of their application on data. Two experiments are conducted to\ndemonstrate the functionality of a na\\\"{\\i}ve implementation of the proposed\narchitecture and to discuss its advantages, trade-offs as well as future\npotential for AutoML.",
            "author": [
                "Corneliu Cofaru",
                "Johan Loeckx"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17124v1",
                "http://arxiv.org/pdf/2311.17124v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16818v1",
            "title": "DI-Net : Decomposed Implicit Garment Transfer Network for Digital\n  Clothed 3D Human",
            "updated": "2023-11-28T14:28:41Z",
            "published": "2023-11-28T14:28:41Z",
            "summary": "3D virtual try-on enjoys many potential applications and hence has attracted\nwide attention. However, it remains a challenging task that has not been\nadequately solved. Existing 2D virtual try-on methods cannot be directly\nextended to 3D since they lack the ability to perceive the depth of each pixel.\nBesides, 3D virtual try-on approaches are mostly built on the fixed topological\nstructure and with heavy computation. To deal with these problems, we propose a\nDecomposed Implicit garment transfer network (DI-Net), which can effortlessly\nreconstruct a 3D human mesh with the newly try-on result and preserve the\ntexture from an arbitrary perspective. Specifically, DI-Net consists of two\nmodules: 1) A complementary warping module that warps the reference image to\nhave the same pose as the source image through dense correspondence learning\nand sparse flow learning; 2) A geometry-aware decomposed transfer module that\ndecomposes the garment transfer into image layout based transfer and texture\nbased transfer, achieving surface and texture reconstruction by constructing\npixel-aligned implicit functions. Experimental results show the effectiveness\nand superiority of our method in the 3D virtual try-on task, which can yield\nmore high-quality results over other existing methods.",
            "author": [
                "Xiaojing Zhong",
                "Yukun Su",
                "Zhonghua Wu",
                "Guosheng Lin",
                "Qingyao Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16818v1",
                "http://arxiv.org/pdf/2311.16818v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16810v1",
            "title": "A Short Overview of 6G V2X Communication Standards",
            "updated": "2023-11-28T14:17:35Z",
            "published": "2023-11-28T14:17:35Z",
            "summary": "We are on the verge of a new age of linked autonomous cars with unheard-of\nuser experiences, dramatically improved air quality and road safety, extremely\nvaried transportation settings, and a plethora of cutting-edge apps. A\nsubstantially improved Vehicle-to-Everything (V2X) communication network that\ncan simultaneously support massive hyper-fast, ultra-reliable, and low-latency\ninformation exchange is necessary to achieve this ambitious goal. These needs\nof the upcoming V2X are expected to be satisfied by the Sixth Generation (6G)\ncommunication system. In this article, we start by introducing the history of\nV2X communications by giving details on the current, developing, and future\ndevelopments. We compare the applications of communication technologies such as\nWi-Fi, LTE, 5G, and 6G. we focus on the new technologies for 6G V2X which are\nbrain-vehicle interface, blocked-based V2X, and Machine Learning (ML). To\nachieve this, we provide a summary of the most recent ML developments in 6G\nvehicle networks. we discuss the security challenges of 6G V2X. We address the\nstrengths, open challenges, development, and improving areas of further study\nin this field.",
            "author": [
                "Donglin Wang",
                "Yann Nana Nganso",
                "Hans D. Schotten"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16810v1",
                "http://arxiv.org/pdf/2311.16810v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP",
                "cs.NI",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16807v1",
            "title": "Agent-Aware Training for Agent-Agnostic Action Advising in Deep\n  Reinforcement Learning",
            "updated": "2023-11-28T14:09:43Z",
            "published": "2023-11-28T14:09:43Z",
            "summary": "Action advising endeavors to leverage supplementary guidance from expert\nteachers to alleviate the issue of sampling inefficiency in Deep Reinforcement\nLearning (DRL). Previous agent-specific action advising methods are hindered by\nimperfections in the agent itself, while agent-agnostic approaches exhibit\nlimited adaptability to the learning agent. In this study, we propose a novel\nframework called Agent-Aware trAining yet Agent-Agnostic Action Advising (A7)\nto strike a balance between the two. The underlying concept of A7 revolves\naround utilizing the similarity of state features as an indicator for\nsoliciting advice. However, unlike prior methodologies, the measurement of\nstate feature similarity is performed by neither the error-prone learning agent\nnor the agent-agnostic advisor. Instead, we employ a proxy model to extract\nstate features that are both discriminative (adaptive to the agent) and\ngenerally applicable (robust to agent noise). Furthermore, we utilize behavior\ncloning to train a model for reusing advice and introduce an intrinsic reward\nfor the advised samples to incentivize the utilization of expert guidance.\nExperiments are conducted on the GridWorld, LunarLander, and six prominent\nscenarios from Atari games. The results demonstrate that A7 significantly\naccelerates the learning process and surpasses existing methods (both\nagent-specific and agent-agnostic) by a substantial margin. Our code will be\nmade publicly available.",
            "author": [
                "Yaoquan Wei",
                "Shunyu Liu",
                "Jie Song",
                "Tongya Zheng",
                "Kaixuan Chen",
                "Yong Wang",
                "Mingli Song"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16807v1",
                "http://arxiv.org/pdf/2311.16807v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16789v1",
            "title": "A Survey of the Evolution of Language Model-Based Dialogue Systems",
            "updated": "2023-11-28T13:51:32Z",
            "published": "2023-11-28T13:51:32Z",
            "summary": "Dialogue systems, including task-oriented_dialogue_system (TOD) and\nopen-domain_dialogue_system (ODD), have undergone significant transformations,\nwith language_models (LM) playing a central role. This survey delves into the\nhistorical trajectory of dialogue systems, elucidating their intricate\nrelationship with advancements in language models by categorizing this\nevolution into four distinct stages, each marked by pivotal LM breakthroughs:\n1) Early_Stage: characterized by statistical LMs, resulting in rule-based or\nmachine-learning-driven dialogue_systems; 2) Independent development of TOD and\nODD based on neural_language_models (NLM; e.g., LSTM and GRU), since NLMs lack\nintrinsic knowledge in their parameters; 3) fusion between different types of\ndialogue systems with the advert of pre-trained_language_models (PLMs),\nstarting from the fusion between four_sub-tasks_within_TOD, and then\nTOD_with_ODD; and 4) current LLM-based_dialogue_system, wherein LLMs can be\nused to conduct TOD and ODD seamlessly. Thus, our survey provides a\nchronological perspective aligned with LM breakthroughs, offering a\ncomprehensive review of state-of-the-art research outcomes. What's more, we\nfocus on emerging topics and discuss open challenges, providing valuable\ninsights into future directions for LLM-based_dialogue_systems. Through this\nexploration, we pave the way for a deeper_comprehension of the evolution,\nguiding future developments in LM-based dialogue_systems.",
            "author": [
                "Hongru Wang",
                "Lingzhi Wang",
                "Yiming Du",
                "Liang Chen",
                "Jingyan Zhou",
                "Yufei Wang",
                "Kam-Fai Wong"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16789v1",
                "http://arxiv.org/pdf/2311.16789v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17122v1",
            "title": "Large Model Based Referring Camouflaged Object Detection",
            "updated": "2023-11-28T13:45:09Z",
            "published": "2023-11-28T13:45:09Z",
            "summary": "Referring camouflaged object detection (Ref-COD) is a recently-proposed\nproblem aiming to segment out specified camouflaged objects matched with a\ntextual or visual reference. This task involves two major challenges: the COD\ndomain-specific perception and multimodal reference-image alignment. Our\nmotivation is to make full use of the semantic intelligence and intrinsic\nknowledge of recent Multimodal Large Language Models (MLLMs) to decompose this\ncomplex task in a human-like way. As language is highly condensed and\ninductive, linguistic expression is the main media of human knowledge learning,\nand the transmission of knowledge information follows a multi-level progression\nfrom simplicity to complexity. In this paper, we propose a large-model-based\nMulti-Level Knowledge-Guided multimodal method for Ref-COD termed MLKG, where\nmulti-level knowledge descriptions from MLLM are organized to guide the large\nvision model of segmentation to perceive the camouflage-targets and\ncamouflage-scene progressively and meanwhile deeply align the textual\nreferences with camouflaged photos. To our knowledge, our contributions mainly\ninclude: (1) This is the first time that the MLLM knowledge is studied for\nRef-COD and COD. (2) We, for the first time, propose decomposing Ref-COD into\ntwo main perspectives of perceiving the target and scene by integrating MLLM\nknowledge, and contribute a multi-level knowledge-guided method. (3) Our method\nachieves the state-of-the-art on the Ref-COD benchmark outperforming numerous\nstrong competitors. Moreover, thanks to the injected rich knowledge, it\ndemonstrates zero-shot generalization ability on uni-modal COD datasets. We\nwill release our code soon.",
            "author": [
                "Shupeng Cheng",
                "Ge-Peng Ji",
                "Pengda Qin",
                "Deng-Ping Fan",
                "Bowen Zhou",
                "Peng Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17122v1",
                "http://arxiv.org/pdf/2311.17122v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17121v1",
            "title": "Generative Data Augmentation Improves Scribble-supervised Semantic\n  Segmentation",
            "updated": "2023-11-28T13:44:33Z",
            "published": "2023-11-28T13:44:33Z",
            "summary": "Recent advances in generative models, such as diffusion models, have made\ngenerating high-quality synthetic images widely accessible. Prior works have\nshown that training on synthetic images improves many perception tasks, such as\nimage classification, object detection, and semantic segmentation. We are the\nfirst to explore generative data augmentations for scribble-supervised semantic\nsegmentation. We propose a generative data augmentation method that leverages a\nControlNet diffusion model conditioned on semantic scribbles to produce\nhigh-quality training data. However, naive implementations of generative data\naugmentations may inadvertently harm the performance of the downstream\nsegmentor rather than improve it. We leverage classifier-free diffusion\nguidance to enforce class consistency and introduce encode ratios to trade off\ndata diversity for data realism. Using the guidance scale and encode ratio, we\nare able to generate a spectrum of high-quality training images. We propose\nmultiple augmentation schemes and find that these schemes significantly impact\nmodel performance, especially in the low-data regime. Our framework further\nreduces the gap between the performance of scribble-supervised segmentation and\nthat of fully-supervised segmentation. We also show that our framework\nsignificantly improves segmentation performance on small datasets, even\nsurpassing fully-supervised segmentation.",
            "author": [
                "Jacob Schnell",
                "Jieke Wang",
                "Lu Qi",
                "Vincent Tao Hu",
                "Meng Tang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17121v1",
                "http://arxiv.org/pdf/2311.17121v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16773v1",
            "title": "Multi-Channel Cross Modal Detection of Synthetic Face Images",
            "updated": "2023-11-28T13:30:10Z",
            "published": "2023-11-28T13:30:10Z",
            "summary": "Synthetically generated face images have shown to be indistinguishable from\nreal images by humans and as such can lead to a lack of trust in digital\ncontent as they can, for instance, be used to spread misinformation. Therefore,\nthe need to develop algorithms for detecting entirely synthetic face images is\napparent. Of interest are images generated by state-of-the-art deep\nlearning-based models, as these exhibit a high level of visual realism. Recent\nworks have demonstrated that detecting such synthetic face images under\nrealistic circumstances remains difficult as new and improved generative models\nare proposed with rapid speed and arbitrary image post-processing can be\napplied. In this work, we propose a multi-channel architecture for detecting\nentirely synthetic face images which analyses information both in the frequency\nand visible spectra using Cross Modal Focal Loss. We compare the proposed\narchitecture with several related architectures trained using Binary Cross\nEntropy and show in cross-model experiments that the proposed architecture\nsupervised using Cross Modal Focal Loss, in general, achieves most competitive\nperformance.",
            "author": [
                "M. Ibsen",
                "C. Rathgeb",
                "S. Marcel",
                "C. Busch"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16773v1",
                "http://arxiv.org/pdf/2311.16773v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16771v1",
            "title": "The HR-Calculus: Enabling Information Processing with Quaternion Algebra",
            "updated": "2023-11-28T13:25:34Z",
            "published": "2023-11-28T13:25:34Z",
            "summary": "From their inception, quaternions and their division algebra have proven to\nbe advantageous in modelling rotation/orientation in three-dimensional spaces\nand have seen use from the initial formulation of electromagnetic filed theory\nthrough to forming the basis of quantum filed theory. Despite their impressive\nversatility in modelling real-world phenomena, adaptive information processing\ntechniques specifically designed for quaternion-valued signals have only\nrecently come to the attention of the machine learning, signal processing, and\ncontrol communities. The most important development in this direction is\nintroduction of the HR-calculus, which provides the required mathematical\nfoundation for deriving adaptive information processing techniques directly in\nthe quaternion domain. In this article, the foundations of the HR-calculus are\nrevised and the required tools for deriving adaptive learning techniques\nsuitable for dealing with quaternion-valued signals, such as the gradient\noperator, chain and product derivative rules, and Taylor series expansion are\npresented. This serves to establish the most important applications of adaptive\ninformation processing in the quaternion domain for both single-node and\nmulti-node formulations. The article is supported by Supplementary Material,\nwhich will be referred to as SM.",
            "author": [
                "Danilo P. Mandic",
                "Sayed Pouria Talebi",
                "Clive Cheong Took",
                "Yili Xia",
                "Dongpo Xu",
                "Min Xiang",
                "Pauline Bourigault"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16771v1",
                "http://arxiv.org/pdf/2311.16771v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03736v1",
            "title": "De-identification of clinical free text using natural language\n  processing: A systematic review of current approaches",
            "updated": "2023-11-28T13:20:41Z",
            "published": "2023-11-28T13:20:41Z",
            "summary": "Background: Electronic health records (EHRs) are a valuable resource for\ndata-driven medical research. However, the presence of protected health\ninformation (PHI) makes EHRs unsuitable to be shared for research purposes.\nDe-identification, i.e. the process of removing PHI is a critical step in\nmaking EHR data accessible. Natural language processing has repeatedly\ndemonstrated its feasibility in automating the de-identification process.\nObjectives: Our study aims to provide systematic evidence on how the\nde-identification of clinical free text has evolved in the last thirteen years,\nand to report on the performances and limitations of the current\nstate-of-the-art systems. In addition, we aim to identify challenges and\npotential research opportunities in this field. Methods: A systematic search in\nPubMed, Web of Science and the DBLP was conducted for studies published between\nJanuary 2010 and February 2023. Titles and abstracts were examined to identify\nthe relevant studies. Selected studies were then analysed in-depth, and\ninformation was collected on de-identification methodologies, data sources, and\nmeasured performance. Results: A total of 2125 publications were identified for\nthe title and abstract screening. 69 studies were found to be relevant. Machine\nlearning (37 studies) and hybrid (26 studies) approaches are predominant, while\nsix studies relied only on rules. Majority of the approaches were trained and\nevaluated on public corpora. The 2014 i2b2/UTHealth corpus is the most\nfrequently used (36 studies), followed by the 2006 i2b2 (18 studies) and 2016\nCEGS N-GRID (10 studies) corpora.",
            "author": [
                "Aleksandar Kova\u010devi\u0107",
                "Bojana Ba\u0161aragin",
                "Nikola Milo\u0161evi\u0107",
                "Goran Nenadi\u0107"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03736v1",
                "http://arxiv.org/pdf/2312.03736v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.CR",
                "cs.DL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16769v1",
            "title": "Equilibrium in the Computing Continuum through Active Inference",
            "updated": "2023-11-28T13:19:54Z",
            "published": "2023-11-28T13:19:54Z",
            "summary": "Computing Continuum (CC) systems are challenged to ensure the intricate\nrequirements of each computational tier. Given the system's scale, the Service\nLevel Objectives (SLOs) which are expressed as these requirements, must be\nbroken down into smaller parts that can be decentralized. We present our\nframework for collaborative edge intelligence enabling individual edge devices\nto (1) develop a causal understanding of how to enforce their SLOs, and (2)\ntransfer knowledge to speed up the onboarding of heterogeneous devices. Through\ncollaboration, they (3) increase the scope of SLO fulfillment. We implemented\nthe framework and evaluated a use case in which a CC system is responsible for\nensuring Quality of Service (QoS) and Quality of Experience (QoE) during video\nstreaming. Our results showed that edge devices required only ten training\nrounds to ensure four SLOs; furthermore, the underlying causal structures were\nalso rationally explainable. The addition of new types of devices can be done a\nposteriori, the framework allowed them to reuse existing models, even though\nthe device type had been unknown. Finally, rebalancing the load within a device\ncluster allowed individual edge devices to recover their SLO compliance after a\nnetwork failure from 22% to 89%.",
            "author": [
                "Boris Sedlak",
                "Victor Casamayor Pujol",
                "Praveen Kumar Donta",
                "Schahram Dustdar"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16769v1",
                "http://arxiv.org/pdf/2311.16769v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC",
                "cs.AI",
                "cs.LG",
                "cs.PF",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16766v1",
            "title": "Rescuing referral failures during automated diagnosis of domain-shifted\n  medical images",
            "updated": "2023-11-28T13:14:55Z",
            "published": "2023-11-28T13:14:55Z",
            "summary": "The success of deep learning models deployed in the real world depends\ncritically on their ability to generalize well across diverse data domains.\nHere, we address a fundamental challenge with selective classification during\nautomated diagnosis with domain-shifted medical images. In this scenario,\nmodels must learn to avoid making predictions when label confidence is low,\nespecially when tested with samples far removed from the training set\n(covariate shift). Such uncertain cases are typically referred to the clinician\nfor further analysis and evaluation. Yet, we show that even state-of-the-art\ndomain generalization approaches fail severely during referral when tested on\nmedical images acquired from a different demographic or using a different\ntechnology. We examine two benchmark diagnostic medical imaging datasets\nexhibiting strong covariate shifts: i) diabetic retinopathy prediction with\nretinal fundus images and ii) multilabel disease prediction with chest X-ray\nimages. We show that predictive uncertainty estimates do not generalize well\nunder covariate shifts leading to non-monotonic referral curves, and severe\ndrops in performance (up to 50%) at high referral rates (>70%). We evaluate\nnovel combinations of robust generalization and post hoc referral approaches,\nthat rescue these failures and achieve significant performance improvements,\ntypically >10%, over baseline methods. Our study identifies a critical\nchallenge with referral in domain-shifted medical images and finds key\napplications in reliable, automated disease diagnosis.",
            "author": [
                "Anuj Srivastava",
                "Karm Patel",
                "Pradeep Shenoy",
                "Devarajan Sridharan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16766v1",
                "http://arxiv.org/pdf/2311.16766v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16762v1",
            "title": "Machine learning methods for American-style path-dependent contracts",
            "updated": "2023-11-28T13:05:06Z",
            "published": "2023-11-28T13:05:06Z",
            "summary": "In the present work, we introduce and compare state-of-the-art algorithms,\nthat are now classified under the name of machine learning, to price Asian and\nlook-back products with early-termination features. These include randomized\nfeed-forward neural networks, randomized recurrent neural networks, and a novel\nmethod based on signatures of the underlying price process. Additionally, we\nexplore potential applications on callable certificates. Furthermore, we\npresent an innovative approach for calculating sensitivities, specifically\nDelta and Gamma, leveraging Chebyshev interpolation techniques.",
            "author": [
                "Matteo Gambara",
                "Giulia Livieri",
                "Andrea Pallavicini"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16762v1",
                "http://arxiv.org/pdf/2311.16762v1"
            ],
            "primary_category": "q-fin.PR",
            "category": [
                "q-fin.PR",
                "q-fin.CP",
                "65C05, 91G20, 91G60"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16754v1",
            "title": "Towards Full-scene Domain Generalization in Multi-agent Collaborative\n  Bird's Eye View Segmentation for Connected and Autonomous Driving",
            "updated": "2023-11-28T12:52:49Z",
            "published": "2023-11-28T12:52:49Z",
            "summary": "Collaborative perception has recently gained significant attention in\nautonomous driving, improving perception quality by enabling the exchange of\nadditional information among vehicles. However, deploying collaborative\nperception systems can lead to domain shifts due to diverse environmental\nconditions and data heterogeneity among connected and autonomous vehicles\n(CAVs). To address these challenges, we propose a unified domain generalization\nframework applicable in both training and inference stages of collaborative\nperception. In the training phase, we introduce an Amplitude Augmentation\n(AmpAug) method to augment low-frequency image variations, broadening the\nmodel's ability to learn across various domains. We also employ a\nmeta-consistency training scheme to simulate domain shifts, optimizing the\nmodel with a carefully designed consistency loss to encourage domain-invariant\nrepresentations. In the inference phase, we introduce an intra-system domain\nalignment mechanism to reduce or potentially eliminate the domain discrepancy\namong CAVs prior to inference. Comprehensive experiments substantiate the\neffectiveness of our method in comparison with the existing state-of-the-art\nworks. Code will be released at https://github.com/DG-CAVs/DG-CoPerception.git.",
            "author": [
                "Senkang Hu",
                "Zhengru Fang",
                "Xianhao Chen",
                "Yuguang Fang",
                "Sam Kwong"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16754v1",
                "http://arxiv.org/pdf/2311.16754v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16751v1",
            "title": "MultiCBR: Multi-view Contrastive Learning for Bundle Recommendation",
            "updated": "2023-11-28T12:50:40Z",
            "published": "2023-11-28T12:50:40Z",
            "summary": "Bundle recommendation seeks to recommend a bundle of related items to users\nto improve both user experience and the profits of platform. Existing bundle\nrecommendation models have progressed from capturing only user-bundle\ninteractions to the modeling of multiple relations among users, bundles and\nitems. CrossCBR, in particular, incorporates cross-view contrastive learning\ninto a two-view preference learning framework, significantly improving SOTA\nperformance. It does, however, have two limitations: 1) the two-view\nformulation does not fully exploit all the heterogeneous relations among users,\nbundles and items; and 2) the \"early contrast and late fusion\" framework is\nless effective in capturing user preference and difficult to generalize to\nmultiple views. In this paper, we present MultiCBR, a novel Multi-view\nContrastive learning framework for Bundle Recommendation. First, we devise a\nmulti-view representation learning framework capable of capturing all the\nuser-bundle, user-item and bundle-item relations, especially better utilizing\nthe bundle-item affiliations to enhance sparse bundles' representations.\nSecond, we innovatively adopt an \"early fusion and late contrast\" design that\nfirst fuses the multi-view representations before performing self-supervised\ncontrastive learning. In comparison to existing approaches, our framework\nreverses the order of fusion and contrast, introducing the following\nadvantages: 1)our framework is capable of modeling both cross-view and ego-view\npreferences, allowing us to achieve enhanced user preference modeling; and 2)\ninstead of requiring quadratic number of cross-view contrastive losses, we only\nrequire two self-supervised contrastive losses, resulting in minimal extra\ncosts. Experimental results on three public datasets indicate that our method\noutperforms SOTA methods.",
            "author": [
                "Yunshan Ma",
                "Yingzhi He",
                "Xiang Wang",
                "Yinwei Wei",
                "Xiaoyu Du",
                "Yuyangzi Fu",
                "Tat-Seng Chua"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16751v1",
                "http://arxiv.org/pdf/2311.16751v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16750v1",
            "title": "Accelerated lignocellulosic molecule adsorption structure determination",
            "updated": "2023-11-28T12:50:11Z",
            "published": "2023-11-28T12:50:11Z",
            "summary": "Here, we present a study combining Bayesian optimisation structural inference\nwith the machine learning interatomic potential NequIP to accelerate and enable\nthe study of the adsorption of the conformationally flexible lignocellulosic\nmolecules $\\beta$-D-xylose and 1,4-$\\beta$-D-xylotetraose on a copper surface.\nThe number of structure evaluations needed to map out the relevant potential\nenergy surfaces are reduced by Bayesian optimisation, while NequIP minimises\nthe time spent on each evaluation, ultimately resulting in cost-efficient and\nreliable sampling of large systems and configurational spaces. Although the\napplicability of Bayesian optimisation for the conformational analysis of the\nmore flexible xylotetraose molecule is restricted by the sample complexity\nbottleneck, the latter can be effectively bypassed with external conformer\nsearch tools, such as the Conformer-Rotamer Ensemble Sampling Tool,\nfacilitating the subsequent lower dimensional global minimum adsorption\nstructure determination. Finally, we demonstrate the applicability of the\ndescribed approach to find adsorption structures practically equivalent to the\ndensity functional theory counterparts at a fraction of the computational cost.",
            "author": [
                "Joakim S. Jestil\u00e4",
                "Nian Wu",
                "Fabio Priante",
                "Adam S. Foster"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16750v1",
                "http://arxiv.org/pdf/2311.16750v1"
            ],
            "primary_category": "physics.chem-ph",
            "category": [
                "physics.chem-ph",
                "physics.comp-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16741v1",
            "title": "Asynchronous Wireless Federated Learning with Probabilistic Client\n  Selection",
            "updated": "2023-11-28T12:39:34Z",
            "published": "2023-11-28T12:39:34Z",
            "summary": "Federated learning (FL) is a promising distributed learning framework where\ndistributed clients collaboratively train a machine learning model coordinated\nby a server. To tackle the stragglers issue in asynchronous FL, we consider\nthat each client keeps local updates and probabilistically transmits the local\nmodel to the server at arbitrary times. We first derive the (approximate)\nexpression for the convergence rate based on the probabilistic client\nselection. Then, an optimization problem is formulated to trade off the\nconvergence rate of asynchronous FL and mobile energy consumption by joint\nprobabilistic client selection and bandwidth allocation. We develop an\niterative algorithm to solve the non-convex problem globally optimally.\nExperiments demonstrate the superiority of the proposed approach compared with\nthe traditional schemes.",
            "author": [
                "Jiarong Yang",
                "Yuan Liu",
                "Fangjiong Chen",
                "Wen Chen",
                "Changle Li"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16741v1",
                "http://arxiv.org/pdf/2311.16741v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.DC",
                "cs.NI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16738v1",
            "title": "Riemannian Self-Attention Mechanism for SPD Networks",
            "updated": "2023-11-28T12:34:46Z",
            "published": "2023-11-28T12:34:46Z",
            "summary": "Symmetric positive definite (SPD) matrix has been demonstrated to be an\neffective feature descriptor in many scientific areas, as it can encode\nspatiotemporal statistics of the data adequately on a curved Riemannian\nmanifold, i.e., SPD manifold. Although there are many different ways to design\nnetwork architectures for SPD matrix nonlinear learning, very few solutions\nexplicitly mine the geometrical dependencies of features at different layers.\nMotivated by the great success of self-attention mechanism in capturing\nlong-range relationships, an SPD manifold self-attention mechanism (SMSA) is\nproposed in this paper using some manifold-valued geometric operations, mainly\nthe Riemannian metric, Riemannian mean, and Riemannian optimization. Then, an\nSMSA-based geometric learning module (SMSA-GLM) is designed for the sake of\nimproving the discrimination of the generated deep structured representations.\nExtensive experimental results achieved on three benchmarking datasets show\nthat our modification against the baseline network further alleviates the\ninformation degradation problem and leads to improved accuracy.",
            "author": [
                "Rui Wang",
                "Xiao-Jun Wu",
                "Hui Li",
                "Josef Kittler"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16738v1",
                "http://arxiv.org/pdf/2311.16738v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17118v1",
            "title": "AdaFocus: Towards End-to-end Weakly Supervised Learning for Long-Video\n  Action Understanding",
            "updated": "2023-11-28T12:30:47Z",
            "published": "2023-11-28T12:30:47Z",
            "summary": "Developing end-to-end models for long-video action understanding tasks\npresents significant computational and memory challenges. Existing works\ngenerally build models on long-video features extracted by off-the-shelf action\nrecognition models, which are trained on short-video datasets in different\ndomains, making the extracted features suffer domain discrepancy. To avoid\nthis, action recognition models can be end-to-end trained on clips, which are\ntrimmed from long videos and labeled using action interval annotations. Such\nfully supervised annotations are expensive to collect. Thus, a weakly\nsupervised method is needed for long-video action understanding at scale. Under\nthe weak supervision setting, action labels are provided for the whole video\nwithout precise start and end times of the action clip. To this end, we propose\nan AdaFocus framework. AdaFocus estimates the spike-actionness and temporal\npositions of actions, enabling it to adaptively focus on action clips that\nfacilitate better training without the need for precise annotations.\nExperiments on three long-video datasets show its effectiveness. Remarkably, on\ntwo of datasets, models trained with AdaFocus under weak supervision outperform\nthose trained under full supervision. Furthermore, we form a weakly supervised\nfeature extraction pipeline with our AdaFocus, which enables significant\nimprovements on three long-video action understanding tasks.",
            "author": [
                "Jiaming Zhou",
                "Hanjun Li",
                "Kun-Yu Lin",
                "Junwei Liang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17118v1",
                "http://arxiv.org/pdf/2311.17118v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03735v1",
            "title": "Advancing State of the Art in Language Modeling",
            "updated": "2023-11-28T12:30:43Z",
            "published": "2023-11-28T12:30:43Z",
            "summary": "Generalization is arguably the most important goal of statistical language\nmodeling research. Publicly available benchmarks and papers published with an\nopen-source code have been critical to advancing the field. However, it is\noften very difficult, and sometimes even impossible, to reproduce the results\nfully as reported in publications. In this paper, we propose a simple framework\nthat should help advance the state of the art in language modeling in terms of\ngeneralization. We propose to publish not just the code, but also probabilities\non dev and test sets with future publications so that one can easily add the\nnew model into an ensemble. This has crucial advantages: it is much easier to\ndetermine whether a newly proposed model is actually complementary to the\ncurrent baseline. Therefore, instead of inventing new names for the old tricks,\nthe scientific community can advance faster. Finally, this approach promotes\ndiversity of ideas: one does not need to create an individual model that is the\nnew state of the art to attract attention; it will be sufficient to develop a\nnew model that learns patterns which other models do not. Thus, even a\nsuboptimal model can be found to have value. Remarkably, our approach has\nyielded new state-of-the-art results across various language modeling\nbenchmarks up to 10%.",
            "author": [
                "David Herel",
                "Tomas Mikolov"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03735v1",
                "http://arxiv.org/pdf/2312.03735v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16728v1",
            "title": "Photo-SLAM: Real-time Simultaneous Localization and Photorealistic\n  Mapping for Monocular, Stereo, and RGB-D Cameras",
            "updated": "2023-11-28T12:19:00Z",
            "published": "2023-11-28T12:19:00Z",
            "summary": "The integration of neural rendering and the SLAM system recently showed\npromising results in joint localization and photorealistic view reconstruction.\nHowever, existing methods, fully relying on implicit representations, are so\nresource-hungry that they cannot run on portable devices, which deviates from\nthe original intention of SLAM. In this paper, we present Photo-SLAM, a novel\nSLAM framework with a hyper primitives map. Specifically, we simultaneously\nexploit explicit geometric features for localization and learn implicit\nphotometric features to represent the texture information of the observed\nenvironment. In addition to actively densifying hyper primitives based on\ngeometric features, we further introduce a Gaussian-Pyramid-based training\nmethod to progressively learn multi-level features, enhancing photorealistic\nmapping performance. The extensive experiments with monocular, stereo, and\nRGB-D datasets prove that our proposed system Photo-SLAM significantly\noutperforms current state-of-the-art SLAM systems for online photorealistic\nmapping, e.g., PSNR is 30% higher and rendering speed is hundreds of times\nfaster in the Replica dataset. Moreover, the Photo-SLAM can run at real-time\nspeed using an embedded platform such as Jetson AGX Orin, showing the potential\nof robotics applications.",
            "author": [
                "Huajian Huang",
                "Longwei Li",
                "Hui Cheng",
                "Sai-Kit Yeung"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16728v1",
                "http://arxiv.org/pdf/2311.16728v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16727v1",
            "title": "Sluggish and Chemically-Biased Interstitial Diffusion in Concentrated\n  Solid Solution Alloys: Mechanisms and Methods",
            "updated": "2023-11-28T12:16:06Z",
            "published": "2023-11-28T12:16:06Z",
            "summary": "Interstitial diffusion is a pivotal process that governs the phase stability\nand irradiation response of materials in non-equilibrium conditions. In this\nwork, we study sluggish and chemically-biased interstitial diffusion in Fe-Ni\nconcentrated solid solution alloys (CSAs) by combining machine learning (ML)\nand kinetic Monte Carlo (kMC), where ML is used to accurately and efficiently\npredict the migration energy barriers on-the-fly. The ML-kMC reproduces the\ndiffusivity that was reported by molecular dynamics results at high\ntemperatures. With this powerful tool, we find that the observed sluggish\ndiffusion and the \"Ni-Ni-Ni\"-biased diffusion in Fe-Ni alloys are ascribed to a\nunique \"Barrier Lock\" mechanism, whereas the \"Fe-Fe-Fe\"-biased diffusion is\ninfluenced by a \"Component Dominance\" mechanism. Inspired by the mentioned\nmechanisms, a practical AvgS-kMC method is proposed for conveniently and\nswiftly determining interstitial-mediated diffusivity by only relying on the\nmean energy barriers of migration patterns. Combining the AvgS-kMC with the\ndifferential evolutionary algorithm, an inverse design strategy for optimizing\nsluggish diffusion properties is applied to emphasize the crucial role of\nfavorable migration patterns.",
            "author": [
                "Biao Xu",
                "Haijun Fu",
                "Shasha Huang",
                "Shihua Ma",
                "Yaoxu Xiong",
                "Jun Zhang",
                "Xuepeng Xiang",
                "Wenyu Lu",
                "Ji-Jung Kai",
                "Shijun Zhao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16727v1",
                "http://arxiv.org/pdf/2311.16727v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci",
                "cs.LG",
                "physics.atm-clus"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17113v1",
            "title": "Human Gaussian Splatting: Real-time Rendering of Animatable Avatars",
            "updated": "2023-11-28T12:05:41Z",
            "published": "2023-11-28T12:05:41Z",
            "summary": "This work addresses the problem of real-time rendering of photorealistic\nhuman body avatars learned from multi-view videos. While the classical\napproaches to model and render virtual humans generally use a textured mesh,\nrecent research has developed neural body representations that achieve\nimpressive visual quality. However, these models are difficult to render in\nreal-time and their quality degrades when the character is animated with body\nposes different than the training observations. We propose the first animatable\nhuman model based on 3D Gaussian Splatting, that has recently emerged as a very\nefficient alternative to neural radiance fields. Our body is represented by a\nset of gaussian primitives in a canonical space which are deformed in a coarse\nto fine approach that combines forward skinning and local non-rigid refinement.\nWe describe how to learn our Human Gaussian Splatting (\\OURS) model in an\nend-to-end fashion from multi-view observations, and evaluate it against the\nstate-of-the-art approaches for novel pose synthesis of clothed body. Our\nmethod presents a PSNR 1.5dbB better than the state-of-the-art on THuman4\ndataset while being able to render at 20fps or more.",
            "author": [
                "Arthur Moreau",
                "Jifei Song",
                "Helisa Dhamo",
                "Richard Shaw",
                "Yiren Zhou",
                "Eduardo P\u00e9rez-Pellitero"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17113v1",
                "http://arxiv.org/pdf/2311.17113v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16720v1",
            "title": "RankingGPT: Empowering Large Language Models in Text Ranking with\n  Progressive Enhancement",
            "updated": "2023-11-28T12:04:19Z",
            "published": "2023-11-28T12:04:19Z",
            "summary": "Text ranking is a critical task in various information retrieval\napplications, and the recent success of Large Language Models (LLMs) in natural\nlanguage processing has sparked interest in their application to text ranking.\nThese methods primarily involve combining query and candidate documents and\nleveraging prompt learning to determine query-document relevance using the\nLLM's output probabilities for specific tokens or by directly generating a\nranked list of candidate documents. Although these approaches have demonstrated\npromise, a noteworthy disparity arises between the training objective of LLMs,\nwhich typically centers around next token prediction, and the objective of\nevaluating query-document relevance. To address this gap and fully leverage LLM\npotential in text ranking tasks, we propose a progressive multi-stage training\nstrategy. Firstly, we introduce a large-scale weakly supervised dataset of\nrelevance texts to enable the LLMs to acquire the ability to predict relevant\ntokens without altering their original training objective. Subsequently, we\nincorporate supervised training to further enhance LLM ranking capability. Our\nexperimental results on multiple benchmarks demonstrate the superior\nperformance of our proposed method compared to previous competitive approaches,\nboth in in-domain and out-of-domain scenarios.",
            "author": [
                "Longhui Zhang",
                "Yanzhao Zhang",
                "Dingkun Long",
                "Pengjun Xie",
                "Meishan Zhang",
                "Min Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16720v1",
                "http://arxiv.org/pdf/2311.16720v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16716v1",
            "title": "Graph Pre-training and Prompt Learning for Recommendation",
            "updated": "2023-11-28T12:00:06Z",
            "published": "2023-11-28T12:00:06Z",
            "summary": "GNN-based recommenders have excelled in modeling intricate user-item\ninteractions through multi-hop message passing. However, existing methods often\noverlook the dynamic nature of evolving user-item interactions, which impedes\nthe adaption to changing user preferences and distribution shifts in newly\narriving data. Thus, their scalability and performances in real-world dynamic\nenvironments are limited. In this study, we propose GraphPL, a framework that\nincorporates parameter-efficient and dynamic graph pre-training with prompt\nlearning. This novel combination empowers GNNs to effectively capture both\nlong-term user preferences and short-term behavior dynamics, enabling the\ndelivery of accurate and timely recommendations. Our GraphPL framework\naddresses the challenge of evolving user preferences by seamlessly integrating\na temporal prompt mechanism and a graph-structural prompt learning mechanism\ninto the pre-trained GNN model. The temporal prompt mechanism encodes time\ninformation on user-item interaction, allowing the model to naturally capture\ntemporal context, while the graph-structural prompt learning mechanism enables\nthe transfer of pre-trained knowledge to adapt to behavior dynamics without the\nneed for continuous incremental training. We further bring in a dynamic\nevaluation setting for recommendation to mimic real-world dynamic scenarios and\nbridge the offline-online gap to a better level. Our extensive experiments\nincluding a large-scale industrial deployment showcases the lightweight plug-in\nscalability of our GraphPL when integrated with various state-of-the-art\nrecommenders, emphasizing the advantages of GraphPL in terms of effectiveness,\nrobustness and efficiency.",
            "author": [
                "Yuhao Yang",
                "Lianghao Xia",
                "Da Luo",
                "Kangyi Lin",
                "Chao Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16716v1",
                "http://arxiv.org/pdf/2311.16716v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16715v1",
            "title": "A computational framework for nanotrusses: input convex neural networks\n  approach",
            "updated": "2023-11-28T11:57:38Z",
            "published": "2023-11-28T11:57:38Z",
            "summary": "The present research aims to provide a practical numerical tool for the\nmechanical analysis of nanoscale trusses with similar accuracy to molecular\ndynamics (MD). As a first step, MD simulations of uniaxial tensile and\ncompression tests of all possible chiralities of single-walled carbon nanotubes\nup to 4 nm in diameter were performed using the AIREBO potential. The results\nrepresent a dataset consisting of stress/strain curves that were then used to\ndevelop a neural network that serves as a surrogate for a constitutive model\nfor all nanotubes considered. The cornerstone of the new framework is a\npartially input convex integrable neural network. It turns out that convexity\nenables favorable convergence properties required for implementation in the\nclassical nonlinear truss finite element available in Abaqus. This completes a\nmolecular dynamics-machine learning-finite element framework suitable for the\nstatic analysis of large, nanoscale, truss-like structures. The performance is\nverified through a comprehensive set of examples that demonstrate ease of use,\naccuracy, and robustness.",
            "author": [
                "Marko \u010cana\u0111ija",
                "Valentina Ko\u0161merl",
                "Martin Zlati\u0107",
                "Domagoj Vrtov\u0161nik",
                "Neven Munjas"
            ],
            "link": [
                "http://dx.doi.org/10.1016/j.euromechsol.2023.105195",
                "http://arxiv.org/abs/2311.16715v1",
                "http://arxiv.org/pdf/2311.16715v1"
            ],
            "primary_category": "physics.app-ph",
            "category": [
                "physics.app-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16714v1",
            "title": "Embodied Multi-Modal Agent trained by an LLM from a Parallel TextWorld",
            "updated": "2023-11-28T11:53:56Z",
            "published": "2023-11-28T11:53:56Z",
            "summary": "While large language models (LLMs) excel in a simulated world of texts, they\nstruggle to interact with the more realistic world without perceptions of other\nmodalities such as visual or audio signals. Although vision-language models\n(VLMs) integrate LLM modules (1) aligned with static image features, and (2)\nmay possess prior knowledge of world dynamics (as demonstrated in the text\nworld), they have not been trained in an embodied visual world and thus cannot\nalign with its dynamics. On the other hand, training an embodied agent in a\nnoisy visual world without expert guidance is often challenging and\ninefficient. In this paper, we train a VLM agent living in a visual world using\nan LLM agent excelling in a parallel text world (but inapplicable to the visual\nworld). Specifically, we distill LLM's reflection outcomes (improved actions by\nanalyzing mistakes) in a text world's tasks to finetune the VLM on the same\ntasks of the visual world, resulting in an Embodied Multi-Modal Agent (EMMA)\nquickly adapting to the visual world dynamics. Such cross-modality imitation\nlearning between the two parallel worlds enables EMMA to generalize to a broad\nscope of new tasks without any further guidance from the LLM expert. Extensive\nevaluations on the ALFWorld benchmark highlight EMMA's superior performance to\nSOTA VLM-based agents across diverse tasks, e.g., 20%-70% improvement in the\nsuccess rate.",
            "author": [
                "Yijun Yang",
                "Tianyi Zhou",
                "Kanxue Li",
                "Dapeng Tao",
                "Lusong Li",
                "Li Shen",
                "Xiaodong He",
                "Jing Jiang",
                "Yuhui Shi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16714v1",
                "http://arxiv.org/pdf/2311.16714v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16711v1",
            "title": "LEDITS++: Limitless Image Editing using Text-to-Image Models",
            "updated": "2023-11-28T11:45:35Z",
            "published": "2023-11-28T11:45:35Z",
            "summary": "Text-to-image diffusion models have recently received increasing interest for\ntheir astonishing ability to produce high-fidelity images from solely text\ninputs. Subsequent research efforts aim to exploit and apply their capabilities\nto real image editing. However, existing image-to-image methods are often\ninefficient, imprecise, and of limited versatility. They either require\ntime-consuming fine-tuning, deviate unnecessarily strongly from the input\nimage, and/or lack support for multiple, simultaneous edits. To address these\nissues, we introduce LEDITS++, an efficient yet versatile and precise textual\nimage manipulation technique. LEDITS++'s novel inversion approach requires no\ntuning nor optimization and produces high-fidelity results with a few diffusion\nsteps. Second, our methodology supports multiple simultaneous edits and is\narchitecture-agnostic. Third, we use a novel implicit masking technique that\nlimits changes to relevant image regions. We propose the novel TEdBench++\nbenchmark as part of our exhaustive evaluation. Our results demonstrate the\ncapabilities of LEDITS++ and its improvements over previous methods. The\nproject page is available at https://leditsplusplus-project.static.hf.space .",
            "author": [
                "Manuel Brack",
                "Felix Friedrich",
                "Katharina Kornmeier",
                "Linoy Tsaban",
                "Patrick Schramowski",
                "Kristian Kersting",
                "Apolin\u00e1rio Passos"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16711v1",
                "http://arxiv.org/pdf/2311.16711v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.HC",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16706v1",
            "title": "Sinkhorn Flow: A Continuous-Time Framework for Understanding and\n  Generalizing the Sinkhorn Algorithm",
            "updated": "2023-11-28T11:29:12Z",
            "published": "2023-11-28T11:29:12Z",
            "summary": "Many problems in machine learning can be formulated as solving\nentropy-regularized optimal transport on the space of probability measures. The\ncanonical approach involves the Sinkhorn iterates, renowned for their rich\nmathematical properties. Recently, the Sinkhorn algorithm has been recast\nwithin the mirror descent framework, thus benefiting from classical\noptimization theory insights. Here, we build upon this result by introducing a\ncontinuous-time analogue of the Sinkhorn algorithm. This perspective allows us\nto derive novel variants of Sinkhorn schemes that are robust to noise and bias.\nMoreover, our continuous-time dynamics not only generalize but also offer a\nunified perspective on several recently discovered dynamics in machine learning\nand mathematics, such as the \"Wasserstein mirror flow\" of (Deb et al. 2023) or\nthe \"mean-field Schr\\\"odinger equation\" of (Claisse et al. 2023).",
            "author": [
                "Mohammad Reza Karimi",
                "Ya-Ping Hsieh",
                "Andreas Krause"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16706v1",
                "http://arxiv.org/pdf/2311.16706v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "math.PR",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16700v1",
            "title": "Rethinking Intermediate Layers design in Knowledge Distillation for\n  Kidney and Liver Tumor Segmentation",
            "updated": "2023-11-28T11:22:08Z",
            "published": "2023-11-28T11:22:08Z",
            "summary": "Knowledge distillation(KD) has demonstrated remarkable success across various\ndomains, but its application to medical imaging tasks, such as kidney and liver\ntumor segmentation, has encountered challenges. Many existing KD methods are\nnot specifically tailored for these tasks. Moreover, prevalent KD methods often\nlack a careful consideration of what and from where to distill knowledge from\nthe teacher to the student. This oversight may lead to issues like the\naccumulation of training bias within shallower student layers, potentially\ncompromising the effectiveness of KD. To address these challenges, we propose\nHierarchical Layer-selective Feedback Distillation (HLFD). HLFD strategically\ndistills knowledge from a combination of middle layers to earlier layers and\ntransfers final layer knowledge to intermediate layers at both the feature and\npixel levels. This design allows the model to learn higher-quality\nrepresentations from earlier layers, resulting in a robust and compact student\nmodel. Extensive quantitative evaluations reveal that HLFD outperforms existing\nmethods by a significant margin. For example, in the kidney segmentation task,\nHLFD surpasses the student model (without KD) by over 10pp, significantly\nimproving its focus on tumor-specific features. From a qualitative standpoint,\nthe student model trained using HLFD excels at suppressing irrelevant\ninformation and can focus sharply on tumor-specific details, which opens a new\npathway for more efficient and accurate diagnostic tools.",
            "author": [
                "Vandan Gorade",
                "Sparsh Mittal",
                "Debesh Jha",
                "Ulas Bagci"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16700v1",
                "http://arxiv.org/pdf/2311.16700v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG",
                "q-bio.TO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16692v1",
            "title": "Learning how to find targets in the micro-world: The case of\n  intermittent active Brownian particles",
            "updated": "2023-11-28T11:05:13Z",
            "published": "2023-11-28T11:05:13Z",
            "summary": "Finding the best strategy to minimize the time needed to find a given target\nis a crucial task both in nature and in reaching decisive technological\nadvances. By considering learning agents able to switch their dynamics between\nstandard and active Brownian motion, here we focus on developing effective\ntarget-search behavioral policies for microswimmers navigating a homogeneous\nenvironment and searching for targets of unknown position. We exploit\nProjective Simulation, a reinforcement learning algorithm, to acquire an\nefficient stochastic policy represented by the probability of switching the\nphase, i.e. the navigation mode, in response to the type and the duration of\nthe current phase. Our findings reveal that the target-search efficiency\nincreases with the particle's self-propulsion during the active phase and that,\nwhile the optimal duration of the passive case decreases monotonically with the\nactivity, the optimal duration of the active phase displays a non-monotonic\nbehavior.",
            "author": [
                "Michele Caraglio",
                "Harpreet Kaur",
                "Lukas J. Fiderer",
                "Andrea L\u00f3pez-Incera",
                "Hans J. Briegel",
                "Thomas Franosch",
                "Gorka Mu\u00f1oz-Gil"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16692v1",
                "http://arxiv.org/pdf/2311.16692v1"
            ],
            "primary_category": "cond-mat.soft",
            "category": [
                "cond-mat.soft",
                "cond-mat.stat-mech",
                "physics.bio-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17110v1",
            "title": "XAI for time-series classification leveraging image highlight methods",
            "updated": "2023-11-28T10:59:18Z",
            "published": "2023-11-28T10:59:18Z",
            "summary": "Although much work has been done on explainability in the computer vision and\nnatural language processing (NLP) fields, there is still much work to be done\nto explain methods applied to time series as time series by nature can not be\nunderstood at first sight. In this paper, we present a Deep Neural Network\n(DNN) in a teacher-student architecture (distillation model) that offers\ninterpretability in time-series classification tasks. The explainability of our\napproach is based on transforming the time series to 2D plots and applying\nimage highlight methods (such as LIME and GradCam), making the predictions\ninterpretable. At the same time, the proposed approach offers increased\naccuracy competing with the baseline model with the trade-off of increasing the\ntraining time.",
            "author": [
                "Georgios Makridis",
                "Georgios Fatouros",
                "Vasileios Koukos",
                "Dimitrios Kotios",
                "Dimosthenis Kyriazis",
                "Ioannis Soldatos"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17110v1",
                "http://arxiv.org/pdf/2311.17110v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16684v1",
            "title": "A Unified Hardware-based Threat Detector for AI Accelerators",
            "updated": "2023-11-28T10:55:02Z",
            "published": "2023-11-28T10:55:02Z",
            "summary": "The proliferation of AI technology gives rise to a variety of security\nthreats, which significantly compromise the confidentiality and integrity of AI\nmodels and applications. Existing software-based solutions mainly target one\nspecific attack, and require the implementation into the models, rendering them\nless practical. We design UniGuard, a novel unified and non-intrusive detection\nmethodology to safeguard FPGA-based AI accelerators. The core idea of UniGuard\nis to harness power side-channel information generated during model inference\nto spot any anomaly. We employ a Time-to-Digital Converter to capture power\nfluctuations and train a supervised machine learning model to identify various\ntypes of threats. Evaluations demonstrate that UniGuard can achieve 94.0%\nattack detection accuracy, with high generalization over unknown or adaptive\nattacks and robustness against varied configurations (e.g., sensor frequency\nand location).",
            "author": [
                "Xiaobei Yan",
                "Han Qiu",
                "Tianwei Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16684v1",
                "http://arxiv.org/pdf/2311.16684v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16683v1",
            "title": "Hyper-Relational Knowledge Graph Neural Network for Next POI",
            "updated": "2023-11-28T10:55:00Z",
            "published": "2023-11-28T10:55:00Z",
            "summary": "With the advancement of mobile technology, Point of Interest (POI)\nrecommendation systems in Location-based Social Networks (LBSN) have brought\nnumerous benefits to both users and companies. Many existing works employ\nKnowledge Graph (KG) to alleviate the data sparsity issue in LBSN. These\napproaches primarily focus on modeling the pair-wise relations in LBSN to\nenrich the semantics and thereby relieve the data sparsity issue. However,\nexisting approaches seldom consider the hyper-relations in LBSN, such as the\nmobility relation (a 3-ary relation: user-POI-time). This makes the model hard\nto exploit the semantics accurately. In addition, prior works overlook the rich\nstructural information inherent in KG, which consists of higher-order relations\nand can further alleviate the impact of data sparsity.To this end, we propose a\nHyper-Relational Knowledge Graph Neural Network (HKGNN) model. In HKGNN, a\nHyper-Relational Knowledge Graph (HKG) that models the LBSN data is constructed\nto maintain and exploit the rich semantics of hyper-relations. Then we proposed\na Hypergraph Neural Network to utilize the structural information of HKG in a\ncohesive way. In addition, a self-attention network is used to leverage\nsequential information and make personalized recommendations. Furthermore, side\ninformation, essential in reducing data sparsity by providing background\nknowledge of POIs, is not fully utilized in current methods. In light of this,\nwe extended the current dataset with available side information to further\nlessen the impact of data sparsity. Results of experiments on four real-world\nLBSN datasets demonstrate the effectiveness of our approach compared to\nexisting state-of-the-art methods.",
            "author": [
                "Jixiao Zhang",
                "Yongkang Li",
                "Ruotong Zou",
                "Jingyuan Zhang",
                "Zipei Fan",
                "Xuan Song"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16683v1",
                "http://arxiv.org/pdf/2311.16683v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.IR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16682v1",
            "title": "ContextSeg: Sketch Semantic Segmentation by Querying the Context with\n  Attention",
            "updated": "2023-11-28T10:53:55Z",
            "published": "2023-11-28T10:53:55Z",
            "summary": "Sketch semantic segmentation is a well-explored and pivotal problem in\ncomputer vision involving the assignment of pre-defined part labels to\nindividual strokes. This paper presents ContextSeg - a simple yet highly\neffective approach to tackling this problem with two stages. In the first\nstage, to better encode the shape and positional information of strokes, we\npropose to predict an extra dense distance field in an autoencoder network to\nreinforce structural information learning. In the second stage, we treat an\nentire stroke as a single entity and label a group of strokes within the same\nsemantic part using an auto-regressive Transformer with the default attention\nmechanism. By group-based labeling, our method can fully leverage the context\ninformation when making decisions for the remaining groups of strokes. Our\nmethod achieves the best segmentation accuracy compared with state-of-the-art\napproaches on two representative datasets and has been extensively evaluated\ndemonstrating its superior performance. Additionally, we offer insights into\nsolving part imbalance in training data and the preliminary experiment on\ncross-category training, which can inspire future research in this field.",
            "author": [
                "Jiawei Wang",
                "Changjian Li"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16682v1",
                "http://arxiv.org/pdf/2311.16682v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16680v2",
            "title": "ROSO: Improving Robotic Policy Inference via Synthetic Observations",
            "updated": "2023-11-29T05:16:40Z",
            "published": "2023-11-28T10:52:35Z",
            "summary": "In this paper, we propose the use of generative artificial intelligence (AI)\nto improve zero-shot performance of a pre-trained policy by altering\nobservations during inference. Modern robotic systems, powered by advanced\nneural networks, have demonstrated remarkable capabilities on pre-trained\ntasks. However, generalizing and adapting to new objects and environments is\nchallenging, and fine-tuning visuomotor policies is time-consuming. To overcome\nthese issues we propose Robotic Policy Inference via Synthetic Observations\n(ROSO). ROSO uses stable diffusion to pre-process a robot's observation of\nnovel objects during inference time to fit within its distribution of\nobservations of the pre-trained policies. This novel paradigm allows us to\ntransfer learned knowledge from known tasks to previously unseen scenarios,\nenhancing the robot's adaptability without requiring lengthy fine-tuning. Our\nexperiments show that incorporating generative AI into robotic inference\nsignificantly improves successful outcomes, finishing up to 57% of tasks\notherwise unsuccessful with the pre-trained policy.",
            "author": [
                "Yusuke Miyashita",
                "Dimitris Gahtidis",
                "Colin La",
                "Jeremy Rabinowicz",
                "Jurgen Leitner"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16680v2",
                "http://arxiv.org/pdf/2311.16680v2"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17109v1",
            "title": "Neural Texture Puppeteer: A Framework for Neural Geometry and Texture\n  Rendering of Articulated Shapes, Enabling Re-Identification at Interactive\n  Speed",
            "updated": "2023-11-28T10:51:05Z",
            "published": "2023-11-28T10:51:05Z",
            "summary": "In this paper, we present a neural rendering pipeline for textured\narticulated shapes that we call Neural Texture Puppeteer. Our method separates\ngeometry and texture encoding. The geometry pipeline learns to capture spatial\nrelationships on the surface of the articulated shape from ground truth data\nthat provides this geometric information. A texture auto-encoder makes use of\nthis information to encode textured images into a global latent code. This\nglobal texture embedding can be efficiently trained separately from the\ngeometry, and used in a downstream task to identify individuals. The neural\ntexture rendering and the identification of individuals run at interactive\nspeeds. To the best of our knowledge, we are the first to offer a promising\nalternative to CNN- or transformer-based approaches for re-identification of\narticulated individuals based on neural rendering. Realistic looking novel view\nand pose synthesis for different synthetic cow textures further demonstrate the\nquality of our method. Restricted by the availability of ground truth data for\nthe articulated shape's geometry, the quality for real-world data synthesis is\nreduced. We further demonstrate the flexibility of our model for real-world\ndata by applying a synthetic to real-world texture domain shift where we\nreconstruct the texture from a real-world 2D RGB image. Thus, our method can be\napplied to endangered species where data is limited. Our novel synthetic\ntexture dataset NePuMoo is publicly available to inspire further development in\nthe field of neural rendering-based re-identification.",
            "author": [
                "Urs Waldmann",
                "Ole Johannsen",
                "Bastian Goldluecke"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17109v1",
                "http://arxiv.org/pdf/2311.17109v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16670v1",
            "title": "PyTorch Geometric High Order: A Unified Library for High Order Graph\n  Neural Network",
            "updated": "2023-11-28T10:34:48Z",
            "published": "2023-11-28T10:34:48Z",
            "summary": "We introduce PyTorch Geometric High Order (PyGHO), a library for High Order\nGraph Neural Networks (HOGNNs) that extends PyTorch Geometric (PyG). Unlike\nordinary Message Passing Neural Networks (MPNNs) that exchange messages between\nnodes, HOGNNs, encompassing subgraph GNNs and k-WL GNNs, encode node tuples, a\nmethod previously lacking a standardized framework and often requiring complex\ncoding. PyGHO's main objective is to provide an unified and user-friendly\ninterface for various HOGNNs. It accomplishes this through streamlined data\nstructures for node tuples, comprehensive data processing utilities, and a\nflexible suite of operators for high-order GNN methodologies. In this work, we\npresent a detailed in-depth of PyGHO and compare HOGNNs implemented with PyGHO\nwith their official implementation on real-world tasks. PyGHO achieves up to\n$50\\%$ acceleration and reduces the code needed for implementation by an order\nof magnitude. Our library is available at\n\\url{https://github.com/GraphPKU/PygHO}.",
            "author": [
                "Xiyuan Wang",
                "Muhan Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16670v1",
                "http://arxiv.org/pdf/2311.16670v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16666v1",
            "title": "MultiModal-Learning for Predicting Molecular Properties: A Framework\n  Based on Image and Graph Structures",
            "updated": "2023-11-28T10:28:35Z",
            "published": "2023-11-28T10:28:35Z",
            "summary": "The quest for accurate prediction of drug molecule properties poses a\nfundamental challenge in the realm of Artificial Intelligence Drug Discovery\n(AIDD). An effective representation of drug molecules emerges as a pivotal\ncomponent in this pursuit. Contemporary leading-edge research predominantly\nresorts to self-supervised learning (SSL) techniques to extract meaningful\nstructural representations from large-scale, unlabeled molecular data,\nsubsequently fine-tuning these representations for an array of downstream\ntasks. However, an inherent shortcoming of these studies lies in their singular\nreliance on one modality of molecular information, such as molecule image or\nSMILES representations, thus neglecting the potential complementarity of\nvarious molecular modalities. In response to this limitation, we propose MolIG,\na novel MultiModaL molecular pre-training framework for predicting molecular\nproperties based on Image and Graph structures. MolIG model innovatively\nleverages the coherence and correlation between molecule graph and molecule\nimage to execute self-supervised tasks, effectively amalgamating the strengths\nof both molecular representation forms. This holistic approach allows for the\ncapture of pivotal molecular structural characteristics and high-level semantic\ninformation. Upon completion of pre-training, Graph Neural Network (GNN)\nEncoder is used for the prediction of downstream tasks. In comparison to\nadvanced baseline models, MolIG exhibits enhanced performance in downstream\ntasks pertaining to molecular property prediction within benchmark groups such\nas MoleculeNet Benchmark Group and ADMET Benchmark Group.",
            "author": [
                "Zhuoyuan Wang",
                "Jiacong Mi",
                "Shan Lu",
                "Jieyue He"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16666v1",
                "http://arxiv.org/pdf/2311.16666v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "physics.chem-ph",
                "q-bio.BM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17107v1",
            "title": "ClimateX: Do LLMs Accurately Assess Human Expert Confidence in Climate\n  Statements?",
            "updated": "2023-11-28T10:26:57Z",
            "published": "2023-11-28T10:26:57Z",
            "summary": "Evaluating the accuracy of outputs generated by Large Language Models (LLMs)\nis especially important in the climate science and policy domain. We introduce\nthe Expert Confidence in Climate Statements (ClimateX) dataset, a novel,\ncurated, expert-labeled dataset consisting of 8094 climate statements collected\nfrom the latest Intergovernmental Panel on Climate Change (IPCC) reports,\nlabeled with their associated confidence levels. Using this dataset, we show\nthat recent LLMs can classify human expert confidence in climate-related\nstatements, especially in a few-shot learning setting, but with limited (up to\n47%) accuracy. Overall, models exhibit consistent and significant\nover-confidence on low and medium confidence statements. We highlight\nimplications of our results for climate communication, LLMs evaluation\nstrategies, and the use of LLMs in information retrieval systems.",
            "author": [
                "Romain Lacombe",
                "Kerrie Wu",
                "Eddie Dilworth"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17107v1",
                "http://arxiv.org/pdf/2311.17107v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CL",
                "cs.CY",
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16664v1",
            "title": "DGNR: Density-Guided Neural Point Rendering of Large Driving Scenes",
            "updated": "2023-11-28T10:25:55Z",
            "published": "2023-11-28T10:25:55Z",
            "summary": "Despite the recent success of Neural Radiance Field (NeRF), it is still\nchallenging to render large-scale driving scenes with long trajectories,\nparticularly when the rendering quality and efficiency are in high demand.\nExisting methods for such scenes usually involve with spatial warping,\ngeometric supervision from zero-shot normal or depth estimation, or scene\ndivision strategies, where the synthesized views are often blurry or fail to\nmeet the requirement of efficient rendering. To address the above challenges,\nthis paper presents a novel framework that learns a density space from the\nscenes to guide the construction of a point-based renderer, dubbed as DGNR\n(Density-Guided Neural Rendering). In DGNR, geometric priors are no longer\nneeded, which can be intrinsically learned from the density space through\nvolumetric rendering. Specifically, we make use of a differentiable renderer to\nsynthesize images from the neural density features obtained from the learned\ndensity space. A density-based fusion module and geometric regularization are\nproposed to optimize the density space. By conducting experiments on a widely\nused autonomous driving dataset, we have validated the effectiveness of DGNR in\nsynthesizing photorealistic driving scenes and achieving real-time capable\nrendering.",
            "author": [
                "Zhuopeng Li",
                "Chenming Wu",
                "Liangjun Zhang",
                "Jianke Zhu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16664v1",
                "http://arxiv.org/pdf/2311.16664v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16656v1",
            "title": "Pseudo-Likelihood Inference",
            "updated": "2023-11-28T10:17:52Z",
            "published": "2023-11-28T10:17:52Z",
            "summary": "Simulation-Based Inference (SBI) is a common name for an emerging family of\napproaches that infer the model parameters when the likelihood is intractable.\nExisting SBI methods either approximate the likelihood, such as Approximate\nBayesian Computation (ABC) or directly model the posterior, such as Sequential\nNeural Posterior Estimation (SNPE). While ABC is efficient on low-dimensional\nproblems, on higher-dimensional tasks, it is generally outperformed by SNPE,\nwhich leverages function approximation. In this paper, we propose\nPseudo-Likelihood Inference (PLI), a new method that brings neural\napproximation into ABC, making it competitive on challenging Bayesian system\nidentification tasks. By utilizing integral probability metrics, we introduce a\nsmooth likelihood kernel with an adaptive bandwidth that is updated based on\ninformation-theoretic trust regions. Thanks to this formulation, our method (i)\nallows for optimizing neural posteriors via gradient descent, (ii) does not\nrely on summary statistics, and (iii) enables multiple observations as input.\nIn comparison to SNPE, it leads to improved performance when more data is\navailable. The effectiveness of PLI is evaluated on four classical SBI\nbenchmark tasks and on a highly dynamic physical system, showing particular\nadvantages on stochastic simulations and multi-modal posterior landscapes.",
            "author": [
                "Theo Gruner",
                "Boris Belousov",
                "Fabio Muratore",
                "Daniel Palenicek",
                "Jan Peters"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16656v1",
                "http://arxiv.org/pdf/2311.16656v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16654v1",
            "title": "Elucidating Discrepancy in Explanations of Predictive Models Developed\n  using EMR",
            "updated": "2023-11-28T10:13:31Z",
            "published": "2023-11-28T10:13:31Z",
            "summary": "The lack of transparency and explainability hinders the clinical adoption of\nMachine learning (ML) algorithms. While explainable artificial intelligence\n(XAI) methods have been proposed, little research has focused on the agreement\nbetween these methods and expert clinical knowledge. This study applies current\nstate-of-the-art explainability methods to clinical decision support algorithms\ndeveloped for Electronic Medical Records (EMR) data to analyse the concordance\nbetween these factors and discusses causes for identified discrepancies from a\nclinical and technical perspective. Important factors for achieving trustworthy\nXAI solutions for clinical decision support are also discussed.",
            "author": [
                "Aida Brankovic",
                "Wenjie Huang",
                "David Cook",
                "Sankalp Khanna",
                "Konstanty Bialkowski"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16654v1",
                "http://arxiv.org/pdf/2311.16654v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16652v1",
            "title": "Augmenting x-ray single particle imaging reconstruction with\n  self-supervised machine learning",
            "updated": "2023-11-28T10:05:44Z",
            "published": "2023-11-28T10:05:44Z",
            "summary": "The development of X-ray Free Electron Lasers (XFELs) has opened numerous\nopportunities to probe atomic structure and ultrafast dynamics of various\nmaterials. Single Particle Imaging (SPI) with XFELs enables the investigation\nof biological particles in their natural physiological states with unparalleled\ntemporal resolution, while circumventing the need for cryogenic conditions or\ncrystallization. However, reconstructing real-space structures from\nreciprocal-space x-ray diffraction data is highly challenging due to the\nabsence of phase and orientation information, which is further complicated by\nweak scattering signals and considerable fluctuations in the number of photons\nper pulse. In this work, we present an end-to-end, self-supervised machine\nlearning approach to recover particle orientations and estimate reciprocal\nspace intensities from diffraction images only. Our method demonstrates great\nrobustness under demanding experimental conditions with significantly enhanced\nreconstruction capabilities compared with conventional algorithms, and\nsignifies a paradigm shift in SPI as currently practiced at XFELs.",
            "author": [
                "Zhantao Chen",
                "Cong Wang",
                "Mingye Gao",
                "Chun Hong Yoon",
                "Jana B. Thayer",
                "Joshua J. Turner"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16652v1",
                "http://arxiv.org/pdf/2311.16652v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "eess.IV",
                "physics.app-ph",
                "physics.comp-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16650v1",
            "title": "Text2Tree: Aligning Text Representation to the Label Tree Hierarchy for\n  Imbalanced Medical Classification",
            "updated": "2023-11-28T10:02:08Z",
            "published": "2023-11-28T10:02:08Z",
            "summary": "Deep learning approaches exhibit promising performances on various text\ntasks. However, they are still struggling on medical text classification since\nsamples are often extremely imbalanced and scarce. Different from existing\nmainstream approaches that focus on supplementary semantics with external\nmedical information, this paper aims to rethink the data challenges in medical\ntexts and present a novel framework-agnostic algorithm called Text2Tree that\nonly utilizes internal label hierarchy in training deep learning models. We\nembed the ICD code tree structure of labels into cascade attention modules for\nlearning hierarchy-aware label representations. Two new learning schemes,\nSimilarity Surrogate Learning (SSL) and Dissimilarity Mixup Learning (DML), are\ndevised to boost text classification by reusing and distinguishing samples of\nother labels following the label representation hierarchy, respectively.\nExperiments on authoritative public datasets and real-world medical records\nshow that our approach stably achieves superior performances over classical and\nadvanced imbalanced classification methods.",
            "author": [
                "Jiahuan Yan",
                "Haojun Gao",
                "Zhang Kai",
                "Weize Liu",
                "Danny Chen",
                "Jian Wu",
                "Jintai Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16650v1",
                "http://arxiv.org/pdf/2311.16650v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16646v1",
            "title": "Rethinking Backdoor Attacks on Dataset Distillation: A Kernel Method\n  Perspective",
            "updated": "2023-11-28T09:53:05Z",
            "published": "2023-11-28T09:53:05Z",
            "summary": "Dataset distillation offers a potential means to enhance data efficiency in\ndeep learning. Recent studies have shown its ability to counteract backdoor\nrisks present in original training samples. In this study, we delve into the\ntheoretical aspects of backdoor attacks and dataset distillation based on\nkernel methods. We introduce two new theory-driven trigger pattern generation\nmethods specialized for dataset distillation. Following a comprehensive set of\nanalyses and experiments, we show that our optimization-based trigger design\nframework informs effective backdoor attacks on dataset distillation. Notably,\ndatasets poisoned by our designed trigger prove resilient against conventional\nbackdoor attack detection and mitigation methods. Our empirical results\nvalidate that the triggers developed using our approaches are proficient at\nexecuting resilient backdoor attacks.",
            "author": [
                "Ming-Yu Chung",
                "Sheng-Yen Chou",
                "Chia-Mu Yu",
                "Pin-Yu Chen",
                "Sy-Yen Kuo",
                "Tsung-Yi Ho"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16646v1",
                "http://arxiv.org/pdf/2311.16646v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16645v1",
            "title": "Deriving and Evaluating a Detailed Taxonomy of Game Bugs",
            "updated": "2023-11-28T09:51:42Z",
            "published": "2023-11-28T09:51:42Z",
            "summary": "Game development has become an extremely competitive multi-billion-dollar\nindustry. Many games fail even after years of development efforts because of\ngame-breaking bugs that disrupt the game-play and ruin the player experience.\nThe goal of this work is to provide a bug taxonomy for games that will help\ngame developers in developing bug-resistant games, game testers in designing\nand executing fault-finding test cases, and researchers in evaluating game\ntesting approaches. For this purpose, we performed a Multivocal Literature\nReview (MLR) by analyzing 436 sources, out of which 189 (78 academic and 111\ngrey) sources reporting bugs encountered in the game development industry were\nselected for analysis. We validate the proposed taxonomy by conducting a survey\ninvolving different game industry practitioners. The MLR allowed us to finalize\na detailed taxonomy of 63 game bug categories in end-user perspective including\neight first-tier categories: Gaming Balance, Implementation Response, Network,\nSound, Temporal, Unexpected Crash, Navigational, and Non-Temporal faults. We\nobserved that manual approaches towards game testing are still widely used.\nOnly one of the approaches targets sound bugs whereas game balancing and how to\nincorporate machine learning in game testing is trending in the recent\nliterature. Most of the game testing techniques are specialized and dependent\non specific platforms.",
            "author": [
                "Nigar Azhar Butt",
                "Salman Sherin",
                "Muhammad Uzair Khan",
                "Atif Aftab Jilani",
                "Muhammad Zohaib Iqbal"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16645v1",
                "http://arxiv.org/pdf/2311.16645v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16638v1",
            "title": "Simultaneous Analysis of Continuously Embedded Reissner-Mindlin Shells\n  in 3D Bulk Domains",
            "updated": "2023-11-28T09:44:51Z",
            "published": "2023-11-28T09:44:51Z",
            "summary": "A mechanical model and numerical method for the simultaneous analysis of\nReissner-Mindlin shells with geometries implied by a continuous set of level\nsets (isosurfaces) over some three-dimensional bulk domain is presented. A\nthree-dimensional mesh in the bulk domain is used in a tailored FEM formulation\nwhere the elements are by no means conforming to the level sets representing\nthe shape of the individual shells. However, the shell geometries are bounded\nby the intersection curves of the level sets with the boundary of the bulk\ndomain so that the boundaries are meshed conformingly. This results in a method\nwhich was coined Bulk Trace FEM before. The simultaneously considered,\ncontinuously embedded shells may be useful in the structural design process or\nfor the continuous reinforcement of bulk domains. Numerical results confirm\nhigher-order convergence rates.",
            "author": [
                "Michael Wolfgang Kaiser",
                "Thomas-Peter Fries"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16638v1",
                "http://arxiv.org/pdf/2311.16638v1"
            ],
            "primary_category": "cs.CE",
            "category": [
                "cs.CE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16633v2",
            "title": "21-cm Signal from the Epoch of Reionization: A Machine Learning upgrade\n  to Foreground Removal with Gaussian Process Regression",
            "updated": "2023-11-29T11:07:01Z",
            "published": "2023-11-28T09:36:38Z",
            "summary": "In recent years, a Gaussian Process Regression (GPR) based framework has been\ndeveloped for foreground mitigation from data collected by the LOw-Frequency\nARray (LOFAR), to measure the 21-cm signal power spectrum from the Epoch of\nReionization (EoR) and Cosmic Dawn. However, it has been noted that through\nthis method there can be a significant amount of signal loss if the EoR signal\ncovariance is misestimated. To obtain better covariance models, we propose to\nuse a kernel trained on the {\\tt GRIZZLY} simulations using a Variational\nAuto-Encoder (VAE) based algorithm. In this work, we explore the abilities of\nthis Machine Learning based kernel (VAE kernel) used with GPR, by testing it on\nmock signals from a variety of simulations, exploring noise levels\ncorresponding to $\\approx$10 nights ($\\approx$141 hours) and $\\approx$100\nnights ($\\approx$1410 hours) of observations with LOFAR. Our work suggests the\npossibility of successful extraction of the 21-cm signal within 2$\\sigma$\nuncertainty in most cases using the VAE kernel, with better recovery of both\nshape and power than with previously used covariance models. We also explore\nthe role of the excess noise component identified in past applications of GPR\nand additionally analyse the possibility of redshift dependence on the\nperformance of the VAE kernel. The latter allows us to prepare for future LOFAR\nobservations at a range of redshifts, as well as compare with results from\nother telescopes.",
            "author": [
                "Anshuman Acharya",
                "Florent Mertens",
                "Benedetta Ciardi",
                "Raghunath Ghara",
                "L\u00e9on V. E. Koopmans",
                "Sambit K. Giri",
                "Ian Hothi",
                "Qing-Bo Ma",
                "Garrelt Mellema",
                "Satyapan Munshi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16633v2",
                "http://arxiv.org/pdf/2311.16633v2"
            ],
            "primary_category": "astro-ph.CO",
            "category": [
                "astro-ph.CO",
                "astro-ph.IM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16632v1",
            "title": "Opening the Black Box: Towards inherently interpretable energy data\n  imputation models using building physics insight",
            "updated": "2023-11-28T09:34:44Z",
            "published": "2023-11-28T09:34:44Z",
            "summary": "Missing data are frequently observed by practitioners and researchers in the\nbuilding energy modeling community. In this regard, advanced data-driven\nsolutions, such as Deep Learning methods, are typically required to reflect the\nnon-linear behavior of these anomalies. As an ongoing research question related\nto Deep Learning, a model's applicability to limited data settings can be\nexplored by introducing prior knowledge in the network. This same strategy can\nalso lead to more interpretable predictions, hence facilitating the field\napplication of the approach. For that purpose, the aim of this paper is to\npropose the use of Physics-informed Denoising Autoencoders (PI-DAE) for missing\ndata imputation in commercial buildings. In particular, the presented method\nenforces physics-inspired soft constraints to the loss function of a Denoising\nAutoencoder (DAE). In order to quantify the benefits of the physical component,\nan ablation study between different DAE configurations is conducted. First,\nthree univariate DAEs are optimized separately on indoor air temperature,\nheating, and cooling data. Then, two multivariate DAEs are derived from the\nprevious configurations. Eventually, a building thermal balance equation is\ncoupled to the last multivariate configuration to obtain PI-DAE. Additionally,\ntwo commonly used benchmarks are employed to support the findings. It is shown\nhow introducing physical knowledge in a multivariate Denoising Autoencoder can\nenhance the inherent model interpretability through the optimized physics-based\ncoefficients. While no significant improvement is observed in terms of\nreconstruction error with the proposed PI-DAE, its enhanced robustness to\nvarying rates of missing data and the valuable insights derived from the\nphysics-based coefficients create opportunities for wider applications within\nbuilding systems and the built environment.",
            "author": [
                "Antonio Liguori",
                "Matias Quintana",
                "Chun Fu",
                "Clayton Miller",
                "J\u00e9r\u00f4me Frisch",
                "Christoph van Treeck"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16632v1",
                "http://arxiv.org/pdf/2311.16632v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17105v1",
            "title": "On the Calibration of Human Pose Estimation",
            "updated": "2023-11-28T09:31:09Z",
            "published": "2023-11-28T09:31:09Z",
            "summary": "Most 2D human pose estimation frameworks estimate keypoint confidence in an\nad-hoc manner, using heuristics such as the maximum value of heatmaps. The\nconfidence is part of the evaluation scheme, e.g., AP for the MSCOCO dataset,\nyet has been largely overlooked in the development of state-of-the-art methods.\nThis paper takes the first steps in addressing miscalibration in pose\nestimation. From a calibration point of view, the confidence should be aligned\nwith the pose accuracy. In practice, existing methods are poorly calibrated. We\nshow, through theoretical analysis, why a miscalibration gap exists and how to\nnarrow the gap. Simply predicting the instance size and adjusting the\nconfidence function gives considerable AP improvements. Given the black-box\nnature of deep neural networks, however, it is not possible to fully close this\ngap with only closed-form adjustments. As such, we go one step further and\nlearn network-specific adjustments by enforcing consistency between confidence\nand pose accuracy. Our proposed Calibrated ConfidenceNet (CCNet) is a\nlight-weight post-hoc addition that improves AP by up to 1.4% on off-the-shelf\npose estimation frameworks. Applied to the downstream task of mesh recovery,\nCCNet facilitates an additional 1.0mm decrease in 3D keypoint error.",
            "author": [
                "Kerui Gu",
                "Rongyu Chen",
                "Angela Yao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17105v1",
                "http://arxiv.org/pdf/2311.17105v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16630v1",
            "title": "Outfit Completion via Conditional Set Transformation",
            "updated": "2023-11-28T09:30:52Z",
            "published": "2023-11-28T09:30:52Z",
            "summary": "In this paper, we formulate the outfit completion problem as a set retrieval\ntask and propose a novel framework for solving this problem. The proposal\nincludes a conditional set transformation architecture with deep neural\nnetworks and a compatibility-based regularization method. The proposed method\nutilizes a map with permutation-invariant for the input set and\npermutation-equivariant for the condition set. This allows retrieving a set\nthat is compatible with the input set while reflecting the properties of the\ncondition set. In addition, since this structure outputs the element of the\noutput set in a single inference, it can achieve a scalable inference speed\nwith respect to the cardinality of the output set. Experimental results on real\ndata reveal that the proposed method outperforms existing approaches in terms\nof accuracy of the outfit completion task, condition satisfaction, and\ncompatibility of completion results.",
            "author": [
                "Takuma Nakamura",
                "Yuki Saito",
                "Ryosuke Goto"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16630v1",
                "http://arxiv.org/pdf/2311.16630v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16628v1",
            "title": "Symmetry-regularized neural ordinary differential equations",
            "updated": "2023-11-28T09:27:44Z",
            "published": "2023-11-28T09:27:44Z",
            "summary": "Neural Ordinary Differential Equations (Neural ODEs) is a class of deep\nneural network models that interpret the hidden state dynamics of neural\nnetworks as an ordinary differential equation, thereby capable of capturing\nsystem dynamics in a continuous time framework. In this work, I integrate\nsymmetry regularization into Neural ODEs. In particular, I use continuous Lie\nsymmetry of ODEs and PDEs associated with the model to derive conservation laws\nand add them to the loss function, making it physics-informed. This\nincorporation of inherent structural properties into the loss function could\nsignificantly improve robustness and stability of the model during training. To\nillustrate this method, I employ a toy model that utilizes a cosine rate of\nchange in the hidden state, showcasing the process of identifying Lie\nsymmetries, deriving conservation laws, and constructing a new loss function.",
            "author": [
                "Wenbo Hao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16628v1",
                "http://arxiv.org/pdf/2311.16628v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16627v1",
            "title": "Searching for gluon quartic gauge couplings at muon colliders using the\n  auto-encoder",
            "updated": "2023-11-28T09:26:55Z",
            "published": "2023-11-28T09:26:55Z",
            "summary": "One of the difficulties one has to face in the future phenomenological\nstudies of the new physics~(NP), is the need to deal with increasing amounts of\ndata. It is therefore increasingly important to improve the efficiency in the\nphenomenological study of the NP. Whether it is the use of the Standard Model\neffective field theory~(SMEFT), the use of machine learning~(ML) algorithms, or\nthe use of quantum computing, all are means of improving the efficiency. In\nthis paper, we use a ML algorithm, the auto-encoder~(AE), to study the\ndimension-8 operators in the SMEFT which contribute to the gluon quartic gauge\ncouplings~(gQGCs) at muon colliders. The AE is one of the ML algorithms that\nhas the potential to be accelerated by the quantum computing. It is found that\nthe AE-based anomaly detection algorithm can be used as event selection\nstrategy to study the gQGCs at the muon colliders, and is effective compared\nwith traditional event selection strategies.",
            "author": [
                "Yu-Ting Zhang",
                "Xin-Tong Wang",
                "Ji-Chong Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16627v1",
                "http://arxiv.org/pdf/2311.16627v1"
            ],
            "primary_category": "hep-ph",
            "category": [
                "hep-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16625v1",
            "title": "Gaussian Processes for Monitoring Air-Quality in Kampala",
            "updated": "2023-11-28T09:25:23Z",
            "published": "2023-11-28T09:25:23Z",
            "summary": "Monitoring air pollution is of vital importance to the overall health of the\npopulation. Unfortunately, devices that can measure air quality can be\nexpensive, and many cities in low and middle-income countries have to rely on a\nsparse allocation of them. In this paper, we investigate the use of Gaussian\nProcesses for both nowcasting the current air-pollution in places where there\nare no sensors and forecasting the air-pollution in the future at the sensor\nlocations. In particular, we focus on the city of Kampala in Uganda, using data\nfrom AirQo's network of sensors. We demonstrate the advantage of removing\noutliers, compare different kernel functions and additional inputs. We also\ncompare two sparse approximations to allow for the large amounts of temporal\ndata in the dataset.",
            "author": [
                "Clara Stoddart",
                "Lauren Shrack",
                "Richard Sserunjogi",
                "Usman Abdul-Ganiy",
                "Engineer Bainomugisha",
                "Deo Okure",
                "Ruth Misener",
                "Jose Pablo Folch",
                "Ruby Sedgwick"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16625v1",
                "http://arxiv.org/pdf/2311.16625v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16623v1",
            "title": "Visual Semantic Navigation with Real Robots",
            "updated": "2023-11-28T09:24:42Z",
            "published": "2023-11-28T09:24:42Z",
            "summary": "Visual Semantic Navigation (VSN) is the ability of a robot to learn visual\nsemantic information for navigating in unseen environments. These VSN models\nare typically tested in those virtual environments where they are trained,\nmainly using reinforcement learning based approaches. Therefore, we do not yet\nhave an in-depth analysis of how these models would behave in the real world.\nIn this work, we propose a new solution to integrate VSN models into real\nrobots, so that we have true embodied agents. We also release a novel ROS-based\nframework for VSN, ROS4VSN, so that any VSN-model can be easily deployed in any\nROS-compatible robot and tested in a real setting. Our experiments with two\ndifferent robots, where we have embedded two state-of-the-art VSN agents,\nconfirm that there is a noticeable performance difference of these VSN\nsolutions when tested in real-world and simulation environments. We hope that\nthis research will endeavor to provide a foundation for addressing this\nconsequential issue, with the ultimate aim of advancing the performance and\nefficiency of embodied agents within authentic real-world scenarios. Code to\nreproduce all our experiments can be found at\nhttps://github.com/gramuah/ros4vsn.",
            "author": [
                "Carlos Guti\u00e9rrez-\u00c1lvarez",
                "Pablo R\u00edos-Navarro",
                "Rafael Flor-Rodr\u00edguez",
                "Francisco Javier Acevedo-Rodr\u00edguez",
                "Roberto J. L\u00f3pez-Sastre"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16623v1",
                "http://arxiv.org/pdf/2311.16623v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16621v1",
            "title": "Beyond Labels: Advancing Cluster Analysis with the Entropy of Distance\n  Distribution (EDD)",
            "updated": "2023-11-28T09:22:17Z",
            "published": "2023-11-28T09:22:17Z",
            "summary": "In the evolving landscape of data science, the accurate quantification of\nclustering in high-dimensional data sets remains a significant challenge,\nespecially in the absence of predefined labels. This paper introduces a novel\napproach, the Entropy of Distance Distribution (EDD), which represents a\nparadigm shift in label-free clustering analysis. Traditional methods, reliant\non discrete labels, often struggle to discern intricate cluster patterns in\nunlabeled data. EDD, however, leverages the characteristic differences in\npairwise point-to-point distances to discern clustering tendencies, independent\nof data labeling.\n  Our method employs the Shannon information entropy to quantify the\n'peakedness' or 'flatness' of distance distributions in a data set. This\nentropy measure, normalized against its maximum value, effectively\ndistinguishes between strongly clustered data (indicated by pronounced peaks in\ndistance distribution) and more homogeneous, non-clustered data sets. This\nlabel-free quantification is resilient against global translations and\npermutations of data points, and with an additional dimension-wise z-scoring,\nit becomes invariant to data set scaling.\n  We demonstrate the efficacy of EDD through a series of experiments involving\ntwo-dimensional data spaces with Gaussian cluster centers. Our findings reveal\na monotonic increase in the EDD value with the widening of cluster widths,\nmoving from well-separated to overlapping clusters. This behavior underscores\nthe method's sensitivity and accuracy in detecting varying degrees of\nclustering. EDD's potential extends beyond conventional clustering analysis,\noffering a robust, scalable tool for unraveling complex data structures without\nreliance on pre-assigned labels.",
            "author": [
                "Claus Metzner",
                "Achim Schilling",
                "Patrick Krauss"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16621v1",
                "http://arxiv.org/pdf/2311.16621v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16620v1",
            "title": "On the Long Range Abilities of Transformers",
            "updated": "2023-11-28T09:21:48Z",
            "published": "2023-11-28T09:21:48Z",
            "summary": "Despite their dominance in modern DL and, especially, NLP domains,\ntransformer architectures exhibit sub-optimal performance on long-range tasks\ncompared to recent layers that are specifically designed for this purpose. In\nthis work, drawing inspiration from key attributes of long-range layers, such\nas state-space layers, linear RNN layers, and global convolution layers, we\ndemonstrate that minimal modifications to the transformer architecture can\nsignificantly enhance performance on the Long Range Arena (LRA) benchmark, thus\nnarrowing the gap with these specialized layers. We identify that two key\nprinciples for long-range tasks are (i) incorporating an inductive bias towards\nsmoothness, and (ii) locality. As we show, integrating these ideas into the\nattention mechanism improves results with a negligible amount of additional\ncomputation and without any additional trainable parameters. Our theory and\nexperiments also shed light on the reasons for the inferior performance of\ntransformers on long-range tasks and identify critical properties that are\nessential for successfully capturing long-range dependencies.",
            "author": [
                "Itamar Zimerman",
                "Lior Wolf"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16620v1",
                "http://arxiv.org/pdf/2311.16620v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL",
                "F.2.2; I.2.7"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17104v1",
            "title": "Single-Cell Clustering via Dual-Graph Alignment",
            "updated": "2023-11-28T09:14:55Z",
            "published": "2023-11-28T09:14:55Z",
            "summary": "In recent years, the field of single-cell RNA sequencing has seen a surge in\nthe development of clustering methods. These methods enable the identification\nof cell subpopulations, thereby facilitating the understanding of tumor\nmicroenvironments. Despite their utility, most existing clustering algorithms\nprimarily focus on the attribute information provided by the cell matrix or the\nnetwork structure between cells, often neglecting the network between genes.\nThis oversight could lead to loss of information and clustering results that\nlack clinical significance. To address this limitation, we develop an advanced\nsingle-cell clustering model incorporating dual-graph alignment, which\nintegrates gene network information into the clustering process based on\nself-supervised and unsupervised optimization. Specifically, we designed a\ngraph-based autoencoder enhanced by an attention mechanism to effectively\ncapture relationships between cells. Moreover, we performed the node2vec method\non Protein-Protein Interaction (PPI) networks to derive the gene network\nstructure and maintained this structure throughout the clustering process. Our\nproposed method has been demonstrated to be effective through experimental\nresults, showcasing its ability to optimize clustering outcomes while\npreserving the original associations between cells and genes. This research\ncontributes to obtaining accurate cell subpopulations and generates clustering\nresults that more closely resemble real-world biological scenarios. It provides\nbetter insights into the characteristics and distribution of diseased cells,\nultimately building a foundation for early disease diagnosis and treatment.",
            "author": [
                "Dayu Hu",
                "Ke Liang",
                "Xinwang Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17104v1",
                "http://arxiv.org/pdf/2311.17104v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "q-bio.MN"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16616v1",
            "title": "Adversarial Distribution Balancing for Counterfactual Reasoning",
            "updated": "2023-11-28T09:12:37Z",
            "published": "2023-11-28T09:12:37Z",
            "summary": "The development of causal prediction models is challenged by the fact that\nthe outcome is only observable for the applied (factual) intervention and not\nfor its alternatives (the so-called counterfactuals); in medicine we only know\npatients' survival for the administered drug and not for other therapeutic\noptions. Machine learning approaches for counterfactual reasoning have to deal\nwith both unobserved outcomes and distributional differences due to non-random\ntreatment administration. Unsupervised domain adaptation (UDA) addresses\nsimilar issues; one has to deal with unobserved outcomes -- the labels of the\ntarget domain -- and distributional differences between source and target\ndomain. We propose Adversarial Distribution Balancing for Counterfactual\nReasoning (ADBCR), which directly uses potential outcome estimates of the\ncounterfactuals to remove spurious causal relations. We show that ADBCR\noutcompetes state-of-the-art methods on three benchmark datasets, and\ndemonstrate that ADBCR's performance can be further improved if unlabeled\nvalidation data are included in the training procedure to better adapt the\nmodel to the validation domain.",
            "author": [
                "Stefan Schrod",
                "Fabian Sinz",
                "Michael Altenbuchinger"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16616v1",
                "http://arxiv.org/pdf/2311.16616v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16614v3",
            "title": "A Multivariate Unimodality Test Harnessing the Dip Statistic of\n  Mahalanobis Distances Over Random Projections",
            "updated": "2023-12-02T15:29:57Z",
            "published": "2023-11-28T09:11:02Z",
            "summary": "Unimodality, pivotal in statistical analysis, offers insights into dataset\nstructures and drives sophisticated analytical procedures. While unimodality's\nconfirmation is straightforward for one-dimensional data using methods like\nSilverman's approach and Hartigans' dip statistic, its generalization to higher\ndimensions remains challenging. By extrapolating one-dimensional unimodality\nprinciples to multi-dimensional spaces through linear random projections and\nleveraging point-to-point distancing, our method, rooted in\n$\\alpha$-unimodality assumptions, presents a novel multivariate unimodality\ntest named mud-pod. Both theoretical and empirical studies confirm the efficacy\nof our method in unimodality assessment of multidimensional datasets as well as\nin estimating the number of clusters.",
            "author": [
                "Prodromos Kolyvakis",
                "Aristidis Likas"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16614v3",
                "http://arxiv.org/pdf/2311.16614v3"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16609v1",
            "title": "Eigenmatrix for unstructured sparse recovery",
            "updated": "2023-11-28T08:54:29Z",
            "published": "2023-11-28T08:54:29Z",
            "summary": "This paper considers the unstructured sparse recovery problems in a general\nform. Examples include rational approximation, spectral function estimation,\nFourier inversion, Laplace inversion, and sparse deconvolution. The main\nchallenges are the noise in the sample values and the unstructured nature of\nthe sample locations. This paper proposes the eigenmatrix, a data-driven\nconstruction with desired approximate eigenvalues and eigenvectors. The\neigenmatrix offers a new way for these sparse recovery problems. Numerical\nresults are provided to demonstrate the efficiency of the proposed method.",
            "author": [
                "Lexing Ying"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16609v1",
                "http://arxiv.org/pdf/2311.16609v1"
            ],
            "primary_category": "math.NA",
            "category": [
                "math.NA",
                "cs.IT",
                "cs.LG",
                "cs.NA",
                "eess.SP",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16605v2",
            "title": "LasTGL: An Industrial Framework for Large-Scale Temporal Graph Learning",
            "updated": "2023-11-30T09:19:39Z",
            "published": "2023-11-28T08:45:37Z",
            "summary": "Over the past few years, graph neural networks (GNNs) have become powerful\nand practical tools for learning on (static) graph-structure data. However,\nmany real-world applications, such as social networks and e-commerce, involve\ntemporal graphs where nodes and edges are dynamically evolving. Temporal graph\nneural networks (TGNNs) have progressively emerged as an extension of GNNs to\naddress time-evolving graphs and have gradually become a trending research\ntopic in both academics and industry. Advancing research and application in\nsuch an emerging field necessitates the development of new tools to compose\nTGNN models and unify their different schemes for dealing with temporal graphs.\nIn this work, we introduce LasTGL, an industrial framework that integrates\nunified and extensible implementations of common temporal graph learning\nalgorithms for various advanced tasks. The purpose of LasTGL is to provide the\nessential building blocks for solving temporal graph learning tasks, focusing\non the guiding principles of user-friendliness and quick prototyping on which\nPyTorch is based. In particular, LasTGL provides comprehensive temporal graph\ndatasets, TGNN models and utilities along with well-documented tutorials,\nmaking it suitable for both absolute beginners and expert deep learning\npractitioners alike.",
            "author": [
                "Jintang Li",
                "Jiawang Dan",
                "Ruofan Wu",
                "Jing Zhou",
                "Sheng Tian",
                "Yunfei Liu",
                "Baokun Wang",
                "Changhua Meng",
                "Weiqiang Wang",
                "Yuchang Zhu",
                "Liang Chen",
                "Zibin Zheng"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16605v2",
                "http://arxiv.org/pdf/2311.16605v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16604v1",
            "title": "LC4SV: A Denoising Framework Learning to Compensate for Unseen Speaker\n  Verification Models",
            "updated": "2023-11-28T08:44:04Z",
            "published": "2023-11-28T08:44:04Z",
            "summary": "The performance of speaker verification (SV) models may drop dramatically in\nnoisy environments. A speech enhancement (SE) module can be used as a front-end\nstrategy. However, existing SE methods may fail to bring performance\nimprovements to downstream SV systems due to artifacts in the predicted signals\nof SE models. To compensate for artifacts, we propose a generic denoising\nframework named LC4SV, which can serve as a pre-processor for various unknown\ndownstream SV models. In LC4SV, we employ a learning-based interpolation agent\nto automatically generate the appropriate coefficients between the enhanced\nsignal and its noisy input to improve SV performance in noisy environments. Our\nexperimental results demonstrate that LC4SV consistently improves the\nperformance of various unseen SV systems. To the best of our knowledge, this\nwork is the first attempt to develop a learning-based interpolation scheme\naiming at improving SV performance in noisy environments.",
            "author": [
                "Chi-Chang Lee",
                "Hong-Wei Chen",
                "Chu-Song Chen",
                "Hsin-Min Wang",
                "Tsung-Te Liu",
                "Yu Tsao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16604v1",
                "http://arxiv.org/pdf/2311.16604v1"
            ],
            "primary_category": "eess.AS",
            "category": [
                "eess.AS",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16602v1",
            "title": "GSP-KalmanNet: Tracking Graph Signals via Neural-Aided Kalman Filtering",
            "updated": "2023-11-28T08:43:10Z",
            "published": "2023-11-28T08:43:10Z",
            "summary": "Dynamic systems of graph signals are encountered in various applications,\nincluding social networks, power grids, and transportation. While such systems\ncan often be described as state space (SS) models, tracking graph signals via\nconventional tools based on the Kalman filter (KF) and its variants is\ntypically challenging. This is due to the nonlinearity, high dimensionality,\nirregularity of the domain, and complex modeling associated with real-world\ndynamic systems of graph signals. In this work, we study the tracking of graph\nsignals using a hybrid model-based/data-driven approach. We develop the\nGSP-KalmanNet, which tracks the hidden graphical states from the graphical\nmeasurements by jointly leveraging graph signal processing (GSP) tools and deep\nlearning (DL) techniques. The derivations of the GSP-KalmanNet are based on\nextending the KF to exploit the inherent graph structure via graph frequency\ndomain filtering, which considerably simplifies the computational complexity\nentailed in processing high-dimensional signals and increases the robustness to\nsmall topology changes. Then, we use data to learn the Kalman gain following\nthe recently proposed KalmanNet framework, which copes with partial and\napproximated modeling, without forcing a specific model over the noise\nstatistics. Our empirical results demonstrate that the proposed GSP-KalmanNet\nachieves enhanced accuracy and run time performance as well as improved\nrobustness to model misspecifications compared with both model-based and\ndata-driven benchmarks.",
            "author": [
                "Itay Buchnik",
                "Guy Sagi",
                "Nimrod Leinwand",
                "Yuval Loya",
                "Nir Shlezinger",
                "Tirza Routtenberg"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16602v1",
                "http://arxiv.org/pdf/2311.16602v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17103v1",
            "title": "Single-cell Multi-view Clustering via Community Detection with Unknown\n  Number of Clusters",
            "updated": "2023-11-28T08:34:58Z",
            "published": "2023-11-28T08:34:58Z",
            "summary": "Single-cell multi-view clustering enables the exploration of cellular\nheterogeneity within the same cell from different views. Despite the\ndevelopment of several multi-view clustering methods, two primary challenges\npersist. Firstly, most existing methods treat the information from both\nsingle-cell RNA (scRNA) and single-cell Assay of Transposase Accessible\nChromatin (scATAC) views as equally significant, overlooking the substantial\ndisparity in data richness between the two views. This oversight frequently\nleads to a degradation in overall performance. Additionally, the majority of\nclustering methods necessitate manual specification of the number of clusters\nby users. However, for biologists dealing with cell data, precisely determining\nthe number of distinct cell types poses a formidable challenge. To this end, we\nintroduce scUNC, an innovative multi-view clustering approach tailored for\nsingle-cell data, which seamlessly integrates information from different views\nwithout the need for a predefined number of clusters. The scUNC method\ncomprises several steps: initially, it employs a cross-view fusion network to\ncreate an effective embedding, which is then utilized to generate initial\nclusters via community detection. Subsequently, the clusters are automatically\nmerged and optimized until no further clusters can be merged. We conducted a\ncomprehensive evaluation of scUNC using three distinct single-cell datasets.\nThe results underscored that scUNC outperforms the other baseline methods.",
            "author": [
                "Dayu Hu",
                "Zhibin Dong",
                "Ke Liang",
                "Jun Wang",
                "Siwei Wang",
                "Xinwang Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17103v1",
                "http://arxiv.org/pdf/2311.17103v1"
            ],
            "primary_category": "q-bio.GN",
            "category": [
                "q-bio.GN",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16595v1",
            "title": "D4AM: A General Denoising Framework for Downstream Acoustic Models",
            "updated": "2023-11-28T08:27:27Z",
            "published": "2023-11-28T08:27:27Z",
            "summary": "The performance of acoustic models degrades notably in noisy environments.\nSpeech enhancement (SE) can be used as a front-end strategy to aid automatic\nspeech recognition (ASR) systems. However, existing training objectives of SE\nmethods are not fully effective at integrating speech-text and noisy-clean\npaired data for training toward unseen ASR systems. In this study, we propose a\ngeneral denoising framework, D4AM, for various downstream acoustic models. Our\nframework fine-tunes the SE model with the backward gradient according to a\nspecific acoustic model and the corresponding classification objective. In\naddition, our method aims to consider the regression objective as an auxiliary\nloss to make the SE model generalize to other unseen acoustic models. To\njointly train an SE unit with regression and classification objectives, D4AM\nuses an adjustment scheme to directly estimate suitable weighting coefficients\nrather than undergoing a grid search process with additional training costs.\nThe adjustment scheme consists of two parts: gradient calibration and\nregression objective weighting. The experimental results show that D4AM can\nconsistently and effectively provide improvements to various unseen acoustic\nmodels and outperforms other combination setups. Specifically, when evaluated\non the Google ASR API with real noisy data completely unseen during SE\ntraining, D4AM achieves a relative WER reduction of 24.65% compared with the\ndirect feeding of noisy input. To our knowledge, this is the first work that\ndeploys an effective combination scheme of regression (denoising) and\nclassification (ASR) objectives to derive a general pre-processor applicable to\nvarious unseen ASR systems. Our code is available at\nhttps://github.com/ChangLee0903/D4AM.",
            "author": [
                "Chi-Chang Lee",
                "Yu Tsao",
                "Hsin-Min Wang",
                "Chu-Song Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16595v1",
                "http://arxiv.org/pdf/2311.16595v1"
            ],
            "primary_category": "cs.SD",
            "category": [
                "cs.SD",
                "cs.LG",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16593v1",
            "title": "Empowering COVID-19 Detection: Optimizing Performance Through Fine-Tuned\n  EfficientNet Deep Learning Architecture",
            "updated": "2023-11-28T08:18:30Z",
            "published": "2023-11-28T08:18:30Z",
            "summary": "The worldwide COVID-19 pandemic has profoundly influenced the health and\neveryday experiences of individuals across the planet. It is a highly\ncontagious respiratory disease requiring early and accurate detection to curb\nits rapid transmission. Initial testing methods primarily revolved around\nidentifying the genetic composition of the coronavirus, exhibiting a relatively\nlow detection rate and requiring a time-intensive procedure. To address this\nchallenge, experts have suggested using radiological imagery, particularly\nchest X-rays, as a valuable approach within the diagnostic protocol. This study\ninvestigates the potential of leveraging radiographic imaging (X-rays) with\ndeep learning algorithms to swiftly and precisely identify COVID-19 patients.\nThe proposed approach elevates the detection accuracy by fine-tuning with\nappropriate layers on various established transfer learning models. The\nexperimentation was conducted on a COVID-19 X-ray dataset containing 2000\nimages. The accuracy rates achieved were impressive of 100% for EfficientNetB4\nmodel. The fine-tuned EfficientNetB4 achieved an excellent accuracy score,\nshowcasing its potential as a robust COVID-19 detection model. Furthermore,\nEfficientNetB4 excelled in identifying Lung disease using Chest X-ray dataset\ncontaining 4,350 Images, achieving remarkable performance with an accuracy of\n99.17%, precision of 99.13%, recall of 99.16%, and f1-score of 99.14%. These\nresults highlight the promise of fine-tuned transfer learning for efficient\nlung detection through medical imaging, especially with X-ray images. This\nresearch offers radiologists an effective means of aiding rapid and precise\nCOVID-19 diagnosis and contributes valuable assistance for healthcare\nprofessionals in accurately identifying affected patients.",
            "author": [
                "Md. Alamin Talukder",
                "Md. Abu Layek",
                "Mohsin Kazi",
                "Md Ashraf Uddin",
                "Sunil Aryal"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16593v1",
                "http://arxiv.org/pdf/2311.16593v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16589v1",
            "title": "Improving Lane Detection Generalization: A Novel Framework using HD Maps\n  for Boosting Diversity",
            "updated": "2023-11-28T08:15:27Z",
            "published": "2023-11-28T08:15:27Z",
            "summary": "Lane detection is a vital task for vehicles to navigate and localize their\nposition on the road. To ensure reliable results, lane detection algorithms\nmust have robust generalization performance in various road environments.\nHowever, despite the significant performance improvement of deep learning-based\nlane detection algorithms, their generalization performance in response to\nchanges in road environments still falls short of expectations. In this paper,\nwe present a novel framework for single-source domain generalization (SSDG) in\nlane detection. By decomposing data into lane structures and surroundings, we\nenhance diversity using High-Definition (HD) maps and generative models. Rather\nthan expanding data volume, we strategically select a core subset of data,\nmaximizing diversity and optimizing performance. Our extensive experiments\ndemonstrate that our framework enhances the generalization performance of lane\ndetection, comparable to the domain adaptation-based method.",
            "author": [
                "Daeun Lee",
                "Minhyeok Heo",
                "Jiwon Kim"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16589v1",
                "http://arxiv.org/pdf/2311.16589v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16586v1",
            "title": "SARDINE: A Simulator for Automated Recommendation in Dynamic and\n  Interactive Environments",
            "updated": "2023-11-28T08:03:56Z",
            "published": "2023-11-28T08:03:56Z",
            "summary": "Simulators can provide valuable insights for researchers and practitioners\nwho wish to improve recommender systems, because they allow one to easily tweak\nthe experimental setup in which recommender systems operate, and as a result\nlower the cost of identifying general trends and uncovering novel findings\nabout the candidate methods. A key requirement to enable this accelerated\nimprovement cycle is that the simulator is able to span the various sources of\ncomplexity that can be found in the real recommendation environment that it\nsimulates.\n  With the emergence of interactive and data-driven methods - e.g.,\nreinforcement learning or online and counterfactual learning-to-rank - that aim\nto achieve user-related goals beyond the traditional accuracy-centric\nobjectives, adequate simulators are needed. In particular, such simulators must\nmodel the various mechanisms that render the recommendation environment dynamic\nand interactive, e.g., the effect of recommendations on the user or the effect\nof biased data on subsequent iterations of the recommender system. We therefore\npropose SARDINE, a flexible and interpretable recommendation simulator that can\nhelp accelerate research in interactive and data-driven recommender systems. We\ndemonstrate its usefulness by studying existing methods within nine diverse\nenvironments derived from SARDINE, and even uncover novel insights about them.",
            "author": [
                "Romain Deffayet",
                "Thibaut Thonet",
                "Dongyoon Hwang",
                "Vassilissa Lehoux",
                "Jean-Michel Renders",
                "Maarten de Rijke"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16586v1",
                "http://arxiv.org/pdf/2311.16586v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16584v1",
            "title": "FedAL: Black-Box Federated Knowledge Distillation Enabled by Adversarial\n  Learning",
            "updated": "2023-11-28T08:01:43Z",
            "published": "2023-11-28T08:01:43Z",
            "summary": "Knowledge distillation (KD) can enable collaborative learning among\ndistributed clients that have different model architectures and do not share\ntheir local data and model parameters with others. Each client updates its\nlocal model using the average model output/feature of all client models as the\ntarget, known as federated KD. However, existing federated KD methods often do\nnot perform well when clients' local models are trained with heterogeneous\nlocal datasets. In this paper, we propose Federated knowledge distillation\nenabled by Adversarial Learning (FedAL) to address the data heterogeneity among\nclients. First, to alleviate the local model output divergence across clients\ncaused by data heterogeneity, the server acts as a discriminator to guide\nclients' local model training to achieve consensus model outputs among clients\nthrough a min-max game between clients and the discriminator. Moreover,\ncatastrophic forgetting may happen during the clients' local training and\nglobal knowledge transfer due to clients' heterogeneous local data. Towards\nthis challenge, we design the less-forgetting regularization for both local\ntraining and global knowledge transfer to guarantee clients' ability to\ntransfer/learn knowledge to/from others. Experimental results show that FedAL\nand its variants achieve higher accuracy than other federated KD baselines.",
            "author": [
                "Pengchao Han",
                "Xingyan Shi",
                "Jianwei Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16584v1",
                "http://arxiv.org/pdf/2311.16584v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16580v1",
            "title": "Clean Label Disentangling for Medical Image Segmentation with Noisy\n  Labels",
            "updated": "2023-11-28T07:54:27Z",
            "published": "2023-11-28T07:54:27Z",
            "summary": "Current methods focusing on medical image segmentation suffer from incorrect\nannotations, which is known as the noisy label issue. Most medical image\nsegmentation with noisy labels methods utilize either noise transition matrix,\nnoise-robust loss functions or pseudo-labeling methods, while none of the\ncurrent research focuses on clean label disentanglement. We argue that the main\nreason is that the severe class-imbalanced issue will lead to the inaccuracy of\nthe selected ``clean'' labels, thus influencing the robustness of the model\nagainst the noises. In this work, we come up with a simple but efficient\nclass-balanced sampling strategy to tackle the class-imbalanced problem, which\nenables our newly proposed clean label disentangling framework to successfully\nselect clean labels from the given label sets and encourages the model to learn\nfrom the correct annotations. However, such a method will filter out too many\nannotations which may also contain useful information. Therefore, we further\nextend our clean label disentangling framework to a new noisy feature-aided\nclean label disentangling framework, which takes the full annotations into\nutilization to learn more semantics. Extensive experiments have validated the\neffectiveness of our methods, where our methods achieve new state-of-the-art\nperformance. Our code is available at https://github.com/xiaoyao3302/2BDenoise.",
            "author": [
                "Zicheng Wang",
                "Zhen Zhao",
                "Erjian Guo",
                "Luping Zhou"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16580v1",
                "http://arxiv.org/pdf/2311.16580v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16576v1",
            "title": "Wireless Powered Metaverse: Joint Task Scheduling and Trajectory Design\n  for Multi-Devices and Multi-UAVs",
            "updated": "2023-11-28T07:32:32Z",
            "published": "2023-11-28T07:32:32Z",
            "summary": "To support the running of human-centric metaverse applications on mobile\ndevices, Unmanned Aerial Vehicle (UAV)-assisted Wireless Powered Mobile Edge\nComputing (WPMEC) is promising to compensate for limited computational\ncapabilities and energy supplies of mobile devices. The high-speed\ncomputational processing demands and significant energy consumption of\nmetaverse applications require joint resource scheduling of multiple devices\nand UAVs, but existing WPMEC solutions address either device or UAV scheduling\ndue to the complexity of combinatorial optimization. To solve the above\nchallenge, we propose a two-stage alternating optimization algorithm based on\nmulti-task Deep Reinforcement Learning (DRL) to jointly allocate charging time,\nschedule computation tasks, and optimize trajectory of UAVs and mobile devices\nin a wireless powered metaverse scenario. First, considering energy constraints\nof both UAVs and mobile devices, we formulate an optimization problem to\nmaximize the computation efficiency of the system. Second, we propose a\nheuristic algorithm to efficiently perform time allocation and charging\nscheduling for mobile devices. Following this, we design a multi-task DRL\nscheme to make charging scheduling and trajectory design decisions for UAVs.\nFinally, theoretical analysis and performance results demonstrate that our\nalgorithm exhibits significant advantages over representative methods in terms\nof convergence speed and average computation efficiency.",
            "author": [
                "Xiaojie Wang",
                "Jiameng Li",
                "Zhaolong Ning",
                "Qingyang Song",
                "Lei Guo",
                "Abbas Jamalipour"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16576v1",
                "http://arxiv.org/pdf/2311.16576v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17097v1",
            "title": "Anonymous Jamming Detection in 5G with Bayesian Network Model Based\n  Inference Analysis",
            "updated": "2023-11-28T07:23:15Z",
            "published": "2023-11-28T07:23:15Z",
            "summary": "Jamming and intrusion detection are critical in 5G research, aiming to\nmaintain reliability, prevent user experience degradation, and avoid\ninfrastructure failure. This paper introduces an anonymous jamming detection\nmodel for 5G based on signal parameters from the protocol stacks. The system\nuses supervised and unsupervised learning for real-time, high-accuracy\ndetection of jamming, including unknown types. Supervised models reach an AUC\nof 0.964 to 1, compared to LSTM models with an AUC of 0.923 to 1. However, the\nneed for data annotation limits the supervised approach. To address this, an\nunsupervised auto-encoder-based anomaly detection is presented with an AUC of\n0.987. The approach is resistant to adversarial training samples. For\ntransparency and domain knowledge injection, a Bayesian network-based causation\nanalysis is introduced.",
            "author": [
                "Ying Wang",
                "Shashank Jere",
                "Soumya Banerjee",
                "Lingjia Liu",
                "Sachin Shetty",
                "Shehadi Dayekh"
            ],
            "link": [
                "http://dx.doi.org/10.1109/HPSR54439.2022.9831286",
                "http://arxiv.org/abs/2311.17097v1",
                "http://arxiv.org/pdf/2311.17097v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CR",
                "cs.NI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16565v2",
            "title": "DiffusionTalker: Personalization and Acceleration for Speech-Driven 3D\n  Face Diffuser",
            "updated": "2023-12-02T16:48:09Z",
            "published": "2023-11-28T07:13:20Z",
            "summary": "Speech-driven 3D facial animation has been an attractive task in both\nacademia and industry. Traditional methods mostly focus on learning a\ndeterministic mapping from speech to animation. Recent approaches start to\nconsider the non-deterministic fact of speech-driven 3D face animation and\nemploy the diffusion model for the task. However, personalizing facial\nanimation and accelerating animation generation are still two major limitations\nof existing diffusion-based methods. To address the above limitations, we\npropose DiffusionTalker, a diffusion-based method that utilizes contrastive\nlearning to personalize 3D facial animation and knowledge distillation to\naccelerate 3D animation generation. Specifically, to enable personalization, we\nintroduce a learnable talking identity to aggregate knowledge in audio\nsequences. The proposed identity embeddings extract customized facial cues\nacross different people in a contrastive learning manner. During inference,\nusers can obtain personalized facial animation based on input audio, reflecting\na specific talking style. With a trained diffusion model with hundreds of\nsteps, we distill it into a lightweight model with 8 steps for acceleration.\nExtensive experiments are conducted to demonstrate that our method outperforms\nstate-of-the-art methods. The code will be released.",
            "author": [
                "Peng Chen",
                "Xiaobao Wei",
                "Ming Lu",
                "Yitong Zhu",
                "Naiming Yao",
                "Xingyu Xiao",
                "Hui Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16565v2",
                "http://arxiv.org/pdf/2311.16565v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.SD",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16556v1",
            "title": "Scalable Label Distribution Learning for Multi-Label Classification",
            "updated": "2023-11-28T06:52:53Z",
            "published": "2023-11-28T06:52:53Z",
            "summary": "Multi-label classification (MLC) refers to the problem of tagging a given\ninstance with a set of relevant labels. Most existing MLC methods are based on\nthe assumption that the correlation of two labels in each label pair is\nsymmetric, which is violated in many real-world scenarios. Moreover, most\nexisting methods design learning processes associated with the number of\nlabels, which makes their computational complexity a bottleneck when scaling up\nto large-scale output space. To tackle these issues, we propose a novel MLC\nlearning method named Scalable Label Distribution Learning (SLDL) for\nmulti-label classification which can describe different labels as distributions\nin a latent space, where the label correlation is asymmetric and the dimension\nis independent of the number of labels. Specifically, SLDL first converts\nlabels into continuous distributions within a low-dimensional latent space and\nleverages the asymmetric metric to establish the correlation between different\nlabels. Then, it learns the mapping from the feature space to the latent space,\nresulting in the computational complexity is no longer related to the number of\nlabels. Finally, SLDL leverages a nearest-neighbor-based strategy to decode the\nlatent representations and obtain the final predictions. Our extensive\nexperiments illustrate that SLDL can achieve very competitive classification\nperformances with little computational consumption.",
            "author": [
                "Xingyu Zhao",
                "Yuexuan An",
                "Lei Qi",
                "Xin Geng"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16556v1",
                "http://arxiv.org/pdf/2311.16556v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00051v1",
            "title": "MIA-BAD: An Approach for Enhancing Membership Inference Attack and its\n  Mitigation with Federated Learning",
            "updated": "2023-11-28T06:51:26Z",
            "published": "2023-11-28T06:51:26Z",
            "summary": "The membership inference attack (MIA) is a popular paradigm for compromising\nthe privacy of a machine learning (ML) model. MIA exploits the natural\ninclination of ML models to overfit upon the training data. MIAs are trained to\ndistinguish between training and testing prediction confidence to infer\nmembership information. Federated Learning (FL) is a privacy-preserving ML\nparadigm that enables multiple clients to train a unified model without\ndisclosing their private data. In this paper, we propose an enhanced Membership\nInference Attack with the Batch-wise generated Attack Dataset (MIA-BAD), a\nmodification to the MIA approach. We investigate that the MIA is more accurate\nwhen the attack dataset is generated batch-wise. This quantitatively decreases\nthe attack dataset while qualitatively improving it. We show how training an ML\nmodel through FL, has some distinct advantages and investigate how the threat\nintroduced with the proposed MIA-BAD approach can be mitigated with FL\napproaches. Finally, we demonstrate the qualitative effects of the proposed\nMIA-BAD methodology by conducting extensive experiments with various target\ndatasets, variable numbers of federated clients, and training batch sizes.",
            "author": [
                "Soumya Banerjee",
                "Sandip Roy",
                "Sayyed Farid Ahamed",
                "Devin Quinn",
                "Marc Vucovich",
                "Dhruv Nandakumar",
                "Kevin Choi",
                "Abdul Rahman",
                "Edward Bowen",
                "Sachin Shetty"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00051v1",
                "http://arxiv.org/pdf/2312.00051v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17096v1",
            "title": "Robust Transductive Few-shot Learning via Joint Message Passing and\n  Prototype-based Soft-label Propagation",
            "updated": "2023-11-28T06:44:27Z",
            "published": "2023-11-28T06:44:27Z",
            "summary": "Few-shot learning (FSL) aims to develop a learning model with the ability to\ngeneralize to new classes using a few support samples. For transductive FSL\ntasks, prototype learning and label propagation methods are commonly employed.\nPrototype methods generally first learn the representative prototypes from the\nsupport set and then determine the labels of queries based on the metric\nbetween query samples and prototypes. Label propagation methods try to\npropagate the labels of support samples on the constructed graph encoding the\nrelationships between both support and query samples. This paper aims to\nintegrate these two principles together and develop an efficient and robust\ntransductive FSL approach, termed Prototype-based Soft-label Propagation\n(PSLP). Specifically, we first estimate the soft-label presentation for each\nquery sample by leveraging prototypes. Then, we conduct soft-label propagation\non our learned query-support graph. Both steps are conducted progressively to\nboost their respective performance. Moreover, to learn effective prototypes for\nsoft-label estimation as well as the desirable query-support graph for\nsoft-label propagation, we design a new joint message passing scheme to learn\nsample presentation and relational graph jointly. Our PSLP method is\nparameter-free and can be implemented very efficiently. On four popular\ndatasets, our method achieves competitive results on both balanced and\nimbalanced settings compared to the state-of-the-art methods. The code will be\nreleased upon acceptance.",
            "author": [
                "Jiahui Wang",
                "Qin Xu",
                "Bo Jiang",
                "Bin Luo"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17096v1",
                "http://arxiv.org/pdf/2311.17096v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17095v1",
            "title": "Plug-and-Play, Dense-Label-Free Extraction of Open-Vocabulary Semantic\n  Segmentation from Vision-Language Models",
            "updated": "2023-11-28T06:42:58Z",
            "published": "2023-11-28T06:42:58Z",
            "summary": "From an enormous amount of image-text pairs, large-scale vision-language\nmodels (VLMs) learn to implicitly associate image regions with words, which is\nvital for tasks such as image captioning and visual question answering.\nHowever, leveraging such pre-trained models for open-vocabulary semantic\nsegmentation remains a challenge. In this paper, we propose a simple, yet\nextremely effective, training-free technique, Plug-and-Play Open-Vocabulary\nSemantic Segmentation (PnP-OVSS) for this task. PnP-OVSS leverages a VLM with\ndirect text-to-image cross-attention and an image-text matching loss to produce\nsemantic segmentation. However, cross-attention alone tends to over-segment,\nwhereas cross-attention plus GradCAM tend to under-segment. To alleviate this\nissue, we introduce Salience Dropout; by iteratively dropping patches that the\nmodel is most attentive to, we are able to better resolve the entire extent of\nthe segmentation mask. Compared to existing techniques, the proposed method\ndoes not require any neural network training and performs hyperparameter tuning\nwithout the need for any segmentation annotations, even for a validation set.\nPnP-OVSS demonstrates substantial improvements over a comparable baseline\n(+29.4% mIoU on Pascal VOC, +13.2% mIoU on Pascal Context, +14.0% mIoU on MS\nCOCO, +2.4% mIoU on COCO Stuff) and even outperforms most baselines that\nconduct additional network training on top of pretrained VLMs.",
            "author": [
                "Luo Jiayun",
                "Siddhesh Khandelwal",
                "Leonid Sigal",
                "Boyang Li"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17095v1",
                "http://arxiv.org/pdf/2311.17095v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16507v1",
            "title": "Exploring Straighter Trajectories of Flow Matching with Diffusion\n  Guidance",
            "updated": "2023-11-28T06:19:30Z",
            "published": "2023-11-28T06:19:30Z",
            "summary": "Flow matching as a paradigm of generative model achieves notable success\nacross various domains. However, existing methods use either multi-round\ntraining or knowledge within minibatches, posing challenges in finding a\nfavorable coupling strategy for straight trajectories. To address this issue,\nwe propose a novel approach, Straighter trajectories of Flow Matching\n(StraightFM). It straightens trajectories with the coupling strategy guided by\ndiffusion model from entire distribution level. First, we propose a coupling\nstrategy to straighten trajectories, creating couplings between image and noise\nsamples under diffusion model guidance. Second, StraightFM also integrates real\ndata to enhance training, employing a neural network to parameterize another\ncoupling process from images to noise samples. StraightFM is jointly optimized\nwith couplings from above two mutually complementary directions, resulting in\nstraighter trajectories and enabling both one-step and few-step generation.\nExtensive experiments demonstrate that StraightFM yields high quality samples\nwith fewer step. StraightFM generates visually appealing images with a lower\nFID among diffusion and traditional flow matching methods within 5 sampling\nsteps when trained on pixel space. In the latent space (i.e., Latent\nDiffusion), StraightFM achieves a lower KID value compared to existing methods\non the CelebA-HQ 256 dataset in fewer than 10 sampling steps.",
            "author": [
                "Siyu Xing",
                "Jie Cao",
                "Huaibo Huang",
                "Xiao-Yu Zhang",
                "Ran He"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16507v1",
                "http://arxiv.org/pdf/2311.16507v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17094v1",
            "title": "In Search of a Data Transformation That Accelerates Neural Field\n  Training",
            "updated": "2023-11-28T06:17:49Z",
            "published": "2023-11-28T06:17:49Z",
            "summary": "Neural field is an emerging paradigm in data representation that trains a\nneural network to approximate the given signal. A key obstacle that prevents\nits widespread adoption is the encoding speed-generating neural fields requires\nan overfitting of a neural network, which can take a significant number of SGD\nsteps to reach the desired fidelity level. In this paper, we delve into the\nimpacts of data transformations on the speed of neural field training,\nspecifically focusing on how permuting pixel locations affect the convergence\nspeed of SGD. Counterintuitively, we find that randomly permuting the pixel\nlocations can considerably accelerate the training. To explain this phenomenon,\nwe examine the neural field training through the lens of PSNR curves, loss\nlandscapes, and error patterns. Our analyses suggest that the random pixel\npermutations remove the easy-to-fit patterns, which facilitate easy\noptimization in the early stage but hinder capturing fine details of the\nsignal.",
            "author": [
                "Junwon Seo",
                "Sangyoon Lee",
                "Kwang In Kim",
                "Jaeho Lee"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17094v1",
                "http://arxiv.org/pdf/2311.17094v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16540v1",
            "title": "Communication Efficiency Optimization of Federated Learning for\n  Computing and Network Convergence of 6G Networks",
            "updated": "2023-11-28T06:12:57Z",
            "published": "2023-11-28T06:12:57Z",
            "summary": "Federated learning effectively addresses issues such as data privacy by\ncollaborating across participating devices to train global models. However,\nfactors such as network topology and device computing power can affect its\ntraining or communication process in complex network environments. A new\nnetwork architecture and paradigm with computing-measurable, perceptible,\ndistributable, dispatchable, and manageable capabilities, computing and network\nconvergence (CNC) of 6G networks can effectively support federated learning\ntraining and improve its communication efficiency. By guiding the participating\ndevices' training in federated learning based on business requirements,\nresource load, network conditions, and arithmetic power of devices, CNC can\nreach this goal. In this paper, to improve the communication efficiency of\nfederated learning in complex networks, we study the communication efficiency\noptimization of federated learning for computing and network convergence of 6G\nnetworks, methods that gives decisions on its training process for different\nnetwork conditions and arithmetic power of participating devices in federated\nlearning. The experiments address two architectures that exist for devices in\nfederated learning and arrange devices to participate in training based on\narithmetic power while achieving optimization of communication efficiency in\nthe process of transferring model parameters. The results show that the method\nwe proposed can (1) cope well with complex network situations (2) effectively\nbalance the delay distribution of participating devices for local training (3)\nimprove the communication efficiency during the transfer of model parameters\n(4) improve the resource utilization in the network.",
            "author": [
                "Yizhuo Cai",
                "Bo Lei",
                "Qianying Zhao",
                "Jing Peng",
                "Min Wei",
                "Yushun Zhang",
                "Xing Zhang"
            ],
            "link": [
                "http://dx.doi.org/10.1631/FITEE.2300122",
                "http://arxiv.org/abs/2311.16540v1",
                "http://arxiv.org/pdf/2311.16540v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.DC",
                "cs.NI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17093v1",
            "title": "Improved Prototypical Semi-Supervised Learning with Foundation Models:\n  Prototype Selection, Parametric vMF-SNE Pretraining and Multi-view\n  Pseudolabelling",
            "updated": "2023-11-28T06:12:28Z",
            "published": "2023-11-28T06:12:28Z",
            "summary": "In this paper we present an improved approach to prototypical semi-supervised\nlearning for computer vision, in the context of leveraging a frozen foundation\nmodel as the backbone of our neural network. As a general tool, we propose\nparametric von-Mises Fisher Stochastic Neighbour Embedding (vMF-SNE) to create\nmappings with neural networks between high-dimensional latent spaces that\npreserve local structure. This enables us to pretrain the projection head of\nour network using the high-quality embeddings of the foundation model with\nvMF-SNE. We also propose soft multi-view pseudolabels, where predictions across\nmultiple views are combined to provide a more reliable supervision signal\ncompared to a consistency or swapped assignment approach. We demonstrate that\nthese ideas improve upon P}redicting View-Assignments with Support Samples\n(PAWS), a current state-of-the-art semi-supervised learning method, as well as\nRobust PAWS (RoPAWS), over a range of benchmarking datasets. We also introduce\nsimple $k$-means prototype selection, a technique that provides superior\nperformance to other unsupervised label selection approaches in this context.\nThese changes improve upon PAWS by an average of +2.9% for CIFAR-10 and +5.7%\nfor CIFAR-100 with four labels per class, and by +15.2% for DeepWeeds, a\nparticularly challenging dataset for semi-supervised learning. We also achieve\nnew state-of-the-art results in semi-supervised learning in this small label\nregime for CIFAR-10 - 95.8% (+0.7%) and CIFAR-100 - 76.6% (+12.0%).",
            "author": [
                "Evelyn Mannix",
                "Howard Bondell"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17093v1",
                "http://arxiv.org/pdf/2311.17093v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16538v1",
            "title": "Federated Learning with Diffusion Models for Privacy-Sensitive Vision\n  Tasks",
            "updated": "2023-11-28T06:08:16Z",
            "published": "2023-11-28T06:08:16Z",
            "summary": "Diffusion models have shown great potential for vision-related tasks,\nparticularly for image generation. However, their training is typically\nconducted in a centralized manner, relying on data collected from publicly\navailable sources. This approach may not be feasible or practical in many\ndomains, such as the medical field, which involves privacy concerns over data\ncollection. Despite the challenges associated with privacy-sensitive data, such\ndomains could still benefit from valuable vision services provided by diffusion\nmodels. Federated learning (FL) plays a crucial role in enabling decentralized\nmodel training without compromising data privacy. Instead of collecting data,\nan FL system gathers model parameters, effectively safeguarding the private\ndata of different parties involved. This makes FL systems vital for managing\ndecentralized learning tasks, especially in scenarios where privacy-sensitive\ndata is distributed across a network of clients. Nonetheless, FL presents its\nown set of challenges due to its distributed nature and privacy-preserving\nproperties. Therefore, in this study, we explore the FL strategy to train\ndiffusion models, paving the way for the development of federated diffusion\nmodels. We conduct experiments on various FL scenarios, and our findings\ndemonstrate that federated diffusion models have great potential to deliver\nvision services to privacy-sensitive domains.",
            "author": [
                "Ye Lin Tun",
                "Chu Myaet Thwal",
                "Ji Su Yoon",
                "Sun Moo Kang",
                "Chaoning Zhang",
                "Choong Seon Hong"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16538v1",
                "http://arxiv.org/pdf/2311.16538v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16536v1",
            "title": "Personalized Predictions of Glioblastoma Infiltration: Mathematical\n  Models, Physics-Informed Neural Networks and Multimodal Scans",
            "updated": "2023-11-28T05:45:20Z",
            "published": "2023-11-28T05:45:20Z",
            "summary": "Predicting the infiltration of Glioblastoma (GBM) from medical MRI scans is\ncrucial for understanding tumor growth dynamics and designing personalized\nradiotherapy treatment plans.Mathematical models of GBM growth can complement\nthe data in the prediction of spatial distributions of tumor cells. However,\nthis requires estimating patient-specific parameters of the model from clinical\ndata, which is a challenging inverse problem due to limited temporal data and\nthe limited time between imaging and diagnosis. This work proposes a method\nthat uses Physics-Informed Neural Networks (PINNs) to estimate patient-specific\nparameters of a reaction-diffusion PDE model of GBM growth from a single 3D\nstructural MRI snapshot. PINNs embed both the data and the PDE into a loss\nfunction, thus integrating theory and data. Key innovations include the\nidentification and estimation of characteristic non-dimensional parameters, a\npre-training step that utilizes the non-dimensional parameters and a\nfine-tuning step to determine the patient specific parameters. Additionally,\nthe diffuse domain method is employed to handle the complex brain geometry\nwithin the PINN framework. Our method is validated both on synthetic and\npatient datasets, and shows promise for real-time parametric inference in the\nclinical setting for personalized GBM treatment.",
            "author": [
                "Ray Zirui Zhang",
                "Ivan Ezhov",
                "Michal Balcerak",
                "Andy Zhu",
                "Benedikt Wiestler",
                "Bjoern Menze",
                "John Lowengrub"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16536v1",
                "http://arxiv.org/pdf/2311.16536v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "eess.IV",
                "q-bio.QM",
                "92-08, 92C50, 35Q92",
                "J.3; J.2; I.2.6"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16535v1",
            "title": "Contrastive encoder pre-training-based clustered federated learning for\n  heterogeneous data",
            "updated": "2023-11-28T05:44:26Z",
            "published": "2023-11-28T05:44:26Z",
            "summary": "Federated learning (FL) is a promising approach that enables distributed\nclients to collaboratively train a global model while preserving their data\nprivacy. However, FL often suffers from data heterogeneity problems, which can\nsignificantly affect its performance. To address this, clustered federated\nlearning (CFL) has been proposed to construct personalized models for different\nclient clusters. One effective client clustering strategy is to allow clients\nto choose their own local models from a model pool based on their performance.\nHowever, without pre-trained model parameters, such a strategy is prone to\nclustering failure, in which all clients choose the same model. Unfortunately,\ncollecting a large amount of labeled data for pre-training can be costly and\nimpractical in distributed environments. To overcome this challenge, we\nleverage self-supervised contrastive learning to exploit unlabeled data for the\npre-training of FL systems. Together, self-supervised pre-training and client\nclustering can be crucial components for tackling the data heterogeneity issues\nof FL. Leveraging these two crucial strategies, we propose contrastive\npre-training-based clustered federated learning (CP-CFL) to improve the model\nconvergence and overall performance of FL systems. In this work, we demonstrate\nthe effectiveness of CP-CFL through extensive experiments in heterogeneous FL\nsettings, and present various interesting observations.",
            "author": [
                "Ye Lin Tun",
                "Minh N. H. Nguyen",
                "Chu Myaet Thwal",
                "Jinwoo Choi",
                "Choong Seon Hong"
            ],
            "link": [
                "http://dx.doi.org/10.1016/j.neunet.2023.06.010",
                "http://arxiv.org/abs/2311.16535v1",
                "http://arxiv.org/pdf/2311.16535v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16534v1",
            "title": "Graph Prompt Learning: A Comprehensive Survey and Beyond",
            "updated": "2023-11-28T05:36:59Z",
            "published": "2023-11-28T05:36:59Z",
            "summary": "Artificial General Intelligence (AGI) has revolutionized numerous fields, yet\nits integration with graph data, a cornerstone in our interconnected world,\nremains nascent. This paper presents a pioneering survey on the emerging domain\nof graph prompts in AGI, addressing key challenges and opportunities in\nharnessing graph data for AGI applications. Despite substantial advancements in\nAGI across natural language processing and computer vision, the application to\ngraph data is relatively underexplored. This survey critically evaluates the\ncurrent landscape of AGI in handling graph data, highlighting the distinct\nchallenges in cross-modality, cross-domain, and cross-task applications\nspecific to graphs. Our work is the first to propose a unified framework for\nunderstanding graph prompt learning, offering clarity on prompt tokens, token\nstructures, and insertion patterns in the graph domain. We delve into the\nintrinsic properties of graph prompts, exploring their flexibility,\nexpressiveness, and interplay with existing graph models. A comprehensive\ntaxonomy categorizes over 100 works in this field, aligning them with\npre-training tasks across node-level, edge-level, and graph-level objectives.\nAdditionally, we present, ProG, a Python library, and an accompanying website,\nto support and advance research in graph prompting. The survey culminates in a\ndiscussion of current challenges and future directions, offering a roadmap for\nresearch in graph prompting within AGI. Through this comprehensive analysis, we\naim to catalyze further exploration and practical applications of AGI in graph\ndata, underlining its potential to reshape AGI fields and beyond. ProG and the\nwebsite can be accessed by\n\\url{https://github.com/WxxShirley/Awesome-Graph-Prompt}, and\n\\url{https://github.com/sheldonresearch/ProG}, respectively.",
            "author": [
                "Xiangguo Sun",
                "Jiawen Zhang",
                "Xixi Wu",
                "Hong Cheng",
                "Yun Xiong",
                "Jia Li"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16534v1",
                "http://arxiv.org/pdf/2311.16534v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16528v1",
            "title": "Utility Fairness in Contextual Dynamic Pricing with Demand Learning",
            "updated": "2023-11-28T05:19:23Z",
            "published": "2023-11-28T05:19:23Z",
            "summary": "This paper introduces a novel contextual bandit algorithm for personalized\npricing under utility fairness constraints in scenarios with uncertain demand,\nachieving an optimal regret upper bound. Our approach, which incorporates\ndynamic pricing and demand learning, addresses the critical challenge of\nfairness in pricing strategies. We first delve into the static full-information\nsetting to formulate an optimal pricing policy as a constrained optimization\nproblem. Here, we propose an approximation algorithm for efficiently and\napproximately computing the ideal policy.\n  We also use mathematical analysis and computational studies to characterize\nthe structures of optimal contextual pricing policies subject to fairness\nconstraints, deriving simplified policies which lays the foundations of more\nin-depth research and extensions.\n  Further, we extend our study to dynamic pricing problems with demand\nlearning, establishing a non-standard regret lower bound that highlights the\ncomplexity added by fairness constraints. Our research offers a comprehensive\nanalysis of the cost of fairness and its impact on the balance between utility\nand revenue maximization. This work represents a step towards integrating\nethical considerations into algorithmic efficiency in data-driven dynamic\npricing.",
            "author": [
                "Xi Chen",
                "David Simchi-Levi",
                "Yining Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16528v1",
                "http://arxiv.org/pdf/2311.16528v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16526v1",
            "title": "On robust overfitting: adversarial training induced distribution matters",
            "updated": "2023-11-28T05:11:53Z",
            "published": "2023-11-28T05:11:53Z",
            "summary": "Adversarial training may be regarded as standard training with a modified\nloss function. But its generalization error appears much larger than standard\ntraining under standard loss. This phenomenon, known as robust overfitting, has\nattracted significant research attention and remains largely as a mystery. In\nthis paper, we first show empirically that robust overfitting correlates with\nthe increasing generalization difficulty of the perturbation-induced\ndistributions along the trajectory of adversarial training (specifically\nPGD-based adversarial training). We then provide a novel upper bound for\ngeneralization error with respect to the perturbation-induced distributions, in\nwhich a notion of the perturbation operator, referred to \"local dispersion\",\nplays an important role.",
            "author": [
                "Runzhi Tian",
                "Yongyi Mao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16526v1",
                "http://arxiv.org/pdf/2311.16526v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16525v1",
            "title": "On the effectiveness of the early introduction of modern physics in\n  school curriculum: the case of the structure of atom versus wave-particle\n  duality",
            "updated": "2023-11-28T05:10:21Z",
            "published": "2023-11-28T05:10:21Z",
            "summary": "The dual nature of matter and radiation and the concept of the structure of\nan atom share a number of key conceptual elements from quantum mechanics.\nDespite the similarities, we find that the concept of the structure of an atom\nis well understood by students, in contrast to the wave-particle duality. The\nstudy analyzes students' comprehension of these two concepts by conducting a\nsemi-structured focus group interview and questionnaire. Through students'\nperformance in the questionnaire and their descriptive responses, we find that\nthe difficulties in their learning and understandings reflect the treatment of\nthe respective topic in the curriculum. The introduction of the structure of an\natom is early and repeated, whereas the dual nature of matter and radiation is\nintroduced late and abruptly. Based on our findings, we propose reforms in the\npresent curriculum that are necessary for an improved way of introducing the\nconcept of modern physics, like wave particle duality, to Indian students.",
            "author": [
                "Somya Swarnkar",
                "Rittick Roy",
                "Tejinder Kaur",
                "David Blair"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16525v1",
                "http://arxiv.org/pdf/2311.16525v1"
            ],
            "primary_category": "physics.ed-ph",
            "category": [
                "physics.ed-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16524v1",
            "title": "3D Teeth Reconstruction from Panoramic Radiographs using Neural Implicit\n  Functions",
            "updated": "2023-11-28T05:06:22Z",
            "published": "2023-11-28T05:06:22Z",
            "summary": "Panoramic radiography is a widely used imaging modality in dental practice\nand research. However, it only provides flattened 2D images, which limits the\ndetailed assessment of dental structures. In this paper, we propose Occudent, a\nframework for 3D teeth reconstruction from panoramic radiographs using neural\nimplicit functions, which, to the best of our knowledge, is the first work to\ndo so. For a given point in 3D space, the implicit function estimates whether\nthe point is occupied by a tooth, and thus implicitly determines the boundaries\nof 3D tooth shapes. Firstly, Occudent applies multi-label segmentation to the\ninput panoramic radiograph. Next, tooth shape embeddings as well as tooth class\nembeddings are generated from the segmentation outputs, which are fed to the\nreconstruction network. A novel module called Conditional eXcitation (CX) is\nproposed in order to effectively incorporate the combined shape and class\nembeddings into the implicit function. The performance of Occudent is evaluated\nusing both quantitative and qualitative measures. Importantly, Occudent is\ntrained and validated with actual panoramic radiographs as input, distinct from\nrecent works which used synthesized images. Experiments demonstrate the\nsuperiority of Occudent over state-of-the-art methods.",
            "author": [
                "Sihwa Park",
                "Seongjun Kim",
                "In-Seok Song",
                "Seung Jun Baek"
            ],
            "link": [
                "http://dx.doi.org/10.1007/978-3-031-43999-5_36",
                "http://arxiv.org/abs/2311.16524v1",
                "http://arxiv.org/pdf/2311.16524v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16522v1",
            "title": "Evaluation of dynamic characteristics of power grid based on GNN and\n  application on knowledge graph",
            "updated": "2023-11-28T05:00:27Z",
            "published": "2023-11-28T05:00:27Z",
            "summary": "A novel method for detecting faults in power grids using a graph neural\nnetwork (GNN) has been developed, aimed at enhancing intelligent fault\ndiagnosis in network operation and maintenance. This GNN-based approach\nidentifies faulty nodes within the power grid through a specialized electrical\nfeature extraction model coupled with a knowledge graph. Incorporating temporal\ndata, the method leverages the status of nodes from preceding and subsequent\ntime periods to aid in current fault detection. To validate the effectiveness\nof this GNN in extracting node features, a correlation analysis of the output\nfeatures from each node within the neural network layer was conducted. The\nresults from experiments show that this method can accurately locate fault\nnodes in simulated scenarios with a remarkable 99.53% accuracy. Additionally,\nthe graph neural network's feature modeling allows for a qualitative\nexamination of how faults spread across nodes, providing valuable insights for\nanalyzing fault nodes.",
            "author": [
                "Hao Pei",
                "Si Lin",
                "Chuanfu Li",
                "Che Wang",
                "Haoming Chen",
                "Sizhe Li"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16522v1",
                "http://arxiv.org/pdf/2311.16522v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16520v1",
            "title": "Value Approximation for Two-Player General-Sum Differential Games with\n  State Constraints",
            "updated": "2023-11-28T04:58:41Z",
            "published": "2023-11-28T04:58:41Z",
            "summary": "Solving Hamilton-Jacobi-Isaacs (HJI) PDEs enables equilibrial feedback\ncontrol in two-player differential games, yet faces the curse of dimensionality\n(CoD). While physics-informed machine learning has been adopted to address CoD\nin solving PDEs, this method falls short in learning discontinuous solutions\ndue to its sampling nature, leading to poor safety performance of the resulting\ncontrollers in robotics applications where values are discontinuous due to\nstate or other temporal logic constraints. In this study, we explore three\npotential solutions to this problem: (1) a hybrid learning method that uses\nboth equilibrium demonstrations and the HJI PDE, (2) a value-hardening method\nwhere a sequence of HJIs are solved with increasing Lipschitz constant on the\nconstraint violation penalty, and (3) the epigraphical technique that lifts the\nvalue to a higher dimensional auxiliary state space where the value becomes\ncontinuous. Evaluations through 5D and 9D vehicle simulations and 13D drone\nsimulations reveal that the hybrid method outperforms others in terms of\ngeneralization and safety performance.",
            "author": [
                "Lei Zhang",
                "Mukesh Ghimire",
                "Wenlong Zhang",
                "Zhe Xu",
                "Yi Ren"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16520v1",
                "http://arxiv.org/pdf/2311.16520v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.GT",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16519v2",
            "title": "B-LSTM-MIONet: Bayesian LSTM-based Neural Operators for Learning the\n  Response of Complex Dynamical Systems to Length-Variant Multiple Input\n  Functions",
            "updated": "2023-11-29T13:38:17Z",
            "published": "2023-11-28T04:58:17Z",
            "summary": "Deep Operator Network (DeepONet) is a neural network framework for learning\nnonlinear operators such as those from ordinary differential equations (ODEs)\ndescribing complex systems. Multiple-input deep neural operators (MIONet)\nextended DeepONet to allow multiple input functions in different Banach spaces.\nMIONet offers flexibility in training dataset grid spacing, without constraints\non output location. However, it requires offline inputs and cannot handle\nvarying sequence lengths in testing datasets, limiting its real-time\napplication in dynamic complex systems. This work redesigns MIONet, integrating\nLong Short Term Memory (LSTM) to learn neural operators from time-dependent\ndata. This approach overcomes data discretization constraints and harnesses\nLSTM's capability with variable-length, real-time data. Factors affecting\nlearning performance, like algorithm extrapolation ability are presented. The\nframework is enhanced with uncertainty quantification through a novel Bayesian\nmethod, sampling from MIONet parameter distributions. Consequently, we develop\nthe B-LSTM-MIONet, incorporating LSTM's temporal strengths with Bayesian\nrobustness, resulting in a more precise and reliable model for noisy datasets.",
            "author": [
                "Zhihao Kong",
                "Amirhossein Mollaali",
                "Christian Moya",
                "Na Lu",
                "Guang Lin"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16519v2",
                "http://arxiv.org/pdf/2311.16519v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.NA",
                "math.NA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16509v1",
            "title": "StyleCap: Automatic Speaking-Style Captioning from Speech Based on\n  Speech and Language Self-supervised Learning Models",
            "updated": "2023-11-28T04:49:17Z",
            "published": "2023-11-28T04:49:17Z",
            "summary": "We propose StyleCap, a method to generate natural language descriptions of\nspeaking styles appearing in speech. Although most of conventional techniques\nfor para-/non-linguistic information recognition focus on the category\nclassification or the intensity estimation of pre-defined labels, they cannot\nprovide the reasoning of the recognition result in an interpretable manner. As\na first step towards an end-to-end method for generating speaking-style prompts\nfrom speech, i.e., automatic speaking-style captioning, StyleCap uses paired\ndata of speech and natural language descriptions to train neural networks that\npredict prefix vectors fed into a large language model (LLM)-based text decoder\nfrom a speech representation vector. We explore an appropriate text decoder and\nspeech feature representation suitable for this new task. The experimental\nresults demonstrate that our StyleCap leveraging richer LLMs for the text\ndecoder, speech self-supervised learning (SSL) features, and sentence\nrephrasing augmentation improves the accuracy and diversity of generated\nspeaking-style captions. Samples of speaking-style captions generated by our\nStyleCap are publicly available.",
            "author": [
                "Kazuki Yamauchi",
                "Yusuke Ijima",
                "Yuki Saito"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16509v1",
                "http://arxiv.org/pdf/2311.16509v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG",
                "cs.SD",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16488v1",
            "title": "Efficient Multimodal Diffusion Models Using Joint Data Infilling with\n  Partially Shared U-Net",
            "updated": "2023-11-28T04:34:44Z",
            "published": "2023-11-28T04:34:44Z",
            "summary": "Recently, diffusion models have been used successfully to fit distributions\nfor cross-modal data translation and multimodal data generation. However, these\nmethods rely on extensive scaling, overlooking the inefficiency and\ninterference between modalities. We develop Partially Shared U-Net (PS-U-Net)\narchitecture which is an efficient multimodal diffusion model that allows text\nand image inputs to pass through dedicated layers and skip-connections for\npreserving modality-specific fine-grained details. Inspired by image\ninpainting, we also propose a new efficient multimodal sampling method that\nintroduces new scenarios for conditional generation while only requiring a\nsimple joint distribution to be learned. Our empirical exploration of the\nMS-COCO dataset demonstrates that our method generates multimodal text and\nimage data with higher quality compared to existing multimodal diffusion models\nwhile having a comparable size, faster training, faster multimodal sampling,\nand more flexible generation.",
            "author": [
                "Zizhao Hu",
                "Shaochong Jia",
                "Mohammad Rostami"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16488v1",
                "http://arxiv.org/pdf/2311.16488v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16487v2",
            "title": "On the Robustness of Decision-Focused Learning",
            "updated": "2023-11-30T03:56:06Z",
            "published": "2023-11-28T04:34:04Z",
            "summary": "Decision-Focused Learning (DFL) is an emerging learning paradigm that tackles\nthe task of training a machine learning (ML) model to predict missing\nparameters of an incomplete optimization problem, where the missing parameters\nare predicted. DFL trains an ML model in an end-to-end system, by integrating\nthe prediction and optimization tasks, providing better alignment of the\ntraining and testing objectives. DFL has shown a lot of promise and holds the\ncapacity to revolutionize decision-making in many real-world applications.\nHowever, very little is known about the performance of these models under\nadversarial attacks. We adopt ten unique DFL methods and benchmark their\nperformance under two distinctly focused attacks adapted towards the\nPredict-then-Optimize problem setting. Our study proposes the hypothesis that\nthe robustness of a model is highly correlated with its ability to find\npredictions that lead to optimal decisions without deviating from the\nground-truth label. Furthermore, we provide insight into how to target the\nmodels that violate this condition and show how these models respond\ndifferently depending on the achieved optimality at the end of their training\ncycles.",
            "author": [
                "Yehya Farhat"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16487v2",
                "http://arxiv.org/pdf/2311.16487v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "math.OC",
                "68Txx"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16486v1",
            "title": "On the adaptation of causal forests to manifold data",
            "updated": "2023-11-28T04:32:36Z",
            "published": "2023-11-28T04:32:36Z",
            "summary": "Researchers often hold the belief that random forests are \"the cure to the\nworld's ills\" (Bickel, 2010). But how exactly do they achieve this? Focused on\nthe recently introduced causal forests (Athey and Imbens, 2016; Wager and\nAthey, 2018), this manuscript aims to contribute to an ongoing research trend\ntowards answering this question, proving that causal forests can adapt to the\nunknown covariate manifold structure. In particular, our analysis shows that a\ncausal forest estimator can achieve the optimal rate of convergence for\nestimating the conditional average treatment effect, with the covariate\ndimension automatically replaced by the manifold dimension. These findings\nalign with analogous observations in the realm of deep learning and resonate\nwith the insights presented in Peter Bickel's 2004 Rietz lecture.",
            "author": [
                "Yiyi Huo",
                "Yingying Fan",
                "Fang Han"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16486v1",
                "http://arxiv.org/pdf/2311.16486v1"
            ],
            "primary_category": "math.ST",
            "category": [
                "math.ST",
                "econ.EM",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16464v1",
            "title": "Bridging the Gap: A Unified Video Comprehension Framework for Moment\n  Retrieval and Highlight Detection",
            "updated": "2023-11-28T03:55:23Z",
            "published": "2023-11-28T03:55:23Z",
            "summary": "Video Moment Retrieval (MR) and Highlight Detection (HD) have attracted\nsignificant attention due to the growing demand for video analysis. Recent\napproaches treat MR and HD as similar video grounding problems and address them\ntogether with transformer-based architecture. However, we observe that the\nemphasis of MR and HD differs, with one necessitating the perception of local\nrelationships and the other prioritizing the understanding of global contexts.\nConsequently, the lack of task-specific design will inevitably lead to\nlimitations in associating the intrinsic specialty of two tasks. To tackle the\nissue, we propose a Unified Video COMprehension framework (UVCOM) to bridge the\ngap and jointly solve MR and HD effectively. By performing progressive\nintegration on intra and inter-modality across multi-granularity, UVCOM\nachieves the comprehensive understanding in processing a video. Moreover, we\npresent multi-aspect contrastive learning to consolidate the local relation\nmodeling and global knowledge accumulation via well aligned multi-modal space.\nExtensive experiments on QVHighlights, Charades-STA, TACoS , YouTube Highlights\nand TVSum datasets demonstrate the effectiveness and rationality of UVCOM which\noutperforms the state-of-the-art methods by a remarkable margin.",
            "author": [
                "Yicheng Xiao",
                "Zhuoyan Luo",
                "Yong Liu",
                "Yue Ma",
                "Hengwei Bian",
                "Yatai Ji",
                "Yujiu Yang",
                "Xiu Li"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16464v1",
                "http://arxiv.org/pdf/2311.16464v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16459v1",
            "title": "On the Effect of Defections in Federated Learning and How to Prevent\n  Them",
            "updated": "2023-11-28T03:34:22Z",
            "published": "2023-11-28T03:34:22Z",
            "summary": "Federated learning is a machine learning protocol that enables a large\npopulation of agents to collaborate over multiple rounds to produce a single\nconsensus model. There are several federated learning applications where agents\nmay choose to defect permanently$-$essentially withdrawing from the\ncollaboration$-$if they are content with their instantaneous model in that\nround. This work demonstrates the detrimental impact of such defections on the\nfinal model's robustness and ability to generalize. We also show that current\nfederated optimization algorithms fail to disincentivize these harmful\ndefections. We introduce a novel optimization algorithm with theoretical\nguarantees to prevent defections while ensuring asymptotic convergence to an\neffective solution for all participating agents. We also provide numerical\nexperiments to corroborate our findings and demonstrate the effectiveness of\nour algorithm.",
            "author": [
                "Minbiao Han",
                "Kumar Kshitij Patel",
                "Han Shao",
                "Lingxiao Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16459v1",
                "http://arxiv.org/pdf/2311.16459v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.DC",
                "cs.GT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17088v1",
            "title": "Unsupervised Multimodal Deepfake Detection Using Intra- and Cross-Modal\n  Inconsistencies",
            "updated": "2023-11-28T03:28:19Z",
            "published": "2023-11-28T03:28:19Z",
            "summary": "Deepfake videos present an increasing threat to society with potentially\nnegative impact on criminal justice, democracy, and personal safety and\nprivacy. Meanwhile, detecting deepfakes, at scale, remains a very challenging\ntasks that often requires labeled training data from existing deepfake\ngeneration methods. Further, even the most accurate supervised learning,\ndeepfake detection methods do not generalize to deepfakes generated using new\ngeneration methods. In this paper, we introduce a novel unsupervised approach\nfor detecting deepfake videos by measuring of intra- and cross-modal\nconsistency among multimodal features; specifically visual, audio, and identity\nfeatures. The fundamental hypothesis behind the proposed detection method is\nthat since deepfake generation attempts to transfer the facial motion of one\nidentity to another, these methods will eventually encounter a trade-off\nbetween motion and identity that enviably leads to detectable inconsistencies.\nWe validate our method through extensive experimentation, demonstrating the\nexistence of significant intra- and cross- modal inconsistencies in deepfake\nvideos, which can be effectively utilized to detect them with high accuracy.\nOur proposed method is scalable because it does not require pristine samples at\ninference, generalizable because it is trained only on real data, and is\nexplainable since it can pinpoint the exact location of modality\ninconsistencies which are then verifiable by a human expert.",
            "author": [
                "Mulin Tian",
                "Mahyar Khayatkhoei",
                "Joe Mathai",
                "Wael AbdAlmageed"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17088v1",
                "http://arxiv.org/pdf/2311.17088v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03732v1",
            "title": "A Rank Stabilization Scaling Factor for Fine-Tuning with LoRA",
            "updated": "2023-11-28T03:23:20Z",
            "published": "2023-11-28T03:23:20Z",
            "summary": "As large language models (LLMs) have become increasingly compute and memory\nintensive, parameter-efficient fine-tuning (PEFT) methods are now a common\nstrategy to fine-tune LLMs. A popular PEFT method is Low-Rank Adapters (LoRA),\nwhich adds trainable low-rank \"adapters\" to selected layers. Each adapter\nconsists of a low-rank matrix product, multiplicatively scaled by a\nrank-dependent factor. This scaling factor, which divides adapters by a factor\nof the rank, results in slowed learning and stunted performance for LoRA with\nhigher-rank adapters. Consequently, the use of LoRA in practice has generally\nbeen limited to very low ranks. In this work, we study the impact of the\nscaling factor on the learning process and prove that LoRA adapters should be\ndivided by a factor of the square root of the rank. Modifying LoRA with the\nappropriate scaling factor, which we call the rank-stabilized LoRA (rsLoRA)\nmethod, easily provides for a fine-tuning compute/performance trade-off, where\nlarger ranks can be used to trade off increased computational resources during\ntraining for better fine-tuning performance, with no change in inference\ncomputing cost.",
            "author": [
                "Damjan Kalajdzievski"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03732v1",
                "http://arxiv.org/pdf/2312.03732v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG",
                "I.2.7"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16452v1",
            "title": "Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case\n  Study in Medicine",
            "updated": "2023-11-28T03:16:12Z",
            "published": "2023-11-28T03:16:12Z",
            "summary": "Generalist foundation models such as GPT-4 have displayed surprising\ncapabilities in a wide variety of domains and tasks. Yet, there is a prevalent\nassumption that they cannot match specialist capabilities of fine-tuned models.\nFor example, most explorations to date on medical competency benchmarks have\nleveraged domain-specific training, as exemplified by efforts on BioGPT and\nMed-PaLM. We build on a prior study of GPT-4's capabilities on medical\nchallenge benchmarks in the absence of special training. Rather than using\nsimple prompting to highlight the model's out-of-the-box capabilities, we\nperform a systematic exploration of prompt engineering. We find that prompting\ninnovation can unlock deeper specialist capabilities and show that GPT-4 easily\ntops prior leading results for medical benchmarks. The prompting methods we\nexplore are general purpose, and make no specific use of domain expertise,\nremoving the need for expert-curated content. Our experimental design carefully\ncontrols for overfitting during the prompt engineering process. We introduce\nMedprompt, based on a composition of several prompting strategies. With\nMedprompt, GPT-4 achieves state-of-the-art results on all nine of the benchmark\ndatasets in the MultiMedQA suite. The method outperforms leading specialist\nmodels such as Med-PaLM 2 by a significant margin with an order of magnitude\nfewer calls to the model. Steering GPT-4 with Medprompt achieves a 27%\nreduction in error rate on the MedQA dataset over the best methods to date\nachieved with specialist models and surpasses a score of 90% for the first\ntime. Beyond medical problems, we show the power of Medprompt to generalize to\nother domains and provide evidence for the broad applicability of the approach\nvia studies of the strategy on exams in electrical engineering, machine\nlearning, philosophy, accounting, law, nursing, and clinical psychology.",
            "author": [
                "Harsha Nori",
                "Yin Tat Lee",
                "Sheng Zhang",
                "Dean Carignan",
                "Richard Edgar",
                "Nicolo Fusi",
                "Nicholas King",
                "Jonathan Larson",
                "Yuanzhi Li",
                "Weishung Liu",
                "Renqian Luo",
                "Scott Mayer McKinney",
                "Robert Osazuwa Ness",
                "Hoifung Poon",
                "Tao Qin",
                "Naoto Usuyama",
                "Chris White",
                "Eric Horvitz"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16452v1",
                "http://arxiv.org/pdf/2311.16452v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "I.2.7"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00812v1",
            "title": "Empowering Autonomous Driving with Large Language Models: A Safety\n  Perspective",
            "updated": "2023-11-28T03:13:09Z",
            "published": "2023-11-28T03:13:09Z",
            "summary": "Autonomous Driving (AD) faces crucial hurdles for commercial launch, notably\nin the form of diminished public trust and safety concerns from long-tail\nunforeseen driving scenarios. This predicament is due to the limitation of deep\nneural networks in AD software, which struggle with interpretability and\nexhibit poor generalization capabilities in out-of-distribution and uncertain\nscenarios. To this end, this paper advocates for the integration of Large\nLanguage Models (LLMs) into the AD system, leveraging their robust common-sense\nknowledge, reasoning abilities, and human-interaction capabilities. The\nproposed approach deploys the LLM as an intelligent decision-maker in planning,\nincorporating safety verifiers for contextual safety learning to enhance\noverall AD performance and safety. We present results from two case studies\nthat affirm the efficacy of our approach. We further discuss the potential\nintegration of LLM for other AD software components including perception,\nprediction, and simulation. Despite the observed challenges in the case\nstudies, the integration of LLMs is promising and beneficial for reinforcing\nboth safety and performance in AD.",
            "author": [
                "Yixuan Wang",
                "Ruochen Jiao",
                "Chengtian Lang",
                "Sinong Simon Zhan",
                "Chao Huang",
                "Zhaoran Wang",
                "Zhuoran Yang",
                "Qi Zhu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00812v1",
                "http://arxiv.org/pdf/2312.00812v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.LG",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16450v2",
            "title": "Typhoon Intensity Prediction with Vision Transformer",
            "updated": "2023-12-04T07:59:05Z",
            "published": "2023-11-28T03:11:33Z",
            "summary": "Predicting typhoon intensity accurately across space and time is crucial for\nissuing timely disaster warnings and facilitating emergency response. This has\nvast potential for minimizing life losses and property damages as well as\nreducing economic and environmental impacts. Leveraging satellite imagery for\nscenario analysis is effective but also introduces additional challenges due to\nthe complex relations among clouds and the highly dynamic context. Existing\ndeep learning methods in this domain rely on convolutional neural networks\n(CNNs), which suffer from limited per-layer receptive fields. This limitation\nhinders their ability to capture long-range dependencies and global contextual\nknowledge during inference. In response, we introduce a novel approach, namely\n\"Typhoon Intensity Transformer\" (Tint), which leverages self-attention\nmechanisms with global receptive fields per layer. Tint adopts a\nsequence-to-sequence feature representation learning perspective. It begins by\ncutting a given satellite image into a sequence of patches and recursively\nemploys self-attention operations to extract both local and global contextual\nrelations between all patch pairs simultaneously, thereby enhancing per-patch\nfeature representation learning. Extensive experiments on a publicly available\ntyphoon benchmark validate the efficacy of Tint in comparison with both\nstate-of-the-art deep learning and conventional meteorological methods. Our\ncode is available at https://github.com/chen-huanxin/Tint.",
            "author": [
                "Huanxin Chen",
                "Pengshuai Yin",
                "Huichou Huang",
                "Qingyao Wu",
                "Ruirui Liu",
                "Xiatian Zhu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16450v2",
                "http://arxiv.org/pdf/2311.16450v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16447v2",
            "title": "TopoSemiSeg: Enforcing Topological Consistency for Semi-Supervised\n  Segmentation of Histopathology Images",
            "updated": "2023-12-06T20:53:15Z",
            "published": "2023-11-28T03:04:35Z",
            "summary": "In computational pathology, segmenting densely distributed objects like\nglands and nuclei is crucial for downstream analysis. To alleviate the burden\nof obtaining pixel-wise annotations, semi-supervised learning methods learn\nfrom large amounts of unlabeled data. Nevertheless, existing semi-supervised\nmethods overlook the topological information hidden in the unlabeled images and\nare thus prone to topological errors, e.g., missing or incorrectly\nmerged/separated glands or nuclei. To address this issue, we propose\nTopoSemiSeg, the first semi-supervised method that learns the topological\nrepresentation from unlabeled data. In particular, we propose a topology-aware\nteacher-student approach in which the teacher and student networks learn shared\ntopological representations. To achieve this, we introduce topological\nconsistency loss, which contains signal consistency and noise removal losses to\nensure the learned representation is robust and focuses on true topological\nsignals. Extensive experiments on public pathology image datasets show the\nsuperiority of our method, especially on topology-wise evaluation metrics. Code\nis available at https://github.com/Melon-Xu/TopoSemiSeg.",
            "author": [
                "Meilong Xu",
                "Xiaoling Hu",
                "Saumya Gupta",
                "Shahira Abousamra",
                "Chao Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16447v2",
                "http://arxiv.org/pdf/2311.16447v2"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16445v1",
            "title": "CLAP: Contrastive Learning with Augmented Prompts for Robustness on\n  Pretrained Vision-Language Models",
            "updated": "2023-11-28T03:00:59Z",
            "published": "2023-11-28T03:00:59Z",
            "summary": "Contrastive vision-language models, e.g., CLIP, have garnered substantial\nattention for their exceptional generalization capabilities. However, their\nrobustness to perturbations has ignited concerns. Existing strategies typically\nreinforce their resilience against adversarial examples by enabling the image\nencoder to \"see\" these perturbed examples, often necessitating a complete\nretraining of the image encoder on both natural and adversarial samples. In\nthis study, we propose a new method to enhance robustness solely through text\naugmentation, eliminating the need for retraining the image encoder on\nadversarial examples. Our motivation arises from the realization that text and\nimage data inherently occupy a shared latent space, comprising latent content\nvariables and style variables. This insight suggests the feasibility of\nlearning to disentangle these latent content variables using text data\nexclusively. To accomplish this, we introduce an effective text augmentation\nmethod that focuses on modifying the style while preserving the content in the\ntext data. By changing the style part of the text data, we empower the text\nencoder to emphasize latent content variables, ultimately enhancing the\nrobustness of vision-language models. Our experiments across various datasets\ndemonstrate substantial improvements in the robustness of the pre-trained CLIP\nmodel.",
            "author": [
                "Yichao Cai",
                "Yuhang Liu",
                "Zhen Zhang",
                "Javen Qinfeng Shi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16445v1",
                "http://arxiv.org/pdf/2311.16445v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16444v2",
            "title": "Exo2EgoDVC: Dense Video Captioning of Egocentric Procedural Activities\n  Using Web Instructional Videos",
            "updated": "2023-11-29T06:01:34Z",
            "published": "2023-11-28T02:51:13Z",
            "summary": "We propose a novel benchmark for cross-view knowledge transfer of dense video\ncaptioning, adapting models from web instructional videos with exocentric views\nto an egocentric view. While dense video captioning (predicting time segments\nand their captions) is primarily studied with exocentric videos (e.g.,\nYouCook2), benchmarks with egocentric videos are restricted due to data\nscarcity. To overcome the limited video availability, transferring knowledge\nfrom abundant exocentric web videos is demanded as a practical approach.\nHowever, learning the correspondence between exocentric and egocentric views is\ndifficult due to their dynamic view changes. The web videos contain mixed views\nfocusing on either human body actions or close-up hand-object interactions,\nwhile the egocentric view is constantly shifting as the camera wearer moves.\nThis necessitates the in-depth study of cross-view transfer under complex view\nchanges. In this work, we first create a real-life egocentric dataset (EgoYC2)\nwhose captions are shared with YouCook2, enabling transfer learning between\nthese datasets assuming their ground-truth is accessible. To bridge the view\ngaps, we propose a view-invariant learning method using adversarial training in\nboth the pre-training and fine-tuning stages. While the pre-training is\ndesigned to learn invariant features against the mixed views in the web videos,\nthe view-invariant fine-tuning further mitigates the view gaps between both\ndatasets. We validate our proposed method by studying how effectively it\novercomes the view change problem and efficiently transfers the knowledge to\nthe egocentric domain. Our benchmark pushes the study of the cross-view\ntransfer into a new task domain of dense video captioning and will envision\nmethodologies to describe egocentric videos in natural language.",
            "author": [
                "Takehiko Ohkawa",
                "Takuma Yagi",
                "Taichi Nishimura",
                "Ryosuke Furuta",
                "Atsushi Hashimoto",
                "Yoshitaka Ushiku",
                "Yoichi Sato"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16444v2",
                "http://arxiv.org/pdf/2311.16444v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16442v1",
            "title": "Enabling Fast 2-bit LLM on GPUs: Memory Alignment, Sparse Outlier, and\n  Asynchronous Dequantization",
            "updated": "2023-11-28T02:44:59Z",
            "published": "2023-11-28T02:44:59Z",
            "summary": "Large language models (LLMs) have demonstrated impressive abilities in\nvarious domains while the inference cost is expensive. The state-of-the-art\nmethods use 2-bit quantization for mainstream LLMs. However, challenges still\nexist: (1) Nonnegligible accuracy loss for 2-bit quantization. Weights are\nquantized by groups, while the ranges of weights are large in some groups,\nresulting in large quantization errors and nonnegligible accuracy loss (e.g.\n>3% for Llama2-7b with 2-bit quantization in GPTQ and Greenbit). (2) Limited\naccuracy improvement by adding 4-bit weights. Increasing 10% extra average bit\nmore 4-bit weights only leads to <0.5% accuracy improvement on a quantized\nLlama2-7b. (3) Time-consuming dequantization operations on GPUs. The\ndequantization operations lead to >50% execution time, hindering the potential\nof reducing LLM inference cost. To tackle these challenges, we propose the\nfollowing techniques: (1) We only quantize a small fraction of groups with the\nlarger range using 4-bit with memory alignment consideration on GPUs. (2) We\npoint out that the distribution of the sparse outliers with larger weights is\ndifferent in 2-bit and 4-bit groups, and only a small fraction of outliers\nrequire 16-bit quantization. Such design leads to >0.5% accuracy improvement\nwith <3% average increased bit for Llama2-7b. (3) We design the asynchronous\ndequantization on GPUs, leading to up to 3.92X speedup. We conduct extensive\nexperiments on different model families and model sizes. We achieve 2.85-bit\nfor each weight and the end-to-end speedup for Llama2-7b is 1.74X over the\noriginal model, and we reduce both runtime cost and hardware cost by up to\n2.70X and 2.81X with less GPU requirements.",
            "author": [
                "Jinhao Li",
                "Shiyao Li",
                "Jiaming Xu",
                "Shan Huang",
                "Yaoxiu Lian",
                "Jun Liu",
                "Yu Wang",
                "Guohao Dai"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16442v1",
                "http://arxiv.org/pdf/2311.16442v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16441v1",
            "title": "ControlRec: Bridging the Semantic Gap between Language Model and\n  Personalized Recommendation",
            "updated": "2023-11-28T02:43:02Z",
            "published": "2023-11-28T02:43:02Z",
            "summary": "The successful integration of large language models (LLMs) into\nrecommendation systems has proven to be a major breakthrough in recent studies,\npaving the way for more generic and transferable recommendations. However, LLMs\nstruggle to effectively utilize user and item IDs, which are crucial\nidentifiers for successful recommendations. This is mainly due to their\ndistinct representation in a semantic space that is different from the natural\nlanguage (NL) typically used to train LLMs. To tackle such issue, we introduce\nControlRec, an innovative Contrastive prompt learning framework for\nRecommendation systems. ControlRec treats user IDs and NL as heterogeneous\nfeatures and encodes them individually. To promote greater alignment and\nintegration between them in the semantic space, we have devised two auxiliary\ncontrastive objectives: (1) Heterogeneous Feature Matching (HFM) aligning item\ndescription with the corresponding ID or user's next preferred ID based on\ntheir interaction sequence, and (2) Instruction Contrastive Learning (ICL)\neffectively merging these two crucial data sources by contrasting probability\ndistributions of output sequences generated by diverse tasks. Experimental\nresults on four public real-world datasets demonstrate the effectiveness of the\nproposed method on improving model performance.",
            "author": [
                "Junyan Qiu",
                "Haitao Wang",
                "Zhaolin Hong",
                "Yiping Yang",
                "Qiang Liu",
                "Xingxing Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16441v1",
                "http://arxiv.org/pdf/2311.16441v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03731v1",
            "title": "MultiGPrompt for Multi-Task Pre-Training and Prompting on Graphs",
            "updated": "2023-11-28T02:36:53Z",
            "published": "2023-11-28T02:36:53Z",
            "summary": "Graphs can inherently model interconnected objects on the Web, thereby\nfacilitating a series of Web applications, such as web analyzing and content\nrecommendation. Recently, Graph Neural Networks (GNNs) have emerged as a\nmainstream technique for graph representation learning. However, their efficacy\nwithin an end-to-end supervised framework is significantly tied to the\navailabilityof task-specific labels. To mitigate labeling costs and enhance\nrobustness in few-shot settings, pre-training on self-supervised tasks has\nemerged as a promising method, while prompting has been proposed to further\nnarrow the objective gap between pretext and downstream tasks. Although there\nhas been some initial exploration of prompt-based learning on graphs, they\nprimarily leverage a single pretext task, resulting in a limited subset of\ngeneral knowledge that could be learned from the pre-training data. Hence, in\nthis paper, we propose MultiGPrompt, a novel multi-task pre-training and\nprompting framework to exploit multiple pretext tasks for more comprehensive\npre-trained knowledge. First, in pre-training, we design a set of pretext\ntokens to synergize multiple pretext tasks. Second, we propose a dual-prompt\nmechanism consisting of composed and open prompts to leverage task-specific and\nglobal pre-training knowledge, to guide downstream tasks in few-shot settings.\nFinally, we conduct extensive experiments on six public datasets to evaluate\nand analyze MultiGPrompt.",
            "author": [
                "Xingtong Yu",
                "Chang Zhou",
                "Yuan Fang",
                "Xinming Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03731v1",
                "http://arxiv.org/pdf/2312.03731v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17085v1",
            "title": "Beyond Visual Cues: Synchronously Exploring Target-Centric Semantics for\n  Vision-Language Tracking",
            "updated": "2023-11-28T02:28:12Z",
            "published": "2023-11-28T02:28:12Z",
            "summary": "Single object tracking aims to locate one specific target in video sequences,\ngiven its initial state. Classical trackers rely solely on visual cues,\nrestricting their ability to handle challenges such as appearance variations,\nambiguity, and distractions. Hence, Vision-Language (VL) tracking has emerged\nas a promising approach, incorporating language descriptions to directly\nprovide high-level semantics and enhance tracking performance. However, current\nVL trackers have not fully exploited the power of VL learning, as they suffer\nfrom limitations such as heavily relying on off-the-shelf backbones for feature\nextraction, ineffective VL fusion designs, and the absence of VL-related loss\nfunctions. Consequently, we present a novel tracker that progressively explores\ntarget-centric semantics for VL tracking. Specifically, we propose the first\nSynchronous Learning Backbone (SLB) for VL tracking, which consists of two\nnovel modules: the Target Enhance Module (TEM) and the Semantic Aware Module\n(SAM). These modules enable the tracker to perceive target-related semantics\nand comprehend the context of both visual and textual modalities at the same\npace, facilitating VL feature extraction and fusion at different semantic\nlevels. Moreover, we devise the dense matching loss to further strengthen\nmulti-modal representation learning. Extensive experiments on VL tracking\ndatasets demonstrate the superiority and effectiveness of our methods.",
            "author": [
                "Jiawei Ge",
                "Xiangmei Chen",
                "Jiuxin Cao",
                "Xuelin Zhu",
                "Weijia Liu",
                "Bo Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17085v1",
                "http://arxiv.org/pdf/2311.17085v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16432v1",
            "title": "Text-Driven Image Editing via Learnable Regions",
            "updated": "2023-11-28T02:27:31Z",
            "published": "2023-11-28T02:27:31Z",
            "summary": "Language has emerged as a natural interface for image editing. In this paper,\nwe introduce a method for region-based image editing driven by textual prompts,\nwithout the need for user-provided masks or sketches. Specifically, our\napproach leverages an existing pretrained text-to-image model and introduces a\nbounding box generator to find the edit regions that are aligned with the\ntextual prompts. We show that this simple approach enables flexible editing\nthat is compatible with current image generation models, and is able to handle\ncomplex prompts featuring multiple objects, complex sentences or long\nparagraphs. We conduct an extensive user study to compare our method against\nstate-of-the-art methods. Experiments demonstrate the competitive performance\nof our method in manipulating images with high fidelity and realism that align\nwith the language descriptions provided. Our project webpage:\nhttps://yuanze-lin.me/LearnableRegions_page.",
            "author": [
                "Yuanze Lin",
                "Yi-Wen Chen",
                "Yi-Hsuan Tsai",
                "Lu Jiang",
                "Ming-Hsuan Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16432v1",
                "http://arxiv.org/pdf/2311.16432v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16424v1",
            "title": "Manifold Preserving Guided Diffusion",
            "updated": "2023-11-28T02:08:06Z",
            "published": "2023-11-28T02:08:06Z",
            "summary": "Despite the recent advancements, conditional image generation still faces\nchallenges of cost, generalizability, and the need for task-specific training.\nIn this paper, we propose Manifold Preserving Guided Diffusion (MPGD), a\ntraining-free conditional generation framework that leverages pretrained\ndiffusion models and off-the-shelf neural networks with minimal additional\ninference cost for a broad range of tasks. Specifically, we leverage the\nmanifold hypothesis to refine the guided diffusion steps and introduce a\nshortcut algorithm in the process. We then propose two methods for on-manifold\ntraining-free guidance using pre-trained autoencoders and demonstrate that our\nshortcut inherently preserves the manifolds when applied to latent diffusion\nmodels. Our experiments show that MPGD is efficient and effective for solving a\nvariety of conditional generation applications in low-compute settings, and can\nconsistently offer up to 3.8x speed-ups with the same number of diffusion steps\nwhile maintaining high sample quality compared to the baselines.",
            "author": [
                "Yutong He",
                "Naoki Murata",
                "Chieh-Hsin Lai",
                "Yuhta Takida",
                "Toshimitsu Uesaka",
                "Dongjun Kim",
                "Wei-Hsiang Liao",
                "Yuki Mitsufuji",
                "J. Zico Kolter",
                "Ruslan Salakhutdinov",
                "Stefano Ermon"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16424v1",
                "http://arxiv.org/pdf/2311.16424v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16420v1",
            "title": "Model-free Test Time Adaptation for Out-Of-Distribution Detection",
            "updated": "2023-11-28T02:00:47Z",
            "published": "2023-11-28T02:00:47Z",
            "summary": "Out-of-distribution (OOD) detection is essential for the reliability of ML\nmodels. Most existing methods for OOD detection learn a fixed decision\ncriterion from a given in-distribution dataset and apply it universally to\ndecide if a data point is OOD. Recent work~\\cite{fang2022is} shows that given\nonly in-distribution data, it is impossible to reliably detect OOD data without\nextra assumptions. Motivated by the theoretical result and recent exploration\nof test-time adaptation methods, we propose a Non-Parametric Test Time\n\\textbf{Ada}ptation framework for \\textbf{O}ut-Of-\\textbf{D}istribution\n\\textbf{D}etection (\\abbr). Unlike conventional methods, \\abbr utilizes online\ntest samples for model adaptation during testing, enhancing adaptability to\nchanging data distributions. The framework incorporates detected OOD instances\ninto decision-making, reducing false positive rates, particularly when ID and\nOOD distributions overlap significantly. We demonstrate the effectiveness of\n\\abbr through comprehensive experiments on multiple OOD detection benchmarks,\nextensive empirical studies show that \\abbr significantly improves the\nperformance of OOD detection over state-of-the-art methods. Specifically, \\abbr\nreduces the false positive rate (FPR95) by $23.23\\%$ on the CIFAR-10 benchmarks\nand $38\\%$ on the ImageNet-1k benchmarks compared to the advanced methods.\nLastly, we theoretically verify the effectiveness of \\abbr.",
            "author": [
                "YiFan Zhang",
                "Xue Wang",
                "Tian Zhou",
                "Kun Yuan",
                "Zhang Zhang",
                "Liang Wang",
                "Rong Jin",
                "Tieniu Tan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16420v1",
                "http://arxiv.org/pdf/2311.16420v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16416v1",
            "title": "A Combinatorial Approach to Robust PCA",
            "updated": "2023-11-28T01:49:51Z",
            "published": "2023-11-28T01:49:51Z",
            "summary": "We study the problem of recovering Gaussian data under adversarial\ncorruptions when the noises are low-rank and the corruptions are on the\ncoordinate level. Concretely, we assume that the Gaussian noises lie in an\nunknown $k$-dimensional subspace $U \\subseteq \\mathbb{R}^d$, and $s$ randomly\nchosen coordinates of each data point fall into the control of an adversary.\nThis setting models the scenario of learning from high-dimensional yet\nstructured data that are transmitted through a highly-noisy channel, so that\nthe data points are unlikely to be entirely clean.\n  Our main result is an efficient algorithm that, when $ks^2 = O(d)$, recovers\nevery single data point up to a nearly-optimal $\\ell_1$ error of $\\tilde\nO(ks/d)$ in expectation. At the core of our proof is a new analysis of the\nwell-known Basis Pursuit (BP) method for recovering a sparse signal, which is\nknown to succeed under additional assumptions (e.g., incoherence or the\nrestricted isometry property) on the underlying subspace $U$. In contrast, we\npresent a novel approach via studying a natural combinatorial problem and show\nthat, over the randomness in the support of the sparse signal, a\nhigh-probability error bound is possible even if the subspace $U$ is arbitrary.",
            "author": [
                "Weihao Kong",
                "Mingda Qiao",
                "Rajat Sen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16416v1",
                "http://arxiv.org/pdf/2311.16416v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16411v1",
            "title": "Learning Markovian Dynamics with Spectral Maps",
            "updated": "2023-11-28T01:40:06Z",
            "published": "2023-11-28T01:40:06Z",
            "summary": "The long-time behavior of many complex molecular systems is often governed by\nslow relaxation dynamics that can be described by a few reaction coordinates\nreferred to as collective variables (CVs). However, identifying CVs hidden in a\nhigh-dimensional configuration space poses a fundamental challenge in chemical\nphysics. To address this problem, we expand on a recently introduced\ndeep-learning technique called spectral map [Rydzewski, J. Phys. Chem. Lett.\n2023, 14, 22, 5216-5220]. Spectral map learns CVs by maximizing a spectral gap\nbetween slow and fast eigenvalues of a Markov transition matrix describing\nanisotropic diffusion. An introduced modification in the learning algorithm\nallows spectral map to represent multiscale free-energy landscapes. Through a\nMarkov state model analysis, we validate that spectral map learns slow CVs\nrelated to the dominant relaxation timescales and discerns between long-lived\nmetastable states.",
            "author": [
                "Jakub Rydzewski",
                "Tu\u011f\u00e7e G\u00f6kdemir"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16411v1",
                "http://arxiv.org/pdf/2311.16411v1"
            ],
            "primary_category": "physics.chem-ph",
            "category": [
                "physics.chem-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16410v1",
            "title": "Reduced-order modeling for parameterized PDEs via implicit neural\n  representations",
            "updated": "2023-11-28T01:35:06Z",
            "published": "2023-11-28T01:35:06Z",
            "summary": "We present a new data-driven reduced-order modeling approach to efficiently\nsolve parametrized partial differential equations (PDEs) for many-query\nproblems. This work is inspired by the concept of implicit neural\nrepresentation (INR), which models physics signals in a continuous manner and\nindependent of spatial/temporal discretization. The proposed framework encodes\nPDE and utilizes a parametrized neural ODE (PNODE) to learn latent dynamics\ncharacterized by multiple PDE parameters. PNODE can be inferred by a\nhypernetwork to reduce the potential difficulties in learning PNODE due to a\ncomplex multilayer perceptron (MLP). The framework uses an INR to decode the\nlatent dynamics and reconstruct accurate PDE solutions. Further, a\nphysics-informed loss is also introduced to correct the prediction of unseen\nparameter instances. Incorporating the physics-informed loss also enables the\nmodel to be fine-tuned in an unsupervised manner on unseen PDE parameters. A\nnumerical experiment is performed on a two-dimensional Burgers equation with a\nlarge variation of PDE parameters. We evaluate the proposed method at a large\nReynolds number and obtain up to speedup of O(10^3) and ~1% relative error to\nthe ground truth values.",
            "author": [
                "Tianshu Wen",
                "Kookjin Lee",
                "Youngsoo Choi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16410v1",
                "http://arxiv.org/pdf/2311.16410v1"
            ],
            "primary_category": "math.NA",
            "category": [
                "math.NA",
                "cs.LG",
                "cs.NA",
                "math-ph",
                "math.MP",
                "physics.comp-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17083v1",
            "title": "CLiC: Concept Learning in Context",
            "updated": "2023-11-28T01:33:18Z",
            "published": "2023-11-28T01:33:18Z",
            "summary": "This paper addresses the challenge of learning a local visual pattern of an\nobject from one image, and generating images depicting objects with that\npattern. Learning a localized concept and placing it on an object in a target\nimage is a nontrivial task, as the objects may have different orientations and\nshapes. Our approach builds upon recent advancements in visual concept\nlearning. It involves acquiring a visual concept (e.g., an ornament) from a\nsource image and subsequently applying it to an object (e.g., a chair) in a\ntarget image. Our key idea is to perform in-context concept learning, acquiring\nthe local visual concept within the broader context of the objects they belong\nto. To localize the concept learning, we employ soft masks that contain both\nthe concept within the mask and the surrounding image area. We demonstrate our\napproach through object generation within an image, showcasing plausible\nembedding of in-context learned concepts. We also introduce methods for\ndirecting acquired concepts to specific locations within target images,\nemploying cross-attention mechanisms, and establishing correspondences between\nsource and target objects. The effectiveness of our method is demonstrated\nthrough quantitative and qualitative experiments, along with comparisons\nagainst baseline techniques.",
            "author": [
                "Mehdi Safaee",
                "Aryan Mikaeili",
                "Or Patashnik",
                "Daniel Cohen-Or",
                "Ali Mahdavi-Amiri"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17083v1",
                "http://arxiv.org/pdf/2311.17083v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17082v2",
            "title": "DreamPropeller: Supercharge Text-to-3D Generation with Parallel Sampling",
            "updated": "2023-12-06T07:15:58Z",
            "published": "2023-11-28T01:28:58Z",
            "summary": "Recent methods such as Score Distillation Sampling (SDS) and Variational\nScore Distillation (VSD) using 2D diffusion models for text-to-3D generation\nhave demonstrated impressive generation quality. However, the long generation\ntime of such algorithms significantly degrades the user experience. To tackle\nthis problem, we propose DreamPropeller, a drop-in acceleration algorithm that\ncan be wrapped around any existing text-to-3D generation pipeline based on\nscore distillation. Our framework generalizes Picard iterations, a classical\nalgorithm for parallel sampling an ODE path, and can account for non-ODE paths\nsuch as momentum-based gradient updates and changes in dimensions during the\noptimization process as in many cases of 3D generation. We show that our\nalgorithm trades parallel compute for wallclock time and empirically achieves\nup to 4.7x speedup with a negligible drop in generation quality for all tested\nframeworks.",
            "author": [
                "Linqi Zhou",
                "Andy Shih",
                "Chenlin Meng",
                "Stefano Ermon"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17082v2",
                "http://arxiv.org/pdf/2311.17082v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00811v1",
            "title": "Seizure detection from Electroencephalogram signals via Wavelets and\n  Graph Theory metrics",
            "updated": "2023-11-28T01:07:14Z",
            "published": "2023-11-28T01:07:14Z",
            "summary": "Epilepsy is one of the most prevalent neurological conditions, where an\nepileptic seizure is a transient occurrence due to abnormal, excessive and\nsynchronous activity in the brain. Electroencephalogram signals emanating from\nthe brain may be captured, analysed and then play a significant role in\ndetection and prediction of epileptic seizures. In this work we enhance upon a\nprevious approach that relied on the differing properties of the wavelet\ntransform. Here we apply the Maximum Overlap Discrete Wavelet Transform to both\nreduce signal \\textit{noise} and use signal variance exhibited at differing\ninherent frequency levels to develop various metrics of connection between the\nelectrodes placed upon the scalp. %The properties of both the noise reduced\nsignal and the interconnected electrodes differ significantly during the\ndifferent brain states.\n  Using short duration epochs, to approximate close to real time monitoring,\ntogether with simple statistical parameters derived from the reconstructed\nnoise reduced signals we initiate seizure detection. To further improve\nperformance we utilise graph theoretic indicators from derived electrode\nconnectivity. From there we build the attribute space. We utilise open-source\nsoftware and publicly available data to highlight the superior\nRecall/Sensitivity performance of our approach, when compared to existing\npublished methods.",
            "author": [
                "Paul Grant",
                "Md Zahidul Islam"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00811v1",
                "http://arxiv.org/pdf/2312.00811v1"
            ],
            "primary_category": "q-bio.NC",
            "category": [
                "q-bio.NC",
                "cs.LG",
                "q-bio.QM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17081v1",
            "title": "I-MedSAM: Implicit Medical Image Segmentation with Segment Anything",
            "updated": "2023-11-28T00:43:52Z",
            "published": "2023-11-28T00:43:52Z",
            "summary": "With the development of Deep Neural Networks (DNNs), many efforts have been\nmade to handle medical image segmentation. Traditional methods such as nnUNet\ntrain specific segmentation models on the individual datasets. Plenty of recent\nmethods have been proposed to adapt the foundational Segment Anything Model\n(SAM) to medical image segmentation. However, they still focus on discrete\nrepresentations to generate pixel-wise predictions, which are spatially\ninflexible and scale poorly to higher resolution. In contrast, implicit methods\nlearn continuous representations for segmentation, which is crucial for medical\nimage segmentation. In this paper, we propose I-MedSAM, which leverages the\nbenefits of both continuous representations and SAM, to obtain better\ncross-domain ability and accurate boundary delineation. Since medical image\nsegmentation needs to predict detailed segmentation boundaries, we designed a\nnovel adapter to enhance the SAM features with high-frequency information\nduring Parameter Efficient Fine Tuning (PEFT). To convert the SAM features and\ncoordinates into continuous segmentation output, we utilize Implicit Neural\nRepresentation (INR) to learn an implicit segmentation decoder. We also propose\nan uncertainty-guided sampling strategy for efficient learning of INR.\nExtensive evaluations on 2D medical image segmentation tasks have shown that\nour proposed method with only 1.6M trainable parameters outperforms existing\nmethods including discrete and continuous methods. The code will be released.",
            "author": [
                "Xiaobao Wei",
                "Jiajun Cao",
                "Yizhu Jin",
                "Ming Lu",
                "Guangyu Wang",
                "Shanghang Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17081v1",
                "http://arxiv.org/pdf/2311.17081v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16388v1",
            "title": "Understanding the Process of Data Labeling in Cybersecurity",
            "updated": "2023-11-28T00:20:07Z",
            "published": "2023-11-28T00:20:07Z",
            "summary": "Many domains now leverage the benefits of Machine Learning (ML), which\npromises solutions that can autonomously learn to solve complex tasks by\ntraining over some data. Unfortunately, in cyberthreat detection, high-quality\ndata is hard to come by. Moreover, for some specific applications of ML, such\ndata must be labeled by human operators. Many works \"assume\" that labeling is\ntough/challenging/costly in cyberthreat detection, thereby proposing solutions\nto address such a hurdle. Yet, we found no work that specifically addresses the\nprocess of labeling 'from the viewpoint of ML security practitioners'. This is\na problem: to this date, it is still mostly unknown how labeling is done in\npractice -- thereby preventing one from pinpointing \"what is needed\" in the\nreal world.\n  In this paper, we take the first step to build a bridge between academic\nresearch and security practice in the context of data labeling. First, we reach\nout to five subject matter experts and carry out open interviews to identify\npain points in their labeling routines. Then, by using our findings as a\nscaffold, we conduct a user study with 13 practitioners from large security\ncompanies, and ask detailed questions on subjects such as active learning,\ncosts of labeling, and revision of labels. Finally, we perform proof-of-concept\nexperiments addressing labeling-related aspects in cyberthreat detection that\nare sometimes overlooked in research. Altogether, our contributions and\nrecommendations serve as a stepping stone to future endeavors aimed at\nimproving the quality and robustness of ML-driven security systems. We release\nour resources.",
            "author": [
                "Tobias Braun",
                "Irdin Pekaric",
                "Giovanni Apruzzese"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16388v1",
                "http://arxiv.org/pdf/2311.16388v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16383v1",
            "title": "\"Do Users fall for Real Adversarial Phishing?\" Investigating the Human\n  response to Evasive Webpages",
            "updated": "2023-11-28T00:08:48Z",
            "published": "2023-11-28T00:08:48Z",
            "summary": "Phishing websites are everywhere, and countermeasures based on static\nblocklists cannot cope with such a threat. To address this problem,\nstate-of-the-art solutions entail the application of machine learning (ML) to\ndetect phishing websites by checking if they visually resemble webpages of\nwell-known brands. These techniques have achieved promising results in research\nand, consequently, some security companies began to deploy them also in their\nphishing detection systems (PDS). However, ML methods are not perfect and some\nsamples are bound to bypass even production-grade PDS.\n  In this paper, we scrutinize whether 'genuine phishing websites' that evade\n'commercial ML-based PDS' represent a problem \"in reality\". Although nobody\nlikes landing on a phishing webpage, a false negative may not lead to serious\nconsequences if the users (i.e., the actual target of phishing) can recognize\nthat \"something is phishy\". Practically, we carry out the first user-study\n(N=126) wherein we assess whether unsuspecting users (having diverse\nbackgrounds) are deceived by 'adversarial' phishing webpages that evaded a real\nPDS. We found that some well-crafted adversarial webpages can trick most\nparticipants (even IT experts), albeit others are easily recognized by most\nusers. Our study is relevant for practitioners, since it allows prioritizing\nphishing webpages that simultaneously fool (i) machines and (ii) humans --\ni.e., their intended targets.",
            "author": [
                "Ajka Draganovic",
                "Savino Dambra",
                "Javier Aldana Iuit",
                "Kevin Roundy",
                "Giovanni Apruzzese"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16383v1",
                "http://arxiv.org/pdf/2311.16383v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16381v1",
            "title": "Deep Learning for Time Series Classification of Parkinson's Disease Eye\n  Tracking Data",
            "updated": "2023-11-28T00:03:18Z",
            "published": "2023-11-28T00:03:18Z",
            "summary": "Eye-tracking is an accessible and non-invasive technology that provides\ninformation about a subject's motor and cognitive abilities. As such, it has\nproven to be a valuable resource in the study of neurodegenerative diseases\nsuch as Parkinson's disease. Saccade experiments, in particular, have proven\nuseful in the diagnosis and staging of Parkinson's disease. However, to date,\nno single eye-movement biomarker has been found to conclusively differentiate\npatients from healthy controls. In the present work, we investigate the use of\nstate-of-the-art deep learning algorithms to perform Parkinson's disease\nclassification using eye-tracking data from saccade experiments. In contrast to\nprevious work, instead of using hand-crafted features from the saccades, we use\nraw $\\sim1.5\\,s$ long fixation intervals recorded during the preparatory phase\nbefore each trial. Using these short time series as input we implement two\ndifferent classification models, InceptionTime and ROCKET. We find that the\nmodels are able to learn the classification task and generalize to unseen\nsubjects. InceptionTime achieves $78\\%$ accuracy, while ROCKET achieves $88\\%$\naccuracy. We also employ a novel method for pruning the ROCKET model to improve\ninterpretability and generalizability, achieving an accuracy of $96\\%$. Our\nresults suggest that fixation data has low inter-subject variability and\npotentially carries useful information about brain cognitive and motor\nconditions, making it suitable for use with machine learning in the discovery\nof disease-relevant biomarkers.",
            "author": [
                "Gonzalo Uribarri",
                "Simon Ekman von Huth",
                "Josefine Waldthaler",
                "Per Svenningsson",
                "Erik Frans\u00e9n"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16381v1",
                "http://arxiv.org/pdf/2311.16381v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "q-bio.QM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17080v1",
            "title": "Combating the \"Sameness\" in AI Art: Reflections on the Interactive AI\n  Installation Fencing Hallucination",
            "updated": "2023-11-28T00:00:34Z",
            "published": "2023-11-28T00:00:34Z",
            "summary": "The article summarizes three types of \"sameness\" issues in Artificial\nIntelligence(AI) art, each occurring at different stages of development in AI\nimage creation tools. Through the Fencing Hallucination project, the article\nreflects on the design of AI art production in alleviating the sense of\nuniformity, maintaining the uniqueness of images from an AI image synthesizer,\nand enhancing the connection between the artworks and the audience. This paper\nendeavors to stimulate the creation of distinctive AI art by recounting the\nefforts and insights derived from the Fencing Hallucination project, all\ndedicated to addressing the issue of \"sameness\".",
            "author": [
                "Weihao Qiu",
                "George Legrady"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17080v1",
                "http://arxiv.org/pdf/2311.17080v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00050v1",
            "title": "Elijah: Eliminating Backdoors Injected in Diffusion Models via\n  Distribution Shift",
            "updated": "2023-11-27T23:58:56Z",
            "published": "2023-11-27T23:58:56Z",
            "summary": "Diffusion models (DM) have become state-of-the-art generative models because\nof their capability to generate high-quality images from noises without\nadversarial training. However, they are vulnerable to backdoor attacks as\nreported by recent studies. When a data input (e.g., some Gaussian noise) is\nstamped with a trigger (e.g., a white patch), the backdoored model always\ngenerates the target image (e.g., an improper photo). However, effective\ndefense strategies to mitigate backdoors from DMs are underexplored. To bridge\nthis gap, we propose the first backdoor detection and removal framework for\nDMs. We evaluate our framework Elijah on hundreds of DMs of 3 types including\nDDPM, NCSN and LDM, with 13 samplers against 3 existing backdoor attacks.\nExtensive experiments show that our approach can have close to 100% detection\naccuracy and reduce the backdoor effects to close to zero without significantly\nsacrificing the model utility.",
            "author": [
                "Shengwei An",
                "Sheng-Yen Chou",
                "Kaiyuan Zhang",
                "Qiuling Xu",
                "Guanhong Tao",
                "Guangyu Shen",
                "Siyuan Cheng",
                "Shiqing Ma",
                "Pin-Yu Chen",
                "Tsung-Yi Ho",
                "Xiangyu Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00050v1",
                "http://arxiv.org/pdf/2312.00050v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16380v1",
            "title": "Learning Multimodal Latent Dynamics for Human-Robot Interaction",
            "updated": "2023-11-27T23:56:59Z",
            "published": "2023-11-27T23:56:59Z",
            "summary": "This article presents a method for learning well-coordinated Human-Robot\nInteraction (HRI) from Human-Human Interactions (HHI). We devise a hybrid\napproach using Hidden Markov Models (HMMs) as the latent space priors for a\nVariational Autoencoder to model a joint distribution over the interacting\nagents. We leverage the interaction dynamics learned from HHI to learn HRI and\nincorporate the conditional generation of robot motions from human observations\ninto the training, thereby predicting more accurate robot trajectories. The\ngenerated robot motions are further adapted with Inverse Kinematics to ensure\nthe desired physical proximity with a human, combining the ease of joint space\nlearning and accurate task space reachability. For contact-rich interactions,\nwe modulate the robot's stiffness using HMM segmentation for a compliant\ninteraction. We verify the effectiveness of our approach deployed on a Humanoid\nrobot via a user study. Our method generalizes well to various humans despite\nbeing trained on data from just two humans. We find that Users perceive our\nmethod as more human-like, timely, and accurate and rank our method with a\nhigher degree of preference over other baselines.",
            "author": [
                "Vignesh Prasad",
                "Lea Heitlinger",
                "Dorothea Koert",
                "Ruth Stock-Homburg",
                "Jan Peters",
                "Georgia Chalvatzaki"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16380v1",
                "http://arxiv.org/pdf/2311.16380v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.HC",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16378v1",
            "title": "Bayesian Formulations for Graph Spectral Denoising",
            "updated": "2023-11-27T23:53:19Z",
            "published": "2023-11-27T23:53:19Z",
            "summary": "We consider noisy signals which are defined on the vertices of a graph and\npresent smoothing algorithms for the cases of Gaussian, dropout, and uniformly\ndistributed noise. The signals are assumed to follow a prior distribution\ndefined in the frequency domain which favors signals which are smooth across\nthe edges of the graph. By pairing this prior distribution with our three\nmodels of noise generation, we propose \\textit{Maximum A Posteriori} (M.A.P.)\nestimates of the true signal in the presence of noisy data and provide\nalgorithms for computing the M.A.P. Finally, we demonstrate the algorithms'\nability to effectively restore white noise on image data, and from severe\ndropout in toy \\& EHR data.",
            "author": [
                "Sam Leone",
                "Xingzhi Sun",
                "Michael Perlmutter",
                "Smita Krishnaswamy"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16378v1",
                "http://arxiv.org/pdf/2311.16378v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16374v1",
            "title": "Physics-Informed Neural Network for Discovering Systems with\n  Unmeasurable States with Application to Lithium-Ion Batteries",
            "updated": "2023-11-27T23:35:40Z",
            "published": "2023-11-27T23:35:40Z",
            "summary": "Combining machine learning with physics is a trending approach for\ndiscovering unknown dynamics, and one of the most intensively studied\nframeworks is the physics-informed neural network (PINN). However, PINN often\nfails to optimize the network due to its difficulty in concurrently minimizing\nmultiple losses originating from the system's governing equations. This problem\ncan be more serious when the system's states are unmeasurable, like lithium-ion\nbatteries (LiBs). In this work, we introduce a robust method for training PINN\nthat uses fewer loss terms and thus constructs a less complex landscape for\noptimization. In particular, instead of having loss terms from each\ndifferential equation, this method embeds the dynamics into a loss function\nthat quantifies the error between observed and predicted system outputs. This\nis accomplished by numerically integrating the predicted states from the neural\nnetwork(NN) using known dynamics and transforming them to obtain a sequence of\npredicted outputs. Minimizing such a loss optimizes the NN to predict states\nconsistent with observations given the physics. Further, the system's\nparameters can be added to the optimization targets. To demonstrate the ability\nof this method to perform various modeling and control tasks, we apply it to a\nbattery model to concurrently estimate its states and parameters.",
            "author": [
                "Yuichi Kajiura",
                "Jorge Espin",
                "Dong Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16374v1",
                "http://arxiv.org/pdf/2311.16374v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16372v1",
            "title": "Joint Deep Image Restoration and Unsupervised Quality Assessment",
            "updated": "2023-11-27T23:32:20Z",
            "published": "2023-11-27T23:32:20Z",
            "summary": "Deep learning techniques have revolutionized the fields of image restoration\nand image quality assessment in recent years. While image restoration methods\ntypically utilize synthetically distorted training data for training, deep\nquality assessment models often require expensive labeled subjective data.\nHowever, recent studies have shown that activations of deep neural networks\ntrained for visual modeling tasks can also be used for perceptual quality\nassessment of images. Following this intuition, we propose a novel\nattention-based convolutional neural network capable of simultaneously\nperforming both image restoration and quality assessment. We achieve this by\ntraining a JPEG deblocking network augmented with \"quality attention\" maps and\ndemonstrating state-of-the-art deblocking accuracy, achieving a high\ncorrelation of predicted quality with human opinion scores.",
            "author": [
                "Hakan Emre Gedik",
                "Abhinau K. Venkataramanan",
                "Alan C. Bovik"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16372v1",
                "http://arxiv.org/pdf/2311.16372v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17932v2",
            "title": "Generating Molecular Conformer Fields",
            "updated": "2023-12-05T05:48:16Z",
            "published": "2023-11-27T22:53:41Z",
            "summary": "In this paper we tackle the problem of generating conformers of a molecule in\n3D space given its molecular graph. We parameterize these conformers as\ncontinuous functions that map elements from the molecular graph to points in 3D\nspace. We then formulate the problem of learning to generate conformers as\nlearning a distribution over these functions using a diffusion generative\nmodel, called Molecular Conformer Fields (MCF). Our approach is simple and\nscalable, and achieves state-of-the-art performance on challenging molecular\nconformer generation benchmarks while making no assumptions about the explicit\nstructure of molecules (e.g. modeling torsional angles). MCF represents an\nadvance in extending diffusion models to handle complex scientific problems in\na conceptually simple, scalable and effective manner.",
            "author": [
                "Yuyang Wang",
                "Ahmed A. Elhag",
                "Navdeep Jaitly",
                "Joshua M. Susskind",
                "Miguel Angel Bautista"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17932v2",
                "http://arxiv.org/pdf/2311.17932v2"
            ],
            "primary_category": "physics.chem-ph",
            "category": [
                "physics.chem-ph",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16361v2",
            "title": "Making Self-supervised Learning Robust to Spurious Correlation via\n  Learning-speed Aware Sampling",
            "updated": "2023-11-29T23:19:30Z",
            "published": "2023-11-27T22:52:45Z",
            "summary": "Self-supervised learning (SSL) has emerged as a powerful technique for\nlearning rich representations from unlabeled data. The data representations are\nable to capture many underlying attributes of data, and be useful in downstream\nprediction tasks. In real-world settings, spurious correlations between some\nattributes (e.g. race, gender and age) and labels for downstream tasks often\nexist, e.g. cancer is usually more prevalent among elderly patients. In this\npaper, we investigate SSL in the presence of spurious correlations and show\nthat the SSL training loss can be minimized by capturing only a subset of the\nconspicuous features relevant to those sensitive attributes, despite the\npresence of other important predictive features for the downstream tasks. To\naddress this issue, we investigate the learning dynamics of SSL and observe\nthat the learning is slower for samples that conflict with such correlations\n(e.g. elder patients without cancer). Motivated by these findings, we propose a\nlearning-speed aware SSL (LA-SSL) approach, in which we sample each training\ndata with a probability that is inversely related to its learning speed. We\nevaluate LA-SSL on three datasets that exhibit spurious correlations between\ndifferent attributes, demonstrating that it improves the robustness of\npretrained representations on downstream classification tasks.",
            "author": [
                "Weicheng Zhu",
                "Sheng Liu",
                "Carlos Fernandez-Granda",
                "Narges Razavian"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16361v2",
                "http://arxiv.org/pdf/2311.16361v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17078v1",
            "title": "Data Imbalance, Uncertainty Quantification, and Generalization via\n  Transfer Learning in Data-driven Parameterizations: Lessons from the\n  Emulation of Gravity Wave Momentum Transport in WACCM",
            "updated": "2023-11-27T22:51:10Z",
            "published": "2023-11-27T22:51:10Z",
            "summary": "Neural networks (NNs) are increasingly used for data-driven subgrid-scale\nparameterization in weather and climate models. While NNs are powerful tools\nfor learning complex nonlinear relationships from data, there are several\nchallenges in using them for parameterizations. Three of these challenges are\n1) data imbalance related to learning rare (often large-amplitude) samples; 2)\nuncertainty quantification (UQ) of the predictions to provide an accuracy\nindicator; and 3) generalization to other climates, e.g., those with higher\nradiative forcing. Here, we examine the performance of methods for addressing\nthese challenges using NN-based emulators of the Whole Atmosphere Community\nClimate Model (WACCM) physics-based gravity wave (GW) parameterizations as the\ntest case. WACCM has complex, state-of-the-art parameterizations for\norography-, convection- and frontal-driven GWs. Convection- and\norography-driven GWs have significant data imbalance due to the absence of\nconvection or orography in many grid points. We address data imbalance using\nresampling and/or weighted loss functions, enabling the successful emulation of\nparameterizations for all three sources. We demonstrate that three UQ methods\n(Bayesian NNs, variational auto-encoders, and dropouts) provide ensemble\nspreads that correspond to accuracy during testing, offering criteria on when a\nNN gives inaccurate predictions. Finally, we show that the accuracy of these\nNNs decreases for a warmer climate (4XCO2). However, the generalization\naccuracy is significantly improved by applying transfer learning, e.g.,\nre-training only one layer using ~1% new data from the warmer climate. The\nfindings of this study offer insights for developing reliable and generalizable\ndata-driven parameterizations for various processes, including (but not\nlimited) to GWs.",
            "author": [
                "Y. Qiang Sun",
                "Hamid A. Pahlavan",
                "Ashesh Chattopadhyay",
                "Pedram Hassanzadeh",
                "Sandro W. Lubis",
                "M. Joan Alexander",
                "Edwin Gerber",
                "Aditi Sheshadri",
                "Yifei Guan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17078v1",
                "http://arxiv.org/pdf/2311.17078v1"
            ],
            "primary_category": "physics.ao-ph",
            "category": [
                "physics.ao-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16357v1",
            "title": "Cross Entropy in Deep Learning of Classifiers Is Unnecessary -- ISBE\n  Error is All You Need",
            "updated": "2023-11-27T22:40:02Z",
            "published": "2023-11-27T22:40:02Z",
            "summary": "In deep learning classifiers, the cost function usually takes the form of a\ncombination of SoftMax and CrossEntropy functions. The SoftMax unit transforms\nthe scores predicted by the model network into assessments of the degree\n(probabilities) of an object's membership to a given class. On the other hand,\nCrossEntropy measures the divergence of this prediction from the distribution\nof target scores. This work introduces the ISBE functionality, justifying the\nthesis about the redundancy of cross entropy computation in deep learning of\nclassifiers. Not only can we omit the calculation of entropy, but also, during\nback-propagation, there is no need to direct the error to the normalization\nunit for its backward transformation. Instead, the error is sent directly to\nthe model's network. Using examples of perceptron and convolutional networks as\nclassifiers of images from the MNIST collection, it is observed for ISBE that\nresults are not degraded with SoftMax only, but also with other activation\nfunctions such as Sigmoid, Tanh, or their hard variants HardSigmoid and\nHardTanh. Moreover, up to three percent of time is saved within the total time\nof forward and backward stages. The article is addressed mainly to programmers\nand students interested in deep model learning. For example, it illustrates in\ncode snippets possible ways to implement ISBE units, but also formally proves\nthat the softmax trick only applies to the class of softmax functions with\nrelocations.",
            "author": [
                "Wladyslaw Skarbek"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16357v1",
                "http://arxiv.org/pdf/2311.16357v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "I.2.5"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16353v1",
            "title": "Improving Denoising Diffusion Probabilistic Models via Exploiting Shared\n  Representations",
            "updated": "2023-11-27T22:30:26Z",
            "published": "2023-11-27T22:30:26Z",
            "summary": "In this work, we address the challenge of multi-task image generation with\nlimited data for denoising diffusion probabilistic models (DDPM), a class of\ngenerative models that produce high-quality images by reversing a noisy\ndiffusion process. We propose a novel method, SR-DDPM, that leverages\nrepresentation-based techniques from few-shot learning to effectively learn\nfrom fewer samples across different tasks. Our method consists of a core meta\narchitecture with shared parameters, i.e., task-specific layers with exclusive\nparameters. By exploiting the similarity between diverse data distributions,\nour method can scale to multiple tasks without compromising the image quality.\nWe evaluate our method on standard image datasets and show that it outperforms\nboth unconditional and conditional DDPM in terms of FID and SSIM metrics.",
            "author": [
                "Delaram Pirhayatifard",
                "Mohammad Taha Toghani",
                "Guha Balakrishnan",
                "C\u00e9sar A. Uribe"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16353v1",
                "http://arxiv.org/pdf/2311.16353v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CV",
                "eess.IV",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16351v1",
            "title": "Microscopic Mechanism of the Thermal Amorphization of ZIF-4 and Melting\n  of ZIF-zni Revealed via Molecular Dynamics and Machine Learning Techniques",
            "updated": "2023-11-27T22:29:50Z",
            "published": "2023-11-27T22:29:50Z",
            "summary": "We investigate the microscopic mechanism of the thermally induced ambient\npressure ordered-disordered phase transitions of two zeolitic imidazolate\nframeworks of formula Zn(C$_3$H$_3$N$_2$)$_2$: a porous (ZIF-4) and a dense,\nnon-porous (ZIF-zni) polymorph via a combination of data science and computer\nsimulation approaches. Molecular dynamics simulations are carried out at the\natomistic level through the nb-ZIF-FF force field that incorporates\nligand-metal reactivity and relies on dummy atoms to reproduce the correct\ntetrahedral topology around Zn$^{2+}$ centres. The force field is capable of\nreproducing the structure of ZIF-4, ZIF-zni and the amorphous (ZIF$\\_$a) and\nliquid (ZIF$\\_$liq) phases that respectively result when these crystalline\nmaterials are heated. Symmetry functions computed over a database of structures\nof the four phases, are used as inputs to train a neural network that predicts\nthe probabilities of belonging to each of the four phases at the local\nZn$^{2+}$ level with 90$\\%$ accuracy. We apply this methodology to follow the\ntime-evolution of the amorphization of ZIF-4 and the melting of ZIF-zni along a\nseries of molecular dynamics trajectories. We first computed the transition\ntemperature and determined associated thermodynamic state functions.\nSubsequently, we studied the mechanisms. Both processes consist of two steps:\n(i) for ZIF-4, a low-density amorphous phase is first formed, followed by the\nfinal ZIF$\\_$a phase while (ii) for ZIF-zni, a ZIF$\\_$a-like phase precedes the\nformation of the liquid phase. These processes involve connectivity changes in\nthe first neighbour ligands around the central Zn$^{2+}$ cations. We find that\nthe amorphization of ZIF-4 is a non-isotropic processes and we trace back the\norigins of this anisotropic behaviour to density and lability of coordination\nbonds.",
            "author": [
                "Emilio Mendez",
                "Rocio Semino"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16351v1",
                "http://arxiv.org/pdf/2311.16351v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16346v1",
            "title": "Small and Dim Target Detection in IR Imagery: A Review",
            "updated": "2023-11-27T22:25:46Z",
            "published": "2023-11-27T22:25:46Z",
            "summary": "While there has been significant progress in object detection using\nconventional image processing and machine learning algorithms, exploring small\nand dim target detection in the IR domain is a relatively new area of study.\nThe majority of small and dim target detection methods are derived from\nconventional object detection algorithms, albeit with some alterations. The\ntask of detecting small and dim targets in IR imagery is complex. This is\nbecause these targets often need distinct features, the background is cluttered\nwith unclear details, and the IR signatures of the scene can change over time\ndue to fluctuations in thermodynamics. The primary objective of this review is\nto highlight the progress made in this field. This is the first review in the\nfield of small and dim target detection in infrared imagery, encompassing\nvarious methodologies ranging from conventional image processing to\ncutting-edge deep learning-based approaches. The authors have also introduced a\ntaxonomy of such approaches. There are two main types of approaches:\nmethodologies using several frames for detection, and single-frame-based\ndetection techniques. Single frame-based detection techniques encompass a\ndiverse range of methods, spanning from traditional image processing-based\napproaches to more advanced deep learning methodologies. Our findings indicate\nthat deep learning approaches perform better than traditional image\nprocessing-based approaches. In addition, a comprehensive compilation of\nvarious available datasets has also been provided. Furthermore, this review\nidentifies the gaps and limitations in existing techniques, paving the way for\nfuture research and development in this area.",
            "author": [
                "Nikhil Kumar",
                "Pravendra Singh"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16346v1",
                "http://arxiv.org/pdf/2311.16346v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17076v1",
            "title": "Compositional Chain-of-Thought Prompting for Large Multimodal Models",
            "updated": "2023-11-27T22:23:27Z",
            "published": "2023-11-27T22:23:27Z",
            "summary": "The combination of strong visual backbones and Large Language Model (LLM)\nreasoning has led to Large Multimodal Models (LMMs) becoming the current\nstandard for a wide range of vision and language (VL) tasks. However, recent\nresearch has shown that even the most advanced LMMs still struggle to capture\naspects of compositional visual reasoning, such as attributes and relationships\nbetween objects. One solution is to utilize scene graphs (SGs)--a formalization\nof objects and their relations and attributes that has been extensively used as\na bridge between the visual and textual domains. Yet, scene graph data requires\nscene graph annotations, which are expensive to collect and thus not easily\nscalable. Moreover, finetuning an LMM based on SG data can lead to catastrophic\nforgetting of the pretraining objective. To overcome this, inspired by\nchain-of-thought methods, we propose Compositional Chain-of-Thought (CCoT), a\nnovel zero-shot Chain-of-Thought prompting method that utilizes SG\nrepresentations in order to extract compositional knowledge from an LMM.\nSpecifically, we first generate an SG using the LMM, and then use that SG in\nthe prompt to produce a response. Through extensive experiments, we find that\nthe proposed CCoT approach not only improves LMM performance on several vision\nand language VL compositional benchmarks but also improves the performance of\nseveral popular LMMs on general multimodal benchmarks, without the need for\nfine-tuning or annotated ground-truth SGs.",
            "author": [
                "Chancharik Mitra",
                "Brandon Huang",
                "Trevor Darrell",
                "Roei Herzig"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17076v1",
                "http://arxiv.org/pdf/2311.17076v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16339v1",
            "title": "Reward Shaping for Improved Learning in Real-time Strategy Game Play",
            "updated": "2023-11-27T21:56:18Z",
            "published": "2023-11-27T21:56:18Z",
            "summary": "We investigate the effect of reward shaping in improving the performance of\nreinforcement learning in the context of the real-time strategy,\ncapture-the-flag game. The game is characterized by sparse rewards that are\nassociated with infrequently occurring events such as grabbing or capturing the\nflag, or tagging the opposing player. We show that appropriately designed\nreward shaping functions applied to different game events can significantly\nimprove the player's performance and training times of the player's learning\nalgorithm. We have validated our reward shaping functions within a simulated\nenvironment for playing a marine capture-the-flag game between two players. Our\nexperimental results demonstrate that reward shaping can be used as an\neffective means to understand the importance of different sub-tasks during\ngame-play towards winning the game, to encode a secondary objective functions\nsuch as energy efficiency into a player's game-playing behavior, and, to\nimprove learning generalizable policies that can perform well against different\nskill levels of the opponent.",
            "author": [
                "John Kliem",
                "Prithviraj Dasgupta"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16339v1",
                "http://arxiv.org/pdf/2311.16339v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "I.2.6"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16337v2",
            "title": "Multi-3D-Models Registration-Based Augmented Reality (AR) Instructions\n  for Assembly",
            "updated": "2023-11-29T03:24:31Z",
            "published": "2023-11-27T21:53:17Z",
            "summary": "This paper introduces a novel, markerless, step-by-step, in-situ 3D Augmented\nReality (AR) instruction method and its application - BRICKxAR (Multi 3D\nModels/M3D) - for small parts assembly. BRICKxAR (M3D) realistically visualizes\nrendered 3D assembly parts at the assembly location of the physical assembly\nmodel (Figure 1). The user controls the assembly process through a user\ninterface. BRICKxAR (M3D) utilizes deep learning-trained 3D model-based\nregistration. Object recognition and tracking become challenging as the\nassembly model updates at each step. Additionally, not every part in a 3D\nassembly may be visible to the camera during the assembly. BRICKxAR (M3D)\ncombines multiple assembly phases with a step count to address these\nchallenges. Thus, using fewer phases simplifies the complex assembly process\nwhile step count facilitates accurate object recognition and precise\nvisualization of each step. A testing and heuristic evaluation of the BRICKxAR\n(M3D) prototype and qualitative analysis were conducted with users and experts\nin visualization and human-computer interaction. Providing robust 3D AR\ninstructions and allowing the handling of the assembly model, BRICKxAR (M3D)\nhas the potential to be used at different scales ranging from manufacturing\nassembly to construction.",
            "author": [
                "Seda Tuzun Canadinc",
                "Wei Yan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16337v2",
                "http://arxiv.org/pdf/2311.16337v2"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01521v1",
            "title": "Neural Markov Prolog",
            "updated": "2023-11-27T21:41:47Z",
            "published": "2023-11-27T21:41:47Z",
            "summary": "The recent rapid advance of AI has been driven largely by innovations in\nneural network architectures. A concomitant concern is how to understand these\nresulting systems. In this paper, we propose a tool to assist in both the\ndesign of further innovative architectures and the simple yet precise\ncommunication of their structure. We propose the language Neural Markov Prolog\n(NMP), based on both Markov logic and Prolog, as a means to both bridge first\norder logic and neural network design and to allow for the easy generation and\npresentation of architectures for images, text, relational databases, or other\ntarget data types or their mixtures.",
            "author": [
                "Alexander Thomson",
                "David Page"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01521v1",
                "http://arxiv.org/pdf/2312.01521v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.LG",
                "cs.PL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16334v2",
            "title": "Robust Basket Recommendation via Noise-tolerated Graph Contrastive\n  Learning",
            "updated": "2023-12-01T02:07:06Z",
            "published": "2023-11-27T21:38:10Z",
            "summary": "The growth of e-commerce has seen a surge in popularity of platforms like\nAmazon, eBay, and Taobao. This has given rise to a unique shopping behavior\ninvolving baskets - sets of items purchased together. As a less studied\ninteraction mode in the community, the question of how should shopping basket\ncomplement personalized recommendation systems remains under-explored. While\nprevious attempts focused on jointly modeling user purchases and baskets, the\ndistinct semantic nature of these elements can introduce noise when directly\nintegrated. This noise negatively impacts the model's performance, further\nexacerbated by significant noise (e.g., a user is misled to click an item or\nrecognizes it as uninteresting after consuming it) within both user and basket\nbehaviors. In order to cope with the above difficulties, we propose a novel\nBasket recommendation framework via Noise-tolerated Contrastive Learning, named\nBNCL, to handle the noise existing in the cross-behavior integration and\nwithin-behavior modeling. First, we represent the basket-item interactions as\nthe hypergraph to model the complex basket behavior, where all items appearing\nin the same basket are treated as a single hyperedge. Second, cross-behavior\ncontrastive learning is designed to suppress the noise during the fusion of\ndiverse behaviors. Next, to further inhibit the within-behavior noise of the\nuser and basket interactions, we propose to exploit invariant properties of the\nrecommenders w.r.t augmentations through within-behavior contrastive learning.\nA novel consistency-aware augmentation approach is further designed to better\nidentify noisy interactions with the consideration of the above two types of\ninteractions. Our framework BNCL offers a generic training paradigm that is\napplicable to different backbones. Extensive experiments on three shopping\ntransaction datasets verify the effectiveness of our proposed method.",
            "author": [
                "Xinrui He",
                "Tianxin Wei",
                "Jingrui He"
            ],
            "link": [
                "http://dx.doi.org/10.1145/3583780.3615039",
                "http://arxiv.org/abs/2311.16334v2",
                "http://arxiv.org/pdf/2311.16334v2"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16333v1",
            "title": "From Reactive to Proactive Volatility Modeling with Hemisphere Neural\n  Networks",
            "updated": "2023-11-27T21:37:50Z",
            "published": "2023-11-27T21:37:50Z",
            "summary": "We reinvigorate maximum likelihood estimation (MLE) for macroeconomic density\nforecasting through a novel neural network architecture with dedicated mean and\nvariance hemispheres. Our architecture features several key ingredients making\nMLE work in this context. First, the hemispheres share a common core at the\nentrance of the network which accommodates for various forms of time variation\nin the error variance. Second, we introduce a volatility emphasis constraint\nthat breaks mean/variance indeterminacy in this class of overparametrized\nnonlinear models. Third, we conduct a blocked out-of-bag reality check to curb\noverfitting in both conditional moments. Fourth, the algorithm utilizes\nstandard deep learning software and thus handles large data sets - both\ncomputationally and statistically. Ergo, our Hemisphere Neural Network (HNN)\nprovides proactive volatility forecasts based on leading indicators when it\ncan, and reactive volatility based on the magnitude of previous prediction\nerrors when it must. We evaluate point and density forecasts with an extensive\nout-of-sample experiment and benchmark against a suite of models ranging from\nclassics to more modern machine learning-based offerings. In all cases, HNN\nfares well by consistently providing accurate mean/variance forecasts for all\ntargets and horizons. Studying the resulting volatility paths reveals its\nversatility, while probabilistic forecasting evaluation metrics showcase its\nenviable reliability. Finally, we also demonstrate how this machinery can be\nmerged with other structured deep learning models by revisiting Goulet Coulombe\n(2022)'s Neural Phillips Curve.",
            "author": [
                "Philippe Goulet Coulombe",
                "Mikael Frenette",
                "Karin Klieber"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16333v1",
                "http://arxiv.org/pdf/2311.16333v1"
            ],
            "primary_category": "econ.EM",
            "category": [
                "econ.EM",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16328v1",
            "title": "Target-Free Compound Activity Prediction via Few-Shot Learning",
            "updated": "2023-11-27T21:23:41Z",
            "published": "2023-11-27T21:23:41Z",
            "summary": "Predicting the activities of compounds against protein-based or phenotypic\nassays using only a few known compounds and their activities is a common task\nin target-free drug discovery. Existing few-shot learning approaches are\nlimited to predicting binary labels (active/inactive). However, in real-world\ndrug discovery, degrees of compound activity are highly relevant. We study\nFew-Shot Compound Activity Prediction (FS-CAP) and design a novel neural\narchitecture to meta-learn continuous compound activities across large\nbioactivity datasets. Our model aggregates encodings generated from the known\ncompounds and their activities to capture assay information. We also introduce\na separate encoder for the unknown compound. We show that FS-CAP surpasses\ntraditional similarity-based techniques as well as other state of the art\nfew-shot learning methods on a variety of target-free drug discovery settings\nand datasets.",
            "author": [
                "Peter Eckmann",
                "Jake Anderson",
                "Michael K. Gilson",
                "Rose Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16328v1",
                "http://arxiv.org/pdf/2311.16328v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "q-bio.QM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16312v1",
            "title": "Domain-Specific Deep Learning Feature Extractor for Diabetic Foot Ulcer\n  Detection",
            "updated": "2023-11-27T21:01:29Z",
            "published": "2023-11-27T21:01:29Z",
            "summary": "Diabetic Foot Ulcer (DFU) is a condition requiring constant monitoring and\nevaluations for treatment. DFU patient population is on the rise and will soon\noutpace the available health resources. Autonomous monitoring and evaluation of\nDFU wounds is a much-needed area in health care. In this paper, we evaluate and\nidentify the most accurate feature extractor that is the core basis for\ndeveloping a deep-learning wound detection network. For the evaluation, we used\nmAP and F1-score on the publicly available DFU2020 dataset. A combination of\nUNet and EfficientNetb3 feature extractor resulted in the best evaluation among\nthe 14 networks compared. UNet and Efficientnetb3 can be used as the classifier\nin the development of a comprehensive DFU domain-specific autonomous wound\ndetection pipeline.",
            "author": [
                "Reza Basiri",
                "Milos R. Popovic",
                "Shehroz S. Khan"
            ],
            "link": [
                "http://dx.doi.org/10.1109/icdmw58026.2022.00041",
                "http://arxiv.org/abs/2311.16312v1",
                "http://arxiv.org/pdf/2311.16312v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16306v1",
            "title": "The search for the lost attractor",
            "updated": "2023-11-27T20:43:41Z",
            "published": "2023-11-27T20:43:41Z",
            "summary": "N-body systems characterized by inverse square attractive forces may display\na self similar collapse known as the gravo-thermal catastrophe. In star\nclusters, collapse is halted by binary stars, and a large fraction of Milky Way\nclusters may have already reached this phase. It has been speculated -- with\nguidance from simulations -- that macroscopic variables such as central density\nand velocity dispersion are governed post-collapse by an effective,\nlow-dimensional system of ODEs. It is still hard to distinguish chaotic, low\ndimensional motion, from high dimensional stochastic noise. Here we apply three\nmachine learning tools to state-of-the-art dynamical simulations to constrain\nthe post collapse dynamics: topological data analysis (TDA) on a lag embedding\nof the relevant time series, Sparse Identification of Nonlinear Dynamics\n(SINDY), and Tests of Accuracy with Random Points (TARP).",
            "author": [
                "Mario Pasquato",
                "Syphax Haddad",
                "Pierfrancesco Di Cintio",
                "Alexandre Adam",
                "Pablo Lemos",
                "No\u00e9 Dia",
                "Mircea Petrache",
                "Ugo Niccol\u00f2 Di Carlo",
                "Alessandro Alberto Trani",
                "Laurence Perreault-Levasseur",
                "Yashar Hezaveh"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16306v1",
                "http://arxiv.org/pdf/2311.16306v1"
            ],
            "primary_category": "astro-ph.GA",
            "category": [
                "astro-ph.GA",
                "nlin.CD"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16303v1",
            "title": "Root causes, ongoing difficulties, proactive prevention techniques, and\n  emerging trends of enterprise data breaches",
            "updated": "2023-11-27T20:34:10Z",
            "published": "2023-11-27T20:34:10Z",
            "summary": "A data breach in the modern digital era is the unintentional or intentional\ndisclosure of private data to uninvited parties. Businesses now consider data\nto be a crucial asset, and any breach of this data can have dire repercussions,\nincluding harming a company's brand and resulting in losses. Enterprises now\nplace a high premium on detecting and preventing data loss due to the growing\namount of data and the increasing frequency of data breaches. Even with a great\ndeal of research, protecting sensitive data is still a difficult task. This\nreview attempts to highlight interesting prospects and offer insightful\ninformation to those who are interested in learning about the risks that\nbusinesses face from data leaks, current occurrences, state-of-the-art methods\nfor detection and prevention, new difficulties, and possible solutions.",
            "author": [
                "Rina Patil",
                "Gayatri Pise",
                "Yatin Bhosale"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16303v1",
                "http://arxiv.org/pdf/2311.16303v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16302v1",
            "title": "Comprehensive Benchmarking of Entropy and Margin Based Scoring Metrics\n  for Data Selection",
            "updated": "2023-11-27T20:33:54Z",
            "published": "2023-11-27T20:33:54Z",
            "summary": "While data selection methods have been studied extensively in active\nlearning, data pruning, and data augmentation settings, there is little\nevidence for the efficacy of these methods in industry scale settings,\nparticularly in low-resource languages. Our work presents ways of assessing\nprospective training examples in those settings for their \"usefulness\" or\n\"difficulty\". We also demonstrate how these measures can be used in selecting\nimportant examples for training supervised machine learning models. We\nprimarily experiment with entropy and Error L2-Norm (EL2N) scores. We use these\nmetrics to curate high quality datasets from a large pool of \\textit{Weak\nSignal Labeled} data, which assigns no-defect high confidence hypotheses during\ninference as ground truth labels. We then conduct training data augmentation\nexperiments using these de-identified datasets and demonstrate that score-based\nselection can result in a 2% decrease in semantic error rate and 4%-7% decrease\nin domain classification error rate when compared to the baseline technique of\nrandom selection.",
            "author": [
                "Anusha Sabbineni",
                "Nikhil Anand",
                "Maria Minakova"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16302v1",
                "http://arxiv.org/pdf/2311.16302v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16298v1",
            "title": "Influence Scores at Scale for Efficient Language Data Sampling",
            "updated": "2023-11-27T20:19:22Z",
            "published": "2023-11-27T20:19:22Z",
            "summary": "Modern ML systems ingest data aggregated from diverse sources, such as\nsynthetic, human-annotated, and live customer traffic. Understanding\n\\textit{which} examples are important to the performance of a learning\nalgorithm is crucial for efficient model training. Recently, a growing body of\nliterature has given rise to various \"influence scores,\" which use training\nartifacts such as model confidence or checkpointed gradients to identify\nimportant subsets of data. However, these methods have primarily been developed\nin computer vision settings, and it remains unclear how well they generalize to\nlanguage-based tasks using pretrained models.\n  In this paper, we explore the applicability of influence scores in language\nclassification tasks. We evaluate a diverse subset of these scores on the SNLI\ndataset by quantifying accuracy changes in response to pruning training data\nthrough random and influence-score-based sampling. We then stress-test one of\nthe scores -- \"variance of gradients\" (VoG) from Agarwal et al. (2022) -- in an\nNLU model stack that was exposed to dynamic user speech patterns in a voice\nassistant type of setting. Our experiments demonstrate that in many cases,\nencoder-based language models can be finetuned on roughly 50% of the original\ndata without degradation in performance metrics. Along the way, we summarize\nlessons learned from applying out-of-the-box implementations of influence\nscores, quantify the effects of noisy and class-imbalanced data, and offer\nrecommendations on score-based sampling for better accuracy and training\nefficiency.",
            "author": [
                "Nikhil Anand",
                "Joshua Tan",
                "Maria Minakova"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16298v1",
                "http://arxiv.org/pdf/2311.16298v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16297v1",
            "title": "Quantum-classical simulation of quantum field theory by quantum circuit\n  learning",
            "updated": "2023-11-27T20:18:39Z",
            "published": "2023-11-27T20:18:39Z",
            "summary": "We employ quantum circuit learning to simulate quantum field theories (QFTs).\nTypically, when simulating QFTs with quantum computers, we encounter\nsignificant challenges due to the technical limitations of quantum devices when\nimplementing the Hamiltonian using Pauli spin matrices. To address this\nchallenge, we leverage quantum circuit learning, employing a compact\nconfiguration of qubits and low-depth quantum circuits to predict real-time\ndynamics in quantum field theories. The key advantage of this approach is that\na single-qubit measurement can accurately forecast various physical parameters,\nincluding fully-connected operators. To demonstrate the effectiveness of our\nmethod, we use it to predict quench dynamics, chiral dynamics and jet\nproduction in a 1+1-dimensional model of quantum electrodynamics. We find that\nour predictions closely align with the results of rigorous classical\ncalculations, exhibiting a high degree of accuracy. This hybrid\nquantum-classical approach illustrates the feasibility of efficiently\nsimulating large-scale QFTs on cutting-edge quantum devices.",
            "author": [
                "Kazuki Ikeda"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16297v1",
                "http://arxiv.org/pdf/2311.16297v1"
            ],
            "primary_category": "hep-th",
            "category": [
                "hep-th",
                "cs.LG",
                "hep-ph",
                "nucl-th",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16295v1",
            "title": "Testing Assumptions Underlying a Unified Theory for the Origin of Grid\n  Cells",
            "updated": "2023-11-27T20:14:11Z",
            "published": "2023-11-27T20:14:11Z",
            "summary": "Representing and reasoning about physical space is fundamental to animal\nsurvival, and the mammalian lineage expresses a wealth of specialized neural\nrepresentations that encode space. Grid cells, whose discovery earned a Nobel\nprize, are a striking example: a grid cell is a neuron that fires if and only\nif the animal is spatially located at the vertices of a regular triangular\nlattice that tiles all explored two-dimensional environments. Significant\ntheoretical work has gone into understanding why mammals have learned these\nparticular representations, and recent work has proposed a ``unified theory for\nthe computational and mechanistic origin of grid cells,\" claiming to answer why\nthe mammalian lineage has learned grid cells. However, the Unified Theory makes\na series of highly specific assumptions about the target readouts of grid cells\n- putatively place cells. In this work, we explicitly identify what these\nmathematical assumptions are, then test two of the critical assumptions using\nbiological place cell data. At both the population and single-cell levels, we\nfind evidence suggesting that neither of the assumptions are likely true in\nbiological neural representations. These results call the Unified Theory into\nquestion, suggesting that biological grid cells likely have a different origin\nthan those obtained in trained artificial neural networks.",
            "author": [
                "Rylan Schaeffer",
                "Mikail Khona",
                "Adrian Bertagnoli",
                "Sanmi Koyejo",
                "Ila Rani Fiete"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16295v1",
                "http://arxiv.org/pdf/2311.16295v1"
            ],
            "primary_category": "q-bio.NC",
            "category": [
                "q-bio.NC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16292v1",
            "title": "Student Mastery or AI Deception? Analyzing ChatGPT's Assessment\n  Proficiency and Evaluating Detection Strategies",
            "updated": "2023-11-27T20:10:13Z",
            "published": "2023-11-27T20:10:13Z",
            "summary": "Generative AI systems such as ChatGPT have a disruptive effect on learning\nand assessment. Computer science requires practice to develop skills in problem\nsolving and programming that are traditionally developed using assignments.\nGenerative AI has the capability of completing these assignments for students\nwith high accuracy, which dramatically increases the potential for academic\nintegrity issues and students not achieving desired learning outcomes. This\nwork investigates the performance of ChatGPT by evaluating it across three\ncourses (CS1,CS2,databases). ChatGPT completes almost all introductory\nassessments perfectly. Existing detection methods, such as MOSS and JPlag\n(based on similarity metrics) and GPTzero (AI detection), have mixed success in\nidentifying AI solutions. Evaluating instructors and teaching assistants using\nheuristics to distinguish between student and AI code shows that their\ndetection is not sufficiently accurate. These observations emphasize the need\nfor adapting assessments and improved detection methods.",
            "author": [
                "Kevin Wang",
                "Seth Akins",
                "Abdallah Mohammed",
                "Ramon Lawrence"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16292v1",
                "http://arxiv.org/pdf/2311.16292v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16286v1",
            "title": "A statistical approach to latent dynamic modeling with differential\n  equations",
            "updated": "2023-11-27T20:02:55Z",
            "published": "2023-11-27T20:02:55Z",
            "summary": "Ordinary differential equations (ODEs) can provide mechanistic models of\ntemporally local changes of processes, where parameters are often informed by\nexternal knowledge. While ODEs are popular in systems modeling, they are less\nestablished for statistical modeling of longitudinal cohort data, e.g., in a\nclinical setting. Yet, modeling of local changes could also be attractive for\nassessing the trajectory of an individual in a cohort in the immediate future\ngiven its current status, where ODE parameters could be informed by further\ncharacteristics of the individual. However, several hurdles so far limit such\nuse of ODEs, as compared to regression-based function fitting approaches. The\npotentially higher level of noise in cohort data might be detrimental to ODEs,\nas the shape of the ODE solution heavily depends on the initial value. In\naddition, larger numbers of variables multiply such problems and might be\ndifficult to handle for ODEs. To address this, we propose to use each\nobservation in the course of time as the initial value to obtain multiple local\nODE solutions and build a combined estimator of the underlying dynamics. Neural\nnetworks are used for obtaining a low-dimensional latent space for dynamic\nmodeling from a potentially large number of variables, and for obtaining\npatient-specific ODE parameters from baseline variables. Simultaneous\nidentification of dynamic models and of a latent space is enabled by recently\ndeveloped differentiable programming techniques. We illustrate the proposed\napproach in an application with spinal muscular atrophy patients and a\ncorresponding simulation study. In particular, modeling of local changes in\nhealth status at any point in time is contrasted to the interpretation of\nfunctions obtained from a global regression. This more generally highlights how\ndifferent application settings might demand different modeling strategies.",
            "author": [
                "Maren Hackenberg",
                "Astrid Pechmann",
                "Clemens Kreutz",
                "Janbernd Kirschner",
                "Harald Binder"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16286v1",
                "http://arxiv.org/pdf/2311.16286v1"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME",
                "cs.LG",
                "stat.AP",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16284v1",
            "title": "Simultaneous Energy Harvesting and Hand Gesture Recognition in Large\n  Area Monolithic Dye-Sensitized Solar Cells",
            "updated": "2023-11-27T19:57:38Z",
            "published": "2023-11-27T19:57:38Z",
            "summary": "Internet of Things (IoT) devices have become prevalent, embedding\nintelligence into our environment. It is projected that over 75 billion IoT\ndevices will be connected by 2025 worldwide, with the majority being operated\nindoors. Dye-sensitized solar cells (DSSC) have recently been optimized for\nambient light, having the capabilities of providing sufficient energy for\nself-powered IoT devices. Interaction with digital technologies, termed Human\nComputer Interaction (HCI), is often achieved via physical mechanisms (e.g.\nremote controls, cell phones) which can hinder the natural interface between\nusers and IoT devices, a key consideration for HCI. What if the solar cell that\nis powering the IoT device can also recognize hand gestures which would allow\nthe user to naturally interact with the system? Previous attempts to achieve\nthis have necessarily employed an array of solar cell/photodiodes to detect\ndirectionality. In this work, we demonstrate that by monitoring the\nphotocurrent output of an asymmetrically patterned monolithic (i.e., single\ncell) DSSC, and using machine learning, we can recognize simple hand gestures,\nachieving an accuracy prediction of 97.71%. This work shows that, DSSCs are the\nperfect choice for self-powered interactive technologies, both in terms of\npowering IoT devices in ambient light conditions and having aesthetic qualities\nthat are prioritized by users. As well as powering interactive technologies,\nthey can also provide a means of interactive control.",
            "author": [
                "Gethin Thomas",
                "Adam Pockett",
                "Kris Seunarine",
                "Matt Carnie"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16284v1",
                "http://arxiv.org/pdf/2311.16284v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci",
                "cs.HC",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16278v1",
            "title": "VehicleGAN: Pair-flexible Pose Guided Image Synthesis for Vehicle\n  Re-identification",
            "updated": "2023-11-27T19:34:04Z",
            "published": "2023-11-27T19:34:04Z",
            "summary": "Vehicle Re-identification (Re-ID) has been broadly studied in the last\ndecade; however, the different camera view angle leading to confused\ndiscrimination in the feature subspace for the vehicles of various poses, is\nstill challenging for the Vehicle Re-ID models in the real world. To promote\nthe Vehicle Re-ID models, this paper proposes to synthesize a large number of\nvehicle images in the target pose, whose idea is to project the vehicles of\ndiverse poses into the unified target pose so as to enhance feature\ndiscrimination. Considering that the paired data of the same vehicles in\ndifferent traffic surveillance cameras might be not available in the real\nworld, we propose the first Pair-flexible Pose Guided Image Synthesis method\nfor Vehicle Re-ID, named as VehicleGAN in this paper, which works for both\nsupervised and unsupervised settings without the knowledge of geometric 3D\nmodels. Because of the feature distribution difference between real and\nsynthetic data, simply training a traditional metric learning based Re-ID model\nwith data-level fusion (i.e., data augmentation) is not satisfactory, therefore\nwe propose a new Joint Metric Learning (JML) via effective feature-level fusion\nfrom both real and synthetic data. Intensive experimental results on the public\nVeRi-776 and VehicleID datasets prove the accuracy and effectiveness of our\nproposed VehicleGAN and JML.",
            "author": [
                "Baolu Li",
                "Ping Liu",
                "Lan Fu",
                "Jinlong Li",
                "Jianwu Fang",
                "Zhigang Xu",
                "Hongkai Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16278v1",
                "http://arxiv.org/pdf/2311.16278v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16277v1",
            "title": "A Graph Neural Network-Based QUBO-Formulated Hamiltonian-Inspired Loss\n  Function for Combinatorial Optimization using Reinforcement Learning",
            "updated": "2023-11-27T19:33:14Z",
            "published": "2023-11-27T19:33:14Z",
            "summary": "Quadratic Unconstrained Binary Optimization (QUBO) is a generic technique to\nmodel various NP-hard Combinatorial Optimization problems (CO) in the form of\nbinary variables. Ising Hamiltonian is used to model the energy function of a\nsystem. QUBO to Ising Hamiltonian is regarded as a technique to solve various\ncanonical optimization problems through quantum optimization algorithms.\nRecently, PI-GNN, a generic framework, has been proposed to address CO problems\nover graphs based on Graph Neural Network (GNN) architecture. They introduced a\ngeneric QUBO-formulated Hamiltonian-inspired loss function that was directly\noptimized using GNN. PI-GNN is highly scalable but there lies a noticeable\ndecrease in the number of satisfied constraints when compared to\nproblem-specific algorithms and becomes more pronounced with increased graph\ndensities. Here, We identify a behavioral pattern related to it and devise\nstrategies to improve its performance. Another group of literature uses\nReinforcement learning (RL) to solve the aforementioned NP-hard problems using\nproblem-specific reward functions. In this work, we also focus on creating a\nbridge between the RL-based solutions and the QUBO-formulated Hamiltonian. We\nformulate and empirically evaluate the compatibility of the QUBO-formulated\nHamiltonian as the generic reward function in the RL-based paradigm in the form\nof rewards. Furthermore, we also introduce a novel Monty Carlo Tree\nSearch-based strategy with GNN where we apply a guided search through manual\nperturbation of node labels during training. We empirically evaluated our\nmethods and observed up to 44% improvement in the number of constraint\nviolations compared to the PI-GNN.",
            "author": [
                "Redwan Ahmed Rizvee",
                "Raheeb Hasan",
                "Md. Mosaddek Khan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16277v1",
                "http://arxiv.org/pdf/2311.16277v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17074v2",
            "title": "Self-Supervised Learning of Whole and Component-Based Semantic\n  Representations for Person Re-Identification",
            "updated": "2023-12-01T20:43:29Z",
            "published": "2023-11-27T19:30:30Z",
            "summary": "Interactive Segmentation Models (ISMs) like the Segment Anything Model have\nsignificantly improved various computer vision tasks, yet their application to\nPerson Re-identification (ReID) remains limited. On the other hand, existing\nsemantic pre-training models for ReID often have limitations like predefined\nparsing ranges or coarse semantics. Additionally, ReID and Clothes-Changing\nReID (CC-ReID) are usually treated separately due to their different domains.\nThis paper investigates whether utilizing precise human-centric semantic\nrepresentation can boost the ReID performance and improve the generalization\namong various ReID tasks. We propose SemReID, a self-supervised ReID model that\nleverages ISMs for adaptive part-based semantic extraction, contributing to the\nimprovement of ReID performance. SemReID additionally refines its semantic\nrepresentation through techniques such as image masking and KoLeo\nregularization. Evaluation across three types of ReID datasets -- standard\nReID, CC-ReID, and unconstrained ReID -- demonstrates superior performance\ncompared to state-of-the-art methods. In addition, recognizing the scarcity of\nlarge person datasets with fine-grained semantics, we introduce the novel\nLUPerson-Part dataset to assist ReID methods in acquiring the fine-grained part\nsemantics for robust performance.",
            "author": [
                "Siyuan Huang",
                "Yifan Zhou",
                "Ram Prabhakar Kathirvel",
                "Rama Chellappa",
                "Chun Pong Lau"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17074v2",
                "http://arxiv.org/pdf/2311.17074v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16276v1",
            "title": "Darknet Traffic Analysis A Systematic Literature Review",
            "updated": "2023-11-27T19:27:50Z",
            "published": "2023-11-27T19:27:50Z",
            "summary": "The primary objective of an anonymity tool is to protect the anonymity of its\nusers through the implementation of strong encryption and obfuscation\ntechniques. As a result, it becomes very difficult to monitor and identify\nusers activities on these networks. Moreover, such systems have strong\ndefensive mechanisms to protect users against potential risks, including the\nextraction of traffic characteristics and website fingerprinting. However, the\nstrong anonymity feature also functions as a refuge for those involved in\nillicit activities who aim to avoid being traced on the network. As a result, a\nsubstantial body of research has been undertaken to examine and classify\nencrypted traffic using machine learning techniques. This paper presents a\ncomprehensive examination of the existing approaches utilized for the\ncategorization of anonymous traffic as well as encrypted network traffic inside\nthe darknet. Also, this paper presents a comprehensive analysis of methods of\ndarknet traffic using machine learning techniques to monitor and identify the\ntraffic attacks inside the darknet.",
            "author": [
                "Javeriah Saleem",
                "Rafiqul Islam",
                "Zahidul Islam"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16276v1",
                "http://arxiv.org/pdf/2311.16276v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16272v1",
            "title": "Optimal Observer Design Using Reinforcement Learning and Quadratic\n  Neural Networks",
            "updated": "2023-11-27T19:21:41Z",
            "published": "2023-11-27T19:21:41Z",
            "summary": "This paper introduces an innovative approach based on policy iteration (PI),\na reinforcement learning (RL) algorithm, to obtain an optimal observer with a\nquadratic cost function. This observer is designed for systems with a given\nlinearized model and a stabilizing Luenberger observer gain. We utilize\ntwo-layer quadratic neural networks (QNN) for policy evaluation and derive a\nlinear correction term using the input and output data. This correction term\neffectively rectifies inaccuracies introduced by the linearized model employed\nwithin the observer design. A unique feature of the proposed methodology is\nthat the QNN is trained through convex optimization. The main advantage is that\nthe QNN's input-output mapping has an analytical expression as a quadratic\nform, which can then be used to obtain a linear correction term policy. This is\nin stark contrast to the available techniques in the literature that must train\na second neural network to obtain policy improvement. It is proven that the\nobtained linear correction term is optimal for linear systems, as both the\nvalue function and the QNN's input-output mapping are quadratic. The proposed\nmethod is applied to a simple pendulum, demonstrating an enhanced correction\nterm policy compared to relying solely on the linearized model. This shows its\npromise for addressing nonlinear systems.",
            "author": [
                "Soroush Asri",
                "Luis Rodrigues"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16272v1",
                "http://arxiv.org/pdf/2311.16272v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16267v1",
            "title": "Applications of Large Language Models in Data Processing: Innovative\n  Approaches to Segmenting and Renewing Information",
            "updated": "2023-11-27T19:17:39Z",
            "published": "2023-11-27T19:17:39Z",
            "summary": "Our paper investigates effective methods for code generation in\n\"specific-domain\" applications, including the use of Large Language Models\n(LLMs) for data segmentation and renewal, as well as stimulating deeper\nthinking in LLMs through prompt adjustments. Using a real company product as an\nexample, we provide user manuals, API documentation, and other data. The ideas\ndiscussed in this paper help segment and then convert this data into semantic\nvectors to better reflect their true positioning. Subsequently, user\nrequirements are transformed into vectors to retrieve the most relevant\ncontent, achieving about 70% accuracy in simple to medium-complexity tasks\nthrough various prompt techniques. This paper is the first to enhance\nspecific-domain code generation effectiveness from this perspective.\nAdditionally, we experiment with generating more scripts from a limited number\nusing llama2-based fine-tuning to test its effectiveness in professional domain\ncode generation. This is a challenging and promising field, and once achieved,\nit will not only lead to breakthroughs in LLM development across multiple\nindustries but also enable LLMs to understand and learn any new knowledge\neffectively.",
            "author": [
                "Yu-Chen Lin",
                "Akhilesh Kumar",
                "Wen-Liang Zhang",
                "Norman Chang",
                "Muhammad Zakir",
                "Rucha Apte",
                "Chao Wang",
                "Jyh-Shing Roger Jang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16267v1",
                "http://arxiv.org/pdf/2311.16267v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17073v1",
            "title": "Practical Layout-Aware Analog/Mixed-Signal Design Automation with\n  Bayesian Neural Networks",
            "updated": "2023-11-27T19:02:43Z",
            "published": "2023-11-27T19:02:43Z",
            "summary": "The high simulation cost has been a bottleneck of practical\nanalog/mixed-signal design automation. Many learning-based algorithms require\nthousands of simulated data points, which is impractical for expensive to\nsimulate circuits. We propose a learning-based algorithm that can be trained\nusing a small amount of data and, therefore, scalable to tasks with expensive\nsimulations. Our efficient algorithm solves the post-layout performance\noptimization problem where simulations are known to be expensive. Our\ncomprehensive study also solves the schematic-level sizing problem. For\nefficient optimization, we utilize Bayesian Neural Networks as a regression\nmodel to approximate circuit performance. For layout-aware optimization, we\nhandle the problem as a multi-fidelity optimization problem and improve\nefficiency by exploiting the correlations from cheaper evaluations. We present\nthree test cases to demonstrate the efficiency of our algorithms. Our tests\nprove that the proposed approach is more efficient than conventional baselines\nand state-of-the-art algorithms.",
            "author": [
                "Ahmet F. Budak",
                "Keren Zhu",
                "David Z. Pan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17073v1",
                "http://arxiv.org/pdf/2311.17073v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CE",
                "cs.SY",
                "eess.SY",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16241v1",
            "title": "SemiVL: Semi-Supervised Semantic Segmentation with Vision-Language\n  Guidance",
            "updated": "2023-11-27T19:00:06Z",
            "published": "2023-11-27T19:00:06Z",
            "summary": "In semi-supervised semantic segmentation, a model is trained with a limited\nnumber of labeled images along with a large corpus of unlabeled images to\nreduce the high annotation effort. While previous methods are able to learn\ngood segmentation boundaries, they are prone to confuse classes with similar\nvisual appearance due to the limited supervision. On the other hand,\nvision-language models (VLMs) are able to learn diverse semantic knowledge from\nimage-caption datasets but produce noisy segmentation due to the image-level\ntraining. In SemiVL, we propose to integrate rich priors from VLM pre-training\ninto semi-supervised semantic segmentation to learn better semantic decision\nboundaries. To adapt the VLM from global to local reasoning, we introduce a\nspatial fine-tuning strategy for label-efficient learning. Further, we design a\nlanguage-guided decoder to jointly reason over vision and language. Finally, we\npropose to handle inherent ambiguities in class labels by providing the model\nwith language guidance in the form of class definitions. We evaluate SemiVL on\n4 semantic segmentation datasets, where it significantly outperforms previous\nsemi-supervised methods. For instance, SemiVL improves the state-of-the-art by\n+13.5 mIoU on COCO with 232 annotated images and by +6.1 mIoU on Pascal VOC\nwith 92 labels. Project page: https://github.com/google-research/semivl",
            "author": [
                "Lukas Hoyer",
                "David Joseph Tan",
                "Muhammad Ferjad Naeem",
                "Luc Van Gool",
                "Federico Tombari"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16241v1",
                "http://arxiv.org/pdf/2311.16241v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17072v1",
            "title": "IG Captioner: Information Gain Captioners are Strong Zero-shot\n  Classifiers",
            "updated": "2023-11-27T19:00:06Z",
            "published": "2023-11-27T19:00:06Z",
            "summary": "Generative training has been demonstrated to be powerful for building\nvisual-language models. However, on zero-shot discriminative benchmarks, there\nis still a performance gap between models trained with generative and\ndiscriminative objectives. In this paper, we aim to narrow this gap by\nimproving the efficacy of generative training on classification tasks, without\nany finetuning processes or additional modules.\n  Specifically, we focus on narrowing the gap between the generative captioner\nand the CLIP classifier. We begin by analysing the predictions made by the\ncaptioner and classifier and observe that the caption generation inherits the\ndistribution bias from the language model trained with pure text modality,\nmaking it less grounded on the visual signal. To tackle this problem, we\nredesign the scoring objective for the captioner to alleviate the\ndistributional bias and focus on measuring the gain of information brought by\nthe visual inputs. We further design a generative training objective to match\nthe evaluation objective. We name our model trained and evaluated from the\nnovel procedures as Information Gain (IG) captioner. We pretrain the models on\nthe public Laion-5B dataset and perform a series of discriminative evaluations.\nFor the zero-shot classification on ImageNet, IG captioner achieves $> 18\\%$\nimprovements over the standard captioner, achieving comparable performances\nwith the CLIP classifier. IG captioner also demonstrated strong performance on\nzero-shot image-text retrieval tasks on MSCOCO and Flickr30K. We hope this\npaper inspires further research towards unifying generative and discriminative\ntraining procedures for visual-language models.",
            "author": [
                "Chenglin Yang",
                "Siyuan Qiao",
                "Yuan Cao",
                "Yu Zhang",
                "Tao Zhu",
                "Alan Yuille",
                "Jiahui Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17072v1",
                "http://arxiv.org/pdf/2311.17072v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG",
                "cs.MM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16238v1",
            "title": "Learning Reionization History from Quasars with Simulation-Based\n  Inference",
            "updated": "2023-11-27T19:00:04Z",
            "published": "2023-11-27T19:00:04Z",
            "summary": "Understanding the entire history of the ionization state of the intergalactic\nmedium (IGM) is at the frontier of astrophysics and cosmology. A promising\nmethod to achieve this is by extracting the damping wing signal from the\nneutral IGM. As hundreds of redshift $z>6$ quasars are observed, we anticipate\ndetermining the detailed time evolution of the ionization fraction with\nunprecedented fidelity. However, traditional approaches to parameter inference\nare not sufficiently accurate. We assess the performance of a simulation-based\ninference (SBI) method to infer the neutral fraction of the universe from\nquasar spectra. The SBI method adeptly exploits the shape information of the\ndamping wing, enabling precise estimations of the neutral fraction\n$\\left<x_{\\rm HI}\\right>_{\\rm v}$ and the wing position $w_p$. Importantly, the\nSBI framework successfully breaks the degeneracy between these two parameters,\noffering unbiased estimates of both. This makes the SBI superior to the\ntraditional method using a pseudo-likelihood function. We anticipate that SBI\nwill be essential to determine robustly the ionization history of the Universe\nthrough joint inference from the hundreds of high-$z$ spectra we will observe.",
            "author": [
                "Huanqing Chen",
                "Joshua Speagle",
                "Keir K. Rogers"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16238v1",
                "http://arxiv.org/pdf/2311.16238v1"
            ],
            "primary_category": "astro-ph.CO",
            "category": [
                "astro-ph.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16227v1",
            "title": "A proof-of-concept neural network for inferring parameters of a black\n  hole from partial interferometric images of its shadow",
            "updated": "2023-11-27T19:00:01Z",
            "published": "2023-11-27T19:00:01Z",
            "summary": "We test the possibility of using a convolutional neural network to infer the\ninclination angle of a black hole directly from the incomplete image of the\nblack hole's shadow in the $uv$-plane. To this end, we develop a\nproof-of-concept network and use it to explicitly find how the error depends on\nthe degree of coverage, type of input and coverage pattern. We arrive at a\ntypical error of $10^\\circ$ at a level of absolute coverage $1\\%$ (for a\npattern covering a central part of the $uv$-plane), $0.3\\%$ (pattern covering\nthe central part and the periphery, the $0.3\\%$ referring to the central part\nonly), and $14\\%$ (uniform pattern). These numbers refer to a network that\ntakes both amplitude and phase of the visibility function as inputs. We find\nthat this type of network works best in terms of the error itself and its\ndistribution for different angles. In addition, the same type of network\ndemonstrates similarly good performance on highly blurred images mimicking\nsources nearing being unresolved. In terms of coverage, the magnitude of the\nerror does not change much as one goes from the central pattern to the uniform\none. We argue that this may be due to the presence of a typical scale which can\nbe mostly learned by the network from the central part alone.",
            "author": [
                "Anton A. Popov",
                "Vladimir Strokov",
                "Aleksey A. Surdyaev"
            ],
            "link": [
                "http://dx.doi.org/10.1016/j.ascom.2021.100467",
                "http://arxiv.org/abs/2311.16227v1",
                "http://arxiv.org/pdf/2311.16227v1"
            ],
            "primary_category": "astro-ph.HE",
            "category": [
                "astro-ph.HE",
                "astro-ph.IM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16102v2",
            "title": "Diffusion-TTA: Test-time Adaptation of Discriminative Models via\n  Generative Feedback",
            "updated": "2023-11-29T20:12:28Z",
            "published": "2023-11-27T18:59:53Z",
            "summary": "The advancements in generative modeling, particularly the advent of diffusion\nmodels, have sparked a fundamental question: how can these models be\neffectively used for discriminative tasks? In this work, we find that\ngenerative models can be great test-time adapters for discriminative models.\nOur method, Diffusion-TTA, adapts pre-trained discriminative models such as\nimage classifiers, segmenters and depth predictors, to each unlabelled example\nin the test set using generative feedback from a diffusion model. We achieve\nthis by modulating the conditioning of the diffusion model using the output of\nthe discriminative model. We then maximize the image likelihood objective by\nbackpropagating the gradients to discriminative model's parameters. We show\nDiffusion-TTA significantly enhances the accuracy of various large-scale\npre-trained discriminative models, such as, ImageNet classifiers, CLIP models,\nimage pixel labellers and image depth predictors. Diffusion-TTA outperforms\nexisting test-time adaptation methods, including TTT-MAE and TENT, and\nparticularly shines in online adaptation setups, where the discriminative model\nis continually adapted to each example in the test set. We provide access to\ncode, results, and visualizations on our website:\nhttps://diffusion-tta.github.io/.",
            "author": [
                "Mihir Prabhudesai",
                "Tsung-Wei Ke",
                "Alexander C. Li",
                "Deepak Pathak",
                "Katerina Fragkiadaki"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16102v2",
                "http://arxiv.org/pdf/2311.16102v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16101v1",
            "title": "How Many Unicorns Are in This Image? A Safety Evaluation Benchmark for\n  Vision LLMs",
            "updated": "2023-11-27T18:59:42Z",
            "published": "2023-11-27T18:59:42Z",
            "summary": "This work focuses on the potential of Vision LLMs (VLLMs) in visual\nreasoning. Different from prior studies, we shift our focus from evaluating\nstandard performance to introducing a comprehensive safety evaluation suite,\ncovering both out-of-distribution (OOD) generalization and adversarial\nrobustness. For the OOD evaluation, we present two novel VQA datasets, each\nwith one variant, designed to test model performance under challenging\nconditions. In exploring adversarial robustness, we propose a straightforward\nattack strategy for misleading VLLMs to produce visual-unrelated responses.\nMoreover, we assess the efficacy of two jailbreaking strategies, targeting\neither the vision or language component of VLLMs. Our evaluation of 21 diverse\nmodels, ranging from open-source VLLMs to GPT-4V, yields interesting\nobservations: 1) Current VLLMs struggle with OOD texts but not images, unless\nthe visual information is limited; and 2) These VLLMs can be easily misled by\ndeceiving vision encoders only, and their vision-language training often\ncompromise safety protocols. We release this safety evaluation suite at\nhttps://github.com/UCSC-VLAA/vllm-safety-benchmark.",
            "author": [
                "Haoqin Tu",
                "Chenhang Cui",
                "Zijun Wang",
                "Yiyang Zhou",
                "Bingchen Zhao",
                "Junlin Han",
                "Wangchunshu Zhou",
                "Huaxiu Yao",
                "Cihang Xie"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16101v1",
                "http://arxiv.org/pdf/2311.16101v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16098v1",
            "title": "On Bringing Robots Home",
            "updated": "2023-11-27T18:59:25Z",
            "published": "2023-11-27T18:59:25Z",
            "summary": "Throughout history, we have successfully integrated various machines into our\nhomes. Dishwashers, laundry machines, stand mixers, and robot vacuums are a few\nrecent examples. However, these machines excel at performing only a single task\neffectively. The concept of a \"generalist machine\" in homes - a domestic\nassistant that can adapt and learn from our needs, all while remaining\ncost-effective - has long been a goal in robotics that has been steadily\npursued for decades. In this work, we initiate a large-scale effort towards\nthis goal by introducing Dobb-E, an affordable yet versatile general-purpose\nsystem for learning robotic manipulation within household settings. Dobb-E can\nlearn a new task with only five minutes of a user showing it how to do it,\nthanks to a demonstration collection tool (\"The Stick\") we built out of cheap\nparts and iPhones. We use the Stick to collect 13 hours of data in 22 homes of\nNew York City, and train Home Pretrained Representations (HPR). Then, in a\nnovel home environment, with five minutes of demonstrations and fifteen minutes\nof adapting the HPR model, we show that Dobb-E can reliably solve the task on\nthe Stretch, a mobile robot readily available on the market. Across roughly 30\ndays of experimentation in homes of New York City and surrounding areas, we\ntest our system in 10 homes, with a total of 109 tasks in different\nenvironments, and finally achieve a success rate of 81%. Beyond success\npercentages, our experiments reveal a plethora of unique challenges absent or\nignored in lab robotics. These range from effects of strong shadows, to\nvariable demonstration quality by non-expert users. With the hope of\naccelerating research on home robots, and eventually seeing robot butlers in\nevery home, we open-source Dobb-E software stack and models, our data, and our\nhardware designs at https://dobb-e.com",
            "author": [
                "Nur Muhammad Mahi Shafiullah",
                "Anant Rai",
                "Haritheja Etukuru",
                "Yiqian Liu",
                "Ishan Misra",
                "Soumith Chintala",
                "Lerrel Pinto"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16098v1",
                "http://arxiv.org/pdf/2311.16098v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16097v1",
            "title": "CG-HOI: Contact-Guided 3D Human-Object Interaction Generation",
            "updated": "2023-11-27T18:59:10Z",
            "published": "2023-11-27T18:59:10Z",
            "summary": "We propose CG-HOI, the first method to address the task of generating dynamic\n3D human-object interactions (HOIs) from text. We model the motion of both\nhuman and object in an interdependent fashion, as semantically rich human\nmotion rarely happens in isolation without any interactions. Our key insight is\nthat explicitly modeling contact between the human body surface and object\ngeometry can be used as strong proxy guidance, both during training and\ninference. Using this guidance to bridge human and object motion enables\ngenerating more realistic and physically plausible interaction sequences, where\nthe human body and corresponding object move in a coherent manner. Our method\nfirst learns to model human motion, object motion, and contact in a joint\ndiffusion process, inter-correlated through cross-attention. We then leverage\nthis learned contact for guidance during inference synthesis of realistic,\ncoherent HOIs. Extensive evaluation shows that our joint contact-based\nhuman-object interaction approach generates realistic and physically plausible\nsequences, and we show two applications highlighting the capabilities of our\nmethod. Conditioned on a given object trajectory, we can generate the\ncorresponding human motion without re-training, demonstrating strong\nhuman-object interdependency learning. Our approach is also flexible, and can\nbe applied to static real-world 3D scene scans.",
            "author": [
                "Christian Diller",
                "Angela Dai"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16097v1",
                "http://arxiv.org/pdf/2311.16097v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "I.2.10; I.4.8; I.5.1; I.5.4"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16095v1",
            "title": "KPZ-type equation from growth driven by a non-Markovian diffusion",
            "updated": "2023-11-27T18:59:04Z",
            "published": "2023-11-27T18:59:04Z",
            "summary": "We study a stochastic geometric flow that describes a growing submanifold\n$\\mathbb{M}(t)\\subseteq\\mathbb{R}^{\\mathrm{d}+1}$. It is an SPDE that comes\nfrom a continuum version of origin-excited random walk or once-reinforced\nrandom walk. It is given by simultaneously smoothing and inflating the boundary\nof $\\\\mathbb{M}(t)$ in a neighborhood of the boundary trace of a reflecting\nBrownian motion. We show that the large-scale fluctuations of an associated\nheight function are given by a regularized Kardar-Parisi-Zhang (KPZ)-type\nequation on a manifold in $\\mathbb{R}^{\\mathrm{d}+1}$, modulated by a\nDirichlet-to-Neumann operator. This is shown in any dimension\n$\\mathrm{d}\\geq1$. We also prove that in dimension $\\mathrm{d}+1=2$, the\nregularization in this KPZ-type SPDE can be removed after renormalization.\nThus, in dimension $\\mathrm{d}+1=2$, fluctuations of the geometric flow have a\ndouble-scaling limit given by a singular KPZ-type equation. To our knowledge,\nthis is the first instance of KPZ-type behavior in stochastic Laplacian growth\nmodels, which was asked about (for somewhat different models) in Parisi-Zhang\n'84 and Ramirez-Sidoravicius '04.",
            "author": [
                "Amir Dembo",
                "Kevin Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16095v1",
                "http://arxiv.org/pdf/2311.16095v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR",
                "82C24, 60H15, 58J65, 35R60"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16096v1",
            "title": "Animatable Gaussians: Learning Pose-dependent Gaussian Maps for\n  High-fidelity Human Avatar Modeling",
            "updated": "2023-11-27T18:59:04Z",
            "published": "2023-11-27T18:59:04Z",
            "summary": "Modeling animatable human avatars from RGB videos is a long-standing and\nchallenging problem. Recent works usually adopt MLP-based neural radiance\nfields (NeRF) to represent 3D humans, but it remains difficult for pure MLPs to\nregress pose-dependent garment details. To this end, we introduce Animatable\nGaussians, a new avatar representation that leverages powerful 2D CNNs and 3D\nGaussian splatting to create high-fidelity avatars. To associate 3D Gaussians\nwith the animatable avatar, we learn a parametric template from the input\nvideos, and then parameterize the template on two front \\& back canonical\nGaussian maps where each pixel represents a 3D Gaussian. The learned template\nis adaptive to the wearing garments for modeling looser clothes like dresses.\nSuch template-guided 2D parameterization enables us to employ a powerful\nStyleGAN-based CNN to learn the pose-dependent Gaussian maps for modeling\ndetailed dynamic appearances. Furthermore, we introduce a pose projection\nstrategy for better generalization given novel poses. Overall, our method can\ncreate lifelike avatars with dynamic, realistic and generalized appearances.\nExperiments show that our method outperforms other state-of-the-art approaches.\nCode: https://github.com/lizhe00/AnimatableGaussians",
            "author": [
                "Zhe Li",
                "Zerong Zheng",
                "Lizhen Wang",
                "Yebin Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16096v1",
                "http://arxiv.org/pdf/2311.16096v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16094v1",
            "title": "Street TryOn: Learning In-the-Wild Virtual Try-On from Unpaired Person\n  Images",
            "updated": "2023-11-27T18:59:02Z",
            "published": "2023-11-27T18:59:02Z",
            "summary": "Virtual try-on has become a popular research topic, but most existing methods\nfocus on studio images with a clean background. They can achieve plausible\nresults for this studio try-on setting by learning to warp a garment image to\nfit a person's body from paired training data, i.e., garment images paired with\nimages of people wearing the same garment. Such data is often collected from\ncommercial websites, where each garment is demonstrated both by itself and on\nseveral models. By contrast, it is hard to collect paired data for in-the-wild\nscenes, and therefore, virtual try-on for casual images of people against\ncluttered backgrounds is rarely studied.\n  In this work, we fill the gap in the current virtual try-on research by (1)\nintroducing a Street TryOn benchmark to evaluate performance on street scenes\nand (2) proposing a novel method that can learn without paired data, from a set\nof in-the-wild person images directly. Our method can achieve robust\nperformance across shop and street domains using a novel DensePose warping\ncorrection method combined with diffusion-based inpainting controlled by pose\nand semantic segmentation. Our experiments demonstrate competitive performance\nfor standard studio try-on tasks and SOTA performance for street try-on and\ncross-domain try-on tasks.",
            "author": [
                "Aiyu Cui",
                "Jay Mahajan",
                "Viraj Shah",
                "Preeti Gomathinayagam",
                "Svetlana Lazebnik"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16094v1",
                "http://arxiv.org/pdf/2311.16094v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16093v1",
            "title": "Have we built machines that think like people?",
            "updated": "2023-11-27T18:58:34Z",
            "published": "2023-11-27T18:58:34Z",
            "summary": "A chief goal of artificial intelligence is to build machines that think like\npeople. Yet it has been argued that deep neural network architectures fail to\naccomplish this. Researchers have asserted these models' limitations in the\ndomains of causal reasoning, intuitive physics, and intuitive psychology. Yet\nrecent advancements, namely the rise of large language models, particularly\nthose designed for visual processing, have rekindled interest in the potential\nto emulate human-like cognitive abilities. This paper evaluates the current\nstate of vision-based large language models in the domains of intuitive\nphysics, causal reasoning, and intuitive psychology. Through a series of\ncontrolled experiments, we investigate the extent to which these modern models\ngrasp complex physical interactions, causal relationships, and intuitive\nunderstanding of others' preferences. Our findings reveal that, while these\nmodels demonstrate a notable proficiency in processing and interpreting visual\ndata, they still fall short of human capabilities in these areas. The models\nexhibit a rudimentary understanding of physical laws and causal relationships,\nbut their performance is hindered by a lack of deeper insights-a key aspect of\nhuman cognition. Furthermore, in tasks requiring an intuitive theory of mind,\nthe models fail altogether. Our results emphasize the need for integrating more\nrobust mechanisms for understanding causality, physical dynamics, and social\ncognition into modern-day, vision-based language models, and point out the\nimportance of cognitively-inspired benchmarks.",
            "author": [
                "Luca M. Schulze Buschoff",
                "Elif Akata",
                "Matthias Bethge",
                "Eric Schulz"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16093v1",
                "http://arxiv.org/pdf/2311.16093v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16091v1",
            "title": "Interactive Autonomous Navigation with Internal State Inference and\n  Interactivity Estimation",
            "updated": "2023-11-27T18:57:42Z",
            "published": "2023-11-27T18:57:42Z",
            "summary": "Deep reinforcement learning (DRL) provides a promising way for intelligent\nagents (e.g., autonomous vehicles) to learn to navigate complex scenarios.\nHowever, DRL with neural networks as function approximators is typically\nconsidered a black box with little explainability and often suffers from\nsuboptimal performance, especially for autonomous navigation in highly\ninteractive multi-agent environments. To address these issues, we propose three\nauxiliary tasks with spatio-temporal relational reasoning and integrate them\ninto the standard DRL framework, which improves the decision making performance\nand provides explainable intermediate indicators. We propose to explicitly\ninfer the internal states (i.e., traits and intentions) of surrounding agents\n(e.g., human drivers) as well as to predict their future trajectories in the\nsituations with and without the ego agent through counterfactual reasoning.\nThese auxiliary tasks provide additional supervision signals to infer the\nbehavior patterns of other interactive agents. Multiple variants of framework\nintegration strategies are compared. We also employ a spatio-temporal graph\nneural network to encode relations between dynamic entities, which enhances\nboth internal state inference and decision making of the ego agent. Moreover,\nwe propose an interactivity estimation mechanism based on the difference\nbetween predicted trajectories in these two situations, which indicates the\ndegree of influence of the ego agent on other agents. To validate the proposed\nmethod, we design an intersection driving simulator based on the Intelligent\nIntersection Driver Model (IIDM) that simulates vehicles and pedestrians. Our\napproach achieves robust and state-of-the-art performance in terms of standard\nevaluation metrics and provides explainable intermediate indicators (i.e.,\ninternal states, and interactivity scores) for decision making.",
            "author": [
                "Jiachen Li",
                "David Isele",
                "Kanghoon Lee",
                "Jinkyoo Park",
                "Kikuo Fujimura",
                "Mykel J. Kochenderfer"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16091v1",
                "http://arxiv.org/pdf/2311.16091v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI",
                "cs.CV",
                "cs.LG",
                "cs.MA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16086v1",
            "title": "MAST: Model-Agnostic Sparsified Training",
            "updated": "2023-11-27T18:56:03Z",
            "published": "2023-11-27T18:56:03Z",
            "summary": "We introduce a novel optimization problem formulation that departs from the\nconventional way of minimizing machine learning model loss as a black-box\nfunction. Unlike traditional formulations, the proposed approach explicitly\nincorporates an initially pre-trained model and random sketch operators,\nallowing for sparsification of both the model and gradient during training. We\nestablish insightful properties of the proposed objective function and\nhighlight its connections to the standard formulation. Furthermore, we present\nseveral variants of the Stochastic Gradient Descent (SGD) method adapted to the\nnew problem formulation, including SGD with general sampling, a distributed\nversion, and SGD with variance reduction techniques. We achieve tighter\nconvergence rates and relax assumptions, bridging the gap between theoretical\nprinciples and practical applications, covering several important techniques\nsuch as Dropout and Sparse training. This work presents promising opportunities\nto enhance the theoretical understanding of model training through a\nsparsification-aware optimization approach.",
            "author": [
                "Yury Demidovich",
                "Grigory Malinovsky",
                "Egor Shulgin",
                "Peter Richt\u00e1rik"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16086v1",
                "http://arxiv.org/pdf/2311.16086v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.DC",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16082v1",
            "title": "Transformer-QEC: Quantum Error Correction Code Decoding with\n  Transferable Transformers",
            "updated": "2023-11-27T18:52:25Z",
            "published": "2023-11-27T18:52:25Z",
            "summary": "Quantum computing has the potential to solve problems that are intractable\nfor classical systems, yet the high error rates in contemporary quantum devices\noften exceed tolerable limits for useful algorithm execution. Quantum Error\nCorrection (QEC) mitigates this by employing redundancy, distributing quantum\ninformation across multiple data qubits and utilizing syndrome qubits to\nmonitor their states for errors. The syndromes are subsequently interpreted by\na decoding algorithm to identify and correct errors in the data qubits. This\ntask is complex due to the multiplicity of error sources affecting both data\nand syndrome qubits as well as syndrome extraction operations. Additionally,\nidentical syndromes can emanate from different error sources, necessitating a\ndecoding algorithm that evaluates syndromes collectively. Although machine\nlearning (ML) decoders such as multi-layer perceptrons (MLPs) and convolutional\nneural networks (CNNs) have been proposed, they often focus on local syndrome\nregions and require retraining when adjusting for different code distances. We\nintroduce a transformer-based QEC decoder which employs self-attention to\nachieve a global receptive field across all input syndromes. It incorporates a\nmixed loss training approach, combining both local physical error and global\nparity label losses. Moreover, the transformer architecture's inherent\nadaptability to variable-length inputs allows for efficient transfer learning,\nenabling the decoder to adapt to varying code distances without retraining.\n  Evaluation on six code distances and ten different error configurations\ndemonstrates that our model consistently outperforms non-ML decoders, such as\nUnion Find (UF) and Minimum Weight Perfect Matching (MWPM), and other ML\ndecoders, thereby achieving best logical error rates. Moreover, the transfer\nlearning can save over 10x of training cost.",
            "author": [
                "Hanrui Wang",
                "Pengyu Liu",
                "Kevin Shao",
                "Dantong Li",
                "Jiaqi Gu",
                "David Z. Pan",
                "Yongshan Ding",
                "Song Han"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16082v1",
                "http://arxiv.org/pdf/2311.16082v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.AI",
                "cs.AR",
                "cs.ET",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16081v1",
            "title": "ViT-Lens-2: Gateway to Omni-modal Intelligence",
            "updated": "2023-11-27T18:52:09Z",
            "published": "2023-11-27T18:52:09Z",
            "summary": "Aiming to advance AI agents, large foundation models significantly improve\nreasoning and instruction execution, yet the current focus on vision and\nlanguage neglects the potential of perceiving diverse modalities in open-world\nenvironments. However, the success of data-driven vision and language models is\ncostly or even infeasible to be reproduced for rare modalities. In this paper,\nwe present ViT-Lens-2 that facilitates efficient omni-modal representation\nlearning by perceiving novel modalities with a pretrained ViT and aligning them\nto a pre-defined space. Specifically, the modality-specific lens is tuned to\nproject any-modal signals to an intermediate embedding space, which are then\nprocessed by a strong ViT with pre-trained visual knowledge. The encoded\nrepresentations are optimized toward aligning with the modal-independent space,\npre-defined by off-the-shelf foundation models. ViT-Lens-2 provides a unified\nsolution for representation learning of increasing modalities with two\nappealing advantages: (i) Unlocking the great potential of pretrained ViTs to\nnovel modalities effectively with efficient data regime; (ii) Enabling emergent\ndownstream capabilities through modality alignment and shared ViT parameters.\nWe tailor ViT-Lens-2 to learn representations for 3D point cloud, depth, audio,\ntactile and EEG, and set new state-of-the-art results across various\nunderstanding tasks, such as zero-shot classification. By seamlessly\nintegrating ViT-Lens-2 into Multimodal Foundation Models, we enable\nAny-modality to Text and Image Generation in a zero-shot manner. Code and\nmodels are available at https://github.com/TencentARC/ViT-Lens.",
            "author": [
                "Weixian Lei",
                "Yixiao Ge",
                "Kun Yi",
                "Jianfeng Zhang",
                "Difei Gao",
                "Dylan Sun",
                "Yuying Ge",
                "Ying Shan",
                "Mike Zheng Shou"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16081v1",
                "http://arxiv.org/pdf/2311.16081v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16080v1",
            "title": "XLB: Distributed Multi-GPU Lattice Boltzmann Simulation Framework for\n  Differentiable Scientific Machine Learning",
            "updated": "2023-11-27T18:50:37Z",
            "published": "2023-11-27T18:50:37Z",
            "summary": "The lattice Boltzmann method (LBM) has emerged as a prominent technique for\nsolving fluid dynamics problems due to its algorithmic potential for\ncomputational scalability. We introduce XLB framework, a Python-based\ndifferentiable LBM library which harnesses the capabilities of the JAX\nframework. The architecture of XLB is predicated upon ensuring accessibility,\nextensibility, and computational performance, enabling scaling effectively\nacross CPU, multi-GPU, and distributed multi-GPU systems. The framework can be\nreadily augmented with novel boundary conditions, collision models, or\nsimulation capabilities. XLB offers the unique advantage of integration with\nJAX's extensive machine learning echosystem, and the ability to utilize\nautomatic differentiation for tackling physics-based machine learning,\noptimization, and inverse problems. XLB has been successfully scaled to handle\nsimulations with billions of cells, achieving giga-scale lattice updates per\nsecond. XLB is released under the permissive Apache-2.0 license and is\navailable on GitHub at https://github.com/Autodesk/XLB.",
            "author": [
                "Mohammadmehdi Ataei",
                "Hesam Salehipour"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16080v1",
                "http://arxiv.org/pdf/2311.16080v1"
            ],
            "primary_category": "physics.comp-ph",
            "category": [
                "physics.comp-ph",
                "cs.CE",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16079v1",
            "title": "MEDITRON-70B: Scaling Medical Pretraining for Large Language Models",
            "updated": "2023-11-27T18:49:43Z",
            "published": "2023-11-27T18:49:43Z",
            "summary": "Large language models (LLMs) can potentially democratize access to medical\nknowledge. While many efforts have been made to harness and improve LLMs'\nmedical knowledge and reasoning capacities, the resulting models are either\nclosed-source (e.g., PaLM, GPT-4) or limited in scale (<= 13B parameters),\nwhich restricts their abilities. In this work, we improve access to large-scale\nmedical LLMs by releasing MEDITRON: a suite of open-source LLMs with 7B and 70B\nparameters adapted to the medical domain. MEDITRON builds on Llama-2 (through\nour adaptation of Nvidia's Megatron-LM distributed trainer), and extends\npretraining on a comprehensively curated medical corpus, including selected\nPubMed articles, abstracts, and internationally-recognized medical guidelines.\nEvaluations using four major medical benchmarks show significant performance\ngains over several state-of-the-art baselines before and after task-specific\nfinetuning. Overall, MEDITRON achieves a 6% absolute performance gain over the\nbest public baseline in its parameter class and 3% over the strongest baseline\nwe finetuned from Llama-2. Compared to closed-source LLMs, MEDITRON-70B\noutperforms GPT-3.5 and Med-PaLM and is within 5% of GPT-4 and 10% of\nMed-PaLM-2. We release our code for curating the medical pretraining corpus and\nthe MEDITRON model weights to drive open-source development of more capable\nmedical LLMs.",
            "author": [
                "Zeming Chen",
                "Alejandro Hern\u00e1ndez Cano",
                "Angelika Romanou",
                "Antoine Bonnet",
                "Kyle Matoba",
                "Francesco Salvi",
                "Matteo Pagliardini",
                "Simin Fan",
                "Andreas K\u00f6pf",
                "Amirkeivan Mohtashami",
                "Alexandre Sallinen",
                "Alireza Sakhaeirad",
                "Vinitra Swamy",
                "Igor Krawczuk",
                "Deniz Bayazit",
                "Axel Marmet",
                "Syrielle Montariol",
                "Mary-Anne Hartley",
                "Martin Jaggi",
                "Antoine Bosselut"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16079v1",
                "http://arxiv.org/pdf/2311.16079v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16078v1",
            "title": "Regulation of Disturbance Magnitude for Locational Frequency Stability\n  Using Machine Learning",
            "updated": "2023-11-27T18:48:21Z",
            "published": "2023-11-27T18:48:21Z",
            "summary": "Power systems must maintain the frequency within acceptable limits when\nsubjected to a disturbance. To ensure this, the most significant credible\ndisturbance in the system is normally used as a benchmark to allocate the\nPrimary Frequency Response (PFR) resources. However, the overall reduction of\nsystem inertia due to increased integration of Converter Interfaced Generation\n(CIG) implies that systems with high penetration of CIG require more frequency\ncontrol services, which are either costly or unavailable. In extreme cases of\ncost and scarcity, regulating the most significant disturbance magnitude can\noffer an efficient solution to this problem. This paper proposes a Machine\nLearning (ML) based technique to regulate the disturbance magnitude of the\npower system to comply with the frequency stability requirements i.e., Rate of\nChange of Frequency (RoCoF) and frequency nadir. Unlike traditional approaches\nwhich limit the disturbance magnitude by using the Centre Of Inertia (COI)\nbecause the locational frequency responses of the network are analytically hard\nto derive, the proposed method is able to capture such complexities using\ndata-driven techniques. The method does not rely on the computationally\nintensive RMS-Time Domain Simulations (TDS), once trained offline.\nConsequently, by considering the locational frequency dynamics of the system,\noperators can identify operating conditions (OC) that fulfil frequency\nrequirements at every monitored bus in the network, without the allocation of\nadditional frequency control services such as inertia. The effectiveness of the\nproposed method is demonstrated on the modified IEEE 39 Bus network.",
            "author": [
                "Alinane B. Kilembe Panagiotis N. Papadopoulos"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16078v1",
                "http://arxiv.org/pdf/2311.16078v1"
            ],
            "primary_category": "math.DS",
            "category": [
                "math.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16075v1",
            "title": "BioLORD-2023: Semantic Textual Representations Fusing LLM and Clinical\n  Knowledge Graph Insights",
            "updated": "2023-11-27T18:46:17Z",
            "published": "2023-11-27T18:46:17Z",
            "summary": "In this study, we investigate the potential of Large Language Models to\ncomplement biomedical knowledge graphs in the training of semantic models for\nthe biomedical and clinical domains. Drawing on the wealth of the UMLS\nknowledge graph and harnessing cutting-edge Large Language Models, we propose a\nnew state-of-the-art approach for obtaining high-fidelity representations of\nbiomedical concepts and sentences, consisting of three steps: an improved\ncontrastive learning phase, a novel self-distillation phase, and a weight\naveraging phase. Through rigorous evaluations via the extensive BioLORD testing\nsuite and diverse downstream tasks, we demonstrate consistent and substantial\nperformance improvements over the previous state of the art (e.g. +2pts on\nMedSTS, +2.5pts on MedNLI-S, +6.1pts on EHR-Rel-B). Besides our new\nstate-of-the-art biomedical model for English, we also distill and release a\nmultilingual model compatible with 50+ languages and finetuned on 7 European\nlanguages. Many clinical pipelines can benefit from our latest models. Our new\nmultilingual model enables a range of languages to benefit from our\nadvancements in biomedical semantic representation learning, opening a new\navenue for bioinformatics researchers around the world. As a result, we hope to\nsee BioLORD-2023 becoming a precious tool for future biomedical applications.",
            "author": [
                "Fran\u00e7ois Remy",
                "Kris Demuynck",
                "Thomas Demeester"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16075v1",
                "http://arxiv.org/pdf/2311.16075v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16065v1",
            "title": "A Survey on Vulnerability of Federated Learning: A Learning Algorithm\n  Perspective",
            "updated": "2023-11-27T18:32:08Z",
            "published": "2023-11-27T18:32:08Z",
            "summary": "This review paper takes a comprehensive look at malicious attacks against FL,\ncategorizing them from new perspectives on attack origins and targets, and\nproviding insights into their methodology and impact. In this survey, we focus\non threat models targeting the learning process of FL systems. Based on the\nsource and target of the attack, we categorize existing threat models into four\ntypes, Data to Model (D2M), Model to Data (M2D), Model to Model (M2M) and\ncomposite attacks. For each attack type, we discuss the defense strategies\nproposed, highlighting their effectiveness, assumptions and potential areas for\nimprovement. Defense strategies have evolved from using a singular metric to\nexcluding malicious clients, to employing a multifaceted approach examining\nclient models at various phases. In this survey paper, our research indicates\nthat the to-learn data, the learning gradients, and the learned model at\ndifferent stages all can be manipulated to initiate malicious attacks that\nrange from undermining model performance, reconstructing private local data,\nand to inserting backdoors. We have also seen these threat are becoming more\ninsidious. While earlier studies typically amplified malicious gradients,\nrecent endeavors subtly alter the least significant weights in local models to\nbypass defense measures. This literature review provides a holistic\nunderstanding of the current FL threat landscape and highlights the importance\nof developing robust, efficient, and privacy-preserving defenses to ensure the\nsafe and trusted adoption of FL in real-world applications.",
            "author": [
                "Xianghua Xie",
                "Chen Hu",
                "Hanchi Ren",
                "Jingjing Deng"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16065v1",
                "http://arxiv.org/pdf/2311.16065v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16214v1",
            "title": "DGR: Tackling Drifted and Correlated Noise in Quantum Error Correction\n  via Decoding Graph Re-weighting",
            "updated": "2023-11-27T18:26:16Z",
            "published": "2023-11-27T18:26:16Z",
            "summary": "Quantum hardware suffers from high error rates and noise, which makes\ndirectly running applications on them ineffective. Quantum Error Correction\n(QEC) is a critical technique towards fault tolerance which encodes the quantum\ninformation distributively in multiple data qubits and uses syndrome qubits to\ncheck parity. Minimum-Weight-Perfect-Matching (MWPM) is a popular QEC decoder\nthat takes the syndromes as input and finds the matchings between syndromes\nthat infer the errors. However, there are two paramount challenges for MWPM\ndecoders. First, as noise in real quantum systems can drift over time, there is\na potential misalignment with the decoding graph's initial weights, leading to\na severe performance degradation in the logical error rates. Second, while the\nMWPM decoder addresses independent errors, it falls short when encountering\ncorrelated errors typical on real hardware, such as those in the 2Q\ndepolarizing channel.\n  We propose DGR, an efficient decoding graph edge re-weighting strategy with\nno quantum overhead. It leverages the insight that the statistics of matchings\nacross decoding iterations offer rich information about errors on real quantum\nhardware. By counting the occurrences of edges and edge pairs in decoded\nmatchings, we can statistically estimate the up-to-date probabilities of each\nedge and the correlations between them. The reweighting process includes two\nvital steps: alignment re-weighting and correlation re-weighting. The former\nupdates the MWPM weights based on statistics to align with actual noise, and\nthe latter adjusts the weight considering edge correlations.\n  Extensive evaluations on surface code and honeycomb code under various\nsettings show that DGR reduces the logical error rate by 3.6x on average-case\nnoise mismatch with exceeding 5000x improvement under worst-case mismatch.",
            "author": [
                "Hanrui Wang",
                "Pengyu Liu",
                "Yilian Liu",
                "Jiaqi Gu",
                "Jonathan Baker",
                "Frederic T. Chong",
                "Song Han"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16214v1",
                "http://arxiv.org/pdf/2311.16214v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.AR",
                "cs.ET",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16213v1",
            "title": "Seeing Beyond Cancer: Multi-Institutional Validation of Object\n  Localization and 3D Semantic Segmentation using Deep Learning for Breast MRI",
            "updated": "2023-11-27T18:22:07Z",
            "published": "2023-11-27T18:22:07Z",
            "summary": "The clinical management of breast cancer depends on an accurate understanding\nof the tumor and its anatomical context to adjacent tissues and landmark\nstructures. This context may be provided by semantic segmentation methods;\nhowever, previous works have been largely limited to a singular focus on the\ntumor alone and rarely other tissue types. In contrast, we present a method\nthat exploits tissue-tissue interactions to accurately segment every major\ntissue type in the breast including: chest wall, skin, adipose tissue,\nfibroglandular tissue, vasculature and tumor via standard-of-care Dynamic\nContrast Enhanced MRI. Comparing our method to prior state-of-the-art, we\nachieved a superior Dice score on tumor segmentation while maintaining\ncompetitive performance on other studied tissues across multiple institutions.\nBriefly, our method proceeds by localizing the tumor using 2D object detectors,\nthen segmenting the tumor and surrounding tissues independently using two 3D\nU-nets, and finally integrating these results while mitigating false positives\nby checking for anatomically plausible tissue-tissue contacts. The object\ndetection models were pre-trained on ImageNet and COCO, and operated on MIP\n(maximum intensity projection) images in the axial and sagittal planes,\nestablishing a 3D tumor bounding box. By integrating multiple relevant\nperi-tumoral tissues, our work enables clinical applications in breast cancer\nstaging, prognosis and surgical planning.",
            "author": [
                "Arda Pekis",
                "Vignesh Kannan",
                "Evandros Kaklamanos",
                "Anu Antony",
                "Snehal Patel",
                "Tyler Earnest"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16213v1",
                "http://arxiv.org/pdf/2311.16213v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV",
                "cs.LG",
                "I.4.6; J.3"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16054v1",
            "title": "Metric Space Magnitude for Evaluating Unsupervised Representation\n  Learning",
            "updated": "2023-11-27T18:19:07Z",
            "published": "2023-11-27T18:19:07Z",
            "summary": "The magnitude of a metric space was recently established as a novel\ninvariant, providing a measure of the `effective size' of a space across\nmultiple scales. By capturing both geometrical and topological properties of\ndata, magnitude is poised to address challenges in unsupervised representation\nlearning tasks. We formalise a novel notion of dissimilarity between magnitude\nfunctions of finite metric spaces and use them to derive a quality measure for\ndimensionality reduction tasks. Our measure is provably stable under\nperturbations of the data, can be efficiently calculated, and enables a\nrigorous multi-scale comparison of embeddings. We show the utility of our\nmeasure in an experimental suite that comprises different domains and tasks,\nincluding the comparison of data visualisations.",
            "author": [
                "Katharina Limbeck",
                "Rayna Andreeva",
                "Rik Sarkar",
                "Bastian Rieck"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16054v1",
                "http://arxiv.org/pdf/2311.16054v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "math.GT",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16051v1",
            "title": "Evaluating the Impact of Personalized Value Alignment in Human-Robot\n  Interaction: Insights into Trust and Team Performance Outcomes",
            "updated": "2023-11-27T18:14:03Z",
            "published": "2023-11-27T18:14:03Z",
            "summary": "This paper examines the effect of real-time, personalized alignment of a\nrobot's reward function to the human's values on trust and team performance. We\npresent and compare three distinct robot interaction strategies: a non-learner\nstrategy where the robot presumes the human's reward function mirrors its own,\na non-adaptive-learner strategy in which the robot learns the human's reward\nfunction for trust estimation and human behavior modeling, but still optimizes\nits own reward function, and an adaptive-learner strategy in which the robot\nlearns the human's reward function and adopts it as its own. Two human-subject\nexperiments with a total number of 54 participants were conducted. In both\nexperiments, the human-robot team searches for potential threats in a town. The\nteam sequentially goes through search sites to look for threats. We model the\ninteraction between the human and the robot as a trust-aware Markov Decision\nProcess (trust-aware MDP) and use Bayesian Inverse Reinforcement Learning (IRL)\nto estimate the reward weights of the human as they interact with the robot. In\nExperiment 1, we start our learning algorithm with an informed prior of the\nhuman's values/goals. In Experiment 2, we start the learning algorithm with an\nuninformed prior. Results indicate that when starting with a good informed\nprior, personalized value alignment does not seem to benefit trust or team\nperformance. On the other hand, when an informed prior is unavailable,\nalignment to the human's values leads to high trust and higher perceived\nperformance while maintaining the same objective team performance.",
            "author": [
                "Shreyas Bhat",
                "Joseph B. Lyons",
                "Cong Shi",
                "X. Jessie Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16051v1",
                "http://arxiv.org/pdf/2311.16051v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16052v1",
            "title": "Exploring Attribute Variations in Style-based GANs using Diffusion\n  Models",
            "updated": "2023-11-27T18:14:03Z",
            "published": "2023-11-27T18:14:03Z",
            "summary": "Existing attribute editing methods treat semantic attributes as binary,\nresulting in a single edit per attribute. However, attributes such as\neyeglasses, smiles, or hairstyles exhibit a vast range of diversity. In this\nwork, we formulate the task of \\textit{diverse attribute editing} by modeling\nthe multidimensional nature of attribute edits. This enables users to generate\nmultiple plausible edits per attribute. We capitalize on disentangled latent\nspaces of pretrained GANs and train a Denoising Diffusion Probabilistic Model\n(DDPM) to learn the latent distribution for diverse edits. Specifically, we\ntrain DDPM over a dataset of edit latent directions obtained by embedding image\npairs with a single attribute change. This leads to latent subspaces that\nenable diverse attribute editing. Applying diffusion in the highly compressed\nlatent space allows us to model rich distributions of edits within limited\ncomputational resources. Through extensive qualitative and quantitative\nexperiments conducted across a range of datasets, we demonstrate the\neffectiveness of our approach for diverse attribute editing. We also showcase\nthe results of our method applied for 3D editing of various face attributes.",
            "author": [
                "Rishubh Parihar",
                "Prasanna Balaji",
                "Raghav Magazine",
                "Sarthak Vora",
                "Tejan Karmali",
                "Varun Jampani",
                "R. Venkatesh Babu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16052v1",
                "http://arxiv.org/pdf/2311.16052v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16050v1",
            "title": "An analysis of localization transitions using non-parametric\n  unsupervised learning",
            "updated": "2023-11-27T18:13:50Z",
            "published": "2023-11-27T18:13:50Z",
            "summary": "Localization transitions induced by disorder in quantum systems have been\nsubject of intense discussion in the past decades. In particular, whether or\nnot a localized phase is stable to the presence of interactions in the\nthermodynamic limit, is still an open question which is difficult to tackle\nboth with numerical and analytical approaches. Here, we provide an alternative\nviewpoint by analyzing the classical encoding configurations of the disordered\nquantum system state and showing that its critical properties can be seen also\nas a geometric transition in data space. We showcase our approach on the\nAnderson model on regular random graphs, estimating the transition point in\nagreement with results in the literature. We provide a simple and coherent\nexplanation of our findings, discussing the applicability of the method in\nreal-world scenarios with a modest number of measurements.",
            "author": [
                "Carlo Vanoni",
                "Vittorio Vitale"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16050v1",
                "http://arxiv.org/pdf/2311.16050v1"
            ],
            "primary_category": "cond-mat.dis-nn",
            "category": [
                "cond-mat.dis-nn",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16042v1",
            "title": "Weakly-Supervised 3D Reconstruction of Clothed Humans via Normal Maps",
            "updated": "2023-11-27T18:06:35Z",
            "published": "2023-11-27T18:06:35Z",
            "summary": "We present a novel deep learning-based approach to the 3D reconstruction of\nclothed humans using weak supervision via 2D normal maps. Given a single RGB\nimage or multiview images, our network infers a signed distance function (SDF)\ndiscretized on a tetrahedral mesh surrounding the body in a rest pose.\nSubsequently, inferred pose and camera parameters are used to generate a normal\nmap from the SDF. A key aspect of our approach is the use of Marching\nTetrahedra to (uniquely) compute a triangulated surface from the SDF on the\ntetrahedral mesh, facilitating straightforward differentiation (and thus\nbackpropagation). Thus, given only ground truth normal maps (with no volumetric\ninformation ground truth information), we can train the network to produce SDF\nvalues from corresponding RGB images. Optionally, an additional multiview loss\nleads to improved results. We demonstrate the efficacy of our approach for both\nnetwork inference and 3D reconstruction.",
            "author": [
                "Jane Wu",
                "Diego Thomas",
                "Ronald Fedkiw"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16042v1",
                "http://arxiv.org/pdf/2311.16042v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16038v1",
            "title": "OccWorld: Learning a 3D Occupancy World Model for Autonomous Driving",
            "updated": "2023-11-27T17:59:41Z",
            "published": "2023-11-27T17:59:41Z",
            "summary": "Understanding how the 3D scene evolves is vital for making decisions in\nautonomous driving. Most existing methods achieve this by predicting the\nmovements of object boxes, which cannot capture more fine-grained scene\ninformation. In this paper, we explore a new framework of learning a world\nmodel, OccWorld, in the 3D Occupancy space to simultaneously predict the\nmovement of the ego car and the evolution of the surrounding scenes. We propose\nto learn a world model based on 3D occupancy rather than 3D bounding boxes and\nsegmentation maps for three reasons: 1) expressiveness. 3D occupancy can\ndescribe the more fine-grained 3D structure of the scene; 2) efficiency. 3D\noccupancy is more economical to obtain (e.g., from sparse LiDAR points). 3)\nversatility. 3D occupancy can adapt to both vision and LiDAR. To facilitate the\nmodeling of the world evolution, we learn a reconstruction-based scene\ntokenizer on the 3D occupancy to obtain discrete scene tokens to describe the\nsurrounding scenes. We then adopt a GPT-like spatial-temporal generative\ntransformer to generate subsequent scene and ego tokens to decode the future\noccupancy and ego trajectory. Extensive experiments on the widely used nuScenes\nbenchmark demonstrate the ability of OccWorld to effectively model the\nevolution of the driving scenes. OccWorld also produces competitive planning\nresults without using instance and map supervision. Code:\nhttps://github.com/wzzheng/OccWorld.",
            "author": [
                "Wenzhao Zheng",
                "Weiliang Chen",
                "Yuanhui Huang",
                "Borui Zhang",
                "Yueqi Duan",
                "Jiwen Lu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16038v1",
                "http://arxiv.org/pdf/2311.16038v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16035v1",
            "title": "RobustState: Boosting Fidelity of Quantum State Preparation via\n  Noise-Aware Variational Training",
            "updated": "2023-11-27T17:55:50Z",
            "published": "2023-11-27T17:55:50Z",
            "summary": "Quantum state preparation, a crucial subroutine in quantum computing,\ninvolves generating a target quantum state from initialized qubits. Arbitrary\nstate preparation algorithms can be broadly categorized into arithmetic\ndecomposition (AD) and variational quantum state preparation (VQSP). AD employs\na predefined procedure to decompose the target state into a series of gates,\nwhereas VQSP iteratively tunes ansatz parameters to approximate target state.\nVQSP is particularly apt for Noisy-Intermediate Scale Quantum (NISQ) machines\ndue to its shorter circuits. However, achieving noise-robust parameter\noptimization still remains challenging.\n  We present RobustState, a novel VQSP training methodology that combines high\nrobustness with high training efficiency. The core idea involves utilizing\nmeasurement outcomes from real machines to perform back-propagation through\nclassical simulators, thus incorporating real quantum noise into gradient\ncalculations. RobustState serves as a versatile, plug-and-play technique\napplicable for training parameters from scratch or fine-tuning existing\nparameters to enhance fidelity on target machines. It is adaptable to various\nansatzes at both gate and pulse levels and can even benefit other variational\nalgorithms, such as variational unitary synthesis.\n  Comprehensive evaluation of RobustState on state preparation tasks for 4\ndistinct quantum algorithms using 10 real quantum machines demonstrates a\ncoherent error reduction of up to 7.1 $\\times$ and state fidelity improvement\nof up to 96\\% and 81\\% for 4-Q and 5-Q states, respectively. On average,\nRobustState improves fidelity by 50\\% and 72\\% for 4-Q and 5-Q states compared\nto baseline approaches.",
            "author": [
                "Hanrui Wang",
                "Yilian Liu",
                "Pengyu Liu",
                "Jiaqi Gu",
                "Zirui Li",
                "Zhiding Liang",
                "Jinglei Cheng",
                "Yongshan Ding",
                "Xuehai Qian",
                "Yiyu Shi",
                "David Z. Pan",
                "Frederic T. Chong",
                "Song Han"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16035v1",
                "http://arxiv.org/pdf/2311.16035v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.AI",
                "cs.AR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16030v1",
            "title": "Machine Learning-Enhanced Aircraft Landing Scheduling under\n  Uncertainties",
            "updated": "2023-11-27T17:50:14Z",
            "published": "2023-11-27T17:50:14Z",
            "summary": "This paper addresses aircraft delays, emphasizing their impact on safety and\nfinancial losses. To mitigate these issues, an innovative machine learning\n(ML)-enhanced landing scheduling methodology is proposed, aiming to improve\nautomation and safety. Analyzing flight arrival delay scenarios reveals strong\nmultimodal distributions and clusters in arrival flight time durations. A\nmulti-stage conditional ML predictor enhances separation time prediction based\non flight events. ML predictions are then integrated as safety constraints in a\ntime-constrained traveling salesman problem formulation, solved using\nmixed-integer linear programming (MILP). Historical flight recordings and model\npredictions address uncertainties between successive flights, ensuring\nreliability. The proposed method is validated using real-world data from the\nAtlanta Air Route Traffic Control Center (ARTCC ZTL). Case studies demonstrate\nan average 17.2% reduction in total landing time compared to the\nFirst-Come-First-Served (FCFS) rule. Unlike FCFS, the proposed methodology\nconsiders uncertainties, instilling confidence in scheduling. The study\nconcludes with remarks and outlines future research directions.",
            "author": [
                "Yutian Pang",
                "Peng Zhao",
                "Jueming Hu",
                "Yongming Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16030v1",
                "http://arxiv.org/pdf/2311.16030v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.LG",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16029v1",
            "title": "Deep Learning Voigt Profiles I. Single-Cloud Doublets",
            "updated": "2023-11-27T17:49:18Z",
            "published": "2023-11-27T17:49:18Z",
            "summary": "Voigt profile (VP) decomposition of quasar absorption lines is key to\nstudying intergalactic gas and the baryon cycle governing the formation and\nevolution of galaxies. The VP velocities, column densities, and Doppler $b$\nparameters inform us of the kinematic, chemical, and ionization conditions of\nthese astrophysical environments. A drawback of traditional VP fitting is that\nit can be human-time intensive. With the coming next generation of large\nall-sky survey telescopes with multi-object high-resolution spectrographs, the\ntime demands will significantly outstrip our resources. Deep learning pipelines\nhold the promise to keep pace and deliver science digestible data products. We\nexplore the application of deep learning convolutional neural networks (CNNs)\nfor predicting VP fitted parameters directly from the normalized pixel flux\nvalues in quasar absorption line profiles. A CNN was applied to 56\nsingle-component MgII2796, 2803 doublet absorption line systems observed with\nHIRES and UVES ($R=45,000$). The CNN predictions were statistically indistinct\nfrom a traditional VP fitter. The advantage is that once trained, the CNN\nprocesses systems $\\sim\\!10^5$ times faster than a human expert VP fitting\nprofiles by hand. Our pilot study shows that CNNs hold promise to perform bulk\nanalysis of quasar absorption line systems in the future.",
            "author": [
                "Bryson Stemock",
                "Christopher W. Churchill",
                "Avery Lee",
                "Sultan Hassan",
                "Caitlin Doughty",
                "Rogelio Ochoa"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16029v1",
                "http://arxiv.org/pdf/2311.16029v1"
            ],
            "primary_category": "astro-ph.GA",
            "category": [
                "astro-ph.GA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16028v1",
            "title": "Machine-to-Machine Transfer Function in Deep Learning-Based Quantitative\n  Ultrasound",
            "updated": "2023-11-27T17:46:08Z",
            "published": "2023-11-27T17:46:08Z",
            "summary": "A Transfer Function approach was recently demonstrated to mitigate data\nmismatches at the acquisition level for a single ultrasound scanner in deep\nlearning (DL) based quantitative ultrasound (QUS). As a natural progression, we\nfurther investigate the transfer function approach and introduce a\nMachine-to-Machine (M2M) Transfer Function, which possesses the ability to\nmitigate data mismatches at a machine level, i.e., mismatches between two\nscanners over the same frequency band. This ability opens the door to\nunprecedented opportunities for reducing DL model development costs, enabling\nthe combination of data from multiple sources or scanners, or facilitating the\ntransfer of DL models between machines with ease. We tested the proposed method\nutilizing a SonixOne machine and a Verasonics machine. In the experiments, we\nused a L9-4 array and conducted two types of acquisitions to obtain calibration\ndata: stable and free-hand, using two different calibration phantoms. Without\nthe proposed calibration method, the mean classification accuracy when applying\na model on data acquired from one system to data acquired from another system\nwas approximately 50%, and the mean AUC was about 0.40. With the proposed\nmethod, mean accuracy increased to approximately 90%, and the AUC rose to the\n0.99. Additional observations include that shifts in statistics for the z-score\nnormalization had a significant impact on performance. Furthermore, the choice\nof the calibration phantom played an important role in the proposed method.\nAdditionally, robust implementation inspired by Wiener filtering provided an\neffective method for transferring the domain from one machine to another\nmachine, and it can succeed using just a single calibration view without the\nneed for multiple independent calibration frames.",
            "author": [
                "Ufuk Soylu",
                "Michael L. Oelze"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16028v1",
                "http://arxiv.org/pdf/2311.16028v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16026v1",
            "title": "A Neural Framework for Generalized Causal Sensitivity Analysis",
            "updated": "2023-11-27T17:40:02Z",
            "published": "2023-11-27T17:40:02Z",
            "summary": "Unobserved confounding is common in many applications, making causal\ninference from observational data challenging. As a remedy, causal sensitivity\nanalysis is an important tool to draw causal conclusions under unobserved\nconfounding with mathematical guarantees. In this paper, we propose NeuralCSA,\na neural framework for generalized causal sensitivity analysis. Unlike previous\nwork, our framework is compatible with (i) a large class of sensitivity models,\nincluding the marginal sensitivity model, f-sensitivity models, and Rosenbaum's\nsensitivity model; (ii) different treatment types (i.e., binary and\ncontinuous); and (iii) different causal queries, including (conditional)\naverage treatment effects and simultaneous effects on multiple outcomes. The\ngenerality of \\frameworkname is achieved by learning a latent distribution\nshift that corresponds to a treatment intervention using two conditional\nnormalizing flows. We provide theoretical guarantees that NeuralCSA is able to\ninfer valid bounds on the causal query of interest and also demonstrate this\nempirically using both simulated and real-world data.",
            "author": [
                "Dennis Frauen",
                "Fergus Imrie",
                "Alicia Curth",
                "Valentyn Melnychuk",
                "Stefan Feuerriegel",
                "Mihaela van der Schaar"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16026v1",
                "http://arxiv.org/pdf/2311.16026v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16021v1",
            "title": "Scheduling and Communication Schemes for Decentralized Federated\n  Learning",
            "updated": "2023-11-27T17:35:28Z",
            "published": "2023-11-27T17:35:28Z",
            "summary": "Federated learning (FL) is a distributed machine learning paradigm in which a\nlarge number of clients coordinate with a central server to learn a model\nwithout sharing their own training data. One central server is not enough, due\nto problems of connectivity with clients. In this paper, a decentralized\nfederated learning (DFL) model with the stochastic gradient descent (SGD)\nalgorithm has been introduced, as a more scalable approach to improve the\nlearning performance in a network of agents with arbitrary topology. Three\nscheduling policies for DFL have been proposed for communications between the\nclients and the parallel servers, and the convergence, accuracy, and loss have\nbeen tested in a totally decentralized mplementation of SGD. The experimental\nresults show that the proposed scheduling polices have an impact both on the\nspeed of convergence and in the final global model.",
            "author": [
                "Bahaa-Eldin Ali Abdelghany",
                "Ana Fern\u00e1ndez-Vilas",
                "Manuel Fern\u00e1ndez-Veiga",
                "Nashwa El-Bendary",
                "Ammar M. Hassan",
                "Walid M. Abdelmoez"
            ],
            "link": [
                "http://dx.doi.org/10.1109/ICCTA58027.2022.10206255.",
                "http://arxiv.org/abs/2311.16021v1",
                "http://arxiv.org/pdf/2311.16021v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16018v1",
            "title": "RIDE: Real-time Intrusion Detection via Explainable Machine Learning\n  Implemented in a Memristor Hardware Architecture",
            "updated": "2023-11-27T17:30:19Z",
            "published": "2023-11-27T17:30:19Z",
            "summary": "Deep Learning (DL) based methods have shown great promise in network\nintrusion detection by identifying malicious network traffic behavior patterns\nwith high accuracy, but their applications to real-time, packet-level\ndetections in high-speed communication networks are challenging due to the high\ncomputation time and resource requirements of Deep Neural Networks (DNNs), as\nwell as lack of explainability. To this end, we propose a packet-level network\nintrusion detection solution that makes novel use of Recurrent Autoencoders to\nintegrate an arbitrary-length sequence of packets into a more compact joint\nfeature embedding, which is fed into a DNN-based classifier. To enable\nexplainability and support real-time detections at micro-second speed, we\nfurther develop a Software-Hardware Co-Design approach to efficiently realize\nthe proposed solution by converting the learned detection policies into\ndecision trees and implementing them using an emerging architecture based on\nmemristor devices. By jointly optimizing associated software and hardware\nconstraints, we show that our approach leads to an extremely efficient,\nreal-time solution with high detection accuracy at the packet level. Evaluation\nresults on real-world datasets (e.g., UNSW and CIC-IDS datasets) demonstrate\nnearly three-nines detection accuracy with a substantial speedup of nearly four\norders of magnitude.",
            "author": [
                "Jingdi Chen",
                "Lei Zhang",
                "Joseph Riem",
                "Gina Adam",
                "Nathaniel D. Bastian",
                "Tian Lan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16018v1",
                "http://arxiv.org/pdf/2311.16018v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16017v1",
            "title": "Decoding Logic Errors: A Comparative Study on Bug Detection by Students\n  and Large Language Models",
            "updated": "2023-11-27T17:28:33Z",
            "published": "2023-11-27T17:28:33Z",
            "summary": "Identifying and resolving logic errors can be one of the most frustrating\nchallenges for novices programmers. Unlike syntax errors, for which a compiler\nor interpreter can issue a message, logic errors can be subtle. In certain\nconditions, buggy code may even exhibit correct behavior -- in other cases, the\nissue might be about how a problem statement has been interpreted. Such errors\ncan be hard to spot when reading the code, and they can also at times be missed\nby automated tests. There is great educational potential in automatically\ndetecting logic errors, especially when paired with suitable feedback for\nnovices. Large language models (LLMs) have recently demonstrated surprising\nperformance for a range of computing tasks, including generating and explaining\ncode. These capabilities are closely linked to code syntax, which aligns with\nthe next token prediction behavior of LLMs. On the other hand, logic errors\nrelate to the runtime performance of code and thus may not be as well suited to\nanalysis by LLMs. To explore this, we investigate the performance of two\npopular LLMs, GPT-3 and GPT-4, for detecting and providing a novice-friendly\nexplanation of logic errors. We compare LLM performance with a large cohort of\nintroductory computing students $(n=964)$ solving the same error detection\ntask. Through a mixed-methods analysis of student and model responses, we\nobserve significant improvement in logic error identification between the\nprevious and current generation of LLMs, and find that both LLM generations\nsignificantly outperform students. We outline how such models could be\nintegrated into computing education tools, and discuss their potential for\nsupporting students when learning programming.",
            "author": [
                "Stephen MacNeil",
                "Paul Denny",
                "Andrew Tran",
                "Juho Leinonen",
                "Seth Bernstein",
                "Arto Hellas",
                "Sami Sarsa",
                "Joanne Kim"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16017v1",
                "http://arxiv.org/pdf/2311.16017v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16497v1",
            "title": "GaitContour: Efficient Gait Recognition based on a Contour-Pose\n  Representation",
            "updated": "2023-11-27T17:06:25Z",
            "published": "2023-11-27T17:06:25Z",
            "summary": "Gait recognition holds the promise to robustly identify subjects based on\nwalking patterns instead of appearance information. In recent years, this field\nhas been dominated by learning methods based on two principal input\nrepresentations: dense silhouette masks or sparse pose keypoints. In this work,\nwe propose a novel, point-based Contour-Pose representation, which compactly\nexpresses both body shape and body parts information. We further propose a\nlocal-to-global architecture, called GaitContour, to leverage this novel\nrepresentation and efficiently compute subject embedding in two stages. The\nfirst stage consists of a local transformer that extracts features from five\ndifferent body regions. The second stage then aggregates the regional features\nto estimate a global human gait representation. Such a design significantly\nreduces the complexity of the attention operation and improves efficiency and\nperformance simultaneously. Through large scale experiments, GaitContour is\nshown to perform significantly better than previous point-based methods, while\nalso being significantly more efficient than silhouette-based methods. On\nchallenging datasets with significant distractors, GaitContour can even\noutperform silhouette-based methods.",
            "author": [
                "Yuxiang Guo",
                "Anshul Shah",
                "Jiang Liu",
                "Rama Chellappa",
                "Cheng Peng"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16497v1",
                "http://arxiv.org/pdf/2311.16497v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16008v1",
            "title": "Using Decentralized Aggregation for Federated Learning with Differential\n  Privacy",
            "updated": "2023-11-27T17:02:56Z",
            "published": "2023-11-27T17:02:56Z",
            "summary": "Nowadays, the ubiquitous usage of mobile devices and networks have raised\nconcerns about the loss of control over personal data and research advance\ntowards the trade-off between privacy and utility in scenarios that combine\nexchange communications, big databases and distributed and collaborative (P2P)\nMachine Learning techniques. On the other hand, although Federated Learning\n(FL) provides some level of privacy by retaining the data at the local node,\nwhich executes a local training to enrich a global model, this scenario is\nstill susceptible to privacy breaches as membership inference attacks. To\nprovide a stronger level of privacy, this research deploys an experimental\nenvironment for FL with Differential Privacy (DP) using benchmark datasets. The\nobtained results show that the election of parameters and techniques of DP is\ncentral in the aforementioned trade-off between privacy and utility by means of\na classification example.",
            "author": [
                "Hadeel Abd El-Kareem",
                "Abd El-Moaty Saleh",
                "Ana Fern\u00e1ndez-Vilas",
                "Manuel Fern\u00e1ndez-Veiga",
                "asser El-Sonbaty"
            ],
            "link": [
                "http://dx.doi.org/10.1145/3551663.3558682",
                "http://arxiv.org/abs/2311.16008v1",
                "http://arxiv.org/pdf/2311.16008v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16004v1",
            "title": "Improved Data Generation for Enhanced Asset Allocation: A Synthetic\n  Dataset Approach for the Fixed Income Universe",
            "updated": "2023-11-27T16:55:04Z",
            "published": "2023-11-27T16:55:04Z",
            "summary": "We present a novel process for generating synthetic datasets tailored to\nassess asset allocation methods and construct portfolios within the fixed\nincome universe. Our approach begins by enhancing the CorrGAN model to generate\nsynthetic correlation matrices. Subsequently, we propose an Encoder-Decoder\nmodel that samples additional data conditioned on a given correlation matrix.\nThe resulting synthetic dataset facilitates in-depth analyses of asset\nallocation methods across diverse asset universes. Additionally, we provide a\ncase study that exemplifies the use of the synthetic dataset to improve\nportfolios constructed within a simulation-based asset allocation process.",
            "author": [
                "Szymon Kubiak",
                "Tillman Weyde",
                "Oleksandr Galkin",
                "Dan Philps",
                "Ram Gopal"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16004v1",
                "http://arxiv.org/pdf/2311.16004v1"
            ],
            "primary_category": "q-fin.ST",
            "category": [
                "q-fin.ST",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16003v1",
            "title": "Forecasting Auxiliary Energy Consumption for Electric Heavy-Duty\n  Vehicles",
            "updated": "2023-11-27T16:52:25Z",
            "published": "2023-11-27T16:52:25Z",
            "summary": "Accurate energy consumption prediction is crucial for optimizing the\noperation of electric commercial heavy-duty vehicles, e.g., route planning for\ncharging. Moreover, understanding why certain predictions are cast is paramount\nfor such a predictive model to gain user trust and be deployed in practice.\nSince commercial vehicles operate differently as transportation tasks, ambient,\nand drivers vary, a heterogeneous population is expected when building an AI\nsystem for forecasting energy consumption. The dependencies between the input\nfeatures and the target values are expected to also differ across\nsub-populations. One well-known example of such a statistical phenomenon is the\nSimpson paradox. In this paper, we illustrate that such a setting poses a\nchallenge for existing XAI methods that produce global feature statistics, e.g.\nLIME or SHAP, causing them to yield misleading results. We demonstrate a\npotential solution by training multiple regression models on subsets of data.\nIt not only leads to superior regression performance but also more relevant and\nconsistent LIME explanations. Given that the employed groupings correspond to\nrelevant sub-populations, the associations between the input features and the\ntarget values are consistent within each cluster but different across clusters.\nExperiments on both synthetic and real-world datasets show that such splitting\nof a complex problem into simpler ones yields better regression performance and\ninterpretability.",
            "author": [
                "Yuantao Fan",
                "Zhenkan Wang",
                "Sepideh Pashami",
                "Slawomir Nowaczyk",
                "Henrik Ydreskog"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16003v1",
                "http://arxiv.org/pdf/2311.16003v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16208v1",
            "title": "InstructMol: Multi-Modal Integration for Building a Versatile and\n  Reliable Molecular Assistant in Drug Discovery",
            "updated": "2023-11-27T16:47:51Z",
            "published": "2023-11-27T16:47:51Z",
            "summary": "The rapid evolution of artificial intelligence in drug discovery encounters\nchallenges with generalization and extensive training, yet Large Language\nModels (LLMs) offer promise in reshaping interactions with complex molecular\ndata. Our novel contribution, InstructMol, a multi-modal LLM, effectively\naligns molecular structures with natural language via an instruction-tuning\napproach, utilizing a two-stage training strategy that adeptly combines limited\ndomain-specific data with molecular and textual information. InstructMol\nshowcases substantial performance improvements in drug discovery-related\nmolecular tasks, surpassing leading LLMs and significantly reducing the gap\nwith specialized models, thereby establishing a robust foundation for a\nversatile and dependable drug discovery assistant.",
            "author": [
                "He Cao",
                "Zijing Liu",
                "Xingyu Lu",
                "Yuan Yao",
                "Yu Li"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16208v1",
                "http://arxiv.org/pdf/2311.16208v1"
            ],
            "primary_category": "q-bio.BM",
            "category": [
                "q-bio.BM",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16001v1",
            "title": "Automated Measurement of Vascular Calcification in Femoral\n  Endarterectomy Patients Using Deep Learning",
            "updated": "2023-11-27T16:47:09Z",
            "published": "2023-11-27T16:47:09Z",
            "summary": "Atherosclerosis, a chronic inflammatory disease affecting the large arteries,\npresents a global health risk. Accurate analysis of diagnostic images, like\ncomputed tomographic angiograms (CTAs), is essential for staging and monitoring\nthe progression of atherosclerosis-related conditions, including peripheral\narterial disease (PAD). However, manual analysis of CTA images is\ntime-consuming and tedious. To address this limitation, we employed a deep\nlearning model to segment the vascular system in CTA images of PAD patients\nundergoing femoral endarterectomy surgery and to measure vascular calcification\nfrom the left renal artery to the patella. Utilizing proprietary CTA images of\n27 patients undergoing femoral endarterectomy surgery provided by Prisma Health\nMidlands, we developed a Deep Neural Network (DNN) model to first segment the\narterial system, starting from the descending aorta to the patella, and second,\nto provide a metric of arterial calcification. Our designed DNN achieved 83.4%\naverage Dice accuracy in segmenting arteries from aorta to patella, advancing\nthe state-of-the-art by 0.8%. Furthermore, our work is the first to present a\nrobust statistical analysis of automated calcification measurement in the lower\nextremities using deep learning, attaining a Mean Absolute Percentage Error\n(MAPE) of 9.5% and a correlation coefficient of 0.978 between automated and\nmanual calcification scores. These findings underscore the potential of deep\nlearning techniques as a rapid and accurate tool for medical professionals to\nassess calcification in the abdominal aorta and its branches above the patella.\nThe developed DNN model and related documentation in this project are available\nat GitHub page at https://github.com/pip-alireza/DeepCalcScoring.",
            "author": [
                "Alireza Bagheri Rajeoni",
                "Breanna Pederson",
                "Daniel G. Clair",
                "Susan M. Lessner",
                "Homayoun Valafar"
            ],
            "link": [
                "http://dx.doi.org/10.3390/diagnostics13213363",
                "http://arxiv.org/abs/2311.16001v1",
                "http://arxiv.org/pdf/2311.16001v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV",
                "cs.LG",
                "I.4.6; I.4.8; I.4.0; I.2.1"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15996v1",
            "title": "Closing the ODE-SDE gap in score-based diffusion models through the\n  Fokker-Planck equation",
            "updated": "2023-11-27T16:44:50Z",
            "published": "2023-11-27T16:44:50Z",
            "summary": "Score-based diffusion models have emerged as one of the most promising\nframeworks for deep generative modelling, due to their state-of-the art\nperformance in many generation tasks while relying on mathematical foundations\nsuch as stochastic differential equations (SDEs) and ordinary differential\nequations (ODEs). Empirically, it has been reported that ODE based samples are\ninferior to SDE based samples. In this paper we rigorously describe the range\nof dynamics and approximations that arise when training score-based diffusion\nmodels, including the true SDE dynamics, the neural approximations, the various\napproximate particle dynamics that result, as well as their associated\nFokker--Planck equations and the neural network approximations of these\nFokker--Planck equations. We systematically analyse the difference between the\nODE and SDE dynamics of score-based diffusion models, and link it to an\nassociated Fokker--Planck equation. We derive a theoretical upper bound on the\nWasserstein 2-distance between the ODE- and SDE-induced distributions in terms\nof a Fokker--Planck residual. We also show numerically that conventional\nscore-based diffusion models can exhibit significant differences between ODE-\nand SDE-induced distributions which we demonstrate using explicit comparisons.\nMoreover, we show numerically that reducing the Fokker--Planck residual by\nadding it as an additional regularisation term leads to closing the gap between\nODE- and SDE-induced distributions. Our experiments suggest that this\nregularisation can improve the distribution generated by the ODE, however that\nthis can come at the cost of degraded SDE sample quality.",
            "author": [
                "Teo Deveney",
                "Jan Stanczuk",
                "Lisa Maria Kreusser",
                "Chris Budd",
                "Carola-Bibiane Sch\u00f6nlieb"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15996v1",
                "http://arxiv.org/pdf/2311.15996v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.NA",
                "math.NA",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15995v1",
            "title": "Sensitivity-Based Layer Insertion for Residual and Feedforward Neural\n  Networks",
            "updated": "2023-11-27T16:44:13Z",
            "published": "2023-11-27T16:44:13Z",
            "summary": "The training of neural networks requires tedious and often manual tuning of\nthe network architecture. We propose a systematic method to insert new layers\nduring the training process, which eliminates the need to choose a fixed\nnetwork size before training. Our technique borrows techniques from constrained\noptimization and is based on first-order sensitivity information of the\nobjective with respect to the virtual parameters that additional layers, if\ninserted, would offer. We consider fully connected feedforward networks with\nselected activation functions as well as residual neural networks. In numerical\nexperiments, the proposed sensitivity-based layer insertion technique exhibits\nimproved training decay, compared to not inserting the layer. Furthermore, the\ncomputational effort is reduced in comparison to inserting the layer from the\nbeginning. The code is available at\n\\url{https://github.com/LeonieKreis/layer_insertion_sensitivity_based}.",
            "author": [
                "Evelyn Herberg",
                "Roland Herzog",
                "Frederik K\u00f6hne",
                "Leonie Kreis",
                "Anton Schiela"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15995v1",
                "http://arxiv.org/pdf/2311.15995v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15993v1",
            "title": "Unified Batch Normalization: Identifying and Alleviating the Feature\n  Condensation in Batch Normalization and a Unified Framework",
            "updated": "2023-11-27T16:41:31Z",
            "published": "2023-11-27T16:41:31Z",
            "summary": "Batch Normalization (BN) has become an essential technique in contemporary\nneural network design, enhancing training stability. Specifically, BN employs\ncentering and scaling operations to standardize features along the batch\ndimension and uses an affine transformation to recover features. Although\nstandard BN has shown its capability to improve deep neural network training\nand convergence, it still exhibits inherent limitations in certain cases. Most\nexisting techniques that enhance BN consider a single or a few aspects of BN.\nIn this paper, we first identify problems with BN from a feature perspective\nand explore that feature condensation exists in the learning when employing BN,\nwhich negatively affects testing performance. To tackle this problem, we\npropose a two-stage unified framework called Unified Batch Normalization (UBN).\nIn the first stage, we utilize a simple feature condensation threshold to\nalleviate the feature condensation, which hinders inappropriate statistic\nupdates in normalization. In the second stage, we unify various normalization\nvariants to boost each component of BN. Our experimental results reveal that\nUBN significantly enhances performance across different visual backbones and\nnotably expedites network training convergence, particularly in early training\nstages. Notably, our method improved about 3% in top-1 accuracy on ImageNet\nclassification with large batch sizes, showing the effectiveness of our\napproach in real-world scenarios.",
            "author": [
                "Shaobo Wang",
                "Xiangdong Zhang",
                "Junchi Yan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15993v1",
                "http://arxiv.org/pdf/2311.15993v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15990v1",
            "title": "Should We Learn Most Likely Functions or Parameters?",
            "updated": "2023-11-27T16:39:55Z",
            "published": "2023-11-27T16:39:55Z",
            "summary": "Standard regularized training procedures correspond to maximizing a posterior\ndistribution over parameters, known as maximum a posteriori (MAP) estimation.\nHowever, model parameters are of interest only insomuch as they combine with\nthe functional form of a model to provide a function that can make good\npredictions. Moreover, the most likely parameters under the parameter posterior\ndo not generally correspond to the most likely function induced by the\nparameter posterior. In fact, we can re-parametrize a model such that any\nsetting of parameters can maximize the parameter posterior. As an alternative,\nwe investigate the benefits and drawbacks of directly estimating the most\nlikely function implied by the model and the data. We show that this procedure\nleads to pathological solutions when using neural networks and prove conditions\nunder which the procedure is well-behaved, as well as a scalable approximation.\nUnder these conditions, we find that function-space MAP estimation can lead to\nflatter minima, better generalization, and improved robustness to overfitting.",
            "author": [
                "Shikai Qiu",
                "Tim G. J. Rudner",
                "Sanyam Kapoor",
                "Andrew Gordon Wilson"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15990v1",
                "http://arxiv.org/pdf/2311.15990v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15985v1",
            "title": "Value-Based Reinforcement Learning for Digital Twins in Cloud Computing",
            "updated": "2023-11-27T16:29:34Z",
            "published": "2023-11-27T16:29:34Z",
            "summary": "The setup considered in the paper consists of sensors in a Networked Control\nSystem that are used to build a digital twin (DT) model of the system dynamics.\nThe focus is on control, scheduling, and resource allocation for sensory\nobservation to ensure timely delivery to the DT model deployed in the cloud.\nLow latency and communication timeliness are instrumental in ensuring that the\nDT model can accurately estimate and predict system states. However, acquiring\ndata for efficient state estimation and control computing poses a non-trivial\nproblem given the limited network resources, partial state vector information,\nand measurement errors encountered at distributed sensors. We propose the\nREinforcement learning and Variational Extended Kalman filter with Robust\nBelief (REVERB), which leverages a reinforcement learning solution combined\nwith a Value of Information-based algorithm for performing optimal control and\nselecting the most informative sensors to satisfy the prediction accuracy of\nDT. Numerical results demonstrate that the DT platform can offer satisfactory\nperformance while reducing the communication overhead up to five times.",
            "author": [
                "Van-Phuc Bui",
                "Shashi Raj Pandey",
                "Pedro M. de Sant Ana",
                "Petar Popovski"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15985v1",
                "http://arxiv.org/pdf/2311.15985v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15983v1",
            "title": "Sparsify-then-Classify: From Internal Neurons of Large Language Models\n  To Efficient Text Classifiers",
            "updated": "2023-11-27T16:28:20Z",
            "published": "2023-11-27T16:28:20Z",
            "summary": "Among the many tasks that Large Language Models (LLMs) have revolutionized is\ntext classification. However, existing approaches for applying pretrained LLMs\nto text classification predominantly rely on using single token outputs from\nonly the last layer of hidden states. As a result, they suffer from limitations\nin efficiency, task-specificity, and interpretability. In our work, we\ncontribute an approach that uses all internal representations by employing\nmultiple pooling strategies on all activation and hidden states. Our novel\nlightweight strategy, Sparsify-then-Classify (STC) first sparsifies\ntask-specific features layer-by-layer, then aggregates across layers for text\nclassification. STC can be applied as a seamless plug-and-play module on top of\nexisting LLMs. Our experiments on a comprehensive set of models and datasets\ndemonstrate that STC not only consistently improves the classification\nperformance of pretrained and fine-tuned models, but is also more efficient for\nboth training and inference, and is more intrinsically interpretable.",
            "author": [
                "Yilun Liu",
                "Difan Jiao",
                "Ashton Anderson"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15983v1",
                "http://arxiv.org/pdf/2311.15983v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15979v1",
            "title": "Soil Organic Carbon Estimation from Climate-related Features with Graph\n  Neural Network",
            "updated": "2023-11-27T16:25:12Z",
            "published": "2023-11-27T16:25:12Z",
            "summary": "Soil organic carbon (SOC) plays a pivotal role in the global carbon cycle,\nimpacting climate dynamics and necessitating accurate estimation for\nsustainable land and agricultural management. While traditional methods of SOC\nestimation face resolution and accuracy challenges, recent technological\nsolutions harness remote sensing, machine learning, and high-resolution\nsatellite mapping. Graph Neural Networks (GNNs), especially when integrated\nwith positional encoders, can capture complex relationships between soil and\nclimate. Using the LUCAS database, this study compared four GNN operators in\nthe positional encoder framework. Results revealed that the PESAGE and\nPETransformer models outperformed others in SOC estimation, indicating their\npotential in capturing the complex relationship between SOC and climate\nfeatures. Our findings confirm the feasibility of applications of GNN\narchitectures in SOC prediction, establishing a framework for future\nexplorations of this topic with more advanced GNN models.",
            "author": [
                "Weiying Zhao",
                "Natalia Efremova"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15979v1",
                "http://arxiv.org/pdf/2311.15979v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15977v1",
            "title": "Text2Loc: 3D Point Cloud Localization from Natural Language",
            "updated": "2023-11-27T16:23:01Z",
            "published": "2023-11-27T16:23:01Z",
            "summary": "We tackle the problem of 3D point cloud localization based on a few natural\nlinguistic descriptions and introduce a novel neural network, Text2Loc, that\nfully interprets the semantic relationship between points and text. Text2Loc\nfollows a coarse-to-fine localization pipeline: text-submap global place\nrecognition, followed by fine localization. In global place recognition,\nrelational dynamics among each textual hint are captured in a hierarchical\ntransformer with max-pooling (HTM), whereas a balance between positive and\nnegative pairs is maintained using text-submap contrastive learning. Moreover,\nwe propose a novel matching-free fine localization method to further refine the\nlocation predictions, which completely removes the need for complicated\ntext-instance matching and is lighter, faster, and more accurate than previous\nmethods. Extensive experiments show that Text2Loc improves the localization\naccuracy by up to $2\\times$ over the state-of-the-art on the KITTI360Pose\ndataset. We will make the code publicly available.",
            "author": [
                "Yan Xia",
                "Letian Shi",
                "Zifeng Ding",
                "Jo\u00e3o F. Henriques",
                "Daniel Cremers"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15977v1",
                "http://arxiv.org/pdf/2311.15977v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15974v1",
            "title": "Adaptive Agents and Data Quality in Agent-Based Financial Markets",
            "updated": "2023-11-27T16:18:30Z",
            "published": "2023-11-27T16:18:30Z",
            "summary": "We present our Agent-Based Market Microstructure Simulation (ABMMS), an\nAgent-Based Financial Market (ABFM) that captures much of the complexity\npresent in the US National Market System for equities (NMS). Agent-Based models\nare a natural choice for understanding financial markets. Financial markets\nfeature a constrained action space that should simplify model creation, produce\na wealth of data that should aid model validation, and a successful ABFM could\nstrongly impact system design and policy development processes. Despite these\nadvantages, ABFMs have largely remained an academic novelty. We hypothesize\nthat two factors limit the usefulness of ABFMs. First, many ABFMs fail to\ncapture relevant microstructure mechanisms, leading to differences in the\nmechanics of trading. Second, the simple agents that commonly populate ABFMs do\nnot display the breadth of behaviors observed in human traders or the trading\nsystems that they create. We investigate these issues through the development\nof ABMMS, which features a fragmented market structure, communication\ninfrastructure with propagation delays, realistic auction mechanisms, and more.\nAs a baseline, we populate ABMMS with simple trading agents and investigate\nproperties of the generated data. We then compare the baseline with\nexperimental conditions that explore the impacts of market topology or\nmeta-reinforcement learning agents. The combination of detailed market\nmechanisms and adaptive agents leads to models whose generated data more\naccurately reproduce stylized facts observed in actual markets. These\nimprovements increase the utility of ABFMs as tools to inform design and policy\ndecisions.",
            "author": [
                "Colin M. Van Oort",
                "Ethan Ratliff-Crain",
                "Brian F. Tivnan",
                "Safwan Wshah"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15974v1",
                "http://arxiv.org/pdf/2311.15974v1"
            ],
            "primary_category": "q-fin.TR",
            "category": [
                "q-fin.TR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15966v1",
            "title": "Towards Transfer Learning for Large-Scale Image Classification Using\n  Annealing-based Quantum Boltzmann Machines",
            "updated": "2023-11-27T16:07:49Z",
            "published": "2023-11-27T16:07:49Z",
            "summary": "Quantum Transfer Learning (QTL) recently gained popularity as a hybrid\nquantum-classical approach for image classification tasks by efficiently\ncombining the feature extraction capabilities of large Convolutional Neural\nNetworks with the potential benefits of Quantum Machine Learning (QML).\nExisting approaches, however, only utilize gate-based Variational Quantum\nCircuits for the quantum part of these procedures. In this work we present an\napproach to employ Quantum Annealing (QA) in QTL-based image classification.\nSpecifically, we propose using annealing-based Quantum Boltzmann Machines as\npart of a hybrid quantum-classical pipeline to learn the classification of\nreal-world, large-scale data such as medical images through supervised\ntraining. We demonstrate our approach by applying it to the three-class\nCOVID-CT-MD dataset, a collection of lung Computed Tomography (CT) scan slices.\nUsing Simulated Annealing as a stand-in for actual QA, we compare our method to\nclassical transfer learning, using a neural network of the same order of\nmagnitude, to display its improved classification performance. We find that our\napproach consistently outperforms its classical baseline in terms of test\naccuracy and AUC-ROC-Score and needs less training epochs to do this.",
            "author": [
                "Dani\u00eblle Schuman",
                "Leo S\u00fcnkel",
                "Philipp Altmann",
                "Jonas Stein",
                "Christoph Roch",
                "Thomas Gabor",
                "Claudia Linnhoff-Popien"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15966v1",
                "http://arxiv.org/pdf/2311.15966v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.ET",
                "cs.LG",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15965v1",
            "title": "FALCON: Fairness Learning via Contrastive Attention Approach to\n  Continual Semantic Scene Understanding in Open World",
            "updated": "2023-11-27T16:07:39Z",
            "published": "2023-11-27T16:07:39Z",
            "summary": "Continual Learning in semantic scene segmentation aims to continually learn\nnew unseen classes in dynamic environments while maintaining previously learned\nknowledge. Prior studies focused on modeling the catastrophic forgetting and\nbackground shift challenges in continual learning. However, fairness, another\nmajor challenge that causes unfair predictions leading to low performance among\nmajor and minor classes, still needs to be well addressed. In addition, prior\nmethods have yet to model the unknown classes well, thus resulting in producing\nnon-discriminative features among unknown classes. This paper presents a novel\nFairness Learning via Contrastive Attention Approach to continual learning in\nsemantic scene understanding. In particular, we first introduce a new Fairness\nContrastive Clustering loss to address the problems of catastrophic forgetting\nand fairness. Then, we propose an attention-based visual grammar approach to\neffectively model the background shift problem and unknown classes, producing\nbetter feature representations for different unknown classes. Through our\nexperiments, our proposed approach achieves State-of-the-Art (SOTA) performance\non different continual learning settings of three standard benchmarks, i.e.,\nADE20K, Cityscapes, and Pascal VOC. It promotes the fairness of the continual\nsemantic segmentation model.",
            "author": [
                "Thanh-Dat Truong",
                "Utsav Prabhu",
                "Bhiksha Raj",
                "Jackson Cothren",
                "Khoa Luu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15965v1",
                "http://arxiv.org/pdf/2311.15965v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15964v1",
            "title": "Efficient Pre-training for Localized Instruction Generation of Videos",
            "updated": "2023-11-27T16:07:37Z",
            "published": "2023-11-27T16:07:37Z",
            "summary": "Procedural videos show step-by-step demonstrations of tasks like recipe\npreparation. Understanding such videos is challenging, involving the precise\nlocalization of steps and the generation of textual instructions. Manually\nannotating steps and writing instructions is costly, which limits the size of\ncurrent datasets and hinders effective learning. Leveraging large but noisy\nvideo-transcript datasets for pre-training can boost performance, but demands\nsignificant computational resources. Furthermore, transcripts contain\nirrelevant content and exhibit style variation compared to instructions written\nby human annotators. To mitigate both issues, we propose a technique,\nSieve-&-Swap, to automatically curate a smaller dataset: (i) Sieve filters\nirrelevant transcripts and (ii) Swap enhances the quality of the text\ninstruction by automatically replacing the transcripts with human-written\ninstructions from a text-only recipe dataset. The curated dataset, three orders\nof magnitude smaller than current web-scale datasets, enables efficient\ntraining of large-scale models with competitive performance. We complement our\nSieve-\\&-Swap approach with a Procedure Transformer (ProcX) for end-to-end step\nlocalization and instruction generation for procedural videos. When this model\nis pre-trained on our curated dataset, it achieves state-of-the-art performance\nin zero-shot and finetuning settings on YouCook2 and Tasty, while using a\nfraction of the computational resources.",
            "author": [
                "Anil Batra",
                "Davide Moltisanti",
                "Laura Sevilla-Lara",
                "Marcus Rohrbach",
                "Frank Keller"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15964v1",
                "http://arxiv.org/pdf/2311.15964v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15962v1",
            "title": "Uncertainty Quantification of Set-Membership Estimation in Control and\n  Perception: Revisiting the Minimum Enclosing Ellipsoid",
            "updated": "2023-11-27T16:07:17Z",
            "published": "2023-11-27T16:07:17Z",
            "summary": "Set-membership estimation (SME) outputs a set estimator that guarantees to\ncover the groundtruth. Such sets are, however, defined by (many) abstract (and\npotentially nonconvex) constraints and therefore difficult to manipulate. We\npresent tractable algorithms to compute simple and tight overapproximations of\nSME in the form of minimum enclosing ellipsoids (MEE). We first introduce the\nhierarchy of enclosing ellipsoids proposed by Nie and Demmel (2005), based on\nsums-ofsquares relaxations, that asymptotically converge to the MEE of a basic\nsemialgebraic set. This framework, however, struggles in modern control and\nperception problems due to computational challenges. We contribute three\ncomputational enhancements to make this framework practical, namely constraints\npruning, generalized relaxed Chebyshev center, and handling non-Euclidean\ngeometry. We showcase numerical examples on system identification and object\npose estimation.",
            "author": [
                "Yukai Tang",
                "Jean-Bernard Lasserre",
                "Heng Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15962v1",
                "http://arxiv.org/pdf/2311.15962v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15961v1",
            "title": "Maximum Likelihood Estimation is All You Need for Well-Specified\n  Covariate Shift",
            "updated": "2023-11-27T16:06:48Z",
            "published": "2023-11-27T16:06:48Z",
            "summary": "A key challenge of modern machine learning systems is to achieve\nOut-of-Distribution (OOD) generalization -- generalizing to target data whose\ndistribution differs from that of source data. Despite its significant\nimportance, the fundamental question of ``what are the most effective\nalgorithms for OOD generalization'' remains open even under the standard\nsetting of covariate shift. This paper addresses this fundamental question by\nproving that, surprisingly, classical Maximum Likelihood Estimation (MLE)\npurely using source data (without any modification) achieves the minimax\noptimality for covariate shift under the well-specified setting. That is, no\nalgorithm performs better than MLE in this setting (up to a constant factor),\njustifying MLE is all you need. Our result holds for a very rich class of\nparametric models, and does not require any boundedness condition on the\ndensity ratio. We illustrate the wide applicability of our framework by\ninstantiating it to three concrete examples -- linear regression, logistic\nregression, and phase retrieval. This paper further complement the study by\nproving that, under the misspecified setting, MLE is no longer the optimal\nchoice, whereas Maximum Weighted Likelihood Estimator (MWLE) emerges as minimax\noptimal in certain scenarios.",
            "author": [
                "Jiawei Ge",
                "Shange Tang",
                "Jianqing Fan",
                "Cong Ma",
                "Chi Jin"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15961v1",
                "http://arxiv.org/pdf/2311.15961v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG",
                "math.ST",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15960v1",
            "title": "Addressing Long-Horizon Tasks by Integrating Program Synthesis and State\n  Machines",
            "updated": "2023-11-27T16:06:39Z",
            "published": "2023-11-27T16:06:39Z",
            "summary": "Deep reinforcement learning excels in various domains but lacks\ngeneralizability and interoperability. Programmatic RL methods (Trivedi et al.,\n2021; Liu et al., 2023) reformulate solving RL tasks as synthesizing\ninterpretable programs that can be executed in the environments. Despite\nencouraging results, these methods are limited to short-horizon tasks. On the\nother hand, representing RL policies using state machines (Inala et al., 2020)\ncan inductively generalize to long-horizon tasks; however, it struggles to\nscale up to acquire diverse and complex behaviors. This work proposes Program\nMachine Policies (POMPs), which bridge the advantages of programmatic RL and\nstate machine policies, allowing for the representation of complex behaviors\nand the address of long-term tasks. Specifically, we introduce a method that\ncan retrieve a set of effective, diverse, compatible programs. Then, we use\nthese programs as modes of a state machine and learn a transition function to\ntransition among mode programs, allowing for capturing long-horizon repetitive\nbehaviors. Our proposed framework outperforms programmatic RL and deep RL\nbaselines on various tasks and demonstrates the ability to generalize to even\nlonger horizons without any fine-tuning inductively. Ablation studies justify\nthe effectiveness of our proposed search algorithm for retrieving a set of\nprograms as modes.",
            "author": [
                "Yu-An Lin",
                "Chen-Tao Lee",
                "Guan-Ting Liu",
                "Pu-Jen Cheng",
                "Shao-Hua Sun"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15960v1",
                "http://arxiv.org/pdf/2311.15960v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.PL",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15955v1",
            "title": "NN bundle method applied to cosmology: an improvement in computational\n  times",
            "updated": "2023-11-27T15:58:37Z",
            "published": "2023-11-27T15:58:37Z",
            "summary": "In the last few years, there has been significant progress in the development\nof machine learning methods tailored to astrophysics and cosmology. Among the\nvarious methods that have been developed, there is one that allows to obtain a\nbundle of solutions of differential systems without the need of using\ntraditional numerical solvers. We have recently applied this to the\ncosmological scenario and showed that in some cases the computational times of\nthe inference process can be reduced. In this paper, we present an improvement\nto the neural network bundle method that results in a significant reduction of\nthe computational times of the statistical analysis. The novelty of the method\nconsists in the use of the neural network bundle method to calculate the\nluminosity distance of type Ia supernovae, which is usually computed through an\nintegral with numerical methods. In this work, we have applied this improvement\nto the Starobinsky $f(R)$ model, which is more difficult to integrate than the\n$f(R)$ models analyzed in our previous work. We performed a statistical\nanalysis with data from type Ia supernovae of the Pantheon+ compilation and\ncosmic chronometers to estimate the values of the free parameters of the\nStarobinsky model. We show that the statistical analyses carried out with our\nnew method require lower computational times than the ones performed with both\nthe numerical and the neural network method from our previous work. This\nreduction in time is more significant in the case of a difficult computational\nproblem such as the one we address in this work.",
            "author": [
                "Augusto T. Chantada",
                "Susana J. Landau",
                "Pavlos Protopapas",
                "Claudia G. Sc\u00f3ccola",
                "Cecilia Garraffo"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15955v1",
                "http://arxiv.org/pdf/2311.15955v1"
            ],
            "primary_category": "astro-ph.CO",
            "category": [
                "astro-ph.CO",
                "gr-qc",
                "hep-th"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15954v1",
            "title": "A Quantitative Approach to Understand Self-Supervised Models as\n  Cross-lingual Feature Extractors",
            "updated": "2023-11-27T15:58:28Z",
            "published": "2023-11-27T15:58:28Z",
            "summary": "In this work, we study the features extracted by English self-supervised\nlearning (SSL) models in cross-lingual contexts and propose a new metric to\npredict the quality of feature representations. Using automatic speech\nrecognition (ASR) as a downstream task, we analyze the effect of model size,\ntraining objectives, and model architecture on the models' performance as a\nfeature extractor for a set of topologically diverse corpora. We develop a\nnovel metric, the Phonetic-Syntax Ratio (PSR), to measure the phonetic and\nsynthetic information in the extracted representations using deep generalized\ncanonical correlation analysis. Results show the contrastive loss in the\nwav2vec2.0 objective facilitates more effective cross-lingual feature\nextraction. There is a positive correlation between PSR scores and ASR\nperformance, suggesting that phonetic information extracted by monolingual SSL\nmodels can be used for downstream tasks in cross-lingual settings. The proposed\nmetric is an effective indicator of the quality of the representations and can\nbe useful for model selection.",
            "author": [
                "Shuyue Stella Li",
                "Beining Xu",
                "Xiangyu Zhang",
                "Hexin Liu",
                "Wenhan Chao",
                "Leibny Paola Garcia"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15954v1",
                "http://arxiv.org/pdf/2311.15954v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15951v2",
            "title": "Replay across Experiments: A Natural Extension of Off-Policy RL",
            "updated": "2023-11-28T15:18:43Z",
            "published": "2023-11-27T15:57:11Z",
            "summary": "Replaying data is a principal mechanism underlying the stability and data\nefficiency of off-policy reinforcement learning (RL). We present an effective\nyet simple framework to extend the use of replays across multiple experiments,\nminimally adapting the RL workflow for sizeable improvements in controller\nperformance and research iteration times. At its core, Replay Across\nExperiments (RaE) involves reusing experience from previous experiments to\nimprove exploration and bootstrap learning while reducing required changes to a\nminimum in comparison to prior work. We empirically show benefits across a\nnumber of RL algorithms and challenging control domains spanning both\nlocomotion and manipulation, including hard exploration tasks from egocentric\nvision. Through comprehensive ablations, we demonstrate robustness to the\nquality and amount of data available and various hyperparameter choices.\nFinally, we discuss how our approach can be applied more broadly across\nresearch life cycles and can increase resilience by reloading data across\nrandom seeds or hyperparameter variations.",
            "author": [
                "Dhruva Tirumala",
                "Thomas Lampe",
                "Jose Enrique Chen",
                "Tuomas Haarnoja",
                "Sandy Huang",
                "Guy Lever",
                "Ben Moran",
                "Tim Hertweck",
                "Leonard Hasenclever",
                "Martin Riedmiller",
                "Nicolas Heess",
                "Markus Wulfmeier"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15951v2",
                "http://arxiv.org/pdf/2311.15951v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15950v1",
            "title": "Auto-CsiNet: Scenario-customized Automatic Neural Network Architecture\n  Generation for Massive MIMO CSI Feedback",
            "updated": "2023-11-27T15:56:58Z",
            "published": "2023-11-27T15:56:58Z",
            "summary": "Deep learning has revolutionized the design of the channel state information\n(CSI) feedback module in wireless communications. However, designing the\noptimal neural network (NN) architecture for CSI feedback can be a laborious\nand time-consuming process. Manual design can be prohibitively expensive for\ncustomizing NNs to different scenarios. This paper proposes using neural\narchitecture search (NAS) to automate the generation of scenario-customized CSI\nfeedback NN architectures, thereby maximizing the potential of deep learning in\nexclusive environments. By employing automated machine learning and\ngradient-descent-based NAS, an efficient and cost-effective architecture design\nprocess is achieved. The proposed approach leverages implicit scene knowledge,\nintegrating it into the scenario customization process in a data-driven manner,\nand fully exploits the potential of deep learning for each specific scenario.\nTo address the issue of excessive search, early stopping and elastic selection\nmechanisms are employed, enhancing the efficiency of the proposed scheme. The\nexperimental results demonstrate that the automatically generated architecture,\nknown as Auto-CsiNet, outperforms manually-designed models in both\nreconstruction performance (achieving approximately a 14% improvement) and\ncomplexity (reducing it by approximately 50%). Furthermore, the paper analyzes\nthe impact of the scenario on the NN architecture and its capacity.",
            "author": [
                "Xiangyi Li",
                "Jiajia Guo",
                "Chao-Kai Wen",
                "Shi Jin"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15950v1",
                "http://arxiv.org/pdf/2311.15950v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "cs.AI",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15947v1",
            "title": "GloNets: Globally Connected Neural Networks",
            "updated": "2023-11-27T15:54:20Z",
            "published": "2023-11-27T15:54:20Z",
            "summary": "Deep learning architectures suffer from depth-related performance\ndegradation, limiting the effective depth of neural networks. Approaches like\nResNet are able to mitigate this, but they do not completely eliminate the\nproblem. We introduce Globally Connected Neural Networks (GloNet), a novel\narchitecture overcoming depth-related issues, designed to be superimposed on\nany model, enhancing its depth without increasing complexity or reducing\nperformance. With GloNet, the network's head uniformly receives information\nfrom all parts of the network, regardless of their level of abstraction. This\nenables GloNet to self-regulate information flow during training, reducing the\ninfluence of less effective deeper layers, and allowing for stable training\nirrespective of network depth. This paper details GloNet's design, its\ntheoretical basis, and a comparison with existing similar architectures.\nExperiments show GloNet's self-regulation ability and resilience to\ndepth-related learning challenges, like performance degradation. Our findings\nsuggest GloNet as a strong alternative to traditional architectures like\nResNets.",
            "author": [
                "Antonio Di Cecco",
                "Carlo Metta",
                "Marco Fantozzi",
                "Francesco Morandin",
                "Maurizio Parton"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15947v1",
                "http://arxiv.org/pdf/2311.15947v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.NE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15946v1",
            "title": "Leveraging deep active learning to identify low-resource mobility\n  functioning information in public clinical notes",
            "updated": "2023-11-27T15:53:11Z",
            "published": "2023-11-27T15:53:11Z",
            "summary": "Function is increasingly recognized as an important indicator of whole-person\nhealth, although it receives little attention in clinical natural language\nprocessing research. We introduce the first public annotated dataset\nspecifically on the Mobility domain of the International Classification of\nFunctioning, Disability and Health (ICF), aiming to facilitate automatic\nextraction and analysis of functioning information from free-text clinical\nnotes. We utilize the National NLP Clinical Challenges (n2c2) research dataset\nto construct a pool of candidate sentences using keyword expansion. Our active\nlearning approach, using query-by-committee sampling weighted by density\nrepresentativeness, selects informative sentences for human annotation. We\ntrain BERT and CRF models, and use predictions from these models to guide the\nselection of new sentences for subsequent annotation iterations. Our final\ndataset consists of 4,265 sentences with a total of 11,784 entities, including\n5,511 Action entities, 5,328 Mobility entities, 306 Assistance entities, and\n639 Quantification entities. The inter-annotator agreement (IAA), averaged over\nall entity types, is 0.72 for exact matching and 0.91 for partial matching. We\nalso train and evaluate common BERT models and state-of-the-art Nested NER\nmodels. The best F1 scores are 0.84 for Action, 0.7 for Mobility, 0.62 for\nAssistance, and 0.71 for Quantification. Empirical results demonstrate\npromising potential of NER models to accurately extract mobility functioning\ninformation from clinical text. The public availability of our annotated\ndataset will facilitate further research to comprehensively capture functioning\ninformation in electronic health records (EHRs).",
            "author": [
                "Tuan-Dung Le",
                "Zhuqi Miao",
                "Samuel Alvarado",
                "Brittany Smith",
                "William Paiva",
                "Thanh Thieu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15946v1",
                "http://arxiv.org/pdf/2311.15946v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15945v1",
            "title": "Over-Squashing in Riemannian Graph Neural Networks",
            "updated": "2023-11-27T15:51:07Z",
            "published": "2023-11-27T15:51:07Z",
            "summary": "Most graph neural networks (GNNs) are prone to the phenomenon of\nover-squashing in which node features become insensitive to information from\ndistant nodes in the graph. Recent works have shown that the topology of the\ngraph has the greatest impact on over-squashing, suggesting graph rewiring\napproaches as a suitable solution. In this work, we explore whether\nover-squashing can be mitigated through the embedding space of the GNN. In\nparticular, we consider the generalization of Hyperbolic GNNs (HGNNs) to\nRiemannian manifolds of variable curvature in which the geometry of the\nembedding space is faithful to the graph's topology. We derive bounds on the\nsensitivity of the node features in these Riemannian GNNs as the number of\nlayers increases, which yield promising theoretical and empirical results for\nalleviating over-squashing in graphs with negative curvature.",
            "author": [
                "Julia Balla"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15945v1",
                "http://arxiv.org/pdf/2311.15945v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15940v2",
            "title": "Physics-informed neural networks for transformed geometries and\n  manifolds",
            "updated": "2023-11-29T15:46:23Z",
            "published": "2023-11-27T15:47:33Z",
            "summary": "Physics-informed neural networks (PINNs) effectively embed physical\nprinciples into machine learning, but often struggle with complex or\nalternating geometries. We propose a novel method for integrating geometric\ntransformations within PINNs to robustly accommodate geometric variations. Our\nmethod incorporates a diffeomorphism as a mapping of a reference domain and\nadapts the derivative computation of the physics-informed loss function. This\ngeneralizes the applicability of PINNs not only to smoothly deformed domains,\nbut also to lower-dimensional manifolds and allows for direct shape\noptimization while training the network. We demonstrate the effectivity of our\napproach on several problems: (i) Eikonal equation on Archimedean spiral, (ii)\nPoisson problem on surface manifold, (iii) Incompressible Stokes flow in\ndeformed tube, and (iv) Shape optimization with Laplace operator. Through these\nexamples, we demonstrate the enhanced flexibility over traditional PINNs,\nespecially under geometric variations. The proposed framework presents an\noutlook for training deep neural operators over parametrized geometries, paving\nthe way for advanced modeling with PDEs on complex geometries in science and\nengineering.",
            "author": [
                "Samuel Burbulla"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15940v2",
                "http://arxiv.org/pdf/2311.15940v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15939v1",
            "title": "Unleashing the Power of Prompt-driven Nucleus Instance Segmentation",
            "updated": "2023-11-27T15:46:47Z",
            "published": "2023-11-27T15:46:47Z",
            "summary": "Nuclear instance segmentation in histology images is crucial for a broad\nspectrum of clinical applications. Current prevailing nuclear instance\nsegmentation algorithms rely on regression of nuclei contours, distance maps,\nwatershed markers or a proxy nuclear representation of star-convex polygons.\nConsequently, these methods necessitate sophisticated post-processing\noperations to distinguish nuclei instances, which are commonly acknowledged to\nbe error-prone and parameter-sensitive. Recently, the segment anything model\n(SAM) has earned attracted huge attention within the domain of medical image\nsegmentation due to its impressive generalization ability and promptable\nproperty. Nevertheless, its potential on nuclear instance segmentation remains\nlargely underexplored. In this paper, we present a novel prompt-driven\nframework that consists of a point prompter and a SAM for automatic nuclei\ninstance segmentation. Specifically, the prompter learns to generate a unique\npoint prompt for each nucleus while the SAM is fine tuned to output the\ncorresponding mask of the cued nucleus. Furthermore, we propose to add adjacent\nnuclei as negative prompts to promote the model's ability to recognize\noverlapping nuclei. Without bells and whistles, our proposed method sets a new\nstate-of-the-art performance on three challenging benchmarks. Our code is\navailable at\n\\textcolor{magenta}{\\url{https://github.com/windygoo/PromptNucSeg}} .",
            "author": [
                "Zhongyi Shui",
                "Yunlong Zhang",
                "Kai Yao",
                "Chenglu Zhu",
                "Yuxuan Sun",
                "Lin Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15939v1",
                "http://arxiv.org/pdf/2311.15939v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15936v3",
            "title": "Towards Responsible Governance of Biological Design Tools",
            "updated": "2023-11-30T11:54:38Z",
            "published": "2023-11-27T15:45:02Z",
            "summary": "Recent advancements in generative machine learning have enabled rapid\nprogress in biological design tools (BDTs) such as protein structure and\nsequence prediction models. The unprecedented predictive accuracy and novel\ndesign capabilities of BDTs present new and significant dual-use risks. For\nexample, their predictive accuracy allows biological agents, whether vaccines\nor pathogens, to be developed more quickly, while the design capabilities could\nbe used to discover drugs or evade DNA screening techniques. Similar to other\ndual-use AI systems, BDTs present a wicked problem: how can regulators uphold\npublic safety without stifling innovation? We highlight how current regulatory\nproposals that are primarily tailored toward large language models may be less\neffective for BDTs, which require fewer computational resources to train and\nare often developed in an open-source manner. We propose a range of measures to\nmitigate the risk that BDTs are misused, across the areas of responsible\ndevelopment, risk assessment, transparency, access management, cybersecurity,\nand investing in resilience. Implementing such measures will require close\ncoordination between developers and governments.",
            "author": [
                "Richard Moulange",
                "Max Langenkamp",
                "Tessa Alexanian",
                "Samuel Curtis",
                "Morgan Livingston"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15936v3",
                "http://arxiv.org/pdf/2311.15936v3"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15930v1",
            "title": "WorldSense: A Synthetic Benchmark for Grounded Reasoning in Large\n  Language Models",
            "updated": "2023-11-27T15:38:17Z",
            "published": "2023-11-27T15:38:17Z",
            "summary": "We propose WorldSense, a benchmark designed to assess the extent to which\nLLMs are consistently able to sustain tacit world models, by testing how they\ndraw simple inferences from descriptions of simple arrangements of entities.\nWorldsense is a synthetic benchmark with three problem types, each with their\nown trivial control, which explicitly avoids bias by decorrelating the abstract\nstructure of problems from the vocabulary and expressions, and by decorrelating\nall problem subparts with the correct response. We run our benchmark on three\nstate-of-the-art chat-LLMs (GPT3.5, GPT4 and Llama2-chat) and show that these\nmodels make errors even with as few as three objects. Furthermore, they have\nquite heavy response biases, preferring certain responses irrespective of the\nquestion. Errors persist even with chain-of-thought prompting and in-context\nlearning. Lastly, we show that while finetuning on similar problems does result\nin substantial improvements -- within- and out-of-distribution -- the finetuned\nmodels do not generalise beyond a constraint problem space.",
            "author": [
                "Youssef Benchekroun",
                "Megi Dervishi",
                "Mark Ibrahim",
                "Jean-Baptiste Gaya",
                "Xavier Martinet",
                "Gr\u00e9goire Mialon",
                "Thomas Scialom",
                "Emmanuel Dupoux",
                "Dieuwke Hupkes",
                "Pascal Vincent"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15930v1",
                "http://arxiv.org/pdf/2311.15930v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03727v1",
            "title": "Content-Localization based System for Analyzing Sentiment and Hate\n  Behaviors in Low-Resource Dialectal Arabic: English to Levantine and Gulf",
            "updated": "2023-11-27T15:37:33Z",
            "published": "2023-11-27T15:37:33Z",
            "summary": "Even though online social movements can quickly become viral on social media,\nlanguages can be a barrier to timely monitoring and analyzing the underlying\nonline social behaviors (OSB). This is especially true for under-resourced\nlanguages on social media like dialectal Arabic; the primary language used by\nArabs on social media. Therefore, it is crucial to provide solutions to\nefficiently exploit resources from high-resourced languages to solve\nlanguage-dependent OSB analysis in under-resourced languages. This paper\nproposes to localize content of resources in high-resourced languages into\nunder-resourced Arabic dialects. Content localization goes beyond content\ntranslation that converts text from one language to another; content\nlocalization adapts culture, language nuances and regional preferences from one\nlanguage to a specific language/dialect. Automating understanding of the\nnatural and familiar day-to-day expressions in different regions, is the key to\nachieve a wider analysis of OSB especially for smart cities. In this paper, we\nutilize content-localization based neural machine translation to develop\nsentiment and hate classifiers for two low-resourced Arabic dialects: Levantine\nand Gulf. Not only this but we also leverage unsupervised learning to\nfacilitate the analysis of sentiment and hate predictions by inferring hidden\ntopics from the corresponding data and providing coherent interpretations of\nthose topics in their native language/dialects. The experimental evaluations\nand proof-of-concept COVID-19 case study on real data have validated the\neffectiveness of our proposed system in precisely distinguishing sentiments and\naccurately identifying hate content in both Levantine and Gulf Arabic dialects.\nOur findings shed light on the importance of considering the unique nature of\ndialects within the same language and ignoring the dialectal aspect would lead\nto misleading analysis.",
            "author": [
                "Fatimah Alzamzami",
                "Abdulmotaleb El Saddik"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03727v1",
                "http://arxiv.org/pdf/2312.03727v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15925v1",
            "title": "Reinforcement Learning for Wildfire Mitigation in Simulated Disaster\n  Environments",
            "updated": "2023-11-27T15:37:05Z",
            "published": "2023-11-27T15:37:05Z",
            "summary": "Climate change has resulted in a year over year increase in adverse weather\nand weather conditions which contribute to increasingly severe fire seasons.\nWithout effective mitigation, these fires pose a threat to life, property,\necology, cultural heritage, and critical infrastructure. To better prepare for\nand react to the increasing threat of wildfires, more accurate fire modelers\nand mitigation responses are necessary. In this paper, we introduce SimFire, a\nversatile wildland fire projection simulator designed to generate realistic\nwildfire scenarios, and SimHarness, a modular agent-based machine learning\nwrapper capable of automatically generating land management strategies within\nSimFire to reduce the overall damage to the area. Together, this publicly\navailable system allows researchers and practitioners the ability to emulate\nand assess the effectiveness of firefighter interventions and formulate\nstrategic plans that prioritize value preservation and resource allocation\noptimization. The repositories are available for download at\nhttps://github.com/mitrefireline.",
            "author": [
                "Alexander Tapley",
                "Marissa Dotter",
                "Michael Doyle",
                "Aidan Fennelly",
                "Dhanuj Gandikota",
                "Savanna Smith",
                "Michael Threet",
                "Tim Welsh"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15925v1",
                "http://arxiv.org/pdf/2311.15925v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.MA",
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15924v1",
            "title": "Diagnosis driven Anomaly Detection for CPS",
            "updated": "2023-11-27T15:34:40Z",
            "published": "2023-11-27T15:34:40Z",
            "summary": "In Cyber-Physical Systems (CPS) research, anomaly detection (detecting\nabnormal behavior) and diagnosis (identifying the underlying root cause) are\noften treated as distinct, isolated tasks. However, diagnosis algorithms\nrequire symptoms, i.e. temporally and spatially isolated anomalies, as input.\nThus, anomaly detection and diagnosis must be developed together to provide a\nholistic solution for diagnosis in CPS. We therefore propose a method for\nutilizing deep learning-based anomaly detection to generate inputs for\nConsistency-Based Diagnosis (CBD). We evaluate our approach on a simulated and\na real-world CPS dataset, where our model demonstrates strong performance\nrelative to other state-of-the-art models.",
            "author": [
                "Henrik S. Steude",
                "Lukas Moddemann",
                "Alexander Diedrich",
                "Jonas Ehrhardt",
                "Oliver Niggemann"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15924v1",
                "http://arxiv.org/pdf/2311.15924v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16207v1",
            "title": "The Graph Convolutional Network with Multi-representation Alignment for\n  Drug Synergy Prediction",
            "updated": "2023-11-27T15:34:14Z",
            "published": "2023-11-27T15:34:14Z",
            "summary": "Drug combination refers to the use of two or more drugs to treat a specific\ndisease at the same time. It is currently the mainstream way to treat complex\ndiseases. Compared with single drugs, drug combinations have better efficacy\nand can better inhibit toxicity and drug resistance. The computational model\nbased on deep learning concatenates the representation of multiple drugs and\nthe corresponding cell line feature as input, and the output is whether the\ndrug combination can have an inhibitory effect on the cell line. However, this\nstrategy of concatenating multiple representations has the following defects:\nthe alignment of drug representation and cell line representation is ignored,\nresulting in the synergistic relationship not being reflected positionally in\nthe embedding space. Moreover, the alignment measurement function in deep\nlearning cannot be suitable for drug synergy prediction tasks due to\ndifferences in input types. Therefore, in this work, we propose a graph\nconvolutional network with multi-representation alignment (GCNMRA) for\npredicting drug synergy. In the GCNMRA model, we designed a\nmulti-representation alignment function suitable for the drug synergy\nprediction task so that the positional relationship between drug\nrepresentations and cell line representation is reflected in the embedding\nspace. In addition, the vector modulus of drug representations and cell line\nrepresentation is considered to improve the accuracy of calculation results and\naccelerate model convergence. Finally, many relevant experiments were run on\nmultiple drug synergy datasets to verify the effectiveness of the above\ninnovative elements and the excellence of the GCNMRA model.",
            "author": [
                "Xinxing Yang",
                "Genke Yang",
                "Jian Chu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16207v1",
                "http://arxiv.org/pdf/2311.16207v1"
            ],
            "primary_category": "q-bio.QM",
            "category": [
                "q-bio.QM",
                "cs.IR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15923v1",
            "title": "SEINE: SEgment-based Indexing for NEural information retrieval",
            "updated": "2023-11-27T15:32:52Z",
            "published": "2023-11-27T15:32:52Z",
            "summary": "Many early neural Information Retrieval (NeurIR) methods are re-rankers that\nrely on a traditional first-stage retriever due to expensive query time\ncomputations. Recently, representation-based retrievers have gained much\nattention, which learns query representation and document representation\nseparately, making it possible to pre-compute document representations offline\nand reduce the workload at query time. Both dense and sparse\nrepresentation-based retrievers have been explored. However, these methods\nfocus on finding the representation that best represents a text (aka metric\nlearning) and the actual retrieval function that is responsible for similarity\nmatching between query and document is kept at a minimum by using dot product.\nOne drawback is that unlike traditional term-level inverted index, the index\nformed by these embeddings cannot be easily re-used by another retrieval\nmethod. Another drawback is that keeping the interaction at minimum hurts\nretrieval effectiveness. On the contrary, interaction-based retrievers are\nknown for their better retrieval effectiveness. In this paper, we propose a\nnovel SEgment-based Neural Indexing method, SEINE, which provides a general\nindexing framework that can flexibly support a variety of interaction-based\nneural retrieval methods. We emphasize on a careful decomposition of common\ncomponents in existing neural retrieval methods and propose to use\nsegment-level inverted index to store the atomic query-document interaction\nvalues. Experiments on LETOR MQ2007 and MQ2008 datasets show that our indexing\nmethod can accelerate multiple neural retrieval methods up to 28-times faster\nwithout sacrificing much effectiveness.",
            "author": [
                "Sibo Dong",
                "Justin Goldstein",
                "Grace Hui Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15923v1",
                "http://arxiv.org/pdf/2311.15923v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15920v1",
            "title": "A Fully Data-Driven Approach for Realistic Traffic Signal Control Using\n  Offline Reinforcement Learning",
            "updated": "2023-11-27T15:29:21Z",
            "published": "2023-11-27T15:29:21Z",
            "summary": "The optimization of traffic signal control (TSC) is critical for an efficient\ntransportation system. In recent years, reinforcement learning (RL) techniques\nhave emerged as a popular approach for TSC and show promising results for\nhighly adaptive control. However, existing RL-based methods suffer from notably\npoor real-world applicability and hardly have any successful deployments. The\nreasons for such failures are mostly due to the reliance on over-idealized\ntraffic simulators for policy optimization, as well as using unrealistic\nfine-grained state observations and reward signals that are not directly\nobtainable from real-world sensors. In this paper, we propose a fully\nData-Driven and simulator-free framework for realistic Traffic Signal Control\n(D2TSC). Specifically, we combine well-established traffic flow theory with\nmachine learning to construct a reward inference model to infer the reward\nsignals from coarse-grained traffic data. With the inferred rewards, we further\npropose a sample-efficient offline RL method to enable direct signal control\npolicy learning from historical offline datasets of real-world intersections.\nTo evaluate our approach, we collect historical traffic data from a real-world\nintersection, and develop a highly customized simulation environment that\nstrictly follows real data characteristics. We demonstrate through extensive\nexperiments that our approach achieves superior performance over conventional\nand offline RL baselines, and also enjoys much better real-world applicability.",
            "author": [
                "Jianxiong Li",
                "Shichao Lin",
                "Tianyu Shi",
                "Chujie Tian",
                "Yu Mei",
                "Jian Song",
                "Xianyuan Zhan",
                "Ruimin Li"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15920v1",
                "http://arxiv.org/pdf/2311.15920v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15906v1",
            "title": "MetaDefa: Meta-learning based on Domain Enhancement and Feature\n  Alignment for Single Domain Generalization",
            "updated": "2023-11-27T15:13:02Z",
            "published": "2023-11-27T15:13:02Z",
            "summary": "The single domain generalization(SDG) based on meta-learning has emerged as\nan effective technique for solving the domain-shift problem. However, the\ninadequate match of data distribution between source and augmented domains and\ndifficult separation of domain-invariant features from domain-related features\nmake SDG model hard to achieve great generalization. Therefore, a novel\nmeta-learning method based on domain enhancement and feature alignment\n(MetaDefa) is proposed to improve the model generalization performance. First,\nthe background substitution and visual corruptions techniques are used to\ngenerate diverse and effective augmented domains. Then, the multi-channel\nfeature alignment module based on class activation maps and class agnostic\nactivation maps is designed to effectively extract adequate transferability\nknowledge. In this module, domain-invariant features can be fully explored by\nfocusing on similar target regions between source and augmented domains feature\nspace and suppressing the feature representation of non-similar target regions.\nExtensive experiments on two publicly available datasets show that MetaDefa has\nsignificant generalization performance advantages in unknown multiple target\ndomains.",
            "author": [
                "Can Sun",
                "Hao Zheng",
                "Zhigang Hu",
                "Liu Yang",
                "Meiguang Zheng",
                "Bo Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15906v1",
                "http://arxiv.org/pdf/2311.15906v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15903v1",
            "title": "Mass reconstruction and noise reduction with cosmic-web environments",
            "updated": "2023-11-27T15:05:55Z",
            "published": "2023-11-27T15:05:55Z",
            "summary": "The clustering of galaxies and their connections to their initial conditions\nis a major means by which we learn about cosmology. However, the stochasticity\nbetween galaxies and their underlying matter field is a major limitation for\nprecise measurements of galaxy clustering. It also hinders accurate mass\nreconstruction for retrieving cosmological information from observations.\nEfforts have been made with an optimal weighting scheme to reduce this\nstochasticity using the mass-dependent clustering of dark matter halos, but its\napplication to observation is challenging due to the difficulties in measuring\nthe mass of halos precisely. Here, we show that this is not optimal. We\ndemonstrate that the cosmic-web environments (voids, sheets, filaments \\&\nknots) of halos provide extra information for reducing stochasticity. Using the\nenvironmental information alone can increase the signal-to-noise of clustering\nby approximately a factor of 3, better than the Poisson level at the scales of\nthe baryon acoustic oscillations. This improvement is comparable to using halo\nmass information alone. The information about the environment and halo mass are\ncomplementary. Their combination increases the signal-to-noise by another\nfactor of 2-3. The information about the cosmic web correlates with other\nproperties of halos, including halo concentrations and tidal forces, thus,\nthese are among the most dominant factors that can help improve the\nreconstruction. We attribute the extra information from the environment and\nsecondary properties of halos primarily to the assembly bias of halos. Our\nfindings open a new avenue for mass reconstruction and noise reduction using\ninformation beyond the halo mass.",
            "author": [
                "Feng Fang",
                "Yan-Chuan Cai",
                "Zhuoyang Li",
                "Shiyu Yue",
                "Weishan Zhu",
                "Longlong Feng"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15903v1",
                "http://arxiv.org/pdf/2311.15903v1"
            ],
            "primary_category": "astro-ph.CO",
            "category": [
                "astro-ph.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16206v1",
            "title": "Continual Instruction Tuning for Large Multimodal Models",
            "updated": "2023-11-27T15:04:48Z",
            "published": "2023-11-27T15:04:48Z",
            "summary": "Instruction tuning is now a widely adopted approach to aligning large\nmultimodal models (LMMs) to follow human intent. It unifies the data format of\nvision-language tasks, enabling multi-task joint training. However,\nvision-language tasks are constantly being created in practice. Instead of\nalways re-training LMMs when new tasks arrive, continual learning offers\nflexibility for models to continually and efficiently exploit the evolving\ndata. This work aims to explore the following two questions: 1) Do LMMs still\nsuffer from catastrophic forgetting in continual instruction tuning? 2) Are the\nexisting three classes of continual learning methods still applicable to the\ncontinual instruction tuning of LMMs? An extensive study is conducted to\naddress the above questions. First, we establish the first benchmark in this\nsetting and reveal that catastrophic forgetting is still observed when\ncontinually instruction-tuning LMMs. However, the multi-task joint instruction\ntuning can facilitate the model's continual learning ability and mitigate\nforgetting. Second, we integrate and adapt classic continual learning methods\nto our context, demonstrating the efficacy of data replay and model expansion\nstrategies across diverse scenarios. In contrast, regularization-based methods\nonly perform well on models that have been jointly instruction-tuned on\nmultiple tasks. Third, we delve into the correlation and forgetting dynamics\nbetween vision-language task pairs and propose task-similarity-informed\nregularization and model expansion methods for continual instruction tuning of\nLMMs. Experimental results show that our approach consistently boosts the\nmodel's performance.",
            "author": [
                "Jinghan He",
                "Haiyun Guo",
                "Ming Tang",
                "Jinqiao Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16206v1",
                "http://arxiv.org/pdf/2311.16206v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15894v1",
            "title": "Distributed Attacks over Federated Reinforcement Learning-enabled Cell\n  Sleep Control",
            "updated": "2023-11-27T14:59:43Z",
            "published": "2023-11-27T14:59:43Z",
            "summary": "Federated learning (FL) is particularly useful in wireless networks due to\nits distributed implementation and privacy-preserving features. However, as a\ndistributed learning system, FL can be vulnerable to malicious attacks from\nboth internal and external sources. Our work aims to investigate the attack\nmodels in a FL-enabled wireless networks. Specifically, we consider a cell\nsleep control scenario, and apply federated reinforcement learning to improve\nenergy-efficiency. We design three attacks, namely free rider attacks,\nByzantine data poisoning attacks and backdoor attacks. The simulation results\nshow that the designed attacks can degrade the network performance and lead to\nlower energy-efficiency. Moreover, we also explore possible ways to mitigate\nthe above attacks. We design a defense model called refined-Krum to defend\nagainst attacks by enabling a secure aggregation on the global server. The\nproposed refined- Krum scheme outperforms the existing Krum scheme and can\neffectively prevent wireless networks from malicious attacks, improving the\nsystem energy-efficiency performance.",
            "author": [
                "Han Zhang",
                "Hao Zhou",
                "Medhat Elsayed",
                "Majid Bavand",
                "Raimundas Gaigalas",
                "Yigit Ozcan",
                "Melike Erol-Kantarci"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15894v1",
                "http://arxiv.org/pdf/2311.15894v1"
            ],
            "primary_category": "cs.NI",
            "category": [
                "cs.NI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15891v1",
            "title": "Sampling a rare protein transition with a hybrid classical-quantum\n  computing algorithm",
            "updated": "2023-11-27T14:58:29Z",
            "published": "2023-11-27T14:58:29Z",
            "summary": "Simulating spontaneous structural rearrangements in macromolecules with\nclassical Molecular Dynamics (MD) is an outstanding challenge. Conventional\nsupercomputers can access time intervals up to tens of $\\mu$s, while many key\nevents occur on exponentially longer time scales. Transition path sampling\ntechniques have the advantage of focusing the computational power on\nbarrier-crossing trajectories, but generating uncorrelated transition paths\nthat explore diverse conformational regions remains an unsolved problem. We\nemploy a path-sampling paradigm combining machine learning (ML) with quantum\ncomputing (QC) to address this issue. We use ML on a classical computer to\nperform a preliminary uncharted exploration of the conformational space. The\ndata set generated in this exploration is then post-processed to obtain a\nnetwork representation of the reactive kinetics.\n  Quantum annealing machines can exploit quantum superposition to encode all\nthe transition pathways in this network in the initial quantum state and ensure\nthe generation of completely uncorrelated transition paths. In particular, we\nresort to the DWAVE quantum computer to perform an all-atom simulation of a\nprotein conformational transition that occurs on the ms timescale. Our results\nmatch those of a special purpose supercomputer designed to perform MD\nsimulations. These results highlight the role of biomolecular simulation as a\nground for applying, testing, and advancing quantum technologies.",
            "author": [
                "Danial Ghamari",
                "Roberto Covino",
                "Pietro Faccioli"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15891v1",
                "http://arxiv.org/pdf/2311.15891v1"
            ],
            "primary_category": "physics.bio-ph",
            "category": [
                "physics.bio-ph",
                "cond-mat.stat-mech",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15890v2",
            "title": "Stability-Informed Initialization of Neural Ordinary Differential\n  Equations",
            "updated": "2023-12-01T07:39:02Z",
            "published": "2023-11-27T14:56:47Z",
            "summary": "This paper addresses the training of Neural Ordinary Differential Equations\n(neural ODEs), and in particular explores the interplay between numerical\nintegration techniques, stability regions, step size, and initialization\ntechniques. It is shown how the choice of integration technique implicitly\nregularizes the learned model, and how the solver's corresponding stability\nregion affects training and prediction performance. From this analysis, a\nstability-informed parameter initialization technique is introduced. The\neffectiveness of the initialization method is displayed across several learning\nbenchmarks and industrial applications.",
            "author": [
                "Theodor Westny",
                "Arman Mohammadi",
                "Daniel Jung",
                "Erik Frisk"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15890v2",
                "http://arxiv.org/pdf/2311.15890v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15887v1",
            "title": "FLASC: A Flare-Sensitive Clustering Algorithm: Extending HDBSCAN* for\n  Detecting Branches in Clusters",
            "updated": "2023-11-27T14:55:16Z",
            "published": "2023-11-27T14:55:16Z",
            "summary": "We present FLASC, an algorithm for flare-sensitive clustering. Our algorithm\nbuilds upon HDBSCAN* -- which provides high-quality density-based clustering\nperformance -- through a post-processing step that differentiates branches\nwithin the detected clusters' manifold, adding a type of pattern that can be\ndiscovered. Two variants of the algorithm are presented, which trade\ncomputational cost for noise robustness. We show that both variants scale\nsimilarly to HDBSCAN* in terms of computational cost and provide stable outputs\nusing synthetic data sets, resulting in an efficient flare-sensitive clustering\nalgorithm. In addition, we demonstrate the algorithm's benefit in data\nexploration over HDBSCAN* clustering on two real-world data sets.",
            "author": [
                "D. M. Bot",
                "J. Peeters",
                "J. Liesenborgs",
                "J. Aerts"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15887v1",
                "http://arxiv.org/pdf/2311.15887v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.DB",
                "I.5.3; H.3.3"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15878v1",
            "title": "Individualized Treatment Allocations with Distributional Welfare",
            "updated": "2023-11-27T14:51:30Z",
            "published": "2023-11-27T14:51:30Z",
            "summary": "In this paper, we explore optimal treatment allocation policies that target\ndistributional welfare. Most literature on treatment choice has considered\nutilitarian welfare based on the conditional average treatment effect (ATE).\nWhile average welfare is intuitive, it may yield undesirable allocations\nespecially when individuals are heterogeneous (e.g., with outliers) - the very\nreason individualized treatments were introduced in the first place. This\nobservation motivates us to propose an optimal policy that allocates the\ntreatment based on the conditional \\emph{quantile of individual treatment\neffects} (QoTE). Depending on the choice of the quantile probability, this\ncriterion can accommodate a policymaker who is either prudent or negligent. The\nchallenge of identifying the QoTE lies in its requirement for knowledge of the\njoint distribution of the counterfactual outcomes, which is generally hard to\nrecover even with experimental data. Therefore, we introduce minimax optimal\npolicies that are robust to model uncertainty. We then propose a range of\nidentifying assumptions under which we can point or partially identify the\nQoTE. We establish the asymptotic bound on the regret of implementing the\nproposed policies. We consider both stochastic and deterministic rules. In\nsimulations and two empirical applications, we compare optimal decisions based\non the QoTE with decisions based on other criteria.",
            "author": [
                "Yifan Cui",
                "Sukjin Han"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15878v1",
                "http://arxiv.org/pdf/2311.15878v1"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME",
                "econ.EM",
                "math.ST",
                "stat.ML",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15876v1",
            "title": "RO-LLaMA: Generalist LLM for Radiation Oncology via Noise Augmentation\n  and Consistency Regularization",
            "updated": "2023-11-27T14:49:06Z",
            "published": "2023-11-27T14:49:06Z",
            "summary": "Recent advancements in Artificial Intelligence (AI) have profoundly\ninfluenced medical fields, by providing tools to reduce clinical workloads.\nHowever, most AI models are constrained to execute uni-modal tasks, in stark\ncontrast to the comprehensive approaches utilized by medical professionals. To\naddress this, here we present RO-LLaMA, a versatile generalist large language\nmodel (LLM) tailored for the field of radiation oncology. This model seamlessly\ncovers a wide range of the workflow of radiation oncologists, adept at various\ntasks such as clinical report summarization, radiation therapy plan suggestion,\nand plan-guided therapy target volume segmentation. In particular, to maximize\nthe end-to-end performance, we further present a novel Consistency Embedding\nFine-Tuning (CEFTune) technique, which boosts LLM's robustness to additional\nerrors at the intermediates while preserving the capability of handling clean\ninputs, and creatively transform this concept into LLM-driven segmentation\nframework as Consistency Embedding Segmentation (CESEG). Experimental results\non multi-centre cohort sets demonstrate our proposed RO-LLaMA's promising\nperformance for diverse tasks with generalization capabilities.",
            "author": [
                "Kwanyoung Kim",
                "Yujin Oh",
                "Sangjoon Park",
                "Hwa Kyung Byun",
                "Jin Sung Kim",
                "Yong Bae Kim",
                "Jong Chul Ye"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15876v1",
                "http://arxiv.org/pdf/2311.15876v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15875v1",
            "title": "Nodal Hydraulic Head Estimation through Unscented Kalman Filter for\n  Data-driven Leak Localization in Water Networks",
            "updated": "2023-11-27T14:48:37Z",
            "published": "2023-11-27T14:48:37Z",
            "summary": "In this paper, we present a nodal hydraulic head estimation methodology for\nwater distribution networks (WDN) based on an Unscented Kalman Filter (UKF)\nscheme with application to leak localization. The UKF refines an initial\nestimation of the hydraulic state by considering the prediction model, as well\nas available pressure and demand measurements. To this end, it provides\ncustomized prediction and data assimilation steps. Additionally, the method is\nenhanced by dynamically updating the prediction function weight matrices.\nPerformance testing on the Modena benchmark under realistic conditions\ndemonstrates the method's effectiveness in enhancing state estimation and\ndata-driven leak localization.",
            "author": [
                "Luis Romero-Ben",
                "Paul Irofti",
                "Florin Stoican",
                "Vicen\u00e7 Puig"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15875v1",
                "http://arxiv.org/pdf/2311.15875v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.LG",
                "cs.NA",
                "cs.SY",
                "math.NA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15868v2",
            "title": "Learning with Errors over Group Rings Constructed by Semi-direct Product",
            "updated": "2023-12-01T15:33:09Z",
            "published": "2023-11-27T14:38:36Z",
            "summary": "The Learning with Errors (LWE) problem has been widely utilized as a\nfoundation for numerous cryptographic tools over the years. In this study, we\nfocus on an algebraic variant of the LWE problem called Group ring LWE\n(GR-LWE). We select group rings (or their direct summands) that underlie\nspecific families of finite groups constructed by taking the semi-direct\nproduct of two cyclic groups. Unlike the Ring-LWE problem described in\n\\cite{lyubashevsky2010ideal}, the multiplication operation in the group rings\nconsidered here is non-commutative. As an extension of Ring-LWE, it maintains\ncomputational hardness and can be potentially applied in many cryptographic\nscenarios. In this paper, we present two polynomial-time quantum reductions.\nFirstly, we provide a quantum reduction from the worst-case shortest\nindependent vectors problem (SIVP) in ideal lattices with polynomial\napproximate factor to the search version of GR-LWE. This reduction requires\nthat the underlying group ring possesses certain mild properties; Secondly, we\npresent another quantum reduction for two types of group rings, where the\nworst-case SIVP problem is directly reduced to the (average-case) decision\nGR-LWE problem. The pseudorandomness of GR-LWE samples guaranteed by this\nreduction can be consequently leveraged to construct semantically secure\npublic-key cryptosystems.",
            "author": [
                "Jiaqi Liu",
                "Fang-Wei Fu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15868v2",
                "http://arxiv.org/pdf/2311.15868v2"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.IT",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15865v1",
            "title": "A precise symbolic emulator of the linear matter power spectrum",
            "updated": "2023-11-27T14:33:21Z",
            "published": "2023-11-27T14:33:21Z",
            "summary": "Computing the matter power spectrum, $P(k)$, as a function of cosmological\nparameters can be prohibitively slow in cosmological analyses, hence emulating\nthis calculation is desirable. Previous analytic approximations are\ninsufficiently accurate for modern applications, so black-box, uninterpretable\nemulators are often used. We utilise an efficient genetic programming based\nsymbolic regression framework to explore the space of potential mathematical\nexpressions which can approximate the power spectrum and $\\sigma_8$. We learn\nthe ratio between an existing low-accuracy fitting function for $P(k)$ and that\nobtained by solving the Boltzmann equations and thus still incorporate the\nphysics which motivated this earlier approximation. We obtain an analytic\napproximation to the linear power spectrum with a root mean squared fractional\nerror of 0.2% between $k = 9\\times10^{-3} - 9 \\, h{\\rm \\, Mpc^{-1}}$ and across\na wide range of cosmological parameters, and we provide physical\ninterpretations for various terms in the expression. We also provide a simple\nanalytic approximation for $\\sigma_8$ with a similar accuracy, with a root mean\nsquared fractional error of just 0.4% when evaluated across the same range of\ncosmologies. This function is easily invertible to obtain $A_{\\rm s}$ as a\nfunction of $\\sigma_8$ and the other cosmological parameters, if preferred. It\nis possible to obtain symbolic approximations to a seemingly complex function\nat a precision required for current and future cosmological analyses without\nresorting to deep-learning techniques, thus avoiding their black-box nature and\nlarge number of parameters. Our emulator will be usable long after the codes on\nwhich numerical approximations are built become outdated.",
            "author": [
                "Deaglan J. Bartlett",
                "Lukas Kammerer",
                "Gabriel Kronberger",
                "Harry Desmond",
                "Pedro G. Ferreira",
                "Benjamin D. Wandelt",
                "Bogdan Burlacu",
                "David Alonso",
                "Matteo Zennaro"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15865v1",
                "http://arxiv.org/pdf/2311.15865v1"
            ],
            "primary_category": "astro-ph.CO",
            "category": [
                "astro-ph.CO",
                "astro-ph.IM",
                "cs.LG",
                "cs.NE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15858v1",
            "title": "Multi-Agent Reinforcement Learning for Power Control in Wireless\n  Networks via Adaptive Graphs",
            "updated": "2023-11-27T14:25:40Z",
            "published": "2023-11-27T14:25:40Z",
            "summary": "The ever-increasing demand for high-quality and heterogeneous wireless\ncommunication services has driven extensive research on dynamic optimization\nstrategies in wireless networks. Among several possible approaches, multi-agent\ndeep reinforcement learning (MADRL) has emerged as a promising method to\naddress a wide range of complex optimization problems like power control.\nHowever, the seamless application of MADRL to a variety of network optimization\nproblems faces several challenges related to convergence. In this paper, we\npresent the use of graphs as communication-inducing structures among\ndistributed agents as an effective means to mitigate these challenges.\nSpecifically, we harness graph neural networks (GNNs) as neural architectures\nfor policy parameterization to introduce a relational inductive bias in the\ncollective decision-making process. Most importantly, we focus on modeling the\ndynamic interactions among sets of neighboring agents through the introduction\nof innovative methods for defining a graph-induced framework for integrated\ncommunication and learning. Finally, the superior generalization capabilities\nof the proposed methodology to larger networks and to networks with different\nuser categories is verified through simulations.",
            "author": [
                "Lorenzo Mario Amorosa",
                "Marco Skocaj",
                "Roberto Verdone",
                "Deniz G\u00fcnd\u00fcz"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15858v1",
                "http://arxiv.org/pdf/2311.15858v1"
            ],
            "primary_category": "cs.NI",
            "category": [
                "cs.NI",
                "cs.LG",
                "cs.MA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15856v1",
            "title": "JSSL: Joint Supervised and Self-supervised Learning for MRI\n  Reconstruction",
            "updated": "2023-11-27T14:23:36Z",
            "published": "2023-11-27T14:23:36Z",
            "summary": "Magnetic Resonance Imaging represents an important diagnostic modality;\nhowever, its inherently slow acquisition process poses challenges in obtaining\nfully sampled k-space data under motion in clinical scenarios such as\nabdominal, cardiac, and prostate imaging. In the absence of fully sampled\nacquisitions, which can serve as ground truth data, training deep learning\nalgorithms in a supervised manner to predict the underlying ground truth image\nbecomes an impossible task. To address this limitation, self-supervised methods\nhave emerged as a viable alternative, leveraging available subsampled k-space\ndata to train deep learning networks for MRI reconstruction. Nevertheless,\nthese self-supervised approaches often fall short when compared to supervised\nmethodologies. In this paper, we introduce JSSL (Joint Supervised and\nSelf-supervised Learning), a novel training approach for deep learning-based\nMRI reconstruction algorithms aimed at enhancing reconstruction quality in\nscenarios where target dataset(s) containing fully sampled k-space measurements\nare unavailable. Our proposed method operates by simultaneously training a\nmodel in a self-supervised learning setting, using subsampled data from the\ntarget dataset(s), and in a supervised learning manner, utilizing data from\nother datasets, referred to as proxy datasets, where fully sampled k-space data\nis accessible. To demonstrate the efficacy of JSSL, we utilized subsampled\nprostate parallel MRI measurements as the target dataset, while employing fully\nsampled brain and knee k-space acquisitions as proxy datasets. Our results\nshowcase a substantial improvement over conventional self-supervised training\nmethods, thereby underscoring the effectiveness of our joint approach. We\nprovide a theoretical motivation for JSSL and establish a practical\n\"rule-of-thumb\" for selecting the most appropriate training approach for deep\nMRI reconstruction.",
            "author": [
                "George Yiasemis",
                "Nikita Moriakov",
                "Clara I. S\u00e1nchez",
                "Jan-Jakob Sonke",
                "Jonas Teuwen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15856v1",
                "http://arxiv.org/pdf/2311.15856v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV",
                "physics.med-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15854v1",
            "title": "A systematic study comparing hyperparameter optimization engines on\n  tabular data",
            "updated": "2023-11-27T14:21:47Z",
            "published": "2023-11-27T14:21:47Z",
            "summary": "We run an independent comparison of all hyperparameter optimization\n(hyperopt) engines available in the Ray Tune library. We introduce two ways to\nnormalize and aggregate statistics across data sets and models, one rank-based,\nand another one sandwiching the score between the random search score and the\nfull grid search score. This affords us i) to rank the hyperopt engines, ii) to\nmake generalized and statistically significant statements on how much they\nimprove over random search, and iii) to make recommendations on which engine\nshould be used to hyperopt a given learning algorithm. We find that most\nengines beat random search, but that only three of them (HEBO, AX, and\nBlendSearch) clearly stand out. We also found that some engines seem to\nspecialize in hyperopting certain learning algorithms, which makes it tricky to\nuse hyperopt in comparison studies, since the choice of the hyperopt technique\nmay favor some of the models in the comparison.",
            "author": [
                "Balazs Kegl"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15854v1",
                "http://arxiv.org/pdf/2311.15854v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15851v1",
            "title": "Single-Model and Any-Modality for Video Object Tracking",
            "updated": "2023-11-27T14:17:41Z",
            "published": "2023-11-27T14:17:41Z",
            "summary": "In the realm of video object tracking, auxiliary modalities such as depth,\nthermal, or event data have emerged as valuable assets to complement the RGB\ntrackers. In practice, most existing RGB trackers learn a single set of\nparameters to use them across datasets and applications. However, a similar\nsingle-model unification for multi-modality tracking presents several\nchallenges. These challenges stem from the inherent heterogeneity of inputs --\neach with modality-specific representations, the scarcity of multi-modal\ndatasets, and the absence of all the modalities at all times. In this work, we\nintroduce Un-Track, a \\underline{Un}ified Tracker of a single set of parameters\nfor any modality. To handle any modality, our method learns their common latent\nspace through low-rank factorization and reconstruction techniques. More\nimportantly, we use only the RGB-X pairs to learn the common latent space. This\nunique shared representation seamlessly binds all modalities together, enabling\neffective unification and accommodating any missing modality, all within a\nsingle transformer-based architecture and without the need for\nmodality-specific fine-tuning. Our Un-Track achieves +8.1 absolute F-score\ngain, on the DepthTrack dataset, by introducing only +2.14 (over 21.50) GFLOPs\nwith +6.6M (over 93M) parameters, through a simple yet efficient prompting\nstrategy. Extensive comparisons on five benchmark datasets with different\nmodalities show that Un-Track surpasses both SOTA unified trackers and\nmodality-specific finetuned counterparts, validating our effectiveness and\npracticality.",
            "author": [
                "Zongwei Wu",
                "Jilai Zheng",
                "Xiangxuan Ren",
                "Florin-Alexandru Vasluianu",
                "Chao Ma",
                "Danda Pani Paudel",
                "Luc Van Gool",
                "Radu Timofte"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15851v1",
                "http://arxiv.org/pdf/2311.15851v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15847v1",
            "title": "Cell Maps Representation For Lung Adenocarcinoma Growth Patterns\n  Classification In Whole Slide Images",
            "updated": "2023-11-27T14:12:51Z",
            "published": "2023-11-27T14:12:51Z",
            "summary": "Lung adenocarcinoma is a morphologically heterogeneous disease, characterized\nby five primary histologic growth patterns. The quantity of these patterns can\nbe related to tumor behavior and has a significant impact on patient prognosis.\nIn this work, we propose a novel machine learning pipeline capable of\nclassifying tissue tiles into one of the five patterns or as non-tumor, with an\nArea Under the Receiver Operating Characteristic Curve (AUCROC) score of 0.97.\nOur model's strength lies in its comprehensive consideration of cellular\nspatial patterns, where it first generates cell maps from Hematoxylin and Eosin\n(H&E) whole slide images (WSIs), which are then fed into a convolutional neural\nnetwork classification model. Exploiting these cell maps provides the model\nwith robust generalizability to new data, achieving approximately 30% higher\naccuracy on unseen test-sets compared to current state of the art approaches.\nThe insights derived from our model can be used to predict prognosis, enhancing\npatient outcomes.",
            "author": [
                "Arwa Al-Rubaian",
                "Gozde N. Gunesli",
                "Wajd A. Althakfi",
                "Ayesha Azam",
                "Nasir Rajpoot",
                "Shan E Ahmed Raza"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15847v1",
                "http://arxiv.org/pdf/2311.15847v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15846v1",
            "title": "Learning with Noisy Low-Cost MOS for Image Quality Assessment via\n  Dual-Bias Calibration",
            "updated": "2023-11-27T14:11:54Z",
            "published": "2023-11-27T14:11:54Z",
            "summary": "Learning based image quality assessment (IQA) models have obtained impressive\nperformance with the help of reliable subjective quality labels, where mean\nopinion score (MOS) is the most popular choice. However, in view of the\nsubjective bias of individual annotators, the labor-abundant MOS (LA-MOS)\ntypically requires a large collection of opinion scores from multiple\nannotators for each image, which significantly increases the learning cost. In\nthis paper, we aim to learn robust IQA models from low-cost MOS (LC-MOS), which\nonly requires very few opinion scores or even a single opinion score for each\nimage. More specifically, we consider the LC-MOS as the noisy observation of\nLA-MOS and enforce the IQA model learned from LC-MOS to approach the unbiased\nestimation of LA-MOS. In this way, we represent the subjective bias between\nLC-MOS and LA-MOS, and the model bias between IQA predictions learned from\nLC-MOS and LA-MOS (i.e., dual-bias) as two latent variables with unknown\nparameters. By means of the expectation-maximization based alternating\noptimization, we can jointly estimate the parameters of the dual-bias, which\nsuppresses the misleading of LC-MOS via a gated dual-bias calibration (GDBC)\nmodule. To the best of our knowledge, this is the first exploration of robust\nIQA model learning from noisy low-cost labels. Theoretical analysis and\nextensive experiments on four popular IQA datasets show that the proposed\nmethod is robust toward different bias rates and annotation numbers and\nsignificantly outperforms the other learning based IQA models when only LC-MOS\nis available. Furthermore, we also achieve comparable performance with respect\nto the other models learned with LA-MOS.",
            "author": [
                "Lei Wang",
                "Qingbo Wu",
                "Desen Yuan",
                "King Ngi Ngan",
                "Hongliang Li",
                "Fanman Meng",
                "Linfeng Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15846v1",
                "http://arxiv.org/pdf/2311.15846v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15845v3",
            "title": "On Learning the Optimal Regularization Parameter in Inverse Problems",
            "updated": "2023-12-01T15:14:52Z",
            "published": "2023-11-27T14:10:28Z",
            "summary": "Selecting the best regularization parameter in inverse problems is a\nclassical and yet challenging problem. Recently, data-driven approaches have\nbecome popular to tackle this challenge. These approaches are appealing since\nthey do require less a priori knowledge, but their theoretical analysis is\nlimited. In this paper, we propose and study a statistical machine learning\napproach, based on empirical risk minimization. Our main contribution is a\ntheoretical analysis, showing that, provided with enough data, this approach\ncan reach sharp rates while being essentially adaptive to the noise and\nsmoothness of the problem. Numerical simulations corroborate and illustrate the\ntheoretical findings. Our results are a step towards grounding theoretically\ndata-driven approaches to inverse problems.",
            "author": [
                "Jonathan Chirinos Rodriguez",
                "Ernesto De Vito",
                "Cesare Molinari",
                "Lorenzo Rosasco",
                "Silvia Villa"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15845v3",
                "http://arxiv.org/pdf/2311.15845v3"
            ],
            "primary_category": "math.ST",
            "category": [
                "math.ST",
                "math.OC",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15841v2",
            "title": "Learning Disentangled Identifiers for Action-Customized Text-to-Image\n  Generation",
            "updated": "2023-11-30T17:51:47Z",
            "published": "2023-11-27T14:07:13Z",
            "summary": "This study focuses on a novel task in text-to-image (T2I) generation, namely\naction customization. The objective of this task is to learn the co-existing\naction from limited data and generalize it to unseen humans or even animals.\nExperimental results show that existing subject-driven customization methods\nfail to learn the representative characteristics of actions and struggle in\ndecoupling actions from context features, including appearance. To overcome the\npreference for low-level features and the entanglement of high-level features,\nwe propose an inversion-based method Action-Disentangled Identifier (ADI) to\nlearn action-specific identifiers from the exemplar images. ADI first expands\nthe semantic conditioning space by introducing layer-wise identifier tokens,\nthereby increasing the representational richness while distributing the\ninversion across different features. Then, to block the inversion of\naction-agnostic features, ADI extracts the gradient invariance from the\nconstructed sample triples and masks the updates of irrelevant channels. To\ncomprehensively evaluate the task, we present an ActionBench that includes a\nvariety of actions, each accompanied by meticulously selected samples. Both\nquantitative and qualitative results show that our ADI outperforms existing\nbaselines in action-customized T2I generation. Our project page is at\nhttps://adi-t2i.github.io/ADI.",
            "author": [
                "Siteng Huang",
                "Biao Gong",
                "Yutong Feng",
                "Xi Chen",
                "Yuqian Fu",
                "Yu Liu",
                "Donglin Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15841v2",
                "http://arxiv.org/pdf/2311.15841v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15839v1",
            "title": "Ontologising Trustworthy in the Telecommunications Domain",
            "updated": "2023-11-27T14:04:46Z",
            "published": "2023-11-27T14:04:46Z",
            "summary": "Based upon trusted and confidential computing platforms, telecommunications\nsystems must provide guaranteed security for the processes and data running\natop them. This in turn requires us to provide trustworthy systems. The term\ntrustworthy is poorly defined with corresponding misunderstanding and\nmisapplication. We present a definition of this term, as well as others,\ndemonstrate its application against certain telecommunications use cases and\naddress how the learnings from ontologising these structures contribute to\nstandardisation and the necessity for FAIR ontologies across telecommunications\nstandards and hosting organisations.",
            "author": [
                "Ian Oliver",
                "Pekka Kuure",
                "Wiktor Sedkowski",
                "Thore Sommer"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15839v1",
                "http://arxiv.org/pdf/2311.15839v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.SE",
                "D.2.1"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15838v1",
            "title": "Utilizing Explainability Techniques for Reinforcement Learning Model\n  Assurance",
            "updated": "2023-11-27T14:02:47Z",
            "published": "2023-11-27T14:02:47Z",
            "summary": "Explainable Reinforcement Learning (XRL) can provide transparency into the\ndecision-making process of a Deep Reinforcement Learning (DRL) model and\nincrease user trust and adoption in real-world use cases. By utilizing XRL\ntechniques, researchers can identify potential vulnerabilities within a trained\nDRL model prior to deployment, therefore limiting the potential for mission\nfailure or mistakes by the system. This paper introduces the ARLIN (Assured RL\nModel Interrogation) Toolkit, an open-source Python library that identifies\npotential vulnerabilities and critical points within trained DRL models through\ndetailed, human-interpretable explainability outputs. To illustrate ARLIN's\neffectiveness, we provide explainability visualizations and vulnerability\nanalysis for a publicly available DRL model. The open-source code repository is\navailable for download at https://github.com/mitre/arlin.",
            "author": [
                "Alexander Tapley",
                "Kyle Gatesman",
                "Luis Robaina",
                "Brett Bissey",
                "Joseph Weissman"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15838v1",
                "http://arxiv.org/pdf/2311.15838v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15836v1",
            "title": "Syn3DWound: A Synthetic Dataset for 3D Wound Bed Analysis",
            "updated": "2023-11-27T13:59:53Z",
            "published": "2023-11-27T13:59:53Z",
            "summary": "Wound management poses a significant challenge, particularly for bedridden\npatients and the elderly. Accurate diagnostic and healing monitoring can\nsignificantly benefit from modern image analysis, providing accurate and\nprecise measurements of wounds. Despite several existing techniques, the\nshortage of expansive and diverse training datasets remains a significant\nobstacle to constructing machine learning-based frameworks. This paper\nintroduces Syn3DWound, an open-source dataset of high-fidelity simulated wounds\nwith 2D and 3D annotations. We propose baseline methods and a benchmarking\nframework for automated 3D morphometry analysis and 2D/3D wound segmentation.",
            "author": [
                "L\u00e9o Lebrat",
                "Rodrigo Santa Cruz",
                "Remi Chierchia",
                "Yulia Arzhaeva",
                "Mohammad Ali Armin",
                "Joshua Goldsmith",
                "Jeremy Oorloff",
                "Prithvi Reddy",
                "Chuong Nguyen",
                "Lars Petersson",
                "Michelle Barakat-Johnson",
                "Georgina Luscombe",
                "Clinton Fookes",
                "Olivier Salvado",
                "David Ahmedt-Aristizabal"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15836v1",
                "http://arxiv.org/pdf/2311.15836v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15831v1",
            "title": "Temporal Action Localization for Inertial-based Human Activity\n  Recognition",
            "updated": "2023-11-27T13:55:21Z",
            "published": "2023-11-27T13:55:21Z",
            "summary": "A persistent trend in Deep Learning has been the applicability of machine\nlearning concepts to other areas than originally introduced for. As of today,\nstate-of-the-art activity recognition from wearable sensors relies on\nclassifiers being trained on fixed windows of data. Contrarily, video-based\nHuman Activity Recognition has followed a segment-based prediction approach,\nlocalizing activity occurrences from start to end. This paper is the first to\nsystematically demonstrate the applicability of state-of-the-art TAL models for\nwearable Human Activity Recongition (HAR) using raw inertial data as input. Our\nresults show that state-of-the-art TAL models are able to outperform popular\ninertial models on 4 out of 6 wearable activity recognition benchmark datasets,\nwith improvements ranging as much as 25% in F1-score. Introducing the TAL\ncommunity's most popular metric to inertial-based HAR, namely mean Average\nPrecision, our analysis shows that TAL models are able to produce more coherent\nsegments along with an overall higher NULL-class accuracy across all datasets.\nBeing the first to provide such an analysis, the TAL community offers an\ninteresting new perspective to inertial-based HAR with yet to be explored\ndesign choices and training concepts, which could be of significant value for\nthe inertial-based HAR community.",
            "author": [
                "Marius Bock",
                "Michael Moeller",
                "Kristof Van Laerhoven"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15831v1",
                "http://arxiv.org/pdf/2311.15831v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.HC",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15830v2",
            "title": "A-JEPA: Joint-Embedding Predictive Architecture Can Listen",
            "updated": "2023-11-28T03:15:50Z",
            "published": "2023-11-27T13:53:53Z",
            "summary": "This paper presents that the masked-modeling principle driving the success of\nlarge foundational vision models can be effectively applied to audio by making\npredictions in a latent space. We introduce Audio-based Joint-Embedding\nPredictive Architecture (A-JEPA), a simple extension method for self-supervised\nlearning from the audio spectrum. Following the design of I-JEPA, our A-JEPA\nencodes visible audio spectrogram patches with a curriculum masking strategy\nvia context encoder, and predicts the representations of regions sampled at\nwell-designed locations. The target representations of those regions are\nextracted by the exponential moving average of context encoder, \\emph{i.e.},\ntarget encoder, on the whole spectrogram. We find it beneficial to transfer\nrandom block masking into time-frequency aware masking in a curriculum manner,\nconsidering the complexity of highly correlated in local time and frequency in\naudio spectrograms. To enhance contextual semantic understanding and\nrobustness, we fine-tune the encoder with a regularized masking on target\ndatasets, instead of input dropping or zero. Empirically, when built with\nVision Transformers structure, we find A-JEPA to be highly scalable and sets\nnew state-of-the-art performance on multiple audio and speech classification\ntasks, outperforming other recent models that use externally supervised\npre-training.",
            "author": [
                "Zhengcong Fei",
                "Mingyuan Fan",
                "Junshi Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15830v2",
                "http://arxiv.org/pdf/2311.15830v2"
            ],
            "primary_category": "cs.SD",
            "category": [
                "cs.SD",
                "cs.CV",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15822v1",
            "title": "Data reconstruction for complex flows using AI: recent progress,\n  obstacles, and perspectives",
            "updated": "2023-11-27T13:47:05Z",
            "published": "2023-11-27T13:47:05Z",
            "summary": "In recent years the fluid mechanics community has been intensely focused on\npursuing solutions to its long-standing open problems by exploiting the new\nmachine learning, (ML), approaches. The exchange between ML and fluid mechanics\nis bringing important paybacks in both directions. The first is benefiting from\nnew physics-inspired ML methods and a scientific playground to perform\nquantitative benchmarks, whilst the latter has been open to a large set of new\ntools inherently well suited to deal with big data, flexible in scope, and\ncapable of revealing unknown correlations. A special case is the problem of\nmodeling missing information of partially observable systems. The aim of this\npaper is to review some of the ML algorithms that are playing an important role\nin the current developments in this field, to uncover potential avenues, and to\ndiscuss the open challenges for applications to fluid mechanics.",
            "author": [
                "Michele Buzzicotti"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15822v1",
                "http://arxiv.org/pdf/2311.15822v1"
            ],
            "primary_category": "physics.flu-dyn",
            "category": [
                "physics.flu-dyn",
                "physics.comp-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15816v1",
            "title": "Scale-Dropout: Estimating Uncertainty in Deep Neural Networks Using\n  Stochastic Scale",
            "updated": "2023-11-27T13:41:20Z",
            "published": "2023-11-27T13:41:20Z",
            "summary": "Uncertainty estimation in Neural Networks (NNs) is vital in improving\nreliability and confidence in predictions, particularly in safety-critical\napplications. Bayesian Neural Networks (BayNNs) with Dropout as an\napproximation offer a systematic approach to quantifying uncertainty, but they\ninherently suffer from high hardware overhead in terms of power, memory, and\ncomputation. Thus, the applicability of BayNNs to edge devices with limited\nresources or to high-performance applications is challenging. Some of the\ninherent costs of BayNNs can be reduced by accelerating them in hardware on a\nComputation-In-Memory (CIM) architecture with spintronic memories and\nbinarizing their parameters. However, numerous stochastic units are required to\nimplement conventional dropout-based BayNN. In this paper, we propose the Scale\nDropout, a novel regularization technique for Binary Neural Networks (BNNs),\nand Monte Carlo-Scale Dropout (MC-Scale Dropout)-based BayNNs for efficient\nuncertainty estimation. Our approach requires only one stochastic unit for the\nentire model, irrespective of the model size, leading to a highly scalable\nBayesian NN. Furthermore, we introduce a novel Spintronic memory-based CIM\narchitecture for the proposed BayNN that achieves more than $100\\times$ energy\nsavings compared to the state-of-the-art. We validated our method to show up to\na $1\\%$ improvement in predictive performance and superior uncertainty\nestimates compared to related works.",
            "author": [
                "Soyed Tuhin Ahmed",
                "Kamal Danouchi",
                "Michael Hefenbrock",
                "Guillaume Prenat",
                "Lorena Anghel",
                "Mehdi B. Tahoori"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15816v1",
                "http://arxiv.org/pdf/2311.15816v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.ET"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15812v1",
            "title": "C-SAW: Self-Supervised Prompt Learning for Image Generalization in\n  Remote Sensing",
            "updated": "2023-11-27T13:35:20Z",
            "published": "2023-11-27T13:35:20Z",
            "summary": "We focus on domain and class generalization problems in analyzing optical\nremote sensing images, using the large-scale pre-trained vision-language model\n(VLM), CLIP. While contrastively trained VLMs show impressive zero-shot\ngeneralization performance, their effectiveness is limited when dealing with\ndiverse domains during training and testing. Existing prompt learning\ntechniques overlook the importance of incorporating domain and content\ninformation into the prompts, which results in a drop in performance while\ndealing with such multi-domain data. To address these challenges, we propose a\nsolution that ensures domain-invariant prompt learning while enhancing the\nexpressiveness of visual features. We observe that CLIP's vision encoder\nstruggles to identify contextual image information, particularly when image\npatches are jumbled up. This issue is especially severe in optical remote\nsensing images, where land-cover classes exhibit well-defined contextual\nappearances. To this end, we introduce C-SAW, a method that complements CLIP\nwith a self-supervised loss in the visual space and a novel prompt learning\ntechnique that emphasizes both visual domain and content-specific features. We\nkeep the CLIP backbone frozen and introduce a small set of projectors for both\nthe CLIP encoders to train C-SAW contrastively. Experimental results\ndemonstrate the superiority of C-SAW across multiple remote sensing benchmarks\nand different generalization tasks.",
            "author": [
                "Avigyan Bhattacharya",
                "Mainak Singha",
                "Ankit Jha",
                "Biplab Banerjee"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15812v1",
                "http://arxiv.org/pdf/2311.15812v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15810v1",
            "title": "Tascade: Hardware Support for Atomic-free, Asynchronous and Efficient\n  Reduction Trees",
            "updated": "2023-11-27T13:32:33Z",
            "published": "2023-11-27T13:32:33Z",
            "summary": "As system parallelism at chip- and server-level increases, challenges that\narose with network-level systems a decade ago, are now being encountered with\nthese massively parallel systems that have become an important workhorse for\nMachine Learning workloads as well as Graph and Sparse workloads. To tackle the\ncommunication bottlenecks, recent works have introduced task-based\nparallelization schemes to accelerate graph search and sparse data-structure\ntraversal, where some solutions scale up to thousands of processing units (PUs)\non a single chip. However, existing communication schemes do not scale to\nlarger than thousands of processing tiles. To address these challenges we\npropose Tascade, a system that offers hardware-supported, efficient and\nbalanced reduction trees to reduce communication overheads in task-based\nparallelization schemes and scales up to a million PUs. Tascade achieves this\nby implementing an execution model utilizing proxy regions and cascading\nupdates, along with a supporting hardware design that enables the execution of\nthe reduction tree at the chip level. The Tascade approach reduces overall\ncommunication and improves load balancing. We evaluate six applications and\nfour datasets to provide a detailed analysis of Tascade's performance, power,\nand traffic-reduction gains over prior work. Our parallelization of\nBreadth-First-Search with RMAT-26 across a million PUs, the largest of the\nliterature, reaches 5305 GTEPS.",
            "author": [
                "Marcelo Orenes-Vera",
                "Esin Tureci",
                "David Wentzlaff",
                "Margaret Martonosi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15810v1",
                "http://arxiv.org/pdf/2311.15810v1"
            ],
            "primary_category": "cs.AR",
            "category": [
                "cs.AR",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15809v1",
            "title": "From deepfake to deep useful: risks and opportunities through a\n  systematic literature review",
            "updated": "2023-11-27T13:31:40Z",
            "published": "2023-11-27T13:31:40Z",
            "summary": "Deepfake videos are defined as a resulting media from the synthesis of\ndifferent persons images and videos, mostly faces, replacing a real one. The\neasy spread of such videos leads to elevated misinformation and represents a\nthreat to society and democracy today. The present study aims to collect and\nanalyze the relevant literature through a systematic procedure. We present 27\narticles from scientific databases revealing threats to society, democracies,\nthe political life but present as well advantages of this technology in\nentertainment, gaming, education, and public life. The research indicates high\nscientific interest in deepfake detection algorithms as well as the ethical\naspect of such technology. This article covers the scientific gap since, to the\nbest of our knowledge, this is the first systematic literature review in the\nfield. A discussion has already started among academics and practitioners\nconcerning the spread of fake news. The next step of fake news considers the\nuse of artificial intelligence and machine learning algorithms that create\nhyper-realistic videos, called deepfake. Deepfake technology has continuously\nattracted the attention of scholars over the last 3 years more and more. The\nimportance of conducting research in this field derives from the necessity to\nunderstand the theory. The first contextual approach is related to the\nepistemological points of view of the concept. The second one is related to the\nphenomenological disadvantages of the field. Despite that, the authors will try\nto focus not only on the disadvantages of the field but also on the positive\naspects of the technology.",
            "author": [
                "Nikolaos Misirlis",
                "Harris Bin Munawar"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15809v1",
                "http://arxiv.org/pdf/2311.15809v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15807v1",
            "title": "Exploring Artificial Intelligence Methods for Energy Prediction in\n  Healthcare Facilities: An In-Depth Extended Systematic Review",
            "updated": "2023-11-27T13:30:20Z",
            "published": "2023-11-27T13:30:20Z",
            "summary": "Hospitals, due to their complexity and unique requirements, play a pivotal\nrole in global energy consumption patterns. This study conducted a\ncomprehensive literature review, utilizing the PRISMA framework, of articles\nthat employed machine learning and artificial intelligence techniques for\npredicting energy consumption in hospital buildings. Of the 1884 publications\nidentified, 17 were found to address this specific domain and have been\nthoroughly reviewed to establish the state-of-the-art and identify gaps where\nfuture research is needed. This review revealed a diverse range of data inputs\ninfluencing energy prediction, with occupancy and meteorological data emerging\nas significant predictors. However, many studies failed to delve deep into the\nimplications of their data choices, and gaps were evident regarding the\nunderstanding of time dynamics, operational status, and preprocessing methods.\nMachine learning, especially deep learning models like ANNs, have shown\npotential in this domain, yet they come with challenges, including\ninterpretability and computational demands. The findings underscore the immense\npotential of AI in optimizing hospital energy consumption but also highlight\nthe need for more comprehensive and granular research. Key areas for future\nresearch include the optimization of ANN approaches, new optimization and data\nintegration techniques, the integration of real-time data into Intelligent\nEnergy Management Systems, and increasing focus on long-term energy\nforecasting.",
            "author": [
                "Marjan FatehiJananloo",
                "Helen Stopps",
                "J. J. McArthur"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15807v1",
                "http://arxiv.org/pdf/2311.15807v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.SY",
                "eess.SY",
                "A.1; I.2; J.2"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15792v1",
            "title": "Rethinking Privacy in Machine Learning Pipelines from an Information\n  Flow Control Perspective",
            "updated": "2023-11-27T13:14:39Z",
            "published": "2023-11-27T13:14:39Z",
            "summary": "Modern machine learning systems use models trained on ever-growing corpora.\nTypically, metadata such as ownership, access control, or licensing information\nis ignored during training. Instead, to mitigate privacy risks, we rely on\ngeneric techniques such as dataset sanitization and differentially private\nmodel training, with inherent privacy/utility trade-offs that hurt model\nperformance. Moreover, these techniques have limitations in scenarios where\nsensitive information is shared across multiple participants and fine-grained\naccess control is required. By ignoring metadata, we therefore miss an\nopportunity to better address security, privacy, and confidentiality\nchallenges. In this paper, we take an information flow control perspective to\ndescribe machine learning systems, which allows us to leverage metadata such as\naccess control policies and define clear-cut privacy and confidentiality\nguarantees with interpretable information flows. Under this perspective, we\ncontrast two different approaches to achieve user-level non-interference: 1)\nfine-tuning per-user models, and 2) retrieval augmented models that access\nuser-specific datasets at inference time. We compare these two approaches to a\ntrivially non-interfering zero-shot baseline using a public model and to a\nbaseline that fine-tunes this model on the whole corpus. We evaluate trained\nmodels on two datasets of scientific articles and demonstrate that retrieval\naugmented architectures deliver the best utility, scalability, and flexibility\nwhile satisfying strict non-interference guarantees.",
            "author": [
                "Lukas Wutschitz",
                "Boris K\u00f6pf",
                "Andrew Paverd",
                "Saravan Rajmohan",
                "Ahmed Salem",
                "Shruti Tople",
                "Santiago Zanella-B\u00e9guelin",
                "Menglin Xia",
                "Victor R\u00fchle"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15792v1",
                "http://arxiv.org/pdf/2311.15792v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16514v1",
            "title": "Video Anomaly Detection via Spatio-Temporal Pseudo-Anomaly Generation :\n  A Unified Approach",
            "updated": "2023-11-27T13:14:06Z",
            "published": "2023-11-27T13:14:06Z",
            "summary": "Video Anomaly Detection (VAD) is an open-set recognition task, which is\nusually formulated as a one-class classification (OCC) problem, where training\ndata is comprised of videos with normal instances while test data contains both\nnormal and anomalous instances. Recent works have investigated the creation of\npseudo-anomalies (PAs) using only the normal data and making strong assumptions\nabout real-world anomalies with regards to abnormality of objects and speed of\nmotion to inject prior information about anomalies in an autoencoder (AE) based\nreconstruction model during training. This work proposes a novel method for\ngenerating generic spatio-temporal PAs by inpainting a masked out region of an\nimage using a pre-trained Latent Diffusion Model and further perturbing the\noptical flow using mixup to emulate spatio-temporal distortions in the data. In\naddition, we present a simple unified framework to detect real-world anomalies\nunder the OCC setting by learning three types of anomaly indicators, namely\nreconstruction quality, temporal irregularity and semantic inconsistency.\nExtensive experiments on four VAD benchmark datasets namely Ped2, Avenue,\nShanghaiTech and UBnormal demonstrate that our method performs on par with\nother existing state-of-the-art PAs generation and reconstruction based methods\nunder the OCC setting. Our analysis also examines the transferability and\ngeneralisation of PAs across these datasets, offering valuable insights by\nidentifying real-world anomalies through PAs.",
            "author": [
                "Ayush K. Rai",
                "Tarun Krishna",
                "Feiyan Hu",
                "Alexandru Drimbarean",
                "Kevin McGuinness",
                "Alan F. Smeaton",
                "Noel E. O'Connor"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16514v1",
                "http://arxiv.org/pdf/2311.16514v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    }
]