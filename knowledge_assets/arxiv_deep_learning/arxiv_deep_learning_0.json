[
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04563v1",
            "title": "Visual Geometry Grounded Deep Structure From Motion",
            "updated": "2023-12-07T18:59:52Z",
            "published": "2023-12-07T18:59:52Z",
            "summary": "Structure-from-motion (SfM) is a long-standing problem in the computer vision\ncommunity, which aims to reconstruct the camera poses and 3D structure of a\nscene from a set of unconstrained 2D images. Classical frameworks solve this\nproblem in an incremental manner by detecting and matching keypoints,\nregistering images, triangulating 3D points, and conducting bundle adjustment.\nRecent research efforts have predominantly revolved around harnessing the power\nof deep learning techniques to enhance specific elements (e.g., keypoint\nmatching), but are still based on the original, non-differentiable pipeline.\nInstead, we propose a new deep pipeline VGGSfM, where each component is fully\ndifferentiable and thus can be trained in an end-to-end manner. To this end, we\nintroduce new mechanisms and simplifications. First, we build on recent\nadvances in deep 2D point tracking to extract reliable pixel-accurate tracks,\nwhich eliminates the need for chaining pairwise matches. Furthermore, we\nrecover all cameras simultaneously based on the image and track features\ninstead of gradually registering cameras. Finally, we optimise the cameras and\ntriangulate 3D points via a differentiable bundle adjustment layer. We attain\nstate-of-the-art performance on three popular datasets, CO3D, IMC Phototourism,\nand ETH3D.",
            "author": [
                "Jianyuan Wang",
                "Nikita Karaev",
                "Christian Rupprecht",
                "David Novotny"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04563v1",
                "http://arxiv.org/pdf/2312.04563v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04561v1",
            "title": "GenDeF: Learning Generative Deformation Field for Video Generation",
            "updated": "2023-12-07T18:59:41Z",
            "published": "2023-12-07T18:59:41Z",
            "summary": "We offer a new perspective on approaching the task of video generation.\nInstead of directly synthesizing a sequence of frames, we propose to render a\nvideo by warping one static image with a generative deformation field (GenDeF).\nSuch a pipeline enjoys three appealing advantages. First, we can sufficiently\nreuse a well-trained image generator to synthesize the static image (also\ncalled canonical image), alleviating the difficulty in producing a video and\nthereby resulting in better visual quality. Second, we can easily convert a\ndeformation field to optical flows, making it possible to apply explicit\nstructural regularizations for motion modeling, leading to temporally\nconsistent results. Third, the disentanglement between content and motion\nallows users to process a synthesized video through processing its\ncorresponding static image without any tuning, facilitating many applications\nlike video editing, keypoint tracking, and video segmentation. Both qualitative\nand quantitative results on three common video generation benchmarks\ndemonstrate the superiority of our GenDeF method.",
            "author": [
                "Wen Wang",
                "Kecheng Zheng",
                "Qiuyu Wang",
                "Hao Chen",
                "Zifan Shi",
                "Ceyuan Yang",
                "Yujun Shen",
                "Chunhua Shen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04561v1",
                "http://arxiv.org/pdf/2312.04561v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04558v1",
            "title": "MonoGaussianAvatar: Monocular Gaussian Point-based Head Avatar",
            "updated": "2023-12-07T18:59:31Z",
            "published": "2023-12-07T18:59:31Z",
            "summary": "The ability to animate photo-realistic head avatars reconstructed from\nmonocular portrait video sequences represents a crucial step in bridging the\ngap between the virtual and real worlds. Recent advancements in head avatar\ntechniques, including explicit 3D morphable meshes (3DMM), point clouds, and\nneural implicit representation have been exploited for this ongoing research.\nHowever, 3DMM-based methods are constrained by their fixed topologies,\npoint-based approaches suffer from a heavy training burden due to the extensive\nquantity of points involved, and the last ones suffer from limitations in\ndeformation flexibility and rendering efficiency. In response to these\nchallenges, we propose MonoGaussianAvatar (Monocular Gaussian Point-based Head\nAvatar), a novel approach that harnesses 3D Gaussian point representation\ncoupled with a Gaussian deformation field to learn explicit head avatars from\nmonocular portrait videos. We define our head avatars with Gaussian points\ncharacterized by adaptable shapes, enabling flexible topology. These points\nexhibit movement with a Gaussian deformation field in alignment with the target\npose and expression of a person, facilitating efficient deformation.\nAdditionally, the Gaussian points have controllable shape, size, color, and\nopacity combined with Gaussian splatting, allowing for efficient training and\nrendering. Experiments demonstrate the superior performance of our method,\nwhich achieves state-of-the-art results among previous methods.",
            "author": [
                "Yufan Chen",
                "Lizhen Wang",
                "Qijing Li",
                "Hongjiang Xiao",
                "Shengping Zhang",
                "Hongxun Yao",
                "Yebin Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04558v1",
                "http://arxiv.org/pdf/2312.04558v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04557v1",
            "title": "GenTron: Delving Deep into Diffusion Transformers for Image and Video\n  Generation",
            "updated": "2023-12-07T18:59:30Z",
            "published": "2023-12-07T18:59:30Z",
            "summary": "In this study, we explore Transformer-based diffusion models for image and\nvideo generation. Despite the dominance of Transformer architectures in various\nfields due to their flexibility and scalability, the visual generative domain\nprimarily utilizes CNN-based U-Net architectures, particularly in\ndiffusion-based models. We introduce GenTron, a family of Generative models\nemploying Transformer-based diffusion, to address this gap. Our initial step\nwas to adapt Diffusion Transformers (DiTs) from class to text conditioning, a\nprocess involving thorough empirical exploration of the conditioning mechanism.\nWe then scale GenTron from approximately 900M to over 3B parameters, observing\nsignificant improvements in visual quality. Furthermore, we extend GenTron to\ntext-to-video generation, incorporating novel motion-free guidance to enhance\nvideo quality. In human evaluations against SDXL, GenTron achieves a 51.1% win\nrate in visual quality (with a 19.8% draw rate), and a 42.3% win rate in text\nalignment (with a 42.9% draw rate). GenTron also excels in the T2I-CompBench,\nunderscoring its strengths in compositional generation. We believe this work\nwill provide meaningful insights and serve as a valuable reference for future\nresearch.",
            "author": [
                "Shoufa Chen",
                "Mengmeng Xu",
                "Jiawei Ren",
                "Yuren Cong",
                "Sen He",
                "Yanping Xie",
                "Animesh Sinha",
                "Ping Luo",
                "Tao Xiang",
                "Juan-Manuel Perez-Rua"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04557v1",
                "http://arxiv.org/pdf/2312.04557v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04556v1",
            "title": "Large Language Models for Mathematicians",
            "updated": "2023-12-07T18:59:29Z",
            "published": "2023-12-07T18:59:29Z",
            "summary": "Large language models (LLMs) such as ChatGPT have received immense interest\nfor their general-purpose language understanding and, in particular, their\nability to generate high-quality text or computer code. For many professions,\nLLMs represent an invaluable tool that can speed up and improve the quality of\nwork. In this note, we discuss to what extent they can aid professional\nmathematicians. We first provide a mathematical description of the transformer\nmodel used in all modern language models. Based on recent studies, we then\noutline best practices and potential issues and report on the mathematical\nabilities of language models. Finally, we shed light on the potential of LMMs\nto change how mathematicians work.",
            "author": [
                "Simon Frieder",
                "Julius Berner",
                "Philipp Petersen",
                "Thomas Lukasiewicz"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04556v1",
                "http://arxiv.org/pdf/2312.04556v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG",
                "math.HO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04554v1",
            "title": "Improved Visual Grounding through Self-Consistent Explanations",
            "updated": "2023-12-07T18:59:22Z",
            "published": "2023-12-07T18:59:22Z",
            "summary": "Vision-and-language models trained to match images with text can be combined\nwith visual explanation methods to point to the locations of specific objects\nin an image. Our work shows that the localization --\"grounding\"-- abilities of\nthese models can be further improved by finetuning for self-consistent visual\nexplanations. We propose a strategy for augmenting existing text-image datasets\nwith paraphrases using a large language model, and SelfEQ, a weakly-supervised\nstrategy on visual explanation maps for paraphrases that encourages\nself-consistency. Specifically, for an input textual phrase, we attempt to\ngenerate a paraphrase and finetune the model so that the phrase and paraphrase\nmap to the same region in the image. We posit that this both expands the\nvocabulary that the model is able to handle, and improves the quality of the\nobject locations highlighted by gradient-based visual explanation methods (e.g.\nGradCAM). We demonstrate that SelfEQ improves performance on Flickr30k,\nReferIt, and RefCOCO+ over a strong baseline method and several prior works.\nParticularly, comparing to other methods that do not use any type of box\nannotations, we obtain 84.07% on Flickr30k (an absolute improvement of 4.69%),\n67.40% on ReferIt (an absolute improvement of 7.68%), and 75.10%, 55.49% on\nRefCOCO+ test sets A and B respectively (an absolute improvement of 3.74% on\naverage).",
            "author": [
                "Ruozhen He",
                "Paola Cascante-Bonilla",
                "Ziyan Yang",
                "Alexander C. Berg",
                "Vicente Ordonez"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04554v1",
                "http://arxiv.org/pdf/2312.04554v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04552v1",
            "title": "Generating Illustrated Instructions",
            "updated": "2023-12-07T18:59:20Z",
            "published": "2023-12-07T18:59:20Z",
            "summary": "We introduce the new task of generating Illustrated Instructions, i.e.,\nvisual instructions customized to a user's needs. We identify desiderata unique\nto this task, and formalize it through a suite of automatic and human\nevaluation metrics, designed to measure the validity, consistency, and efficacy\nof the generations. We combine the power of large language models (LLMs)\ntogether with strong text-to-image generation diffusion models to propose a\nsimple approach called StackedDiffusion, which generates such illustrated\ninstructions given text as input. The resulting model strongly outperforms\nbaseline approaches and state-of-the-art multimodal LLMs; and in 30% of cases,\nusers even prefer it to human-generated articles. Most notably, it enables\nvarious new and exciting applications far beyond what static articles on the\nweb can provide, such as personalized instructions complete with intermediate\nsteps and pictures in response to a user's individual situation.",
            "author": [
                "Sachit Menon",
                "Ishan Misra",
                "Rohit Girdhar"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04552v1",
                "http://arxiv.org/pdf/2312.04552v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG",
                "cs.MM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04548v1",
            "title": "Multiview Aerial Visual Recognition (MAVREC): Can Multi-view Improve\n  Aerial Visual Perception?",
            "updated": "2023-12-07T18:59:14Z",
            "published": "2023-12-07T18:59:14Z",
            "summary": "Despite the commercial abundance of UAVs, aerial data acquisition remains\nchallenging, and the existing Asia and North America-centric open-source UAV\ndatasets are small-scale or low-resolution and lack diversity in scene\ncontextuality. Additionally, the color content of the scenes, solar-zenith\nangle, and population density of different geographies influence the data\ndiversity. These two factors conjointly render suboptimal aerial-visual\nperception of the deep neural network (DNN) models trained primarily on the\nground-view data, including the open-world foundational models.\n  To pave the way for a transformative era of aerial detection, we present\nMultiview Aerial Visual RECognition or MAVREC, a video dataset where we record\nsynchronized scenes from different perspectives -- ground camera and\ndrone-mounted camera. MAVREC consists of around 2.5 hours of industry-standard\n2.7K resolution video sequences, more than 0.5 million frames, and 1.1 million\nannotated bounding boxes. This makes MAVREC the largest ground and aerial-view\ndataset, and the fourth largest among all drone-based datasets across all\nmodalities and tasks. Through our extensive benchmarking on MAVREC, we\nrecognize that augmenting object detectors with ground-view images from the\ncorresponding geographical location is a superior pre-training strategy for\naerial detection. Building on this strategy, we benchmark MAVREC with a\ncurriculum-based semi-supervised object detection approach that leverages\nlabeled (ground and aerial) and unlabeled (only aerial) images to enhance the\naerial detection. We publicly release the MAVREC dataset:\nhttps://mavrec.github.io.",
            "author": [
                "Aritra Dutta",
                "Srijan Das",
                "Jacob Nielsen",
                "Rajatsubhra Chakraborty",
                "Mubarak Shah"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04548v1",
                "http://arxiv.org/pdf/2312.04548v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG",
                "I.4.0; I.4.8; I.5.1; I.5.4; I.2.10"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04549v1",
            "title": "PlayFusion: Skill Acquisition via Diffusion from Language-Annotated Play",
            "updated": "2023-12-07T18:59:14Z",
            "published": "2023-12-07T18:59:14Z",
            "summary": "Learning from unstructured and uncurated data has become the dominant\nparadigm for generative approaches in language and vision. Such unstructured\nand unguided behavior data, commonly known as play, is also easier to collect\nin robotics but much more difficult to learn from due to its inherently\nmultimodal, noisy, and suboptimal nature. In this paper, we study this problem\nof learning goal-directed skill policies from unstructured play data which is\nlabeled with language in hindsight. Specifically, we leverage advances in\ndiffusion models to learn a multi-task diffusion model to extract robotic\nskills from play data. Using a conditional denoising diffusion process in the\nspace of states and actions, we can gracefully handle the complexity and\nmultimodality of play data and generate diverse and interesting robot\nbehaviors. To make diffusion models more useful for skill learning, we\nencourage robotic agents to acquire a vocabulary of skills by introducing\ndiscrete bottlenecks into the conditional behavior generation process. In our\nexperiments, we demonstrate the effectiveness of our approach across a wide\nvariety of environments in both simulation and the real world. Results\nvisualizations and videos at https://play-fusion.github.io",
            "author": [
                "Lili Chen",
                "Shikhar Bahl",
                "Deepak Pathak"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04549v1",
                "http://arxiv.org/pdf/2312.04549v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI",
                "cs.CV",
                "cs.LG",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04546v1",
            "title": "Adversarial Learning for Feature Shift Detection and Correction",
            "updated": "2023-12-07T18:58:40Z",
            "published": "2023-12-07T18:58:40Z",
            "summary": "Data shift is a phenomenon present in many real-world applications, and while\nthere are multiple methods attempting to detect shifts, the task of localizing\nand correcting the features originating such shifts has not been studied in\ndepth. Feature shifts can occur in many datasets, including in multi-sensor\ndata, where some sensors are malfunctioning, or in tabular and structured data,\nincluding biomedical, financial, and survey data, where faulty standardization\nand data processing pipelines can lead to erroneous features. In this work, we\nexplore using the principles of adversarial learning, where the information\nfrom several discriminators trained to distinguish between two distributions is\nused to both detect the corrupted features and fix them in order to remove the\ndistribution shift between datasets. We show that mainstream supervised\nclassifiers, such as random forest or gradient boosting trees, combined with\nsimple iterative heuristics, can localize and correct feature shifts,\noutperforming current statistical and neural network-based techniques. The code\nis available at https://github.com/AI-sandbox/DataFix.",
            "author": [
                "Miriam Barrabes",
                "Daniel Mas Montserrat",
                "Margarita Geleta",
                "Xavier Giro-i-Nieto",
                "Alexander G. Ioannidis"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04546v1",
                "http://arxiv.org/pdf/2312.04546v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "stat.AP",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04543v1",
            "title": "HyperDreamer: Hyper-Realistic 3D Content Generation and Editing from a\n  Single Image",
            "updated": "2023-12-07T18:58:09Z",
            "published": "2023-12-07T18:58:09Z",
            "summary": "3D content creation from a single image is a long-standing yet highly\ndesirable task. Recent advances introduce 2D diffusion priors, yielding\nreasonable results. However, existing methods are not hyper-realistic enough\nfor post-generation usage, as users cannot view, render and edit the resulting\n3D content from a full range. To address these challenges, we introduce\nHyperDreamer with several key designs and appealing properties: 1) Viewable:\n360 degree mesh modeling with high-resolution textures enables the creation of\nvisually compelling 3D models from a full range of observation points. 2)\nRenderable: Fine-grained semantic segmentation and data-driven priors are\nincorporated as guidance to learn reasonable albedo, roughness, and specular\nproperties of the materials, enabling semantic-aware arbitrary material\nestimation. 3) Editable: For a generated model or their own data, users can\ninteractively select any region via a few clicks and efficiently edit the\ntexture with text-based guidance. Extensive experiments demonstrate the\neffectiveness of HyperDreamer in modeling region-aware materials with\nhigh-resolution textures and enabling user-friendly editing. We believe that\nHyperDreamer holds promise for advancing 3D content creation and finding\napplications in various domains.",
            "author": [
                "Tong Wu",
                "Zhibing Li",
                "Shuai Yang",
                "Pan Zhang",
                "Xinggang Pan",
                "Jiaqi Wang",
                "Dahua Lin",
                "Ziwei Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04543v1",
                "http://arxiv.org/pdf/2312.04543v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04542v1",
            "title": "SoK: Unintended Interactions among Machine Learning Defenses and Risks",
            "updated": "2023-12-07T18:57:36Z",
            "published": "2023-12-07T18:57:36Z",
            "summary": "Machine learning (ML) models cannot neglect risks to security, privacy, and\nfairness. Several defenses have been proposed to mitigate such risks. When a\ndefense is effective in mitigating one risk, it may correspond to increased or\ndecreased susceptibility to other risks. Existing research lacks an effective\nframework to recognize and explain these unintended interactions. We present\nsuch a framework, based on the conjecture that overfitting and memorization\nunderlie unintended interactions. We survey existing literature on unintended\ninteractions, accommodating them within our framework. We use our framework to\nconjecture on two previously unexplored interactions, and empirically validate\nour conjectures.",
            "author": [
                "Vasisht Duddu",
                "Sebastian Szyller",
                "N. Asokan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04542v1",
                "http://arxiv.org/pdf/2312.04542v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04540v1",
            "title": "Sim-to-Real Causal Transfer: A Metric Learning Approach to\n  Causally-Aware Interaction Representations",
            "updated": "2023-12-07T18:57:03Z",
            "published": "2023-12-07T18:57:03Z",
            "summary": "Modeling spatial-temporal interactions among neighboring agents is at the\nheart of multi-agent problems such as motion forecasting and crowd navigation.\nDespite notable progress, it remains unclear to which extent modern\nrepresentations can capture the causal relationships behind agent interactions.\nIn this work, we take an in-depth look at the causal awareness of these\nrepresentations, from computational formalism to real-world practice. First, we\ncast doubt on the notion of non-causal robustness studied in the recent\nCausalAgents benchmark. We show that recent representations are already\npartially resilient to perturbations of non-causal agents, and yet modeling\nindirect causal effects involving mediator agents remains challenging. To\naddress this challenge, we introduce a metric learning approach that\nregularizes latent representations with causal annotations. Our controlled\nexperiments show that this approach not only leads to higher degrees of causal\nawareness but also yields stronger out-of-distribution robustness. To further\noperationalize it in practice, we propose a sim-to-real causal transfer method\nvia cross-domain multi-task learning. Experiments on pedestrian datasets show\nthat our method can substantially boost generalization, even in the absence of\nreal-world causal annotations. We hope our work provides a new perspective on\nthe challenges and potential pathways towards causally-aware representations of\nmulti-agent interactions. Our code is available at\nhttps://github.com/socialcausality.",
            "author": [
                "Yuejiang Liu",
                "Ahmad Rahimi",
                "Po-Chien Luan",
                "Frano Raji\u010d",
                "Alexandre Alahi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04540v1",
                "http://arxiv.org/pdf/2312.04540v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CV",
                "cs.MA",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04535v1",
            "title": "Trajeglish: Learning the Language of Driving Scenarios",
            "updated": "2023-12-07T18:53:27Z",
            "published": "2023-12-07T18:53:27Z",
            "summary": "A longstanding challenge for self-driving development is simulating dynamic\ndriving scenarios seeded from recorded driving logs. In pursuit of this\nfunctionality, we apply tools from discrete sequence modeling to model how\nvehicles, pedestrians and cyclists interact in driving scenarios. Using a\nsimple data-driven tokenization scheme, we discretize trajectories to\ncentimeter-level resolution using a small vocabulary. We then model the\nmulti-agent sequence of motion tokens with a GPT-like encoder-decoder that is\nautoregressive in time and takes into account intra-timestep interaction\nbetween agents. Scenarios sampled from our model exhibit state-of-the-art\nrealism; our model tops the Waymo Sim Agents Benchmark, surpassing prior work\nalong the realism meta metric by 3.3% and along the interaction metric by 9.9%.\nWe ablate our modeling choices in full autonomy and partial autonomy settings,\nand show that the representations learned by our model can quickly be adapted\nto improve performance on nuScenes. We additionally evaluate the scalability of\nour model with respect to parameter count and dataset size, and use density\nestimates from our model to quantify the saliency of context length and\nintra-timestep interaction for the traffic modeling task.",
            "author": [
                "Jonah Philion",
                "Xue Bin Peng",
                "Sanja Fidler"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04535v1",
                "http://arxiv.org/pdf/2312.04535v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04533v1",
            "title": "Dream2Real: Zero-Shot 3D Object Rearrangement with Vision-Language\n  Models",
            "updated": "2023-12-07T18:51:19Z",
            "published": "2023-12-07T18:51:19Z",
            "summary": "We introduce Dream2Real, a robotics framework which integrates\nvision-language models (VLMs) trained on 2D data into a 3D object rearrangement\npipeline. This is achieved by the robot autonomously constructing a 3D\nrepresentation of the scene, where objects can be rearranged virtually and an\nimage of the resulting arrangement rendered. These renders are evaluated by a\nVLM, so that the arrangement which best satisfies the user instruction is\nselected and recreated in the real world with pick-and-place. This enables\nlanguage-conditioned rearrangement to be performed zero-shot, without needing\nto collect a training dataset of example arrangements. Results on a series of\nreal-world tasks show that this framework is robust to distractors,\ncontrollable by language, capable of understanding complex multi-object\nrelations, and readily applicable to both tabletop and 6-DoF rearrangement\ntasks.",
            "author": [
                "Ivan Kapelyukh",
                "Yifei Ren",
                "Ignacio Alzugaray",
                "Edward Johns"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04533v1",
                "http://arxiv.org/pdf/2312.04533v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04530v1",
            "title": "Camera Height Doesn't Change: Unsupervised Monocular Scale-Aware\n  Road-Scene Depth Estimation",
            "updated": "2023-12-07T18:50:01Z",
            "published": "2023-12-07T18:50:01Z",
            "summary": "Monocular depth estimators either require explicit scale supervision through\nauxiliary sensors or suffer from scale ambiguity, which renders them difficult\nto deploy in downstream applications. A possible source of scale is the sizes\nof objects found in the scene, but inaccurate localization makes them difficult\nto exploit. In this paper, we introduce a novel scale-aware monocular depth\nestimation method called StableCamH that does not require any auxiliary sensor\nor supervision. The key idea is to exploit prior knowledge of object heights in\nthe scene but aggregate the height cues into a single invariant measure common\nto all frames in a road video sequence, namely the camera height. By\nformulating monocular depth estimation as camera height optimization, we\nachieve robust and accurate unsupervised end-to-end training. To realize\nStableCamH, we devise a novel learning-based size prior that can directly\nconvert car appearance into its dimensions. Extensive experiments on KITTI and\nCityscapes show the effectiveness of StableCamH, its state-of-the-art accuracy\ncompared with related methods, and its generalizability. The training framework\nof StableCamH can be used for any monocular depth estimation method and will\nhopefully become a fundamental building block for further work.",
            "author": [
                "Genki Kinoshita",
                "Ko Nishino"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04530v1",
                "http://arxiv.org/pdf/2312.04530v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04529v1",
            "title": "Diffusion Reflectance Map: Single-Image Stochastic Inverse Rendering of\n  Illumination and Reflectance",
            "updated": "2023-12-07T18:50:00Z",
            "published": "2023-12-07T18:50:00Z",
            "summary": "Reflectance bounds the frequency spectrum of illumination in the object\nappearance. In this paper, we introduce the first stochastic inverse rendering\nmethod, which recovers the full frequency spectrum of an illumination jointly\nwith the object reflectance from a single image. Our key idea is to solve this\nblind inverse problem in the reflectance map, an appearance representation\ninvariant to the underlying geometry, by learning to reverse the image\nformation with a novel diffusion model which we refer to as the Diffusion\nReflectance Map Network (DRMNet). Given an observed reflectance map converted\nand completed from the single input image, DRMNet generates a reflectance map\ncorresponding to a perfect mirror sphere while jointly estimating the\nreflectance. The forward process can be understood as gradually filtering a\nnatural illumination with lower and lower frequency reflectance and additive\nGaussian noise. DRMNet learns to invert this process with two subnetworks,\nIllNet and RefNet, which work in concert towards this joint estimation. The\nnetwork is trained on an extensive synthetic dataset and is demonstrated to\ngeneralize to real images, showing state-of-the-art accuracy on established\ndatasets.",
            "author": [
                "Yuto Enyo",
                "Ko Nishino"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04529v1",
                "http://arxiv.org/pdf/2312.04529v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04528v1",
            "title": "Using Large Language Models for Hyperparameter Optimization",
            "updated": "2023-12-07T18:46:50Z",
            "published": "2023-12-07T18:46:50Z",
            "summary": "This paper studies using foundational large language models (LLMs) to make\ndecisions during hyperparameter optimization (HPO). Empirical evaluations\ndemonstrate that in settings with constrained search budgets, LLMs can perform\ncomparably or better than traditional HPO methods like random search and\nBayesian optimization on standard benchmarks. Furthermore, we propose to treat\nthe code specifying our model as a hyperparameter, which the LLM outputs, going\nbeyond the capabilities of existing HPO approaches. Our findings suggest that\nLLMs are a promising tool for improving efficiency in the traditional\ndecision-making problem of hyperparameter optimization.",
            "author": [
                "Michael R. Zhang",
                "Nishkrit Desai",
                "Juhan Bae",
                "Jonathan Lorraine",
                "Jimmy Ba"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04528v1",
                "http://arxiv.org/pdf/2312.04528v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04521v1",
            "title": "Multimodal Industrial Anomaly Detection by Crossmodal Feature Mapping",
            "updated": "2023-12-07T18:41:21Z",
            "published": "2023-12-07T18:41:21Z",
            "summary": "The paper explores the industrial multimodal Anomaly Detection (AD) task,\nwhich exploits point clouds and RGB images to localize anomalies. We introduce\na novel light and fast framework that learns to map features from one modality\nto the other on nominal samples. At test time, anomalies are detected by\npinpointing inconsistencies between observed and mapped features. Extensive\nexperiments show that our approach achieves state-of-the-art detection and\nsegmentation performance in both the standard and few-shot settings on the\nMVTec 3D-AD dataset while achieving faster inference and occupying less memory\nthan previous multimodal AD methods. Moreover, we propose a layer-pruning\ntechnique to improve memory and time efficiency with a marginal sacrifice in\nperformance.",
            "author": [
                "Alex Costanzino",
                "Pierluigi Zama Ramirez",
                "Giuseppe Lisanti",
                "Luigi Di Stefano"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04521v1",
                "http://arxiv.org/pdf/2312.04521v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04519v1",
            "title": "Bootstrapping Autonomous Radars with Self-Supervised Learning",
            "updated": "2023-12-07T18:38:39Z",
            "published": "2023-12-07T18:38:39Z",
            "summary": "The perception of autonomous vehicles using radars has attracted increased\nresearch interest due its ability to operate in fog and bad weather. However,\ntraining radar models is hindered by the cost and difficulty of annotating\nlarge-scale radar data. To overcome this bottleneck, we propose a\nself-supervised learning framework to leverage the large amount of unlabeled\nradar data to pre-train radar-only embeddings for self-driving perception\ntasks. The proposed method combines radar-to-radar and radar-to-vision\ncontrastive losses to learn a general representation from unlabeled radar\nheatmaps paired with their corresponding camera images. When used for\ndownstream object detection, we demonstrate that the proposed self-supervision\nframework can improve the accuracy of state-of-the-art supervised baselines by\n5.8% in mAP.",
            "author": [
                "Yiduo Hao",
                "Sohrab Madani",
                "Junfeng Guan",
                "Mohammed Alloulah",
                "Saurabh Gupta",
                "Haitham Hassanieh"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04519v1",
                "http://arxiv.org/pdf/2312.04519v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04514v1",
            "title": "Channel Charting for Streaming CSI Data",
            "updated": "2023-12-07T18:34:25Z",
            "published": "2023-12-07T18:34:25Z",
            "summary": "Channel charting (CC) applies dimensionality reduction to channel state\ninformation (CSI) data at the infrastructure basestation side with the goal of\nextracting pseudo-position information for each user. The self-supervised\nnature of CC enables predictive tasks that depend on user position without\nrequiring any ground-truth position information. In this work, we focus on the\npractically relevant streaming CSI data scenario, in which CSI is constantly\nestimated. To deal with storage limitations, we develop a novel streaming CC\narchitecture that maintains a small core CSI dataset from which the channel\ncharts are learned. Curation of the core CSI dataset is achieved using a\nmin-max-similarity criterion. Numerical validation with measured CSI data\ndemonstrates that our method approaches the accuracy obtained from the complete\nCSI dataset while using only a fraction of CSI storage and avoiding\ncatastrophic forgetting of old CSI data.",
            "author": [
                "Sueda Taner",
                "Maxime Guillaud",
                "Olav Tirkkonen",
                "Christoph Studer"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04514v1",
                "http://arxiv.org/pdf/2312.04514v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "eess.SP",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04512v1",
            "title": "MuFuzz: Sequence-Aware Mutation and Seed Mask Guidance for Blockchain\n  Smart Contract Fuzzing",
            "updated": "2023-12-07T18:32:19Z",
            "published": "2023-12-07T18:32:19Z",
            "summary": "As blockchain smart contracts become more widespread and carry more valuable\ndigital assets, they become an increasingly attractive target for attackers.\nOver the past few years, smart contracts have been subject to a plethora of\ndevastating attacks, resulting in billions of dollars in financial losses.\nThere has been a notable surge of research interest in identifying defects in\nsmart contracts. However, existing smart contract fuzzing tools are still\nunsatisfactory. They struggle to screen out meaningful transaction sequences\nand specify critical inputs for each transaction. As a result, they can only\ntrigger a limited range of contract states, making it difficult to unveil\ncomplicated vulnerabilities hidden in the deep state space.\n  In this paper, we shed light on smart contract fuzzing by employing a\nsequence-aware mutation and seed mask guidance strategy. In particular, we\nfirst utilize data-flow-based feedback to determine transaction orders in a\nmeaningful way and further introduce a sequence-aware mutation technique to\nexplore deeper states. Thereafter, we design a mask-guided seed mutation\nstrategy that biases the generated transaction inputs to hit target branches.\nIn addition, we develop a dynamic-adaptive energy adjustment paradigm that\nbalances the fuzzing resource allocation during a fuzzing campaign. We\nimplement our designs into a new smart contract fuzzer named MuFuzz, and\nextensively evaluate it on three benchmarks. Empirical results demonstrate that\nMuFuzz outperforms existing tools in terms of both branch coverage and bug\nfinding. Overall, MuFuzz achieves higher branch coverage than state-of-the-art\nfuzzers (up to 25%) and detects 30% more bugs than existing bug detectors.",
            "author": [
                "Peng Qian",
                "Hanjie Wu",
                "Zeren Du",
                "Turan Vural",
                "Dazhong Rong",
                "Zheng Cao",
                "Lun Zhang",
                "Yanbin Wang",
                "Jianhai Chen",
                "Qinming He"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04512v1",
                "http://arxiv.org/pdf/2312.04512v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04510v1",
            "title": "A Block Metropolis-Hastings Sampler for Controllable Energy-based Text\n  Generation",
            "updated": "2023-12-07T18:30:15Z",
            "published": "2023-12-07T18:30:15Z",
            "summary": "Recent work has shown that energy-based language modeling is an effective\nframework for controllable text generation because it enables flexible\nintegration of arbitrary discriminators. However, because energy-based LMs are\nglobally normalized, approximate techniques like Metropolis-Hastings (MH) are\nrequired for inference. Past work has largely explored simple proposal\ndistributions that modify a single token at a time, like in Gibbs sampling. In\nthis paper, we develop a novel MH sampler that, in contrast, proposes re-writes\nof the entire sequence in each step via iterative prompting of a large language\nmodel. Our new sampler (a) allows for more efficient and accurate sampling from\na target distribution and (b) allows generation length to be determined through\nthe sampling procedure rather than fixed in advance, as past work has required.\nWe perform experiments on two controlled generation tasks, showing both\ndownstream performance gains and more accurate target distribution sampling in\ncomparison with single-token proposal techniques.",
            "author": [
                "Jarad Forristal",
                "Niloofar Mireshghallah",
                "Greg Durrett",
                "Taylor Berg-Kirkpatrick"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04510v1",
                "http://arxiv.org/pdf/2312.04510v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04509v1",
            "title": "In-context learning of state estimators",
            "updated": "2023-12-07T18:29:36Z",
            "published": "2023-12-07T18:29:36Z",
            "summary": "State estimation has a pivotal role in several applications, including but\nnot limited to advanced control design. Especially when dealing with nonlinear\nsystems state estimation is a nontrivial task, often entailing approximations\nand challenging fine-tuning phases. In this work, we propose to overcome these\nchallenges by formulating an in-context state-estimation problem, enabling us\nto learn a state estimator for a class of (nonlinear) systems abstracting from\nparticular instances of the state seen during training. To this end, we extend\nan in-context learning framework recently proposed for system identification,\nshowing via a benchmark numerical example that this approach allows us to (i)\nuse training data directly for the design of the state estimator, (ii) not\nrequiring extensive fine-tuning procedures, while (iii) achieving superior\nperformance compared to state-of-the-art benchmarks.",
            "author": [
                "Riccardo Busetto",
                "Valentina Breschi",
                "Marco Forgione",
                "Dario Piga",
                "Simone Formentin"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04509v1",
                "http://arxiv.org/pdf/2312.04509v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04504v1",
            "title": "Coordination-free Decentralised Federated Learning on Complex Networks:\n  Overcoming Heterogeneity",
            "updated": "2023-12-07T18:24:19Z",
            "published": "2023-12-07T18:24:19Z",
            "summary": "Federated Learning (FL) is a well-known framework for successfully performing\na learning task in an edge computing scenario where the devices involved have\nlimited resources and incomplete data representation. The basic assumption of\nFL is that the devices communicate directly or indirectly with a parameter\nserver that centrally coordinates the whole process, overcoming several\nchallenges associated with it. However, in highly pervasive edge scenarios, the\npresence of a central controller that oversees the process cannot always be\nguaranteed, and the interactions (i.e., the connectivity graph) between devices\nmight not be predetermined, resulting in a complex network structure. Moreover,\nthe heterogeneity of data and devices further complicates the learning process.\nThis poses new challenges from a learning standpoint that we address by\nproposing a communication-efficient Decentralised Federated Learning (DFL)\nalgorithm able to cope with them. Our solution allows devices communicating\nonly with their direct neighbours to train an accurate model, overcoming the\nheterogeneity induced by data and different training histories. Our results\nshow that the resulting local models generalise better than those trained with\ncompeting approaches, and do so in a more communication-efficient way.",
            "author": [
                "Lorenzo Valerio",
                "Chiara Boldrini",
                "Andrea Passarella",
                "J\u00e1nos Kert\u00e9sz",
                "M\u00e1rton Karsai",
                "Gerardo I\u00f1iguez"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04504v1",
                "http://arxiv.org/pdf/2312.04504v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.DC",
                "cs.MA",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04503v1",
            "title": "Data-Driven Robust Reinforcement Learning Control of Uncertain Nonlinear\n  Systems: Towards a Fully-Automated, Insulin-Based Artificial Pancreas",
            "updated": "2023-12-07T18:24:03Z",
            "published": "2023-12-07T18:24:03Z",
            "summary": "In this paper, a novel robust tracking control scheme for a general class of\ndiscrete-time nonlinear systems affected by unknown bounded uncertainty is\npresented. By solving a parameterized optimal tracking control problem subject\nto the unknown nominal system and a suitable cost function, the resulting\noptimal tracking control policy can ensure closed-loop stability by achieving a\nsufficiently small tracking error for the original uncertain nonlinear system.\nThe computation of the optimal tracking controller is accomplished through the\nderivation of a novel Q-function-based $\\lambda$-Policy Iteration algorithm.\nThe proposed algorithm not only enjoys rigorous theoretical guarantees, but\nalso avoids technical weaknesses of conventional reinforcement learning\nmethods. By employing a data-driven, critic-only least squares implementation,\nthe performance of the proposed algorithm is evaluated to the problem of\nfully-automated, insulin-based, closed-loop glucose control for patients\ndiagnosed with Type 1 and Type 2 Diabetes Mellitus. The U.S. FDA-accepted\nDMMS.R simulator from the Epsilon Group is used to conduct a comprehensive in\nsilico clinical campaign on a rich set of virtual subjects under completely\nunannounced meal and exercise settings. Simulation results underline the\nsuperior glycaemic behavior achieved by the derived approach, as well as its\noverall maturity for the design of highly-effective, closed-loop drug delivery\nsystems for personalized medicine.",
            "author": [
                "Alexandros Tanzanakis",
                "John Lygeros"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04503v1",
                "http://arxiv.org/pdf/2312.04503v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04501v1",
            "title": "Graph Metanetworks for Processing Diverse Neural Architectures",
            "updated": "2023-12-07T18:21:52Z",
            "published": "2023-12-07T18:21:52Z",
            "summary": "Neural networks efficiently encode learned information within their\nparameters. Consequently, many tasks can be unified by treating neural networks\nthemselves as input data. When doing so, recent studies demonstrated the\nimportance of accounting for the symmetries and geometry of parameter spaces.\nHowever, those works developed architectures tailored to specific networks such\nas MLPs and CNNs without normalization layers, and generalizing such\narchitectures to other types of networks can be challenging. In this work, we\novercome these challenges by building new metanetworks - neural networks that\ntake weights from other neural networks as input. Put simply, we carefully\nbuild graphs representing the input neural networks and process the graphs\nusing graph neural networks. Our approach, Graph Metanetworks (GMNs),\ngeneralizes to neural architectures where competing methods struggle, such as\nmulti-head attention layers, normalization layers, convolutional layers, ResNet\nblocks, and group-equivariant linear layers. We prove that GMNs are expressive\nand equivariant to parameter permutation symmetries that leave the input neural\nnetwork functions unchanged. We validate the effectiveness of our method on\nseveral metanetwork tasks over diverse neural network architectures.",
            "author": [
                "Derek Lim",
                "Haggai Maron",
                "Marc T. Law",
                "Jonathan Lorraine",
                "James Lucas"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04501v1",
                "http://arxiv.org/pdf/2312.04501v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04474v1",
            "title": "Chain of Code: Reasoning with a Language Model-Augmented Code Emulator",
            "updated": "2023-12-07T17:51:43Z",
            "published": "2023-12-07T17:51:43Z",
            "summary": "Code provides a general syntactic structure to build complex programs and\nperform precise computations when paired with a code interpreter -- we\nhypothesize that language models (LMs) can leverage code-writing to improve\nChain of Thought reasoning not only for logic and arithmetic tasks, but also\nfor linguistic ones (and in particular, those that are a mix of both). For\nexample, consider prompting an LM to write code that counts the number of times\nit detects sarcasm in an essay: the LM may struggle to write an implementation\nfor \"detect_sarcasm(string)\" that can be executed by the interpreter (handling\nthe edge cases would be insurmountable). However, LMs may still produce a valid\nsolution if they are used not only to write the code, but also to selectively\n\"emulate\" the interpreter by generating the expected output of\n\"detect_sarcasm(string)\" and other lines of code (e.g., that the interpreter\ncould not compile). In this work, we propose Chain of Code (CoT), a simple yet\nsurprisingly effective extension that improves LM code-driven reasoning. The\nkey idea is to encourage LMs to format linguistic sub-tasks in a program as\nflexible pseudocode that the compiler can explicitly catch undefined behaviors\nand hand off to simulate with an LM (as an \"LMulator\"). Experiments demonstrate\nthat Chain of Code outperforms Chain of Thought and other baselines across a\nvariety of benchmarks; on BIG-Bench Hard, Chain of Code achieves 84%, a gain of\n12% over Chain of Thought. CoT scales well with large and small models alike,\nand broadens the scope of reasoning questions that LMs can correctly answer by\n\"thinking in code\". Project webpage: https://chain-of-code.github.io/.",
            "author": [
                "Chengshu Li",
                "Jacky Liang",
                "Andy Zeng",
                "Xinyun Chen",
                "Karol Hausman",
                "Dorsa Sadigh",
                "Sergey Levine",
                "Li Fei-Fei",
                "Fei Xia",
                "Brian Ichter"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04474v1",
                "http://arxiv.org/pdf/2312.04474v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04469v1",
            "title": "On the Learnability of Watermarks for Language Models",
            "updated": "2023-12-07T17:41:44Z",
            "published": "2023-12-07T17:41:44Z",
            "summary": "Watermarking of language model outputs enables statistical detection of\nmodel-generated text, which has many applications in the responsible deployment\nof language models. Existing watermarking strategies operate by altering the\ndecoder of an existing language model, and the ability for a language model to\ndirectly learn to generate the watermark would have significant implications\nfor the real-world deployment of watermarks. First, learned watermarks could be\nused to build open models that naturally generate watermarked text, allowing\nfor open models to benefit from watermarking. Second, if watermarking is used\nto determine the provenance of generated text, an adversary can hurt the\nreputation of a victim model by spoofing its watermark and generating damaging\nwatermarked text. To investigate the learnability of watermarks, we propose\nwatermark distillation, which trains a student model to behave like a teacher\nmodel that uses decoding-based watermarking. We test our approach on three\ndistinct decoding-based watermarking strategies and various hyperparameter\nsettings, finding that models can learn to generate watermarked text with high\ndetectability. We also find limitations to learnability, including the loss of\nwatermarking capabilities under fine-tuning on normal text and high sample\ncomplexity when learning low-distortion watermarks.",
            "author": [
                "Chenchen Gu",
                "Xiang Lisa Li",
                "Percy Liang",
                "Tatsunori Hashimoto"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04469v1",
                "http://arxiv.org/pdf/2312.04469v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04464v1",
            "title": "Horizon-Free and Instance-Dependent Regret Bounds for Reinforcement\n  Learning with General Function Approximation",
            "updated": "2023-12-07T17:35:34Z",
            "published": "2023-12-07T17:35:34Z",
            "summary": "To tackle long planning horizon problems in reinforcement learning with\ngeneral function approximation, we propose the first algorithm, termed as\nUCRL-WVTR, that achieves both \\emph{horizon-free} and\n\\emph{instance-dependent}, since it eliminates the polynomial dependency on the\nplanning horizon. The derived regret bound is deemed \\emph{sharp}, as it\nmatches the minimax lower bound when specialized to linear mixture MDPs up to\nlogarithmic factors. Furthermore, UCRL-WVTR is \\emph{computationally efficient}\nwith access to a regression oracle. The achievement of such a horizon-free,\ninstance-dependent, and sharp regret bound hinges upon (i) novel algorithm\ndesigns: weighted value-targeted regression and a high-order moment estimator\nin the context of general function approximation; and (ii) fine-grained\nanalyses: a novel concentration bound of weighted non-linear least squares and\na refined analysis which leads to the tight instance-dependent bound. We also\nconduct comprehensive experiments to corroborate our theoretical findings.",
            "author": [
                "Jiayi Huang",
                "Han Zhong",
                "Liwei Wang",
                "Lin F. Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04464v1",
                "http://arxiv.org/pdf/2312.04464v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04463v1",
            "title": "Leveraging Transformer-based Language Models to Automate Requirements\n  Satisfaction Assessment",
            "updated": "2023-12-07T17:33:31Z",
            "published": "2023-12-07T17:33:31Z",
            "summary": "Requirements Satisfaction Assessment (RSA) evaluates whether the set of\ndesign elements linked to a single requirement provide sufficient coverage of\nthat requirement -- typically meaning that all concepts in the requirement are\naddressed by at least one of the design elements. RSA is an important software\nengineering activity for systems with any form of hierarchical decomposition --\nespecially safety or mission critical ones. In previous studies, researchers\nused basic Information Retrieval (IR) models to decompose requirements and\ndesign elements into chunks, and then evaluated the extent to which chunks of\ndesign elements covered all chunks in the requirement. However, results had low\naccuracy because many critical concepts that extend across the entirety of the\nsentence were not well represented when the sentence was parsed into\nindependent chunks. In this paper we leverage recent advances in natural\nlanguage processing to deliver significantly more accurate results. We propose\ntwo major architectures: Satisfaction BERT (Sat-BERT), and Dual-Satisfaction\nBERT (DSat-BERT), along with their multitask learning variants to improve\nsatisfaction assessments. We perform RSA on five different datasets and compare\nresults from our variants against the chunk-based legacy approach. All\nBERT-based models significantly outperformed the legacy baseline, and Sat-BERT\ndelivered the best results returning an average improvement of 124.75% in Mean\nAverage Precision.",
            "author": [
                "Amrit Poudel",
                "Jinfeng Lin",
                "Jane Cleland-Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04463v1",
                "http://arxiv.org/pdf/2312.04463v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04461v1",
            "title": "PhotoMaker: Customizing Realistic Human Photos via Stacked ID Embedding",
            "updated": "2023-12-07T17:32:29Z",
            "published": "2023-12-07T17:32:29Z",
            "summary": "Recent advances in text-to-image generation have made remarkable progress in\nsynthesizing realistic human photos conditioned on given text prompts. However,\nexisting personalized generation methods cannot simultaneously satisfy the\nrequirements of high efficiency, promising identity (ID) fidelity, and flexible\ntext controllability. In this work, we introduce PhotoMaker, an efficient\npersonalized text-to-image generation method, which mainly encodes an arbitrary\nnumber of input ID images into a stack ID embedding for preserving ID\ninformation. Such an embedding, serving as a unified ID representation, can not\nonly encapsulate the characteristics of the same input ID comprehensively, but\nalso accommodate the characteristics of different IDs for subsequent\nintegration. This paves the way for more intriguing and practically valuable\napplications. Besides, to drive the training of our PhotoMaker, we propose an\nID-oriented data construction pipeline to assemble the training data. Under the\nnourishment of the dataset constructed through the proposed pipeline, our\nPhotoMaker demonstrates better ID preservation ability than test-time\nfine-tuning based methods, yet provides significant speed improvements,\nhigh-quality generation results, strong generalization capabilities, and a wide\nrange of applications. Our project page is available at\nhttps://photo-maker.github.io/",
            "author": [
                "Zhen Li",
                "Mingdeng Cao",
                "Xintao Wang",
                "Zhongang Qi",
                "Ming-Ming Cheng",
                "Ying Shan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04461v1",
                "http://arxiv.org/pdf/2312.04461v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG",
                "cs.MM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04460v1",
            "title": "Probabilistic volumetric speckle suppression in OCT using deep learning",
            "updated": "2023-12-07T17:32:24Z",
            "published": "2023-12-07T17:32:24Z",
            "summary": "We present a deep learning framework for volumetric speckle reduction in\noptical coherence tomography (OCT) based on a conditional generative\nadversarial network (cGAN) that leverages the volumetric nature of OCT data. In\norder to utilize the volumetric nature of OCT data, our network takes partial\nOCT volumes as input, resulting in artifact-free despeckled volumes that\nexhibit excellent speckle reduction and resolution preservation in all three\ndimensions. Furthermore, we address the ongoing challenge of generating ground\ntruth data for supervised speckle suppression deep learning frameworks by using\nvolumetric non-local means despeckling-TNode to generate training data. We show\nthat, while TNode processing is computationally demanding, it serves as a\nconvenient, accessible gold-standard source for training data; our cGAN\nreplicates efficient suppression of speckle while preserving tissue structures\nwith dimensions approaching the system resolution of non-local means\ndespeckling while being two orders of magnitude faster than TNode. We\ndemonstrate fast, effective, and high-quality despeckling of the proposed\nnetwork in different tissue types acquired with three different OCT systems\ncompared to existing deep learning methods. The open-source nature of our work\nfacilitates re-training and deployment in any OCT system with an all-software\nimplementation, working around the challenge of generating high-quality,\nspeckle-free training data.",
            "author": [
                "Bhaskara Rao Chintada",
                "Sebasti\u00e1n Ruiz-Lopera",
                "Ren\u00e9 Restrepo",
                "Brett E. Bouma",
                "Martin Villiger",
                "N\u00e9stor Uribe-Patarroyo"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04460v1",
                "http://arxiv.org/pdf/2312.04460v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "physics.med-ph",
                "physics.optics"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04455v1",
            "title": "Fortify the Shortest Stave in Attention: Enhancing Context Awareness of\n  Large Language Models for Effective Tool Use",
            "updated": "2023-12-07T17:24:51Z",
            "published": "2023-12-07T17:24:51Z",
            "summary": "Recent advancements in large language models (LLMs) have significantly\nexpanded their functionality and skills as tool agents. In this paper, we argue\nthat a waveform pattern in the model's attention allocation has an impact on\nthe tool use performance, which degrades when the position of essential\ninformation hits the trough zone. To address this issue, we propose a novel\ninference method named Attention Buckets. This approach enables LLMs to handle\ncontext by conducting parallel processes, each featuring a unique RoPE angle\nbase that shapes the attention waveform. Attention Buckets ensures that an\nattention trough of a particular process can be compensated with an attention\npeak of another run, reducing the risk of the LLM missing essential information\nresiding within the attention trough. Our extensive experiments on the widely\nrecognized tool use benchmark demonstrate the efficacy of our approach, where a\n7B-parameter open-source model enhanced by Attention Buckets achieves SOTA\nperformance on par with GPT-4.",
            "author": [
                "Yuhan Chen",
                "Ang Lv",
                "Ting-En Lin",
                "Changyu Chen",
                "Yuchuan Wu",
                "Fei Huang",
                "Yongbin Li",
                "Rui Yan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04455v1",
                "http://arxiv.org/pdf/2312.04455v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04447v1",
            "title": "Privacy-preserving quantum federated learning via gradient hiding",
            "updated": "2023-12-07T17:16:30Z",
            "published": "2023-12-07T17:16:30Z",
            "summary": "Distributed quantum computing, particularly distributed quantum machine\nlearning, has gained substantial prominence for its capacity to harness the\ncollective power of distributed quantum resources, transcending the limitations\nof individual quantum nodes. Meanwhile, the critical concern of privacy within\ndistributed computing protocols remains a significant challenge, particularly\nin standard classical federated learning (FL) scenarios where data of\nparticipating clients is susceptible to leakage via gradient inversion attacks\nby the server. This paper presents innovative quantum protocols with quantum\ncommunication designed to address the FL problem, strengthen privacy measures,\nand optimize communication efficiency. In contrast to previous works that\nleverage expressive variational quantum circuits or differential privacy\ntechniques, we consider gradient information concealment using quantum states\nand propose two distinct FL protocols, one based on private inner-product\nestimation and the other on incremental learning. These protocols offer\nsubstantial advancements in privacy preservation with low communication\nresources, forging a path toward efficient quantum communication-assisted FL\nprotocols and contributing to the development of secure distributed quantum\nmachine learning, thus addressing critical privacy concerns in the quantum\ncomputing era.",
            "author": [
                "Changhao Li",
                "Niraj Kumar",
                "Zhixin Song",
                "Shouvanik Chakrabarti",
                "Marco Pistoia"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04447v1",
                "http://arxiv.org/pdf/2312.04447v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.CR",
                "cs.DC",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04435v1",
            "title": "Deep3DSketch: 3D modeling from Free-hand Sketches with View- and\n  Structural-Aware Adversarial Training",
            "updated": "2023-12-07T16:57:38Z",
            "published": "2023-12-07T16:57:38Z",
            "summary": "This work aims to investigate the problem of 3D modeling using single\nfree-hand sketches, which is one of the most natural ways we humans express\nideas. Although sketch-based 3D modeling can drastically make the 3D modeling\nprocess more accessible, the sparsity and ambiguity of sketches bring\nsignificant challenges for creating high-fidelity 3D models that reflect the\ncreators' ideas. In this work, we propose a view- and structural-aware deep\nlearning approach, \\textit{Deep3DSketch}, which tackles the ambiguity and fully\nuses sparse information of sketches, emphasizing the structural information.\nSpecifically, we introduced random pose sampling on both 3D shapes and 2D\nsilhouettes, and an adversarial training scheme with an effective progressive\ndiscriminator to facilitate learning of the shape structures. Extensive\nexperiments demonstrated the effectiveness of our approach, which outperforms\nexisting methods -- with state-of-the-art (SOTA) performance on both synthetic\nand real datasets.",
            "author": [
                "Tianrun Chen",
                "Chenglong Fu",
                "Lanyun Zhu",
                "Papa Mao",
                "Jia Zhang",
                "Ying Zang",
                "Lingyun Sun"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04435v1",
                "http://arxiv.org/pdf/2312.04435v1"
            ],
            "primary_category": "cs.MM",
            "category": [
                "cs.MM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04433v1",
            "title": "DreamVideo: Composing Your Dream Videos with Customized Subject and\n  Motion",
            "updated": "2023-12-07T16:57:26Z",
            "published": "2023-12-07T16:57:26Z",
            "summary": "Customized generation using diffusion models has made impressive progress in\nimage generation, but remains unsatisfactory in the challenging video\ngeneration task, as it requires the controllability of both subjects and\nmotions. To that end, we present DreamVideo, a novel approach to generating\npersonalized videos from a few static images of the desired subject and a few\nvideos of target motion. DreamVideo decouples this task into two stages,\nsubject learning and motion learning, by leveraging a pre-trained video\ndiffusion model. The subject learning aims to accurately capture the fine\nappearance of the subject from provided images, which is achieved by combining\ntextual inversion and fine-tuning of our carefully designed identity adapter.\nIn motion learning, we architect a motion adapter and fine-tune it on the given\nvideos to effectively model the target motion pattern. Combining these two\nlightweight and efficient adapters allows for flexible customization of any\nsubject with any motion. Extensive experimental results demonstrate the\nsuperior performance of our DreamVideo over the state-of-the-art methods for\ncustomized video generation. Our project page is at\nhttps://dreamvideo-t2v.github.io.",
            "author": [
                "Yujie Wei",
                "Shiwei Zhang",
                "Zhiwu Qing",
                "Hangjie Yuan",
                "Zhiheng Liu",
                "Yu Liu",
                "Yingya Zhang",
                "Jingren Zhou",
                "Hongming Shan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04433v1",
                "http://arxiv.org/pdf/2312.04433v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04432v1",
            "title": "FreqFed: A Frequency Analysis-Based Approach for Mitigating Poisoning\n  Attacks in Federated Learning",
            "updated": "2023-12-07T16:56:24Z",
            "published": "2023-12-07T16:56:24Z",
            "summary": "Federated learning (FL) is a collaborative learning paradigm allowing\nmultiple clients to jointly train a model without sharing their training data.\nHowever, FL is susceptible to poisoning attacks, in which the adversary injects\nmanipulated model updates into the federated model aggregation process to\ncorrupt or destroy predictions (untargeted poisoning) or implant hidden\nfunctionalities (targeted poisoning or backdoors). Existing defenses against\npoisoning attacks in FL have several limitations, such as relying on specific\nassumptions about attack types and strategies or data distributions or not\nsufficiently robust against advanced injection techniques and strategies and\nsimultaneously maintaining the utility of the aggregated model. To address the\ndeficiencies of existing defenses, we take a generic and completely different\napproach to detect poisoning (targeted and untargeted) attacks. We present\nFreqFed, a novel aggregation mechanism that transforms the model updates (i.e.,\nweights) into the frequency domain, where we can identify the core frequency\ncomponents that inherit sufficient information about weights. This allows us to\neffectively filter out malicious updates during local training on the clients,\nregardless of attack types, strategies, and clients' data distributions. We\nextensively evaluate the efficiency and effectiveness of FreqFed in different\napplication domains, including image classification, word prediction, IoT\nintrusion detection, and speech recognition. We demonstrate that FreqFed can\nmitigate poisoning attacks effectively with a negligible impact on the utility\nof the aggregated model.",
            "author": [
                "Hossein Fereidooni",
                "Alessandro Pegoraro",
                "Phillip Rieger",
                "Alexandra Dmitrienko",
                "Ahmad-Reza Sadeghi"
            ],
            "link": [
                "http://dx.doi.org/10.14722/ndss.2024.23620",
                "http://arxiv.org/abs/2312.04432v1",
                "http://arxiv.org/pdf/2312.04432v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04426v1",
            "title": "SN2023ixf in Messier 101: the twilight years of the progenitor as seen\n  by Pan-STARRS",
            "updated": "2023-12-07T16:51:40Z",
            "published": "2023-12-07T16:51:40Z",
            "summary": "The nearby type II supernova, SN2023ixf in M101 exhibits signatures of\nearly-time interaction with circumstellar material in the first week\npost-explosion. This material may be the consequence of prior mass loss\nsuffered by the progenitor which possibly manifested in the form of a\ndetectable pre-supernova outburst. We present an analysis of the long-baseline\npre-explosion photometric data in $g$, $w$, $r$, $i$, $z$ and $y$ filters from\nPan-STARRS as part of the Young Supernova Experiment, spanning $\\sim$5,000\ndays. We find no significant detections in the Pan-STARRS pre-explosion light\ncurve. We train a multilayer perceptron neural network to classify\npre-supernova outbursts. We find no evidence of eruptive pre-supernova activity\nto a limiting absolute magnitude of $-7$. The limiting magnitudes from the full\nset of $gwrizy$ (average absolute magnitude $\\approx$-8) data are consistent\nwith previous pre-explosion studies. We use deep photometry from the literature\nto constrain the progenitor of SN2023ixf, finding that these data are\nconsistent with a dusty red supergiant (RSG) progenitor with luminosity\n$\\log\\left(L/L_\\odot\\right)$$\\approx$5.12 and temperature $\\approx$3950K,\ncorresponding to a mass of 14-20 M$_\\odot$",
            "author": [
                "Conor L. Ransome",
                "V. Ashley Villar",
                "Anna Tartaglia",
                "Sebastian Javier Gonzalez",
                "Wynn V. Jacobson-Gal\u00e1n",
                "Charles D. Kilpatrick",
                "Raffaella Margutti",
                "Ryan J. Foley",
                "Matthew Grayling",
                "Yuan Qi Ni",
                "Ricardo Yarza",
                "Christine Ye",
                "Katie Auchettl",
                "Thomas de Boer",
                "Kenneth C. Chambers",
                "David A. Coulter",
                "Maria R. Drout",
                "Diego Farias",
                "Christa Gall",
                "Hua Gao",
                "Mark E. Huber",
                "Adaeze L. Ibik",
                "David O. Jones",
                "Nandita Khetan",
                "Chien-Cheng Lin",
                "Collin A. Politsch",
                "Sandra I. Raimundo",
                "Armin Rest",
                "Richard J. Wainscoat",
                "S. Karthik Yadavalli",
                "Yossef Zenati"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04426v1",
                "http://arxiv.org/pdf/2312.04426v1"
            ],
            "primary_category": "astro-ph.SR",
            "category": [
                "astro-ph.SR",
                "astro-ph.GA",
                "astro-ph.HE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04423v1",
            "title": "Scalable Knowledge Graph Construction and Inference on Human Genome\n  Variants",
            "updated": "2023-12-07T16:48:32Z",
            "published": "2023-12-07T16:48:32Z",
            "summary": "Real-world knowledge can be represented as a graph consisting of entities and\nrelationships between the entities. The need for efficient and scalable\nsolutions arises when dealing with vast genomic data, like RNA-sequencing.\nKnowledge graphs offer a powerful approach for various tasks in such\nlarge-scale genomic data, such as analysis and inference. In this work,\nvariant-level information extracted from the RNA-sequences of vaccine-na\\\"ive\nCOVID-19 patients have been represented as a unified, large knowledge graph.\nVariant call format (VCF) files containing the variant-level information were\nannotated to include further information for each variant. The data records in\nthe annotated files were then converted to Resource Description Framework (RDF)\ntriples. Each VCF file obtained had an associated CADD scores file that\ncontained the raw and Phred-scaled scores for each variant. An ontology was\ndefined for the VCF and CADD scores files. Using this ontology and the\nextracted information, a large, scalable knowledge graph was created. Available\ngraph storage was then leveraged to query and create datasets for further\ndownstream tasks. We also present a case study using the knowledge graph and\nperform a classification task using graph machine learning. We also draw\ncomparisons between different Graph Neural Networks (GNNs) for the case study.",
            "author": [
                "Shivika Prasanna",
                "Deepthi Rao",
                "Eduardo Simoes",
                "Praveen Rao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04423v1",
                "http://arxiv.org/pdf/2312.04423v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.DB",
                "q-bio.QM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04416v1",
            "title": "Monitoring Sustainable Global Development Along Shared Socioeconomic\n  Pathways",
            "updated": "2023-12-07T16:38:20Z",
            "published": "2023-12-07T16:38:20Z",
            "summary": "Sustainable global development is one of the most prevalent challenges facing\nthe world today, hinging on the equilibrium between socioeconomic growth and\nenvironmental sustainability. We propose approaches to monitor and quantify\nsustainable development along the Shared Socioeconomic Pathways (SSPs),\nincluding mathematically derived scoring algorithms, and machine learning\nmethods. These integrate socioeconomic and environmental datasets, to produce\nan interpretable metric for SSP alignment. An initial study demonstrates\npromising results, laying the groundwork for the application of different\nmethods to the monitoring of sustainable global development.",
            "author": [
                "Michelle W. L. Wan",
                "Jeffrey N. Clark",
                "Edward A. Small",
                "Elena Fillola Mayoral",
                "Ra\u00fal Santos-Rodr\u00edguez"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04416v1",
                "http://arxiv.org/pdf/2312.04416v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04412v1",
            "title": "Developing Elementary Federated Learning Algorithms Leveraging the\n  ChatGPT",
            "updated": "2023-12-07T16:34:47Z",
            "published": "2023-12-07T16:34:47Z",
            "summary": "The Python Testbed for Federated Learning Algorithms is a simple Python FL\nframework easy to use by ML&AI developers who do not need to be professional\nprogrammers, and this paper shows that it is also amenable to emerging AI\ntools. In this paper, we successfully developed three elementary FL algorithms\nusing the following three steps process: (i) specify context, (ii) ask ChatGPT\nto complete server and clients' callback functions, and (iii) verify the\ngenerated code.",
            "author": [
                "Miroslav Popovic",
                "Marko Popovic",
                "Ivan Kastelan",
                "Miodrag Djukic",
                "Ilija Basicevic"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04412v1",
                "http://arxiv.org/pdf/2312.04412v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04411v1",
            "title": "Family Structure, Gender and Subjective Well-being: Effect of Child ren\n  before and after COVID 19 in Japan",
            "updated": "2023-12-07T16:29:18Z",
            "published": "2023-12-07T16:29:18Z",
            "summary": "Grandparents were anticipated to participated in grand-rearing. The COVID-19\npandemic had detached grandparents from rearing grandchildren. The research\nquestions of this study were as follows: How does the change in family\nrelations impact the well-being (SWB) of grandparents and parents? We examined\nhow family structure influenced subjective SWB before and after COVID-19. We\nfocused on the effects of children, grandchildren, and their gender on\ngrandparents and parents. We found that compared with the happiness level\nbefore COVID-19, (1) granddaughters increased their grandmothers SWB after\nCOVID-19, (2) both daughters and sons reduced their fathers SWB after COVID-19,\nwhereas neither daughters nor sons changed their mothers SWB, and (3) the\nnegative effect of sons reduced substantially if their fathers had younger\nbrothers. Learning from interactions with younger brothers in childhood,\nfathers could avoid the deterioration of relationships with their sons, even\nwhen unexpected events possibly changed the lifestyle of the family and their\nrelationship.",
            "author": [
                "Eiji Yamamura",
                "Fumio Ohtake"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04411v1",
                "http://arxiv.org/pdf/2312.04411v1"
            ],
            "primary_category": "econ.GN",
            "category": [
                "econ.GN",
                "q-fin.EC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04404v1",
            "title": "On the Impact of Multi-dimensional Local Differential Privacy on\n  Fairness",
            "updated": "2023-12-07T16:17:34Z",
            "published": "2023-12-07T16:17:34Z",
            "summary": "Automated decision systems are increasingly used to make consequential\ndecisions in people's lives. Due to the sensitivity of the manipulated data as\nwell as the resulting decisions, several ethical concerns need to be addressed\nfor the appropriate use of such technologies, in particular, fairness and\nprivacy. Unlike previous work, which focused on centralized differential\nprivacy (DP) or local DP (LDP) for a single sensitive attribute, in this paper,\nwe examine the impact of LDP in the presence of several sensitive attributes\n(i.e., multi-dimensional data) on fairness. Detailed empirical analysis on\nsynthetic and benchmark datasets revealed very relevant observations. In\nparticular, (1) multi-dimensional LDP is an efficient approach to reduce\ndisparity, (2) the multi-dimensional approach of LDP (independent vs. combined)\nmatters only at low privacy guarantees, and (3) the outcome Y distribution has\nan important effect on which group is more sensitive to the obfuscation. Last,\nwe summarize our findings in the form of recommendations to guide practitioners\nin adopting effective privacy-preserving practices while maintaining fairness\nand utility in ML applications.",
            "author": [
                "karima Makhlouf",
                "Heber H. Arcolezi",
                "Sami Zhioua",
                "Ghassen Ben Brahim",
                "Catuscia Palamidessi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04404v1",
                "http://arxiv.org/pdf/2312.04404v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CR",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04402v1",
            "title": "Semi-Supervised Active Learning for Semantic Segmentation in Unknown\n  Environments Using Informative Path Planning",
            "updated": "2023-12-07T16:16:47Z",
            "published": "2023-12-07T16:16:47Z",
            "summary": "Semantic segmentation enables robots to perceive and reason about their\nenvironments beyond geometry. Most of such systems build upon deep learning\napproaches. As autonomous robots are commonly deployed in initially unknown\nenvironments, pre-training on static datasets cannot always capture the variety\nof domains and limits the robot's perception performance during missions.\nRecently, self-supervised and fully supervised active learning methods emerged\nto improve a robot's vision. These approaches rely on large in-domain\npre-training datasets or require substantial human labelling effort. We propose\na planning method for semi-supervised active learning of semantic segmentation\nthat substantially reduces human labelling requirements compared to fully\nsupervised approaches. We leverage an adaptive map-based planner guided towards\nthe frontiers of unexplored space with high model uncertainty collecting\ntraining data for human labelling. A key aspect of our approach is to combine\nthe sparse high-quality human labels with pseudo labels automatically extracted\nfrom highly certain environment map areas. Experimental results show that our\nmethod reaches segmentation performance close to fully supervised approaches\nwith drastically reduced human labelling effort while outperforming\nself-supervised approaches.",
            "author": [
                "Julius R\u00fcckin",
                "Federico Magistri",
                "Cyrill Stachniss",
                "Marija Popovi\u0107"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04402v1",
                "http://arxiv.org/pdf/2312.04402v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04398v1",
            "title": "Intelligent Anomaly Detection for Lane Rendering Using Transformer with\n  Self-Supervised Pre-Training and Customized Fine-Tuning",
            "updated": "2023-12-07T16:10:10Z",
            "published": "2023-12-07T16:10:10Z",
            "summary": "The burgeoning navigation services using digital maps provide great\nconvenience to drivers. Nevertheless, the presence of anomalies in lane\nrendering map images occasionally introduces potential hazards, as such\nanomalies can be misleading to human drivers and consequently contribute to\nunsafe driving conditions. In response to this concern and to accurately and\neffectively detect the anomalies, this paper transforms lane rendering image\nanomaly detection into a classification problem and proposes a four-phase\npipeline consisting of data pre-processing, self-supervised pre-training with\nthe masked image modeling (MiM) method, customized fine-tuning using\ncross-entropy based loss with label smoothing, and post-processing to tackle it\nleveraging state-of-the-art deep learning techniques, especially those\ninvolving Transformer models. Various experiments verify the effectiveness of\nthe proposed pipeline. Results indicate that the proposed pipeline exhibits\nsuperior performance in lane rendering image anomaly detection, and notably,\nthe self-supervised pre-training with MiM can greatly enhance the detection\naccuracy while significantly reducing the total training time. For instance,\nemploying the Swin Transformer with Uniform Masking as self-supervised\npretraining (Swin-Trans-UM) yielded a heightened accuracy at 94.77% and an\nimproved Area Under The Curve (AUC) score of 0.9743 compared with the pure Swin\nTransformer without pre-training (Swin-Trans) with an accuracy of 94.01% and an\nAUC of 0.9498. The fine-tuning epochs were dramatically reduced to 41 from the\noriginal 280. In conclusion, the proposed pipeline, with its incorporation of\nself-supervised pre-training using MiM and other advanced deep learning\ntechniques, emerges as a robust solution for enhancing the accuracy and\nefficiency of lane rendering image anomaly detection in digital navigation\nsystems.",
            "author": [
                "Yongqi Dong",
                "Xingmin Lu",
                "Ruohan Li",
                "Wei Song",
                "Bart van Arem",
                "Haneen Farah"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04398v1",
                "http://arxiv.org/pdf/2312.04398v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG",
                "eess.IV",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04393v1",
            "title": "PhysHOI: Physics-Based Imitation of Dynamic Human-Object Interaction",
            "updated": "2023-12-07T16:06:31Z",
            "published": "2023-12-07T16:06:31Z",
            "summary": "Humans interact with objects all the time. Enabling a humanoid to learn\nhuman-object interaction (HOI) is a key step for future smart animation and\nintelligent robotics systems. However, recent progress in physics-based HOI\nrequires carefully designed task-specific rewards, making the system unscalable\nand labor-intensive. This work focuses on dynamic HOI imitation: teaching\nhumanoid dynamic interaction skills through imitating kinematic HOI\ndemonstrations. It is quite challenging because of the complexity of the\ninteraction between body parts and objects and the lack of dynamic HOI data. To\nhandle the above issues, we present PhysHOI, the first physics-based whole-body\nHOI imitation approach without task-specific reward designs. Except for the\nkinematic HOI representations of humans and objects, we introduce the contact\ngraph to model the contact relations between body parts and objects explicitly.\nA contact graph reward is also designed, which proved to be critical for\nprecise HOI imitation. Based on the key designs, PhysHOI can imitate diverse\nHOI tasks simply yet effectively without prior knowledge. To make up for the\nlack of dynamic HOI scenarios in this area, we introduce the BallPlay dataset\nthat contains eight whole-body basketball skills. We validate PhysHOI on\ndiverse HOI tasks, including whole-body grasping and basketball skills.",
            "author": [
                "Yinhuai Wang",
                "Jing Lin",
                "Ailing Zeng",
                "Zhengyi Luo",
                "Jian Zhang",
                "Lei Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04393v1",
                "http://arxiv.org/pdf/2312.04393v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04386v1",
            "title": "Model-Based Epistemic Variance of Values for Risk-Aware Policy\n  Optimization",
            "updated": "2023-12-07T15:55:58Z",
            "published": "2023-12-07T15:55:58Z",
            "summary": "We consider the problem of quantifying uncertainty over expected cumulative\nrewards in model-based reinforcement learning. In particular, we focus on\ncharacterizing the variance over values induced by a distribution over MDPs.\nPrevious work upper bounds the posterior variance over values by solving a\nso-called uncertainty Bellman equation (UBE), but the over-approximation may\nresult in inefficient exploration. We propose a new UBE whose solution\nconverges to the true posterior variance over values and leads to lower regret\nin tabular exploration problems. We identify challenges to apply the UBE theory\nbeyond tabular problems and propose a suitable approximation. Based on this\napproximation, we introduce a general-purpose policy optimization algorithm,\nQ-Uncertainty Soft Actor-Critic (QU-SAC), that can be applied for either\nrisk-seeking or risk-averse policy optimization with minimal changes.\nExperiments in both online and offline RL demonstrate improved performance\ncompared to other uncertainty estimation methods.",
            "author": [
                "Carlos E. Luis",
                "Alessandro G. Bottero",
                "Julia Vinogradska",
                "Felix Berkenkamp",
                "Jan Peters"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04386v1",
                "http://arxiv.org/pdf/2312.04386v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04382v1",
            "title": "Adversarial Denoising Diffusion Model for Unsupervised Anomaly Detection",
            "updated": "2023-12-07T15:51:19Z",
            "published": "2023-12-07T15:51:19Z",
            "summary": "In this paper, we propose the Adversarial Denoising Diffusion Model (ADDM).\nThe ADDM is based on the Denoising Diffusion Probabilistic Model (DDPM) but\ncomplementarily trained by adversarial learning. The proposed adversarial\nlearning is achieved by classifying model-based denoised samples and samples to\nwhich random Gaussian noise is added to a specific sampling step. With the\naddition of explicit adversarial learning on data samples, ADDM can learn the\nsemantic characteristics of the data more robustly during training, which\nachieves a similar data sampling performance with much fewer sampling steps\nthan DDPM. We apply ADDM to anomaly detection in unsupervised MRI images.\nExperimental results show that the proposed ADDM outperformed existing\ngenerative model-based unsupervised anomaly detection methods. In particular,\ncompared to other DDPM-based anomaly detection methods, the proposed ADDM shows\nbetter performance with the same number of sampling steps and similar\nperformance with 50% fewer sampling steps.",
            "author": [
                "Jongmin Yu",
                "Hyeontaek Oh",
                "Jinhong Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04382v1",
                "http://arxiv.org/pdf/2312.04382v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04377v1",
            "title": "HARQ-IR Aided Short Packet Communications: BLER Analysis and Throughput\n  Maximization",
            "updated": "2023-12-07T15:47:18Z",
            "published": "2023-12-07T15:47:18Z",
            "summary": "This paper introduces hybrid automatic repeat request with incremental\nredundancy (HARQ-IR) to boost the reliability of short packet communications.\nThe finite blocklength information theory and correlated decoding events\ntremendously preclude the analysis of average block error rate (BLER).\nFortunately, the recursive form of average BLER motivates us to calculate its\nvalue through the trapezoidal approximation and Gauss-Laguerre quadrature.\nMoreover, the asymptotic analysis is performed to derive a simple expression\nfor the average BLER at high signal-to-noise ratio (SNR). Then, we study the\nmaximization of long term average throughput (LTAT) via power allocation\nmeanwhile ensuring the power and the BLER constraints. For tractability, the\nasymptotic BLER is employed to solve the problem through geometric programming\n(GP). However, the GP-based solution underestimates the LTAT at low SNR due to\na large approximation error in this case. Alternatively, we also develop a deep\nreinforcement learning (DRL)-based framework to learn power allocation policy.\nIn particular, the optimization problem is transformed into a constrained\nMarkov decision process, which is solved by integrating deep deterministic\npolicy gradient (DDPG) with subgradient method. The numerical results finally\ndemonstrate that the DRL-based method outperforms the GP-based one at low SNR,\nalbeit at the cost of increasing computational burden.",
            "author": [
                "Fuchao He",
                "Zheng Shi",
                "Guanghua Yang",
                "Xiaofan Li",
                "Xinrong Ye",
                "Shaodan Ma"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04377v1",
                "http://arxiv.org/pdf/2312.04377v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "eess.SP",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04374v1",
            "title": "Deep Dynamics: Vehicle Dynamics Modeling with a Physics-Informed Neural\n  Network for Autonomous Racing",
            "updated": "2023-12-07T15:44:56Z",
            "published": "2023-12-07T15:44:56Z",
            "summary": "Autonomous racing is a critical research area for autonomous driving,\npresenting significant challenges in vehicle dynamics modeling, such as\nbalancing model precision and computational efficiency at high speeds\n(>280kmph), where minor errors in modeling have severe consequences. Existing\nphysics-based models for vehicle dynamics require elaborate testing setups and\ntuning, which are hard to implement, time-intensive, and cost-prohibitive.\nConversely, purely data-driven approaches do not generalize well and cannot\nadequately ensure physical constraints on predictions. This paper introduces\nDeep Dynamics, a physics-informed neural network (PINN) for vehicle dynamics\nmodeling of an autonomous racecar. It combines physics coefficient estimation\nand dynamical equations to accurately predict vehicle states at high speeds and\nincludes a unique Physics Guard layer to ensure internal coefficient estimates\nremain within their nominal physical ranges. Open-loop and closed-loop\nperformance assessments, using a physics-based simulator and full-scale\nautonomous Indy racecar data, highlight Deep Dynamics as a promising approach\nfor modeling racecar vehicle dynamics.",
            "author": [
                "John Chrosniak",
                "Jingyun Ning",
                "Madhur Behl"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04374v1",
                "http://arxiv.org/pdf/2312.04374v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI",
                "cs.LG",
                "I.2.9; I.6"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04371v1",
            "title": "A Scalable Network-Aware Multi-Agent Reinforcement Learning Framework\n  for Decentralized Inverter-based Voltage Control",
            "updated": "2023-12-07T15:42:53Z",
            "published": "2023-12-07T15:42:53Z",
            "summary": "This paper addresses the challenges associated with decentralized voltage\ncontrol in power grids due to an increase in distributed generations (DGs).\nTraditional model-based voltage control methods struggle with the rapid energy\nfluctuations and uncertainties of these DGs. While multi-agent reinforcement\nlearning (MARL) has shown potential for decentralized secondary control,\nscalability issues arise when dealing with a large number of DGs. This problem\nlies in the dominant centralized training and decentralized execution (CTDE)\nframework, where the critics take global observations and actions. To overcome\nthese challenges, we propose a scalable network-aware (SNA) framework that\nleverages network structure to truncate the input to the critic's Q-function,\nthereby improving scalability and reducing communication costs during training.\nFurther, the SNA framework is theoretically grounded with provable\napproximation guarantee, and it can seamlessly integrate with multiple\nmulti-agent actor-critic algorithms. The proposed SNA framework is successfully\ndemonstrated in a system with 114 DGs, providing a promising solution for\ndecentralized voltage control in increasingly complex power grid systems.",
            "author": [
                "Han Xu",
                "Jialin Zheng",
                "Guannan Qu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04371v1",
                "http://arxiv.org/pdf/2312.04371v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "cs.LG",
                "cs.MA",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04370v1",
            "title": "Investigating the Design Space of Diffusion Models for Speech\n  Enhancement",
            "updated": "2023-12-07T15:40:55Z",
            "published": "2023-12-07T15:40:55Z",
            "summary": "Diffusion models are a new class of generative models that have shown\noutstanding performance in image generation literature. As a consequence,\nstudies have attempted to apply diffusion models to other tasks, such as speech\nenhancement. A popular approach in adapting diffusion models to speech\nenhancement consists in modelling a progressive transformation between the\nclean and noisy speech signals. However, one popular diffusion model framework\npreviously laid in image generation literature did not account for such a\ntransformation towards the system input, which prevents from relating the\nexisting diffusion-based speech enhancement systems with the aforementioned\ndiffusion model framework. To address this, we extend this framework to account\nfor the progressive transformation between the clean and noisy speech signals.\nThis allows us to apply recent developments from image generation literature,\nand to systematically investigate design aspects of diffusion models that\nremain largely unexplored for speech enhancement, such as the neural network\npreconditioning, the training loss weighting, the stochastic differential\nequation (SDE), or the amount of stochasticity injected in the reverse process.\nWe show that the performance of previous diffusion-based speech enhancement\nsystems cannot be attributed to the progressive transformation between the\nclean and noisy speech signals. Moreover, we show that a proper choice of\npreconditioning, training loss weighting, SDE and sampler allows to outperform\na popular diffusion-based speech enhancement system in terms of perceptual\nmetrics while using fewer sampling steps, thus reducing the computational cost\nby a factor of four.",
            "author": [
                "Philippe Gonzalez",
                "Zheng-Hua Tan",
                "Jan \u00d8stergaard",
                "Jesper Jensen",
                "Tommy Sonne Alstr\u00f8m",
                "Tobias May"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04370v1",
                "http://arxiv.org/pdf/2312.04370v1"
            ],
            "primary_category": "eess.AS",
            "category": [
                "eess.AS",
                "cs.LG",
                "cs.SD"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04361v1",
            "title": "Detecting the prompt optical flashes of gamma-ray bursts with small\n  telescope arrays",
            "updated": "2023-12-07T15:28:44Z",
            "published": "2023-12-07T15:28:44Z",
            "summary": "We present an observational approach for the independent detection of the\nprompt optical emission of long gamma-ray bursts (GRBs). For this purpose, we\nexplore the potential of the Large Array Survey Telescope (LAST). This array of\nsmall optical telescopes can be used to scan a wide region of the sky, and to\nfocus on a smaller field of view with increased sensitivity, as needed. The\nmodularity of the array facilitates dynamic scanning of multiple fields, by\nshifting telescope pointing directions with high cadence. This can\nsignificantly increase the effective sky-coverage of a blind survey on short\ntime scales. For events associated with gamma-ray counterparts, the valuable\nearly-time data can supplement high-energy observations. Regardless of\ngamma-ray association, detections can potentially be used to explore various\nphenomena associated with GRBs, such as orphan afterglows; dirty fireballs; and\nchoked jets. We simulate a sample of GRBs and their respective optical signals\nat early times. After accounting for dynamic cadence, the light curves are\ngiven as input to a machine learning classifier, used to identify astrophysical\ntransients. We find that by dedicating half of a LAST array to a blind search,\none would expect to discover 7-11 GRBs per year, corresponding to an\napproximate intrinsic event rate of 0.12 per square degree per year.",
            "author": [
                "Iftach Sadeh"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04361v1",
                "http://arxiv.org/pdf/2312.04361v1"
            ],
            "primary_category": "astro-ph.HE",
            "category": [
                "astro-ph.HE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04356v1",
            "title": "NeuJeans: Private Neural Network Inference with Joint Optimization of\n  Convolution and Bootstrapping",
            "updated": "2023-12-07T15:23:07Z",
            "published": "2023-12-07T15:23:07Z",
            "summary": "Fully homomorphic encryption (FHE) is a promising cryptographic primitive for\nrealizing private neural network inference (PI) services by allowing a client\nto fully offload the inference task to a cloud server while keeping the client\ndata oblivious to the server. This work proposes NeuJeans, an FHE-based\nsolution for the PI of deep convolutional neural networks (CNNs). NeuJeans\ntackles the critical problem of the enormous computational cost for the FHE\nevaluation of convolutional layers (conv2d), mainly due to the high cost of\ndata reordering and bootstrapping. We first propose an encoding method\nintroducing nested structures inside encoded vectors for FHE, which enables us\nto develop efficient conv2d algorithms with reduced data reordering costs.\nHowever, the new encoding method also introduces additional computations for\nconversion between encoding methods, which could negate its advantages. We\ndiscover that fusing conv2d with bootstrapping eliminates such computations\nwhile reducing the cost of bootstrapping. Then, we devise optimized execution\nflows for various types of conv2d and apply them to end-to-end implementation\nof CNNs. NeuJeans accelerates the performance of conv2d by up to 5.68 times\ncompared to state-of-the-art FHE-based PI work and performs the PI of a CNN at\nthe scale of ImageNet (ResNet18) within a mere few seconds",
            "author": [
                "Jae Hyung Ju",
                "Jaiyoung Park",
                "Jongmin Kim",
                "Donghwan Kim",
                "Jung Ho Ahn"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04356v1",
                "http://arxiv.org/pdf/2312.04356v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04350v1",
            "title": "CLadder: A Benchmark to Assess Causal Reasoning Capabilities of Language\n  Models",
            "updated": "2023-12-07T15:12:12Z",
            "published": "2023-12-07T15:12:12Z",
            "summary": "The ability to perform causal reasoning is widely considered a core feature\nof intelligence. In this work, we investigate whether large language models\n(LLMs) can coherently reason about causality. Much of the existing work in\nnatural language processing (NLP) focuses on evaluating commonsense causal\nreasoning in LLMs, thus failing to assess whether a model can perform causal\ninference in accordance with a set of well-defined formal rules. To address\nthis, we propose a new NLP task, causal inference in natural language, inspired\nby the \"causal inference engine\" postulated by Judea Pearl et al. We compose a\nlarge dataset, CLadder, with 10K samples: based on a collection of causal\ngraphs and queries (associational, interventional, and counterfactual), we\nobtain symbolic questions and ground-truth answers, through an oracle causal\ninference engine. These are then translated into natural language. We evaluate\nmultiple LLMs on our dataset, and we introduce and evaluate a bespoke\nchain-of-thought prompting strategy, CausalCoT. We show that our task is highly\nchallenging for LLMs, and we conduct an in-depth analysis to gain deeper\ninsight into the causal reasoning abilities of LLMs. Our data is open-sourced\nat https://huggingface.co/datasets/causalNLP/cladder, and our code can be found\nat https://github.com/causalNLP/cladder.",
            "author": [
                "Zhijing Jin",
                "Yuen Chen",
                "Felix Leeb",
                "Luigi Gresele",
                "Ojasv Kamal",
                "Zhiheng Lyu",
                "Kevin Blin",
                "Fernando Gonzalez Adauto",
                "Max Kleiman-Weiner",
                "Mrinmaya Sachan",
                "Bernhard Sch\u00f6lkopf"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04350v1",
                "http://arxiv.org/pdf/2312.04350v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04346v1",
            "title": "Improved Efficient Two-Stage Denoising Diffusion Power System\n  Measurement Recovery Against False Data Injection Attacks and Data Losses",
            "updated": "2023-12-07T15:06:06Z",
            "published": "2023-12-07T15:06:06Z",
            "summary": "Measurement uncertainties, represented by cyber-attacks and data losses,\nseriously degrade the quality of power system measurements. Fortunately, the\npowerful generation ability of the denoising diffusion models can enable more\nprecise measurement generation for power system data recovery. However, the\ncontrollable data generation and efficient computing methods of denoising\ndiffusion models for deterministic trajectory still need further investigation.\nTo this end, this paper proposes an improved two-stage denoising diffusion\nmodel (TSDM) to identify and reconstruct the measurements with various\nmeasurement uncertainties. The first stage of the model comprises a\nclassifier-guided conditional anomaly detection component, while the second\nstage involves diffusion-based measurement imputation component. Moreover, the\nproposed TSDM adopts precise means and optimal variances to accelerate the\ndiffusion generation process with subsequence sampling. Extensive numerical\ncase studies demonstrate that the proposed TSDM can accurately recover power\nsystem measurements despite strong randomness under renewable energy\nintegration and highly nonlinear dynamics under complex cyber-physical\ncontingencies. Additionally, the proposed TSDM has stronger robustness compared\nto existing reconstruction networks and exhibits lower computational complexity\nthan general denoising diffusion models.",
            "author": [
                "Jianhua Pei",
                "Jingyu Wang",
                "Dongyuan Shi",
                "Ping Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04346v1",
                "http://arxiv.org/pdf/2312.04346v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04344v1",
            "title": "Enhancing Medical Task Performance in GPT-4V: A Comprehensive Study on\n  Prompt Engineering Strategies",
            "updated": "2023-12-07T15:05:59Z",
            "published": "2023-12-07T15:05:59Z",
            "summary": "OpenAI's latest large vision-language model (LVLM), GPT-4V(ision), has piqued\nconsiderable interest for its potential in medical applications. Despite its\npromise, recent studies and internal reviews highlight its underperformance in\nspecialized medical tasks. This paper explores the boundary of GPT-4V's\ncapabilities in medicine, particularly in processing complex imaging data from\nendoscopies, CT scans, and MRIs etc. Leveraging open-source datasets, we\nassessed its foundational competencies, identifying substantial areas for\nenhancement. Our research emphasizes prompt engineering, an often-underutilized\nstrategy for improving AI responsiveness. Through iterative testing, we refined\nthe model's prompts, significantly improving its interpretative accuracy and\nrelevance in medical imaging. From our comprehensive evaluations, we distilled\n10 effective prompt engineering techniques, each fortifying GPT-4V's medical\nacumen. These methodical enhancements facilitate more reliable, precise, and\nclinically valuable insights from GPT-4V, advancing its operability in critical\nhealthcare environments. Our findings are pivotal for those employing AI in\nmedicine, providing clear, actionable guidance on harnessing GPT-4V's full\ndiagnostic potential.",
            "author": [
                "Pengcheng Chen",
                "Ziyan Huang",
                "Zhongying Deng",
                "Tianbin Li",
                "Yanzhou Su",
                "Haoyu Wang",
                "Jin Ye",
                "Yu Qiao",
                "Junjun He"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04344v1",
                "http://arxiv.org/pdf/2312.04344v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04343v1",
            "title": "Causality and Explainability for Trustworthy Integrated Pest Management",
            "updated": "2023-12-07T15:05:26Z",
            "published": "2023-12-07T15:05:26Z",
            "summary": "Pesticides serve as a common tool in agricultural pest control but\nsignificantly contribute to the climate crisis. To combat this, Integrated Pest\nManagement (IPM) stands as a climate-smart alternative. Despite its potential,\nIPM faces low adoption rates due to farmers' skepticism about its\neffectiveness. To address this challenge, we introduce an advanced data\nanalysis framework tailored to enhance IPM adoption. Our framework provides i)\nrobust pest population predictions across diverse environments with invariant\nand causal learning, ii) interpretable pest presence predictions using\ntransparent models, iii) actionable advice through counterfactual explanations\nfor in-season IPM interventions, iv) field-specific treatment effect\nestimations, and v) assessments of the effectiveness of our advice using causal\ninference. By incorporating these features, our framework aims to alleviate\nskepticism and encourage wider adoption of IPM practices among farmers.",
            "author": [
                "Ilias Tsoumas",
                "Vasileios Sitokonstantinou",
                "Georgios Giannarakis",
                "Evagelia Lampiri",
                "Christos Athanassiou",
                "Gustau Camps-Valls",
                "Charalampos Kontoes",
                "Ioannis Athanasiadis"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04343v1",
                "http://arxiv.org/pdf/2312.04343v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04339v1",
            "title": "Merging by Matching Models in Task Subspaces",
            "updated": "2023-12-07T14:59:15Z",
            "published": "2023-12-07T14:59:15Z",
            "summary": "Model merging aims to cheaply combine individual task-specific models into a\nsingle multitask model. In this work, we view past merging methods as\nleveraging different notions of a ''task subspace'' in which models are matched\nbefore being merged. We connect the task subspace of a given model to its loss\nlandscape and formalize how this approach to model merging can be seen as\nsolving a linear system of equations. While past work has generally been\nlimited to linear systems that have a closed-form solution, we consider using\nthe conjugate gradient method to find a solution. We show that using the\nconjugate gradient method can outperform closed-form solutions, enables merging\nvia linear systems that are otherwise intractable to solve, and flexibly allows\nchoosing from a wide variety of initializations and estimates for the ''task\nsubspace''. We ultimately demonstrate that our merging framework called\n''Matching Models in their Task Subspace'' (MaTS) achieves state-of-the-art\nresults in multitask and intermediate-task model merging. We release all of the\ncode and checkpoints used in our work at https://github.com/r-three/mats.",
            "author": [
                "Derek Tam",
                "Mohit Bansal",
                "Colin Raffel"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04339v1",
                "http://arxiv.org/pdf/2312.04339v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04334v1",
            "title": "Towards a Perceptual Evaluation Framework for Lighting Estimation",
            "updated": "2023-12-07T14:51:12Z",
            "published": "2023-12-07T14:51:12Z",
            "summary": "Progress in lighting estimation is tracked by computing existing image\nquality assessment (IQA) metrics on images from standard datasets. While this\nmay appear to be a reasonable approach, we demonstrate that doing so does not\ncorrelate to human preference when the estimated lighting is used to relight a\nvirtual scene into a real photograph. To study this, we design a controlled\npsychophysical experiment where human observers must choose their preference\namongst rendered scenes lit using a set of lighting estimation algorithms\nselected from the recent literature, and use it to analyse how these algorithms\nperform according to human perception. Then, we demonstrate that none of the\nmost popular IQA metrics from the literature, taken individually, correctly\nrepresent human perception. Finally, we show that by learning a combination of\nexisting IQA metrics, we can more accurately represent human preference. This\nprovides a new perceptual framework to help evaluate future lighting estimation\nalgorithms.",
            "author": [
                "Justine Giroux",
                "Mohammad Reza Karimi Dastjerdi",
                "Yannick Hold-Geoffroy",
                "Javier Vazquez-Corral",
                "Jean-Fran\u00e7ois Lalonde"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04334v1",
                "http://arxiv.org/pdf/2312.04334v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04330v1",
            "title": "Surrogate Modelling for Sea Ice Concentration using Lightweight Neural\n  Ensemble",
            "updated": "2023-12-07T14:48:30Z",
            "published": "2023-12-07T14:48:30Z",
            "summary": "The modeling and forecasting of sea ice conditions in the Arctic region are\nimportant tasks for ship routing, offshore oil production, and environmental\nmonitoring. We propose the adaptive surrogate modeling approach named LANE-SI\n(Lightweight Automated Neural Ensembling for Sea Ice) that uses ensemble of\nrelatively simple deep learning models with different loss functions for\nforecasting of spatial distribution for sea ice concentration in the specified\nwater area. Experimental studies confirm the quality of a long-term forecast\nbased on a deep learning model fitted to the specific water area is comparable\nto resource-intensive physical modeling, and for some periods of the year, it\nis superior. We achieved a 20% improvement against the state-of-the-art\nphysics-based forecast system SEAS5 for the Kara Sea.",
            "author": [
                "Julia Borisova",
                "Nikolay O. Nikitin"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04330v1",
                "http://arxiv.org/pdf/2312.04330v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "physics.ao-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04327v1",
            "title": "Learning to sample in Cartesian MRI",
            "updated": "2023-12-07T14:38:07Z",
            "published": "2023-12-07T14:38:07Z",
            "summary": "Despite its exceptional soft tissue contrast, Magnetic Resonance Imaging\n(MRI) faces the challenge of long scanning times compared to other modalities\nlike X-ray radiography. Shortening scanning times is crucial in clinical\nsettings, as it increases patient comfort, decreases examination costs and\nimproves throughput. Recent advances in compressed sensing (CS) and deep\nlearning allow accelerated MRI acquisition by reconstructing high-quality\nimages from undersampled data. While reconstruction algorithms have received\nmost of the focus, designing acquisition trajectories to optimize\nreconstruction quality remains an open question. This thesis explores two\napproaches to address this gap in the context of Cartesian MRI. First, we\npropose two algorithms, lazy LBCS and stochastic LBCS, that significantly\nimprove upon G\\\"ozc\\\"u et al.'s greedy learning-based CS (LBCS) approach. These\nalgorithms scale to large, clinically relevant scenarios like multi-coil 3D MR\nand dynamic MRI, previously inaccessible to LBCS. Additionally, we demonstrate\nthat generative adversarial networks (GANs) can serve as a natural criterion\nfor adaptive sampling by leveraging variance in the measurement domain to guide\nacquisition. Second, we delve into the underlying structures or assumptions\nthat enable mask design algorithms to perform well in practice. Our experiments\nreveal that state-of-the-art deep reinforcement learning (RL) approaches, while\ncapable of adaptation and long-horizon planning, offer only marginal\nimprovements over stochastic LBCS, which is neither adaptive nor does long-term\nplanning. Altogether, our findings suggest that stochastic LBCS and similar\nmethods represent promising alternatives to deep RL. They shine in particular\nby their scalability and computational efficiency and could be key in the\ndeployment of optimized acquisition trajectories in Cartesian MRI.",
            "author": [
                "Thomas Sanchez"
            ],
            "link": [
                "http://dx.doi.org/10.5075/epfl-thesis-9981",
                "http://arxiv.org/abs/2312.04327v1",
                "http://arxiv.org/pdf/2312.04327v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04326v1",
            "title": "iDesigner: A High-Resolution and Complex-Prompt Following Text-to-Image\n  Diffusion Model for Interior Design",
            "updated": "2023-12-07T14:37:01Z",
            "published": "2023-12-07T14:37:01Z",
            "summary": "With the open-sourcing of text-to-image models (T2I) such as stable diffusion\n(SD) and stable diffusion XL (SD-XL), there is an influx of models fine-tuned\nin specific domains based on the open-source SD model, such as in anime,\ncharacter portraits, etc. However, there are few specialized models in certain\ndomains, such as interior design, which is attributed to the complex textual\ndescriptions and detailed visual elements inherent in design, alongside the\nnecessity for adaptable resolution. Therefore, text-to-image models for\ninterior design are required to have outstanding prompt-following capabilities,\nas well as iterative collaboration with design professionals to achieve the\ndesired outcome. In this paper, we collect and optimize text-image data in the\ndesign field and continue training in both English and Chinese on the basis of\nthe open-source CLIP model. We also proposed a fine-tuning strategy with\ncurriculum learning and reinforcement learning from CLIP feedback to enhance\nthe prompt-following capabilities of our approach so as to improve the quality\nof image generation. The experimental results on the collected dataset\ndemonstrate the effectiveness of the proposed approach, which achieves\nimpressive results and outperforms strong baselines.",
            "author": [
                "Ruyi Gan",
                "Xiaojun Wu",
                "Junyu Lu",
                "Yuanhe Tian",
                "Dixiang Zhang",
                "Ziwei Wu",
                "Renliang Sun",
                "Chang Liu",
                "Jiaxing Zhang",
                "Pingjian Zhang",
                "Yan Song"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04326v1",
                "http://arxiv.org/pdf/2312.04326v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04323v1",
            "title": "Equivariant Scalar Fields for Molecular Docking with Fast Fourier\n  Transforms",
            "updated": "2023-12-07T14:32:32Z",
            "published": "2023-12-07T14:32:32Z",
            "summary": "Molecular docking is critical to structure-based virtual screening, yet the\nthroughput of such workflows is limited by the expensive optimization of\nscoring functions involved in most docking algorithms. We explore how machine\nlearning can accelerate this process by learning a scoring function with a\nfunctional form that allows for more rapid optimization. Specifically, we\ndefine the scoring function to be the cross-correlation of multi-channel ligand\nand protein scalar fields parameterized by equivariant graph neural networks,\nenabling rapid optimization over rigid-body degrees of freedom with fast\nFourier transforms. The runtime of our approach can be amortized at several\nlevels of abstraction, and is particularly favorable for virtual screening\nsettings with a common binding pocket. We benchmark our scoring functions on\ntwo simplified docking-related tasks: decoy pose scoring and rigid conformer\ndocking. Our method attains similar but faster performance on crystal\nstructures compared to the widely-used Vina and Gnina scoring functions, and is\nmore robust on computationally predicted structures. Code is available at\nhttps://github.com/bjing2016/scalar-fields.",
            "author": [
                "Bowen Jing",
                "Tommi Jaakkola",
                "Bonnie Berger"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04323v1",
                "http://arxiv.org/pdf/2312.04323v1"
            ],
            "primary_category": "q-bio.BM",
            "category": [
                "q-bio.BM",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04318v1",
            "title": "MIMo: A Multi-Modal Infant Model for Studying Cognitive Development",
            "updated": "2023-12-07T14:21:31Z",
            "published": "2023-12-07T14:21:31Z",
            "summary": "Human intelligence and human consciousness emerge gradually during the\nprocess of cognitive development. Understanding this development is an\nessential aspect of understanding the human mind and may facilitate the\nconstruction of artificial minds with similar properties. Importantly, human\ncognitive development relies on embodied interactions with the physical and\nsocial environment, which is perceived via complementary sensory modalities.\nThese interactions allow the developing mind to probe the causal structure of\nthe world. This is in stark contrast to common machine learning approaches,\ne.g., for large language models, which are merely passively ``digesting'' large\namounts of training data, but are not in control of their sensory inputs.\nHowever, computational modeling of the kind of self-determined embodied\ninteractions that lead to human intelligence and consciousness is a formidable\nchallenge. Here we present MIMo, an open-source multi-modal infant model for\nstudying early cognitive development through computer simulations. MIMo's body\nis modeled after an 18-month-old child with detailed five-fingered hands. MIMo\nperceives its surroundings via binocular vision, a vestibular system,\nproprioception, and touch perception through a full-body virtual skin, while\ntwo different actuation models allow control of his body. We describe the\ndesign and interfaces of MIMo and provide examples illustrating its use. All\ncode is available at https://github.com/trieschlab/MIMo .",
            "author": [
                "Dominik Mattern",
                "Pierre Schumacher",
                "Francisco M. L\u00f3pez",
                "Marcel C. Raabe",
                "Markus R. Ernst",
                "Arthur Aubret",
                "Jochen Triesch"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04318v1",
                "http://arxiv.org/pdf/2312.04318v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04316v1",
            "title": "Towards Knowledge-driven Autonomous Driving",
            "updated": "2023-12-07T14:17:17Z",
            "published": "2023-12-07T14:17:17Z",
            "summary": "This paper explores the emerging knowledge-driven autonomous driving\ntechnologies. Our investigation highlights the limitations of current\nautonomous driving systems, in particular their sensitivity to data bias,\ndifficulty in handling long-tail scenarios, and lack of interpretability.\nConversely, knowledge-driven methods with the abilities of cognition,\ngeneralization and life-long learning emerge as a promising way to overcome\nthese challenges. This paper delves into the essence of knowledge-driven\nautonomous driving and examines its core components: dataset \\& benchmark,\nenvironment, and driver agent. By leveraging large language models, world\nmodels, neural rendering, and other advanced artificial intelligence\ntechniques, these components collectively contribute to a more holistic,\nadaptive, and intelligent autonomous driving system. The paper systematically\norganizes and reviews previous research efforts in this area, and provides\ninsights and guidance for future research and practical applications of\nautonomous driving. We will continually share the latest updates on\ncutting-edge developments in knowledge-driven autonomous driving along with the\nrelevant valuable open-source resources at:\n\\url{https://github.com/PJLab-ADG/awesome-knowledge-driven-AD}.",
            "author": [
                "Xin Li",
                "Yeqi Bai",
                "Pinlong Cai",
                "Licheng Wen",
                "Daocheng Fu",
                "Bo Zhang",
                "Xuemeng Yang",
                "Xinyu Cai",
                "Tao Ma",
                "Jianfei Guo",
                "Xing Gao",
                "Min Dou",
                "Botian Shi",
                "Yong Liu",
                "Liang He",
                "Yu Qiao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04316v1",
                "http://arxiv.org/pdf/2312.04316v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04314v1",
            "title": "GPT4SGG: Synthesizing Scene Graphs from Holistic and Region-specific\n  Narratives",
            "updated": "2023-12-07T14:11:00Z",
            "published": "2023-12-07T14:11:00Z",
            "summary": "Learning scene graphs from natural language descriptions has proven to be a\ncheap and promising scheme for Scene Graph Generation (SGG). However, such\nunstructured caption data and its processing are troubling the learning an\nacurrate and complete scene graph. This dilema can be summarized as three\npoints. First, traditional language parsers often fail to extract meaningful\nrelationship triplets from caption data. Second, grounding unlocalized objects\nin parsed triplets will meet ambiguity in visual-language alignment. Last,\ncaption data typically are sparse and exhibit bias to partial observations of\nimage content. These three issues make it hard for the model to generate\ncomprehensive and accurate scene graphs. To fill this gap, we propose a simple\nyet effective framework, GPT4SGG, to synthesize scene graphs from holistic and\nregion-specific narratives. The framework discards traditional language parser,\nand localize objects before obtaining relationship triplets. To obtain\nrelationship triplets, holistic and dense region-specific narratives are\ngenerated from the image. With such textual representation of image data and a\ntask-specific prompt, an LLM, particularly GPT-4, directly synthesizes a scene\ngraph as \"pseudo labels\". Experimental results showcase GPT4SGG significantly\nimproves the performance of SGG models trained on image-caption data. We\nbelieve this pioneering work can motivate further research into mining the\nvisual reasoning capabilities of LLMs.",
            "author": [
                "Zuyao Chen",
                "Jinlin Wu",
                "Zhen Lei",
                "Zhaoxiang Zhang",
                "Changwen Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04314v1",
                "http://arxiv.org/pdf/2312.04314v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04312v1",
            "title": "Stochastic-Constrained Stochastic Optimization with Markovian Data",
            "updated": "2023-12-07T14:09:27Z",
            "published": "2023-12-07T14:09:27Z",
            "summary": "This paper considers stochastic-constrained stochastic optimization where the\nstochastic constraint is to satisfy that the expectation of a random function\nis below a certain threshold. In particular, we study the setting where data\nsamples are drawn from a Markov chain and thus are not independent and\nidentically distributed. We generalize the drift-plus-penalty framework, a\nprimal-dual stochastic gradient method developed for the i.i.d. case, to the\nMarkov chain sampling setting. We propose two variants of drift-plus-penalty;\none is for the case when the mixing time of the underlying Markov chain is\nknown while the other is for the case of unknown mixing time. In fact, our\nalgorithms apply to a more general setting of constrained online convex\noptimization where the sequence of constraint functions follows a Markov chain.\nBoth algorithms are adaptive in that the first works without knowledge of the\ntime horizon while the second uses AdaGrad-style algorithm parameters, which is\nof independent interest. We demonstrate the effectiveness of our proposed\nmethods through numerical experiments on classification with fairness\nconstraints.",
            "author": [
                "Yeongjong Kim",
                "Dabeen Lee"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04312v1",
                "http://arxiv.org/pdf/2312.04312v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04311v1",
            "title": "Finding Interpretable Class-Specific Patterns through Efficient Neural\n  Search",
            "updated": "2023-12-07T14:09:18Z",
            "published": "2023-12-07T14:09:18Z",
            "summary": "Discovering patterns in data that best describe the differences between\nclasses allows to hypothesize and reason about class-specific mechanisms. In\nmolecular biology, for example, this bears promise of advancing the\nunderstanding of cellular processes differing between tissues or diseases,\nwhich could lead to novel treatments. To be useful in practice, methods that\ntackle the problem of finding such differential patterns have to be readily\ninterpretable by domain experts, and scalable to the extremely high-dimensional\ndata.\n  In this work, we propose a novel, inherently interpretable binary neural\nnetwork architecture DIFFNAPS that extracts differential patterns from data.\nDiffNaps is scalable to hundreds of thousands of features and robust to noise,\nthus overcoming the limitations of current state-of-the-art methods in\nlarge-scale applications such as in biology. We show on synthetic and real\nworld data, including three biological applications, that, unlike its\ncompetitors, DiffNaps consistently yields accurate, succinct, and interpretable\nclass descriptions",
            "author": [
                "Nils Philipp Walter",
                "Jonas Fischer",
                "Jilles Vreeken"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04311v1",
                "http://arxiv.org/pdf/2312.04311v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "q-bio.QM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04308v1",
            "title": "Multi Actor-Critic DDPG for Robot Action Space Decomposition: A\n  Framework to Control Large 3D Deformation of Soft Linear Objects",
            "updated": "2023-12-07T14:07:17Z",
            "published": "2023-12-07T14:07:17Z",
            "summary": "Robotic manipulation of deformable linear objects (DLOs) has great potential\nfor applications in diverse fields such as agriculture or industry. However, a\nmajor challenge lies in acquiring accurate deformation models that describe the\nrelationship between robot motion and DLO deformations. Such models are\ndifficult to calculate analytically and vary among DLOs. Consequently,\nmanipulating DLOs poses significant challenges, particularly in achieving large\ndeformations that require highly accurate global models. To address these\nchallenges, this paper presents MultiAC6: a new multi Actor-Critic framework\nfor robot action space decomposition to control large 3D deformations of DLOs.\nIn our approach, two deep reinforcement learning (DRL) agents orient and\nposition a robot gripper to deform a DLO into the desired shape. Unlike\nprevious DRL-based studies, MultiAC6 is able to solve the sim-to-real gap,\nachieving large 3D deformations up to 40 cm in real-world settings.\nExperimental results also show that MultiAC6 has a 66\\% higher success rate\nthan a single-agent approach. Further experimental studies demonstrate that\nMultiAC6 generalizes well, without retraining, to DLOs with different lengths\nor materials.",
            "author": [
                "M\u00e9lodie Daniel",
                "Aly Magassouba",
                "Miguel Aranda",
                "Laurent Lequi\u00e8vre",
                "Juan Antonio Corrales Ramon",
                "Roberto Iglesias Rodriguez",
                "Youcef Mezouar"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04308v1",
                "http://arxiv.org/pdf/2312.04308v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04307v1",
            "title": "A Structural-Clustering Based Active Learning for Graph Neural Networks",
            "updated": "2023-12-07T14:04:38Z",
            "published": "2023-12-07T14:04:38Z",
            "summary": "In active learning for graph-structured data, Graph Neural Networks (GNNs)\nhave shown effectiveness. However, a common challenge in these applications is\nthe underutilization of crucial structural information. To address this\nproblem, we propose the Structural-Clustering PageRank method for improved\nActive learning (SPA) specifically designed for graph-structured data. SPA\nintegrates community detection using the SCAN algorithm with the PageRank\nscoring method for efficient and informative sample selection. SPA prioritizes\nnodes that are not only informative but also central in structure. Through\nextensive experiments, SPA demonstrates higher accuracy and macro-F1 score over\nexisting methods across different annotation budgets and achieves significant\nreductions in query time. In addition, the proposed method only adds two\nhyperparameters, $\\epsilon$ and $\\mu$ in the algorithm to finely tune the\nbalance between structural learning and node selection. This simplicity is a\nkey advantage in active learning scenarios, where extensive hyperparameter\ntuning is often impractical.",
            "author": [
                "Ricky Maulana Fajri",
                "Yulong Pei",
                "Lu Yin",
                "Mykola Pechenizkiy"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04307v1",
                "http://arxiv.org/pdf/2312.04307v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04306v1",
            "title": "nerblackbox: A High-level Library for Named Entity Recognition in Python",
            "updated": "2023-12-07T14:04:15Z",
            "published": "2023-12-07T14:04:15Z",
            "summary": "We present nerblackbox, a python library to facilitate the use of\nstate-of-the-art transformer-based models for named entity recognition. It\nprovides simple-to-use yet powerful methods to access data and models from a\nwide range of sources, for fully automated model training and evaluation as\nwell as versatile model inference. While many technical challenges are solved\nand hidden from the user by default, nerblackbox also offers fine-grained\ncontrol and a rich set of customizable features. It is thus targeted both at\napplication-oriented developers as well as machine learning experts and\nresearchers.",
            "author": [
                "Felix Stollenwerk"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04306v1",
                "http://arxiv.org/pdf/2312.04306v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04296v1",
            "title": "Cross-codex Learning for Reliable Scribe Identification in Medieval\n  Manuscripts",
            "updated": "2023-12-07T13:40:20Z",
            "published": "2023-12-07T13:40:20Z",
            "summary": "Historic scribe identification is a substantial task for obtaining\ninformation about the past. Uniform script styles, such as the Carolingian\nminuscule, make it a difficult task for classification to focus on meaningful\nfeatures. Therefore, we demonstrate in this paper the importance of cross-codex\ntraining data for CNN based text-independent off-line scribe identification, to\novercome codex dependent overfitting. We report three main findings: First, we\nfound that preprocessing with masked grayscale images instead of RGB images\nclearly increased the F1-score of the classification results. Second, we\ntrained different neural networks on our complex data, validating time and\naccuracy differences in order to define the most reliable network architecture.\nWith AlexNet, the network with the best trade-off between F1-score and time, we\nachieved for individual classes F1-scores of up to 0,96 on line level and up to\n1.0 on page level in classification. Third, we could replicate the finding that\nthe CNN output can be further improved by implementing a reject option, giving\nmore stable results. We present the results on our large scale open source\ndataset -- the Codex Claustroneoburgensis database (CCl-DB) -- containing a\nsignificant number of writings from different scribes in several codices. We\ndemonstrate for the first time on a dataset with such a variety of codices that\npaleographic decisions can be reproduced automatically and precisely with CNNs.\nThis gives manifold new and fast possibilities for paleographers to gain\ninsights into unlabeled material, but also to develop further hypotheses.",
            "author": [
                "Julius Wei\u00dfmann",
                "Markus Seidl",
                "Anya Dietrich",
                "Martin Haltrich"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04296v1",
                "http://arxiv.org/pdf/2312.04296v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "I.4.9; I.5.4"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04291v1",
            "title": "Simulating the Air Quality Impact of Prescribed Fires Using a Graph\n  Neural Network-Based PM$_{2.5}$ Emissions Forecasting System",
            "updated": "2023-12-07T13:18:36Z",
            "published": "2023-12-07T13:18:36Z",
            "summary": "The increasing size and severity of wildfires across western North America\nhave generated dangerous levels of PM$_{2.5}$ pollution in recent years. In a\nwarming climate, expanding the use of prescribed fires is widely considered to\nbe the most robust fire mitigation strategy. However, reliably forecasting the\npotential air quality impact from these prescribed fires, a critical ingredient\nin determining the fires' location and time, at hourly to daily time scales\nremains a challenging problem. This paper proposes a novel integration of\nprescribed fire simulation with a spatio-temporal graph neural network-based\nPM$_{2.5}$ forecasting model. The experiments in this work focus on determining\nthe optimal time for implementing prescribed fires in California as well as\nquantifying the potential air quality trade-offs involved in conducting more\nprescribed fires outside the fire season.",
            "author": [
                "Kyleen Liao",
                "Jatan Buch",
                "Kara Lamb",
                "Pierre Gentine"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04291v1",
                "http://arxiv.org/pdf/2312.04291v1"
            ],
            "primary_category": "physics.ao-ph",
            "category": [
                "physics.ao-ph",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04289v1",
            "title": "Fast simulation of airfoil flow field via deep neural network",
            "updated": "2023-12-07T13:16:02Z",
            "published": "2023-12-07T13:16:02Z",
            "summary": "Computational Fluid Dynamics (CFD) has become an indispensable tool in the\noptimization design, and evaluation of aircraft aerodynamics. However, solving\nthe Navier-Stokes (NS) equations is a time-consuming, memory demanding and\ncomputationally expensive task. Artificial intelligence offers a promising\navenue for flow field solving. In this work, we propose a novel deep learning\nframework for rapidly reconstructing airfoil flow fields. Channel attention and\nspatial attention modules are utilized in the downsampling stage of the UNet to\nenhance the feature learning capabilities of the deep learning model.\nAdditionally, integrating the predicted flow field values generated by the deep\nlearning model into the NS equation solver validates the credibility of the\nflow field prediction results. The NACA series airfoils were used to validate\nthe prediction accuracy and generalization of the deep learning model. The\nexperimental results represent the deep learning model achieving flow field\nprediction speeds three orders of magnitude faster than CFD solver.\nFurthermore, the CFD solver integrated with deep learning model demonstrates a\nthreefold acceleration compared to CFD solver. By extensively mining historical\nflow field data, an efficient solution is derived for the rapid simulation of\naircraft flow fields.",
            "author": [
                "Kuijun Zuo",
                "Zhengyin Ye",
                "Shuhui Bu",
                "Xianxu Yuan",
                "Weiwei Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04289v1",
                "http://arxiv.org/pdf/2312.04289v1"
            ],
            "primary_category": "physics.flu-dyn",
            "category": [
                "physics.flu-dyn"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04281v1",
            "title": "Factor-Assisted Federated Learning for Personalized Optimization with\n  Heterogeneous Data",
            "updated": "2023-12-07T13:05:47Z",
            "published": "2023-12-07T13:05:47Z",
            "summary": "Federated learning is an emerging distributed machine learning framework\naiming at protecting data privacy. Data heterogeneity is one of the core\nchallenges in federated learning, which could severely degrade the convergence\nrate and prediction performance of deep neural networks. To address this issue,\nwe develop a novel personalized federated learning framework for heterogeneous\ndata, which we refer to as FedSplit. This modeling framework is motivated by\nthe finding that, data in different clients contain both common knowledge and\npersonalized knowledge. Then the hidden elements in each neural layer can be\nsplit into the shared and personalized groups. With this decomposition, a novel\nobjective function is established and optimized. We demonstrate FedSplit\nenjoyers a faster convergence speed than the standard federated learning method\nboth theoretically and empirically. The generalization bound of the FedSplit\nmethod is also studied. To practically implement the proposed method on real\ndatasets, factor analysis is introduced to facilitate the decoupling of hidden\nelements. This leads to a practically implemented model for FedSplit and we\nfurther refer to as FedFac. We demonstrated by simulation studies that, using\nfactor analysis can well recover the underlying shared/personalized\ndecomposition. The superior prediction performance of FedFac is further\nverified empirically by comparison with various state-of-the-art federated\nlearning methods on several real datasets.",
            "author": [
                "Feifei Wang",
                "Huiyun Tang",
                "Yang Li"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04281v1",
                "http://arxiv.org/pdf/2312.04281v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04275v1",
            "title": "Estimating Countries with Similar Maternal Mortality Rate using Cluster\n  Analysis and Pairing Countries with Identical MMR",
            "updated": "2023-12-07T12:54:16Z",
            "published": "2023-12-07T12:54:16Z",
            "summary": "In the evolving world, we require more additionally the young era to flourish\nand evolve into developed land. Most of the population all around the world are\nunaware of the complications involved in the routine they follow while they are\npregnant and how hospital facilities affect maternal health. Maternal Mortality\nis the death of a pregnant woman due to intricacies correlated to pregnancy,\nunderlying circumstances exacerbated by the pregnancy or management of these\nsituations. It is crucial to consider the Maternal Mortality Rate (MMR) in\ndiverse locations and determine which human routines and hospital facilities\ndiminish the Maternal Mortality Rate (MMR). This research aims to examine and\ndiscover the countries which are keeping more lavish threats of MMR and\ncountries alike in MMR encountered. Data is examined and collected for various\ncountries, data consists of the earlier years' observation. From the\nperspective of Machine Learning, Unsupervised Machine Learning is implemented\nto perform Cluster Analysis. Therefore the pairs of countries with similar MMR\nas well as the extreme opposite pair concerning the MMR are found.",
            "author": [
                "S. Nandini",
                "Sanjjushri Varshini R"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04275v1",
                "http://arxiv.org/pdf/2312.04275v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04273v1",
            "title": "Invariant Random Forest: Tree-Based Model Solution for OOD\n  Generalization",
            "updated": "2023-12-07T12:53:05Z",
            "published": "2023-12-07T12:53:05Z",
            "summary": "Out-Of-Distribution (OOD) generalization is an essential topic in machine\nlearning. However, recent research is only focusing on the corresponding\nmethods for neural networks. This paper introduces a novel and effective\nsolution for OOD generalization of decision tree models, named Invariant\nDecision Tree (IDT). IDT enforces a penalty term with regard to the\nunstable/varying behavior of a split across different environments during the\ngrowth of the tree. Its ensemble version, the Invariant Random Forest (IRF), is\nconstructed. Our proposed method is motivated by a theoretical result under\nmild conditions, and validated by numerical tests with both synthetic and real\ndatasets. The superior performance compared to non-OOD tree models implies that\nconsidering OOD generalization for tree models is absolutely necessary and\nshould be given more attention.",
            "author": [
                "Yufan Liao",
                "Qi Wu",
                "Xing Yan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04273v1",
                "http://arxiv.org/pdf/2312.04273v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04238v1",
            "title": "Long-lived Particles Anomaly Detection with Parametrized Quantum\n  Circuits",
            "updated": "2023-12-07T11:50:42Z",
            "published": "2023-12-07T11:50:42Z",
            "summary": "We investigate the possibility to apply quantum machine learning techniques\nfor data analysis, with particular regard to an interesting use-case in\nhigh-energy physics. We propose an anomaly detection algorithm based on a\nparametrized quantum circuit. This algorithm has been trained on a classical\ncomputer and tested with simulations as well as on real quantum hardware. Tests\non NISQ devices have been performed with IBM quantum computers. For the\nexecution on quantum hardware specific hardware driven adaptations have been\ndevised and implemented. The quantum anomaly detection algorithm is able to\ndetect simple anomalies like different characters in handwritten digits as well\nas more complex structures like anomalous patterns in the particle detectors\nproduced by the decay products of long-lived particles produced at a collider\nexperiment. For the high-energy physics application, performance is estimated\nin simulation only, as the quantum circuit is not simple enough to be executed\non the available quantum hardware. This work demonstrates that it is possible\nto perform anomaly detection with quantum algorithms, however, as amplitude\nencoding of classical data is required for the task, due to the noise level in\nthe available quantum hardware, current implementation cannot outperform\nclassic anomaly detection algorithms based on deep neural networks.",
            "author": [
                "Simone Bordoni",
                "Denis Stanev",
                "Tommaso Santantonio",
                "Stefano Giagu"
            ],
            "link": [
                "http://dx.doi.org/10.3390/particles6010016",
                "http://arxiv.org/abs/2312.04238v1",
                "http://arxiv.org/pdf/2312.04238v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "hep-ex"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04237v1",
            "title": "A coherent radio flash following a neutron star merger",
            "updated": "2023-12-07T11:50:01Z",
            "published": "2023-12-07T11:50:01Z",
            "summary": "The mergers of two neutron stars are exceptional multi-messenger events that\nenable us to probe fundamental physics in one of the most extreme environments\nin the Universe. Multi-wavelength follow-up observations are essential in order\nto probe the physics of the outflows from and remnants of these neutron star\nmergers, both when detected as short gamma-ray bursts (GRBs) and as\ngravitational wave events. Rapid follow-up can provide localisations for\ntargeted deep follow-up observations and, ideally, a distance measurement,\nwhich constrains for instance the energetics of the merger. A key outstanding\nquestion is the remnant's nature: with its expected mass and rapid spin, it\ncould either be a black hole or a supramassive, likely highly magnetised\nneutron star (a magnetar). Both can power a GRB, but rapidly spinning magnetars\nare additionally predicted to emit coherent radio bursts following their\nformation and may constitute a small fraction of the progenitors of fast radio\nbursts. Black holes, by contrast, are not expected to emit coherent radio\nbursts in the time following the GRB itself. Here we present rapid follow-up\nobservations of the short GRB 201006A using LOFAR. We have detected a\n5.6$\\sigma$, short, coherent radio flash at 144 MHz at 76.6 mins post-burst.\nThis radio flash is 27 arcsec offset from the GRB location, which has a\nprobability of occurring by chance of $\\sim$0.5% (2.6$\\sigma$) when accounting\nfor measurement uncertainties. Despite the offset, we show that the probability\nof finding an unrelated transient within 40 arcsec of the GRB location is\n$<10^{-6}$ and conclude that this is likely to be the radio counterpart to GRB\n201006A. The radio flash is tentatively (2.5$\\sigma$) shown to be highly\ndispersed, allowing a distance estimate, corresponding to a redshift of\n$0.58\\pm0.06$, that is in the range of typical short GRB distances. Using the\nestimated distance, the...",
            "author": [
                "A. Rowlinson",
                "I. de Ruiter",
                "R. L. C. Starling",
                "K. M. Rajwade",
                "A. Hennessy",
                "R. A. M. J. Wijers",
                "G. E. Anderson",
                "M. Mevius",
                "D. Ruhe",
                "K. Gourdji",
                "A. J. van der Horst",
                "S. ter Veen",
                "K. Wiersema"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04237v1",
                "http://arxiv.org/pdf/2312.04237v1"
            ],
            "primary_category": "astro-ph.HE",
            "category": [
                "astro-ph.HE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04234v1",
            "title": "Graph Convolutions Enrich the Self-Attention in Transformers!",
            "updated": "2023-12-07T11:40:32Z",
            "published": "2023-12-07T11:40:32Z",
            "summary": "Transformers, renowned for their self-attention mechanism, have achieved\nstate-of-the-art performance across various tasks in natural language\nprocessing, computer vision, time-series modeling, etc. However, one of the\nchallenges with deep Transformer models is the oversmoothing problem, where\nrepresentations across layers converge to indistinguishable values, leading to\nsignificant performance degradation. We interpret the original self-attention\nas a simple graph filter and redesign it from a graph signal processing (GSP)\nperspective. We propose graph-filter-based self-attention (GFSA) to learn a\ngeneral yet effective one, whose complexity, however, is slightly larger than\nthat of the original self-attention mechanism. We demonstrate that GFSA\nimproves the performance of Transformers in various fields, including computer\nvision, natural language processing, graph pattern classification, speech\nrecognition, and code classification.",
            "author": [
                "Jeongwhan Choi",
                "Hyowon Wi",
                "Jayoung Kim",
                "Yehjin Shin",
                "Kookjin Lee",
                "Nathaniel Trask",
                "Noseong Park"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04234v1",
                "http://arxiv.org/pdf/2312.04234v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04233v1",
            "title": "Fine-tune vision foundation model for crack segmentation in civil\n  infrastructures",
            "updated": "2023-12-07T11:39:11Z",
            "published": "2023-12-07T11:39:11Z",
            "summary": "Large-scale foundation models have become the mainstream method in the field\nof deep learning, while in civil engineering, the scale of AI models is\nstrictly limited. In this work, vision foundation model is introduced for crack\nsegmentation. Two Parameter-efficient fine-tuning methods, adapter and low-rank\nadaptation, are adopted to fine-tune the foundation model in the field of\nsemantic segmentation: Segment Anything Model (SAM). The fine-tuned model\nCrackSAM is much larger than all the existing crack segmentation models, but\nshows excellent performance. To test the zero-shot performance of the proposed\nmethod, two unique datasets related to road and exterior wall cracks are\ncollected, annotated and open-sourced, in total 810 images. Comparative\nexperiments are conducted with twelve mature semantic segmentation models. On\ndatasets with artificial noise and previously unseen datasets, the performance\nof CrackSAM far exceeds that of all state-of-the-art models. CrackSAM exhibits\nremarkable superiority, particularly in challenging conditions such as dim\nlighting, shadows, road markings, construction joints, and other interference\nfactors. Such cross-scenario results demonstrate the outstanding zero-shot\ncapability of foundation models, and provide new ideas for the development of\nvision models in civil engineering.",
            "author": [
                "Kang Ge",
                "Chen Wang",
                "Yutao Guo"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04233v1",
                "http://arxiv.org/pdf/2312.04233v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04231v1",
            "title": "Adventures of Trustworthy Vision-Language Models: A Survey",
            "updated": "2023-12-07T11:31:20Z",
            "published": "2023-12-07T11:31:20Z",
            "summary": "Recently, transformers have become incredibly popular in computer vision and\nvision-language tasks. This notable rise in their usage can be primarily\nattributed to the capabilities offered by attention mechanisms and the\noutstanding ability of transformers to adapt and apply themselves to a variety\nof tasks and domains. Their versatility and state-of-the-art performance have\nestablished them as indispensable tools for a wide array of applications.\nHowever, in the constantly changing landscape of machine learning, the\nassurance of the trustworthiness of transformers holds utmost importance. This\npaper conducts a thorough examination of vision-language transformers,\nemploying three fundamental principles of responsible AI: Bias, Robustness, and\nInterpretability. The primary objective of this paper is to delve into the\nintricacies and complexities associated with the practical use of transformers,\nwith the overarching goal of advancing our comprehension of how to enhance\ntheir reliability and accountability.",
            "author": [
                "Mayank Vatsa",
                "Anubhooti Jain",
                "Richa Singh"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04231v1",
                "http://arxiv.org/pdf/2312.04231v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04226v1",
            "title": "Dynamic Data-Driven Digital Twins for Blockchain Systems",
            "updated": "2023-12-07T11:18:57Z",
            "published": "2023-12-07T11:18:57Z",
            "summary": "In recent years, we have seen an increase in the adoption of blockchain-based\nsystems in non-financial applications, looking to benefit from what the\ntechnology has to offer. Although many fields have managed to include\nblockchain in their core functionalities, the adoption of blockchain, in\ngeneral, is constrained by the so-called trilemma trade-off between\ndecentralization, scalability, and security. In our previous work, we have\nshown that using a digital twin for dynamically managing blockchain systems\nduring runtime can be effective in managing the trilemma trade-off. Our Digital\nTwin leverages DDDAS feedback loop, which is responsible for getting the data\nfrom the system to the digital twin, conducting optimisation, and updating the\nphysical system. This paper examines how leveraging DDDAS feedback loop can\nsupport the optimisation component of the trilemma benefiting from\nReinforcement Learning agents and a simulation component to augment the quality\nof the learned model while reducing the computational overhead required for\ndecision-making.",
            "author": [
                "Georgios Diamantopoulos",
                "Nikos Tziritas",
                "Rami Bahsoon",
                "Georgios Theodoropoulos"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04226v1",
                "http://arxiv.org/pdf/2312.04226v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.AI",
                "cs.DC",
                "cs.PF"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04225v1",
            "title": "TLCE: Transfer-Learning Based Classifier Ensembles for Few-Shot\n  Class-Incremental Learning",
            "updated": "2023-12-07T11:16:00Z",
            "published": "2023-12-07T11:16:00Z",
            "summary": "Few-shot class-incremental learning (FSCIL) struggles to incrementally\nrecognize novel classes from few examples without catastrophic forgetting of\nold classes or overfitting to new classes. We propose TLCE, which ensembles\nmultiple pre-trained models to improve separation of novel and old classes.\nTLCE minimizes interference between old and new classes by mapping old class\nimages to quasi-orthogonal prototypes using episodic training. It then\nensembles diverse pre-trained models to better adapt to novel classes despite\ndata imbalance. Extensive experiments on various datasets demonstrate that our\ntransfer learning ensemble approach outperforms state-of-the-art FSCIL methods.",
            "author": [
                "Shuangmei Wang",
                "Yang Cao",
                "Tieru Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04225v1",
                "http://arxiv.org/pdf/2312.04225v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04223v1",
            "title": "Sensing the Body: Towards Best Practices for Integrating Physiological\n  Signals in HCI",
            "updated": "2023-12-07T11:14:05Z",
            "published": "2023-12-07T11:14:05Z",
            "summary": "Recently, we saw a trend toward using physiological signals in interactive\nsystems. These signals, offering deep insights into users' internal states and\nhealth, herald a new era for HCI. However, as this is an interdisciplinary\napproach, many challenges arise for HCI researchers, such as merging diverse\ndisciplines, from understanding physiological functions to design expertise.\nAlso, isolated research endeavors limit the scope and reach of findings. This\nworkshop aims to bridge these gaps, fostering cross-disciplinary discussions on\nusability, open science, and ethics tied to physiological data in HCI. In this\nworkshop, we will discuss best practices for embedding physiological signals in\ninteractive systems. Through collective efforts, we seek to craft a guiding\ndocument for best practices in physiological HCI research, ensuring that it\nremains grounded in shared principles and methodologies as the field advances.",
            "author": [
                "Francesco Chiossi",
                "Ekaterina R. Stepanova",
                "Benjamin Tag",
                "Monica Perusquia-Hernandez",
                "Alexandra Kitson",
                "Arindam Dey",
                "Sven Mayer",
                "Abdallah El Ali"
            ],
            "link": [
                "http://dx.doi.org/10.1145/3613905.3636286",
                "http://arxiv.org/abs/2312.04223v1",
                "http://arxiv.org/pdf/2312.04223v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04216v1",
            "title": "CODEX: A Cluster-Based Method for Explainable Reinforcement Learning",
            "updated": "2023-12-07T11:04:37Z",
            "published": "2023-12-07T11:04:37Z",
            "summary": "Despite the impressive feats demonstrated by Reinforcement Learning (RL),\nthese algorithms have seen little adoption in high-risk, real-world\napplications due to current difficulties in explaining RL agent actions and\nbuilding user trust. We present Counterfactual Demonstrations for Explanation\n(CODEX), a method that incorporates semantic clustering, which can effectively\nsummarize RL agent behavior in the state-action space. Experimentation on the\nMiniGrid and StarCraft II gaming environments reveals the semantic clusters\nretain temporal as well as entity information, which is reflected in the\nconstructed summary of agent behavior. Furthermore, clustering the\ndiscrete+continuous game-state latent representations identifies the most\ncrucial episodic events, demonstrating a relationship between the latent and\nsemantic spaces. This work contributes to the growing body of work that strives\nto unlock the power of RL for widespread use by leveraging and extending\ntechniques from Natural Language Processing.",
            "author": [
                "Timothy K. Mathes",
                "Jessica Inman",
                "Andr\u00e9s Col\u00f3n",
                "Simon Khan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04216v1",
                "http://arxiv.org/pdf/2312.04216v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04215v1",
            "title": "Guided Reconstruction with Conditioned Diffusion Models for Unsupervised\n  Anomaly Detection in Brain MRIs",
            "updated": "2023-12-07T11:03:42Z",
            "published": "2023-12-07T11:03:42Z",
            "summary": "Unsupervised anomaly detection in Brain MRIs aims to identify abnormalities\nas outliers from a healthy training distribution. Reconstruction-based\napproaches that use generative models to learn to reconstruct healthy brain\nanatomy are commonly used for this task. Diffusion models are an emerging class\nof deep generative models that show great potential regarding reconstruction\nfidelity. However, they face challenges in preserving intensity characteristics\nin the reconstructed images, limiting their performance in anomaly detection.\nTo address this challenge, we propose to condition the denoising mechanism of\ndiffusion models with additional information about the image to reconstruct\ncoming from a latent representation of the noise-free input image. This\nconditioning enables high-fidelity reconstruction of healthy brain structures\nwhile aligning local intensity characteristics of input-reconstruction pairs.\nWe evaluate our method's reconstruction quality, domain adaptation features and\nfinally segmentation performance on publicly available data sets with various\npathologies. Using our proposed conditioning mechanism we can reduce the\nfalse-positive predictions and enable a more precise delineation of anomalies\nwhich significantly enhances the anomaly detection performance compared to\nestablished state-of-the-art approaches to unsupervised anomaly detection in\nbrain MRI. Furthermore, our approach shows promise in domain adaptation across\ndifferent MRI acquisitions and simulated contrasts, a crucial property of\ngeneral anomaly detection methods.",
            "author": [
                "Finn Behrendt",
                "Debayan Bhattacharya",
                "Robin Mieling",
                "Lennart Maack",
                "Julia Kr\u00fcger",
                "Roland Opfer",
                "Alexander Schlaefer"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04215v1",
                "http://arxiv.org/pdf/2312.04215v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04214v1",
            "title": "Accurate Distances Measures and Machine Learning of the Texture-Property\n  Relation for Crystallographic Textures Represented by One-Point Statistics",
            "updated": "2023-12-07T11:02:41Z",
            "published": "2023-12-07T11:02:41Z",
            "summary": "The crystallographic texture of metallic materials is a key microstructural\nfeature that is responsible for the anisotropic behavior, e.g., important in\nforming operations. In materials science, crystallographic texture is commonly\ndescribed by the orientation distribution function, which is defined as the\nprobability density function of the orientations of the monocrystal grains\nconforming a polycrystalline material. For representing the orientation\ndistribution function, there are several approaches such as using generalized\nspherical harmonics, orientation histograms, and pole figure images . Measuring\ndistances between crystallographic textures is essential for any task that\nrequires assessing texture similarities, e.g. to guide forming processes.\nTherefore, we introduce novel distance measures based on (i) the Earth Movers\nDistance that takes into account local distance information encoded in\nhistogram-based texture representations and (ii) a distance measure based on\npole figure images. For this purpose, we evaluate and compare existing distance\nmeasures for selected use-cases. The present study gives insights into\nadvantages and drawbacks of using certain texture representations and distance\nmeasures with emphasis on applications in materials design and optimal process\ncontrol.",
            "author": [
                "Tarek Iraki",
                "Lukas Morand",
                "Norbert Link",
                "Stefan Sandfeld",
                "Dirk Helm"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04214v1",
                "http://arxiv.org/pdf/2312.04214v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04209v1",
            "title": "Constrained Hierarchical Clustering via Graph Coarsening and Optimal\n  Cuts",
            "updated": "2023-12-07T10:52:06Z",
            "published": "2023-12-07T10:52:06Z",
            "summary": "Motivated by extracting and summarizing relevant information in short\nsentence settings, such as satisfaction questionnaires, hotel reviews, and\nX/Twitter, we study the problem of clustering words in a hierarchical fashion.\nIn particular, we focus on the problem of clustering with horizontal and\nvertical structural constraints. Horizontal constraints are typically\ncannot-link and must-link among words, while vertical constraints are\nprecedence constraints among cluster levels. We overcome state-of-the-art\nbottlenecks by formulating the problem in two steps: first, as a\nsoft-constrained regularized least-squares which guides the result of a\nsequential graph coarsening algorithm towards the horizontal feasible set.\nThen, flat clusters are extracted from the resulting hierarchical tree by\ncomputing optimal cut heights based on the available constraints. We show that\nthe resulting approach compares very well with respect to existing algorithms\nand is computationally light.",
            "author": [
                "Eliabelle Mauduit",
                "Andrea Simonetto"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04209v1",
                "http://arxiv.org/pdf/2312.04209v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04204v1",
            "title": "Wavelength-multiplexed Delayed Inputs for Memory Enhancement of\n  Microring-based Reservoir Computing",
            "updated": "2023-12-07T10:40:37Z",
            "published": "2023-12-07T10:40:37Z",
            "summary": "We numerically demonstrate a silicon add-drop microring-based reservoir\ncomputing scheme that combines parallel delayed inputs and wavelength division\nmultiplexing. The scheme solves memory-demanding tasks like time-series\nprediction with good performance without requiring external optical feedback.",
            "author": [
                "Bernard J. Giron Castro",
                "Christophe Peucheret",
                "Francesco Da Ros"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04204v1",
                "http://arxiv.org/pdf/2312.04204v1"
            ],
            "primary_category": "cs.NE",
            "category": [
                "cs.NE",
                "cs.ET",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04193v1",
            "title": "Language Model Knowledge Distillation for Efficient Question Answering\n  in Spanish",
            "updated": "2023-12-07T10:21:22Z",
            "published": "2023-12-07T10:21:22Z",
            "summary": "Recent advances in the development of pre-trained Spanish language models has\nled to significant progress in many Natural Language Processing (NLP) tasks,\nsuch as question answering. However, the lack of efficient models imposes a\nbarrier for the adoption of such models in resource-constrained environments.\nTherefore, smaller distilled models for the Spanish language could be proven to\nbe highly scalable and facilitate their further adoption on a variety of tasks\nand scenarios. In this work, we take one step in this direction by developing\nSpanishTinyRoBERTa, a compressed language model based on RoBERTa for efficient\nquestion answering in Spanish. To achieve this, we employ knowledge\ndistillation from a large model onto a lighter model that allows for a wider\nimplementation, even in areas with limited computational resources, whilst\nattaining negligible performance sacrifice. Our experiments show that the dense\ndistilled model can still preserve the performance of its larger counterpart,\nwhile significantly increasing inference speedup. This work serves as a\nstarting point for further research and investigation of model compression\nefforts for Spanish language models across various NLP tasks.",
            "author": [
                "Adri\u00e1n Bazaga",
                "Pietro Li\u00f2",
                "Gos Micklem"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04193v1",
                "http://arxiv.org/pdf/2312.04193v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04189v1",
            "title": "Joint-Individual Fusion Structure with Fusion Attention Module for\n  Multi-Modal Skin Cancer Classification",
            "updated": "2023-12-07T10:16:21Z",
            "published": "2023-12-07T10:16:21Z",
            "summary": "Most convolutional neural network (CNN) based methods for skin cancer\nclassification obtain their results using only dermatological images. Although\ngood classification results have been shown, more accurate results can be\nachieved by considering the patient's metadata, which is valuable clinical\ninformation for dermatologists. Current methods only use the simple joint\nfusion structure (FS) and fusion modules (FMs) for the multi-modal\nclassification methods, there still is room to increase the accuracy by\nexploring more advanced FS and FM. Therefore, in this paper, we design a new\nfusion method that combines dermatological images (dermoscopy images or\nclinical images) and patient metadata for skin cancer classification from the\nperspectives of FS and FM. First, we propose a joint-individual fusion (JIF)\nstructure that learns the shared features of multi-modality data and preserves\nspecific features simultaneously. Second, we introduce a fusion attention (FA)\nmodule that enhances the most relevant image and metadata features based on\nboth the self and mutual attention mechanism to support the decision-making\npipeline. We compare the proposed JIF-MMFA method with other state-of-the-art\nfusion methods on three different public datasets. The results show that our\nJIF-MMFA method improves the classification results for all tested CNN\nbackbones and performs better than the other fusion methods on the three public\ndatasets, demonstrating our method's effectiveness and robustness",
            "author": [
                "Peng Tang",
                "Xintong Yan",
                "Yang Nan",
                "Xiaobin Hu",
                "Xiaobin Hu",
                "Bjoern H Menzee. Sebastian Krammer",
                "Tobias Lasser"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04189v1",
                "http://arxiv.org/pdf/2312.04189v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04180v1",
            "title": "AI and Jobs: Has the Inflection Point Arrived? Evidence from an Online\n  Labor Platform",
            "updated": "2023-12-07T10:06:34Z",
            "published": "2023-12-07T10:06:34Z",
            "summary": "Artificial intelligence (AI) refers to the ability of machines or software to\nmimic or even surpass human intelligence in a given cognitive task. While\nhumans learn by both induction and deduction, the success of current AI is\nrooted in induction, relying on its ability to detect statistical regularities\nin task input -- an ability learnt from a vast amount of training data using\nenormous computation resources. We examine the performance of such a\nstatistical AI in a human task through the lens of four factors, including task\nlearnability, statistical resource, computation resource, and learning\ntechniques, and then propose a three-phase visual framework to understand the\nevolving relation between AI and jobs. Based on this conceptual framework, we\ndevelop a simple economic model of competition to show the existence of an\ninflection point for each occupation. Before AI performance crosses the\ninflection point, human workers always benefit from an improvement in AI\nperformance, but after the inflection point, human workers become worse off\nwhenever such an improvement occurs. To offer empirical evidence, we first\nargue that AI performance has passed the inflection point for the occupation of\ntranslation but not for the occupation of web development. We then study how\nthe launch of ChatGPT, which led to significant improvement of AI performance\non many tasks, has affected workers in these two occupations on a large online\nlabor platform. Consistent with the inflection point conjecture, we find that\ntranslators are negatively affected by the shock both in terms of the number of\naccepted jobs and the earnings from those jobs, while web developers are\npositively affected by the very same shock. Given the potentially large\ndisruption of AI on employment, more studies on more occupations using data\nfrom different platforms are urgently needed.",
            "author": [
                "Dandan Qiao",
                "Huaxia Rui",
                "Qian Xiong"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04180v1",
                "http://arxiv.org/pdf/2312.04180v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CY",
                "econ.GN",
                "q-fin.EC",
                "J.4"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04174v1",
            "title": "Coherent energy and force uncertainty in deep learning force fields",
            "updated": "2023-12-07T09:49:05Z",
            "published": "2023-12-07T09:49:05Z",
            "summary": "In machine learning energy potentials for atomic systems, forces are commonly\nobtained as the negative derivative of the energy function with respect to\natomic positions. To quantify aleatoric uncertainty in the predicted energies,\na widely used modeling approach involves predicting both a mean and variance\nfor each energy value. However, this model is not differentiable under the\nusual white noise assumption, so energy uncertainty does not naturally\ntranslate to force uncertainty. In this work we propose a machine learning\npotential energy model in which energy and force aleatoric uncertainty are\nlinked through a spatially correlated noise process. We demonstrate our\napproach on an equivariant messages passing neural network potential trained on\nenergies and forces on two out-of-equilibrium molecular datasets. Furthermore,\nwe also show how to obtain epistemic uncertainties in this setting based on a\nBayesian interpretation of deep ensemble models.",
            "author": [
                "Peter Bj\u00f8rn J\u00f8rgensen",
                "Jonas Busk",
                "Ole Winther",
                "Mikkel N. Schmidt"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04174v1",
                "http://arxiv.org/pdf/2312.04174v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG",
                "physics.comp-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04171v1",
            "title": "A novel feature selection framework for incomplete data",
            "updated": "2023-12-07T09:45:14Z",
            "published": "2023-12-07T09:45:14Z",
            "summary": "Feature selection on incomplete datasets is an exceptionally challenging\ntask. Existing methods address this challenge by first employing imputation\nmethods to complete the incomplete data and then conducting feature selection\nbased on the imputed data. Since imputation and feature selection are entirely\nindependent steps, the importance of features cannot be considered during\nimputation. However, in real-world scenarios or datasets, different features\nhave varying degrees of importance. To address this, we propose a novel\nincomplete data feature selection framework that considers feature importance.\nThe framework mainly consists of two alternating iterative stages: the M-stage\nand the W-stage. In the M-stage, missing values are imputed based on a given\nfeature importance vector and multiple initial imputation results. In the\nW-stage, an improved reliefF algorithm is employed to learn the feature\nimportance vector based on the imputed data. Specifically, the feature\nimportance vector obtained in the current iteration of the W-stage serves as\ninput for the next iteration of the M-stage. Experimental results on both\nartificially generated and real incomplete datasets demonstrate that the\nproposed method outperforms other approaches significantly.",
            "author": [
                "Cong Guo"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04171v1",
                "http://arxiv.org/pdf/2312.04171v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04168v1",
            "title": "Augmentation-Free Dense Contrastive Knowledge Distillation for Efficient\n  Semantic Segmentation",
            "updated": "2023-12-07T09:37:28Z",
            "published": "2023-12-07T09:37:28Z",
            "summary": "In recent years, knowledge distillation methods based on contrastive learning\nhave achieved promising results on image classification and object detection\ntasks. However, in this line of research, we note that less attention is paid\nto semantic segmentation. Existing methods heavily rely on data augmentation\nand memory buffer, which entail high computational resource demands when\napplying them to handle semantic segmentation that requires to preserve\nhigh-resolution feature maps for making dense pixel-wise predictions. In order\nto address this problem, we present Augmentation-free Dense Contrastive\nKnowledge Distillation (Af-DCD), a new contrastive distillation learning\nparadigm to train compact and accurate deep neural networks for semantic\nsegmentation applications. Af-DCD leverages a masked feature mimicking\nstrategy, and formulates a novel contrastive learning loss via taking advantage\nof tactful feature partitions across both channel and spatial dimensions,\nallowing to effectively transfer dense and structured local knowledge learnt by\nthe teacher model to a target student model while maintaining training\nefficiency. Extensive experiments on five mainstream benchmarks with various\nteacher-student network pairs demonstrate the effectiveness of our approach.\nFor instance, the DeepLabV3-Res18|DeepLabV3-MBV2 model trained by Af-DCD\nreaches 77.03%|76.38% mIOU on Cityscapes dataset when choosing DeepLabV3-Res101\nas the teacher, setting new performance records. Besides that, Af-DCD achieves\nan absolute mIOU improvement of 3.26%|3.04%|2.75%|2.30%|1.42% compared with\nindividually trained counterpart on Cityscapes|Pascal\nVOC|Camvid|ADE20K|COCO-Stuff-164K. Code is available at\nhttps://github.com/OSVAI/Af-DCD",
            "author": [
                "Jiawei Fan",
                "Chao Li",
                "Xiaolong Liu",
                "Meina Song",
                "Anbang Yao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04168v1",
                "http://arxiv.org/pdf/2312.04168v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04167v1",
            "title": "Mixture of Dynamical Variational Autoencoders for Multi-Source\n  Trajectory Modeling and Separation",
            "updated": "2023-12-07T09:36:31Z",
            "published": "2023-12-07T09:36:31Z",
            "summary": "In this paper, we propose a latent-variable generative model called mixture\nof dynamical variational autoencoders (MixDVAE) to model the dynamics of a\nsystem composed of multiple moving sources. A DVAE model is pre-trained on a\nsingle-source dataset to capture the source dynamics. Then, multiple instances\nof the pre-trained DVAE model are integrated into a multi-source mixture model\nwith a discrete observation-to-source assignment latent variable. The posterior\ndistributions of both the discrete observation-to-source assignment variable\nand the continuous DVAE variables representing the sources content/position are\nestimated using a variational expectation-maximization algorithm, leading to\nmulti-source trajectories estimation. We illustrate the versatility of the\nproposed MixDVAE model on two tasks: a computer vision task, namely\nmulti-object tracking, and an audio processing task, namely single-channel\naudio source separation. Experimental results show that the proposed method\nworks well on these two tasks, and outperforms several baseline methods.",
            "author": [
                "Xiaoyu Lin",
                "Laurent Girin",
                "Xavier Alameda-Pineda"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04167v1",
                "http://arxiv.org/pdf/2312.04167v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04166v1",
            "title": "Improving Communication Efficiency of Federated Distillation via\n  Accumulating Local Updates",
            "updated": "2023-12-07T09:36:18Z",
            "published": "2023-12-07T09:36:18Z",
            "summary": "As an emerging federated learning paradigm, federated distillation enables\ncommunication-efficient model training by transmitting only small-scale\nknowledge during the learning process. To further improve the communication\nefficiency of federated distillation, we propose a novel technique, ALU, which\naccumulates multiple rounds of local updates before transferring the knowledge\nto the central server. ALU drastically decreases the frequency of communication\nin federated distillation, thereby significantly reducing the communication\noverhead during the training process. Empirical experiments demonstrate the\nsubstantial effect of ALU in improving the communication efficiency of\nfederated distillation.",
            "author": [
                "Zhiyuan Wu",
                "Sheng Sun",
                "Yuwei Wang",
                "Min Liu",
                "Tian Wen",
                "Wen Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04166v1",
                "http://arxiv.org/pdf/2312.04166v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04163v1",
            "title": "Multi-scale Residual Transformer for VLF Lightning Transients\n  Classification",
            "updated": "2023-12-07T09:26:58Z",
            "published": "2023-12-07T09:26:58Z",
            "summary": "The utilization of Very Low Frequency (VLF) electromagnetic signals in\nnavigation systems is widespread. However, the non-stationary behavior of\nlightning signals can affect VLF electromagnetic signal transmission.\nAccurately classifying lightning signals is important for reducing interference\nand noise in VLF, thereby improving the reliability and overall performance of\nnavigation systems. In recent years, the evolution of deep learning,\nspecifically Convolutional Neural Network (CNNs), has sparked a transformation\nin lightning classification, surpassing traditional statistical methodologies.\nExisting CNN models have limitations as they overlook the diverse attributes of\nlightning signals across different scales and neglect the significance of\ntemporal sequencing in sequential signals. This study introduces an innovative\nmulti-scale residual transform (MRTransformer) that not only has the ability to\ndiscern intricate fine-grained patterns while also weighing the significance of\ndifferent aspects within the input lightning signal sequence. This model\nperforms the attributes of the lightning signal across different scales and the\nlevel of accuracy reached 90% in the classification. In future work, this model\nhas the potential applied to a comprehensive understanding of the localization\nand waveform characteristics of lightning signals.",
            "author": [
                "Jinghao Sun",
                "Tingting Ji",
                "Guoyu Wang",
                "Rui Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04163v1",
                "http://arxiv.org/pdf/2312.04163v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04160v1",
            "title": "Text as Image: Learning Transferable Adapter for Multi-Label\n  Classification",
            "updated": "2023-12-07T09:22:20Z",
            "published": "2023-12-07T09:22:20Z",
            "summary": "Pre-trained vision-language models have notably accelerated progress of\nopen-world concept recognition. Their impressive zero-shot ability has recently\nbeen transferred to multi-label image classification via prompt tuning,\nenabling to discover novel labels in an open-vocabulary manner. However, this\nparadigm suffers from non-trivial training costs, and becomes computationally\nprohibitive for a large number of candidate labels. To address this issue, we\nnote that vision-language pre-training aligns images and texts in a unified\nembedding space, making it potential for an adapter network to identify labels\nin visual modality while be trained in text modality. To enhance such\ncross-modal transfer ability, a simple yet effective method termed random\nperturbation is proposed, which enables the adapter to search for potential\nvisual embeddings by perturbing text embeddings with noise during training,\nresulting in better performance in visual modality. Furthermore, we introduce\nan effective approach to employ large language models for multi-label\ninstruction-following text generation. In this way, a fully automated pipeline\nfor visual label recognition is developed without relying on any manual data.\nExtensive experiments on public benchmarks show the superiority of our method\nin various multi-label classification tasks.",
            "author": [
                "Xuelin Zhu",
                "Jiuxin Cao",
                "Jian liu",
                "Dongqi Tang",
                "Furong Xu",
                "Weijia Liu",
                "Jiawei Ge",
                "Bo Liu",
                "Qingpei Guo",
                "Tianyi Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04160v1",
                "http://arxiv.org/pdf/2312.04160v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04159v1",
            "title": "Zero-Touch Networks: Towards Next-Generation Network Automation",
            "updated": "2023-12-07T09:21:41Z",
            "published": "2023-12-07T09:21:41Z",
            "summary": "The Zero-touch network and Service Management (ZSM) framework represents an\nemerging paradigm in the management of the fifth-generation (5G) and Beyond\n(5G+) networks, offering automated self-management and self-healing\ncapabilities to address the escalating complexity and the growing data volume\nof modern networks. ZSM frameworks leverage advanced technologies such as\nMachine Learning (ML) to enable intelligent decision-making and reduce human\nintervention. This paper presents a comprehensive survey of Zero-Touch Networks\n(ZTNs) within the ZSM framework, covering network optimization, traffic\nmonitoring, energy efficiency, and security aspects of next-generational\nnetworks. The paper explores the challenges associated with ZSM, particularly\nthose related to ML, which necessitate the need to explore diverse network\nautomation solutions. In this context, the study investigates the application\nof Automated ML (AutoML) in ZTNs, to reduce network management costs and\nenhance performance. AutoML automates the selection and tuning process of a ML\nmodel for a given task. Specifically, the focus is on AutoML's ability to\npredict application throughput and autonomously adapt to data drift.\nExperimental results demonstrate the superiority of the proposed AutoML\npipeline over traditional ML in terms of prediction accuracy. Integrating\nAutoML and ZSM concepts significantly reduces network configuration and\nmanagement efforts, allowing operators to allocate more time and resources to\nother important tasks. The paper also provides a high-level 5G system\narchitecture incorporating AutoML and ZSM concepts. This research highlights\nthe potential of ZTNs and AutoML to revolutionize the management of 5G+\nnetworks, enabling automated decision-making and empowering network operators\nto achieve higher efficiency, improved performance, and enhanced user\nexperience.",
            "author": [
                "Mirna El Rajab",
                "Li Yang",
                "Abdallah Shami"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04159v1",
                "http://arxiv.org/pdf/2312.04159v1"
            ],
            "primary_category": "cs.NI",
            "category": [
                "cs.NI",
                "cs.LG",
                "68T01, 68M10, 90B18",
                "I.2.0; I.2.2; C.2.0"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04158v1",
            "title": "Safety-Enhanced Self-Learning for Optimal Power Converter Control",
            "updated": "2023-12-07T09:19:38Z",
            "published": "2023-12-07T09:19:38Z",
            "summary": "Data-driven learning-based control methods such as reinforcement learning\n(RL) have become increasingly popular with recent proliferation of the machine\nlearning paradigm. These methods address the parameter sensitiveness and\nunmodeled dynamics in model-based controllers, such as finite control-set model\npredictive control. RL agents are typically utilized in simulation\nenvironments, where they are allowed to explore multiple \"unsafe\" actions\nduring the learning process. However, this type of learning is not applicable\nto online self-learning of controllers in physical power converters, because\nunsafe actions would damage them. To address this, this letter proposes a safe\nonline RL-based control framework to autonomously find the optimal switching\nstrategy for the power converters, while ensuring system safety during the\nentire self-learning process. The proposed safe online RL-based control is\nvalidated in a practical testbed on a two-level voltage source converter\nsystem, and the results confirm the effectiveness of the proposed method.",
            "author": [
                "Yihao Wan",
                "Qianwen Xu",
                "Tomislav Dragi\u010devi\u0107"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04158v1",
                "http://arxiv.org/pdf/2312.04158v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04155v1",
            "title": "Resource Allocation for Semantic Communication under Physical-layer\n  Security",
            "updated": "2023-12-07T09:12:26Z",
            "published": "2023-12-07T09:12:26Z",
            "summary": "Semantic communication is deemed as a revolution of Shannon's paradigm in the\nsix-generation (6G) wireless networks. It aims at transmitting the extracted\ninformation rather than the original data, which receivers will try to recover.\nIntuitively, the larger extracted information, the longer latency of semantic\ncommunication will be. Besides, larger extracted information will result in\nmore accurate reconstructed information, thereby causing a higher utility of\nthe semantic communication system. Shorter latency and higher utility are\ndesirable objectives for the system, so there will be a trade-off between\nutility and latency. This paper proposes a joint optimization algorithm for\ntotal latency and utility. Moreover, security is essential for the semantic\ncommunication system. We incorporate the secrecy rate, a physical-layer\nsecurity method, into the optimization problem. The secrecy rate is the\ncommunication rate at which no information is disclosed to an eavesdropper.\nExperimental results demonstrate that the proposed algorithm obtains the best\njoint optimization performance compared to the baselines.",
            "author": [
                "Yang Li",
                "Xinyu Zhou",
                "Jun Zhao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04155v1",
                "http://arxiv.org/pdf/2312.04155v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP",
                "cs.CR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04152v1",
            "title": "EulerMormer: Robust Eulerian Motion Magnification via Dynamic Filtering\n  within Transformer",
            "updated": "2023-12-07T09:10:16Z",
            "published": "2023-12-07T09:10:16Z",
            "summary": "Video Motion Magnification (VMM) aims to break the resolution limit of human\nvisual perception capability and reveal the imperceptible minor motion that\ncontains valuable information in the macroscopic domain. However, challenges\narise in this task due to photon noise inevitably introduced by photographic\ndevices and spatial inconsistency in amplification, leading to flickering\nartifacts in static fields and motion blur and distortion in dynamic fields in\nthe video. Existing methods focus on explicit motion modeling without\nemphasizing prioritized denoising during the motion magnification process. This\npaper proposes a novel dynamic filtering strategy to achieve static-dynamic\nfield adaptive denoising. Specifically, based on Eulerian theory, we separate\ntexture and shape to extract motion representation through inter-frame shape\ndifferences, expecting to leverage these subdivided features to solve this task\nfinely. Then, we introduce a novel dynamic filter that eliminates noise cues\nand preserves critical features in the motion magnification and amplification\ngeneration phases. Overall, our unified framework, EulerMormer, is a pioneering\neffort to first equip with Transformer in learning-based VMM. The core of the\ndynamic filter lies in a global dynamic sparse cross-covariance attention\nmechanism that explicitly removes noise while preserving vital information,\ncoupled with a multi-scale dual-path gating mechanism that selectively\nregulates the dependence on different frequency features to reduce spatial\nattenuation and complement motion boundaries. We demonstrate extensive\nexperiments that EulerMormer achieves more robust video motion magnification\nfrom the Eulerian perspective, significantly outperforming state-of-the-art\nmethods. The source code is available at\nhttps://github.com/VUT-HFUT/EulerMormer.",
            "author": [
                "Fei Wang",
                "Dan Guo",
                "Kun Li",
                "Meng Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04152v1",
                "http://arxiv.org/pdf/2312.04152v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04151v1",
            "title": "The Ly$\u03b1$ non-detection by JWST NIRSpec of a strong Ly$\u03b1$\n  emitter at $z=5.66$ confirmed by MUSE",
            "updated": "2023-12-07T09:07:13Z",
            "published": "2023-12-07T09:07:13Z",
            "summary": "The detections of Lyman-$\\alpha$ ($\\rm Ly\\alpha$) emission in galaxies with\nredshifts above 5 are of utmost importance for constraining the cosmic\nreionization timeline, yet such detections are usually based on slit\nspectroscopy. Here we investigate the significant bias induced by slit\nplacement on the estimate of $\\rm Ly\\alpha$ escape fraction ( $f_{\\rm\nesc}^{\\mathrm{Ly\\alpha}}$), by presenting a galaxy (dubbed A2744-z6Lya) at\n$z=5.66$ where its deep JWST NIRSpec prism spectroscopy completely misses the\nstrong $\\rm Ly\\alpha$ emission detected in the MUSE data. A2744-z6Lya exhibits\na pronounced UV continuum with an extremely steep spectral slope of\n$\\beta=-2.574_{-0.008}^{+0.008}$, and it has a stellar mass of\n$\\mathrm{\\sim10^{8.82}~M_\\odot}$, a star-formation rate of\n$\\mathrm{\\sim8.35~M_\\odot yr^{-1}}$ and gas-phase metallicity of\n$\\mathrm{12+log\\,(O/H)\\sim7.88}$. The observed flux and rest-frame equivalent\nwidth of its Ly$\\alpha$ from MUSE spectroscopy are $1.2\\times \\rm 10^{-16}\nerg~s^{-1}cm^{-2}$ and 75\\r{A}, equivalent to $f_{\\rm\nesc}^{\\mathrm{Ly\\alpha}}=78\\pm4 \\%$. However, its Ly$\\alpha$ non-detection from\nJWST NIRSpec gives a 5-$\\sigma$ upper limit of $<13 \\%$, in stark contrast to\nthat derived from MUSE. To explore the reasons for this bias, we perform\nspatially resolved stellar population analysis of A2744-z6Lya using the JWST\nNIRCam imaging data to construct 2-dimensional maps of SFR, dust extinction and\nneutral hydrogen column density. We find that the absence of Ly$\\alpha$ in the\nslit regions probably stems from both the resonance scattering effect of\nneutral hydrogen and dust extinction. Through analyzing an extreme case in\ndetail, this work highlights the important caveat of inferring $f_{\\rm\nesc}^{\\mathrm{Ly\\alpha}}$ from slit spectroscopy, particularly when using the\nJWST multiplexed NIRSpec microshutter assembly.",
            "author": [
                "Haochen Jiang",
                "Xin Wang",
                "Cheng Cheng",
                "Xu Kong",
                "QianQiao Zhou",
                "Xiao-Lei Meng",
                "Xianlong He",
                "Tucker Jones",
                "Kristan Boyett"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04151v1",
                "http://arxiv.org/pdf/2312.04151v1"
            ],
            "primary_category": "astro-ph.GA",
            "category": [
                "astro-ph.GA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04147v1",
            "title": "An Improved Masking Strategy for Self-supervised Masked Reconstruction\n  in Human Activity Recognition",
            "updated": "2023-12-07T09:02:11Z",
            "published": "2023-12-07T09:02:11Z",
            "summary": "Masked reconstruction serves as a fundamental pretext task for\nself-supervised learning, enabling the model to enhance its feature extraction\ncapabilities by reconstructing the masked segments from extensive unlabeled\ndata. In human activity recognition, this pretext task employed a masking\nstrategy centered on the time dimension. However, this masking strategy fails\nto fully exploit the inherent characteristics of wearable sensor data and\noverlooks the inter-channel information coupling, thereby limiting its\npotential as a powerful pretext task. To address these limitations, we propose\na novel masking strategy called Channel Masking. It involves masking the sensor\ndata along the channel dimension, thereby compelling the encoder to extract\nchannel-related features while performing the masked reconstruction task.\nMoreover, Channel Masking can be seamlessly integrated with masking strategies\nalong the time dimension, thereby motivating the self-supervised model to\nundertake the masked reconstruction task in both the time and channel\ndimensions. Integrated masking strategies are named Time-Channel Masking and\nSpan-Channel Masking. Finally, we optimize the reconstruction loss function to\nincorporate the reconstruction loss in both the time and channel dimensions. We\nevaluate proposed masking strategies on three public datasets, and experimental\nresults show that the proposed strategies outperform prior strategies in both\nself-supervised and semi-supervised scenarios.",
            "author": [
                "Jinqiang Wang",
                "Tao Zhu",
                "Huansheng Ning"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04147v1",
                "http://arxiv.org/pdf/2312.04147v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04145v1",
            "title": "Diffusing Colors: Image Colorization with Text Guided Diffusion",
            "updated": "2023-12-07T08:59:20Z",
            "published": "2023-12-07T08:59:20Z",
            "summary": "The colorization of grayscale images is a complex and subjective task with\nsignificant challenges. Despite recent progress in employing large-scale\ndatasets with deep neural networks, difficulties with controllability and\nvisual quality persist. To tackle these issues, we present a novel image\ncolorization framework that utilizes image diffusion techniques with granular\ntext prompts. This integration not only produces colorization outputs that are\nsemantically appropriate but also greatly improves the level of control users\nhave over the colorization process. Our method provides a balance between\nautomation and control, outperforming existing techniques in terms of visual\nquality and semantic coherence. We leverage a pretrained generative Diffusion\nModel, and show that we can finetune it for the colorization task without\nlosing its generative power or attention to text prompts. Moreover, we present\na novel CLIP-based ranking model that evaluates color vividness, enabling\nautomatic selection of the most suitable level of vividness based on the\nspecific scene semantics. Our approach holds potential particularly for color\nenhancement and historical image colorization.",
            "author": [
                "Nir Zabari",
                "Aharon Azulay",
                "Alexey Gorkor",
                "Tavi Halperin",
                "Ohad Fried"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04145v1",
                "http://arxiv.org/pdf/2312.04145v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04142v1",
            "title": "TimeDRL: Disentangled Representation Learning for Multivariate\n  Time-Series",
            "updated": "2023-12-07T08:56:44Z",
            "published": "2023-12-07T08:56:44Z",
            "summary": "Multivariate time-series data in numerous real-world applications (e.g.,\nhealthcare and industry) are informative but challenging due to the lack of\nlabels and high dimensionality. Recent studies in self-supervised learning have\nshown their potential in learning rich representations without relying on\nlabels, yet they fall short in learning disentangled embeddings and addressing\nissues of inductive bias (e.g., transformation-invariance). To tackle these\nchallenges, we propose TimeDRL, a generic multivariate time-series\nrepresentation learning framework with disentangled dual-level embeddings.\nTimeDRL is characterized by three novel features: (i) disentangled derivation\nof timestamp-level and instance-level embeddings from patched time-series data\nusing a [CLS] token strategy; (ii) utilization of timestamp-predictive and\ninstance-contrastive tasks for disentangled representation learning, with the\nformer optimizing timestamp-level embeddings with predictive loss, and the\nlatter optimizing instance-level embeddings with contrastive loss; and (iii)\navoidance of augmentation methods to eliminate inductive biases, such as\ntransformation-invariance from cropping and masking. Comprehensive experiments\non 6 time-series forecasting datasets and 5 time-series classification datasets\nhave shown that TimeDRL consistently surpasses existing representation learning\napproaches, achieving an average improvement of forecasting by 57.98% in MSE\nand classification by 1.25% in accuracy. Furthermore, extensive ablation\nstudies confirmed the relative contribution of each component in TimeDRL's\narchitecture, and semi-supervised learning evaluations demonstrated its\neffectiveness in real-world scenarios, even with limited labeled data.",
            "author": [
                "Ching Chang",
                "Chiao-Tung Chan",
                "Wei-Yao Wang",
                "Wen-Chih Peng",
                "Tien-Fu Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04142v1",
                "http://arxiv.org/pdf/2312.04142v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04135v1",
            "title": "A Novel Federated Learning-based Intrusion Detection System for Flying\n  Ad Hoc Networks",
            "updated": "2023-12-07T08:50:25Z",
            "published": "2023-12-07T08:50:25Z",
            "summary": "Unmanned aerial vehicles (UAVs) in flying ad-hoc networks (FANETs) face\nsecurity challenges due to the dynamic and distributed nature of these\nnetworks. This paper presents the Federated Learning-based Intrusion Detection\nSystem (FL-IDS), an innovative approach designed to improve FANET security.\nFL-IDS leverages federated learning to address privacy concerns of centralized\nintrusion detection systems. FL-IDS operates in a decentralized manner,\nenabling UAVs to collaboratively train a global intrusion detection model\nwithout sharing raw data. Local models are assigned to each UAV, using\nclient-specific data, and only updated model weights are shared with a central\nserver. This preserves privacy while utilizing collective intelligence for\neffective intrusion detection. Experimental results show FL-IDS's competitive\nperformance with Central IDS (C-IDS) while mitigating privacy concerns. The\nBias Towards Specific Clients (BTSC) method further enhances FL-IDS\nperformance, surpassing C-IDS even at lower attacker ratios. A comparative\nanalysis with traditional intrusion detection methods, including Local IDS\n(L-IDS), provides insights into FL-IDS's strengths. This study significantly\ncontributes to FANET security by introducing a privacy-aware, decentralized\nintrusion detection approach tailored to the unique challenges of UAV networks.",
            "author": [
                "Ozlem Ceviz",
                "Pinar Sadioglu",
                "Sevil Sen",
                "Vassilios G. Vassilakis"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04135v1",
                "http://arxiv.org/pdf/2312.04135v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04132v1",
            "title": "Spin-flip gluon GTMD $F_{1,2}$ at small-$x$",
            "updated": "2023-12-07T08:41:49Z",
            "published": "2023-12-07T08:41:49Z",
            "summary": "Spin-flip processes in the deep inelastic scatterings are thought to be\nsuppressed in the high energy. Recent studies by Hatta and Zhou, however, show\nthat gluon generalized parton distribution (GPD) $E_g$, which is associated\nwith spin-flip processes, exhibits the Regge behavior identical to the BFKL\nPomeron. This was done by deriving the small-$x$ evolution equation for the\nreal part of $F$-type spin-flip gluon GTMDs $F_{1,2}$. In this article, we have\nshown that though the evolution equation for ${\\rm Re}(F_{1,2})$ has IR poles -\nthey all mutually cancel - making the equation IR finite and self-consistent.\nWe also have analytically solved the equations in the dilute regime and find\nsmall-$x$ asymptotics of the GTMDs ${\\rm Re}(F_{1,2})$ as \\begin{eqnarray} {\\rm\nRe}(F_{1,2}) \\sim \\left(\\frac{1}{x}\\right)^{\\alpha_s\\left(4\\ln2-8/3\\right)}\n\\left(\\cos 3\\phi_{k\\Delta} +\\cos \\phi_{k\\Delta}\\right). \\nonumber\n\\end{eqnarray} Interestingly, the surviving solution corresponds to conformal\nspin $n=2$ and carries an explicit $\\cos 3\\phi_{k\\Delta} + \\cos \\phi_{k\\Delta}$\nazimuthal dependence. As the imaginary part of $F_{1,2}$, is related to the\nspin-dependent odderon or Gluon Siver function and scales as ${\\rm Im}(F_{1,2})\n\\sim x^{0}$, the positive intercept for ${\\rm Re}(F_{1,2})$, implies that it is\nexpected to dominate over the gluon Siver function in the small-$x$ limit - and\nmay directly impact the modeling of unpolarised GTMDs and associated spin-flip\nprocesses.",
            "author": [
                "Sanskriti Agrawal",
                "Nahid Vasim",
                "Raktim Abir"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04132v1",
                "http://arxiv.org/pdf/2312.04132v1"
            ],
            "primary_category": "hep-ph",
            "category": [
                "hep-ph",
                "hep-lat",
                "nucl-th"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04118v1",
            "title": "Caregiver Talk Shapes Toddler Vision: A Computational Study of Dyadic\n  Play",
            "updated": "2023-12-07T08:18:40Z",
            "published": "2023-12-07T08:18:40Z",
            "summary": "Infants' ability to recognize and categorize objects develops gradually. The\nsecond year of life is marked by both the emergence of more semantic visual\nrepresentations and a better understanding of word meaning. This suggests that\nlanguage input may play an important role in shaping visual representations.\nHowever, even in suitable contexts for word learning like dyadic play sessions,\ncaregivers utterances are sparse and ambiguous, often referring to objects that\nare different from the one to which the child attends. Here, we systematically\ninvestigate to what extent caregivers' utterances can nevertheless enhance\nvisual representations. For this we propose a computational model of visual\nrepresentation learning during dyadic play. We introduce a synthetic dataset of\nego-centric images perceived by a toddler-agent that moves and rotates toy\nobjects in different parts of its home environment while hearing caregivers'\nutterances, modeled as captions. We propose to model toddlers' learning as\nsimultaneously aligning representations for 1) close-in-time images and 2)\nco-occurring images and utterances. We show that utterances with statistics\nmatching those of real caregivers give rise to representations supporting\nimproved category recognition. Our analysis reveals that a small\ndecrease/increase in object-relevant naming frequencies can drastically impact\nthe learned representations. This affects the attention on object names within\nan utterance, which is required for efficient visuo-linguistic alignment.\nOverall, our results support the hypothesis that caregivers' naming utterances\ncan improve toddlers' visual representations.",
            "author": [
                "Timothy Schauml\u00f6ffel",
                "Arthur Aubret",
                "Gemma Roig",
                "Jochen Triesch"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04118v1",
                "http://arxiv.org/pdf/2312.04118v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04116v1",
            "title": "Unconventional mechanical and thermal behaviors of MOF CALF-20",
            "updated": "2023-12-07T08:06:34Z",
            "published": "2023-12-07T08:06:34Z",
            "summary": "CALF-20 was recently identified as a novel benchmark sorbent for CO$_2$\ncapture at the industrial scale, however comprehensive atomistic insight into\nits mechanical/thermal properties under working conditions is still lacking. In\nthis study, we developed a general-purpose machine-learned potential (MLP) for\nthe CALF-20 MOF framework that predicts the thermodynamic and mechanical\nproperties of the structure at finite temperatures within first-principles\naccuracy. Interestingly, CALF-20 was demonstrated to exhibit both negative area\ncompression and negative thermal expansion. Most strikingly, upon application\nof the tensile strain along the [001] direction, CALF-20 was shown to display a\ndistinct two-step elastic deformation behavior, unlike typical MOFs that\nundergo plastic deformation after elasticity. Furthermore, this MOF was shown\nto exhibit a spectacular fracture strain of up to 27% along the [001] direction\nat room temperature comparable to that of MOF glasses. These abnormal thermal\nand mechanical properties make CALF-20 as attractive material for flexible and\nstretchable electronics and sensors.",
            "author": [
                "Dong Fan",
                "Supriyo Naskar",
                "Guillaume Maurin"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04116v1",
                "http://arxiv.org/pdf/2312.04116v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci",
                "physics.comp-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04111v1",
            "title": "Breaking the Entanglement of Homophily and Heterophily in\n  Semi-supervised Node Classification",
            "updated": "2023-12-07T07:54:11Z",
            "published": "2023-12-07T07:54:11Z",
            "summary": "Recently, graph neural networks (GNNs) have shown prominent performance in\nsemi-supervised node classification by leveraging knowledge from the graph\ndatabase. However, most existing GNNs follow the homophily assumption, where\nconnected nodes are more likely to exhibit similar feature distributions and\nthe same labels, and such an assumption has proven to be vulnerable in a\ngrowing number of practical applications. As a supplement, heterophily reflects\ndissimilarity in connected nodes, which has gained significant attention in\ngraph learning. To this end, data engineers aim to develop a powerful GNN model\nthat can ensure performance under both homophily and heterophily. Despite\nnumerous attempts, most existing GNNs struggle to achieve optimal node\nrepresentations due to the constraints of undirected graphs. The neglect of\ndirected edges results in sub-optimal graph representations, thereby hindering\nthe capacity of GNNs. To address this issue, we introduce AMUD, which\nquantifies the relationship between node profiles and topology from a\nstatistical perspective, offering valuable insights for \\underline{A}daptively\n\\underline{M}odeling the natural directed graphs as the \\underline{U}ndirected\nor \\underline{D}irected graph to maximize the benefits from subsequent graph\nlearning. Furthermore, we propose \\underline{A}daptive \\underline{D}irected\n\\underline{P}attern \\underline{A}ggregation (ADPA) as a new directed graph\nlearning paradigm for AMUD. Empirical studies have demonstrated that AMUD\nguides efficient graph learning. Meanwhile, extensive experiments on 14\nbenchmark datasets substantiate the impressive performance of ADPA,\noutperforming baselines by significant margins of 3.96\\%.",
            "author": [
                "Henan Sun",
                "Xunkai Li",
                "Zhengyu Wu",
                "Daohan Su",
                "Rong-Hua Li",
                "Guoren Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04111v1",
                "http://arxiv.org/pdf/2312.04111v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04110v1",
            "title": "Small Area Estimation of Case Growths for Timely COVID-19 Outbreak\n  Detection",
            "updated": "2023-12-07T07:53:00Z",
            "published": "2023-12-07T07:53:00Z",
            "summary": "The COVID-19 pandemic has exerted a profound impact on the global economy and\ncontinues to exact a significant toll on human lives. The COVID-19 case growth\nrate stands as a key epidemiological parameter to estimate and monitor for\neffective detection and containment of the resurgence of outbreaks. A\nfundamental challenge in growth rate estimation and hence outbreak detection is\nbalancing the accuracy-speed tradeoff, where accuracy typically degrades with\nshorter fitting windows. In this paper, we develop a machine learning (ML)\nalgorithm, which we call Transfer Learning Generalized Random Forest (TLGRF),\nthat balances this accuracy-speed tradeoff. Specifically, we estimate the\ninstantaneous COVID-19 exponential growth rate for each U.S. county by using\nTLGRF that chooses an adaptive fitting window size based on relevant day-level\nand county-level features affecting the disease spread. Through transfer\nlearning, TLGRF can accurately estimate case growth rates for counties with\nsmall sample sizes. Out-of-sample prediction analysis shows that TLGRF\noutperforms established growth rate estimation methods. Furthermore, we\nconducted a case study based on outbreak case data from the state of Colorado\nand showed that the timely detection of outbreaks could have been improved by\nup to 224% using TLGRF when compared to the decisions made by Colorado's\nDepartment of Health and Environment (CDPHE). To facilitate implementation, we\nhave developed a publicly available outbreak detection tool for timely\ndetection of COVID-19 outbreaks in each U.S. county, which received substantial\nattention from policymakers.",
            "author": [
                "Zhaowei She",
                "Zilong Wang",
                "Jagpreet Chhatwal",
                "Turgay Ayer"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04110v1",
                "http://arxiv.org/pdf/2312.04110v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG",
                "physics.soc-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04103v1",
            "title": "Enhancing the Rationale-Input Alignment for Self-explaining\n  Rationalization",
            "updated": "2023-12-07T07:37:15Z",
            "published": "2023-12-07T07:37:15Z",
            "summary": "Rationalization empowers deep learning models with self-explaining\ncapabilities through a cooperative game, where a generator selects a\nsemantically consistent subset of the input as a rationale, and a subsequent\npredictor makes predictions based on the selected rationale. In this paper, we\ndiscover that rationalization is prone to a problem named \\emph{rationale\nshift}, which arises from the algorithmic bias of the cooperative game.\nRationale shift refers to a situation where the semantics of the selected\nrationale may deviate from the original input, but the predictor still produces\naccurate predictions based on the deviation, resulting in a compromised\ngenerator with misleading feedback.\n  To address this issue, we first demonstrate the importance of the alignment\nbetween the rationale and the full input through both empirical observations\nand theoretical analysis. Subsequently, we introduce a novel approach called\nDAR (\\textbf{D}iscriminatively \\textbf{A}ligned \\textbf{R}ationalization),\nwhich utilizes an auxiliary module pretrained on the full input to\ndiscriminatively align the selected rationale and the original input. We\ntheoretically illustrate how DAR accomplishes the desired alignment, thereby\novercoming the rationale shift problem. The experiments on two widely used\nreal-world benchmarks show that the proposed method significantly improves the\nexplanation quality (measured by the overlap between the model-selected\nexplanation and the human-annotated rationale) as compared to state-of-the-art\ntechniques. Additionally, results on two synthetic settings further validate\nthe effectiveness of DAR in addressing the rationale shift problem.",
            "author": [
                "Wei Liu",
                "Haozhao Wang",
                "Jun Wang",
                "Zhiying Deng",
                "YuanKai Zhang",
                "Cheng Wang",
                "Ruixuan Li"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04103v1",
                "http://arxiv.org/pdf/2312.04103v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04096v1",
            "title": "MediHunt: A Network Forensics Framework for Medical IoT Devices",
            "updated": "2023-12-07T07:19:56Z",
            "published": "2023-12-07T07:19:56Z",
            "summary": "The Medical Internet of Things (MIoT) has enabled small, ubiquitous medical\ndevices to communicate with each other to facilitate interconnected healthcare\ndelivery. These devices interact using communication protocols like MQTT,\nBluetooth, and Wi-Fi. However, as MIoT devices proliferate, these networked\ndevices are vulnerable to cyber-attacks. This paper focuses on the\nvulnerabilities present in the Message Queuing Telemetry and Transport (MQTT)\nprotocol. The MQTT protocol is prone to cyber-attacks that can harm the\nsystem's functionality. The memory-constrained MIoT devices enforce a\nlimitation on storing all data logs that are required for comprehensive network\nforensics. This paper solves the data log availability challenge by detecting\nthe attack in real-time and storing the corresponding logs for further analysis\nwith the proposed network forensics framework: MediHunt. Machine learning (ML)\ntechniques are the most real safeguard against cyber-attacks. However, these\nmodels require a specific dataset that covers diverse attacks on the MQTT-based\nIoT system for training. The currently available datasets do not encompass a\nvariety of applications and TCP layer attacks. To address this issue, we\nleveraged the usage of a flow-based dataset containing flow data for TCP/IP\nlayer and application layer attacks. Six different ML models are trained with\nthe generated dataset to evaluate the effectiveness of the MediHunt framework\nin detecting real-time attacks. F1 scores and detection accuracy exceeded 0.99\nfor the proposed MediHunt framework with our custom dataset.",
            "author": [
                "Ayushi Mishra",
                "Tej Kiran Boppana",
                "Priyanka Bagade"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04096v1",
                "http://arxiv.org/pdf/2312.04096v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04095v1",
            "title": "Learn to Unlearn for Deep Neural Networks: Minimizing Unlearning\n  Interference with Gradient Projection",
            "updated": "2023-12-07T07:17:24Z",
            "published": "2023-12-07T07:17:24Z",
            "summary": "Recent data-privacy laws have sparked interest in machine unlearning, which\ninvolves removing the effect of specific training samples from a learnt model\nas if they were never present in the original training dataset. The challenge\nof machine unlearning is to discard information about the ``forget'' data in\nthe learnt model without altering the knowledge about the remaining dataset and\nto do so more efficiently than the naive retraining approach. To achieve this,\nwe adopt a projected-gradient based learning method, named as\nProjected-Gradient Unlearning (PGU), in which the model takes steps in the\northogonal direction to the gradient subspaces deemed unimportant for the\nretaining dataset, so as to its knowledge is preserved. By utilizing Stochastic\nGradient Descent (SGD) to update the model weights, our method can efficiently\nscale to any model and dataset size. We provide empirically evidence to\ndemonstrate that our unlearning method can produce models that behave similar\nto models retrained from scratch across various metrics even when the training\ndataset is no longer accessible. Our code is available at\nhttps://github.com/hnanhtuan/projected_gradient_unlearning.",
            "author": [
                "Tuan Hoang",
                "Santu Rana",
                "Sunil Gupta",
                "Svetha Venkatesh"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04095v1",
                "http://arxiv.org/pdf/2312.04095v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04083v1",
            "title": "On the adaptation of in-context learners for system identification",
            "updated": "2023-12-07T06:51:55Z",
            "published": "2023-12-07T06:51:55Z",
            "summary": "In-context system identification aims at constructing meta-models to describe\nclasses of systems, differently from traditional approaches that model single\nsystems. This paradigm facilitates the leveraging of knowledge acquired from\nobserving the behaviour of different, yet related dynamics. This paper\ndiscusses the role of meta-model adaptation. Through numerical examples, we\ndemonstrate how meta-model adaptation can enhance predictive performance in\nthree realistic scenarios: tailoring the meta-model to describe a specific\nsystem rather than a class; extending the meta-model to capture the behaviour\nof systems beyond the initial training class; and recalibrating the model for\nnew prediction tasks. Results highlight the effectiveness of meta-model\nadaptation to achieve a more robust and versatile meta-learning framework for\nsystem identification.",
            "author": [
                "Dario Piga",
                "Filippo Pura",
                "Marco Forgione"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04083v1",
                "http://arxiv.org/pdf/2312.04083v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04080v1",
            "title": "Mass Ratio Dependence of Three-Body Resonance Lifetimes in 1D and 3D",
            "updated": "2023-12-07T06:48:42Z",
            "published": "2023-12-07T06:48:42Z",
            "summary": "We present a theoretical study of resonance lifetimes in a two-component\nthree-body system, specifically examining the decay of three-body resonances\ninto a deep dimer and an unbound particle. Utilising the Gaussian expansion\nmethod together with the complex scaling method, we obtain the widths of these\nresonances from first principles. We focus on mass ratios in the typical range\nfor mixtures of ultracold atoms and reveal a pronounced dependence of the\nresonance widths on the mass ratio: a distinct maximum near the equal-mass\nscenario and a rapid decrease away from it. Moreover, we show that this\nbehaviour is not covered by the analytical formula of Pen'kov~[Phys. Rev. A 60,\n3756 (1999)]. Notably, near the mass ratio for Caesium-Lithium mixtures, we\nobtain nearly vanishing widths of the resonances which validates to treat them\nin the bound state approximation. In addition, we perform our analysis on the\nresonance widths in both one and three dimensions and find that their\nqualitative dependence on the mass ratio agrees.",
            "author": [
                "Lucas Happ",
                "Pascal Naidon",
                "Emiko Hiyama"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04080v1",
                "http://arxiv.org/pdf/2312.04080v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cond-mat.quant-gas"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04078v1",
            "title": "A Review and Taxonomy of Methods for Quantifying Dataset Similarity",
            "updated": "2023-12-07T06:44:14Z",
            "published": "2023-12-07T06:44:14Z",
            "summary": "In statistics and machine learning, measuring the similarity between two or\nmore datasets is important for several purposes. The performance of a\npredictive model on novel datasets, referred to as generalizability, critically\ndepends on how similar the dataset used for fitting the model is to the novel\ndatasets. Exploiting or transferring insights between similar datasets is a key\naspect of meta-learning and transfer-learning. In two-sample testing, it is\nchecked, whether the underlying (multivariate) distributions of two datasets\ncoincide or not.\n  Extremely many approaches for quantifying dataset similarity have been\nproposed in the literature. A structured overview is a crucial first step for\ncomparisons of approaches. We examine more than 100 methods and provide a\ntaxonomy, classifying them into ten classes, including (i) comparisons of\ncumulative distribution functions, density functions, or characteristic\nfunctions, (ii) methods based on multivariate ranks, (iii) discrepancy measures\nfor distributions, (iv) graph-based methods, (v) methods based on inter-point\ndistances, (vi) kernel-based methods, (vii) methods based on binary\nclassification, (viii) distance and similarity measures for datasets, (ix)\ncomparisons based on summary statistics, and (x) different testing approaches.\nHere, we present an extensive review of these methods. We introduce the main\nunderlying ideas, formal definitions, and important properties.",
            "author": [
                "Marieke Stolte",
                "Andrea Bommert",
                "J\u00f6rg Rahnenf\u00fchrer"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04078v1",
                "http://arxiv.org/pdf/2312.04078v1"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME",
                "62E99, 62G10, 62H15, 62H30, 05C90"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04076v1",
            "title": "Large Language Models are Good Prompt Learners for Low-Shot Image\n  Classification",
            "updated": "2023-12-07T06:43:34Z",
            "published": "2023-12-07T06:43:34Z",
            "summary": "Low-shot image classification, where training images are limited or\ninaccessible, has benefited from recent progress on pre-trained vision-language\n(VL) models with strong generalizability, e.g. CLIP. Prompt learning methods\nbuilt with VL models generate text features from the class names that only have\nconfined class-specific information. Large Language Models (LLMs), with their\nvast encyclopedic knowledge, emerge as the complement. Thus, in this paper, we\ndiscuss the integration of LLMs to enhance pre-trained VL models, specifically\non low-shot classification. However, the domain gap between language and vision\nblocks the direct application of LLMs. Thus, we propose LLaMP, Large Language\nModels as Prompt learners, that produces adaptive prompts for the CLIP text\nencoder, establishing it as the connecting bridge. Experiments show that,\ncompared with other state-of-the-art prompt learning methods, LLaMP yields\nbetter performance on both zero-shot generalization and few-shot image\nclassification, over a spectrum of 11 datasets.",
            "author": [
                "Zhaoheng Zheng",
                "Jingmin Wei",
                "Xuefeng Hu",
                "Haidong Zhu",
                "Ram Nevatia"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04076v1",
                "http://arxiv.org/pdf/2312.04076v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04071v1",
            "title": "Synergistic Signals: Exploiting Co-Engagement and Semantic Links via\n  Graph Neural Networks",
            "updated": "2023-12-07T06:29:26Z",
            "published": "2023-12-07T06:29:26Z",
            "summary": "Given a set of candidate entities (e.g. movie titles), the ability to\nidentify similar entities is a core capability of many recommender systems.\nMost often this is achieved by collaborative filtering approaches, i.e. if\nusers co-engage with a pair of entities frequently enough, the embeddings\nshould be similar. However, relying on co-engagement data alone can result in\nlower-quality embeddings for new and unpopular entities. We study this problem\nin the context recommender systems at Netflix. We observe that there is\nabundant semantic information such as genre, content maturity level, themes,\netc. that complements co-engagement signals and provides interpretability in\nsimilarity models. To learn entity similarities from both data sources\nholistically, we propose a novel graph-based approach called SemanticGNN.\nSemanticGNN models entities, semantic concepts, collaborative edges, and\nsemantic edges within a large-scale knowledge graph and conducts representation\nlearning over it. Our key technical contributions are twofold: (1) we develop a\nnovel relation-aware attention graph neural network (GNN) to handle the\nimbalanced distribution of relation types in our graph; (2) to handle web-scale\ngraph data that has millions of nodes and billions of edges, we develop a novel\ndistributed graph training paradigm. The proposed model is successfully\ndeployed within Netflix and empirical experiments indicate it yields up to 35%\nimprovement in performance on similarity judgment tasks.",
            "author": [
                "Zijie Huang",
                "Baolin Li",
                "Hafez Asgharzadeh",
                "Anne Cocos",
                "Lingyi Liu",
                "Evan Cox",
                "Colby Wise",
                "Sudarshan Lamkhede"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04071v1",
                "http://arxiv.org/pdf/2312.04071v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04070v1",
            "title": "A Transformer Model for Symbolic Regression towards Scientific Discovery",
            "updated": "2023-12-07T06:27:48Z",
            "published": "2023-12-07T06:27:48Z",
            "summary": "Symbolic Regression (SR) searches for mathematical expressions which best\ndescribe numerical datasets. This allows to circumvent interpretation issues\ninherent to artificial neural networks, but SR algorithms are often\ncomputationally expensive. This work proposes a new Transformer model aiming at\nSymbolic Regression particularly focused on its application for Scientific\nDiscovery. We propose three encoder architectures with increasing flexibility\nbut at the cost of column-permutation equivariance violation. Training results\nindicate that the most flexible architecture is required to prevent from\noverfitting. Once trained, we apply our best model to the SRSD datasets\n(Symbolic Regression for Scientific Discovery datasets) which yields\nstate-of-the-art results using the normalized tree-based edit distance, at no\nextra computational cost.",
            "author": [
                "Florian Lalande",
                "Yoshitomo Matsubara",
                "Naoya Chiba",
                "Tatsunori Taniai",
                "Ryo Igarashi",
                "Yoshitala Ushiku"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04070v1",
                "http://arxiv.org/pdf/2312.04070v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04068v1",
            "title": "Making Translators Privacy-aware on the User's Side",
            "updated": "2023-12-07T06:23:17Z",
            "published": "2023-12-07T06:23:17Z",
            "summary": "We propose PRISM to enable users of machine translation systems to preserve\nthe privacy of data on their own initiative. There is a growing demand to apply\nmachine translation systems to data that require privacy protection. While\nseveral machine translation engines claim to prioritize privacy, the extent and\nspecifics of such protection are largely ambiguous. First, there is often a\nlack of clarity on how and to what degree the data is protected. Even if\nservice providers believe they have sufficient safeguards in place,\nsophisticated adversaries might still extract sensitive information. Second,\nvulnerabilities may exist outside of these protective measures, such as within\ncommunication channels, potentially leading to data leakage. As a result, users\nare hesitant to utilize machine translation engines for data demanding high\nlevels of privacy protection, thereby missing out on their benefits. PRISM\nresolves this problem. Instead of relying on the translation service to keep\ndata safe, PRISM provides the means to protect data on the user's side. This\napproach ensures that even machine translation engines with inadequate privacy\nmeasures can be used securely. For platforms already equipped with privacy\nsafeguards, PRISM acts as an additional protection layer, reinforcing their\nsecurity furthermore. PRISM adds these privacy features without significantly\ncompromising translation accuracy. Our experiments demonstrate the\neffectiveness of PRISM using real-world translators, T5 and ChatGPT\n(GPT-3.5-turbo), and the datasets with two languages. PRISM effectively\nbalances privacy protection with translation accuracy.",
            "author": [
                "Ryoma Sato"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04068v1",
                "http://arxiv.org/pdf/2312.04068v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04067v1",
            "title": "MeanCut: A Greedy-Optimized Graph Clustering via Path-based Similarity\n  and Degree Descent Criterion",
            "updated": "2023-12-07T06:19:39Z",
            "published": "2023-12-07T06:19:39Z",
            "summary": "As the most typical graph clustering method, spectral clustering is popular\nand attractive due to the remarkable performance, easy implementation, and\nstrong adaptability. Classical spectral clustering measures the edge weights of\ngraph using pairwise Euclidean-based metric, and solves the optimal graph\npartition by relaxing the constraints of indicator matrix and performing\nLaplacian decomposition. However, Euclidean-based similarity might cause skew\ngraph cuts when handling non-spherical data distributions, and the relaxation\nstrategy introduces information loss. Meanwhile, spectral clustering requires\nspecifying the number of clusters, which is hard to determine without enough\nprior knowledge. In this work, we leverage the path-based similarity to enhance\nintra-cluster associations, and propose MeanCut as the objective function and\ngreedily optimize it in degree descending order for a nondestructive graph\npartition. This algorithm enables the identification of arbitrary shaped\nclusters and is robust to noise. To reduce the computational complexity of\nsimilarity calculation, we transform optimal path search into generating the\nmaximum spanning tree (MST), and develop a fast MST (FastMST) algorithm to\nfurther improve its time-efficiency. Moreover, we define a density gradient\nfactor (DGF) for separating the weakly connected clusters. The validity of our\nalgorithm is demonstrated by testifying on real-world benchmarks and\napplication of face recognition. The source code of MeanCut is available at\nhttps://github.com/ZPGuiGroupWhu/MeanCut-Clustering.",
            "author": [
                "Dehua Peng",
                "Zhipeng Gui",
                "Huayi Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04067v1",
                "http://arxiv.org/pdf/2312.04067v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "I.5.3"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04065v1",
            "title": "A Robust and Efficient Boundary Point Detection Method by Measuring\n  Local Direction Dispersion",
            "updated": "2023-12-07T06:09:21Z",
            "published": "2023-12-07T06:09:21Z",
            "summary": "Boundary points pose a significant challenge for machine learning tasks,\nincluding classification, clustering, and dimensionality reduction. Due to the\nsimilarity of features, boundary areas can result in mixed-up classes or\nclusters, leading to a crowding problem in dimensionality reduction. To address\nthis challenge, numerous boundary point detection methods have been developed,\nbut they are insufficiently to accurately and efficiently identify the boundary\npoints in non-convex structures and high-dimensional manifolds. In this work,\nwe propose a robust and efficient method for detecting boundary points using\nLocal Direction Dispersion (LoDD). LoDD considers that internal points are\nsurrounded by neighboring points in all directions, while neighboring points of\na boundary point tend to be distributed only in a certain directional range.\nLoDD adopts a density-independent K-Nearest Neighbors (KNN) method to determine\nneighboring points, and defines a statistic-based metric using the eigenvalues\nof the covariance matrix of KNN coordinates to measure the centrality of a\nquery point. We demonstrated the validity of LoDD on five synthetic datasets\n(2-D and 3-D) and ten real-world benchmarks, and tested its clustering\nperformance by equipping with two typical clustering methods, K-means and Ncut.\nOur results show that LoDD achieves promising and robust detection accuracy in\na time-efficient manner.",
            "author": [
                "Dehua Peng",
                "Zhipeng Gui",
                "Huayi Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04065v1",
                "http://arxiv.org/pdf/2312.04065v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "I.5.2"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04064v1",
            "title": "DiscoBAX: Discovery of Optimal Intervention Sets in Genomic Experiment\n  Design",
            "updated": "2023-12-07T06:05:39Z",
            "published": "2023-12-07T06:05:39Z",
            "summary": "The discovery of therapeutics to treat genetically-driven pathologies relies\non identifying genes involved in the underlying disease mechanisms. Existing\napproaches search over the billions of potential interventions to maximize the\nexpected influence on the target phenotype. However, to reduce the risk of\nfailure in future stages of trials, practical experiment design aims to find a\nset of interventions that maximally change a target phenotype via diverse\nmechanisms. We propose DiscoBAX, a sample-efficient method for maximizing the\nrate of significant discoveries per experiment while simultaneously probing for\na wide range of diverse mechanisms during a genomic experiment campaign. We\nprovide theoretical guarantees of approximate optimality under standard\nassumptions, and conduct a comprehensive experimental evaluation covering both\nsynthetic as well as real-world experimental design tasks. DiscoBAX outperforms\nexisting state-of-the-art methods for experimental design, selecting effective\nand diverse perturbations in biological systems.",
            "author": [
                "Clare Lyle",
                "Arash Mehrjou",
                "Pascal Notin",
                "Andrew Jesson",
                "Stefan Bauer",
                "Yarin Gal",
                "Patrick Schwab"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04064v1",
                "http://arxiv.org/pdf/2312.04064v1"
            ],
            "primary_category": "q-bio.QM",
            "category": [
                "q-bio.QM",
                "cs.LG",
                "stat.ME"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04062v1",
            "title": "A Low-Overhead Incorporation-Extrapolation based Few-Shot CSI Feedback\n  Framework for Massive MIMO Systems",
            "updated": "2023-12-07T06:01:47Z",
            "published": "2023-12-07T06:01:47Z",
            "summary": "Accurate channel state information (CSI) is essential for downlink precoding\nat the base station (BS), especially for frequency FDD wideband massive MIMO\nsystems with OFDM. In FDD systems, CSI is attained through CSI feedback from\nthe user equipment (UE). However, large-scale antennas and large number of\nsubcarriers significantly increase CSI feedback overhead. Deep learning-based\nCSI feedback methods have received tremendous attention in recent years due to\ntheir great capability of compressing CSI. Nonetheless, large amounts of\ncollected samples are required to train deep learning models, which is severely\nchallenging in practice. Besides, with the rapidly increasing number of\nantennas and subcarriers, most of these deep learning methods' CSI feedback\noverhead also grow dramatically, owing to their focus on full-dimensional CSI\nfeedback. To address this issue, in this paper, we propose a low-overhead\nIncorporation-Extrapolation based Few-Shot CSI feedback Framework (IEFSF) for\nmassive MIMO systems. To further reduce the feedback overhead, a\nlow-dimensional eigenvector-based CSI matrix is first formed with the\nincorporation process at the UE, and then recovered to the full-dimensional\neigenvector-based CSI matrix at the BS via the extrapolation process. After\nthat, to alleviate the necessity of the extensive collected samples and enable\nfew-shot CSI feedback, we further propose a knowledge-driven data augmentation\nmethod and an artificial intelligence-generated content (AIGC) -based data\naugmentation method by exploiting the domain knowledge of wireless channels and\nby exploiting a novel generative model, respectively. Numerical results\ndemonstrate that the proposed IEFSF can significantly reduce CSI feedback\noverhead by 16 times compared with existing CSI feedback methods while\nmaintaining higher feedback accuracy using only several hundreds of collected\nsamples.",
            "author": [
                "Binggui Zhou",
                "Xi Yang",
                "Jintao Wang",
                "Shaodan Ma",
                "Feifei Gao",
                "Guanghua Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04062v1",
                "http://arxiv.org/pdf/2312.04062v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "cs.AI",
                "eess.SP",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04061v1",
            "title": "Depth Zero Supercuspidal Representations of Classical Groups into\n  L-packets: the Typically Almost Symmetric Case",
            "updated": "2023-12-07T06:00:42Z",
            "published": "2023-12-07T06:00:42Z",
            "summary": "We classify what we call ``typically almost symmetric'' depth zero\nsupercuspidal representations of classical groups into L-packets. Our main\nresults resolve an ambiguity in the paper of Lust-Stevens \\cite{Lust-Stevens}\nin this case, where they could only classify these representations in two or\nfour, if not one, L-packets. By assuming the expected numbers of supercuspidal\nrepresentations in the L-packets, we employ only simple properties of the\nrepresentations to prove the main results. In particular, we do not require any\ndeep calculations of character values.",
            "author": [
                "Geo Kam-Fai Tam"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04061v1",
                "http://arxiv.org/pdf/2312.04061v1"
            ],
            "primary_category": "math.RT",
            "category": [
                "math.RT",
                "math.NT",
                "22E50, 11F70, 20G05"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04060v1",
            "title": "Differentiable Registration of Images and LiDAR Point Clouds with\n  VoxelPoint-to-Pixel Matching",
            "updated": "2023-12-07T05:46:10Z",
            "published": "2023-12-07T05:46:10Z",
            "summary": "Cross-modality registration between 2D images from cameras and 3D point\nclouds from LiDARs is a crucial task in computer vision and robotic. Previous\nmethods estimate 2D-3D correspondences by matching point and pixel patterns\nlearned by neural networks, and use Perspective-n-Points (PnP) to estimate\nrigid transformation during post-processing. However, these methods struggle to\nmap points and pixels to a shared latent space robustly since points and pixels\nhave very different characteristics with patterns learned in different manners\n(MLP and CNN), and they also fail to construct supervision directly on the\ntransformation since the PnP is non-differentiable, which leads to unstable\nregistration results. To address these problems, we propose to learn a\nstructured cross-modality latent space to represent pixel features and 3D\nfeatures via a differentiable probabilistic PnP solver. Specifically, we design\na triplet network to learn VoxelPoint-to-Pixel matching, where we represent 3D\nelements using both voxels and points to learn the cross-modality latent space\nwith pixels. We design both the voxel and pixel branch based on CNNs to operate\nconvolutions on voxels/pixels represented in grids, and integrate an additional\npoint branch to regain the information lost during voxelization. We train our\nframework end-to-end by imposing supervisions directly on the predicted pose\ndistribution with a probabilistic PnP solver. To explore distinctive patterns\nof cross-modality features, we design a novel loss with adaptive-weighted\noptimization for cross-modality feature description. The experimental results\non KITTI and nuScenes datasets show significant improvements over the\nstate-of-the-art methods. The code and models are available at\nhttps://github.com/junshengzhou/VP2P-Match.",
            "author": [
                "Junsheng Zhou",
                "Baorui Ma",
                "Wenyuan Zhang",
                "Yi Fang",
                "Yu-Shen Liu",
                "Zhizhong Han"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04060v1",
                "http://arxiv.org/pdf/2312.04060v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04057v1",
            "title": "UOCS-XI. Study of blue straggler stars in open cluster NGC 7142 using\n  UVIT/AstroSat",
            "updated": "2023-12-07T05:38:33Z",
            "published": "2023-12-07T05:38:33Z",
            "summary": "We present a study of blue straggler stars (BSSs) of open cluster NGC 7142\nusing AstroSat/UVIT data and other archival data. Using a machine\nlearning-based algorithm, ML-MOC, on Gaia DR3 data, we find 546 sources as\ncluster members. Based on the location on the Gaia color-magnitude diagram, we\nidentify ten BSS candidates, also detected in UVIT/F148W filter. We study the\nvariable nature of BSSs by constructing their light curves using the TESS data.\nTwo BSSs reported as eclipsing binaries in Gaia DR3 are confirmed to be\neclipsing binaries based on our analysis and also show the presence of hot\ncompanions as per the multi-wavelength spectral energy distributions (SEDs).\nThe physical parameters of the hot companions of these two BSSs derived by\nfitting binary models to their light curves and those derived from the SEDs are\nfound to be in good agreement. Additionally, five more BSSs in the cluster\nshows UV excess, four of which are likely to have a hot companion based on\nSEDs. The hot companions with the estimated temperatures $\\sim$14000 $-$ 28000\nK, radii $\\sim$0.01 $-$ 0.05 R$_{\\odot}$, and luminosities $\\sim$0.03 $-$ 0.1\nL$_{\\odot}$, are inferred to be extremely low mass ($<$ 0.2 M$_{\\odot}$),\nlow-mass (0.2 $-$ 0.4 M$_{\\odot}$), normal-mass (0.4 $-$ 0.6 M$_{\\odot}$), and\nhigh-mass ($>$ 0.6 M$_{\\odot}$) white dwarfs (WD). For the first time in an\nopen cluster, we find the entire range of masses in WDs found as hot companions\nof BSSs. These masses imply that the Case-A/Case-B mass transfer as well as\nmerger are responsible for the formation of at least 60$\\%$ of the BSSs of this\ncluster.",
            "author": [
                "Anju Panthi",
                "Kaushar Vaidya",
                "Nagaraj Vernekar",
                "Annapurni Subramaniam",
                "Vikrant Jadhav",
                "Manan Agarwal"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04057v1",
                "http://arxiv.org/pdf/2312.04057v1"
            ],
            "primary_category": "astro-ph.SR",
            "category": [
                "astro-ph.SR",
                "astro-ph.GA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04055v1",
            "title": "Jointly spatial-temporal representation learning for individual\n  trajectories",
            "updated": "2023-12-07T05:27:24Z",
            "published": "2023-12-07T05:27:24Z",
            "summary": "Individual trajectories, containing substantial information on\nhuman-environment interactions across space and time, is a crucial input for\ngeospatial foundation models (GeoFMs). However, existing attempts, leveraging\ntrajectory data for various applications have overlooked the implicit\nspatial-temporal dependency within trajectories and failed to encode and\nrepresent it in a format friendly to deep learning, posing a challenge in\nobtaining general-purpose trajectory representations. Therefore, this paper\nproposes a spatial-temporal joint representation learning method (ST-GraphRL)\nto formalize learnable spatial-temporal dependencies into trajectory\nrepresentations. The proposed ST-GraphRL consists of three compositions: (i) a\nweighted directed spatial-temporal graph to explicitly construct mobility\ninteractions over both space and time dimensions; (ii) a two-stage jointly\nencoder (i.e., decoupling and fusion) to learn entangled spatial-temporal\ndependencies by independently decomposing and jointly aggregating space and\ntime information; (iii) a decoder guides ST-GraphRL to learn explicit mobility\nregularities by simulating the spatial-temporal distributions of trajectories.\nTested on three real-world human mobility datasets, the proposed ST-GraphRL\noutperformed all the baseline models in predicting movement spatial-temporal\ndistributions and preserving trajectory similarity with high spatial-temporal\ncorrelations. We also explore how spatial-temporal features presented in latent\nspace, validating that ST-GraphRL understands spatial-temporal patterns. This\nmethod is also transferable for general-purpose geospatial data representations\nfor broad downstream tasks, as well advancing GeoFMs developing.",
            "author": [
                "Fei Huang",
                "Jianrong Lv",
                "Yang Yue"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04055v1",
                "http://arxiv.org/pdf/2312.04055v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04044v1",
            "title": "Residual Graph Convolutional Network for Bird's-Eye-View Semantic\n  Segmentation",
            "updated": "2023-12-07T05:04:41Z",
            "published": "2023-12-07T05:04:41Z",
            "summary": "Retrieving spatial information and understanding the semantic information of\nthe surroundings are important for Bird's-Eye-View (BEV) semantic segmentation.\nIn the application of autonomous driving, autonomous vehicles need to be aware\nof their surroundings to drive safely. However, current BEV semantic\nsegmentation techniques, deep Convolutional Neural Networks (CNNs) and\ntransformers, have difficulties in obtaining the global semantic relationships\nof the surroundings at the early layers of the network. In this paper, we\npropose to incorporate a novel Residual Graph Convolutional (RGC) module in\ndeep CNNs to acquire both the global information and the region-level semantic\nrelationship in the multi-view image domain. Specifically, the RGC module\nemploys a non-overlapping graph space projection to efficiently project the\ncomplete BEV information into graph space. It then builds interconnected\nspatial and channel graphs to extract spatial information between each node and\nchannel information within each node (i.e., extract contextual relationships of\nthe global features). Furthermore, it uses a downsample residual process to\nenhance the coordinate feature reuse to maintain the global information. The\nsegmentation data augmentation and alignment module helps to simultaneously\naugment and align BEV features and ground truth to geometrically preserve their\nalignment to achieve better segmentation results. Our experimental results on\nthe nuScenes benchmark dataset demonstrate that the RGC network outperforms\nfour state-of-the-art networks and its four variants in terms of IoU and mIoU.\nThe proposed RGC network achieves a higher mIoU of 3.1% than the best\nstate-of-the-art network, BEVFusion. Code and models will be released.",
            "author": [
                "Qiuxiao Chen",
                "Xiaojun Qi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04044v1",
                "http://arxiv.org/pdf/2312.04044v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04038v1",
            "title": "Reconstruction of dynamical systems from data without time labels",
            "updated": "2023-12-07T04:43:04Z",
            "published": "2023-12-07T04:43:04Z",
            "summary": "In this paper, we study the method to reconstruct dynamical systems from data\nwithout time labels. Data without time labels appear in many applications, such\nas molecular dynamics, single-cell RNA sequencing etc. Reconstruction of\ndynamical system from time sequence data has been studied extensively. However,\nthese methods do not apply if time labels are unknown. Without time labels,\nsequence data becomes distribution data. Based on this observation, we propose\nto treat the data as samples from a probability distribution and try to\nreconstruct the underlying dynamical system by minimizing the distribution\nloss, sliced Wasserstein distance more specifically. Extensive experiment\nresults demonstrate the effectiveness of the proposed method.",
            "author": [
                "Zhijun Zeng",
                "Pipi Hu",
                "Chenglong Bao",
                "Yi Zhu",
                "Zuoqiang Shi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04038v1",
                "http://arxiv.org/pdf/2312.04038v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.NA",
                "math.DS",
                "math.NA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04037v1",
            "title": "A Survey on Radar-Based Fall Detection",
            "updated": "2023-12-07T04:41:32Z",
            "published": "2023-12-07T04:41:32Z",
            "summary": "Fall detection, particularly critical for high-risk demographics like the\nelderly, is a key public health concern where timely detection can greatly\nminimize harm. With the advancements in radio frequency technology, radar has\nemerged as a powerful tool for human detection and tracking. Traditional\nmachine learning algorithms, such as Support Vector Machines (SVM) and\nk-Nearest Neighbors (kNN), have shown promising outcomes. However, deep\nlearning approaches, notably Convolutional Neural Networks (CNN) and Recurrent\nNeural Networks (RNN), have outperformed in learning intricate features and\nmanaging large, unstructured datasets. This survey offers an in-depth analysis\nof radar-based fall detection, with emphasis on Micro-Doppler, Range-Doppler,\nand Range-Doppler-Angles techniques. We discuss the intricacies and challenges\nin fall detection and emphasize the necessity for a clear definition of falls\nand appropriate detection criteria, informed by diverse influencing factors. We\npresent an overview of radar signal processing principles and the underlying\ntechnology of radar-based fall detection, providing an accessible insight into\nmachine learning and deep learning algorithms. After examining 74 research\narticles on radar-based fall detection published since 2000, we aim to bridge\ncurrent research gaps and underscore the potential future research strategies,\nemphasizing the real-world applications possibility and the unexplored\npotential of deep learning in improving radar-based fall detection.",
            "author": [
                "Shuting Hu",
                "Siyang Cao",
                "Nima Toosizadeh",
                "Jennifer Barton",
                "Melvin G. Hector",
                "Mindy J. Fain"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04037v1",
                "http://arxiv.org/pdf/2312.04037v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04036v1",
            "title": "DiffusionPhase: Motion Diffusion in Frequency Domain",
            "updated": "2023-12-07T04:39:22Z",
            "published": "2023-12-07T04:39:22Z",
            "summary": "In this study, we introduce a learning-based method for generating\nhigh-quality human motion sequences from text descriptions (e.g., ``A person\nwalks forward\"). Existing techniques struggle with motion diversity and smooth\ntransitions in generating arbitrary-length motion sequences, due to limited\ntext-to-motion datasets and the pose representations used that often lack\nexpressiveness or compactness. To address these issues, we propose the first\nmethod for text-conditioned human motion generation in the frequency domain of\nmotions. We develop a network encoder that converts the motion space into a\ncompact yet expressive parameterized phase space with high-frequency details\nencoded, capturing the local periodicity of motions in time and space with high\naccuracy. We also introduce a conditional diffusion model for predicting\nperiodic motion parameters based on text descriptions and a start pose,\nefficiently achieving smooth transitions between motion sequences associated\nwith different text descriptions. Experiments demonstrate that our approach\noutperforms current methods in generating a broader variety of high-quality\nmotions, and synthesizing long sequences with natural transitions.",
            "author": [
                "Weilin Wan",
                "Yiming Huang",
                "Shutong Wu",
                "Taku Komura",
                "Wenping Wang",
                "Dinesh Jayaraman",
                "Lingjie Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04036v1",
                "http://arxiv.org/pdf/2312.04036v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04035v1",
            "title": "Defense against ML-based Power Side-channel Attacks on DNN Accelerators\n  with Adversarial Attacks",
            "updated": "2023-12-07T04:38:01Z",
            "published": "2023-12-07T04:38:01Z",
            "summary": "Artificial Intelligence (AI) hardware accelerators have been widely adopted\nto enhance the efficiency of deep learning applications. However, they also\nraise security concerns regarding their vulnerability to power side-channel\nattacks (SCA). In these attacks, the adversary exploits unintended\ncommunication channels to infer sensitive information processed by the\naccelerator, posing significant privacy and copyright risks to the models.\nAdvanced machine learning algorithms are further employed to facilitate the\nside-channel analysis and exacerbate the privacy issue of AI accelerators.\nTraditional defense strategies naively inject execution noise to the runtime of\nAI models, which inevitably introduce large overheads.\n  In this paper, we present AIAShield, a novel defense methodology to safeguard\nFPGA-based AI accelerators and mitigate model extraction threats via\npower-based SCAs. The key insight of AIAShield is to leverage the prominent\nadversarial attack technique from the machine learning community to craft\ndelicate noise, which can significantly obfuscate the adversary's side-channel\nobservation while incurring minimal overhead to the execution of the protected\nmodel. At the hardware level, we design a new module based on ring oscillators\nto achieve fine-grained noise generation. At the algorithm level, we repurpose\nNeural Architecture Search to worsen the adversary's extraction results.\nExtensive experiments on the Nvidia Deep Learning Accelerator (NVDLA)\ndemonstrate that AIAShield outperforms existing solutions with excellent\ntransferability.",
            "author": [
                "Xiaobei Yan",
                "Chip Hong Chang",
                "Tianwei Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04035v1",
                "http://arxiv.org/pdf/2312.04035v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04032v1",
            "title": "RoAST: Robustifying Language Models via Adversarial Perturbation with\n  Selective Training",
            "updated": "2023-12-07T04:23:36Z",
            "published": "2023-12-07T04:23:36Z",
            "summary": "Fine-tuning pre-trained language models (LMs) has become the de facto\nstandard in many NLP tasks. Nevertheless, fine-tuned LMs are still prone to\nrobustness issues, such as adversarial robustness and model calibration.\nSeveral perspectives of robustness for LMs have been studied independently, but\nlacking a unified consideration in multiple perspectives. In this paper, we\npropose Robustifying LMs via Adversarial perturbation with Selective Training\n(RoAST), a simple yet effective fine-tuning technique to enhance the\nmulti-perspective robustness of LMs in a unified way. RoAST effectively\nincorporates two important sources for the model robustness, robustness on the\nperturbed inputs and generalizable knowledge in pre-trained LMs. To be\nspecific, RoAST introduces adversarial perturbation during fine-tuning while\nthe model parameters are selectively updated upon their relative importance to\nminimize unnecessary deviation. Under a unified evaluation of fine-tuned LMs by\nincorporating four representative perspectives of model robustness, we\ndemonstrate the effectiveness of RoAST compared to state-of-the-art fine-tuning\nmethods on six different types of LMs, which indicates its usefulness in\npractice.",
            "author": [
                "Jaehyung Kim",
                "Yuning Mao",
                "Rui Hou",
                "Hanchao Yu",
                "Davis Liang",
                "Pascale Fung",
                "Qifan Wang",
                "Fuli Feng",
                "Lifu Huang",
                "Madian Khabsa"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04032v1",
                "http://arxiv.org/pdf/2312.04032v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04030v1",
            "title": "Modeling Boundedly Rational Agents with Latent Inference Budgets",
            "updated": "2023-12-07T03:55:51Z",
            "published": "2023-12-07T03:55:51Z",
            "summary": "We study the problem of modeling a population of agents pursuing unknown\ngoals subject to unknown computational constraints. In standard models of\nbounded rationality, sub-optimal decision-making is simulated by adding\nhomoscedastic noise to optimal decisions rather than explicitly simulating\nconstrained inference. In this work, we introduce a latent inference budget\nmodel (L-IBM) that models agents' computational constraints explicitly, via a\nlatent variable (inferred jointly with a model of agents' goals) that controls\nthe runtime of an iterative inference algorithm. L-IBMs make it possible to\nlearn agent models using data from diverse populations of suboptimal actors. In\nthree modeling tasks -- inferring navigation goals from routes, inferring\ncommunicative intents from human utterances, and predicting next moves in human\nchess games -- we show that L-IBMs match or outperform Boltzmann models of\ndecision-making under uncertainty. Inferred inference budgets are themselves\nmeaningful, efficient to compute, and correlated with measures of player skill,\npartner skill and task difficulty.",
            "author": [
                "Athul Paul Jacob",
                "Abhishek Gupta",
                "Jacob Andreas"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04030v1",
                "http://arxiv.org/pdf/2312.04030v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04029v1",
            "title": "Improved Face Representation via Joint Label Classification and\n  Supervised Contrastive Clustering",
            "updated": "2023-12-07T03:55:20Z",
            "published": "2023-12-07T03:55:20Z",
            "summary": "Face clustering tasks can learn hierarchical semantic information from\nlarge-scale data, which has the potential to help facilitate face recognition.\nHowever, there are few works on this problem. This paper explores it by\nproposing a joint optimization task of label classification and supervised\ncontrastive clustering to introduce the cluster knowledge to the traditional\nface recognition task in two ways. We first extend ArcFace with a\ncluster-guided angular margin to adjust the within-class feature distribution\naccording to the hard level of face clustering. Secondly, we propose a\nsupervised contrastive clustering approach to pull the features to the cluster\ncenter and propose the cluster-aligning procedure to align the cluster center\nand the learnable class center in the classifier for joint training. Finally,\nextensive qualitative and quantitative experiments on popular facial benchmarks\ndemonstrate the effectiveness of our paradigm and its superiority over the\nexisting approaches to face recognition.",
            "author": [
                "Zhenduo Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04029v1",
                "http://arxiv.org/pdf/2312.04029v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04028v1",
            "title": "ImFace++: A Sophisticated Nonlinear 3D Morphable Face Model with\n  Implicit Neural Representations",
            "updated": "2023-12-07T03:53:53Z",
            "published": "2023-12-07T03:53:53Z",
            "summary": "Accurate representations of 3D faces are of paramount importance in various\ncomputer vision and graphics applications. However, the challenges persist due\nto the limitations imposed by data discretization and model linearity, which\nhinder the precise capture of identity and expression clues in current studies.\nThis paper presents a novel 3D morphable face model, named ImFace++, to learn a\nsophisticated and continuous space with implicit neural representations.\nImFace++ first constructs two explicitly disentangled deformation fields to\nmodel complex shapes associated with identities and expressions, respectively,\nwhich simultaneously facilitate the automatic learning of correspondences\nacross diverse facial shapes. To capture more sophisticated facial details, a\nrefinement displacement field within the template space is further\nincorporated, enabling a fine-grained learning of individual-specific facial\ndetails. Furthermore, a Neural Blend-Field is designed to reinforce the\nrepresentation capabilities through adaptive blending of an array of local\nfields. In addition to ImFace++, we have devised an improved learning strategy\nto extend expression embeddings, allowing for a broader range of expression\nvariations. Comprehensive qualitative and quantitative evaluations demonstrate\nthat ImFace++ significantly advances the state-of-the-art in terms of both face\nreconstruction fidelity and correspondence accuracy.",
            "author": [
                "Mingwu Zheng",
                "Haiyu Zhang",
                "Hongyu Yang",
                "Liming Chen",
                "Di Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04028v1",
                "http://arxiv.org/pdf/2312.04028v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04027v1",
            "title": "The sample complexity of multi-distribution learning",
            "updated": "2023-12-07T03:53:17Z",
            "published": "2023-12-07T03:53:17Z",
            "summary": "Multi-distribution learning generalizes the classic PAC learning to handle\ndata coming from multiple distributions. Given a set of $k$ data distributions\nand a hypothesis class of VC dimension $d$, the goal is to learn a hypothesis\nthat minimizes the maximum population loss over $k$ distributions, up to\n$\\epsilon$ additive error. In this paper, we settle the sample complexity of\nmulti-distribution learning by giving an algorithm of sample complexity\n$\\widetilde{O}((d+k)\\epsilon^{-2}) \\cdot (k/\\epsilon)^{o(1)}$. This matches the\nlower bound up to sub-polynomial factor and resolves the COLT 2023 open problem\nof Awasthi, Haghtalab and Zhao [AHZ23].",
            "author": [
                "Binghui Peng"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04027v1",
                "http://arxiv.org/pdf/2312.04027v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.DS",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04025v1",
            "title": "Moirai: Towards Optimal Placement for Distributed Inference on\n  Heterogeneous Devices",
            "updated": "2023-12-07T03:46:14Z",
            "published": "2023-12-07T03:46:14Z",
            "summary": "The escalating size of Deep Neural Networks (DNNs) has spurred a growing\nresearch interest in hosting and serving DNN models across multiple devices. A\nnumber of studies have been reported to partition a DNN model across devices,\nproviding device placement solutions. The methods appeared in the literature,\nhowever, either suffer from poor placement performance due to the exponential\nsearch space or miss an optimal placement as a consequence of the reduced\nsearch space with limited heuristics. Moreover, these methods have ignored the\nruntime inter-operator optimization of a computation graph when coarsening the\ngraph, which degrades the end-to-end inference performance. This paper presents\nMoirai that better exploits runtime inter-operator fusion in a model to render\na coarsened computation graph, reducing the search space while maintaining the\ninter-operator optimization provided by inference backends. Moirai also\ngeneralizes the device placement algorithm from multiple perspectives by\nconsidering inference constraints and device heterogeneity.Extensive\nexperimental evaluation with 11 large DNNs demonstrates that Moirai outperforms\nthe state-of-the-art counterparts, i.e., Placeto, m-SCT, and GETF, up to\n4.28$\\times$ in reduction of the end-to-end inference latency. Moirai code is\nanonymously released at \\url{https://github.com/moirai-placement/moirai}.",
            "author": [
                "Beibei Zhang",
                "Hongwei Zhu",
                "Feng Gao",
                "Zhihui Yang",
                "Sean Xiaoyang Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04025v1",
                "http://arxiv.org/pdf/2312.04025v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04024v1",
            "title": "k* Distribution: Evaluating the Latent Space of Deep Neural Networks\n  using Local Neighborhood Analysis",
            "updated": "2023-12-07T03:42:48Z",
            "published": "2023-12-07T03:42:48Z",
            "summary": "Most examinations of neural networks' learned latent spaces typically employ\ndimensionality reduction techniques such as t-SNE or UMAP. While these methods\neffectively capture the overall sample distribution in the entire learned\nlatent space, they tend to distort the structure of sample distributions within\nspecific classes in the subset of the latent space. This distortion complicates\nthe task of easily distinguishing classes identifiable by neural networks. In\nresponse to this challenge, we introduce the k* Distribution methodology. This\napproach focuses on capturing the characteristics and structure of sample\ndistributions for individual classes within the subset of the learned latent\nspace using local neighborhood analysis. The key concept is to facilitate easy\ncomparison of different k* distributions, enabling analysis of how various\nclasses are processed by the same neural network. This provides a more profound\nunderstanding of existing contemporary visualizations. Our study reveals three\ndistinct distributions of samples within the learned latent space subset: a)\nFractured, b) Overlapped, and c) Clustered. We note and demonstrate that the\ndistribution of samples within the network's learned latent space significantly\nvaries depending on the class. Furthermore, we illustrate that our analysis can\nbe applied to explore the latent space of diverse neural network architectures,\nvarious layers within neural networks, transformations applied to input\nsamples, and the distribution of training and testing data for neural networks.\nWe anticipate that our approach will facilitate more targeted investigations\ninto neural networks by collectively examining the distribution of different\nsamples within the learned latent space.",
            "author": [
                "Shashank Kotyan",
                "Ueda Tatsuya",
                "Danilo Vasconcellos Vargas"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04024v1",
                "http://arxiv.org/pdf/2312.04024v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04021v1",
            "title": "A Study on the Calibration of In-context Learning",
            "updated": "2023-12-07T03:37:39Z",
            "published": "2023-12-07T03:37:39Z",
            "summary": "Modern auto-regressive language models are trained to minimize log loss on\nbroad data by predicting the next token so they are expected to get calibrated\nanswers when framing a problem as a next-token prediction task. We study this\nfor in-context learning (ICL), a widely used way to adapt frozen large language\nmodels (LLMs) via crafting prompts, and investigate the trade-offs between\nperformance and calibration on a wide range of natural language understanding\nand reasoning tasks. We conduct extensive experiments to show that such\ntrade-offs may get worse as we increase model size, incorporate more ICL\nexamples, and fine-tune models using instruction, dialog, or reinforcement\nlearning from human feedback (RLHF) on carefully curated datasets. Furthermore,\nwe find that common recalibration techniques that are widely effective such as\ntemperature scaling provide limited gains in calibration errors, suggesting\nthat new methods may be required for settings where models are expected to be\nreliable.",
            "author": [
                "Hanlin Zhang",
                "Yi-Fan Zhang",
                "Yaodong Yu",
                "Dhruv Madeka",
                "Dean Foster",
                "Eric Xing",
                "Hima Lakkaraju",
                "Sham Kakade"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04021v1",
                "http://arxiv.org/pdf/2312.04021v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04016v1",
            "title": "PartDistill: 3D Shape Part Segmentation by Vision-Language Model\n  Distillation",
            "updated": "2023-12-07T03:10:03Z",
            "published": "2023-12-07T03:10:03Z",
            "summary": "This paper proposes a cross-modal distillation framework, PartDistill, which\ntransfers 2D knowledge from vision-language models (VLMs) to facilitate 3D\nshape part segmentation. PartDistill addresses three major challenges in this\ntask: the lack of 3D segmentation in invisible or undetected regions in the 2D\nprojections, inaccurate and inconsistent 2D predictions by VLMs, and the lack\nof knowledge accumulation across different 3D shapes. PartDistill consists of a\nteacher network that uses a VLM to make 2D predictions and a student network\nthat learns from the 2D predictions while extracting geometrical features from\nmultiple 3D shapes to carry out 3D part segmentation. A bi-directional\ndistillation, including forward and backward distillations, is carried out\nwithin the framework, where the former forward distills the 2D predictions to\nthe student network, and the latter improves the quality of the 2D predictions,\nwhich subsequently enhances the final 3D part segmentation. Moreover,\nPartDistill can exploit generative models that facilitate effortless 3D shape\ncreation for generating knowledge sources to be distilled. Through extensive\nexperiments, PartDistill boosts the existing methods with substantial margins\non widely used ShapeNetPart and PartE datasets, by more than 15% and 12% higher\nmIoU scores, respectively.",
            "author": [
                "Ardian Umam",
                "Cheng-Kun Yang",
                "Min-Hung Chen",
                "Jen-Hui Chuang",
                "Yen-Yu Lin"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04016v1",
                "http://arxiv.org/pdf/2312.04016v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04013v1",
            "title": "A self-improvable Polymer Discovery Framework Based on Conditional\n  Generative Model",
            "updated": "2023-12-07T03:00:38Z",
            "published": "2023-12-07T03:00:38Z",
            "summary": "In this work, we introduce a polymer discovery platform designed to identify\npolymers with tailored properties efficiently, exemplified through the\ndiscovery of high-performance polymer electrolytes. The platform integrates\nthree core components: a conditioned generative model, validation modules, and\na feedback mechanism, creating a self-improving system for material innovation.\nTo demonstrate the efficacy of this platform, it is used to identify polymer\nelectrolyte materials with high ionic conductivity. A simple conditional\ngenerative model, based on the minGPT architecture, can effectively generate\ncandidate polymers that exhibit a mean ionic conductivity that is significantly\ngreater than those in the original training set. This approach, coupled with\nmolecular dynamics simulations for validation and a specifically designed\nacquisition mechanism, allows the platform to refine its output iteratively.\nNotably, after the first iteration, we observed an increase in both the mean\nand the lower bound of the ionic conductivity of the new polymer candidates.\nThe platform's effectiveness is underscored by the identification of 19 polymer\nrepeating units, each displaying a computed ionic conductivity surpassing that\nof Polyethylene Oxide (PEO). The discovery of these polymers validates the\nplatform's efficacy in identifying potential polymer materials. Acknowledging\ncurrent limitations, future work will focus on enhancing modeling techniques,\nvalidation processes, and acquisition strategies, aiming for broader\napplicability in polymer science and machine learning.",
            "author": [
                "Xiangyun Lei",
                "Weike Ye",
                "Zhenze Yang",
                "Daniel Schweigert",
                "Ha-Kyung Kwon",
                "Arash Khajeh"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04013v1",
                "http://arxiv.org/pdf/2312.04013v1"
            ],
            "primary_category": "physics.chem-ph",
            "category": [
                "physics.chem-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04000v1",
            "title": "LiDAR: Sensing Linear Probing Performance in Joint Embedding SSL\n  Architectures",
            "updated": "2023-12-07T02:31:28Z",
            "published": "2023-12-07T02:31:28Z",
            "summary": "Joint embedding (JE) architectures have emerged as a promising avenue for\nacquiring transferable data representations. A key obstacle to using JE\nmethods, however, is the inherent challenge of evaluating learned\nrepresentations without access to a downstream task, and an annotated dataset.\nWithout efficient and reliable evaluation, it is difficult to iterate on\narchitectural and training choices for JE methods. In this paper, we introduce\nLiDAR (Linear Discriminant Analysis Rank), a metric designed to measure the\nquality of representations within JE architectures. Our metric addresses\nseveral shortcomings of recent approaches based on feature covariance rank by\ndiscriminating between informative and uninformative features. In essence,\nLiDAR quantifies the rank of the Linear Discriminant Analysis (LDA) matrix\nassociated with the surrogate SSL task -- a measure that intuitively captures\nthe information content as it pertains to solving the SSL task. We empirically\ndemonstrate that LiDAR significantly surpasses naive rank based approaches in\nits predictive power of optimal hyperparameters. Our proposed criterion\npresents a more robust and intuitive means of assessing the quality of\nrepresentations within JE architectures, which we hope facilitates broader\nadoption of these powerful techniques in various domains.",
            "author": [
                "Vimal Thilak",
                "Chen Huang",
                "Omid Saremi",
                "Laurent Dinh",
                "Hanlin Goh",
                "Preetum Nakkiran",
                "Joshua M. Susskind",
                "Etai Littwin"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04000v1",
                "http://arxiv.org/pdf/2312.04000v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03998v1",
            "title": "Series2Vec: Similarity-based Self-supervised Representation Learning for\n  Time Series Classification",
            "updated": "2023-12-07T02:30:40Z",
            "published": "2023-12-07T02:30:40Z",
            "summary": "We argue that time series analysis is fundamentally different in nature to\neither vision or natural language processing with respect to the forms of\nmeaningful self-supervised learning tasks that can be defined. Motivated by\nthis insight, we introduce a novel approach called \\textit{Series2Vec} for\nself-supervised representation learning. Unlike other self-supervised methods\nin time series, which carry the risk of positive sample variants being less\nsimilar to the anchor sample than series in the negative set, Series2Vec is\ntrained to predict the similarity between two series in both temporal and\nspectral domains through a self-supervised task. Series2Vec relies primarily on\nthe consistency of the unsupervised similarity step, rather than the intrinsic\nquality of the similarity measurement, without the need for hand-crafted data\naugmentation. To further enforce the network to learn similar representations\nfor similar time series, we propose a novel approach that applies\norder-invariant attention to each representation within the batch during\ntraining. Our evaluation of Series2Vec on nine large real-world datasets, along\nwith the UCR/UEA archive, shows enhanced performance compared to current\nstate-of-the-art self-supervised techniques for time series. Additionally, our\nextensive experiments show that Series2Vec performs comparably with fully\nsupervised training and offers high efficiency in datasets with limited-labeled\ndata. Finally, we show that the fusion of Series2Vec with other representation\nlearning models leads to enhanced performance for time series classification.\nCode and models are open-source at\n\\url{https://github.com/Navidfoumani/Series2Vec.}",
            "author": [
                "Navid Mohammadi Foumani",
                "Chang Wei Tan",
                "Geoffrey I. Webb",
                "Mahsa Salehi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03998v1",
                "http://arxiv.org/pdf/2312.03998v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03991v1",
            "title": "MICRO: Model-Based Offline Reinforcement Learning with a Conservative\n  Bellman Operator",
            "updated": "2023-12-07T02:17:45Z",
            "published": "2023-12-07T02:17:45Z",
            "summary": "Offline reinforcement learning (RL) faces a significant challenge of\ndistribution shift. Model-free offline RL penalizes the Q value for\nout-of-distribution (OOD) data or constrains the policy closed to the behavior\npolicy to tackle this problem, but this inhibits the exploration of the OOD\nregion. Model-based offline RL, which uses the trained environment model to\ngenerate more OOD data and performs conservative policy optimization within\nthat model, has become an effective method for this problem. However, the\ncurrent model-based algorithms rarely consider agent robustness when\nincorporating conservatism into policy. Therefore, the new model-based offline\nalgorithm with a conservative Bellman operator (MICRO) is proposed. This method\ntrades off performance and robustness via introducing the robust Bellman\noperator into the algorithm. Compared with previous model-based algorithms with\nrobust adversarial models, MICRO can significantly reduce the computation cost\nby only choosing the minimal Q value in the state uncertainty set. Extensive\nexperiments demonstrate that MICRO outperforms prior RL algorithms in offline\nRL benchmark and is considerably robust to adversarial perturbations.",
            "author": [
                "Xiao-Yin Liu",
                "Xiao-Hu Zhou",
                "Guo-Tao Li",
                "Hao Li",
                "Mei-Jiang Gui",
                "Tian-Yu Xiang",
                "De-Xing Huang",
                "Zeng-Guang Hou"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03991v1",
                "http://arxiv.org/pdf/2312.03991v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03989v1",
            "title": "Rapid detection of rare events from in situ X-ray diffraction data using\n  machine learning",
            "updated": "2023-12-07T02:14:39Z",
            "published": "2023-12-07T02:14:39Z",
            "summary": "High-energy X-ray diffraction methods can non-destructively map the 3D\nmicrostructure and associated attributes of metallic polycrystalline\nengineering materials in their bulk form. These methods are often combined with\nexternal stimuli such as thermo-mechanical loading to take snapshots over time\nof the evolving microstructure and attributes. However, the extreme data\nvolumes and the high costs of traditional data acquisition and reduction\napproaches pose a barrier to quickly extracting actionable insights and\nimproving the temporal resolution of these snapshots. Here we present a fully\nautomated technique capable of rapidly detecting the onset of plasticity in\nhigh-energy X-ray microscopy data. Our technique is computationally faster by\nat least 50 times than the traditional approaches and works for data sets that\nare up to 9 times sparser than a full data set. This new technique leverages\nself-supervised image representation learning and clustering to transform\nmassive data into compact, semantic-rich representations of visually salient\ncharacteristics (e.g., peak shapes). These characteristics can be a rapid\nindicator of anomalous events such as changes in diffraction peak shapes. We\nanticipate that this technique will provide just-in-time actionable information\nto drive smarter experiments that effectively deploy multi-modal X-ray\ndiffraction methods that span many decades of length scales.",
            "author": [
                "Weijian Zheng",
                "Jun-Sang Park",
                "Peter Kenesei",
                "Ahsan Ali",
                "Zhengchun Liu",
                "Ian T. Foster",
                "Nicholas Schwarz",
                "Rajkumar Kettimuthu",
                "Antonino Miceli",
                "Hemant Sharma"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03989v1",
                "http://arxiv.org/pdf/2312.03989v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cond-mat.mtrl-sci",
                "eess.IV",
                "physics.data-an"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03987v1",
            "title": "Cost-Effective In-Context Learning for Entity Resolution: A Design Space\n  Exploration",
            "updated": "2023-12-07T02:09:27Z",
            "published": "2023-12-07T02:09:27Z",
            "summary": "Entity resolution (ER) is an important data integration task with a wide\nspectrum of applications. The state-of-the-art solutions on ER rely on\npre-trained language models (PLMs), which require fine-tuning on a lot of\nlabeled matching/non-matching entity pairs. Recently, large languages models\n(LLMs), such as GPT-4, have shown the ability to perform many tasks without\ntuning model parameters, which is known as in-context learning (ICL) that\nfacilitates effective learning from a few labeled input context demonstrations.\nHowever, existing ICL approaches to ER typically necessitate providing a task\ndescription and a set of demonstrations for each entity pair and thus have\nlimitations on the monetary cost of interfacing LLMs. To address the problem,\nin this paper, we provide a comprehensive study to investigate how to develop a\ncost-effective batch prompting approach to ER. We introduce a framework BATCHER\nconsisting of demonstration selection and question batching and explore\ndifferent design choices that support batch prompting for ER. We also devise a\ncovering-based demonstration selection strategy that achieves an effective\nbalance between matching accuracy and monetary cost. We conduct a thorough\nevaluation to explore the design space and evaluate our proposed strategies.\nThrough extensive experiments, we find that batch prompting is very\ncost-effective for ER, compared with not only PLM-based methods fine-tuned with\nextensive labeled data but also LLM-based methods with manually designed\nprompting. We also provide guidance for selecting appropriate design choices\nfor batch prompting.",
            "author": [
                "Meihao Fan",
                "Xiaoyue Han",
                "Ju Fan",
                "Chengliang Chai",
                "Nan Tang",
                "Guoliang Li",
                "Xiaoyong Du"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03987v1",
                "http://arxiv.org/pdf/2312.03987v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03986v1",
            "title": "An Unsupervised Machine Learning Scheme for Index-Based CSI Feedback in\n  Wi-Fi",
            "updated": "2023-12-07T02:04:32Z",
            "published": "2023-12-07T02:04:32Z",
            "summary": "With the ever-increasing demand for high-speed wireless data transmission,\nbeamforming techniques have been proven to be crucial in improving the data\nrate and the signal-to-noise ratio (SNR) at the receiver. However, they require\nfeedback mechanisms that need an overhead of information and increase the\nsystem complexity, potentially challenging the efficiency and capacity of\nmodern wireless networks. This paper investigates novel index-based feedback\nmechanisms that aim at reducing the beamforming feedback overhead in Wi-Fi\nlinks. The proposed methods mitigate the overhead by generating a set of\ncandidate beamforming vectors using an unsupervised learning-based framework.\nThe amount of feedback information required is thus reduced by using the index\nof the candidate as feedback instead of transmitting the entire beamforming\nmatrix. We explore several methods that consider different representations of\nthe data in the candidate set. In particular, we propose five different ways to\ngenerate and represent the candidate sets that consider the covariance matrices\nof the channel, serialize the feedback matrix, and account for the effective\ndistance, among others. Additionally, we also discuss the implications of using\npartial information in the compressed beamforming feedback on the link\nperformance and compare it with the newly proposed index-based methods.\nExtensive IEEE 802.11 standard-compliant simulation results show that the\nproposed methods effectively minimize the feedback overhead, enhancing the\nthroughput while maintaining an adequate link performance.",
            "author": [
                "Mrugen Deshmukh",
                "Zinan Lin",
                "Hanqing Lou",
                "Mahmoud Kamel",
                "Rui Yang",
                "Ismail Guvenc"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03986v1",
                "http://arxiv.org/pdf/2312.03986v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03979v1",
            "title": "Node-aware Bi-smoothing: Certified Robustness against Graph Injection\n  Attacks",
            "updated": "2023-12-07T01:24:48Z",
            "published": "2023-12-07T01:24:48Z",
            "summary": "Deep Graph Learning (DGL) has emerged as a crucial technique across various\ndomains. However, recent studies have exposed vulnerabilities in DGL models,\nsuch as susceptibility to evasion and poisoning attacks. While empirical and\nprovable robustness techniques have been developed to defend against graph\nmodification attacks (GMAs), the problem of certified robustness against graph\ninjection attacks (GIAs) remains largely unexplored. To bridge this gap, we\nintroduce the node-aware bi-smoothing framework, which is the first certifiably\nrobust approach for general node classification tasks against GIAs. Notably,\nthe proposed node-aware bi-smoothing scheme is model-agnostic and is applicable\nfor both evasion and poisoning attacks. Through rigorous theoretical analysis,\nwe establish the certifiable conditions of our smoothing scheme. We also\nexplore the practical implications of our node-aware bi-smoothing schemes in\ntwo contexts: as an empirical defense approach against real-world GIAs and in\nthe context of recommendation systems. Furthermore, we extend two\nstate-of-the-art certified robustness frameworks to address node injection\nattacks and compare our approach against them. Extensive evaluations\ndemonstrate the effectiveness of our proposed certificates.",
            "author": [
                "Yuni Lai",
                "Yulin Zhu",
                "Bailin Pan",
                "Kai Zhou"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03979v1",
                "http://arxiv.org/pdf/2312.03979v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03957v1",
            "title": "PerSival: Neural-network-based visualisation for pervasive\n  continuum-mechanical simulations in musculoskeletal biomechanics",
            "updated": "2023-12-07T00:07:35Z",
            "published": "2023-12-07T00:07:35Z",
            "summary": "This paper presents a novel neural network architecture for the purpose of\npervasive visualisation of a 3D human upper limb musculoskeletal system model.\nBringing simulation capabilities to resource-poor systems like mobile devices\nis of growing interest across many research fields, to widen applicability of\nmethods and results. Until recently, this goal was thought to be out of reach\nfor realistic continuum-mechanical simulations of musculoskeletal systems, due\nto prohibitive computational cost. Within this work we use a sparse grid\nsurrogate to capture the surface deformation of the m.~biceps brachii in order\nto train a deep learning model, used for real-time visualisation of the same\nmuscle. Both these surrogate models take 5 muscle activation levels as input\nand output Cartesian coordinate vectors for each mesh node on the muscle's\nsurface. Thus, the neural network architecture features a significantly lower\ninput than output dimension. 5 muscle activation levels were sufficient to\nachieve an average error of 0.97 +/- 0.16 mm, or 0.57 +/- 0.10 % for the 2809\nmesh node positions of the biceps. The model achieved evaluation times of 9.88\nms per predicted deformation state on CPU only and 3.48 ms with GPU-support,\nleading to theoretical frame rates of 101 fps and 287 fps respectively. Deep\nlearning surrogates thus provide a way to make continuum-mechanical simulations\naccessible for visual real-time applications.",
            "author": [
                "David Rosin",
                "Johannes K\u00e4ssinger",
                "Xingyao Yu",
                "Okan Avci",
                "Christian Bleiler",
                "Oliver R\u00f6hrle"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03957v1",
                "http://arxiv.org/pdf/2312.03957v1"
            ],
            "primary_category": "q-bio.TO",
            "category": [
                "q-bio.TO",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03954v1",
            "title": "Disentangling Fact from Grid Cell Fiction in Trained Deep Path\n  Integrators",
            "updated": "2023-12-06T23:44:43Z",
            "published": "2023-12-06T23:44:43Z",
            "summary": "Work on deep learning-based models of grid cells suggests that grid cells\ngenerically and robustly arise from optimizing networks to path integrate,\ni.e., track one's spatial position by integrating self-velocity signals. In\nprevious work, we challenged this path integration hypothesis by showing that\ndeep neural networks trained to path integrate almost always do so, but almost\nnever learn grid-like tuning unless separately inserted by researchers via\nmechanisms unrelated to path integration. In this work, we restate the key\nevidence substantiating these insights, then address a response to by authors\nof one of the path integration hypothesis papers. First, we show that the\nresponse misinterprets our work, indirectly confirming our points. Second, we\nevaluate the response's preferred ``unified theory for the origin of grid\ncells\" in trained deep path integrators and show that it is at best\n``occasionally suggestive,\" not exact or comprehensive. We finish by\nconsidering why assessing model quality through prediction of biological neural\nactivity by regression of activity in deep networks can lead to the wrong\nconclusions.",
            "author": [
                "Rylan Schaeffer",
                "Mikail Khona",
                "Sanmi Koyejo",
                "Ila Rani Fiete"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03954v1",
                "http://arxiv.org/pdf/2312.03954v1"
            ],
            "primary_category": "q-bio.NC",
            "category": [
                "q-bio.NC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03951v1",
            "title": "Understanding the Role of Optimization in Double Descent",
            "updated": "2023-12-06T23:29:00Z",
            "published": "2023-12-06T23:29:00Z",
            "summary": "The phenomenon of model-wise double descent, where the test error peaks and\nthen reduces as the model size increases, is an interesting topic that has\nattracted the attention of researchers due to the striking observed gap between\ntheory and practice \\citep{Belkin2018ReconcilingMM}. Additionally, while double\ndescent has been observed in various tasks and architectures, the peak of\ndouble descent can sometimes be noticeably absent or diminished, even without\nexplicit regularization, such as weight decay and early stopping. In this\npaper, we investigate this intriguing phenomenon from the optimization\nperspective and propose a simple optimization-based explanation for why double\ndescent sometimes occurs weakly or not at all. To the best of our knowledge, we\nare the first to demonstrate that many disparate factors contributing to\nmodel-wise double descent (initialization, normalization, batch size, learning\nrate, optimization algorithm) are unified from the viewpoint of optimization:\nmodel-wise double descent is observed if and only if the optimizer can find a\nsufficiently low-loss minimum. These factors directly affect the condition\nnumber of the optimization problem or the optimizer and thus affect the final\nminimum found by the optimizer, reducing or increasing the height of the double\ndescent peak. We conduct a series of controlled experiments on random feature\nmodels and two-layer neural networks under various optimization settings,\ndemonstrating this optimization-based unified view. Our results suggest the\nfollowing implication: Double descent is unlikely to be a problem for\nreal-world machine learning setups. Additionally, our results help explain the\ngap between weak double descent peaks in practice and strong peaks observable\nin carefully designed setups.",
            "author": [
                "Chris Yuhao Liu",
                "Jeffrey Flanigan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03951v1",
                "http://arxiv.org/pdf/2312.03951v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03950v1",
            "title": "A Scalable and Generalizable Pathloss Map Prediction",
            "updated": "2023-12-06T23:22:49Z",
            "published": "2023-12-06T23:22:49Z",
            "summary": "Large-scale channel prediction, i.e., estimation of the pathloss from\ngeographical/morphological/building maps, is an essential component of wireless\nnetwork planning. Ray tracing (RT)-based methods have been widely used for many\nyears, but they require significant computational effort that may become\nprohibitive with the increased network densification and/or use of higher\nfrequencies in B5G/6G systems. In this paper, we propose a data-driven,\nmodel-free pathloss map prediction (PMP) method, called PMNet. PMNet uses a\nsupervised learning approach: it is trained on a limited amount of RT (or\nchannel measurement) data and map data. Once trained, PMNet can predict\npathloss over location with high accuracy (an RMSE level of $10^{-2}$) in a few\nmilliseconds. We further extend PMNet by employing transfer learning (TL). TL\nallows PMNet to learn a new network scenario quickly (x5.6 faster training) and\nefficiently (using x4.5 less data) by transferring knowledge from a pre-trained\nmodel, while retaining accuracy. Our results demonstrate that PMNet is a\nscalable and generalizable ML-based PMP method, showing its potential to be\nused in several network optimization applications.",
            "author": [
                "Ju-Hyung Lee",
                "Andreas F. Molisch"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03950v1",
                "http://arxiv.org/pdf/2312.03950v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "cs.LG",
                "cs.NI",
                "eess.SP",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03940v1",
            "title": "PECANN: Parallel Efficient Clustering with Graph-Based Approximate\n  Nearest Neighbor Search",
            "updated": "2023-12-06T22:43:50Z",
            "published": "2023-12-06T22:43:50Z",
            "summary": "This paper studies density-based clustering of point sets. These methods use\ndense regions of points to detect clusters of arbitrary shapes. In particular,\nwe study variants of density peaks clustering, a popular type of algorithm that\nhas been shown to work well in practice. Our goal is to cluster large\nhigh-dimensional datasets, which are prevalent in practice. Prior solutions are\neither sequential, and cannot scale to large data, or are specialized for\nlow-dimensional data.\n  This paper unifies the different variants of density peaks clustering into a\nsingle framework, PECANN, by abstracting out several key steps common to this\nclass of algorithms. One such key step is to find nearest neighbors that\nsatisfy a predicate function, and one of the main contributions of this paper\nis an efficient way to do this predicate search using graph-based approximate\nnearest neighbor search (ANNS). To provide ample parallelism, we propose a\ndoubling search technique that enables points to find an approximate nearest\nneighbor satisfying the predicate in a small number of rounds. Our technique\ncan be applied to many existing graph-based ANNS algorithms, which can all be\nplugged into PECANN.\n  We implement five clustering algorithms with PECANN and evaluate them on\nsynthetic and real-world datasets with up to 1.28 million points and up to 1024\ndimensions on a 30-core machine with two-way hyper-threading. Compared to the\nstate-of-the-art FASTDP algorithm for high-dimensional density peaks\nclustering, which is sequential, our best algorithm is 45x-734x faster while\nachieving competitive ARI scores. Compared to the state-of-the-art parallel\nDPC-based algorithm, which is optimized for low dimensions, we show that PECANN\nis two orders of magnitude faster. As far as we know, our work is the first to\nevaluate DPC variants on large high-dimensional real-world image and text\nembedding datasets.",
            "author": [
                "Shangdi Yu",
                "Joshua Engels",
                "Yihao Huang",
                "Julian Shun"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03940v1",
                "http://arxiv.org/pdf/2312.03940v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "cs.DC",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03936v1",
            "title": "The Potential of Vision-Language Models for Content Moderation of\n  Children's Videos",
            "updated": "2023-12-06T22:29:16Z",
            "published": "2023-12-06T22:29:16Z",
            "summary": "Natural language supervision has been shown to be effective for zero-shot\nlearning in many computer vision tasks, such as object detection and activity\nrecognition. However, generating informative prompts can be challenging for\nmore subtle tasks, such as video content moderation. This can be difficult, as\nthere are many reasons why a video might be inappropriate, beyond violence and\nobscenity. For example, scammers may attempt to create junk content that is\nsimilar to popular educational videos but with no meaningful information. This\npaper evaluates the performance of several CLIP variations for content\nmoderation of children's cartoons in both the supervised and zero-shot setting.\nWe show that our proposed model (Vanilla CLIP with Projection Layer)\noutperforms previous work conducted on the Malicious or Benign (MOB) benchmark\nfor video content moderation. This paper presents an in depth analysis of how\ncontext-specific language prompts affect content moderation performance. Our\nresults indicate that it is important to include more context in content\nmoderation prompts, particularly for cartoon videos as they are not well\nrepresented in the CLIP training data.",
            "author": [
                "Syed Hammad Ahmed",
                "Shengnan Hu",
                "Gita Sukthankar"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03936v1",
                "http://arxiv.org/pdf/2312.03936v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.CY",
                "cs.LG",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03932v1",
            "title": "Stochastic Gravitational Wave Background Detection through NANOGrav\n  15-year Data Set in the View of Massive Gravity",
            "updated": "2023-12-06T22:19:35Z",
            "published": "2023-12-06T22:19:35Z",
            "summary": "Convincing evidence of a stochastic gravitational wave background has been\nfound by the NANOGrav collaboration in the 15-Year data set. From this signal,\nwe can evaluate the possibility of its source being from the early universe\nthrough the tensor perturbations induced by a massive spin-2 graviton field. We\nconsider a time dependent model of the minimal theory of massive gravity, and\nfind values of the graviton mass, mass cutoff time, and Hubble rate of\ninflation that amplify the energy spectra of primordial gravitational waves\nsufficiently to reproduce the signal from the NANOGrav data within 1-3 standard\ndeviation. However, a suppression mechanism for high frequency modes must be\nintroduced to conservatively obey the big bang nucleosynthesis (BBN) bound.\nWhile there are regions of the parameter space that reproduces the signal, it\nremains a challenge to simultaneously respect the BBN and cosmic microwave\nbackground (CMB) bounds without making the graviton mass cutoff time too deep\ninto the matter dominated era.",
            "author": [
                "Chris Choi",
                "Jacob Magallanes",
                "Murman Gurgenidze",
                "Tina Kahniashvili"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03932v1",
                "http://arxiv.org/pdf/2312.03932v1"
            ],
            "primary_category": "astro-ph.CO",
            "category": [
                "astro-ph.CO",
                "gr-qc",
                "hep-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03928v1",
            "title": "Adaptive Weighted Co-Learning for Cross-Domain Few-Shot Learning",
            "updated": "2023-12-06T22:09:52Z",
            "published": "2023-12-06T22:09:52Z",
            "summary": "Due to the availability of only a few labeled instances for the novel target\nprediction task and the significant domain shift between the well annotated\nsource domain and the target domain, cross-domain few-shot learning (CDFSL)\ninduces a very challenging adaptation problem. In this paper, we propose a\nsimple Adaptive Weighted Co-Learning (AWCoL) method to address the CDFSL\nchallenge by adapting two independently trained source prototypical\nclassification models to the target task in a weighted co-learning manner. The\nproposed method deploys a weighted moving average prediction strategy to\ngenerate probabilistic predictions from each model, and then conducts adaptive\nco-learning by jointly fine-tuning the two models in an alternating manner\nbased on the pseudo-labels and instance weights produced from the predictions.\nMoreover, a negative pseudo-labeling regularizer is further deployed to improve\nthe fine-tuning process by penalizing false predictions. Comprehensive\nexperiments are conducted on multiple benchmark datasets and the empirical\nresults demonstrate that the proposed method produces state-of-the-art CDFSL\nperformance.",
            "author": [
                "Abdullah Alchihabi",
                "Marzi Heidari",
                "Yuhong Guo"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03928v1",
                "http://arxiv.org/pdf/2312.03928v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03924v1",
            "title": "Integrating Traditional CS Class Activities with Computing for Social\n  Good, Ethics, and Communication and Leadership Skills",
            "updated": "2023-12-06T21:48:55Z",
            "published": "2023-12-06T21:48:55Z",
            "summary": "Software and information technologies are becoming increasingly integrated\nand pervasive in human society and range from automated decision making and\nsocial media and entertainment, to running critical social and physical\ninfrastructures like government programs, utilities, and financial\ninstitutions. As a result, there is a growing awareness of the need to develop\nprofessionals who will harness these technologies in fair and inclusive ways\nand use them to address global issues like health, water management, poverty,\nand human rights. In this regard, many academic researchers have expressed the\nneed to complement traditional teaching of CS technical skills with computer\nand information ethics (computing for social good), as well as communication\nand leadership skills. In this paper, we describe our goals and some possible\nclass activities we have developed and refined over the past few years with\nencouraging results, to help CS students understand the potential uses of\ncomputing for social good. In these carefully planned project assignments, we\nseamlessly integrate traditional approaches to develop technical skills with\nbroader professional responsibility and soft skills. We then discuss the\nlessons learned from these activities and briefly outline future plans.",
            "author": [
                "Renato Cortinovis",
                "Devender Goyal",
                "Luiz Fernando Capretz"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03924v1",
                "http://arxiv.org/pdf/2312.03924v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03911v1",
            "title": "Improving Gradient-guided Nested Sampling for Posterior Inference",
            "updated": "2023-12-06T21:09:18Z",
            "published": "2023-12-06T21:09:18Z",
            "summary": "We present a performant, general-purpose gradient-guided nested sampling\nalgorithm, ${\\tt GGNS}$, combining the state of the art in differentiable\nprogramming, Hamiltonian slice sampling, clustering, mode separation, dynamic\nnested sampling, and parallelization. This unique combination allows ${\\tt\nGGNS}$ to scale well with dimensionality and perform competitively on a variety\nof synthetic and real-world problems. We also show the potential of combining\nnested sampling with generative flow networks to obtain large amounts of\nhigh-quality samples from the posterior distribution. This combination leads to\nfaster mode discovery and more accurate estimates of the partition function.",
            "author": [
                "Pablo Lemos",
                "Nikolay Malkin",
                "Will Handley",
                "Yoshua Bengio",
                "Yashar Hezaveh",
                "Laurence Perreault-Levasseur"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03911v1",
                "http://arxiv.org/pdf/2312.03911v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.CO",
                "stat.ME",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03905v1",
            "title": "A Pseudo-Semantic Loss for Autoregressive Models with Logical\n  Constraints",
            "updated": "2023-12-06T20:58:07Z",
            "published": "2023-12-06T20:58:07Z",
            "summary": "Neuro-symbolic AI bridges the gap between purely symbolic and neural\napproaches to learning. This often requires maximizing the likelihood of a\nsymbolic constraint w.r.t the neural network's output distribution. Such output\ndistributions are typically assumed to be fully-factorized. This limits the\napplicability of neuro-symbolic learning to the more expressive autoregressive\ndistributions, e.g., transformers. Under such distributions, computing the\nlikelihood of even simple constraints is #P-hard. Instead of attempting to\nenforce the constraint on the entire output distribution, we propose to do so\non a random, local approximation thereof. More precisely, we optimize the\nlikelihood of the constraint under a pseudolikelihood-based approximation\ncentered around a model sample. Our approximation is factorized, allowing the\nreuse of solutions to sub-problems, a main tenet for efficiently computing\nneuro-symbolic losses. Moreover, it is a local, high-fidelity approximation of\nthe likelihood, exhibiting low entropy and KL-divergence around the model\nsample. We evaluate our approach on Sudoku and shortest-path prediction cast as\nautoregressive generation, and observe that we greatly improve upon the base\nmodel's ability to predict logically-consistent outputs. We also evaluate on\nthe task of detoxifying large language models. Using a simple constraint\ndisallowing a list of toxic words, we are able to steer the model's outputs\naway from toxic generations, achieving SoTA detoxification compared to previous\napproaches.",
            "author": [
                "Kareem Ahmed",
                "Kai-Wei Chang",
                "Guy Van den Broeck"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03905v1",
                "http://arxiv.org/pdf/2312.03905v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03903v1",
            "title": "Adaptive Dependency Learning Graph Neural Networks",
            "updated": "2023-12-06T20:56:23Z",
            "published": "2023-12-06T20:56:23Z",
            "summary": "Graph Neural Networks (GNN) have recently gained popularity in the\nforecasting domain due to their ability to model complex spatial and temporal\npatterns in tasks such as traffic forecasting and region-based demand\nforecasting. Most of these methods require a predefined graph as input, whereas\nin real-life multivariate time series problems, a well-predefined dependency\ngraph rarely exists. This requirement makes it harder for GNNs to be utilised\nwidely for multivariate forecasting problems in other domains such as retail or\nenergy. In this paper, we propose a hybrid approach combining neural networks\nand statistical structure learning models to self-learn the dependencies and\nconstruct a dynamically changing dependency graph from multivariate data aiming\nto enable the use of GNNs for multivariate forecasting even when a well-defined\ngraph does not exist. The statistical structure modeling in conjunction with\nneural networks provides a well-principled and efficient approach by bringing\nin causal semantics to determine dependencies among the series. Finally, we\ndemonstrate significantly improved performance using our proposed approach on\nreal-world benchmark datasets without a pre-defined dependency graph.",
            "author": [
                "Abishek Sriramulu",
                "Nicolas Fourrier",
                "Christoph Bergmeir"
            ],
            "link": [
                "http://dx.doi.org/10.1016/j.ins.2022.12.086",
                "http://arxiv.org/abs/2312.03903v1",
                "http://arxiv.org/pdf/2312.03903v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03895v1",
            "title": "HLoOP -- Hyperbolic 2-space Local Outlier Probabilities",
            "updated": "2023-12-06T20:38:39Z",
            "published": "2023-12-06T20:38:39Z",
            "summary": "Hyperbolic geometry has recently garnered considerable attention in machine\nlearning due to its capacity to embed hierarchical graph structures with low\ndistortions for further downstream processing. This paper introduces a simple\nframework to detect local outliers for datasets grounded in hyperbolic 2-space\nreferred to as HLoOP (Hyperbolic Local Outlier Probability). Within a Euclidean\nspace, well-known techniques for local outlier detection are based on the Local\nOutlier Factor (LOF) and its variant, the LoOP (Local Outlier Probability),\nwhich incorporates probabilistic concepts to model the outlier level of a data\nvector. The developed HLoOP combines the idea of finding nearest neighbors,\ndensity-based outlier scoring with a probabilistic, statistically oriented\napproach. Therefore, the method consists in computing the Riemmanian distance\nof a data point to its nearest neighbors following a Gaussian probability\ndensity function expressed in a hyperbolic space. This is achieved by defining\na Gaussian cumulative distribution in this space. The HLoOP algorithm is tested\non the WordNet dataset yielding promising results. Code and data will be made\navailable on request for reproductibility.",
            "author": [
                "Cl\u00e9mence Allietta",
                "Jean-Philippe Condomines",
                "Jean-Yves Tourneret",
                "Emmanuel Lochin"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03895v1",
                "http://arxiv.org/pdf/2312.03895v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03891v1",
            "title": "Evaluation of Infrastructure-based Warning System on Driving Behaviors-A\n  Roundabout Study",
            "updated": "2023-12-06T20:31:22Z",
            "published": "2023-12-06T20:31:22Z",
            "summary": "Smart intersections have the potential to improve road safety with sensing,\ncommunication, and edge computing technologies. Perception sensors installed at\na smart intersection can monitor the traffic environment in real time and send\ninfrastructure-based warnings to nearby travelers through V2X communication.\nThis paper investigated how infrastructure-based warnings can influence driving\nbehaviors and improve roundabout safety through a driving-simulator study - a\nchallenging driving scenario for human drivers. A co-simulation platform\nintegrating Simulation of Urban Mobility (SUMO) and Webots was developed to\nserve as the driving simulator. A real-world roundabout in Ann Arbor, Michigan\nwas built in the co-simulation platform as the study area, and the merging\nscenarios were investigated. 36 participants were recruited and asked to\nnavigate the roundabout under three danger levels (e.g., low, medium, high) and\nthree collision warning designs (e.g., no warning, warning issued 1 second in\nadvance, warning issued 2 seconds in advance). Results indicated that advanced\nwarnings can significantly enhance safety by minimizing potential risks\ncompared to scenarios without warnings. Earlier warnings enabled smoother\ndriver responses and reduced abrupt decelerations. In addition, a personalized\nintention prediction model was developed to predict drivers' stop-or-go\ndecisions when the warning is displayed. Among all tested machine learning\nmodels, the XGBoost model achieved the highest prediction accuracy with a\nprecision rate of 95.56% and a recall rate of 97.73%.",
            "author": [
                "Cong Zhang",
                "Chi Tian",
                "Tianfang Han",
                "Hang Li",
                "Yiheng Feng",
                "Yunfeng Chen",
                "Robert W. Proctor",
                "Jiansong Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03891v1",
                "http://arxiv.org/pdf/2312.03891v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03889v1",
            "title": "A Masked Pruning Approach for Dimensionality Reduction in\n  Communication-Efficient Federated Learning Systems",
            "updated": "2023-12-06T20:29:23Z",
            "published": "2023-12-06T20:29:23Z",
            "summary": "Federated Learning (FL) represents a growing machine learning (ML) paradigm\ndesigned for training models across numerous nodes that retain local datasets,\nall without directly exchanging the underlying private data with the parameter\nserver (PS). Its increasing popularity is attributed to notable advantages in\nterms of training deep neural network (DNN) models under privacy aspects and\nefficient utilization of communication resources. Unfortunately, DNNs suffer\nfrom high computational and communication costs, as well as memory consumption\nin intricate tasks. These factors restrict the applicability of FL algorithms\nin communication-constrained systems with limited hardware resources.\n  In this paper, we develop a novel algorithm that overcomes these limitations\nby synergistically combining a pruning-based method with the FL process,\nresulting in low-dimensional representations of the model with minimal\ncommunication cost, dubbed Masked Pruning over FL (MPFL). The algorithm\noperates by initially distributing weights to the nodes through the PS.\nSubsequently, each node locally trains its model and computes pruning masks.\nThese low-dimensional masks are then transmitted back to the PS, which\ngenerates a consensus pruning mask, broadcasted back to the nodes. This\niterative process enhances the robustness and stability of the masked pruning\nmodel. The generated mask is used to train the FL model, achieving significant\nbandwidth savings. We present an extensive experimental study demonstrating the\nsuperior performance of MPFL compared to existing methods. Additionally, we\nhave developed an open-source software package for the benefit of researchers\nand developers in related fields.",
            "author": [
                "Tamir L. S. Gez",
                "Kobi Cohen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03889v1",
                "http://arxiv.org/pdf/2312.03889v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03886v1",
            "title": "On The Fairness Impacts of Hardware Selection in Machine Learning",
            "updated": "2023-12-06T20:24:17Z",
            "published": "2023-12-06T20:24:17Z",
            "summary": "In the machine learning ecosystem, hardware selection is often regarded as a\nmere utility, overshadowed by the spotlight on algorithms and data. This\noversight is particularly problematic in contexts like ML-as-a-service\nplatforms, where users often lack control over the hardware used for model\ndeployment. How does the choice of hardware impact generalization properties?\nThis paper investigates the influence of hardware on the delicate balance\nbetween model performance and fairness. We demonstrate that hardware choices\ncan exacerbate existing disparities, attributing these discrepancies to\nvariations in gradient flows and loss surfaces across different demographic\ngroups. Through both theoretical and empirical analysis, the paper not only\nidentifies the underlying factors but also proposes an effective strategy for\nmitigating hardware-induced performance imbalances.",
            "author": [
                "Sree Harsha Nelaturu",
                "Nishaanth Kanna Ravichandran",
                "Cuong Tran",
                "Sara Hooker",
                "Ferdinando Fioretto"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03886v1",
                "http://arxiv.org/pdf/2312.03886v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03885v1",
            "title": "Adapting Newton's Method to Neural Networks through a Summary of\n  Higher-Order Derivatives",
            "updated": "2023-12-06T20:24:05Z",
            "published": "2023-12-06T20:24:05Z",
            "summary": "We consider a gradient-based optimization method applied to a function\n$\\mathcal{L}$ of a vector of variables $\\boldsymbol{\\theta}$, in the case where\n$\\boldsymbol{\\theta}$ is represented as a tuple of tensors $(\\mathbf{T}_1,\n\\cdots, \\mathbf{T}_S)$. This framework encompasses many common use-cases, such\nas training neural networks by gradient descent. First, we propose a\ncomputationally inexpensive technique providing higher-order information on\n$\\mathcal{L}$, especially about the interactions between the tensors\n$\\mathbf{T}_s$, based on automatic differentiation and computational tricks.\nSecond, we use this technique at order 2 to build a second-order optimization\nmethod which is suitable, among other things, for training deep neural networks\nof various architectures. This second-order method leverages the partition\nstructure of $\\boldsymbol{\\theta}$ into tensors $(\\mathbf{T}_1, \\cdots,\n\\mathbf{T}_S)$, in such a way that it requires neither the computation of the\nHessian of $\\mathcal{L}$ according to $\\boldsymbol{\\theta}$, nor any\napproximation of it. The key part consists in computing a smaller matrix\ninterpretable as a \"Hessian according to the partition\", which can be computed\nexactly and efficiently. In contrast to many existing practical second-order\nmethods used in neural networks, which perform a diagonal or block-diagonal\napproximation of the Hessian or its inverse, the method we propose does not\nneglect interactions between layers. Finally, we can tune the coarseness of the\npartition to recover well-known optimization methods: the coarsest case\ncorresponds to Cauchy's steepest descent method, the finest case corresponds to\nthe usual Newton's method.",
            "author": [
                "Pierre Wolinski"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03885v1",
                "http://arxiv.org/pdf/2312.03885v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03881v1",
            "title": "FoMo Rewards: Can we cast foundation models as reward functions?",
            "updated": "2023-12-06T20:11:02Z",
            "published": "2023-12-06T20:11:02Z",
            "summary": "We explore the viability of casting foundation models as generic reward\nfunctions for reinforcement learning. To this end, we propose a simple pipeline\nthat interfaces an off-the-shelf vision model with a large language model.\nSpecifically, given a trajectory of observations, we infer the likelihood of an\ninstruction describing the task that the user wants an agent to perform. We\nshow that this generic likelihood function exhibits the characteristics ideally\nexpected from a reward function: it associates high values with the desired\nbehaviour and lower values for several similar, but incorrect policies.\nOverall, our work opens the possibility of designing open-ended agents for\ninteractive tasks via foundation models.",
            "author": [
                "Ekdeep Singh Lubana",
                "Johann Brehmer",
                "Pim de Haan",
                "Taco Cohen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03881v1",
                "http://arxiv.org/pdf/2312.03881v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03878v1",
            "title": "Domain constraints improve risk prediction when outcome data is missing",
            "updated": "2023-12-06T19:49:06Z",
            "published": "2023-12-06T19:49:06Z",
            "summary": "Machine learning models are often trained to predict the outcome resulting\nfrom a human decision. For example, if a doctor decides to test a patient for\ndisease, will the patient test positive? A challenge is that the human decision\ncensors the outcome data: we only observe test outcomes for patients doctors\nhistorically tested. Untested patients, for whom outcomes are unobserved, may\ndiffer from tested patients along observed and unobserved dimensions. We\npropose a Bayesian model class which captures this setting. The purpose of the\nmodel is to accurately estimate risk for both tested and untested patients.\nEstimating this model is challenging due to the wide range of possibilities for\nuntested patients. To address this, we propose two domain constraints which are\nplausible in health settings: a prevalence constraint, where the overall\ndisease prevalence is known, and an expertise constraint, where the human\ndecision-maker deviates from purely risk-based decision-making only along a\nconstrained feature set. We show theoretically and on synthetic data that\ndomain constraints improve parameter inference. We apply our model to a case\nstudy of cancer risk prediction, showing that the model's inferred risk\npredicts cancer diagnoses, its inferred testing policy captures known public\nhealth policies, and it can identify suboptimalities in test allocation. Though\nour case study is in healthcare, our analysis reveals a general class of domain\nconstraints which can improve model estimation in many settings.",
            "author": [
                "Sidhika Balachandar",
                "Nikhil Garg",
                "Emma Pierson"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03878v1",
                "http://arxiv.org/pdf/2312.03878v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03876v1",
            "title": "Scaling transformer neural networks for skillful and reliable\n  medium-range weather forecasting",
            "updated": "2023-12-06T19:46:06Z",
            "published": "2023-12-06T19:46:06Z",
            "summary": "Weather forecasting is a fundamental problem for anticipating and mitigating\nthe impacts of climate change. Recently, data-driven approaches for weather\nforecasting based on deep learning have shown great promise, achieving\naccuracies that are competitive with operational systems. However, those\nmethods often employ complex, customized architectures without sufficient\nablation analysis, making it difficult to understand what truly contributes to\ntheir success. Here we introduce Stormer, a simple transformer model that\nachieves state-of-the-art performance on weather forecasting with minimal\nchanges to the standard transformer backbone. We identify the key components of\nStormer through careful empirical analyses, including weather-specific\nembedding, randomized dynamics forecast, and pressure-weighted loss. At the\ncore of Stormer is a randomized forecasting objective that trains the model to\nforecast the weather dynamics over varying time intervals. During inference,\nthis allows us to produce multiple forecasts for a target lead time and combine\nthem to obtain better forecast accuracy. On WeatherBench 2, Stormer performs\ncompetitively at short to medium-range forecasts and outperforms current\nmethods beyond 7 days, while requiring orders-of-magnitude less training data\nand compute. Additionally, we demonstrate Stormer's favorable scaling\nproperties, showing consistent improvements in forecast accuracy with increases\nin model size and training tokens. Code and checkpoints will be made publicly\navailable.",
            "author": [
                "Tung Nguyen",
                "Rohan Shah",
                "Hritik Bansal",
                "Troy Arcomano",
                "Sandeep Madireddy",
                "Romit Maulik",
                "Veerabhadra Kotamarthi",
                "Ian Foster",
                "Aditya Grover"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03876v1",
                "http://arxiv.org/pdf/2312.03876v1"
            ],
            "primary_category": "physics.ao-ph",
            "category": [
                "physics.ao-ph",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03875v1",
            "title": "Data-driven state-space and Koopman operator models of coherent state\n  dynamics on invariant manifolds",
            "updated": "2023-12-06T19:46:00Z",
            "published": "2023-12-06T19:46:00Z",
            "summary": "The accurate simulation of complex dynamics in fluid flows demands a\nsubstantial number of degrees of freedom, i.e. a high-dimensional state space.\nNevertheless, the swift attenuation of small-scale perturbations due to viscous\ndiffusion permits in principle the representation of these flows using a\nsignificantly reduced dimensionality. Over time, the dynamics of such flows\nevolve towards a finite-dimensional invariant manifold. Using only data from\ndirect numerical simulations, in the present work we identify the manifold and\ndetermine evolution equations for the dynamics on it. We use an advanced\nautoencoder framework to automatically estimate the intrinsic dimension of the\nmanifold and provide an orthogonal coordinate system. Then, we learn the\ndynamics by determining an equation on the manifold by using both a function\nspace approach (approximating the Koopman operator) and a state space approach\n(approximating the vector field on the manifold). We apply this method to exact\ncoherent states for Kolmogorov flow and minimal flow unit pipe flow. Fully\nresolved simulations for these cases require O(103) and O(105) degrees of\nfreedom respectively, and we build models with two or three degrees of freedom\nthat faithfully capture the dynamics of these flows. For these examples, both\nthe state space and function space time evaluations provide highly accurate\npredictions of the long-time dynamics in manifold coordinates.",
            "author": [
                "C. Ricardo Constante-Amores",
                "Michael D. Graham"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03875v1",
                "http://arxiv.org/pdf/2312.03875v1"
            ],
            "primary_category": "physics.flu-dyn",
            "category": [
                "physics.flu-dyn"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03873v1",
            "title": "Optimizing $CO_{2}$ Capture in Pressure Swing Adsorption Units: A Deep\n  Neural Network Approach with Optimality Evaluation and Operating Maps for\n  Decision-Making",
            "updated": "2023-12-06T19:43:37Z",
            "published": "2023-12-06T19:43:37Z",
            "summary": "This study presents a methodology for surrogate optimization of cyclic\nadsorption processes, focusing on enhancing Pressure Swing Adsorption units for\ncarbon dioxide ($CO_{2}$) capture. We developed and implemented a\nmultiple-input, single-output (MISO) framework comprising two deep neural\nnetwork (DNN) models, predicting key process performance indicators. These\nmodels were then integrated into an optimization framework, leveraging particle\nswarm optimization (PSO) and statistical analysis to generate a comprehensive\nPareto front representation. This approach delineated feasible operational\nregions (FORs) and highlighted the spectrum of optimal decision-making\nscenarios. A key aspect of our methodology was the evaluation of optimization\neffectiveness. This was accomplished by testing decision variables derived from\nthe Pareto front against a phenomenological model, affirming the surrogate\nmodels reliability. Subsequently, the study delved into analyzing the feasible\noperational domains of these decision variables. A detailed correlation map was\nconstructed to elucidate the interplay between these variables, thereby\nuncovering the most impactful factors influencing process behavior. The study\noffers a practical, insightful operational map that aids operators in\npinpointing the optimal process location and prioritizing specific operational\ngoals.",
            "author": [
                "Carine Menezes Rebello",
                "Idelfonso B. R. Nogueira"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03873v1",
                "http://arxiv.org/pdf/2312.03873v1"
            ],
            "primary_category": "physics.chem-ph",
            "category": [
                "physics.chem-ph",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03872v1",
            "title": "The BigCode Project Governance Card",
            "updated": "2023-12-06T19:37:08Z",
            "published": "2023-12-06T19:37:08Z",
            "summary": "This document serves as an overview of the different mechanisms and areas of\ngovernance in the BigCode project. It aims to support transparency by providing\nrelevant information about choices that were made during the project to the\nbroader public, and to serve as an example of intentional governance of an open\nresearch project that future endeavors can leverage to shape their own\napproach. The first section, Project Structure, covers the project\norganization, its stated goals and values, its internal decision processes, and\nits funding and resources. The second section, Data and Model Governance,\ncovers decisions relating to the questions of data subject consent, privacy,\nand model release.",
            "author": [
                "BigCode collaboration",
                "Sean Hughes",
                "Harm de Vries",
                "Jennifer Robinson",
                "Carlos Mu\u00f1oz Ferrandis",
                "Loubna Ben Allal",
                "Leandro von Werra",
                "Jennifer Ding",
                "Sebastien Paquet",
                "Yacine Jernite"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03872v1",
                "http://arxiv.org/pdf/2312.03872v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY",
                "cs.AI",
                "cs.CL",
                "cs.LG",
                "cs.PL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03871v1",
            "title": "Hidden yet quantifiable: A lower bound for confounding strength using\n  randomized trials",
            "updated": "2023-12-06T19:33:34Z",
            "published": "2023-12-06T19:33:34Z",
            "summary": "In the era of fast-paced precision medicine, observational studies play a\nmajor role in properly evaluating new treatments in clinical practice. Yet,\nunobserved confounding can significantly compromise causal conclusions drawn\nfrom non-randomized data. We propose a novel strategy that leverages randomized\ntrials to quantify unobserved confounding. First, we design a statistical test\nto detect unobserved confounding with strength above a given threshold. Then,\nwe use the test to estimate an asymptotically valid lower bound on the\nunobserved confounding strength. We evaluate the power and validity of our\nstatistical test on several synthetic and semi-synthetic datasets. Further, we\nshow how our lower bound can correctly identify the absence and presence of\nunobserved confounding in a real-world setting.",
            "author": [
                "Piersilvio De Bartolomeis",
                "Javier Abad",
                "Konstantin Donhauser",
                "Fanny Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03871v1",
                "http://arxiv.org/pdf/2312.03871v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03869v1",
            "title": "Inpaint3D: 3D Scene Content Generation using 2D Inpainting Diffusion",
            "updated": "2023-12-06T19:30:04Z",
            "published": "2023-12-06T19:30:04Z",
            "summary": "This paper presents a novel approach to inpainting 3D regions of a scene,\ngiven masked multi-view images, by distilling a 2D diffusion model into a\nlearned 3D scene representation (e.g. a NeRF). Unlike 3D generative methods\nthat explicitly condition the diffusion model on camera pose or multi-view\ninformation, our diffusion model is conditioned only on a single masked 2D\nimage. Nevertheless, we show that this 2D diffusion model can still serve as a\ngenerative prior in a 3D multi-view reconstruction problem where we optimize a\nNeRF using a combination of score distillation sampling and NeRF reconstruction\nlosses. Predicted depth is used as additional supervision to encourage accurate\ngeometry. We compare our approach to 3D inpainting methods that focus on object\nremoval. Because our method can generate content to fill any 3D masked region,\nwe additionally demonstrate 3D object completion, 3D object replacement, and 3D\nscene completion.",
            "author": [
                "Kira Prabhu",
                "Jane Wu",
                "Lynn Tsai",
                "Peter Hedman",
                "Dan B Goldman",
                "Ben Poole",
                "Michael Broxton"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03869v1",
                "http://arxiv.org/pdf/2312.03869v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03867v1",
            "title": "Multi-Group Fairness Evaluation via Conditional Value-at-Risk Testing",
            "updated": "2023-12-06T19:25:32Z",
            "published": "2023-12-06T19:25:32Z",
            "summary": "Machine learning (ML) models used in prediction and classification tasks may\ndisplay performance disparities across population groups determined by\nsensitive attributes (e.g., race, sex, age). We consider the problem of\nevaluating the performance of a fixed ML model across population groups defined\nby multiple sensitive attributes (e.g., race and sex and age). Here, the sample\ncomplexity for estimating the worst-case performance gap across groups (e.g.,\nthe largest difference in error rates) increases exponentially with the number\nof group-denoting sensitive attributes. To address this issue, we propose an\napproach to test for performance disparities based on Conditional Value-at-Risk\n(CVaR). By allowing a small probabilistic slack on the groups over which a\nmodel has approximately equal performance, we show that the sample complexity\nrequired for discovering performance violations is reduced exponentially to be\nat most upper bounded by the square root of the number of groups. As a\nbyproduct of our analysis, when the groups are weighted by a specific prior\ndistribution, we show that R\\'enyi entropy of order $2/3$ of the prior\ndistribution captures the sample complexity of the proposed CVaR test\nalgorithm. Finally, we also show that there exists a non-i.i.d. data collection\nstrategy that results in a sample complexity independent of the number of\ngroups.",
            "author": [
                "Lucas Monteiro Paes",
                "Ananda Theertha Suresh",
                "Alex Beutel",
                "Flavio P. Calmon",
                "Ahmad Beirami"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03867v1",
                "http://arxiv.org/pdf/2312.03867v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CY",
                "cs.IT",
                "math.IT",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03865v1",
            "title": "Learning Genomic Sequence Representations using Graph Neural Networks\n  over De Bruijn Graphs",
            "updated": "2023-12-06T19:23:53Z",
            "published": "2023-12-06T19:23:53Z",
            "summary": "The rapid expansion of genomic sequence data calls for new methods to achieve\nrobust sequence representations. Existing techniques often neglect intricate\nstructural details, emphasizing mainly contextual information. To address this,\nwe developed k-mer embeddings that merge contextual and structural string\ninformation by enhancing De Bruijn graphs with structural similarity\nconnections. Subsequently, we crafted a self-supervised method based on\nContrastive Learning that employs a heterogeneous Graph Convolutional Network\nencoder and constructs positive pairs based on node similarities. Our\nembeddings consistently outperform prior techniques for Edit Distance\nApproximation and Closest String Retrieval tasks.",
            "author": [
                "Kacper Kapu\u015bniak",
                "Manuel Burger",
                "Gunnar R\u00e4tsch",
                "Amir Joudaki"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03865v1",
                "http://arxiv.org/pdf/2312.03865v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "q-bio.GN"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03864v1",
            "title": "Geometry Matching for Multi-Embodiment Grasping",
            "updated": "2023-12-06T19:20:01Z",
            "published": "2023-12-06T19:20:01Z",
            "summary": "Many existing learning-based grasping approaches concentrate on a single\nembodiment, provide limited generalization to higher DoF end-effectors and\ncannot capture a diverse set of grasp modes. We tackle the problem of grasping\nusing multiple embodiments by learning rich geometric representations for both\nobjects and end-effectors using Graph Neural Networks. Our novel method -\nGeoMatch - applies supervised learning on grasping data from multiple\nembodiments, learning end-to-end contact point likelihood maps as well as\nconditional autoregressive predictions of grasps keypoint-by-keypoint. We\ncompare our method against baselines that support multiple embodiments. Our\napproach performs better across three end-effectors, while also producing\ndiverse grasps. Examples, including real robot demos, can be found at\ngeo-match.github.io.",
            "author": [
                "Maria Attarian",
                "Muhammad Adil Asif",
                "Jingzhou Liu",
                "Ruthrash Hari",
                "Animesh Garg",
                "Igor Gilitschenski",
                "Jonathan Tompson"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03864v1",
                "http://arxiv.org/pdf/2312.03864v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03862v1",
            "title": "An inductive bias from quantum mechanics: learning order effects with\n  non-commuting measurements",
            "updated": "2023-12-06T19:18:33Z",
            "published": "2023-12-06T19:18:33Z",
            "summary": "There are two major approaches to building good machine learning algorithms:\nfeeding lots of data into large models, or picking a model class with an\n''inductive bias'' that suits the structure of the data. When taking the second\napproach as a starting point to design quantum algorithms for machine learning,\nit is important to understand how mathematical structures in quantum mechanics\ncan lead to useful inductive biases in quantum models. In this work, we bring a\ncollection of theoretical evidence from the Quantum Cognition literature to the\nfield of Quantum Machine Learning to investigate how non-commutativity of\nquantum observables can help to learn data with ''order effects'', such as the\nchanges in human answering patterns when swapping the order of questions in a\nsurvey. We design a multi-task learning setting in which a generative quantum\nmodel consisting of sequential learnable measurements can be adapted to a given\ntask -- or question order -- by changing the order of observables, and we\nprovide artificial datasets inspired by human psychology to carry out our\ninvestigation. Our first experimental simulations show that in some cases the\nquantum model learns more non-commutativity as the amount of order effect\npresent in the data is increased, and that the quantum model can learn to\ngenerate better samples for unseen question orders when trained on others -\nboth signs that the model architecture suits the task.",
            "author": [
                "Kaitlin Gili",
                "Guillermo Alonso",
                "Maria Schuld"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03862v1",
                "http://arxiv.org/pdf/2312.03862v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03853v1",
            "title": "Dr. Jekyll and Mr. Hyde: Two Faces of LLMs",
            "updated": "2023-12-06T19:07:38Z",
            "published": "2023-12-06T19:07:38Z",
            "summary": "This year, we witnessed a rise in the use of Large Language Models,\nespecially when combined with applications like chatbot assistants. Safety\nmechanisms and specialized training procedures are put in place to prevent\nimproper responses from these assistants. In this work, we bypass these\nmeasures for ChatGPT and Bard (and, to some extent, Bing chat) by making them\nimpersonate complex personas with opposite characteristics as those of the\ntruthful assistants they are supposed to be. We start by creating elaborate\nbiographies of these personas, which we then use in a new session with the same\nchatbots. Our conversation followed a role-play style to get the response the\nassistant was not allowed to provide. By making use of personas, we show that\nthe response that is prohibited is actually provided, making it possible to\nobtain unauthorized, illegal, or harmful information. This work shows that by\nusing adversarial personas, one can overcome safety mechanisms set out by\nChatGPT and Bard. It also introduces several ways of activating such\nadversarial personas, altogether showing that both chatbots are vulnerable to\nthis kind of attack.",
            "author": [
                "Matteo Gioele Collu",
                "Tom Janssen-Groesbeek",
                "Stefanos Koffas",
                "Mauro Conti",
                "Stjepan Picek"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03853v1",
                "http://arxiv.org/pdf/2312.03853v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03850v1",
            "title": "Learning the Dynamics of Future Marine Microgrids Using Temporal\n  Convolutional Neural Network",
            "updated": "2023-12-06T19:03:09Z",
            "published": "2023-12-06T19:03:09Z",
            "summary": "Medium-voltage direct-current (MVDC) ship-board microgrids (SMGs) are the\nstate-of-the-art architecture for onboard power distribution in navy. These\nsystems are considered to be highly dynamic due to high penetration of power\nelectronic converters and volatile load patterns such as pulsed-power load\n(PPL) and propulsion motors demand variation. Obtaining the dynamic model of an\nMVDC SMG is a challenging task due to the confidentiality of system components\nmodels and uncertainty in the dynamic models through time. In this paper, a\ndynamic identification framework based on a temporal convolutional neural\nnetwork (TCN) is developed to learn the system dynamics from measurement data.\nDifferent kinds of testing scenarios are implemented, and the testing results\nshow that this approach achieves an exceptional performance and high\ngeneralization ability, thus holding substantial promise for development of\nadvanced data-driven control strategies and stability prediction of the system.",
            "author": [
                "Xiaoyu Ge",
                "Ali Hosseinipour",
                "Saskia Putri",
                "Faegheh Moazeni",
                "Javad Khazaei"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03850v1",
                "http://arxiv.org/pdf/2312.03850v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03849v1",
            "title": "LEGO: Learning EGOcentric Action Frame Generation via Visual Instruction\n  Tuning",
            "updated": "2023-12-06T19:02:40Z",
            "published": "2023-12-06T19:02:40Z",
            "summary": "Generating instructional images of human daily actions from an egocentric\nviewpoint serves a key step towards efficient skill transfer. In this paper, we\nintroduce a novel problem -- egocentric action frame generation. The goal is to\nsynthesize the action frame conditioning on the user prompt question and an\ninput egocentric image that captures user's environment. Notably, existing\negocentric datasets lack the detailed annotations that describe the execution\nof actions. Additionally, the diffusion-based image manipulation models fail to\ncontrol the state change of an action within the corresponding egocentric image\npixel space. To this end, we finetune a visual large language model (VLLM) via\nvisual instruction tuning for curating the enriched action descriptions to\naddress our proposed problem. Moreover, we propose to Learn EGOcentric (LEGO)\naction frame generation using image and text embeddings from VLLM as additional\nconditioning. We validate our proposed model on two egocentric datasets --\nEgo4D and Epic-Kitchens. Our experiments show prominent improvement over prior\nimage manipulation models in both quantitative and qualitative evaluation. We\nalso conduct detailed ablation studies and analysis to provide insights on our\nmethod.",
            "author": [
                "Bolin Lai",
                "Xiaoliang Dai",
                "Lawrence Chen",
                "Guan Pang",
                "James M. Rehg",
                "Miao Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03849v1",
                "http://arxiv.org/pdf/2312.03849v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03843v1",
            "title": "Exposing Disparities in Flood Adaptation for Equitable Future\n  Interventions",
            "updated": "2023-12-06T19:00:12Z",
            "published": "2023-12-06T19:00:12Z",
            "summary": "As governments race to implement new climate adaptation policies that prepare\nfor more frequent flooding, they must seek policies that are effective for all\ncommunities and uphold climate justice. This requires evaluating policies not\nonly on their overall effectiveness but also on whether their benefits are felt\nacross all communities. We illustrate the importance of considering such\ndisparities for flood adaptation using the FEMA National Flood Insurance\nProgram Community Rating System and its dataset of $\\sim$2.5 million flood\ninsurance claims. We use ${\\rm C{\\scriptsize AUSAL}F{\\scriptsize LOW}}$, a\ncausal inference method based on deep generative models, to estimate the\ntreatment effect of flood adaptation interventions based on a community's\nincome, diversity, population, flood risk, educational attainment, and\nprecipitation. We find that the program saves communities \\$5,000--15,000 per\nhousehold. However, these savings are not evenly spread across communities. For\nexample, for low-income communities savings sharply decline as flood-risk\nincreases in contrast to their high-income counterparts with all else equal.\nEven among low-income communities, there is a gap in savings between\npredominantly white and non-white communities: savings of predominantly white\ncommunities can be higher by more than \\$6000 per household. As communities\nworldwide ramp up efforts to reduce losses inflicted by floods, simply\nprescribing a series flood adaptation measures is not enough. Programs must\nprovide communities with the necessary technical and economic support to\ncompensate for historical patterns of disenfranchisement, racism, and\ninequality. Future flood adaptation efforts should go beyond reducing losses\noverall and aim to close existing gaps to equitably support communities in the\nrace for climate adaptation.",
            "author": [
                "Lidia Cano Pecharroman",
                "ChangHoon Hahn"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03843v1",
                "http://arxiv.org/pdf/2312.03843v1"
            ],
            "primary_category": "econ.GN",
            "category": [
                "econ.GN",
                "cs.LG",
                "q-fin.EC",
                "stat.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03823v1",
            "title": "High Pileup Particle Tracking with Object Condensation",
            "updated": "2023-12-06T19:00:00Z",
            "published": "2023-12-06T19:00:00Z",
            "summary": "Recent work has demonstrated that graph neural networks (GNNs) can match the\nperformance of traditional algorithms for charged particle tracking while\nimproving scalability to meet the computing challenges posed by the HL-LHC.\nMost GNN tracking algorithms are based on edge classification and identify\ntracks as connected components from an initial graph containing spurious\nconnections. In this talk, we consider an alternative based on object\ncondensation (OC), a multi-objective learning framework designed to cluster\npoints (hits) belonging to an arbitrary number of objects (tracks) and regress\nthe properties of each object. Building on our previous results, we present a\nstreamlined model and show progress toward a one-shot OC tracking algorithm in\na high-pileup environment.",
            "author": [
                "Kilian Lieret",
                "Gage DeZoort",
                "Devdoot Chatterjee",
                "Jian Park",
                "Siqi Miao",
                "Pan Li"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03823v1",
                "http://arxiv.org/pdf/2312.03823v1"
            ],
            "primary_category": "physics.data-an",
            "category": [
                "physics.data-an",
                "cs.LG",
                "hep-ex"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03824v1",
            "title": "nbi: the Astronomer's Package for Neural Posterior Estimation",
            "updated": "2023-12-06T19:00:00Z",
            "published": "2023-12-06T19:00:00Z",
            "summary": "Despite the promise of Neural Posterior Estimation (NPE) methods in\nastronomy, the adaptation of NPE into the routine inference workflow has been\nslow. We identify three critical issues: the need for custom featurizer\nnetworks tailored to the observed data, the inference inexactness, and the\nunder-specification of physical forward models. To address the first two\nissues, we introduce a new framework and open-source software nbi (Neural\nBayesian Inference), which supports both amortized and sequential NPE. First,\nnbi provides built-in \"featurizer\" networks with demonstrated efficacy on\nsequential data, such as light curve and spectra, thus obviating the need for\nthis customization on the user end. Second, we introduce a modified algorithm\nSNPE-IS, which facilities asymptotically exact inference by using the surrogate\nposterior under NPE only as a proposal distribution for importance sampling.\nThese features allow nbi to be applied off-the-shelf to astronomical inference\nproblems involving light curves and spectra. We discuss how nbi may serve as an\neffective alternative to existing methods such as Nested Sampling. Our package\nis at https://github.com/kmzzhang/nbi.",
            "author": [
                "Keming Zhang",
                "Joshua Bloom",
                "St\u00e9fan van der Walt",
                "Nina Hernitschek"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03824v1",
                "http://arxiv.org/pdf/2312.03824v1"
            ],
            "primary_category": "astro-ph.IM",
            "category": [
                "astro-ph.IM",
                "cs.LG",
                "physics.data-an"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03703v1",
            "title": "Skeleton-in-Context: Unified Skeleton Sequence Modeling with In-Context\n  Learning",
            "updated": "2023-12-06T18:59:44Z",
            "published": "2023-12-06T18:59:44Z",
            "summary": "In-context learning provides a new perspective for multi-task modeling for\nvision and NLP. Under this setting, the model can perceive tasks from prompts\nand accomplish them without any extra task-specific head predictions or model\nfine-tuning. However, Skeleton sequence modeling via in-context learning\nremains unexplored. Directly applying existing in-context models from other\nareas onto skeleton sequences fails due to the inter-frame and cross-task pose\nsimilarity that makes it outstandingly hard to perceive the task correctly from\na subtle context. To address this challenge, we propose Skeleton-in-Context\n(SiC), an effective framework for in-context skeleton sequence modeling. Our\nSiC is able to handle multiple skeleton-based tasks simultaneously after a\nsingle training process and accomplish each task from context according to the\ngiven prompt. It can further generalize to new, unseen tasks according to\ncustomized prompts. To facilitate context perception, we additionally propose a\ntask-unified prompt, which adaptively learns tasks of different natures, such\nas partial joint-level generation, sequence-level prediction, or 2D-to-3D\nmotion prediction. We conduct extensive experiments to evaluate the\neffectiveness of our SiC on multiple tasks, including motion prediction, pose\nestimation, joint completion, and future pose estimation. We also evaluate its\ngeneralization capability on unseen tasks such as motion-in-between. These\nexperiments show that our model achieves state-of-the-art multi-task\nperformance and even outperforms single-task methods on certain tasks.",
            "author": [
                "Xinshun Wang",
                "Zhongbin Fang",
                "Xia Li",
                "Xiangtai Li",
                "Chen Chen",
                "Mengyuan Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03703v1",
                "http://arxiv.org/pdf/2312.03703v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03818v1",
            "title": "Alpha-CLIP: A CLIP Model Focusing on Wherever You Want",
            "updated": "2023-12-06T18:59:30Z",
            "published": "2023-12-06T18:59:30Z",
            "summary": "Contrastive Language-Image Pre-training (CLIP) plays an essential role in\nextracting valuable content information from images across diverse tasks. It\naligns textual and visual modalities to comprehend the entire image, including\nall the details, even those irrelevant to specific tasks. However, for a finer\nunderstanding and controlled editing of images, it becomes crucial to focus on\nspecific regions of interest, which can be indicated as points, masks, or boxes\nby humans or perception models. To fulfill the requirements, we introduce\nAlpha-CLIP, an enhanced version of CLIP with an auxiliary alpha channel to\nsuggest attentive regions and fine-tuned with constructed millions of RGBA\nregion-text pairs. Alpha-CLIP not only preserves the visual recognition ability\nof CLIP but also enables precise control over the emphasis of image contents.\nIt demonstrates effectiveness in various tasks, including but not limited to\nopen-world recognition, multimodal large language models, and conditional 2D /\n3D generation. It has a strong potential to serve as a versatile tool for\nimage-related tasks.",
            "author": [
                "Zeyi Sun",
                "Ye Fang",
                "Tong Wu",
                "Pan Zhang",
                "Yuhang Zang",
                "Shu Kong",
                "Yuanjun Xiong",
                "Dahua Lin",
                "Jiaqi Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03818v1",
                "http://arxiv.org/pdf/2312.03818v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03700v1",
            "title": "OneLLM: One Framework to Align All Modalities with Language",
            "updated": "2023-12-06T18:59:19Z",
            "published": "2023-12-06T18:59:19Z",
            "summary": "Multimodal large language models (MLLMs) have gained significant attention\ndue to their strong multimodal understanding capability. However, existing\nworks rely heavily on modality-specific encoders, which usually differ in\narchitecture and are limited to common modalities. In this paper, we present\nOneLLM, an MLLM that aligns eight modalities to language using a unified\nframework. We achieve this through a unified multimodal encoder and a\nprogressive multimodal alignment pipeline. In detail, we first train an image\nprojection module to connect a vision encoder with LLM. Then, we build a\nuniversal projection module (UPM) by mixing multiple image projection modules\nand dynamic routing. Finally, we progressively align more modalities to LLM\nwith the UPM. To fully leverage the potential of OneLLM in following\ninstructions, we also curated a comprehensive multimodal instruction dataset,\nincluding 2M items from image, audio, video, point cloud, depth/normal map, IMU\nand fMRI brain activity. OneLLM is evaluated on 25 diverse benchmarks,\nencompassing tasks such as multimodal captioning, question answering and\nreasoning, where it delivers excellent performance. Code, data, model and\nonline demo are available at https://github.com/csuhan/OneLLM",
            "author": [
                "Jiaming Han",
                "Kaixiong Gong",
                "Yiyuan Zhang",
                "Jiaqi Wang",
                "Kaipeng Zhang",
                "Dahua Lin",
                "Yu Qiao",
                "Peng Gao",
                "Xiangyu Yue"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03700v1",
                "http://arxiv.org/pdf/2312.03700v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.CL",
                "cs.LG",
                "cs.MM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03696v1",
            "title": "Efficient Learning in Polyhedral Games via Best Response Oracles",
            "updated": "2023-12-06T18:57:53Z",
            "published": "2023-12-06T18:57:53Z",
            "summary": "We study online learning and equilibrium computation in games with polyhedral\ndecision sets, a property shared by both normal-form games and extensive-form\ngames (EFGs), when the learning agent is restricted to using a best-response\noracle. We show how to achieve constant regret in zero-sum games and\n$O(T^{1/4})$ regret in general-sum games while using only $O(\\log t)$\nbest-response queries at a given iteration $t$, thus improving over the best\nprior result, which required $O(T)$ queries per iteration. Moreover, our\nframework yields the first last-iterate convergence guarantees for self-play\nwith best-response oracles in zero-sum games. This convergence occurs at a\nlinear rate, though with a condition-number dependence. We go on to show a\n$O(1/\\sqrt{T})$ best-iterate convergence rate without such a dependence. Our\nresults build on linear-rate convergence results for variants of the\nFrank-Wolfe (FW) algorithm for strongly convex and smooth minimization problems\nover polyhedral domains. These FW results depend on a condition number of the\npolytope, known as facial distance. In order to enable application to settings\nsuch as EFGs, we show two broad new results: 1) the facial distance for\npolytopes in standard form is at least $\\gamma/\\sqrt{k}$ where $\\gamma$ is the\nminimum value of a nonzero coordinate of a vertex of the polytope and $k\\leq n$\nis the number of tight inequality constraints in the optimal face, and 2) the\nfacial distance for polytopes of the form\n$\\mathbf{A}\\boldsymbol{x}=\\boldsymbol{b},\\mathbf{C}\\boldsymbol{x}\\leq\\boldsymbol{d},\n\\boldsymbol{x}\\geq \\mathbf{0}$ where $\\boldsymbol{x}\\in\\mathbb{R}^n$,\n$\\mathbf{C}\\geq\\boldsymbol{0}$ is a nonzero integral matrix, and\n$\\boldsymbol{d}\\geq \\boldsymbol{0}$, is at least\n$1/(\\|\\mathbf{C}\\|_\\infty\\sqrt{n})$. This yields the first such results for\nseveral problems such as sequence-form polytopes, flow polytopes, and matching\npolytopes.",
            "author": [
                "Darshan Chakrabarti",
                "Gabriele Farina",
                "Christian Kroer"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03696v1",
                "http://arxiv.org/pdf/2312.03696v1"
            ],
            "primary_category": "cs.GT",
            "category": [
                "cs.GT",
                "cs.MA",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03694v2",
            "title": "Parameter-Efficient Transfer Learning of Audio Spectrogram Transformers",
            "updated": "2023-12-07T08:13:58Z",
            "published": "2023-12-06T18:55:34Z",
            "summary": "The common modus operandi of fine-tuning large pre-trained Transformer models\nentails the adaptation of all their parameters (i.e., full fine-tuning). While\nachieving striking results on multiple tasks, this approach becomes unfeasible\nas the model size and the number of downstream tasks increase. In natural\nlanguage processing and computer vision, parameter-efficient approaches like\nprompt-tuning and adapters have emerged as solid alternatives by fine-tuning\nonly a small number of extra parameters, without sacrificing performance\naccuracy. Specifically, adapters, due to their flexibility, have recently\ngarnered significant attention, leading to several variants. For audio\nclassification tasks, the Audio Spectrogram Transformer model shows impressive\nresults. However, surprisingly, how to efficiently adapt it to several\ndownstream tasks has not been tackled before. In this paper, we bridge this gap\nand present a detailed investigation of common parameter-efficient methods,\nrevealing that adapters consistently outperform the other methods across four\nbenchmarks. This trend is also confirmed in few-shot learning settings and when\nthe total number of trainable parameters increases, demonstrating adapters\nsuperior scalability. We finally study the best adapter configuration, as well\nas the role of residual connections in the learning process.",
            "author": [
                "Umberto Cappellazzo",
                "Daniele Falavigna",
                "Alessio Brutti",
                "Mirco Ravanelli"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03694v2",
                "http://arxiv.org/pdf/2312.03694v2"
            ],
            "primary_category": "eess.AS",
            "category": [
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03692v1",
            "title": "Memory Triggers: Unveiling Memorization in Text-To-Image Generative\n  Models through Word-Level Duplication",
            "updated": "2023-12-06T18:54:44Z",
            "published": "2023-12-06T18:54:44Z",
            "summary": "Diffusion-based models, such as the Stable Diffusion model, have\nrevolutionized text-to-image synthesis with their ability to produce\nhigh-quality, high-resolution images. These advancements have prompted\nsignificant progress in image generation and editing tasks. However, these\nmodels also raise concerns due to their tendency to memorize and potentially\nreplicate exact training samples, posing privacy risks and enabling adversarial\nattacks. Duplication in training datasets is recognized as a major factor\ncontributing to memorization, and various forms of memorization have been\nstudied so far. This paper focuses on two distinct and underexplored types of\nduplication that lead to replication during inference in diffusion-based\nmodels, particularly in the Stable Diffusion model. We delve into these\nlesser-studied duplication phenomena and their implications through two case\nstudies, aiming to contribute to the safer and more responsible use of\ngenerative models in various applications.",
            "author": [
                "Ali Naseh",
                "Jaechul Roh",
                "Amir Houmansadr"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03692v1",
                "http://arxiv.org/pdf/2312.03692v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03691v1",
            "title": "On the Role of Edge Dependency in Graph Generative Models",
            "updated": "2023-12-06T18:54:27Z",
            "published": "2023-12-06T18:54:27Z",
            "summary": "In this work, we introduce a novel evaluation framework for generative models\nof graphs, emphasizing the importance of model-generated graph overlap\n(Chanpuriya et al., 2021) to ensure both accuracy and edge-diversity. We\ndelineate a hierarchy of graph generative models categorized into three levels\nof complexity: edge independent, node independent, and fully dependent models.\nThis hierarchy encapsulates a wide range of prevalent methods. We derive\ntheoretical bounds on the number of triangles and other short-length cycles\nproducible by each level of the hierarchy, contingent on the model overlap. We\nprovide instances demonstrating the asymptotic optimality of our bounds.\nFurthermore, we introduce new generative models for each of the three\nhierarchical levels, leveraging dense subgraph discovery (Gionis & Tsourakakis,\n2015). Our evaluation, conducted on real-world datasets, focuses on assessing\nthe output quality and overlap of our proposed models in comparison to other\npopular models. Our results indicate that our simple, interpretable models\nprovide competitive baselines to popular generative models. Through this\ninvestigation, we aim to propel the advancement of graph generative models by\noffering a structured framework and robust evaluation metrics, thereby\nfacilitating the development of models capable of generating accurate and\nedge-diverse graphs.",
            "author": [
                "Sudhanshu Chanpuriya",
                "Cameron Musco",
                "Konstantinos Sotiropoulos",
                "Charalampos Tsourakakis"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03691v1",
                "http://arxiv.org/pdf/2312.03691v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03690v1",
            "title": "Inverse Design of Vitrimeric Polymers by Molecular Dynamics and\n  Generative Modeling",
            "updated": "2023-12-06T18:53:45Z",
            "published": "2023-12-06T18:53:45Z",
            "summary": "Vitrimer is a new class of sustainable polymers with the ability of\nself-healing through rearrangement of dynamic covalent adaptive networks.\nHowever, a limited choice of constituent molecules restricts their property\nspace, prohibiting full realization of their potential applications. Through a\ncombination of molecular dynamics (MD) simulations and machine learning (ML),\nparticularly a novel graph variational autoencoder (VAE) model, we establish a\nmethod for generating novel vitrimers and guide their inverse design based on\ndesired glass transition temperature (Tg). We build the first vitrimer dataset\nof one million and calculate Tg on 8,424 of them by high-throughput MD\nsimulations calibrated by a Gaussian process model. The proposed VAE employs\ndual graph encoders and a latent dimension overlapping scheme which allows for\nindividual representation of multi-component vitrimers. By constructing a\ncontinuous latent space containing necessary information of vitrimers, we\ndemonstrate high accuracy and efficiency of our framework in discovering novel\nvitrimers with desirable Tg beyond the training regime. The proposed vitrimers\nwith reasonable synthesizability cover a wide range of Tg and broaden the\npotential widespread usage of vitrimeric materials.",
            "author": [
                "Yiwen Zheng",
                "Prakash Thakolkaran",
                "Jake A. Smith",
                "Ziheng Lu",
                "Shuxin Zheng",
                "Bichlien H. Nguyen",
                "Siddhant Kumar",
                "Aniruddh Vashisth"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03690v1",
                "http://arxiv.org/pdf/2312.03690v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03689v1",
            "title": "Evaluating and Mitigating Discrimination in Language Model Decisions",
            "updated": "2023-12-06T18:53:01Z",
            "published": "2023-12-06T18:53:01Z",
            "summary": "As language models (LMs) advance, interest is growing in applying them to\nhigh-stakes societal decisions, such as determining financing or housing\neligibility. However, their potential for discrimination in such contexts\nraises ethical concerns, motivating the need for better methods to evaluate\nthese risks. We present a method for proactively evaluating the potential\ndiscriminatory impact of LMs in a wide range of use cases, including\nhypothetical use cases where they have not yet been deployed. Specifically, we\nuse an LM to generate a wide array of potential prompts that decision-makers\nmay input into an LM, spanning 70 diverse decision scenarios across society,\nand systematically vary the demographic information in each prompt. Applying\nthis methodology reveals patterns of both positive and negative discrimination\nin the Claude 2.0 model in select settings when no interventions are applied.\nWhile we do not endorse or permit the use of language models to make automated\ndecisions for the high-risk use cases we study, we demonstrate techniques to\nsignificantly decrease both positive and negative discrimination through\ncareful prompt engineering, providing pathways toward safer deployment in use\ncases where they may be appropriate. Our work enables developers and\npolicymakers to anticipate, measure, and address discrimination as language\nmodel capabilities and applications continue to expand. We release our dataset\nand prompts at https://huggingface.co/datasets/Anthropic/discrim-eval",
            "author": [
                "Alex Tamkin",
                "Amanda Askell",
                "Liane Lovitt",
                "Esin Durmus",
                "Nicholas Joseph",
                "Shauna Kravec",
                "Karina Nguyen",
                "Jared Kaplan",
                "Deep Ganguli"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03689v1",
                "http://arxiv.org/pdf/2312.03689v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03815v1",
            "title": "LLM as OS (llmao), Agents as Apps: Envisioning AIOS, Agents and the\n  AIOS-Agent Ecosystem",
            "updated": "2023-12-06T18:50:26Z",
            "published": "2023-12-06T18:50:26Z",
            "summary": "This paper envisions a revolutionary AIOS-Agent ecosystem, where Large\nLanguage Model (LLM) serves as the (Artificial) Intelligent Operating System\n(IOS, or AIOS)--an operating system ``with soul''. Upon this foundation, a\ndiverse range of LLM-based AI Agent Applications (Agents, or AAPs) are\ndeveloped, enriching the AIOS-Agent ecosystem and signaling a paradigm shift\nfrom the traditional OS-APP ecosystem. We envision that LLM's impact will not\nbe limited to the AI application level, instead, it will in turn revolutionize\nthe design and implementation of computer system, architecture, software, and\nprogramming language, featured by several main concepts: LLM as OS\n(system-level), Agents as Applications (application-level), Natural Language as\nProgramming Interface (user-level), and Tools as Devices/Libraries\n(hardware/middleware-level).",
            "author": [
                "Yingqiang Ge",
                "Yujie Ren",
                "Wenyue Hua",
                "Shuyuan Xu",
                "Juntao Tan",
                "Yongfeng Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03815v1",
                "http://arxiv.org/pdf/2312.03815v1"
            ],
            "primary_category": "cs.OS",
            "category": [
                "cs.OS",
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03682v1",
            "title": "What Planning Problems Can A Relational Neural Network Solve?",
            "updated": "2023-12-06T18:47:28Z",
            "published": "2023-12-06T18:47:28Z",
            "summary": "Goal-conditioned policies are generally understood to be \"feed-forward\"\ncircuits, in the form of neural networks that map from the current state and\nthe goal specification to the next action to take. However, under what\ncircumstances such a policy can be learned and how efficient the policy will be\nare not well understood. In this paper, we present a circuit complexity\nanalysis for relational neural networks (such as graph neural networks and\ntransformers) representing policies for planning problems, by drawing\nconnections with serialized goal regression search (S-GRS). We show that there\nare three general classes of planning problems, in terms of the growth of\ncircuit width and depth as a function of the number of objects and planning\nhorizon, providing constructive proofs. We also illustrate the utility of this\nanalysis for designing neural networks for policy learning.",
            "author": [
                "Jiayuan Mao",
                "Tom\u00e1s Lozano-P\u00e9rez",
                "Joshua B. Tenenbaum",
                "Leslie Pack Kaelbling"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03682v1",
                "http://arxiv.org/pdf/2312.03682v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.NE",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03678v1",
            "title": "Hybrid Functional Maps for Crease-Aware Non-Isometric Shape Matching",
            "updated": "2023-12-06T18:41:01Z",
            "published": "2023-12-06T18:41:01Z",
            "summary": "Non-isometric shape correspondence remains a fundamental challenge in\ncomputer vision. Traditional methods using Laplace-Beltrami operator (LBO)\neigenmodes face limitations in characterizing high-frequency extrinsic shape\nchanges like bending and creases. We propose a novel approach of combining the\nnon-orthogonal extrinsic basis of eigenfunctions of the elastic thin-shell\nhessian with the intrinsic ones of the LBO, creating a hybrid spectral space in\nwhich we construct functional maps. To this end, we present a theoretical\nframework to effectively integrate non-orthogonal basis functions into\ndescriptor- and learning-based functional map methods. Our approach can be\nincorporated easily into existing functional map pipelines across varying\napplications and is able to handle complex deformations beyond isometries. We\nshow extensive evaluations across various supervised and unsupervised settings\nand demonstrate significant improvements. Notably, our approach achieves up to\n15% better mean geodesic error for non-isometric correspondence settings and up\nto 45% improvement in scenarios with topological noise.",
            "author": [
                "Lennart Bastian",
                "Yizheng Xie",
                "Nassir Navab",
                "Zorah L\u00e4hner"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03678v1",
                "http://arxiv.org/pdf/2312.03678v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03675v1",
            "title": "GeoShapley: A Game Theory Approach to Measuring Spatial Effects in\n  Machine Learning Models",
            "updated": "2023-12-06T18:39:29Z",
            "published": "2023-12-06T18:39:29Z",
            "summary": "This paper introduces GeoShapley, a game theory approach to measuring spatial\neffects in machine learning models. GeoShapley extends the Nobel Prize-winning\nShapley value framework in game theory by conceptualizing location as a player\nin a model prediction game, which enables the quantification of the importance\nof location and the synergies between location and other features in a model.\nGeoShapley is a model-agnostic approach and can be applied to statistical or\nblack-box machine learning models in various structures. The interpretation of\nGeoShapley is directly linked with spatially varying coefficient models for\nexplaining spatial effects and additive models for explaining non-spatial\neffects. Using simulated data, GeoShapley values are validated against known\ndata-generating processes and are used for cross-comparison of seven\nstatistical and machine learning models. An empirical example of house price\nmodeling is used to illustrate GeoShapley's utility and interpretation with\nreal world data. The method is available as an open-source Python package named\ngeoshapley.",
            "author": [
                "Ziqi Li"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03675v1",
                "http://arxiv.org/pdf/2312.03675v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03673v1",
            "title": "On the Role of the Action Space in Robot Manipulation Learning and\n  Sim-to-Real Transfer",
            "updated": "2023-12-06T18:38:05Z",
            "published": "2023-12-06T18:38:05Z",
            "summary": "We study the choice of action space in robot manipulation learning and\nsim-to-real transfer. We define metrics that assess the performance, and\nexamine the emerging properties in the different action spaces. We train over\n250 reinforcement learning~(RL) agents in simulated reaching and pushing tasks,\nusing 13 different control spaces. The choice of action spaces spans popular\nchoices in the literature as well as novel combinations of common design\ncharacteristics. We evaluate the training performance in simulation and the\ntransfer to a real-world environment. We identify good and bad characteristics\nof robotic action spaces and make recommendations for future designs. Our\nfindings have important implications for the design of RL algorithms for robot\nmanipulation tasks, and highlight the need for careful consideration of action\nspaces when training and transferring RL agents for real-world robotics.",
            "author": [
                "Elie Aljalbout",
                "Felix Frank",
                "Maximilian Karl",
                "Patrick van der Smagt"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03673v1",
                "http://arxiv.org/pdf/2312.03673v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03671v1",
            "title": "Direct Exoplanet Detection Using Deep Convolutional Image Reconstruction\n  (ConStruct): A New Algorithm for Post-Processing High-Contrast Images",
            "updated": "2023-12-06T18:36:03Z",
            "published": "2023-12-06T18:36:03Z",
            "summary": "We present a novel machine-learning approach for detecting faint point\nsources in high-contrast adaptive optics imaging datasets. The most widely used\nalgorithms for primary subtraction aim to decouple bright stellar speckle noise\nfrom planetary signatures by subtracting an approximation of the temporally\nevolving stellar noise from each frame in an imaging sequence. Our approach\naims to improve the stellar noise approximation and increase the planet\ndetection sensitivity by leveraging deep learning in a novel direct imaging\npost-processing algorithm. We show that a convolutional autoencoder neural\nnetwork, trained on an extensive reference library of real imaging sequences,\naccurately reconstructs the stellar speckle noise at the location of a\npotential planet signal. This tool is used in a post-processing algorithm we\ncall Direct Exoplanet Detection with Convolutional Image Reconstruction, or\nConStruct. The reliability and sensitivity of ConStruct are assessed using real\nKeck/NIRC2 angular differential imaging datasets. Of the 30 unique point\nsources we examine, ConStruct yields a higher S/N than traditional PCA-based\nprocessing for 67$\\%$ of the cases and improves the relative contrast by up to\na factor of 2.6. This work demonstrates the value and potential of deep\nlearning to take advantage of a diverse reference library of point spread\nfunction realizations to improve direct imaging post-processing. ConStruct and\nits future improvements may be particularly useful as tools for post-processing\nhigh-contrast images from the James Webb Space Telescope and extreme adaptive\noptics instruments, both for the current generation and those being designed\nfor the upcoming 30 meter-class telescopes.",
            "author": [
                "Trevor N. Wolf",
                "Brandon A. Jones",
                "Brendan P. Bowler"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03671v1",
                "http://arxiv.org/pdf/2312.03671v1"
            ],
            "primary_category": "astro-ph.IM",
            "category": [
                "astro-ph.IM",
                "astro-ph.EP",
                "cs.LG",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03668v1",
            "title": "An Integration of Pre-Trained Speech and Language Models for End-to-End\n  Speech Recognition",
            "updated": "2023-12-06T18:34:42Z",
            "published": "2023-12-06T18:34:42Z",
            "summary": "Advances in machine learning have made it possible to perform various text\nand speech processing tasks, including automatic speech recognition (ASR), in\nan end-to-end (E2E) manner. Since typical E2E approaches require large amounts\nof training data and resources, leveraging pre-trained foundation models\ninstead of training from scratch is gaining attention. Although there have been\nattempts to use pre-trained speech and language models in ASR, most of them are\nlimited to using either. This paper explores the potential of integrating a\npre-trained speech representation model with a large language model (LLM) for\nE2E ASR. The proposed model enables E2E ASR by generating text tokens in an\nautoregressive manner via speech representations as speech prompts, taking\nadvantage of the vast knowledge provided by the LLM. Furthermore, the proposed\nmodel can incorporate remarkable developments for LLM utilization, such as\ninference optimization and parameter-efficient domain adaptation. Experimental\nresults show that the proposed model achieves performance comparable to modern\nE2E ASR models.",
            "author": [
                "Yukiya Hono",
                "Koh Mitsuda",
                "Tianyu Zhao",
                "Kentaro Mitsui",
                "Toshiaki Wakatsuki",
                "Kei Sawada"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03668v1",
                "http://arxiv.org/pdf/2312.03668v1"
            ],
            "primary_category": "eess.AS",
            "category": [
                "eess.AS",
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03666v1",
            "title": "Towards small and accurate convolutional neural networks for acoustic\n  biodiversity monitoring",
            "updated": "2023-12-06T18:34:01Z",
            "published": "2023-12-06T18:34:01Z",
            "summary": "Automated classification of animal sounds is a prerequisite for large-scale\nmonitoring of biodiversity. Convolutional Neural Networks (CNNs) are among the\nmost promising algorithms but they are slow, often achieve poor classification\nin the field and typically require large training data sets. Our objective was\nto design CNNs that are fast at inference time and achieve good classification\nperformance while learning from moderate-sized data. Recordings from a\nrainforest ecosystem were used. Start and end-point of sounds from 20 bird\nspecies were manually annotated. Spectrograms from 10 second segments were used\nas CNN input. We designed simple CNNs with a frequency unwrapping layer\n(SIMP-FU models) such that any output unit was connected to all spectrogram\nfrequencies but only to a sub-region of time, the Receptive Field (RF). Our\nmodels allowed experimentation with different RF durations. Models either used\nthe time-indexed labels that encode start and end-point of sounds or simpler\nsegment-level labels. Models learning from time-indexed labels performed\nconsiderably better than their segment-level counterparts. Best classification\nperformances was achieved for models with intermediate RF duration of 1.5\nseconds. The best SIMP-FU models achieved AUCs over 0.95 in 18 of 20 classes on\nthe test set. On compact low-cost hardware the best SIMP-FU models evaluated up\nto seven times faster than real-time data acquisition. RF duration was a major\ndriver of classification performance. The optimum of 1.5 s was in the same\nrange as the duration of the sounds. Our models achieved good classification\nperformance while learning from moderate-sized training data. This is explained\nby the usage of time-indexed labels during training and adequately sized RF.\nResults confirm the feasibility of deploying small CNNs with good\nclassification performance on compact low-cost devices.",
            "author": [
                "Serge Zaugg",
                "Mike van der Schaar",
                "Florence Erbs",
                "Antonio Sanchez",
                "Joan V. Castell",
                "Emiliano Ramallo",
                "Michel Andr\u00e9"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03666v1",
                "http://arxiv.org/pdf/2312.03666v1"
            ],
            "primary_category": "cs.SD",
            "category": [
                "cs.SD",
                "cs.LG",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03814v1",
            "title": "Pearl: A Production-ready Reinforcement Learning Agent",
            "updated": "2023-12-06T18:29:23Z",
            "published": "2023-12-06T18:29:23Z",
            "summary": "Reinforcement Learning (RL) offers a versatile framework for achieving\nlong-term goals. Its generality allows us to formalize a wide range of problems\nthat real-world intelligent systems encounter, such as dealing with delayed\nrewards, handling partial observability, addressing the exploration and\nexploitation dilemma, utilizing offline data to improve online performance, and\nensuring safety constraints are met. Despite considerable progress made by the\nRL research community in addressing these issues, existing open-source RL\nlibraries tend to focus on a narrow portion of the RL solution pipeline,\nleaving other aspects largely unattended. This paper introduces Pearl, a\nProduction-ready RL agent software package explicitly designed to embrace these\nchallenges in a modular fashion. In addition to presenting preliminary\nbenchmark results, this paper highlights Pearl's industry adoptions to\ndemonstrate its readiness for production usage. Pearl is open sourced on Github\nat github.com/facebookresearch/pearl and its official website is located at\npearlagent.github.io.",
            "author": [
                "Zheqing Zhu",
                "Rodrigo de Salvo Braz",
                "Jalaj Bhandari",
                "Daniel Jiang",
                "Yi Wan",
                "Yonathan Efroni",
                "Liyuan Wang",
                "Ruiyang Xu",
                "Hongbo Guo",
                "Alex Nikulkov",
                "Dmytro Korenkevych",
                "Urun Dogan",
                "Frank Cheng",
                "Zheng Wu",
                "Wanqiao Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03814v1",
                "http://arxiv.org/pdf/2312.03814v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03813v1",
            "title": "Improving Activation Steering in Language Models with Mean-Centring",
            "updated": "2023-12-06T18:27:07Z",
            "published": "2023-12-06T18:27:07Z",
            "summary": "Recent work in activation steering has demonstrated the potential to better\ncontrol the outputs of Large Language Models (LLMs), but it involves finding\nsteering vectors. This is difficult because engineers do not typically know how\nfeatures are represented in these models. We seek to address this issue by\napplying the idea of mean-centring to steering vectors. We find that taking the\naverage of activations associated with a target dataset, and then subtracting\nthe mean of all training activations, results in effective steering vectors. We\ntest this method on a variety of models on natural language tasks by steering\naway from generating toxic text, and steering the completion of a story towards\na target genre. We also apply mean-centring to extract function vectors, more\neffectively triggering the execution of a range of natural language tasks by a\nsignificant margin (compared to previous baselines). This suggests that\nmean-centring can be used to easily improve the effectiveness of activation\nsteering in a wide range of contexts.",
            "author": [
                "Ole Jorgensen",
                "Dylan Cope",
                "Nandi Schoots",
                "Murray Shanahan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03813v1",
                "http://arxiv.org/pdf/2312.03813v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03656v1",
            "title": "Interpretability Illusions in the Generalization of Simplified Models",
            "updated": "2023-12-06T18:25:53Z",
            "published": "2023-12-06T18:25:53Z",
            "summary": "A common method to study deep learning systems is to use simplified model\nrepresentations -- for example, using singular value decomposition to visualize\nthe model's hidden states in a lower dimensional space. This approach assumes\nthat the results of these simplified are faithful to the original model. Here,\nwe illustrate an important caveat to this assumption: even if the simplified\nrepresentations can accurately approximate the full model on the training set,\nthey may fail to accurately capture the model's behavior out of distribution --\nthe understanding developed from simplified representations may be an illusion.\nWe illustrate this by training Transformer models on controlled datasets with\nsystematic generalization splits. First, we train models on the Dyck\nbalanced-parenthesis languages. We simplify these models using tools like\ndimensionality reduction and clustering, and then explicitly test how these\nsimplified proxies match the behavior of the original model on various\nout-of-distribution test sets. We find that the simplified proxies are\ngenerally less faithful out of distribution. In cases where the original model\ngeneralizes to novel structures or deeper depths, the simplified versions may\nfail, or generalize better. This finding holds even if the simplified\nrepresentations do not directly depend on the training distribution. Next, we\nstudy a more naturalistic task: predicting the next character in a dataset of\ncomputer code. We find similar generalization gaps between the original model\nand simplified proxies, and conduct further analysis to investigate which\naspects of the code completion task are associated with the largest gaps.\nTogether, our results raise questions about the extent to which mechanistic\ninterpretations derived using tools like SVD can reliably predict what a model\nwill do in novel situations.",
            "author": [
                "Dan Friedman",
                "Andrew Lampinen",
                "Lucas Dixon",
                "Danqi Chen",
                "Asma Ghandeharioun"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03656v1",
                "http://arxiv.org/pdf/2312.03656v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03812v1",
            "title": "Seeing the random forest through the decision trees. Supporting learning\n  health systems from histopathology with machine learning models: Challenges\n  and opportunities",
            "updated": "2023-12-06T18:24:21Z",
            "published": "2023-12-06T18:24:21Z",
            "summary": "This paper discusses some overlooked challenges faced when working with\nmachine learning models for histopathology and presents a novel opportunity to\nsupport \"Learning Health Systems\" with them. Initially, the authors elaborate\non these challenges after separating them according to their mitigation\nstrategies: those that need innovative approaches, time, or future\ntechnological capabilities and those that require a conceptual reappraisal from\na critical perspective. Then, a novel opportunity to support \"Learning Health\nSystems\" by integrating hidden information extracted by ML models from\ndigitalized histopathology slides with other healthcare big data is presented.",
            "author": [
                "Ricardo Gonzalez",
                "Ashirbani Saha",
                "Clinton J. V. Campbell",
                "Peyman Nejat",
                "Cynthia Lokker",
                "Andrew P. Norgan"
            ],
            "link": [
                "http://dx.doi.org/10.1016/j.jpi.2023.100347",
                "http://arxiv.org/abs/2312.03812v1",
                "http://arxiv.org/pdf/2312.03812v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03654v1",
            "title": "Efficient Inverse Design Optimization through Multi-fidelity\n  Simulations, Machine Learning, and Search Space Reduction Strategies",
            "updated": "2023-12-06T18:20:46Z",
            "published": "2023-12-06T18:20:46Z",
            "summary": "This paper introduces a methodology designed to augment the inverse design\noptimization process in scenarios constrained by limited compute, through the\nstrategic synergy of multi-fidelity evaluations, machine learning models, and\noptimization algorithms. The proposed methodology is analyzed on two distinct\nengineering inverse design problems: airfoil inverse design and the scalar\nfield reconstruction problem. It leverages a machine learning model trained\nwith low-fidelity simulation data, in each optimization cycle, thereby\nproficiently predicting a target variable and discerning whether a\nhigh-fidelity simulation is necessitated, which notably conserves computational\nresources. Additionally, the machine learning model is strategically deployed\nprior to optimization to reduce the search space, thereby further accelerating\nconvergence toward the optimal solution. The methodology has been employed to\nenhance two optimization algorithms, namely Differential Evolution and Particle\nSwarm Optimization. Comparative analyses illustrate performance improvements\nacross both algorithms. Notably, this method is adeptly adaptable across any\ninverse design application, facilitating a harmonious synergy between a\nrepresentative low-fidelity machine learning model, and high-fidelity\nsimulation, and can be seamlessly applied across any variety of\npopulation-based optimization algorithms.",
            "author": [
                "Luka Grbcic",
                "Juliane M\u00fcller",
                "Wibe Albert de Jong"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03654v1",
                "http://arxiv.org/pdf/2312.03654v1"
            ],
            "primary_category": "cs.CE",
            "category": [
                "cs.CE",
                "cs.AI",
                "cs.LG",
                "cs.NE",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03653v1",
            "title": "Quantum Picturalism: Learning Quantum Theory in High School",
            "updated": "2023-12-06T18:16:12Z",
            "published": "2023-12-06T18:16:12Z",
            "summary": "Quantum theory is often regarded as challenging to learn and teach, with\nadvanced mathematical prerequisites ranging from complex numbers and\nprobability theory to matrix multiplication, vector space algebra and symbolic\nmanipulation within the Hilbert space formalism. It is traditionally considered\nan advanced undergraduate or graduate-level subject.\n  In this work, we challenge the conventional view by proposing \"Quantum\nPicturalism\" as a new approach to teaching the fundamental concepts of quantum\ntheory and computation. We establish the foundations and methodology for an\nongoing educational experiment to investigate the question \"From what age can\nstudents learn quantum theory if taught using a diagrammatic approach?\". We\nanticipate that the primary benefit of leveraging such a diagrammatic approach,\nwhich is conceptually intuitive yet mathematically rigorous, will be\neliminating some of the most daunting barriers to teaching and learning this\nsubject while enabling young learners to reason proficiently about high-level\nproblems. We posit that transitioning from symbolic presentations to pictorial\nones will increase the appeal of STEM education, attracting more diverse\naudience.",
            "author": [
                "Selma D\u00fcndar-Coecke",
                "Lia Yeh",
                "Caterina Puca",
                "Sieglinde M. -L. Pfaendler",
                "Muhammad Hamza Waseem",
                "Thomas Cervoni",
                "Aleks Kissinger",
                "Stefano Gogioso",
                "Bob Coecke"
            ],
            "link": [
                "http://dx.doi.org/10.1109/QCE57702.2023.20321",
                "http://arxiv.org/abs/2312.03653v1",
                "http://arxiv.org/pdf/2312.03653v1"
            ],
            "primary_category": "physics.ed-ph",
            "category": [
                "physics.ed-ph",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03651v2",
            "title": "MIRACLE: Inverse Reinforcement and Curriculum Learning Model for\n  Human-inspired Mobile Robot Navigation",
            "updated": "2023-12-07T02:26:52Z",
            "published": "2023-12-06T18:13:21Z",
            "summary": "In emergency scenarios, mobile robots must navigate like humans, interpreting\nstimuli to locate potential victims rapidly without interfering with first\nresponders. Existing socially-aware navigation algorithms face computational\nand adaptability challenges. To overcome these, we propose a solution, MIRACLE\n-- an inverse reinforcement and curriculum learning model, that employs\ngamified learning to gather stimuli-driven human navigational data. This data\nis then used to train a Deep Inverse Maximum Entropy Reinforcement Learning\nmodel, reducing reliance on demonstrator abilities. Testing reveals a low loss\nof 2.7717 within a 400-sized environment, signifying human-like response\nreplication. Current databases lack comprehensive stimuli-driven data,\nnecessitating our approach. By doing so, we enable robots to navigate emergency\nsituations with human-like perception, enhancing their life-saving\ncapabilities.",
            "author": [
                "Nihal Gunukula",
                "Kshitij Tiwari",
                "Aniket Bera"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03651v2",
                "http://arxiv.org/pdf/2312.03651v2"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03647v1",
            "title": "Editable Stain Transformation Of Histological Images Using Unpaired GANs",
            "updated": "2023-12-06T18:05:41Z",
            "published": "2023-12-06T18:05:41Z",
            "summary": "Double staining in histopathology, particularly for metaplastic breast\ncancer, typically employs H&E and P63 dyes. However, P63's tissue damage and\nhigh cost necessitate alternative methods. This study introduces xAI-CycleGAN,\nan advanced architecture combining Mask CycleGAN with explainability features\nand structure-preserving capabilities for transforming H&E stained breast\ntissue images into P63-like images. The architecture allows for output editing,\nenhancing resemblance to actual images and enabling further model refinement.\nWe showcase xAI-CycleGAN's efficacy in maintaining structural integrity and\ngenerating high-quality images. Additionally, a histopathologist survey\nindicates the generated images' realism is often comparable to actual images,\nvalidating our model's high-quality output.",
            "author": [
                "Tibor Sloboda",
                "Luk\u00e1\u0161 Hudec",
                "Wanda Bene\u0161ov\u00e1"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03647v1",
                "http://arxiv.org/pdf/2312.03647v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03645v1",
            "title": "Fairness and Deception in Human Interactions with Artificial Agents",
            "updated": "2023-12-06T18:00:02Z",
            "published": "2023-12-06T18:00:02Z",
            "summary": "Online information ecosystems are now central to our everyday social\ninteractions. Of the many opportunities and challenges this presents, the\ncapacity for artificial agents to shape individual and collective human\ndecision-making in such environments is of particular importance. In order to\nassess and manage the impact of artificial agents on human well-being, we must\nconsider not only the technical capabilities of such agents, but the impact\nthey have on human social dynamics at the individual and population level. We\napproach this problem by modelling the potential for artificial agents to\n\"nudge\" attitudes to fairness and cooperation in populations of human agents,\nwho update their behavior according to a process of social learning. We show\nthat the presence of artificial agents in a population playing the ultimatum\ngame generates highly divergent, multi-stable outcomes in the learning dynamics\nof human agents' behaviour. These outcomes correspond to universal fairness\n(successful nudging), universal selfishness (failed nudging), and a strategy of\nfairness towards artificial agents and selfishness towards other human agents\n(unintended consequences of nudging). We then consider the consequences of\nhuman agents shifting their behavior when they are aware that they are\ninteracting with an artificial agent. We show that under a wide range of\ncircumstances artificial agents can achieve optimal outcomes in their\ninteractions with human agents while avoiding deception. However we also find\nthat, in the donation game, deception tends to make nudging easier to achieve.",
            "author": [
                "Theodor Cimpeanu",
                "Alexander J. Stewart"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03645v1",
                "http://arxiv.org/pdf/2312.03645v1"
            ],
            "primary_category": "q-bio.PE",
            "category": [
                "q-bio.PE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03644v1",
            "title": "MACCA: Offline Multi-agent Reinforcement Learning with Causal Credit\n  Assignment",
            "updated": "2023-12-06T17:59:34Z",
            "published": "2023-12-06T17:59:34Z",
            "summary": "Offline Multi-agent Reinforcement Learning (MARL) is valuable in scenarios\nwhere online interaction is impractical or risky. While independent learning in\nMARL offers flexibility and scalability, accurately assigning credit to\nindividual agents in offline settings poses challenges due to partial\nobservability and emergent behavior. Directly transferring the online credit\nassignment method to offline settings results in suboptimal outcomes due to the\nabsence of real-time feedback and intricate agent interactions. Our approach,\nMACCA, characterizing the generative process as a Dynamic Bayesian Network,\ncaptures relationships between environmental variables, states, actions, and\nrewards. Estimating this model on offline data, MACCA can learn each agent's\ncontribution by analyzing the causal relationship of their individual rewards,\nensuring accurate and interpretable credit assignment. Additionally, the\nmodularity of our approach allows it to seamlessly integrate with various\noffline MARL methods. Theoretically, we proved that under the setting of the\noffline dataset, the underlying causal structure and the function for\ngenerating the individual rewards of agents are identifiable, which laid the\nfoundation for the correctness of our modeling. Experimentally, we tested MACCA\nin two environments, including discrete and continuous action settings. The\nresults show that MACCA outperforms SOTA methods and improves performance upon\ntheir backbones.",
            "author": [
                "Ziyan Wang",
                "Yali Du",
                "Yudi Zhang",
                "Meng Fang",
                "Biwei Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03644v1",
                "http://arxiv.org/pdf/2312.03644v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.MA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03642v1",
            "title": "Transformer-Powered Surrogates Close the ICF Simulation-Experiment Gap\n  with Extremely Limited Data",
            "updated": "2023-12-06T17:53:06Z",
            "published": "2023-12-06T17:53:06Z",
            "summary": "Recent advances in machine learning, specifically transformer architecture,\nhave led to significant advancements in commercial domains. These powerful\nmodels have demonstrated superior capability to learn complex relationships and\noften generalize better to new data and problems. This paper presents a novel\ntransformer-powered approach for enhancing prediction accuracy in multi-modal\noutput scenarios, where sparse experimental data is supplemented with\nsimulation data. The proposed approach integrates transformer-based\narchitecture with a novel graph-based hyper-parameter optimization technique.\nThe resulting system not only effectively reduces simulation bias, but also\nachieves superior prediction accuracy compared to the prior method. We\ndemonstrate the efficacy of our approach on inertial confinement fusion\nexperiments, where only 10 shots of real-world data are available, as well as\nsynthetic versions of these experiments.",
            "author": [
                "Matthew L. Olson",
                "Shusen Liu",
                "Jayaraman J. Thiagarajan",
                "Bogdan Kustowski",
                "Weng-Keen Wong",
                "Rushil Anirudh"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03642v1",
                "http://arxiv.org/pdf/2312.03642v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03641v1",
            "title": "MotionCtrl: A Unified and Flexible Motion Controller for Video\n  Generation",
            "updated": "2023-12-06T17:49:57Z",
            "published": "2023-12-06T17:49:57Z",
            "summary": "Motions in a video primarily consist of camera motion, induced by camera\nmovement, and object motion, resulting from object movement. Accurate control\nof both camera and object motion is essential for video generation. However,\nexisting works either mainly focus on one type of motion or do not clearly\ndistinguish between the two, limiting their control capabilities and diversity.\nTherefore, this paper presents MotionCtrl, a unified and flexible motion\ncontroller for video generation designed to effectively and independently\ncontrol camera and object motion. The architecture and training strategy of\nMotionCtrl are carefully devised, taking into account the inherent properties\nof camera motion, object motion, and imperfect training data. Compared to\nprevious methods, MotionCtrl offers three main advantages: 1) It effectively\nand independently controls camera motion and object motion, enabling more\nfine-grained motion control and facilitating flexible and diverse combinations\nof both types of motion. 2) Its motion conditions are determined by camera\nposes and trajectories, which are appearance-free and minimally impact the\nappearance or shape of objects in generated videos. 3) It is a relatively\ngeneralizable model that can adapt to a wide array of camera poses and\ntrajectories once trained. Extensive qualitative and quantitative experiments\nhave been conducted to demonstrate the superiority of MotionCtrl over existing\nmethods.",
            "author": [
                "Zhouxia Wang",
                "Ziyang Yuan",
                "Xintao Wang",
                "Tianshui Chen",
                "Menghan Xia",
                "Ping Luo",
                "Ying Shan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03641v1",
                "http://arxiv.org/pdf/2312.03641v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG",
                "cs.MM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03637v1",
            "title": "A lithographically-defined quantum dot with sub-wavelength confinement\n  of light",
            "updated": "2023-12-06T17:34:01Z",
            "published": "2023-12-06T17:34:01Z",
            "summary": "We present an optical cavity with deep sub-wavelength confinement of light in\na region that simultaneously works as a quantum dot. The design is based on a\ndielectric membrane with a buried quantum well and restricts the electron and\nhole wave functions to the area of the optical hotspot in order to overcome the\nchallenge of co-locating an optical cavity with a quantum emitter. Combined\nwith proper surface passivation, this geometry points towards the deterministic\nfabrication of functional quantum dots in optical cavities by lithographic\nmeans.",
            "author": [
                "George Kountouris",
                "Anne Sofie Darket",
                "Lea Vestergaard",
                "Emil Vosmar Denning",
                "Jesper M\u00f8rk",
                "Philip Tr\u00f8st Kristensen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03637v1",
                "http://arxiv.org/pdf/2312.03637v1"
            ],
            "primary_category": "physics.optics",
            "category": [
                "physics.optics"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03636v1",
            "title": "Fed-urlBERT: Client-side Lightweight Federated Transformers for URL\n  Threat Analysis",
            "updated": "2023-12-06T17:31:16Z",
            "published": "2023-12-06T17:31:16Z",
            "summary": "In evolving cyber landscapes, the detection of malicious URLs calls for\ncooperation and knowledge sharing across domains. However, collaboration is\noften hindered by concerns over privacy and business sensitivities. Federated\nlearning addresses these issues by enabling multi-clients collaboration without\ndirect data exchange. Unfortunately, if highly expressive Transformer models\nare used, clients may face intolerable computational burdens, and the exchange\nof weights could quickly deplete network bandwidth. In this paper, we propose\nFed-urlBERT, a federated URL pre-trained model designed to address both privacy\nconcerns and the need for cross-domain collaboration in cybersecurity.\nFed-urlBERT leverages split learning to divide the pre-training model into\nclient and server part, so that the client part takes up less extensive\ncomputation resources and bandwidth. Our appraoch achieves performance\ncomparable to centralized model under both independently and identically\ndistributed (IID) and two non-IID data scenarios. Significantly, our federated\nmodel shows about an 7% decrease in the FPR compared to the centralized model.\nAdditionally, we implement an adaptive local aggregation strategy that\nmitigates heterogeneity among clients, demonstrating promising performance\nimprovements. Overall, our study validates the applicability of the proposed\nTransformer federated learning for URL threat analysis, establishing a\nfoundation for real-world collaborative cybersecurity efforts. The source code\nis accessible at https://github.com/Davidup1/FedURLBERT.",
            "author": [
                "Yujie Li",
                "Yanbin Wang",
                "Haitao Xu",
                "Zhenhao Guo",
                "Fan Zhang",
                "Ruitong Liu",
                "Wenrui Ma"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03636v1",
                "http://arxiv.org/pdf/2312.03636v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03633v1",
            "title": "Not All Large Language Models (LLMs) Succumb to the \"Reversal Curse\": A\n  Comparative Study of Deductive Logical Reasoning in BERT and GPT Models",
            "updated": "2023-12-06T17:29:45Z",
            "published": "2023-12-06T17:29:45Z",
            "summary": "The \"Reversal Curse\" refers to the scenario where auto-regressive decoder\nlarge language models (LLMs), such as ChatGPT, trained on \"A is B\" fail to\nlearn \"B is A\", demonstrating a basic failure of logical deduction. This raises\na red flag in the use of GPT models for certain general tasks such as\nconstructing knowledge graphs, considering their adherence to this symmetric\nprinciple. In our study, we examined a bidirectional LLM, BERT, and found that\nit is immune to the reversal curse. Driven by ongoing efforts to construct\nbiomedical knowledge graphs with LLMs, we also embarked on evaluating more\ncomplex but essential deductive reasoning capabilities. This process included\nfirst training encoder and decoder language models to master the intersection\n($\\cap$) and union ($\\cup$) operations on two sets and then moving on to assess\ntheir capability to infer different combinations of union ($\\cup$) and\nintersection ($\\cap$) operations on three newly created sets. The findings\nshowed that while both encoder and decoder language models, trained for tasks\ninvolving two sets (union/intersection), were proficient in such scenarios,\nthey encountered difficulties when dealing with operations that included three\nsets (various combinations of union and intersection). Our research highlights\nthe distinct characteristics of encoder and decoder models in simple and\ncomplex logical reasoning. In practice, the choice between BERT and GPT should\nbe guided by the specific requirements and nature of the task at hand,\nleveraging their respective strengths in bidirectional context comprehension\nand sequence prediction.",
            "author": [
                "Jingye Yang",
                "Da Wu",
                "Kai Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03633v1",
                "http://arxiv.org/pdf/2312.03633v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03632v1",
            "title": "Multimodal Data and Resource Efficient Device-Directed Speech Detection\n  with Large Foundation Models",
            "updated": "2023-12-06T17:29:03Z",
            "published": "2023-12-06T17:29:03Z",
            "summary": "Interactions with virtual assistants typically start with a trigger phrase\nfollowed by a command. In this work, we explore the possibility of making these\ninteractions more natural by eliminating the need for a trigger phrase. Our\ngoal is to determine whether a user addressed the virtual assistant based on\nsignals obtained from the streaming audio recorded by the device microphone. We\naddress this task by combining 1-best hypotheses and decoder signals from an\nautomatic speech recognition system with acoustic representations from an audio\nencoder as input features to a large language model (LLM). In particular, we\nare interested in data and resource efficient systems that require only a small\namount of training data and can operate in scenarios with only a single frozen\nLLM available on a device. For this reason, our model is trained on 80k or less\nexamples of multimodal data using a combination of low-rank adaptation and\nprefix tuning. We compare the proposed system to unimodal baselines and show\nthat the multimodal approach achieves lower equal-error-rates (EERs), while\nusing only a fraction of the training data. We also show that low-dimensional\nspecialized audio representations lead to lower EERs than high-dimensional\ngeneral audio representations.",
            "author": [
                "Dominik Wagner",
                "Alexander Churchill",
                "Siddharth Sigtia",
                "Panayiotis Georgiou",
                "Matt Mirsamadi",
                "Aarshee Mishra",
                "Erik Marchi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03632v1",
                "http://arxiv.org/pdf/2312.03632v1"
            ],
            "primary_category": "cs.SD",
            "category": [
                "cs.SD",
                "cs.LG",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03631v1",
            "title": "MOCHa: Multi-Objective Reinforcement Mitigating Caption Hallucinations",
            "updated": "2023-12-06T17:28:03Z",
            "published": "2023-12-06T17:28:03Z",
            "summary": "While recent years have seen rapid progress in image-conditioned text\ngeneration, image captioning still suffers from the fundamental issue of\nhallucinations, the generation of spurious details that cannot be inferred from\nthe given image. Dedicated methods for reducing hallucinations in image\ncaptioning largely focus on closed-vocabulary object tokens, ignoring most\ntypes of hallucinations that occur in practice. In this work, we propose MOCHa,\nan approach that harnesses advancements in reinforcement learning (RL) to\naddress the sequence-level nature of hallucinations in an open-world setup. To\noptimize for caption fidelity to the input image, we leverage ground-truth\nreference captions as proxies to measure the logical consistency of generated\ncaptions. However, optimizing for caption fidelity alone fails to preserve the\nsemantic adequacy of generations; therefore, we propose a multi-objective\nreward function that jointly targets these qualities, without requiring any\nstrong supervision. We demonstrate that these goals can be simultaneously\noptimized with our framework, enhancing performance for various captioning\nmodels of different scales. Our qualitative and quantitative results\ndemonstrate MOCHa's superior performance across various established metrics. We\nalso demonstrate the benefit of our method in the open-vocabulary setting. To\nthis end, we contribute OpenCHAIR, a new benchmark for quantifying\nopen-vocabulary hallucinations in image captioning models, constructed using\ngenerative foundation models. We will release our code, benchmark, and trained\nmodels.",
            "author": [
                "Assaf Ben-Kish",
                "Moran Yanuka",
                "Morris Alper",
                "Raja Giryes",
                "Hadar Averbuch-Elor"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03631v1",
                "http://arxiv.org/pdf/2312.03631v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03628v1",
            "title": "Boosting Segment Anything Model Towards Open-Vocabulary Learning",
            "updated": "2023-12-06T17:19:00Z",
            "published": "2023-12-06T17:19:00Z",
            "summary": "The recent Segment Anything Model (SAM) has emerged as a new paradigmatic\nvision foundation model, showcasing potent zero-shot generalization and\nflexible prompting. Despite SAM finding applications and adaptations in various\ndomains, its primary limitation lies in the inability to grasp object\nsemantics. In this paper, we present Sambor to seamlessly integrate SAM with\nthe open-vocabulary object detector in an end-to-end framework. While retaining\nall the remarkable capabilities inherent to SAM, we enhance it with the\ncapacity to detect arbitrary objects based on human inputs like category names\nor reference expressions. To accomplish this, we introduce a novel SideFormer\nmodule that extracts SAM features to facilitate zero-shot object localization\nand inject comprehensive semantic information for open-vocabulary recognition.\nIn addition, we devise an open-set region proposal network (Open-set RPN),\nenabling the detector to acquire the open-set proposals generated by SAM.\nSambor demonstrates superior zero-shot performance across benchmarks, including\nCOCO and LVIS, proving highly competitive against previous SoTA methods. We\naspire for this work to serve as a meaningful endeavor in endowing SAM to\nrecognize diverse object categories and advancing open-vocabulary learning with\nthe support of vision foundation models.",
            "author": [
                "Xumeng Han",
                "Longhui Wei",
                "Xuehui Yu",
                "Zhiyang Dou",
                "Xin He",
                "Kuiran Wang",
                "Zhenjun Han",
                "Qi Tian"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03628v1",
                "http://arxiv.org/pdf/2312.03628v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03619v2",
            "title": "Evaluation of Active Feature Acquisition Methods for Static Feature\n  Settings",
            "updated": "2023-12-07T18:45:10Z",
            "published": "2023-12-06T17:07:42Z",
            "summary": "Active feature acquisition (AFA) agents, crucial in domains like healthcare\nwhere acquiring features is often costly or harmful, determine the optimal set\nof features for a subsequent classification task. As deploying an AFA agent\nintroduces a shift in missingness distribution, it's vital to assess its\nexpected performance at deployment using retrospective data. In a companion\npaper, we introduce a semi-offline reinforcement learning (RL) framework for\nactive feature acquisition performance evaluation (AFAPE) where features are\nassumed to be time-dependent. Here, we study and extend the AFAPE problem to\ncover static feature settings, where features are time-invariant, and hence\nprovide more flexibility to the AFA agents in deciding the order of the\nacquisitions. In this static feature setting, we derive and adapt new inverse\nprobability weighting (IPW), direct method (DM), and double reinforcement\nlearning (DRL) estimators within the semi-offline RL framework. These\nestimators can be applied when the missingness in the retrospective dataset\nfollows a missing-at-random (MAR) pattern. They also can be applied to\nmissing-not-at-random (MNAR) patterns in conjunction with appropriate existing\nmissing data techniques. We illustrate the improved data efficiency offered by\nthe semi-offline RL estimators in synthetic and real-world data experiments\nunder synthetic MAR and MNAR missingness.",
            "author": [
                "Henrik von Kleist",
                "Alireza Zamanian",
                "Ilya Shpitser",
                "Narges Ahmidi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03619v2",
                "http://arxiv.org/pdf/2312.03619v2"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03613v1",
            "title": "Augmenting optimization-based molecular design with graph neural\n  networks",
            "updated": "2023-12-06T16:56:38Z",
            "published": "2023-12-06T16:56:38Z",
            "summary": "Computer-aided molecular design (CAMD) studies quantitative\nstructure-property relationships and discovers desired molecules using\noptimization algorithms. With the emergence of machine learning models, CAMD\nscore functions may be replaced by various surrogates to automatically learn\nthe structure-property relationships. Due to their outstanding performance on\ngraph domains, graph neural networks (GNNs) have recently appeared frequently\nin CAMD. But using GNNs introduces new optimization challenges. This paper\nformulates GNNs using mixed-integer programming and then integrates this GNN\nformulation into the optimization and machine learning toolkit OMLT. To\ncharacterize and formulate molecules, we inherit the well-established\nmixed-integer optimization formulation for CAMD and propose symmetry-breaking\nconstraints to remove symmetric solutions caused by graph isomorphism. In two\ncase studies, we investigate fragment-based odorant molecular design with more\npractical requirements to test the compatibility and performance of our\napproaches.",
            "author": [
                "Shiqiang Zhang",
                "Juan S. Campos",
                "Christian Feldmann",
                "Frederik Sandfort",
                "Miriam Mathea",
                "Ruth Misener"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03613v1",
                "http://arxiv.org/pdf/2312.03613v1"
            ],
            "primary_category": "cs.CE",
            "category": [
                "cs.CE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03612v1",
            "title": "Physical Symbolic Optimization",
            "updated": "2023-12-06T16:56:28Z",
            "published": "2023-12-06T16:56:28Z",
            "summary": "We present a framework for constraining the automatic sequential generation\nof equations to obey the rules of dimensional analysis by construction.\nCombining this approach with reinforcement learning, we built $\\Phi$-SO, a\nPhysical Symbolic Optimization method for recovering analytical functions from\nphysical data leveraging units constraints. Our symbolic regression algorithm\nachieves state-of-the-art results in contexts in which variables and constants\nhave known physical units, outperforming all other methods on SRBench's Feynman\nbenchmark in the presence of noise (exceeding 0.1%) and showing resilience even\nin the presence of significant (10%) levels of noise.",
            "author": [
                "Wassim Tenachi",
                "Rodrigo Ibata",
                "Foivos I. Diakogiannis"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03612v1",
                "http://arxiv.org/pdf/2312.03612v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "astro-ph.IM",
                "cs.SC",
                "physics.comp-ph",
                "physics.data-an"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03611v1",
            "title": "DreamComposer: Controllable 3D Object Generation via Multi-View\n  Conditions",
            "updated": "2023-12-06T16:55:53Z",
            "published": "2023-12-06T16:55:53Z",
            "summary": "Utilizing pre-trained 2D large-scale generative models, recent works are\ncapable of generating high-quality novel views from a single in-the-wild image.\nHowever, due to the lack of information from multiple views, these works\nencounter difficulties in generating controllable novel views. In this paper,\nwe present DreamComposer, a flexible and scalable framework that can enhance\nexisting view-aware diffusion models by injecting multi-view conditions.\nSpecifically, DreamComposer first uses a view-aware 3D lifting module to obtain\n3D representations of an object from multiple views. Then, it renders the\nlatent features of the target view from 3D representations with the multi-view\nfeature fusion module. Finally the target view features extracted from\nmulti-view inputs are injected into a pre-trained diffusion model. Experiments\nshow that DreamComposer is compatible with state-of-the-art diffusion models\nfor zero-shot novel view synthesis, further enhancing them to generate\nhigh-fidelity novel view images with multi-view conditions, ready for\ncontrollable 3D object reconstruction and various other applications.",
            "author": [
                "Yunhan Yang",
                "Yukun Huang",
                "Xiaoyang Wu",
                "Yuan-Chen Guo",
                "Song-Hai Zhang",
                "Hengshuang Zhao",
                "Tong He",
                "Xihui Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03611v1",
                "http://arxiv.org/pdf/2312.03611v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03608v1",
            "title": "Automated Multimodal Data Annotation via Calibration With Indoor\n  Positioning System",
            "updated": "2023-12-06T16:54:24Z",
            "published": "2023-12-06T16:54:24Z",
            "summary": "Learned object detection methods based on fusion of LiDAR and camera data\nrequire labeled training samples, but niche applications, such as warehouse\nrobotics or automated infrastructure, require semantic classes not available in\nlarge existing datasets. Therefore, to facilitate the rapid creation of\nmultimodal object detection datasets and alleviate the burden of human\nlabeling, we propose a novel automated annotation pipeline. Our method uses an\nindoor positioning system (IPS) to produce accurate detection labels for both\npoint clouds and images and eliminates manual annotation entirely. In an\nexperiment, the system annotates objects of interest 261.8 times faster than a\nhuman baseline and speeds up end-to-end dataset creation by 61.5%.",
            "author": [
                "Ryan Rubel",
                "Andrew Dudash",
                "Mohammad Goli",
                "James O'Hara",
                "Karl Wunderlich"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03608v1",
                "http://arxiv.org/pdf/2312.03608v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "I.4.m"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03607v1",
            "title": "From concrete mixture to structural design -- a holistic optimization\n  procedure in the presence of uncertainties",
            "updated": "2023-12-06T16:54:14Z",
            "published": "2023-12-06T16:54:14Z",
            "summary": "Designing civil structures such as bridges, dams or buildings is a complex\ntask requiring many synergies from several experts. Each is responsible for\ndifferent parts of the process. This is often done in a sequential manner, e.g.\nthe structural engineer makes a design under the assumption of certain material\nproperties (e.g. the strength class of the concrete), and then the material\nengineer optimizes the material with these restrictions. This paper proposes a\nholistic optimization procedure, which combines the concrete mixture design and\nstructural simulations in a joint, forward workflow that we ultimately seek to\ninvert. In this manner, new mixtures beyond standard ranges can be considered.\nAny design effort should account for the presence of uncertainties which can be\naleatoric or epistemic as when data is used to calibrate physical models or\nidentify models that fill missing links in the workflow. Inverting the causal\nrelations established poses several challenges especially when these involve\nphysics-based models which most often than not do not provide\nderivatives/sensitivities or when design constraints are present. To this end,\nwe advocate Variational Optimization, with proposed extensions and\nappropriately chosen heuristics to overcome the aforementioned challenges. The\nproposed methodology is illustrated using the design of a precast concrete beam\nwith the objective to minimize the global warming potential while satisfying a\nnumber of constraints associated with its load-bearing capacity after 28days\naccording to the Eurocode, the demoulding time as computed by a complex\nnonlinear Finite Element model, and the maximum temperature during the\nhydration.",
            "author": [
                "Atul Agrawal",
                "Erik Tamsen",
                "Phaedon-Stelios Koutsourelakis",
                "Joerg F. Unger"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03607v1",
                "http://arxiv.org/pdf/2312.03607v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "physics.comp-ph",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03606v1",
            "title": "DiffusionSat: A Generative Foundation Model for Satellite Imagery",
            "updated": "2023-12-06T16:53:17Z",
            "published": "2023-12-06T16:53:17Z",
            "summary": "Diffusion models have achieved state-of-the-art results on many modalities\nincluding images, speech, and video. However, existing models are not tailored\nto support remote sensing data, which is widely used in important applications\nincluding environmental monitoring and crop-yield prediction. Satellite images\nare significantly different from natural images -- they can be multi-spectral,\nirregularly sampled across time -- and existing diffusion models trained on\nimages from the Web do not support them. Furthermore, remote sensing data is\ninherently spatio-temporal, requiring conditional generation tasks not\nsupported by traditional methods based on captions or images. In this paper, we\npresent DiffusionSat, to date the largest generative foundation model trained\non a collection of publicly available large, high-resolution remote sensing\ndatasets. As text-based captions are sparsely available for satellite images,\nwe incorporate the associated metadata such as geolocation as conditioning\ninformation. Our method produces realistic samples and can be used to solve\nmultiple generative tasks including temporal generation, superresolution given\nmulti-spectral inputs and in-painting. Our method outperforms previous\nstate-of-the-art methods for satellite image generation and is the first\nlarge-scale $\\textit{generative}$ foundation model for satellite imagery.",
            "author": [
                "Samar Khanna",
                "Patrick Liu",
                "Linqi Zhou",
                "Chenlin Meng",
                "Robin Rombach",
                "Marshall Burke",
                "David Lobell",
                "Stefano Ermon"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03606v1",
                "http://arxiv.org/pdf/2312.03606v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03598v1",
            "title": "A Machine-Learning-Accelerated Quantum Transport Study on the Effects of\n  Superlattice Disorder and Strain in a Mid-wave Infrared Curved Sensor",
            "updated": "2023-12-06T16:42:34Z",
            "published": "2023-12-06T16:42:34Z",
            "summary": "An emerging device architecture for infrared imaging is the curved\nfocal-plane array which benefits from several optical advantages over the\ntraditional flat design. However, the curving process introduces additional\nstrain in the active region which must be taken into account. Type-II\nsuperlattices, a promising alternative to traditional bulk materials for use in\ninfrared photodetectors, is a candidate material for use in these devices, but\nthe transport properties of these highly heterogeneous materials are not\nstraightforward and can be affected by different material conditions, such as\nsuperlattice disorder and external strain. We present a comprehensive study of\nthe internal QE calculated for a curved device that incorporates finite element\nanalysis (FEA) modeling, nonequilibirium Green's functions (NEGF) calculations,\nand Gaussian Process (GP) regression. FEA is used for predicting the strain\nconfiguration throughout the active region induced by the curving procedure of\nthe device. NEGF is used to calculate the vertical hole mobility for a select\nset of strain configurations, from which the internal quantum efficiency of the\ndevice is approximated to predict performance under strained conditions. Then\nthis data set is used to train a GP model that maps the quantum efficiency QE\npredictions onto the spatial coordinates of the curved device, based on the\nstrain configuration predicted using FEA. This analysis is performed for ideal\nand disordered SLs to understand both the fundamental and practical limitations\nof the performance of these materials in curved devices.",
            "author": [
                "John Glennon",
                "Alexandros Kyrtsos",
                "Mark O'Masta",
                "Binh-Minh Nyguyen",
                "Enrico Bellotti"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03598v1",
                "http://arxiv.org/pdf/2312.03598v1"
            ],
            "primary_category": "cond-mat.mes-hall",
            "category": [
                "cond-mat.mes-hall"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03596v1",
            "title": "MMM: Generative Masked Motion Model",
            "updated": "2023-12-06T16:35:59Z",
            "published": "2023-12-06T16:35:59Z",
            "summary": "Recent advances in text-to-motion generation using diffusion and\nautoregressive models have shown promising results. However, these models often\nsuffer from a trade-off between real-time performance, high fidelity, and\nmotion editability. To address this gap, we introduce MMM, a novel yet simple\nmotion generation paradigm based on Masked Motion Model. MMM consists of two\nkey components: (1) a motion tokenizer that transforms 3D human motion into a\nsequence of discrete tokens in latent space, and (2) a conditional masked\nmotion transformer that learns to predict randomly masked motion tokens,\nconditioned on the pre-computed text tokens. By attending to motion and text\ntokens in all directions, MMM explicitly captures inherent dependency among\nmotion tokens and semantic mapping between motion and text tokens. During\ninference, this allows parallel and iterative decoding of multiple motion\ntokens that are highly consistent with fine-grained text descriptions,\ntherefore simultaneously achieving high-fidelity and high-speed motion\ngeneration. In addition, MMM has innate motion editability. By simply placing\nmask tokens in the place that needs editing, MMM automatically fills the gaps\nwhile guaranteeing smooth transitions between editing and non-editing parts.\nExtensive experiments on the HumanML3D and KIT-ML datasets demonstrate that MMM\nsurpasses current leading methods in generating high-quality motion (evidenced\nby superior FID scores of 0.08 and 0.429), while offering advanced editing\nfeatures such as body-part modification, motion in-betweening, and the\nsynthesis of long motion sequences. In addition, MMM is two orders of magnitude\nfaster on a single mid-range GPU than editable motion diffusion models. Our\nproject page is available at \\url{https://exitudio.github.io/MMM-page}.",
            "author": [
                "Ekkasit Pinyoanuntapong",
                "Pu Wang",
                "Minwoo Lee",
                "Chen Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03596v1",
                "http://arxiv.org/pdf/2312.03596v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03807v1",
            "title": "Achieving ${O}(\u03b5^{-1.5})$ Complexity in Hessian/Jacobian-free\n  Stochastic Bilevel Optimization",
            "updated": "2023-12-06T16:34:58Z",
            "published": "2023-12-06T16:34:58Z",
            "summary": "In this paper, we revisit the bilevel optimization problem, in which the\nupper-level objective function is generally nonconvex and the lower-level\nobjective function is strongly convex. Although this type of problem has been\nstudied extensively, it still remains an open question how to achieve an\n${O}(\\epsilon^{-1.5})$ sample complexity of ${O}(\\epsilon^{-1.5})$ in\nHessian/Jacobian-free stochastic bilevel optimization without any second-order\nderivative computation. To fill this gap, we propose a novel\nHessian/Jacobian-free bilevel optimizer named FdeHBO, which features a simple\nfully single-loop structure, a projection-aided finite-difference\nHessian/Jacobian-vector approximation, and momentum-based updates.\nTheoretically, we show that FdeHBO requires ${O}(\\epsilon^{-1.5})$ iterations\n(each using ${O}(1)$ samples and only first-order gradient information) to find\nan $\\epsilon$-accurate stationary point. As far as we know, this is the first\nHessian/Jacobian-free method with an ${O}(\\epsilon^{-1.5})$ sample complexity\nfor nonconvex-strongly-convex stochastic bilevel optimization.",
            "author": [
                "Yifan Yang",
                "Peiyao Xiao",
                "Kaiyi Ji"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03807v1",
                "http://arxiv.org/pdf/2312.03807v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03594v2",
            "title": "A Task is Worth One Word: Learning with Task Prompts for High-Quality\n  Versatile Image Inpainting",
            "updated": "2023-12-07T03:13:33Z",
            "published": "2023-12-06T16:34:46Z",
            "summary": "Achieving high-quality versatile image inpainting, where user-specified\nregions are filled with plausible content according to user intent, presents a\nsignificant challenge. Existing methods face difficulties in simultaneously\naddressing context-aware image inpainting and text-guided object inpainting due\nto the distinct optimal training strategies required. To overcome this\nchallenge, we introduce PowerPaint, the first high-quality and versatile\ninpainting model that excels in both tasks. First, we introduce learnable task\nprompts along with tailored fine-tuning strategies to guide the model's focus\non different inpainting targets explicitly. This enables PowerPaint to\naccomplish various inpainting tasks by utilizing different task prompts,\nresulting in state-of-the-art performance. Second, we demonstrate the\nversatility of the task prompt in PowerPaint by showcasing its effectiveness as\na negative prompt for object removal. Additionally, we leverage prompt\ninterpolation techniques to enable controllable shape-guided object inpainting.\nFinally, we extensively evaluate PowerPaint on various inpainting benchmarks to\ndemonstrate its superior performance for versatile image inpainting. We release\nour codes and models on our project page: https://powerpaint.github.io/.",
            "author": [
                "Junhao Zhuang",
                "Yanhong Zeng",
                "Wenran Liu",
                "Chun Yuan",
                "Kai Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03594v2",
                "http://arxiv.org/pdf/2312.03594v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03587v1",
            "title": "Language-Informed Visual Concept Learning",
            "updated": "2023-12-06T16:24:47Z",
            "published": "2023-12-06T16:24:47Z",
            "summary": "Our understanding of the visual world is centered around various concept\naxes, characterizing different aspects of visual entities. While different\nconcept axes can be easily specified by language, e.g. color, the exact visual\nnuances along each axis often exceed the limitations of linguistic\narticulations, e.g. a particular style of painting. In this work, our goal is\nto learn a language-informed visual concept representation, by simply\ndistilling large pre-trained vision-language models. Specifically, we train a\nset of concept encoders to encode the information pertinent to a set of\nlanguage-informed concept axes, with an objective of reproducing the input\nimage through a pre-trained Text-to-Image (T2I) model. To encourage better\ndisentanglement of different concept encoders, we anchor the concept embeddings\nto a set of text embeddings obtained from a pre-trained Visual Question\nAnswering (VQA) model. At inference time, the model extracts concept embeddings\nalong various axes from new test images, which can be remixed to generate\nimages with novel compositions of visual concepts. With a lightweight test-time\nfinetuning procedure, it can also generalize to novel concepts unseen at\ntraining.",
            "author": [
                "Sharon Lee",
                "Yunzhi Zhang",
                "Shangzhe Wu",
                "Jiajun Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03587v1",
                "http://arxiv.org/pdf/2312.03587v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03806v1",
            "title": "XCube ($\\mathcal{X}^3$): Large-Scale 3D Generative Modeling using Sparse\n  Voxel Hierarchies",
            "updated": "2023-12-06T16:23:26Z",
            "published": "2023-12-06T16:23:26Z",
            "summary": "We present $\\mathcal{X}^3$ (pronounced XCube), a novel generative model for\nhigh-resolution sparse 3D voxel grids with arbitrary attributes. Our model can\ngenerate millions of voxels with a finest effective resolution of up to\n$1024^3$ in a feed-forward fashion without time-consuming test-time\noptimization. To achieve this, we employ a hierarchical voxel latent diffusion\nmodel which generates progressively higher resolution grids in a coarse-to-fine\nmanner using a custom framework built on the highly efficient VDB data\nstructure. Apart from generating high-resolution objects, we demonstrate the\neffectiveness of XCube on large outdoor scenes at scales of 100m$\\times$100m\nwith a voxel size as small as 10cm. We observe clear qualitative and\nquantitative improvements over past approaches. In addition to unconditional\ngeneration, we show that our model can be used to solve a variety of tasks such\nas user-guided editing, scene completion from a single scan, and text-to-3D.\nMore results and details can be found at\nhttps://research.nvidia.com/labs/toronto-ai/xcube/.",
            "author": [
                "Xuanchi Ren",
                "Jiahui Huang",
                "Xiaohui Zeng",
                "Ken Museth",
                "Sanja Fidler",
                "Francis Williams"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03806v1",
                "http://arxiv.org/pdf/2312.03806v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03585v1",
            "title": "Foundation Model Assisted Weakly Supervised Semantic Segmentation",
            "updated": "2023-12-06T16:21:06Z",
            "published": "2023-12-06T16:21:06Z",
            "summary": "This work aims to leverage pre-trained foundation models, such as contrastive\nlanguage-image pre-training (CLIP) and segment anything model (SAM), to address\nweakly supervised semantic segmentation (WSSS) using image-level labels. To\nthis end, we propose a coarse-to-fine framework based on CLIP and SAM for\ngenerating high-quality segmentation seeds. Specifically, we construct an image\nclassification task and a seed segmentation task, which are jointly performed\nby CLIP with frozen weights and two sets of learnable task-specific prompts. A\nSAM-based seeding (SAMS) module is designed and applied to each task to produce\neither coarse or fine seed maps. Moreover, we design a multi-label contrastive\nloss supervised by image-level labels and a CAM activation loss supervised by\nthe generated coarse seed map. These losses are used to learn the prompts,\nwhich are the only parts need to be learned in our framework. Once the prompts\nare learned, we input each image along with the learned segmentation-specific\nprompts into CLIP and the SAMS module to produce high-quality segmentation\nseeds. These seeds serve as pseudo labels to train an off-the-shelf\nsegmentation network like other two-stage WSSS methods. Experiments show that\nour method achieves the state-of-the-art performance on PASCAL VOC 2012 and\ncompetitive results on MS COCO 2014.",
            "author": [
                "Xiaobo Yang",
                "Xiaojin Gong"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03585v1",
                "http://arxiv.org/pdf/2312.03585v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03584v1",
            "title": "Context Diffusion: In-Context Aware Image Generation",
            "updated": "2023-12-06T16:19:51Z",
            "published": "2023-12-06T16:19:51Z",
            "summary": "We propose Context Diffusion, a diffusion-based framework that enables image\ngeneration models to learn from visual examples presented in context. Recent\nwork tackles such in-context learning for image generation, where a query image\nis provided alongside context examples and text prompts. However, the quality\nand fidelity of the generated images deteriorate when the prompt is not\npresent, demonstrating that these models are unable to truly learn from the\nvisual context. To address this, we propose a novel framework that separates\nthe encoding of the visual context and preserving the structure of the query\nimages. This results in the ability to learn from the visual context and text\nprompts, but also from either one of them. Furthermore, we enable our model to\nhandle few-shot settings, to effectively address diverse in-context learning\nscenarios. Our experiments and user study demonstrate that Context Diffusion\nexcels in both in-domain and out-of-domain tasks, resulting in an overall\nenhancement in image quality and fidelity compared to counterpart models.",
            "author": [
                "Ivona Najdenkoska",
                "Animesh Sinha",
                "Abhimanyu Dubey",
                "Dhruv Mahajan",
                "Vignesh Ramanathan",
                "Filip Radenovic"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03584v1",
                "http://arxiv.org/pdf/2312.03584v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03580v1",
            "title": "Invariance & Causal Representation Learning: Prospects and Limitations",
            "updated": "2023-12-06T16:16:31Z",
            "published": "2023-12-06T16:16:31Z",
            "summary": "In causal models, a given mechanism is assumed to be invariant to changes of\nother mechanisms. While this principle has been utilized for inference in\nsettings where the causal variables are observed, theoretical insights when the\nvariables of interest are latent are largely missing. We assay the connection\nbetween invariance and causal representation learning by establishing\nimpossibility results which show that invariance alone is insufficient to\nidentify latent causal variables. Together with practical considerations, we\nuse these theoretical findings to highlight the need for additional constraints\nin order to identify representations by exploiting invariance.",
            "author": [
                "Simon Bing",
                "Jonas Wahl",
                "Urmi Ninad",
                "Jakob Runge"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03580v1",
                "http://arxiv.org/pdf/2312.03580v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03577v1",
            "title": "Improving Bias Mitigation through Bias Experts in Natural Language\n  Understanding",
            "updated": "2023-12-06T16:15:00Z",
            "published": "2023-12-06T16:15:00Z",
            "summary": "Biases in the dataset often enable the model to achieve high performance on\nin-distribution data, while poorly performing on out-of-distribution data. To\nmitigate the detrimental effect of the bias on the networks, previous works\nhave proposed debiasing methods that down-weight the biased examples identified\nby an auxiliary model, which is trained with explicit bias labels. However,\nfinding a type of bias in datasets is a costly process. Therefore, recent\nstudies have attempted to make the auxiliary model biased without the guidance\n(or annotation) of bias labels, by constraining the model's training\nenvironment or the capability of the model itself. Despite the promising\ndebiasing results of recent works, the multi-class learning objective, which\nhas been naively used to train the auxiliary model, may harm the bias\nmitigation effect due to its regularization effect and competitive nature\nacross classes. As an alternative, we propose a new debiasing framework that\nintroduces binary classifiers between the auxiliary model and the main model,\ncoined bias experts. Specifically, each bias expert is trained on a binary\nclassification task derived from the multi-class classification task via the\nOne-vs-Rest approach. Experimental results demonstrate that our proposed\nstrategy improves the bias identification ability of the auxiliary model.\nConsequently, our debiased model consistently outperforms the state-of-the-art\non various challenge datasets.",
            "author": [
                "Eojin Jeon",
                "Mingyu Lee",
                "Juhyeong Park",
                "Yeachan Kim",
                "Wing-Lam Mok",
                "SangKeun Lee"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03577v1",
                "http://arxiv.org/pdf/2312.03577v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03567v1",
            "title": "XAIQA: Explainer-Based Data Augmentation for Extractive Question\n  Answering",
            "updated": "2023-12-06T15:59:06Z",
            "published": "2023-12-06T15:59:06Z",
            "summary": "Extractive question answering (QA) systems can enable physicians and\nresearchers to query medical records, a foundational capability for designing\nclinical studies and understanding patient medical history. However, building\nthese systems typically requires expert-annotated QA pairs. Large language\nmodels (LLMs), which can perform extractive QA, depend on high quality data in\ntheir prompts, specialized for the application domain. We introduce a novel\napproach, XAIQA, for generating synthetic QA pairs at scale from data naturally\navailable in electronic health records. Our method uses the idea of a\nclassification model explainer to generate questions and answers about medical\nconcepts corresponding to medical codes. In an expert evaluation with two\nphysicians, our method identifies $2.2\\times$ more semantic matches and\n$3.8\\times$ more clinical abbreviations than two popular approaches that use\nsentence transformers to create QA pairs. In an ML evaluation, adding our QA\npairs improves performance of GPT-4 as an extractive QA model, including on\ndifficult questions. In both the expert and ML evaluations, we examine\ntrade-offs between our method and sentence transformers for QA pair generation\ndepending on question difficulty.",
            "author": [
                "Joel Stremmel",
                "Ardavan Saeedi",
                "Hamid Hassanzadeh",
                "Sanjit Batra",
                "Jeffrey Hertzberg",
                "Jaime Murillo",
                "Eran Halperin"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03567v1",
                "http://arxiv.org/pdf/2312.03567v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "I.2.7"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03805v1",
            "title": "SYNC-CLIP: Synthetic Data Make CLIP Generalize Better in Data-Limited\n  Scenarios",
            "updated": "2023-12-06T15:54:05Z",
            "published": "2023-12-06T15:54:05Z",
            "summary": "Prompt learning is a powerful technique for transferring Vision-Language\nModels (VLMs) such as CLIP to downstream tasks. However, the prompt-based\nmethods that are fine-tuned solely with base classes may struggle to generalize\nto novel classes in open-vocabulary scenarios, especially when data are\nlimited. To address this issue, we propose an innovative approach called\nSYNC-CLIP that leverages SYNthetiC data for enhancing the generalization\ncapability of CLIP. Based on the observation of the distribution shift between\nthe real and synthetic samples, we treat real and synthetic samples as distinct\ndomains and propose to optimize separate domain prompts to capture\ndomain-specific information, along with the shared visual prompts to preserve\nthe semantic consistency between two domains. By aligning the cross-domain\nfeatures, the synthetic data from novel classes can provide implicit guidance\nto rebalance the decision boundaries. Experimental results on three model\ngeneralization tasks demonstrate that our method performs very competitively\nacross various benchmarks. Notably, SYNC-CLIP outperforms the state-of-the-art\ncompetitor PromptSRC by an average improvement of 3.0% on novel classes across\n11 datasets in open-vocabulary scenarios.",
            "author": [
                "Mushui Liu",
                "Weijie He",
                "Ziqian Lu",
                "Yunlong Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03805v1",
                "http://arxiv.org/pdf/2312.03805v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03562v1",
            "title": "Enhancing Kinship Verification through Multiscale Retinex and Combined\n  Deep-Shallow features",
            "updated": "2023-12-06T15:52:31Z",
            "published": "2023-12-06T15:52:31Z",
            "summary": "The challenge of kinship verification from facial images represents a\ncutting-edge and formidable frontier in the realms of pattern recognition and\ncomputer vision. This area of study holds a myriad of potential applications,\nspanning from image annotation and forensic analysis to social media research.\nOur research stands out by integrating a preprocessing method named Multiscale\nRetinex (MSR), which elevates image quality and amplifies contrast, ultimately\nbolstering the end results. Strategically, our methodology capitalizes on the\nharmonious blend of deep and shallow texture descriptors, merging them\nproficiently at the score level through the Logistic Regression (LR) method. To\nelucidate, we employ the Local Phase Quantization (LPQ) descriptor to extract\nshallow texture characteristics. For deep feature extraction, we turn to the\nprowess of the VGG16 model, which is pre-trained on a convolutional neural\nnetwork (CNN). The robustness and efficacy of our method have been put to the\ntest through meticulous experiments on three rigorous kinship datasets, namely:\nCornell Kin Face, UB Kin Face, and TS Kin Face.",
            "author": [
                "El Ouanas Belabbaci",
                "Mohammed Khammari",
                "Ammar Chouchane",
                "Mohcene Bessaoudi",
                "Abdelmalik Ouamane",
                "Yassine Himeur",
                "Shadi Atalla",
                "Wathiq Mansoor"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03562v1",
                "http://arxiv.org/pdf/2312.03562v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03561v1",
            "title": "Blueprinting the Future: Automatic Item Categorization using\n  Hierarchical Zero-Shot and Few-Shot Classifiers",
            "updated": "2023-12-06T15:51:49Z",
            "published": "2023-12-06T15:51:49Z",
            "summary": "In testing industry, precise item categorization is pivotal to align exam\nquestions with the designated content domains outlined in the assessment\nblueprint. Traditional methods either entail manual classification, which is\nlaborious and error-prone, or utilize machine learning requiring extensive\ntraining data, often leading to model underfit or overfit issues. This study\nunveils a novel approach employing the zero-shot and few-shot Generative\nPretrained Transformer (GPT) classifier for hierarchical item categorization,\nminimizing the necessity for training data, and instead, leveraging human-like\nlanguage descriptions to define categories. Through a structured python\ndictionary, the hierarchical nature of examination blueprints is navigated\nseamlessly, allowing for a tiered classification of items across multiple\nlevels. An initial simulation with artificial data demonstrates the efficacy of\nthis method, achieving an average accuracy of 92.91% measured by the F1 score.\nThis method was further applied to real exam items from the 2022 In-Training\nExamination (ITE) conducted by the American Board of Family Medicine (ABFM),\nreclassifying 200 items according to a newly formulated blueprint swiftly in 15\nminutes, a task that traditionally could span several days among editors and\nphysicians. This innovative approach not only drastically cuts down\nclassification time but also ensures a consistent, principle-driven\ncategorization, minimizing human biases and discrepancies. The ability to\nrefine classifications by adjusting definitions adds to its robustness and\nsustainability.",
            "author": [
                "Ting Wang",
                "Keith Stelter",
                "Jenn Floyd",
                "Thomas O'Neill",
                "Nathaniel Hendrix",
                "Andrew Bazemore",
                "Kevin Rode",
                "Warren Newton"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03561v1",
                "http://arxiv.org/pdf/2312.03561v1"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME",
                "cs.CY",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03556v1",
            "title": "Personalized Face Inpainting with Diffusion Models by Parallel Visual\n  Attention",
            "updated": "2023-12-06T15:39:03Z",
            "published": "2023-12-06T15:39:03Z",
            "summary": "Face inpainting is important in various applications, such as photo\nrestoration, image editing, and virtual reality. Despite the significant\nadvances in face generative models, ensuring that a person's unique facial\nidentity is maintained during the inpainting process is still an elusive goal.\nCurrent state-of-the-art techniques, exemplified by MyStyle, necessitate\nresource-intensive fine-tuning and a substantial number of images for each new\nidentity. Furthermore, existing methods often fall short in accommodating\nuser-specified semantic attributes, such as beard or expression. To improve\ninpainting results, and reduce the computational complexity during inference,\nthis paper proposes the use of Parallel Visual Attention (PVA) in conjunction\nwith diffusion models. Specifically, we insert parallel attention matrices to\neach cross-attention module in the denoising network, which attends to features\nextracted from reference images by an identity encoder. We train the added\nattention modules and identity encoder on CelebAHQ-IDI, a dataset proposed for\nidentity-preserving face inpainting. Experiments demonstrate that PVA attains\nunparalleled identity resemblance in both face inpainting and face inpainting\nwith language guidance tasks, in comparison to various benchmarks, including\nMyStyle, Paint by Example, and Custom Diffusion. Our findings reveal that PVA\nensures good identity preservation while offering effective\nlanguage-controllability. Additionally, in contrast to Custom Diffusion, PVA\nrequires just 40 fine-tuning steps for each new identity, which translates to a\nsignificant speed increase of over 20 times.",
            "author": [
                "Jianjin Xu",
                "Saman Motamed",
                "Praneetha Vaddamanu",
                "Chen Henry Wu",
                "Christian Haene",
                "Jean-Charles Bazin",
                "Fernando de la Torre"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03556v1",
                "http://arxiv.org/pdf/2312.03556v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03555v1",
            "title": "Enabling Edge Artificial Intelligence via Goal-oriented Deep Neural\n  Network Splitting",
            "updated": "2023-12-06T15:38:53Z",
            "published": "2023-12-06T15:38:53Z",
            "summary": "Deep Neural Network (DNN) splitting is one of the key enablers of edge\nArtificial Intelligence (AI), as it allows end users to pre-process data and\noffload part of the computational burden to nearby Edge Cloud Servers (ECSs).\nThis opens new opportunities and degrees of freedom in balancing energy\nconsumption, delay, accuracy, privacy, and other trustworthiness metrics. In\nthis work, we explore the opportunity of DNN splitting at the edge of 6G\nwireless networks to enable low energy cooperative inference with target delay\nand accuracy with a goal-oriented perspective. Going beyond the current\nliterature, we explore new trade-offs that take into account the accuracy\ndegradation as a function of the Splitting Point (SP) selection and wireless\nchannel conditions. Then, we propose an algorithm that dynamically controls SP\nselection, local computing resources, uplink transmit power and bandwidth\nallocation, in a goal-oriented fashion, to meet a target goal-effectiveness. To\nthe best of our knowledge, this is the first work proposing adaptive SP\nselection on the basis of all learning performance (i.e., energy, delay,\naccuracy), with the aim of guaranteeing the accomplishment of a goal (e.g.,\nminimize the energy consumption under latency and accuracy constraints).\nNumerical results show the advantages of the proposed SP selection and resource\nallocation, to enable energy frugal and effective edge AI.",
            "author": [
                "Francesco Binucci",
                "Mattia Merluzzi",
                "Paolo Banelli",
                "Emilio Calvanese Strinati",
                "Paolo Di Lorenzo"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03555v1",
                "http://arxiv.org/pdf/2312.03555v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03802v1",
            "title": "Emergent self-adaptation in an integrated photonic neural network for\n  backpropagation-free learning",
            "updated": "2023-12-06T15:25:18Z",
            "published": "2023-12-06T15:25:18Z",
            "summary": "Plastic self-adaptation, nonlinear recurrent dynamics and multi-scale memory\nare desired features in hardware implementations of neural networks, because\nthey enable them to learn, adapt and process information similarly to the way\nbiological brains do. In this work, we experimentally demonstrate these\nproperties occurring in arrays of photonic neurons. Importantly, this is\nrealised autonomously in an emergent fashion, without the need for an external\ncontroller setting weights and without explicit feedback of a global reward\nsignal. Using a hierarchy of such arrays coupled to a backpropagation-free\ntraining algorithm based on simple logistic regression, we are able to achieve\na performance of 98.2% on the MNIST task, a popular benchmark task looking at\nclassification of written digits. The plastic nodes consist of silicon\nphotonics microring resonators covered by a patch of phase-change material that\nimplements nonvolatile memory. The system is compact, robust, and\nstraightforward to scale up through the use of multiple wavelengths. Moreover,\nit constitutes a unique platform to test and efficiently implement biologically\nplausible learning schemes at a high processing speed.",
            "author": [
                "Alessio Lugnan",
                "Samarth Aggarwal",
                "Frank Br\u00fcckerhoff-Pl\u00fcckelmann",
                "C. David Wright",
                "Wolfram H. P. Pernice",
                "Harish Bhaskaran",
                "Peter Bienstman"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03802v1",
                "http://arxiv.org/pdf/2312.03802v1"
            ],
            "primary_category": "physics.optics",
            "category": [
                "physics.optics"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03801v1",
            "title": "Generalization to New Sequential Decision Making Tasks with In-Context\n  Learning",
            "updated": "2023-12-06T15:19:28Z",
            "published": "2023-12-06T15:19:28Z",
            "summary": "Training autonomous agents that can learn new tasks from only a handful of\ndemonstrations is a long-standing problem in machine learning. Recently,\ntransformers have been shown to learn new language or vision tasks without any\nweight updates from only a few examples, also referred to as in-context\nlearning. However, the sequential decision making setting poses additional\nchallenges having a lower tolerance for errors since the environment's\nstochasticity or the agent's actions can lead to unseen, and sometimes\nunrecoverable, states. In this paper, we use an illustrative example to show\nthat naively applying transformers to sequential decision making problems does\nnot enable in-context learning of new tasks. We then demonstrate how training\non sequences of trajectories with certain distributional properties leads to\nin-context learning of new sequential decision making tasks. We investigate\ndifferent design choices and find that larger model and dataset sizes, as well\nas more task diversity, environment stochasticity, and trajectory burstiness,\nall result in better in-context learning of new out-of-distribution tasks. By\ntraining on large diverse offline datasets, our model is able to learn new\nMiniHack and Procgen tasks without any weight updates from just a handful of\ndemonstrations.",
            "author": [
                "Sharath Chandra Raparthy",
                "Eric Hambro",
                "Robert Kirk",
                "Mikael Henaff",
                "Roberta Raileanu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03801v1",
                "http://arxiv.org/pdf/2312.03801v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03545v1",
            "title": "HI Galaxy Signatures in the SARAO MeerKAT Galactic Plane Survey $-$ I.\n  Probing the richness of the Great Attractor Wall across the inner Zone of\n  Avoidance",
            "updated": "2023-12-06T15:16:04Z",
            "published": "2023-12-06T15:16:04Z",
            "summary": "This paper presents the first HI results extracted from the SARAO MeerKAT\nGalactic Plane Survey (SMGPS) $-$ a narrow strip ($b \\sim 3^\\circ$) along the\nsouthern Milky Way. The primary goal consisted in tracing the Great Attractor\n(GA) Wall across the innermost Zone of Avoidance. We reduced a segment spanning\nthe longitude range $302^\\circ \\leq \\ell \\leq 332^\\circ$ for the redshift range\n$z \\leq 0.08$. The superb SMGPS sensitivity (rms = 0.3-0.5 mJy beam$^{-1}$ per\n44 kms$^{-1}$ channel) and angular resolution ($\\sim$ 31\" $\\times$ 26\") lead to\na detection limit of log$(M_{\\rm HI}/$M$_\\odot) \\geq$ 8.5 at the GA distance\n($V_{\\rm hel} \\sim 3500 - 6500$ kms$^{-1}$). A total of 477 galaxy candidates\nwere identified over the full redshift range. A comparison of the few HI\ndetections with counterparts in the literature (mostly HIZOA) found the HI\nfluxes and other HI parameters to be highly consistent. The continuation of the\nGA Wall is confirmed through a prominent overdensity of $N = 214$ detections in\nthe GA distance range. At higher latitudes, the wall moves to higher redshifts,\nsupportive of a possible link with the Ophiuchus cluster located behind the\nGalactic Bulge. This deep interferometric HI survey demonstrates the power of\nthe SMGPS in improving our insight of large-scale structures at these extremely\nlow latitudes, despite the high obscuration and continuum background.",
            "author": [
                "Nadia Steyn",
                "Ren\u00e9e C. Kraan-Korteweg",
                "Sambatriniaina H. A. Rajohnson",
                "Sushma Kurapati",
                "Hao Chen",
                "Bradley Frank",
                "Paolo Serra",
                "Lister Staveley-Smith",
                "Fernando Camilo",
                "Sharmila Goedhart"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03545v1",
                "http://arxiv.org/pdf/2312.03545v1"
            ],
            "primary_category": "astro-ph.GA",
            "category": [
                "astro-ph.GA",
                "astro-ph.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03543v1",
            "title": "GPT-4 Enhanced Multimodal Grounding for Autonomous Driving: Leveraging\n  Cross-Modal Attention with Large Language Models",
            "updated": "2023-12-06T15:14:30Z",
            "published": "2023-12-06T15:14:30Z",
            "summary": "In the field of autonomous vehicles (AVs), accurately discerning commander\nintent and executing linguistic commands within a visual context presents a\nsignificant challenge. This paper introduces a sophisticated encoder-decoder\nframework, developed to address visual grounding in AVs.Our Context-Aware\nVisual Grounding (CAVG) model is an advanced system that integrates five core\nencoders-Text, Image, Context, and Cross-Modal-with a Multimodal decoder. This\nintegration enables the CAVG model to adeptly capture contextual semantics and\nto learn human emotional features, augmented by state-of-the-art Large Language\nModels (LLMs) including GPT-4. The architecture of CAVG is reinforced by the\nimplementation of multi-head cross-modal attention mechanisms and a\nRegion-Specific Dynamic (RSD) layer for attention modulation. This\narchitectural design enables the model to efficiently process and interpret a\nrange of cross-modal inputs, yielding a comprehensive understanding of the\ncorrelation between verbal commands and corresponding visual scenes. Empirical\nevaluations on the Talk2Car dataset, a real-world benchmark, demonstrate that\nCAVG establishes new standards in prediction accuracy and operational\nefficiency. Notably, the model exhibits exceptional performance even with\nlimited training data, ranging from 50% to 75% of the full dataset. This\nfeature highlights its effectiveness and potential for deployment in practical\nAV applications. Moreover, CAVG has shown remarkable robustness and\nadaptability in challenging scenarios, including long-text command\ninterpretation, low-light conditions, ambiguous command contexts, inclement\nweather conditions, and densely populated urban environments. The code for the\nproposed model is available at our Github.",
            "author": [
                "Haicheng Liao",
                "Huanming Shen",
                "Zhenning Li",
                "Chengyue Wang",
                "Guofa Li",
                "Yiming Bie",
                "Chengzhong Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03543v1",
                "http://arxiv.org/pdf/2312.03543v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03533v1",
            "title": "Low-shot Object Learning with Mutual Exclusivity Bias",
            "updated": "2023-12-06T14:54:10Z",
            "published": "2023-12-06T14:54:10Z",
            "summary": "This paper introduces Low-shot Object Learning with Mutual Exclusivity Bias\n(LSME), the first computational framing of mutual exclusivity bias, a\nphenomenon commonly observed in infants during word learning. We provide a\nnovel dataset, comprehensive baselines, and a state-of-the-art method to enable\nthe ML community to tackle this challenging learning task. The goal of LSME is\nto analyze an RGB image of a scene containing multiple objects and correctly\nassociate a previously-unknown object instance with a provided category label.\nThis association is then used to perform low-shot learning to test category\ngeneralization. We provide a data generation pipeline for the LSME problem and\nconduct a thorough analysis of the factors that contribute to its difficulty.\nAdditionally, we evaluate the performance of multiple baselines, including\nstate-of-the-art foundation models. Finally, we present a baseline approach\nthat outperforms state-of-the-art models in terms of low-shot accuracy.",
            "author": [
                "Anh Thai",
                "Ahmad Humayun",
                "Stefan Stojanov",
                "Zixuan Huang",
                "Bikram Boote",
                "James M. Rehg"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03533v1",
                "http://arxiv.org/pdf/2312.03533v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03798v1",
            "title": "Single Image Reflection Removal with Reflection Intensity Prior\n  Knowledge",
            "updated": "2023-12-06T14:52:11Z",
            "published": "2023-12-06T14:52:11Z",
            "summary": "Single Image Reflection Removal (SIRR) in real-world images is a challenging\ntask due to diverse image degradations occurring on the glass surface during\nlight transmission and reflection. Many existing methods rely on specific prior\nassumptions to resolve the problem. In this paper, we propose a general\nreflection intensity prior that captures the intensity of the reflection\nphenomenon and demonstrate its effectiveness. To learn the reflection intensity\nprior, we introduce the Reflection Prior Extraction Network (RPEN). By\nsegmenting images into regional patches, RPEN learns non-uniform reflection\nprior in an image. We propose Prior-based Reflection Removal Network (PRRN)\nusing a simple transformer U-Net architecture that adapts reflection prior fed\nfrom RPEN. Experimental results on real-world benchmarks demonstrate the\neffectiveness of our approach achieving state-of-the-art accuracy in SIRR.",
            "author": [
                "Dongshen Han",
                "Seungkyu Lee",
                "Chaoning Zhang",
                "Heechan Yoon",
                "Hyukmin Kwon",
                "HyunCheol Kim",
                "HyonGon Choo"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03798v1",
                "http://arxiv.org/pdf/2312.03798v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03526v1",
            "title": "On the Diversity and Realism of Distilled Dataset: An Efficient Dataset\n  Distillation Paradigm",
            "updated": "2023-12-06T14:40:05Z",
            "published": "2023-12-06T14:40:05Z",
            "summary": "Contemporary machine learning requires training large neural networks on\nmassive datasets and thus faces the challenges of high computational demands.\nDataset distillation, as a recent emerging strategy, aims to compress\nreal-world datasets for efficient training. However, this line of research\ncurrently struggle with large-scale and high-resolution datasets, hindering its\npracticality and feasibility. To this end, we re-examine the existing dataset\ndistillation methods and identify three properties required for large-scale\nreal-world applications, namely, realism, diversity, and efficiency. As a\nremedy, we propose RDED, a novel computationally-efficient yet effective data\ndistillation paradigm, to enable both diversity and realism of the distilled\ndata. Extensive empirical results over various neural architectures and\ndatasets demonstrate the advancement of RDED: we can distill the full\nImageNet-1K to a small dataset comprising 10 images per class within 7 minutes,\nachieving a notable 42% top-1 accuracy with ResNet-18 on a single RTX-4090 GPU\n(while the SOTA only achieves 21% but requires 6 hours).",
            "author": [
                "Peng Sun",
                "Bei Shi",
                "Daiwei Yu",
                "Tao Lin"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03526v1",
                "http://arxiv.org/pdf/2312.03526v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03796v1",
            "title": "Multi-Scale and Multi-Modal Contrastive Learning Network for Biomedical\n  Time Series",
            "updated": "2023-12-06T14:33:50Z",
            "published": "2023-12-06T14:33:50Z",
            "summary": "Multi-modal biomedical time series (MBTS) data offers a holistic view of the\nphysiological state, holding significant importance in various bio-medical\napplications. Owing to inherent noise and distribution gaps across different\nmodalities, MBTS can be complex to model. Various deep learning models have\nbeen developed to learn representations of MBTS but still fall short in\nrobustness due to the ignorance of modal-to-modal variations. This paper\npresents a multi-scale and multi-modal biomedical time series representation\nlearning (MBSL) network with contrastive learning to migrate these variations.\nFirstly, MBTS is grouped based on inter-modal distances, then each group with\nminimum intra-modal variations can be effectively modeled by individual\nencoders. Besides, to enhance the multi-scale feature extraction (encoder),\nvarious patch lengths and mask ratios are designed to generate tokens with\nsemantic information at different scales and diverse contextual perspectives\nrespectively. Finally, cross-modal contrastive learning is proposed to maximize\nconsistency among inter-modal groups, maintaining useful information and\neliminating noises. Experiments against four bio-medical applications show that\nMBSL outperforms state-of-the-art models by 33.9% mean average errors (MAE) in\nrespiration rate, by 13.8% MAE in exercise heart rate, by 1.41% accuracy in\nhuman activity recognition, and by 1.14% F1-score in obstructive sleep\napnea-hypopnea syndrome.",
            "author": [
                "Hongbo Guo",
                "Xinzi Xu",
                "Hao Wu",
                "Guoxing Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03796v1",
                "http://arxiv.org/pdf/2312.03796v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03520v1",
            "title": "Defense Against Adversarial Attacks using Convolutional Auto-Encoders",
            "updated": "2023-12-06T14:29:16Z",
            "published": "2023-12-06T14:29:16Z",
            "summary": "Deep learning models, while achieving state-of-the-art performance on many\ntasks, are susceptible to adversarial attacks that exploit inherent\nvulnerabilities in their architectures. Adversarial attacks manipulate the\ninput data with imperceptible perturbations, causing the model to misclassify\nthe data or produce erroneous outputs. This work is based on enhancing the\nrobustness of targeted classifier models against adversarial attacks. To\nachieve this, an convolutional autoencoder-based approach is employed that\neffectively counters adversarial perturbations introduced to the input images.\nBy generating images closely resembling the input images, the proposed\nmethodology aims to restore the model's accuracy.",
            "author": [
                "Shreyasi Mandal"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03520v1",
                "http://arxiv.org/pdf/2312.03520v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "I.4.5; I.5.1; I.5.4"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03516v1",
            "title": "Clustering by Contour coreset and variational quantum eigensolver",
            "updated": "2023-12-06T14:21:17Z",
            "published": "2023-12-06T14:21:17Z",
            "summary": "Recent work has proposed solving the k-means clustering problem on quantum\ncomputers via the Quantum Approximate Optimization Algorithm (QAOA) and coreset\ntechniques. Although the current method demonstrates the possibility of quantum\nk-means clustering, it does not ensure high accuracy and consistency across a\nwide range of datasets. The existing coreset techniques are designed for\nclassical algorithms and there has been no quantum-tailored coreset technique\nwhich is designed to boost the accuracy of quantum algorithms. In this work, we\npropose solving the k-means clustering problem with the variational quantum\neigensolver (VQE) and a customised coreset method, the Contour coreset, which\nhas been formulated with specific focus on quantum algorithms. Extensive\nsimulations with synthetic and real-life data demonstrated that our VQE+Contour\nCoreset approach outperforms existing QAOA+Coreset k-means clustering\napproaches with higher accuracy and lower standard deviation. Our work has\nshown that quantum tailored coreset techniques has the potential to\nsignificantly boost the performance of quantum algorithms when compared to\nusing generic off-the-shelf coreset techniques.",
            "author": [
                "Canaan Yung",
                "Muhammad Usman"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03516v1",
                "http://arxiv.org/pdf/2312.03516v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03511v1",
            "title": "Kandinsky 3.0 Technical Report",
            "updated": "2023-12-06T14:13:38Z",
            "published": "2023-12-06T14:13:38Z",
            "summary": "We present Kandinsky 3.0, a large-scale text-to-image generation model based\non latent diffusion, continuing the series of text-to-image Kandinsky models\nand reflecting our progress to achieve higher quality and realism of image\ngeneration. Compared to previous versions of Kandinsky 2.x, Kandinsky 3.0\nleverages a two times larger U-Net backbone, a ten times larger text encoder\nand removes diffusion mapping. We describe the architecture of the model, the\ndata collection procedure, the training technique, and the production system of\nuser interaction. We focus on the key components that, as we have identified as\na result of a large number of experiments, had the most significant impact on\nimproving the quality of our model compared to the others. By our side-by-side\ncomparisons, Kandinsky becomes better in text understanding and works better on\nspecific domains. Project page: https://ai-forever.github.io/Kandinsky-3",
            "author": [
                "Vladimir Arkhipkin",
                "Andrei Filatov",
                "Viacheslav Vasilev",
                "Anastasia Maltseva",
                "Said Azizov",
                "Igor Pavlov",
                "Julia Agafonova",
                "Andrey Kuznetsov",
                "Denis Dimitrov"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03511v1",
                "http://arxiv.org/pdf/2312.03511v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG",
                "cs.MM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03510v2",
            "title": "Towards Sobolev Pruning",
            "updated": "2023-12-07T10:38:56Z",
            "published": "2023-12-06T14:13:30Z",
            "summary": "The increasing use of stochastic models for describing complex phenomena\nwarrants surrogate models that capture the reference model characteristics at a\nfraction of the computational cost, foregoing potentially expensive Monte Carlo\nsimulation. The predominant approach of fitting a large neural network and then\npruning it to a reduced size has commonly neglected shortcomings. The produced\nsurrogate models often will not capture the sensitivities and uncertainties\ninherent in the original model. In particular, (higher-order) derivative\ninformation of such surrogates could differ drastically. Given a large enough\nnetwork, we expect this derivative information to match. However, the pruned\nmodel will almost certainly not share this behavior.\n  In this paper, we propose to find surrogate models by using sensitivity\ninformation throughout the learning and pruning process. We build on work using\nInterval Adjoint Significance Analysis for pruning and combine it with the\nrecent advancements in Sobolev Training to accurately model the original\nsensitivity information in the pruned neural network based surrogate model. We\nexperimentally underpin the method on an example of pricing a multidimensional\nBasket option modelled through a stochastic differential equation with Brownian\nmotion. The proposed method is, however, not limited to the domain of\nquantitative finance, which was chosen as a case study for intuitive\ninterpretations of the sensitivities. It serves as a foundation for building\nfurther surrogate modelling techniques considering sensitivity information.",
            "author": [
                "Neil Kichler",
                "Sher Afghan",
                "Uwe Naumann"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03510v2",
                "http://arxiv.org/pdf/2312.03510v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "q-fin.CP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03509v1",
            "title": "Gravitational cell detection and tracking in fluorescence microscopy\n  data",
            "updated": "2023-12-06T14:08:05Z",
            "published": "2023-12-06T14:08:05Z",
            "summary": "Automatic detection and tracking of cells in microscopy images are major\napplications of computer vision technologies in both biomedical research and\nclinical practice. Though machine learning methods are increasingly common in\nthese fields, classical algorithms still offer significant advantages for both\ntasks, including better explainability, faster computation, lower hardware\nrequirements and more consistent performance. In this paper, we present a novel\napproach based on gravitational force fields that can compete with, and\npotentially outperform modern machine learning models when applied to\nfluorescence microscopy images. This method includes detection, segmentation,\nand tracking elements, with the results demonstrated on a Cell Tracking\nChallenge dataset.",
            "author": [
                "Nikomidisz Eftimiu",
                "Michal Kozubek"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03509v1",
                "http://arxiv.org/pdf/2312.03509v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "q-bio.CB",
                "I.4.6"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03508v1",
            "title": "Convolutional neural network based decoders for surface codes",
            "updated": "2023-12-06T14:07:31Z",
            "published": "2023-12-06T14:07:31Z",
            "summary": "The decoding of error syndromes of surface codes with classical algorithms\nmay slow down quantum computation. To overcome this problem it is possible to\nimplement decoding algorithms based on artificial neural networks. This work\nreports a study of decoders based on convolutional neural networks, tested on\ndifferent code distances and noise models. The results show that decoders based\non convolutional neural networks have good performance and can adapt to\ndifferent noise models. Moreover, explainable machine learning techniques have\nbeen applied to the neural network of the decoder to better understand the\nbehaviour and errors of the algorithm, in order to produce a more robust and\nperforming algorithm.",
            "author": [
                "Simone Bordoni",
                "Stefano Giagu"
            ],
            "link": [
                "http://dx.doi.org/10.1007/s11128-023-03898-2",
                "http://arxiv.org/abs/2312.03508v1",
                "http://arxiv.org/pdf/2312.03508v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03506v1",
            "title": "Task-Parameterized Imitation Learning with Time-Sensitive Constraints",
            "updated": "2023-12-06T14:06:40Z",
            "published": "2023-12-06T14:06:40Z",
            "summary": "Programming a robot manipulator should be as intuitive as possible. To\nachieve that, the paradigm of teaching motion skills by providing few\ndemonstrations has become widely popular in recent years. Probabilistic\nversions thereof take into account the uncertainty given by the distribution of\nthe training data. However, precise execution of start-, via-, and end-poses at\ngiven times can not always be guaranteed. This limits the technology transfer\nto industrial application. To address this problem, we propose a novel\nconstrained formulation of the Expectation Maximization algorithm for learning\nGaussian Mixture Models (GMM) on Riemannian Manifolds. Our approach applies to\nprobabilistic imitation learning and extends also to the well-established\nTP-GMM framework with Task-Parameterization. It allows to prescribe\nend-effector poses at defined execution times, for instance for precise pick &\nplace scenarios. The probabilistic approach is compared with state-of-the-art\nlearning-from-demonstration methods using the KUKA LBR iiwa robot. The reader\nis encouraged to watch the accompanying video available at\nhttps://youtu.be/JMI1YxtN9C0",
            "author": [
                "Julian Richter",
                "Jo\u00e3o Oliveira",
                "Christian Scheurer",
                "Jochen Steil",
                "Niels Dehio"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03506v1",
                "http://arxiv.org/pdf/2312.03506v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03503v1",
            "title": "Transfer learning for galaxy feature detection: Finding Giant\n  Star-forming Clumps in low redshift galaxies using Faster R-CNN",
            "updated": "2023-12-06T13:59:48Z",
            "published": "2023-12-06T13:59:48Z",
            "summary": "Giant Star-forming Clumps (GSFCs) are areas of intensive star-formation that\nare commonly observed in high-redshift (z>1) galaxies but their formation and\nrole in galaxy evolution remain unclear. High-resolution observations of\nlow-redshift clumpy galaxy analogues are rare and restricted to a limited set\nof galaxies but the increasing availability of wide-field galaxy survey data\nmakes the detection of large clumpy galaxy samples increasingly feasible. Deep\nLearning, and in particular CNNs, have been successfully applied to image\nclassification tasks in astrophysical data analysis. However, one application\nof DL that remains relatively unexplored is that of automatically identifying\nand localising specific objects or features in astrophysical imaging data. In\nthis paper we demonstrate the feasibility of using Deep learning-based object\ndetection models to localise GSFCs in astrophysical imaging data. We apply the\nFaster R-CNN object detection framework (FRCNN) to identify GSFCs in low\nredshift (z<0.3) galaxies. Unlike other studies, we train different FRCNN\nmodels not on simulated images with known labels but on real observational data\nthat was collected by the Sloan Digital Sky Survey Legacy Survey and labelled\nby volunteers from the citizen science project `Galaxy Zoo: Clump Scout'. The\nFRCNN model relies on a CNN component as a `backbone' feature extractor. We\nshow that CNNs, that have been pre-trained for image classification using\nastrophysical images, outperform those that have been pre-trained on\nterrestrial images. In particular, we compare a domain-specific CNN -`Zoobot' -\nwith a generic classification backbone and find that Zoobot achieves higher\ndetection performance and also requires smaller training data sets to do so.\nOur final model is capable of producing GSFC detections with a completeness and\npurity of >=0.8 while only being trained on ~5,000 galaxy images.",
            "author": [
                "J\u00fcrgen Popp",
                "Hugh Dickinson",
                "Stephen Serjeant",
                "Mike Walmsley",
                "Dominic Adams",
                "Kameswara Mantha",
                "Vihang Mehta",
                "James Dawson"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03503v1",
                "http://arxiv.org/pdf/2312.03503v1"
            ],
            "primary_category": "astro-ph.GA",
            "category": [
                "astro-ph.GA",
                "astro-ph.IM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03497v1",
            "title": "Speculative Exploration on the Concept of Artificial Agents Conducting\n  Autonomous Research",
            "updated": "2023-12-06T13:46:30Z",
            "published": "2023-12-06T13:46:30Z",
            "summary": "This paper engages in a speculative exploration of the concept of an\nartificial agent capable of conducting research. Initially, it examines how the\nact of research can be conceptually characterized, aiming to provide a starting\npoint for discussions about what it means to create such agents. The focus then\nshifts to the core components of research: question formulation, hypothesis\ngeneration, and hypothesis verification. This discussion includes a\nconsideration of the potential and challenges associated with enabling machines\nto autonomously perform these tasks. Subsequently, this paper briefly considers\nthe overlapping themes and interconnections that underlie them. Finally, the\npaper presents preliminary thoughts on prototyping as an initial step towards\nuncovering the challenges involved in developing these research-capable agents.",
            "author": [
                "Shiro Takagi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03497v1",
                "http://arxiv.org/pdf/2312.03497v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03793v1",
            "title": "AnimateZero: Video Diffusion Models are Zero-Shot Image Animators",
            "updated": "2023-12-06T13:39:35Z",
            "published": "2023-12-06T13:39:35Z",
            "summary": "Large-scale text-to-video (T2V) diffusion models have great progress in\nrecent years in terms of visual quality, motion and temporal consistency.\nHowever, the generation process is still a black box, where all attributes\n(e.g., appearance, motion) are learned and generated jointly without precise\ncontrol ability other than rough text descriptions. Inspired by image animation\nwhich decouples the video as one specific appearance with the corresponding\nmotion, we propose AnimateZero to unveil the pre-trained text-to-video\ndiffusion model, i.e., AnimateDiff, and provide more precise appearance and\nmotion control abilities for it. For appearance control, we borrow intermediate\nlatents and their features from the text-to-image (T2I) generation for ensuring\nthe generated first frame is equal to the given generated image. For temporal\ncontrol, we replace the global temporal attention of the original T2V model\nwith our proposed positional-corrected window attention to ensure other frames\nalign with the first frame well. Empowered by the proposed methods, AnimateZero\ncan successfully control the generating progress without further training. As a\nzero-shot image animator for given images, AnimateZero also enables multiple\nnew applications, including interactive video generation and real image\nanimation. The detailed experiments demonstrate the effectiveness of the\nproposed method in both T2V and related applications.",
            "author": [
                "Jiwen Yu",
                "Xiaodong Cun",
                "Chenyang Qi",
                "Yong Zhang",
                "Xintao Wang",
                "Ying Shan",
                "Jian Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03793v1",
                "http://arxiv.org/pdf/2312.03793v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03792v1",
            "title": "PCDP-SGD: Improving the Convergence of Differentially Private SGD via\n  Projection in Advance",
            "updated": "2023-12-06T13:34:15Z",
            "published": "2023-12-06T13:34:15Z",
            "summary": "The paradigm of Differentially Private SGD~(DP-SGD) can provide a theoretical\nguarantee for training data in both centralized and federated settings.\nHowever, the utility degradation caused by DP-SGD limits its wide application\nin high-stakes tasks, such as medical image diagnosis. In addition to the\nnecessary perturbation, the convergence issue is attributed to the information\nloss on the gradient clipping. In this work, we propose a general framework\nPCDP-SGD, which aims to compress redundant gradient norms and preserve more\ncrucial top gradient components via projection operation before gradient\nclipping. Additionally, we extend PCDP-SGD as a fundamental component in\ndifferential privacy federated learning~(DPFL) for mitigating the data\nheterogeneous challenge and achieving efficient communication. We prove that\npre-projection enhances the convergence of DP-SGD by reducing the dependence of\nclipping error and bias to a fraction of the top gradient eigenspace, and in\ntheory, limits cross-client variance to improve the convergence under\nheterogeneous federation. Experimental results demonstrate that PCDP-SGD\nachieves higher accuracy compared with state-of-the-art DP-SGD variants in\ncomputer vision tasks. Moreover, PCDP-SGD outperforms current federated\nlearning frameworks when DP is guaranteed on local training sets.",
            "author": [
                "Haichao Sha",
                "Ruixuan Liu",
                "Yixuan Liu",
                "Hong Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03792v1",
                "http://arxiv.org/pdf/2312.03792v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03492v1",
            "title": "Learning From Scenarios for Stochastic Repairable Scheduling",
            "updated": "2023-12-06T13:32:17Z",
            "published": "2023-12-06T13:32:17Z",
            "summary": "When optimizing problems with uncertain parameter values in a linear\nobjective, decision-focused learning enables end-to-end learning of these\nvalues. We are interested in a stochastic scheduling problem, in which\nprocessing times are uncertain, which brings uncertain values in the\nconstraints, and thus repair of an initial schedule may be needed. Historical\nrealizations of the stochastic processing times are available. We show how\nexisting decision-focused learning techniques based on stochastic smoothing can\nbe adapted to this scheduling problem. We include an extensive experimental\nevaluation to investigate in which situations decision-focused learning\noutperforms the state of the art for such situations: scenario-based stochastic\noptimization.",
            "author": [
                "Kim van den Houten",
                "David M. J. Tax",
                "Esteban Freydell",
                "Mathijs de Weerdt"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03492v1",
                "http://arxiv.org/pdf/2312.03492v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03491v1",
            "title": "Schrodinger Bridges Beat Diffusion Models on Text-to-Speech Synthesis",
            "updated": "2023-12-06T13:31:55Z",
            "published": "2023-12-06T13:31:55Z",
            "summary": "In text-to-speech (TTS) synthesis, diffusion models have achieved promising\ngeneration quality. However, because of the pre-defined data-to-noise diffusion\nprocess, their prior distribution is restricted to a noisy representation,\nwhich provides little information of the generation target. In this work, we\npresent a novel TTS system, Bridge-TTS, making the first attempt to substitute\nthe noisy Gaussian prior in established diffusion-based TTS methods with a\nclean and deterministic one, which provides strong structural information of\nthe target. Specifically, we leverage the latent representation obtained from\ntext input as our prior, and build a fully tractable Schrodinger bridge between\nit and the ground-truth mel-spectrogram, leading to a data-to-data process.\nMoreover, the tractability and flexibility of our formulation allow us to\nempirically study the design spaces such as noise schedules, as well as to\ndevelop stochastic and deterministic samplers. Experimental results on the\nLJ-Speech dataset illustrate the effectiveness of our method in terms of both\nsynthesis quality and sampling efficiency, significantly outperforming our\ndiffusion counterpart Grad-TTS in 50-step/1000-step synthesis and strong fast\nTTS models in few-step scenarios. Project page: https://bridge-tts.github.io/",
            "author": [
                "Zehua Chen",
                "Guande He",
                "Kaiwen Zheng",
                "Xu Tan",
                "Jun Zhu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03491v1",
                "http://arxiv.org/pdf/2312.03491v1"
            ],
            "primary_category": "cs.SD",
            "category": [
                "cs.SD",
                "cs.LG",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03488v1",
            "title": "Modeling Aggregate Downwash Forces for Dense Multirotor Flight",
            "updated": "2023-12-06T13:30:49Z",
            "published": "2023-12-06T13:30:49Z",
            "summary": "Dense formation flight with multirotor swarms is a powerful, nature-inspired\nflight regime with numerous applications in the realworld. However, when\nmultirotors fly in close vertical proximity to each other, the propeller\ndownwash from the vehicles can have a destabilising effect on each other.\nUnfortunately, even in a homogeneous team, an accurate model of downwash forces\nfrom one vehicle is unlikely to be sufficient for predicting aggregate forces\nfrom multiple vehicles in formation.\n  In this work, we model the interaction patterns produced by one or more\nvehicles flying in close proximity to an ego-vehicle. We first present an\nexperimental test rig designed to capture 6-DOF exogenic forces acting on a\nmultirotor frame. We then study and characterize these measured forces as a\nfunction of the relative states of two multirotors flying various patterns in\nits vicinity.\n  Our analysis captures strong non-linearities present in the aggregation of\nthese interactions. Then, by modeling the formation as a graph, we present a\nnovel approach for learning the force aggregation function, and contrast it\nagainst simpler linear models. Finally, we explore how our proposed models\ngeneralize when a fourth vehicle is added to the formation.",
            "author": [
                "Jennifer Gielis",
                "Ajay Shankar",
                "Ryan Kortvelesy",
                "Amanda Prorok"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03488v1",
                "http://arxiv.org/pdf/2312.03488v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.MA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03486v1",
            "title": "New dwarf galaxy candidates in the sphere of influence of the Sombrero\n  galaxy",
            "updated": "2023-12-06T13:29:58Z",
            "published": "2023-12-06T13:29:58Z",
            "summary": "We report the discovery of 40 new satellite dwarf galaxy candidates in the\nsphere of influence of the Sombrero galaxy (M104) the most luminous galaxy in\nthe Local Volume. Using the Subaru Hyper Suprime-Cam, we surveyed 14.4 square\ndegrees of its surroundings, extending to the virial radius. Visual inspection\nof the deep images and GALFIT modelling yielded a galaxy sample highly complete\ndown to $M_{g}\\sim-9$ ($L_{g}\\sim3\\times 10^{5}\\,L_\\odot$) and spanning\nmagnitudes $-16.4 < M_g < -8$ and half-light radii $50\\,pc\\ <\\ r_e\\ <\\\n1600\\,pc$ assuming the distance of M104. These 40 new, out of which 27 are\ngroup members with high confidence, double the number of potential satellites\nof M104 within the virial radius, placing it among the richest hosts in the\nLocal Volume. Using a Principle Component Analysis (PCA), we find that the\nentire sample of candidates consistent with an almost circular on-sky\ndistribution, more circular than any comparable environment found in the\nIllustris TNG100-1 simulation. However the distribution of the high probability\nsample is more oblate and consistent with the simulation. The cumulative\nsatellite luminosity function is broadly consistent with analogues from the\nsimulation, albeit it contains no bright satellite with $M_{g}<-16.4$\n($L_{g}\\sim3 \\times 10^{8}\\,L_\\odot$), a $2.3\\,\\sigma$ occurrence. Follow-up\nspectroscopy to confirm group membership will begin to demonstrate how these\nsystems can act as probes of the structure and formation history of the halo of\nM104.",
            "author": [
                "Ethan Crosby",
                "Helmut Jerjen",
                "Oliver M\u00fcller",
                "Marcel S. Pawlowski",
                "Mario Mateo",
                "Federico Lelli"
            ],
            "link": [
                "http://dx.doi.org/10.1093/mnras/stad3741",
                "http://arxiv.org/abs/2312.03486v1",
                "http://arxiv.org/pdf/2312.03486v1"
            ],
            "primary_category": "astro-ph.GA",
            "category": [
                "astro-ph.GA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03485v1",
            "title": "Precision of Individual Shapley Value Explanations",
            "updated": "2023-12-06T13:29:23Z",
            "published": "2023-12-06T13:29:23Z",
            "summary": "Shapley values are extensively used in explainable artificial intelligence\n(XAI) as a framework to explain predictions made by complex machine learning\n(ML) models. In this work, we focus on conditional Shapley values for\npredictive models fitted to tabular data and explain the prediction\n$f(\\boldsymbol{x}^{*})$ for a single observation $\\boldsymbol{x}^{*}$ at the\ntime. Numerous Shapley value estimation methods have been proposed and\nempirically compared on an average basis in the XAI literature. However, less\nfocus has been devoted to analyzing the precision of the Shapley value\nexplanations on an individual basis. We extend our work in Olsen et al. (2023)\nby demonstrating and discussing that the explanations are systematically less\nprecise for observations on the outer region of the training data distribution\nfor all used estimation methods. This is expected from a statistical point of\nview, but to the best of our knowledge, it has not been systematically\naddressed in the Shapley value literature. This is crucial knowledge for\nShapley values practitioners, who should be more careful in applying these\nobservations' corresponding Shapley value explanations.",
            "author": [
                "Lars Henry Berge Olsen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03485v1",
                "http://arxiv.org/pdf/2312.03485v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG",
                "stat.AP",
                "stat.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03483v1",
            "title": "Exploring Answer Information Methods for Question Generation with\n  Transformers",
            "updated": "2023-12-06T13:26:16Z",
            "published": "2023-12-06T13:26:16Z",
            "summary": "There has been a lot of work in question generation where different methods\nto provide target answers as input, have been employed. This experimentation\nhas been mostly carried out for RNN based models. We use three different\nmethods and their combinations for incorporating answer information and explore\ntheir effect on several automatic evaluation metrics. The methods that are used\nare answer prompting, using a custom product method using answer embeddings and\nencoder outputs, choosing sentences from the input paragraph that have answer\nrelated information, and using a separate cross-attention attention block in\nthe decoder which attends to the answer. We observe that answer prompting\nwithout any additional modes obtains the best scores across rouge, meteor\nscores. Additionally, we use a custom metric to calculate how many of the\ngenerated questions have the same answer, as the answer which is used to\ngenerate them.",
            "author": [
                "Talha Chafekar",
                "Aafiya Hussain",
                "Grishma Sharma",
                "Deepak Sharma"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03483v1",
                "http://arxiv.org/pdf/2312.03483v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03481v1",
            "title": "Experimental demonstration of mice tumor control with a\n  laser-accelerated high-energy electron radiotherapy prototype",
            "updated": "2023-12-06T13:21:52Z",
            "published": "2023-12-06T13:21:52Z",
            "summary": "Radiotherapy using very-high-energy electron (VHEE) beams (50-300 MeV) has\nattracted considerable attention due to its advantageous dose deposition\ncharacteristics, enabling deep penetration and the potential for ultra-high\ndose rate treatment. One promising approach to compactly delivering these high\nenergy electron beams in a cost-effective manner is laser wakefield\nacceleration (LWFA), which offers ultra-strong accelerating gradients. However,\nthe transition from this concept to a functional machine intended for tumor\ntreatment is still being investigated. Here we present the first self-developed\nprototype for LWFA-based VHEE radiotherapy, exhibiting high compactness\n(occupying less than 5 square meters) and high operational stability (validated\nover a period of one month). Subsequently, we employed this device to irradiate\na tumor implanted in a mouse model. Following a dose delivery of $5.8\\pm0.2$ Gy\nwith precise tumor conformity, all irradiated mice exhibited pronounced control\nof tumor growth. For comparison, this tumor-control efficacy was similar to\nthat achieved using commercial X-ray radiotherapy equipment operating at\nequivalent doses. These results demonstrate the potential of a compact\nlaser-driven VHEE system for preclinical studies involving small animal models\nand its promising prospects for future clinical translation in cancer therapy.",
            "author": [
                "Zhiyuan Guo",
                "Shuang Liu",
                "Bing Zhou",
                "Junqi Liu",
                "Haiyang Wang",
                "Yang Wan",
                "Yifei Pi",
                "Xiaoyan Wang",
                "Yingyi Mo",
                "Bo Guo",
                "Jianfei Hua",
                "Wei Lu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03481v1",
                "http://arxiv.org/pdf/2312.03481v1"
            ],
            "primary_category": "physics.med-ph",
            "category": [
                "physics.med-ph",
                "physics.acc-ph",
                "physics.optics",
                "physics.plasm-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03477v1",
            "title": "From Detection to Action Recognition: An Edge-Based Pipeline for Robot\n  Human Perception",
            "updated": "2023-12-06T13:10:02Z",
            "published": "2023-12-06T13:10:02Z",
            "summary": "Mobile service robots are proving to be increasingly effective in a range of\napplications, such as healthcare, monitoring Activities of Daily Living (ADL),\nand facilitating Ambient Assisted Living (AAL). These robots heavily rely on\nHuman Action Recognition (HAR) to interpret human actions and intentions.\nHowever, for HAR to function effectively on service robots, it requires prior\nknowledge of human presence (human detection) and identification of individuals\nto monitor (human tracking). In this work, we propose an end-to-end pipeline\nthat encompasses the entire process, starting from human detection and\ntracking, leading to action recognition. The pipeline is designed to operate in\nnear real-time while ensuring all stages of processing are performed on the\nedge, reducing the need for centralised computation. To identify the most\nsuitable models for our mobile robot, we conducted a series of experiments\ncomparing state-of-the-art solutions based on both their detection performance\nand efficiency. To evaluate the effectiveness of our proposed pipeline, we\nproposed a dataset comprising daily household activities. By presenting our\nfindings and analysing the results, we demonstrate the efficacy of our approach\nin enabling mobile robots to understand and respond to human behaviour in\nreal-world scenarios relying mainly on the data from their RGB cameras.",
            "author": [
                "Petros Toupas",
                "Georgios Tsamis",
                "Dimitrios Giakoumis",
                "Konstantinos Votis",
                "Dimitrios Tzovaras"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03477v1",
                "http://arxiv.org/pdf/2312.03477v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03475v1",
            "title": "Molecule Joint Auto-Encoding: Trajectory Pretraining with 2D and 3D\n  Diffusion",
            "updated": "2023-12-06T12:58:37Z",
            "published": "2023-12-06T12:58:37Z",
            "summary": "Recently, artificial intelligence for drug discovery has raised increasing\ninterest in both machine learning and chemistry domains. The fundamental\nbuilding block for drug discovery is molecule geometry and thus, the molecule's\ngeometrical representation is the main bottleneck to better utilize machine\nlearning techniques for drug discovery. In this work, we propose a pretraining\nmethod for molecule joint auto-encoding (MoleculeJAE). MoleculeJAE can learn\nboth the 2D bond (topology) and 3D conformation (geometry) information, and a\ndiffusion process model is applied to mimic the augmented trajectories of such\ntwo modalities, based on which, MoleculeJAE will learn the inherent chemical\nstructure in a self-supervised manner. Thus, the pretrained geometrical\nrepresentation in MoleculeJAE is expected to benefit downstream\ngeometry-related tasks. Empirically, MoleculeJAE proves its effectiveness by\nreaching state-of-the-art performance on 15 out of 20 tasks by comparing it\nwith 12 competitive baselines.",
            "author": [
                "Weitao Du",
                "Jiujiu Chen",
                "Xuecang Zhang",
                "Zhiming Ma",
                "Shengchao Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03475v1",
                "http://arxiv.org/pdf/2312.03475v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "q-bio.BM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03466v1",
            "title": "Search Strategies for Self-driving Laboratories with Pending Experiments",
            "updated": "2023-12-06T12:41:53Z",
            "published": "2023-12-06T12:41:53Z",
            "summary": "Self-driving laboratories (SDLs) consist of multiple stations that perform\nmaterial synthesis and characterisation tasks. To minimize station downtime and\nmaximize experimental throughput, it is practical to run experiments in\nasynchronous parallel, in which multiple experiments are being performed at\nonce in different stages. Asynchronous parallelization of experiments, however,\nintroduces delayed feedback (i.e. \"pending experiments\"), which is known to\nreduce Bayesian optimiser performance. Here, we build a simulator for a\nmulti-stage SDL and compare optimisation strategies for dealing with delayed\nfeedback and asynchronous parallelized operation. Using data from a real SDL,\nwe build a ground truth Bayesian optimisation simulator from 177 previously run\nexperiments for maximizing the conductivity of functional coatings. We then\ncompare search strategies such as expected improvement, noisy expected\nimprovement, 4-mode exploration and random sampling. We evaluate their\nperformance in terms of amount of delay and problem dimensionality. Our\nsimulation results showcase the trade-off between the asynchronous parallel\noperation and delayed feedback.",
            "author": [
                "Hao Wen",
                "Jakob Zeitler",
                "Connor Rupnow"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03466v1",
                "http://arxiv.org/pdf/2312.03466v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03464v1",
            "title": "Subnetwork-to-go: Elastic Neural Network with Dynamic Training and\n  Customizable Inference",
            "updated": "2023-12-06T12:40:06Z",
            "published": "2023-12-06T12:40:06Z",
            "summary": "Deploying neural networks to different devices or platforms is in general\nchallenging, especially when the model size is large or model complexity is\nhigh. Although there exist ways for model pruning or distillation, it is\ntypically required to perform a full round of model training or finetuning\nprocedure in order to obtain a smaller model that satisfies the model size or\ncomplexity constraints. Motivated by recent works on dynamic neural networks,\nwe propose a simple way to train a large network and flexibly extract a\nsubnetwork from it given a model size or complexity constraint during\ninference. We introduce a new way to allow a large model to be trained with\ndynamic depth and width during the training phase, and after the large model is\ntrained we can select a subnetwork from it with arbitrary depth and width\nduring the inference phase with a relatively better performance compared to\ntraining the subnetwork independently from scratch. Experiment results on a\nmusic source separation model show that our proposed method can effectively\nimprove the separation performance across different subnetwork sizes and\ncomplexities with a single large model, and training the large model takes\nsignificantly shorter time than training all the different subnetworks.",
            "author": [
                "Kai Li",
                "Yi Luo"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03464v1",
                "http://arxiv.org/pdf/2312.03464v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.SD",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03463v1",
            "title": "DBCopilot: Scaling Natural Language Querying to Massive Databases",
            "updated": "2023-12-06T12:37:28Z",
            "published": "2023-12-06T12:37:28Z",
            "summary": "Text-to-SQL simplifies database interactions by enabling non-experts to\nconvert their natural language (NL) questions into Structured Query Language\n(SQL) queries. While recent advances in large language models (LLMs) have\nimproved the zero-shot text-to-SQL paradigm, existing methods face scalability\nchallenges when dealing with massive, dynamically changing databases. This\npaper introduces DBCopilot, a framework that addresses these challenges by\nemploying a compact and flexible copilot model for routing across massive\ndatabases. Specifically, DBCopilot decouples the text-to-SQL process into\nschema routing and SQL generation, leveraging a lightweight\nsequence-to-sequence neural network-based router to formulate database\nconnections and navigate natural language questions through databases and\ntables. The routed schemas and questions are then fed into LLMs for efficient\nSQL generation. Furthermore, DBCopilot also introduced a reverse\nschema-to-question generation paradigm, which can learn and adapt the router\nover massive databases automatically without requiring manual intervention.\nExperimental results demonstrate that DBCopilot is a scalable and effective\nsolution for real-world text-to-SQL tasks, providing a significant advancement\nin handling large-scale schemas.",
            "author": [
                "Tianshu Wang",
                "Hongyu Lin",
                "Xianpei Han",
                "Le Sun",
                "Xiaoyang Chen",
                "Hao Wang",
                "Zhenyu Zeng"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03463v1",
                "http://arxiv.org/pdf/2312.03463v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.DB",
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03458v1",
            "title": "Think from Words(TFW): Initiating Human-Like Cognition in Large Language\n  Models Through Think from Words for Japanese Text-level Classification",
            "updated": "2023-12-06T12:34:46Z",
            "published": "2023-12-06T12:34:46Z",
            "summary": "The proliferation of Large Language Models (LLMs) has spurred extensive\nresearch into LLM-related Prompt investigations, such as Instruction Learning\n(IL), In-context Learning (ICL), and Chain-of-Thought (CoT). These approaches\naim to improve LLMs' responses by enabling them to provide concise statements\nor examples for deeper contemplation when addressing questions. However,\nindependent thinking by LLMs can introduce variability in their thought\nprocesses, leading to potential inaccuracies. In response, our study seeks to\nbridge the gap between LLM and human-like thinking processes, recognizing that\ntext comprehension begins with understanding individual words. To tackle this\nchallenge, we have expanded the CoT method to cater to a specific domain. Our\napproach, known as \"Think from Words\" (TFW), initiates the comprehension\nprocess at the word level and then extends it to encompass the entire text. We\nalso propose \"TFW with Extra word-level information\" (TFW Extra), augmenting\ncomprehension with additional word-level data. To assess our methods, we employ\ntext classification on six Japanese datasets comprising text-level and\nword-level elements. Our findings not only validate the effectiveness of TFW\nbut also shed light on the impact of various word-level information types on\nLLMs' text comprehension, offering insights into their potential to cause\nmisinterpretations and errors in the overall comprehension of the final text.",
            "author": [
                "Chengguang Gan",
                "Qinghao Zhang",
                "Tatsunori Mori"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03458v1",
                "http://arxiv.org/pdf/2312.03458v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03455v1",
            "title": "Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence\n  of Training Data",
            "updated": "2023-12-06T12:27:25Z",
            "published": "2023-12-06T12:27:25Z",
            "summary": "Perceptual metrics are traditionally used to evaluate the quality of natural\nsignals, such as images and audio. They are designed to mimic the perceptual\nbehaviour of human observers and usually reflect structures found in natural\nsignals. This motivates their use as loss functions for training generative\nmodels such that models will learn to capture the structure held in the metric.\nWe take this idea to the extreme in the audio domain by training a compressive\nautoencoder to reconstruct uniform noise, in lieu of natural data. We show that\ntraining with perceptual losses improves the reconstruction of spectrograms and\nre-synthesized audio at test time over models trained with a standard Euclidean\nloss. This demonstrates better generalisation to unseen natural signals when\nusing perceptual metrics.",
            "author": [
                "Tashi Namgyal",
                "Alexander Hepburn",
                "Raul Santos-Rodriguez",
                "Valero Laparra",
                "Jesus Malo"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03455v1",
                "http://arxiv.org/pdf/2312.03455v1"
            ],
            "primary_category": "cs.SD",
            "category": [
                "cs.SD",
                "cs.AI",
                "cs.CV",
                "cs.LG",
                "eess.AS",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03450v1",
            "title": "Variational Autoencoder for Channel Estimation: Real-World Measurement\n  Insights",
            "updated": "2023-12-06T12:17:15Z",
            "published": "2023-12-06T12:17:15Z",
            "summary": "This work utilizes a variational autoencoder for channel estimation and\nevaluates it on real-world measurements. The estimator is trained solely on\nnoisy channel observations and parameterizes an approximation to the mean\nsquared error-optimal estimator by learning observation-dependent conditional\nfirst and second moments. The proposed estimator significantly outperforms\nrelated state-of-the-art estimators on real-world measurements. We investigate\nthe effect of pre-training with synthetic data and find that the proposed\nestimator exhibits comparable results to the related estimators if trained on\nsynthetic data and evaluated on the measurement data. Furthermore, pre-training\non synthetic data also helps to reduce the required measurement training\ndataset size.",
            "author": [
                "Michael Baur",
                "Benedikt B\u00f6ck",
                "Nurettin Turan",
                "Wolfgang Utschick"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03450v1",
                "http://arxiv.org/pdf/2312.03450v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP",
                "cs.IT",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03447v1",
            "title": "Quantum-Inspired Neural Network Model of Optical Illusions",
            "updated": "2023-12-06T12:10:56Z",
            "published": "2023-12-06T12:10:56Z",
            "summary": "Ambiguous optical illusions have been a paradigmatic object of fascination,\nresearch and inspiration in arts, psychology and video games. However, accurate\ncomputational models of perception of ambiguous figures have been elusive. In\nthis paper, we design and train a deep neural network model to simulate the\nhuman's perception of the Necker cube, an ambiguous drawing with several\nalternating possible interpretations. Defining the weights of the neural\nnetwork connection using a quantum generator of truly random numbers, in\nagreement with the emerging concepts of quantum artificial intelligence and\nquantum cognition we reveal that the actual perceptual state of the Necker cube\nis a qubit-like superposition of the two fundamental perceptual states\npredicted by classical theories. Our results will find applications in video\ngames and virtual reality systems employed for training of astronauts and\noperators of unmanned aerial vehicles. They will also be useful for researchers\nworking in the fields of machine learning and vision, psychology of perception\nand quantum-mechanical models of human mind and decision-making.",
            "author": [
                "Ivan S. Maksymov"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03447v1",
                "http://arxiv.org/pdf/2312.03447v1"
            ],
            "primary_category": "physics.soc-ph",
            "category": [
                "physics.soc-ph",
                "cs.AI",
                "cs.CV",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03789v1",
            "title": "Comparative Analysis of Multilingual Text Classification &\n  Identification through Deep Learning and Embedding Visualization",
            "updated": "2023-12-06T12:03:27Z",
            "published": "2023-12-06T12:03:27Z",
            "summary": "This research conducts a comparative study on multilingual text\nclassification methods, utilizing deep learning and embedding visualization.\nThe study employs LangDetect, LangId, FastText, and Sentence Transformer on a\ndataset encompassing 17 languages. It explores dimensionality's impact on\nclustering, revealing FastText's clearer clustering in 2D visualization due to\nits extensive multilingual corpus training. Notably, the FastText multi-layer\nperceptron model achieved remarkable accuracy, precision, recall, and F1 score,\noutperforming the Sentence Transformer model. The study underscores the\neffectiveness of these techniques in multilingual text classification,\nemphasizing the importance of large multilingual corpora for training\nembeddings. It lays the groundwork for future research and assists\npractitioners in developing language detection and classification systems.\nAdditionally, it includes the comparison of multi-layer perceptron, LSTM, and\nConvolution models for classification.",
            "author": [
                "Arinjay Wyawhare"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03789v1",
                "http://arxiv.org/pdf/2312.03789v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03443v1",
            "title": "Data-driven Crop Growth Simulation on Time-varying Generated Images\n  using Multi-conditional Generative Adversarial Networks",
            "updated": "2023-12-06T11:54:50Z",
            "published": "2023-12-06T11:54:50Z",
            "summary": "Image-based crop growth modeling can substantially contribute to precision\nagriculture by revealing spatial crop development over time, which allows an\nearly and location-specific estimation of relevant future plant traits, such as\nleaf area or biomass. A prerequisite for realistic and sharp crop image\ngeneration is the integration of multiple growth-influencing conditions in a\nmodel, such as an image of an initial growth stage, the associated growth time,\nand further information about the field treatment. We present a two-stage\nframework consisting first of an image prediction model and second of a growth\nestimation model, which both are independently trained. The image prediction\nmodel is a conditional Wasserstein generative adversarial network (CWGAN). In\nthe generator of this model, conditional batch normalization (CBN) is used to\nintegrate different conditions along with the input image. This allows the\nmodel to generate time-varying artificial images dependent on multiple\ninfluencing factors of different kinds. These images are used by the second\npart of the framework for plant phenotyping by deriving plant-specific traits\nand comparing them with those of non-artificial (real) reference images. For\nvarious crop datasets, the framework allows realistic, sharp image predictions\nwith a slight loss of quality from short-term to long-term predictions.\nSimulations of varying growth-influencing conditions performed with the trained\nframework provide valuable insights into how such factors relate to crop\nappearances, which is particularly useful in complex, less explored crop\nmixture systems. Further results show that adding process-based simulated\nbiomass as a condition increases the accuracy of the derived phenotypic traits\nfrom the predicted images. This demonstrates the potential of our framework to\nserve as an interface between an image- and process-based crop growth model.",
            "author": [
                "Lukas Drees",
                "Dereje T. Demie",
                "Madhuri R. Paul",
                "Johannes Leonhardt",
                "Sabine J. Seidel",
                "Thomas F. D\u00f6ring",
                "Ribana Roscher"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03443v1",
                "http://arxiv.org/pdf/2312.03443v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03442v1",
            "title": "High-Quality Facial Geometry and Appearance Capture at Home",
            "updated": "2023-12-06T11:51:06Z",
            "published": "2023-12-06T11:51:06Z",
            "summary": "Facial geometry and appearance capture have demonstrated tremendous success\nin 3D scanning real humans in studios. Recent works propose to democratize this\ntechnique while keeping the results high quality. However, they are still\ninconvenient for daily usage. In addition, they focus on an easier problem of\nonly capturing facial skin. This paper proposes a novel method for high-quality\nface capture, featuring an easy-to-use system and the capability to model the\ncomplete face with skin, mouth interior, hair, and eyes. We reconstruct facial\ngeometry and appearance from a single co-located smartphone flashlight sequence\ncaptured in a dim room where the flashlight is the dominant light source (e.g.\nrooms with curtains or at night). To model the complete face, we propose a\nnovel hybrid representation to effectively model both eyes and other facial\nregions, along with novel techniques to learn it from images. We apply a\ncombined lighting model to compactly represent real illuminations and exploit a\nmorphable face albedo model as a reflectance prior to disentangle diffuse and\nspecular. Experiments show that our method can capture high-quality 3D\nrelightable scans.",
            "author": [
                "Yuxuan Han",
                "Junfeng Lyu",
                "Feng Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03442v1",
                "http://arxiv.org/pdf/2312.03442v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03438v1",
            "title": "On the Estimation Performance of Generalized Power Method for\n  Heteroscedastic Probabilistic PCA",
            "updated": "2023-12-06T11:41:17Z",
            "published": "2023-12-06T11:41:17Z",
            "summary": "The heteroscedastic probabilistic principal component analysis (PCA)\ntechnique, a variant of the classic PCA that considers data heterogeneity, is\nreceiving more and more attention in the data science and signal processing\ncommunities. In this paper, to estimate the underlying low-dimensional linear\nsubspace (simply called \\emph{ground truth}) from available heterogeneous data\nsamples, we consider the associated non-convex maximum-likelihood estimation\nproblem, which involves maximizing a sum of heterogeneous quadratic forms over\nan orthogonality constraint (HQPOC). We propose a first-order method --\ngeneralized power method (GPM) -- to tackle the problem and establish its\n\\emph{estimation performance} guarantee. Specifically, we show that, given a\nsuitable initialization, the distances between the iterates generated by GPM\nand the ground truth decrease at least geometrically to some threshold\nassociated with the residual part of certain \"population-residual\ndecomposition\". In establishing the estimation performance result, we prove a\nnovel local error bound property of another closely related optimization\nproblem, namely quadratic optimization with orthogonality constraint (QPOC),\nwhich is new and can be of independent interest. Numerical experiments are\nconducted to demonstrate the superior performance of GPM in both Gaussian noise\nand sub-Gaussian noise settings.",
            "author": [
                "Jinxin Wang",
                "Chonghe Jiang",
                "Huikang Liu",
                "Anthony Man-Cho So"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03438v1",
                "http://arxiv.org/pdf/2312.03438v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "eess.SP",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03437v1",
            "title": "Data-Centric Digital Agriculture: A Perspective",
            "updated": "2023-12-06T11:38:26Z",
            "published": "2023-12-06T11:38:26Z",
            "summary": "In response to the increasing global demand for food, feed, fiber, and fuel,\ndigital agriculture is rapidly evolving to meet these demands while reducing\nenvironmental impact. This evolution involves incorporating data science,\nmachine learning, sensor technologies, robotics, and new management strategies\nto establish a more sustainable agricultural framework. So far, machine\nlearning research in digital agriculture has predominantly focused on\nmodel-centric approaches, focusing on model design and evaluation. These\nefforts aim to optimize model accuracy and efficiency, often treating data as a\nstatic benchmark. Despite the availability of agricultural data and\nmethodological advancements, a saturation point has been reached, with many\nestablished machine learning methods achieving comparable levels of accuracy\nand facing similar limitations. To fully realize the potential of digital\nagriculture, it is crucial to have a comprehensive understanding of the role of\ndata in the field and to adopt data-centric machine learning. This involves\ndeveloping strategies to acquire and curate valuable data and implementing\neffective learning and evaluation strategies that utilize the intrinsic value\nof data. This approach has the potential to create accurate, generalizable, and\nadaptable machine learning methods that effectively and sustainably address\nagricultural tasks such as yield prediction, weed detection, and early disease\nidentification",
            "author": [
                "Ribana Roscher",
                "Lukas Roth",
                "Cyrill Stachniss",
                "Achim Walter"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03437v1",
                "http://arxiv.org/pdf/2312.03437v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03434v1",
            "title": "Quark-versus-gluon tagging in CMS Open Data with CWoLa and TopicFlow",
            "updated": "2023-12-06T11:28:54Z",
            "published": "2023-12-06T11:28:54Z",
            "summary": "We use the CMS Open Data to examine the performance of weakly-supervised\nlearning for tagging quark and gluon jets at the LHC. We target $Z$+jet and\ndijet events as respective quark- and gluon-enriched mixtures and derive\nsamples both from data taken in 2011 at 7 TeV, and from Monte Carlo. CWoLa and\nTopicFlow models are trained on real data and compared to fully-supervised\nclassifiers trained on simulation. In order to obtain estimates for the\ndiscrimination power in real data, we consider three different estimates of the\nquark/gluon mixture fractions in the data. Compared to when the models are\nevaluated on simulation, we find reversed rankings for the fully- and\nweakly-supervised approaches. Further, these rankings based on data are robust\nto the estimate of the mixture fraction in the test set. Finally, we use\nTopicFlow to smooth statistical fluctuations in the small testing set, and to\nprovide uncertainty on the performance in real data.",
            "author": [
                "Matthew J. Dolan",
                "John Gargalionis",
                "Ayodele Ore"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03434v1",
                "http://arxiv.org/pdf/2312.03434v1"
            ],
            "primary_category": "hep-ph",
            "category": [
                "hep-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03430v1",
            "title": "ShareCMP: Polarization-Aware RGB-P Semantic Segmentation",
            "updated": "2023-12-06T11:25:40Z",
            "published": "2023-12-06T11:25:40Z",
            "summary": "Multimodal semantic segmentation is developing rapidly, but the modality of\nRGB-Polarization remains underexplored. To delve into this problem, we\nconstruct a UPLight RGB-P segmentation benchmark with 12 typical underwater\nsemantic classes which provides data support for Autonomous Underwater Vehicles\n(AUVs) to perform special perception tasks. In this work, we design the\nShareCMP, an RGB-P semantic segmentation framework with a shared dual-branch\narchitecture, which reduces the number of parameters by about 26-33% compared\nto previous dual-branch models. It encompasses a Polarization Generate\nAttention (PGA) module designed to generate polarization modal images with\nricher polarization properties for the encoder. In addition, we introduce the\nClass Polarization-Aware Loss (CPALoss) to improve the learning and\nunderstanding of the encoder for polarization modal information and to optimize\nthe PGA module. With extensive experiments on a total of three RGB-P\nbenchmarks, our ShareCMP achieves state-of-the-art performance in mIoU with\nfewer parameters on the UPLight (92.45%), ZJU (92.7%), and MCubeS (50.99%)\ndatasets. The code is available at https://github.com/LEFTeyex/ShareCMP.",
            "author": [
                "Zhuoyan Liu",
                "Bo Wang",
                "Lizhi Wang",
                "Chenyu Mao",
                "Ye Li"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03430v1",
                "http://arxiv.org/pdf/2312.03430v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "I.4.6"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03788v1",
            "title": "SmoothQuant+: Accurate and Efficient 4-bit Post-Training\n  WeightQuantization for LLM",
            "updated": "2023-12-06T11:10:55Z",
            "published": "2023-12-06T11:10:55Z",
            "summary": "Large language models (LLMs) have shown remarkable capabilities in various\ntasks. However their huge model size and the consequent demand for\ncomputational and memory resources also pose challenges to model deployment.\nCurrently, 4-bit post-training quantization (PTQ) has achieved some success in\nLLMs, reducing the memory footprint by approximately 75% compared to FP16\nmodels, albeit with some accuracy loss. In this paper, we propose SmoothQuant+,\nan accurate and efficient 4-bit weight-only PTQ that requires no additional\ntraining, which enables lossless in accuracy for LLMs for the first time. Based\non the fact that the loss of weight quantization is amplified by the activation\noutliers, SmoothQuant+ smoothes the activation outliers by channel before\nquantization, while adjusting the corresponding weights for mathematical\nequivalence, and then performs group-wise 4-bit weight quantization for linear\nlayers. We have integrated SmoothQuant+ into the vLLM framework, an advanced\nhigh-throughput inference engine specially developed for LLMs, and equipped it\nwith an efficient W4A16 CUDA kernels, so that vLLM can seamlessly support\nSmoothQuant+ 4-bit weight quantization. Our results show that, with\nSmoothQuant+, the Code Llama-34B model can be quantized and deployed on a A100\n40GB GPU, achieving lossless accuracy and a throughput increase of 1.9 to 4.0\ntimes compared to the FP16 model deployed on two A100 40GB GPUs. Moreover, the\nlatency per token is only 68% of the FP16 model deployed on two A100 40GB GPUs.\nThis is the state-of-the-art 4-bit weight quantization for LLMs as we know.",
            "author": [
                "Jiayi Pan",
                "Chengcan Wang",
                "Kaifu Zheng",
                "Yangguang Li",
                "Zhenyu Wang",
                "Bin Feng"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03788v1",
                "http://arxiv.org/pdf/2312.03788v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03419v1",
            "title": "Synthesizing Physical Backdoor Datasets: An Automated Framework\n  Leveraging Deep Generative Models",
            "updated": "2023-12-06T11:05:11Z",
            "published": "2023-12-06T11:05:11Z",
            "summary": "Backdoor attacks, representing an emerging threat to the integrity of deep\nneural networks, have garnered significant attention due to their ability to\ncompromise deep learning systems clandestinely. While numerous backdoor attacks\noccur within the digital realm, their practical implementation in real-world\nprediction systems remains limited and vulnerable to disturbances in the\nphysical world. Consequently, this limitation has given rise to the development\nof physical backdoor attacks, where trigger objects manifest as physical\nentities within the real world. However, creating the requisite dataset to\ntrain or evaluate a physical backdoor model is a daunting task, limiting the\nbackdoor researchers and practitioners from studying such physical attack\nscenarios. This paper unleashes a recipe that empowers backdoor researchers to\neffortlessly create a malicious, physical backdoor dataset based on advances in\ngenerative modeling. Particularly, this recipe involves 3 automatic modules:\nsuggesting the suitable physical triggers, generating the poisoned candidate\nsamples (either by synthesizing new samples or editing existing clean samples),\nand finally refining for the most plausible ones. As such, it effectively\nmitigates the perceived complexity associated with creating a physical backdoor\ndataset, transforming it from a daunting task into an attainable objective.\nExtensive experiment results show that datasets created by our \"recipe\" enable\nadversaries to achieve an impressive attack success rate on real physical world\ndata and exhibit similar properties compared to previous physical backdoor\nattack studies. This paper offers researchers a valuable toolkit for studies of\nphysical backdoors, all within the confines of their laboratories.",
            "author": [
                "Sze Jue Yang",
                "Chinh D. La",
                "Quang H. Nguyen",
                "Eugene Bagdasaryan",
                "Kok-Seng Wong",
                "Anh Tuan Tran",
                "Chee Seng Chan",
                "Khoa D. Doan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03419v1",
                "http://arxiv.org/pdf/2312.03419v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03415v1",
            "title": "Run LoRA Run: Faster and Lighter LoRA Implementations",
            "updated": "2023-12-06T10:54:34Z",
            "published": "2023-12-06T10:54:34Z",
            "summary": "LoRA is a technique that reduces the number of trainable parameters in a\nneural network by introducing low-rank adapters to linear layers. This\ntechnique is used both for fine-tuning (LoRA, QLoRA) and full train (ReLoRA).\nThis paper presents the RunLoRA framework for efficient implementations of LoRA\nthat significantly improves the speed of neural network training and\nfine-tuning using low-rank adapters. The proposed implementation optimizes the\ncomputation of LoRA operations based on dimensions of corresponding linear\nlayer, layer input dimensions and lora rank by choosing best forward and\nbackward computation graph based on FLOPs and time estimations, resulting in\nfaster training without sacrificing accuracy. The experimental results show up\nto 17% speedup on Llama family of models.",
            "author": [
                "Daria Cherniuk",
                "Aleksandr Mikhalev",
                "Ivan Oseledets"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03415v1",
                "http://arxiv.org/pdf/2312.03415v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03414v1",
            "title": "Compressed Context Memory For Online Language Model Interaction",
            "updated": "2023-12-06T10:50:43Z",
            "published": "2023-12-06T10:50:43Z",
            "summary": "This paper presents a novel context compression method for Transformer\nlanguage models in online scenarios such as ChatGPT, where the context\ncontinually expands. As the context lengthens, the attention process requires\nmore memory and computational resources, which in turn reduces the throughput\nof the language model. To this end, we propose a compressed context memory\nsystem that continually compresses the growing context into a compact memory\nspace. The compression process simply involves integrating a lightweight\nconditional LoRA into the language model's forward pass during inference. Based\non the compressed context memory, the language model can perform inference with\nreduced memory and attention operations. Through evaluations on conversation,\npersonalization, and multi-task learning, we demonstrate that our approach\nachieves the performance level of a full context model with $5\\times$ smaller\ncontext memory space. Codes are available at\nhttps://github.com/snu-mllab/context-memory.",
            "author": [
                "Jang-Hyun Kim",
                "Junyoung Yeom",
                "Sangdoo Yun",
                "Hyun Oh Song"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03414v1",
                "http://arxiv.org/pdf/2312.03414v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03413v1",
            "title": "Approximating Solutions to the Knapsack Problem using the Lagrangian\n  Dual Framework",
            "updated": "2023-12-06T10:50:27Z",
            "published": "2023-12-06T10:50:27Z",
            "summary": "The Knapsack Problem is a classic problem in combinatorial optimisation.\nSolving these problems may be computationally expensive. Recent years have seen\na growing interest in the use of deep learning methods to approximate the\nsolutions to such problems. A core problem is how to enforce or encourage\nconstraint satisfaction in predicted solutions. A promising approach for\npredicting solutions to constrained optimisation problems is the Lagrangian\nDual Framework which builds on the method of Lagrangian Relaxation. In this\npaper we develop neural network models to approximate Knapsack Problem\nsolutions using the Lagrangian Dual Framework while improving constraint\nsatisfaction. We explore the problems of output interpretation and model\nselection within this context. Experimental results show strong constraint\nsatisfaction with a minor reduction of optimality as compared to a baseline\nneural network which does not explicitly model the constraints.",
            "author": [
                "Mitchell Keegan",
                "Mahdi Abolghasemi"
            ],
            "link": [
                "http://dx.doi.org/10.1007/978-981-99-8388-9_37",
                "http://arxiv.org/abs/2312.03413v1",
                "http://arxiv.org/pdf/2312.03413v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03410v1",
            "title": "Detecting Voice Cloning Attacks via Timbre Watermarking",
            "updated": "2023-12-06T10:48:36Z",
            "published": "2023-12-06T10:48:36Z",
            "summary": "Nowadays, it is common to release audio content to the public. However, with\nthe rise of voice cloning technology, attackers have the potential to easily\nimpersonate a specific person by utilizing his publicly released audio without\nany permission. Therefore, it becomes significant to detect any potential\nmisuse of the released audio content and protect its timbre from being\nimpersonated. To this end, we introduce a novel concept, \"Timbre Watermarking\",\nwhich embeds watermark information into the target individual's speech,\neventually defeating the voice cloning attacks. To ensure the watermark is\nrobust to the voice cloning model's learning process, we design an end-to-end\nvoice cloning-resistant detection framework. The core idea of our solution is\nto embed and extract the watermark in the frequency domain in a temporally\ninvariant manner. To acquire generalization across different voice cloning\nattacks, we modulate their shared process and integrate it into our framework\nas a distortion layer. Experiments demonstrate that the proposed timbre\nwatermarking can defend against different voice cloning attacks, exhibit strong\nresistance against various adaptive attacks (e.g., reconstruction-based removal\nattacks, watermark overwriting attacks), and achieve practicality in real-world\nservices such as PaddleSpeech, Voice-Cloning-App, and so-vits-svc. In addition,\nablation studies are also conducted to verify the effectiveness of our design.\nSome audio samples are available at\nhttps://timbrewatermarking.github.io/samples.",
            "author": [
                "Chang Liu",
                "Jie Zhang",
                "Tianwei Zhang",
                "Xi Yang",
                "Weiming Zhang",
                "Nenghai Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03410v1",
                "http://arxiv.org/pdf/2312.03410v1"
            ],
            "primary_category": "cs.SD",
            "category": [
                "cs.SD",
                "cs.MM",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03407v1",
            "title": "Extremal Fitting CQs do not Generalize",
            "updated": "2023-12-06T10:43:08Z",
            "published": "2023-12-06T10:43:08Z",
            "summary": "A fitting algorithm for conjunctive queries (CQs) produces, given a set of\npositively and negatively labeled data examples, a CQ that fits these examples.\nIn general, there may be many non-equivalent fitting CQs and thus the algorithm\nhas some freedom in producing its output. Additional desirable properties of\nthe produced CQ are that it generalizes well to unseen examples in the sense of\nPAC learning and that it is most general or most specific in the set of all\nfitting CQs. In this research note, we show that these desiderata are\nincompatible when we require PAC-style generalization from a polynomial sample:\nwe prove that any fitting algorithm that produces a most-specific fitting CQ\ncannot be a sample-efficient PAC learning algorithm, and the same is true for\nfitting algorithms that produce a most-general fitting CQ (when it exists). Our\nproofs rely on a polynomial construction of relativized homomorphism dualities\nfor path-shaped structures.",
            "author": [
                "Balder ten Cate",
                "Maurice Funk",
                "Jean Christoph Jung",
                "Carsten Lutz"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03407v1",
                "http://arxiv.org/pdf/2312.03407v1"
            ],
            "primary_category": "cs.DB",
            "category": [
                "cs.DB"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03406v2",
            "title": "SVQ: Sparse Vector Quantization for Spatiotemporal Forecasting",
            "updated": "2023-12-07T01:24:54Z",
            "published": "2023-12-06T10:42:40Z",
            "summary": "Spatiotemporal forecasting tasks, such as weather forecasting and traffic\nprediction, offer significant societal benefits. These tasks can be effectively\napproached as image forecasting problems using computer vision models. Vector\nquantization (VQ) is a well-known method for discrete representation that\nimproves the latent space, leading to enhanced generalization and transfer\nlearning capabilities. One of the main challenges in using VQ for\nspatiotemporal forecasting is how to balance between keeping enough details and\nremoving noises from the original patterns for better generalization. We\naddress this challenge by developing sparse vector quantization, or {\\bf SVQ}\nfor short, that leverages sparse regression to make better trade-off between\nthe two objectives. The main innovation of this work is to approximate sparse\nregression by a two-layer MLP and a randomly fixed or learnable matrix,\ndramatically improving its computational efficiency. Through experiments\nconducted on diverse datasets in multiple fields including weather forecasting,\ntraffic flow prediction, and video forecasting, we unequivocally demonstrate\nthat our proposed method consistently enhances the performance of base models\nand achieves state-of-the-art results across all benchmarks.",
            "author": [
                "Chao Chen",
                "Tian Zhou",
                "Yanjun Zhao",
                "Hui Liu",
                "Liang Sun",
                "Rong Jin"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03406v2",
                "http://arxiv.org/pdf/2312.03406v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03404v1",
            "title": "An AI for Scientific Discovery Route between Amorphous Networks and\n  Mechanical Behavior",
            "updated": "2023-12-06T10:40:33Z",
            "published": "2023-12-06T10:40:33Z",
            "summary": "\"AI for science\" is widely recognized as a future trend in the development of\nscientific research. Currently, although machine learning algorithms have\nplayed a crucial role in scientific research with numerous successful cases,\nrelatively few instances exist where AI assists researchers in uncovering the\nunderlying physical mechanisms behind a certain phenomenon and subsequently\nusing that mechanism to improve machine learning algorithms' efficiency. This\narticle uses the investigation into the relationship between extreme Poisson's\nratio values and the structure of amorphous networks as a case study to\nillustrate how machine learning methods can assist in revealing underlying\nphysical mechanisms. Upon recognizing that the Poisson's ratio relies on the\nlow-frequency vibrational modes of dynamical matrix, we can then employ a\nconvolutional neural network, trained on the dynamical matrix instead of\ntraditional image recognition, to predict the Poisson's ratio of amorphous\nnetworks with a much higher efficiency. Through this example, we aim to\nshowcase the role that artificial intelligence can play in revealing\nfundamental physical mechanisms, which subsequently improves the machine\nlearning algorithms significantly.",
            "author": [
                "Changliang Zhu",
                "Chenchao Fang",
                "Zhipeng Jin",
                "Baowen Li",
                "Xiangying Shen",
                "Lei Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03404v1",
                "http://arxiv.org/pdf/2312.03404v1"
            ],
            "primary_category": "cond-mat.soft",
            "category": [
                "cond-mat.soft",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03401v1",
            "title": "Predicting Postoperative Intraocular Lens Dislocation in Cataract\n  Surgery via Deep Learning",
            "updated": "2023-12-06T10:27:15Z",
            "published": "2023-12-06T10:27:15Z",
            "summary": "A critical yet unpredictable complication following cataract surgery is\nintraocular lens dislocation. Postoperative stability is imperative, as even a\ntiny decentration of multifocal lenses or inadequate alignment of the torus in\ntoric lenses due to postoperative rotation can lead to a significant drop in\nvisual acuity. Investigating possible intraoperative indicators that can\npredict post-surgical instabilities of intraocular lenses can help prevent this\ncomplication. In this paper, we develop and evaluate the first fully-automatic\nframework for the computation of lens unfolding delay, rotation, and\ninstability during surgery. Adopting a combination of three types of CNNs,\nnamely recurrent, region-based, and pixel-based, the proposed framework is\nemployed to assess the possibility of predicting post-operative lens\ndislocation during cataract surgery. This is achieved via performing a\nlarge-scale study on the statistical differences between the behavior of\ndifferent brands of intraocular lenses and aligning the results with expert\nsurgeons' hypotheses and observations about the lenses. We exploit a\nlarge-scale dataset of cataract surgery videos featuring four intraocular lens\nbrands. Experimental results confirm the reliability of the proposed framework\nin evaluating the lens' statistics during the surgery. The Pearson correlation\nand t-test results reveal significant correlations between lens unfolding delay\nand lens rotation and significant differences between the intra-operative\nrotations stability of four groups of lenses. These results suggest that the\nproposed framework can help surgeons select the lenses based on the patient's\neye conditions and predict post-surgical lens dislocation.",
            "author": [
                "Negin Ghamsarian",
                "Doris Putzgruber-Adamitsch",
                "Stephanie Sarny",
                "Raphael Sznitman",
                "Klaus Schoeffmann",
                "Yosuf El-Shabrawi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03401v1",
                "http://arxiv.org/pdf/2312.03401v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03400v1",
            "title": "Full and fractional defects across the Berezinskii-Kosterlitz-Thouless\n  transition in a driven-dissipative spinor quantum fluid",
            "updated": "2023-12-06T10:25:31Z",
            "published": "2023-12-06T10:25:31Z",
            "summary": "We investigate the properties of a two-dimensional \\emph{spinor} microcavity\npolariton system driven by a linearly polarised continuous pump. In particular,\nwe establish the role of the elementary excitations, namely the so-called\nhalf-vortices and full-vortices; these objects carry a quantum rotation only in\none of the two, or both, spin components respectively. Our numerical analysis\nof the steady-state shows that it is only the half-vortices that are present in\nthe vortex-antivortex pairing/dissociation responsible for the\nBerezinskii-Kosterlitz-Thouless transition. These are the relevant elementary\nexcitations close to the critical point. However, by exploring the\nphase-ordering dynamics following a sudden quench across the transition we\nprove that full-vortices become the relevant excitations away from the critical\npoint in a deep quasi-ordered state at late times. The time-scales for\nhalf-vortices binding into full vortices are much faster than the\nvortex-antivortex annihilations.",
            "author": [
                "G. Dagvadorj",
                "P. Comaron",
                "M. H. Szymanska"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03400v1",
                "http://arxiv.org/pdf/2312.03400v1"
            ],
            "primary_category": "cond-mat.quant-gas",
            "category": [
                "cond-mat.quant-gas",
                "cond-mat.mes-hall",
                "physics.flu-dyn",
                "physics.optics"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03397v1",
            "title": "Generalized Contrastive Divergence: Joint Training of Energy-Based Model\n  and Diffusion Model through Inverse Reinforcement Learning",
            "updated": "2023-12-06T10:10:21Z",
            "published": "2023-12-06T10:10:21Z",
            "summary": "We present Generalized Contrastive Divergence (GCD), a novel objective\nfunction for training an energy-based model (EBM) and a sampler simultaneously.\nGCD generalizes Contrastive Divergence (Hinton, 2002), a celebrated algorithm\nfor training EBM, by replacing Markov Chain Monte Carlo (MCMC) distribution\nwith a trainable sampler, such as a diffusion model. In GCD, the joint training\nof EBM and a diffusion model is formulated as a minimax problem, which reaches\nan equilibrium when both models converge to the data distribution. The minimax\nlearning with GCD bears interesting equivalence to inverse reinforcement\nlearning, where the energy corresponds to a negative reward, the diffusion\nmodel is a policy, and the real data is expert demonstrations. We present\npreliminary yet promising results showing that joint training is beneficial for\nboth EBM and a diffusion model. GCD enables EBM training without MCMC while\nimproving the sample quality of a diffusion model.",
            "author": [
                "Sangwoong Yoon",
                "Dohyun Kwon",
                "Himchan Hwang",
                "Yung-Kyun Noh",
                "Frank C. Park"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03397v1",
                "http://arxiv.org/pdf/2312.03397v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03395v1",
            "title": "Diffused Task-Agnostic Milestone Planner",
            "updated": "2023-12-06T10:09:22Z",
            "published": "2023-12-06T10:09:22Z",
            "summary": "Addressing decision-making problems using sequence modeling to predict future\ntrajectories shows promising results in recent years. In this paper, we take a\nstep further to leverage the sequence predictive method in wider areas such as\nlong-term planning, vision-based control, and multi-task decision-making. To\nthis end, we propose a method to utilize a diffusion-based generative sequence\nmodel to plan a series of milestones in a latent space and to have an agent to\nfollow the milestones to accomplish a given task. The proposed method can learn\ncontrol-relevant, low-dimensional latent representations of milestones, which\nmakes it possible to efficiently perform long-term planning and vision-based\ncontrol. Furthermore, our approach exploits generation flexibility of the\ndiffusion model, which makes it possible to plan diverse trajectories for\nmulti-task decision-making. We demonstrate the proposed method across offline\nreinforcement learning (RL) benchmarks and an visual manipulation environment.\nThe results show that our approach outperforms offline RL methods in solving\nlong-horizon, sparse-reward tasks and multi-task problems, while also achieving\nthe state-of-the-art performance on the most challenging vision-based\nmanipulation benchmark.",
            "author": [
                "Mineui Hong",
                "Minjae Kang",
                "Songhwai Oh"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03395v1",
                "http://arxiv.org/pdf/2312.03395v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03386v1",
            "title": "An Infinite-Width Analysis on the Jacobian-Regularised Training of a\n  Neural Network",
            "updated": "2023-12-06T09:52:18Z",
            "published": "2023-12-06T09:52:18Z",
            "summary": "The recent theoretical analysis of deep neural networks in their\ninfinite-width limits has deepened our understanding of initialisation, feature\nlearning, and training of those networks, and brought new practical techniques\nfor finding appropriate hyperparameters, learning network weights, and\nperforming inference. In this paper, we broaden this line of research by\nshowing that this infinite-width analysis can be extended to the Jacobian of a\ndeep neural network. We show that a multilayer perceptron (MLP) and its\nJacobian at initialisation jointly converge to a Gaussian process (GP) as the\nwidths of the MLP's hidden layers go to infinity and characterise this GP. We\nalso prove that in the infinite-width limit, the evolution of the MLP under the\nso-called robust training (i.e., training with a regulariser on the Jacobian)\nis described by a linear first-order ordinary differential equation that is\ndetermined by a variant of the Neural Tangent Kernel. We experimentally show\nthe relevance of our theoretical claims to wide finite networks, and\nempirically analyse the properties of kernel regression solution to obtain an\ninsight into Jacobian regularisation.",
            "author": [
                "Taeyoung Kim",
                "Hongseok Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03386v1",
                "http://arxiv.org/pdf/2312.03386v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03781v1",
            "title": "Lite-Mind: Towards Efficient and Versatile Brain Representation Network",
            "updated": "2023-12-06T09:39:38Z",
            "published": "2023-12-06T09:39:38Z",
            "summary": "Research in decoding visual information from the brain, particularly through\nthe non-invasive fMRI method, is rapidly progressing. The challenge arises from\nthe limited data availability and the low signal-to-noise ratio of fMRI\nsignals, leading to a low-precision task of fMRI-to-image retrieval.\nState-of-the-art MindEye remarkably improves fMRI-to-image retrieval\nperformance by leveraging a deep MLP with a high parameter count orders of\nmagnitude, i.e., a 996M MLP Backbone per subject, to align fMRI embeddings to\nthe final hidden layer of CLIP's vision transformer. However, significant\nindividual variations exist among subjects, even within identical experimental\nsetups, mandating the training of subject-specific models. The substantial\nparameters pose significant challenges in deploying fMRI decoding on practical\ndevices, especially with the necessitating of specific models for each subject.\nTo this end, we propose Lite-Mind, a lightweight, efficient, and versatile\nbrain representation network based on discrete Fourier transform, that\nefficiently aligns fMRI voxels to fine-grained information of CLIP. Our\nexperiments demonstrate that Lite-Mind achieves an impressive 94.3%\nfMRI-to-image retrieval accuracy on the NSD dataset for Subject 1, with 98.7%\nfewer parameters than MindEye. Lite-Mind is also proven to be able to be\nmigrated to smaller brain datasets and establishes a new state-of-the-art for\nzero-shot classification on the GOD dataset. The code is available at\nhttps://github.com/gongzix/Lite-Mind.",
            "author": [
                "Zixuan Gong",
                "Qi Zhang",
                "Duoqian Miao",
                "Guangyin Bao",
                "Liang Hu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03781v1",
                "http://arxiv.org/pdf/2312.03781v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03378v1",
            "title": "Riemannian Complex Matrix Convolution Network for PolSAR Image\n  Classification",
            "updated": "2023-12-06T09:33:33Z",
            "published": "2023-12-06T09:33:33Z",
            "summary": "Recently, deep learning methods have achieved superior performance for\nPolarimetric Synthetic Aperture Radar(PolSAR) image classification. Existing\ndeep learning methods learn PolSAR data by converting the covariance matrix\ninto a feature vector or complex-valued vector as the input. However, all these\nmethods cannot learn the structure of complex matrix directly and destroy the\nchannel correlation. To learn geometric structure of complex matrix, we propose\na Riemannian complex matrix convolution network for PolSAR image classification\nin Riemannian space for the first time, which directly utilizes the complex\nmatrix as the network input and defines the Riemannian operations to learn\ncomplex matrix's features. The proposed Riemannian complex matrix convolution\nnetwork considers PolSAR complex matrix endowed in Riemannian manifold, and\ndefines a series of new Riemannian convolution, ReLu and LogEig operations in\nRiemannian space, which breaks through the Euclidean constraint of conventional\nnetworks. Then, a CNN module is appended to enhance contextual Riemannian\nfeatures. Besides, a fast kernel learning method is developed for the proposed\nmethod to learn class-specific features and reduce the computation time\neffectively. Experiments are conducted on three sets of real PolSAR data with\ndifferent bands and sensors. Experiments results demonstrates the proposed\nmethod can obtain superior performance than the state-of-the-art methods.",
            "author": [
                "Junfei Shi",
                "Wei Wang",
                "Haiyan Jin",
                "Mengmeng Nie",
                "Shanshan Ji"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03378v1",
                "http://arxiv.org/pdf/2312.03378v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03368v1",
            "title": "Bottom-Up Instance Segmentation of Catheters for Chest X-Rays",
            "updated": "2023-12-06T09:09:27Z",
            "published": "2023-12-06T09:09:27Z",
            "summary": "Chest X-ray (CXR) is frequently employed in emergency departments and\nintensive care units to verify the proper placement of central lines and tubes\nand to rule out related complications. The automation of the X-ray reading\nprocess can be a valuable support tool for non-specialist technicians and\nminimize reporting delays due to non-availability of experts. While existing\nsolutions for automated catheter segmentation and malposition detection show\npromising results, the disentanglement of individual catheters remains an open\nchallenge, especially in complex cases where multiple devices appear\nsuperimposed in the X-ray projection. Moreover, conventional top-down instance\nsegmentation methods are ineffective on such thin and long devices, that often\nextend through the entire image. In this paper, we propose a deep learning\napproach based on associative embeddings for catheter instance segmentation,\nable to overcome those limitations and effectively handle device intersections.",
            "author": [
                "Francesca Boccardi",
                "Axel Saalbach",
                "Heinrich Schulz",
                "Samuele Salti",
                "Ilyas Sirazitdinov"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03368v1",
                "http://arxiv.org/pdf/2312.03368v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03365v1",
            "title": "Demand response for residential building heating: Effective Monte Carlo\n  Tree Search control based on physics-informed neural networks",
            "updated": "2023-12-06T09:06:14Z",
            "published": "2023-12-06T09:06:14Z",
            "summary": "Controlling energy consumption in buildings through demand response (DR) has\nbecome increasingly important to reduce global carbon emissions and limit\nclimate change. In this paper, we specifically focus on controlling the heating\nsystem of a residential building to optimize its energy consumption while\nrespecting user's thermal comfort. Recent works in this area have mainly\nfocused on either model-based control, e.g., model predictive control (MPC), or\nmodel-free reinforcement learning (RL) to implement practical DR algorithms. A\nspecific RL method that recently has achieved impressive success in domains\nsuch as board games (go, chess) is Monte Carlo Tree Search (MCTS). Yet, for\nbuilding control it has remained largely unexplored. Thus, we study MCTS\nspecifically for building demand response. Its natural structure allows a\nflexible optimization that implicitly integrate exogenous constraints (as\nopposed, for example, to conventional RL solutions), making MCTS a promising\ncandidate for DR control problems. We demonstrate how to improve MCTS control\nperformance by incorporating a Physics-informed Neural Network (PiNN) model for\nits underlying thermal state prediction, as opposed to traditional purely\ndata-driven Black-Box approaches. Our MCTS implementation aligned with a PiNN\nmodel is able to obtain a 3% increment of the obtained reward compared to a\nrule-based controller; leading to a 10% cost reduction and 35% reduction on\ntemperature difference with the desired one when applied to an artificial price\nprofile. We further implemented a Deep Learning layer into the Monte Carlo Tree\nSearch technique using a neural network that leads the tree search through more\noptimal nodes. We then compared this addition with its Vanilla version, showing\nthe improvement in computational cost required.",
            "author": [
                "Fabio Pavirani",
                "Gargya Gokhale",
                "Bert Claessens",
                "Chris Develder"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03365v1",
                "http://arxiv.org/pdf/2312.03365v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.AI",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03361v1",
            "title": "KhabarChin: Automatic Detection of Important News in the Persian\n  Language",
            "updated": "2023-12-06T09:01:21Z",
            "published": "2023-12-06T09:01:21Z",
            "summary": "Being aware of important news is crucial for staying informed and making\nwell-informed decisions efficiently. Natural Language Processing (NLP)\napproaches can significantly automate this process. This paper introduces the\ndetection of important news, in a previously unexplored area, and presents a\nnew benchmarking dataset (Khabarchin) for detecting important news in the\nPersian language. We define important news articles as those deemed significant\nfor a considerable portion of society, capable of influencing their mindset or\ndecision-making. The news articles are obtained from seven different prominent\nPersian news agencies, resulting in the annotation of 7,869 samples and the\ncreation of the dataset. Two challenges of high disagreement and imbalance\nbetween classes were faced, and solutions were provided for them. We also\npropose several learning-based models, ranging from conventional machine\nlearning to state-of-the-art transformer models, to tackle this task.\nFurthermore, we introduce the second task of important sentence detection in\nnews articles, as they often come with a significant contextual length that\nmakes it challenging for readers to identify important information. We identify\nthese sentences in a weakly supervised manner.",
            "author": [
                "Hamed Hematian Hemati",
                "Arash Lagzian",
                "Moein Salimi Sartakhti",
                "Hamid Beigy",
                "Ehsaneddin Asgari"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03361v1",
                "http://arxiv.org/pdf/2312.03361v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03360v1",
            "title": "Teaching Specific Scientific Knowledge into Large Language Models\n  through Additional Training",
            "updated": "2023-12-06T08:55:55Z",
            "published": "2023-12-06T08:55:55Z",
            "summary": "Through additional training, we explore embedding specialized scientific\nknowledge into the Llama 2 Large Language Model (LLM). Key findings reveal that\neffective knowledge integration requires reading texts from multiple\nperspectives, especially in instructional formats. We utilize text augmentation\nto tackle the scarcity of specialized texts, including style conversions and\ntranslations. Hyperparameter optimization proves crucial, with different size\nmodels (7b, 13b, and 70b) reasonably undergoing additional training. Validating\nour methods, we construct a dataset of 65,000 scientific papers. Although we\nhave succeeded in partially embedding knowledge, the study highlights the\ncomplexities and limitations of incorporating specialized information into\nLLMs, suggesting areas for further improvement.",
            "author": [
                "Kan Hatakeyama-Sato",
                "Yasuhiko Igarashi",
                "Shun Katakami",
                "Yuta Nabae",
                "Teruaki Hayakawa"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03360v1",
                "http://arxiv.org/pdf/2312.03360v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03356v1",
            "title": "Bile Duct Segmentation Methods Under 3D Slicer Applied to ERCP:\n  Advantages and Disadvantages",
            "updated": "2023-12-06T08:53:15Z",
            "published": "2023-12-06T08:53:15Z",
            "summary": "This article presents an evaluation of biliary tract segmentation methods\nused for 3D reconstruction, which may be very usefull in various critical\ninterventions, such as endoscopic retrograde cholangiopancreatography (ERCP),\nusing the 3D Slicer software. This article provides an assessment of biliary\ntract segmentation techniques employed for 3D reconstruction, which can prove\nhighly valuable in diverse critical procedures like endoscopic retrograde\ncholangiopancreatography (ERCP) through the utilization of 3D Slicer software.\nThree different methods, namely thresholding, flood filling, and region\ngrowing, were assessed in terms of their advantages and disadvantages. The\nstudy involved 10 patient cases and employed quantitative indices and\nqualitative evaluation to assess the segmentations obtained by the different\nsegmentation methods against ground truth. The results indicate that the\nthresholding method is almost manual and time-consuming, while the flood\nfilling method is semi-automatic and also time-consuming. Although both methods\nimprove segmentation quality, they are not reproducible. Therefore, an\nautomatic method based on region growing was developed to reduce segmentation\ntime, albeit at the expense of quality. These findings highlight the pros and\ncons of different conventional segmentation methods and underscore the need to\nexplore alternative approaches, such as deep learning, to optimize biliary\ntract segmentation in the context of ERCP.",
            "author": [
                "Abdelhadi Essamlali",
                "Vincent Millot-Maysounabe",
                "Marion Chartier",
                "Gr\u00e9goire Salin",
                "Aymeric Becq",
                "Lionel Arriv\u00e9",
                "Marine Duboc Camus",
                "J\u00e9r\u00f4me Szewczyk",
                "Isabelle Claude"
            ],
            "link": [
                "http://dx.doi.org/10.11648/j.ijbecs.20230904.11",
                "http://arxiv.org/abs/2312.03356v1",
                "http://arxiv.org/pdf/2312.03356v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03351v1",
            "title": "On the variants of SVM methods applied to GPR data to classify tack coat\n  characteristics in French pavements: two experimental case studies",
            "updated": "2023-12-06T08:50:01Z",
            "published": "2023-12-06T08:50:01Z",
            "summary": "Among the commonly used non-destructive techniques, the Ground Penetrating\nRadar (GPR) is one of the most widely adopted today for assessing pavement\nconditions in France. However, conventional radar systems and their forward\nprocessing methods have shown their limitations for the physical and\ngeometrical characterization of very thin layers such as tack coats. However,\nthe use of Machine Learning methods applied to GPR with an inverse approach\nshowed that it was numerically possible to identify the tack coat\ncharacteristics despite masking effects due to low timefrequency resolution\nnoted in the raw B-scans. Thus, we propose in this paper to apply the inverse\napproach based on Machine Learning, already validated in previous works on\nnumerical data, on two experimental cases with different pavement structures.\nThe first case corresponds to a validation on known pavement structures on the\nGustave Eiffel University (Nantes, France) with its pavement fatigue carousel\nand the second case focuses on a new real road in Vend{\\'e}e department\n(France). In both case studies, the performances of SVM/SVR methods showed the\nefficiency of supervised learning methods to classify and estimate the emulsion\nproportioning in the tack coats.",
            "author": [
                "Gr\u00e9gory Andreoli",
                "Amine Ihamouten",
                "Mai Lan Nguyen",
                "Yannick Fargier",
                "Cyrille Fauchard",
                "Jean-Michel Simonin",
                "Viktoriia Buliuk",
                "David Souriou",
                "Xavier D\u00e9robert"
            ],
            "link": [
                "http://dx.doi.org/10.1109/IWAGPR57138.2023.10329070",
                "http://arxiv.org/abs/2312.03351v1",
                "http://arxiv.org/pdf/2312.03351v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03350v1",
            "title": "PointMoment:Mixed-Moment-based Self-Supervised Representation Learning\n  for 3D Point Clouds",
            "updated": "2023-12-06T08:49:55Z",
            "published": "2023-12-06T08:49:55Z",
            "summary": "Large and rich data is a prerequisite for effective training of deep neural\nnetworks. However, the irregularity of point cloud data makes manual annotation\ntime-consuming and laborious. Self-supervised representation learning, which\nleverages the intrinsic structure of large-scale unlabelled data to learn\nmeaningful feature representations, has attracted increasing attention in the\nfield of point cloud research. However, self-supervised representation learning\noften suffers from model collapse, resulting in reduced information and\ndiversity of the learned representation, and consequently degrading the\nperformance of downstream tasks. To address this problem, we propose\nPointMoment, a novel framework for point cloud self-supervised representation\nlearning that utilizes a high-order mixed moment loss function rather than the\nconventional contrastive loss function. Moreover, our framework does not\nrequire any special techniques such as asymmetric network architectures,\ngradient stopping, etc. Specifically, we calculate the high-order mixed moment\nof the feature variables and force them to decompose into products of their\nindividual moment, thereby making multiple variables more independent and\nminimizing the feature redundancy. We also incorporate a contrastive learning\napproach to maximize the feature invariance under different data augmentations\nof the same point cloud. Experimental results show that our approach\noutperforms previous unsupervised learning methods on the downstream task of 3D\npoint cloud classification and segmentation.",
            "author": [
                "Xin Cao",
                "Xinxin Han",
                "Yifan Wang",
                "Mengna Yang",
                "Kang Li"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03350v1",
                "http://arxiv.org/pdf/2312.03350v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03780v1",
            "title": "Predicting the Transportation Activities of Construction Waste Hauling\n  Trucks: An Input-Output Hidden Markov Approach",
            "updated": "2023-12-06T08:40:54Z",
            "published": "2023-12-06T08:40:54Z",
            "summary": "Construction waste hauling trucks (CWHTs), as one of the most commonly seen\nheavy-duty vehicles in major cities around the globe, are usually subject to a\nseries of regulations and spatial-temporal access restrictions because they not\nonly produce significant NOx and PM emissions but also causes on-road fugitive\ndust. The timely and accurate prediction of CWHTs' destinations and dwell times\nplay a key role in effective environmental management. To address this\nchallenge, we propose a prediction method based on an interpretable\nactivity-based model, input-output hidden Markov model (IOHMM), and validate it\non 300 CWHTs in Chengdu, China. Contextual factors are considered in the model\nto improve its prediction power. Results show that the IOHMM outperforms\nseveral baseline models, including Markov chains, linear regression, and long\nshort-term memory. Factors influencing the predictability of CWHTs'\ntransportation activities are also explored using linear regression models.\nResults suggest the proposed model holds promise in assisting authorities by\npredicting the upcoming transportation activities of CWHTs and administering\nintervention in a timely and effective manner.",
            "author": [
                "Hongtai Yang",
                "Boyi Lei",
                "Ke Han",
                "Luna Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03780v1",
                "http://arxiv.org/pdf/2312.03780v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03345v1",
            "title": "GraNet: A Multi-Level Graph Network for 6-DoF Grasp Pose Generation in\n  Cluttered Scenes",
            "updated": "2023-12-06T08:36:29Z",
            "published": "2023-12-06T08:36:29Z",
            "summary": "6-DoF object-agnostic grasping in unstructured environments is a critical yet\nchallenging task in robotics. Most current works use non-optimized approaches\nto sample grasp locations and learn spatial features without concerning the\ngrasping task. This paper proposes GraNet, a graph-based grasp pose generation\nframework that translates a point cloud scene into multi-level graphs and\npropagates features through graph neural networks. By building graphs at the\nscene level, object level, and grasp point level, GraNet enhances feature\nembedding at multiple scales while progressively converging to the ideal\ngrasping locations by learning. Our pipeline can thus characterize the spatial\ndistribution of grasps in cluttered scenes, leading to a higher rate of\neffective grasping. Furthermore, we enhance the representation ability of\nscalable graph networks by a structure-aware attention mechanism to exploit\nlocal relations in graphs. Our method achieves state-of-the-art performance on\nthe large-scale GraspNet-1Billion benchmark, especially in grasping unseen\nobjects (+11.62 AP). The real robot experiment shows a high success rate in\ngrasping scattered objects, verifying the effectiveness of the proposed\napproach in unstructured environments.",
            "author": [
                "Haowen Wang",
                "Wanhao Niu",
                "Chungang Zhuang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03345v1",
                "http://arxiv.org/pdf/2312.03345v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03344v1",
            "title": "Interpretable Mechanistic Representations for Meal-level Glycemic\n  Control in the Wild",
            "updated": "2023-12-06T08:36:23Z",
            "published": "2023-12-06T08:36:23Z",
            "summary": "Diabetes encompasses a complex landscape of glycemic control that varies\nwidely among individuals. However, current methods do not faithfully capture\nthis variability at the meal level. On the one hand, expert-crafted features\nlack the flexibility of data-driven methods; on the other hand, learned\nrepresentations tend to be uninterpretable which hampers clinical adoption. In\nthis paper, we propose a hybrid variational autoencoder to learn interpretable\nrepresentations of CGM and meal data. Our method grounds the latent space to\nthe inputs of a mechanistic differential equation, producing embeddings that\nreflect physiological quantities, such as insulin sensitivity, glucose\neffectiveness, and basal glucose levels. Moreover, we introduce a novel method\nto infer the glucose appearance rate, making the mechanistic model robust to\nunreliable meal logs. On a dataset of CGM and self-reported meals from\nindividuals with type-2 diabetes and pre-diabetes, our unsupervised\nrepresentation discovers a separation between individuals proportional to their\ndisease severity. Our embeddings produce clusters that are up to 4x better than\nnaive, expert, black-box, and pure mechanistic features. Our method provides a\nnuanced, yet interpretable, embedding space to compare glycemic control within\nand across individuals, directly learnable from in-the-wild data.",
            "author": [
                "Ke Alexander Wang",
                "Emily B. Fox"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03344v1",
                "http://arxiv.org/pdf/2312.03344v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "math.DS",
                "stat.AP",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03341v1",
            "title": "Online Vectorized HD Map Construction using Geometry",
            "updated": "2023-12-06T08:26:26Z",
            "published": "2023-12-06T08:26:26Z",
            "summary": "The construction of online vectorized High-Definition (HD) maps is critical\nfor downstream prediction and planning. Recent efforts have built strong\nbaselines for this task, however, shapes and relations of instances in urban\nroad systems are still under-explored, such as parallelism, perpendicular, or\nrectangle-shape. In our work, we propose GeMap ($\\textbf{Ge}$ometry\n$\\textbf{Map}$), which end-to-end learns Euclidean shapes and relations of map\ninstances beyond basic perception. Specifically, we design a geometric loss\nbased on angle and distance clues, which is robust to rigid transformations. We\nalso decouple self-attention to independently handle Euclidean shapes and\nrelations. Our method achieves new state-of-the-art performance on the NuScenes\nand Argoverse 2 datasets. Remarkably, it reaches a 71.8% mAP on the large-scale\nArgoverse 2 dataset, outperforming MapTR V2 by +4.4% and surpassing the 70% mAP\nthreshold for the first time. Code is available at\nhttps://github.com/cnzzx/GeMap",
            "author": [
                "Zhixin Zhang",
                "Yiyuan Zhang",
                "Xiaohan Ding",
                "Fusheng Jin",
                "Xiangyu Yue"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03341v1",
                "http://arxiv.org/pdf/2312.03341v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03339v1",
            "title": "PointJEM: Self-supervised Point Cloud Understanding for Reducing Feature\n  Redundancy via Joint Entropy Maximization",
            "updated": "2023-12-06T08:21:42Z",
            "published": "2023-12-06T08:21:42Z",
            "summary": "Most deep learning-based point cloud processing methods are supervised and\nrequire large scale of labeled data. However, manual labeling of point cloud\ndata is laborious and time-consuming. Self-supervised representation learning\ncan address the aforementioned issue by learning robust and generalized\nrepresentations from unlabeled datasets. Nevertheless, the embedded features\nobtained by representation learning usually contain redundant information, and\nmost current methods reduce feature redundancy by linear correlation\nconstraints. In this paper, we propose PointJEM, a self-supervised\nrepresentation learning method applied to the point cloud field. PointJEM\ncomprises an embedding scheme and a loss function based on joint entropy. The\nembedding scheme divides the embedding vector into different parts, each part\ncan learn a distinctive feature. To reduce redundant information in the\nfeatures, PointJEM maximizes the joint entropy between the different parts,\nthereby rendering the learned feature variables pairwise independent. To\nvalidate the effectiveness of our method, we conducted experiments on multiple\ndatasets. The results demonstrate that our method can significantly reduce\nfeature redundancy beyond linear correlation. Furthermore, PointJEM achieves\ncompetitive performance in downstream tasks such as classification and\nsegmentation.",
            "author": [
                "Xin Cao",
                "Huan Xia",
                "Xinxin Han",
                "Yifan Wang",
                "Kang Li",
                "Linzhi Su"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03339v1",
                "http://arxiv.org/pdf/2312.03339v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03330v1",
            "title": "Measuring Misogyny in Natural Language Generation: Preliminary Results\n  from a Case Study on two Reddit Communities",
            "updated": "2023-12-06T07:38:46Z",
            "published": "2023-12-06T07:38:46Z",
            "summary": "Generic `toxicity' classifiers continue to be used for evaluating the\npotential for harm in natural language generation, despite mounting evidence of\ntheir shortcomings. We consider the challenge of measuring misogyny in natural\nlanguage generation, and argue that generic `toxicity' classifiers are\ninadequate for this task. We use data from two well-characterised `Incel'\ncommunities on Reddit that differ primarily in their degrees of misogyny to\nconstruct a pair of training corpora which we use to fine-tune two language\nmodels. We show that an open source `toxicity' classifier is unable to\ndistinguish meaningfully between generations from these models. We contrast\nthis with a misogyny-specific lexicon recently proposed by feminist\nsubject-matter experts, demonstrating that, despite the limitations of simple\nlexicon-based approaches, this shows promise as a benchmark to evaluate\nlanguage models for misogyny, and that it is sensitive enough to reveal the\nknown differences in these Reddit communities. Our preliminary findings\nhighlight the limitations of a generic approach to evaluating harms, and\nfurther emphasise the need for careful benchmark design and selection in\nnatural language evaluation.",
            "author": [
                "Aaron J. Snoswell",
                "Lucinda Nelson",
                "Hao Xue",
                "Flora D. Salim",
                "Nicolas Suzor",
                "Jean Burgess"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03330v1",
                "http://arxiv.org/pdf/2312.03330v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.CY",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03328v1",
            "title": "Deep Learning for Koopman-based Dynamic Movement Primitives",
            "updated": "2023-12-06T07:33:22Z",
            "published": "2023-12-06T07:33:22Z",
            "summary": "The challenge of teaching robots to perform dexterous manipulation, dynamic\nlocomotion, or whole--body manipulation from a small number of demonstrations\nis an important research field that has attracted interest from across the\nrobotics community. In this work, we propose a novel approach by joining the\ntheories of Koopman Operators and Dynamic Movement Primitives to Learning from\nDemonstration. Our approach, named \\gls{admd}, projects nonlinear dynamical\nsystems into linear latent spaces such that a solution reproduces the desired\ncomplex motion. Use of an autoencoder in our approach enables generalizability\nand scalability, while the constraint to a linear system attains\ninterpretability. Our results are comparable to the Extended Dynamic Mode\nDecomposition on the LASA Handwriting dataset but with training on only a small\nfractions of the letters.",
            "author": [
                "Tyler Han",
                "Carl Glen Henshaw"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03328v1",
                "http://arxiv.org/pdf/2312.03328v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03327v1",
            "title": "Building Category Graphs Representation with Spatial and Temporal\n  Attention for Visual Navigation",
            "updated": "2023-12-06T07:28:43Z",
            "published": "2023-12-06T07:28:43Z",
            "summary": "Given an object of interest, visual navigation aims to reach the object's\nlocation based on a sequence of partial observations. To this end, an agent\nneeds to 1) learn a piece of certain knowledge about the relations of object\ncategories in the world during training and 2) look for the target object based\non the pre-learned object category relations and its moving trajectory in the\ncurrent unseen environment. In this paper, we propose a Category Relation Graph\n(CRG) to learn the knowledge of object category layout relations and a\nTemporal-Spatial-Region (TSR) attention architecture to perceive the long-term\nspatial-temporal dependencies of objects helping the navigation. We learn prior\nknowledge of object layout, establishing a category relationship graph to\ndeduce the positions of specific objects. Subsequently, we introduced TSR to\ncapture the relationships of objects in temporal, spatial, and regions within\nthe observation trajectories. Specifically, we propose a Temporal attention\nmodule (T) to model the temporal structure of the observation sequence, which\nimplicitly encodes the historical moving or trajectory information. Then, a\nSpatial attention module (S) is used to uncover the spatial context of the\ncurrent observation objects based on the category relation graph and past\nobservations. Last, a Region attention module (R) shifts the attention to the\ntarget-relevant region. Based on the visual representation extracted by our\nmethod, the agent can better perceive the environment and easily learn superior\nnavigation policy. Experiments on AI2-THOR demonstrate our CRG-TSR method\nsignificantly outperforms existing methods regarding both effectiveness and\nefficiency. The code has been included in the supplementary material and will\nbe publicly available.",
            "author": [
                "Xiaobo Hu",
                "Youfang Lin",
                "HeHe Fan",
                "Shuo Wang",
                "Zhihao Wu",
                "Kai Lv"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03327v1",
                "http://arxiv.org/pdf/2312.03327v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03325v1",
            "title": "GCFA:Geodesic Curve Feature Augmentation via Shape Space Theory",
            "updated": "2023-12-06T07:26:02Z",
            "published": "2023-12-06T07:26:02Z",
            "summary": "Deep learning has yielded remarkable outcomes in various domains. However,\nthe challenge of requiring large-scale labeled samples still persists in deep\nlearning. Thus, data augmentation has been introduced as a critical strategy to\ntrain deep learning models. However, data augmentation suffers from information\nloss and poor performance in small sample environments. To overcome these\ndrawbacks, we propose a feature augmentation method based on shape space\ntheory, i.e., Geodesic curve feature augmentation, called GCFA in brevity.\nFirst, we extract features from the image with the neural network model. Then,\nthe multiple image features are projected into a pre-shape space as features.\nIn the pre-shape space, a Geodesic curve is built to fit the features. Finally,\nthe many generated features on the Geodesic curve are used to train the various\nmachine learning models. The GCFA module can be seamlessly integrated with most\nmachine learning methods. And the proposed method is simple, effective and\ninsensitive for the small sample datasets. Several examples demonstrate that\nthe GCFA method can greatly improve the performance of the data preprocessing\nmodel in a small sample environment.",
            "author": [
                "Yuexing Han",
                "Guanxin Wan",
                "Bing Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03325v1",
                "http://arxiv.org/pdf/2312.03325v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03322v1",
            "title": "Background Clustering Pre-training for Few-shot Segmentation",
            "updated": "2023-12-06T07:16:32Z",
            "published": "2023-12-06T07:16:32Z",
            "summary": "Recent few-shot segmentation (FSS) methods introduce an extra pre-training\nstage before meta-training to obtain a stronger backbone, which has become a\nstandard step in few-shot learning. Despite the effectiveness, current\npre-training scheme suffers from the merged background problem: only base\nclasses are labelled as foregrounds, making it hard to distinguish between\nnovel classes and actual background. In this paper, we propose a new\npre-training scheme for FSS via decoupling the novel classes from background,\ncalled Background Clustering Pre-Training (BCPT). Specifically, we adopt online\nclustering to the pixel embeddings of merged background to explore the\nunderlying semantic structures, bridging the gap between pre-training and\nadaptation to novel classes. Given the clustering results, we further propose\nthe background mining loss and leverage base classes to guide the clustering\nprocess, improving the quality and stability of clustering results. Experiments\non PASCAL-5i and COCO-20i show that BCPT yields advanced performance. Code will\nbe available.",
            "author": [
                "Zhimiao Yu",
                "Tiancheng Lin",
                "Yi Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03322v1",
                "http://arxiv.org/pdf/2312.03322v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03318v1",
            "title": "Complementary Benefits of Contrastive Learning and Self-Training Under\n  Distribution Shift",
            "updated": "2023-12-06T07:02:22Z",
            "published": "2023-12-06T07:02:22Z",
            "summary": "Self-training and contrastive learning have emerged as leading techniques for\nincorporating unlabeled data, both under distribution shift (unsupervised\ndomain adaptation) and when it is absent (semi-supervised learning). However,\ndespite the popularity and compatibility of these techniques, their efficacy in\ncombination remains unexplored. In this paper, we undertake a systematic\nempirical investigation of this combination, finding that (i) in domain\nadaptation settings, self-training and contrastive learning offer significant\ncomplementary gains; and (ii) in semi-supervised learning settings,\nsurprisingly, the benefits are not synergistic. Across eight distribution shift\ndatasets (e.g., BREEDs, WILDS), we demonstrate that the combined method obtains\n3--8% higher accuracy than either approach independently. We then theoretically\nanalyze these techniques in a simplified model of distribution shift,\ndemonstrating scenarios under which the features produced by contrastive\nlearning can yield a good initialization for self-training to further amplify\ngains and achieve optimal performance, even when either method alone would\nfail.",
            "author": [
                "Saurabh Garg",
                "Amrith Setlur",
                "Zachary Chase Lipton",
                "Sivaraman Balakrishnan",
                "Virginia Smith",
                "Aditi Raghunathan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03318v1",
                "http://arxiv.org/pdf/2312.03318v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03312v1",
            "title": "Optimizing Two-Pass Cross-Lingual Transfer Learning: Phoneme Recognition\n  and Phoneme to Grapheme Translation",
            "updated": "2023-12-06T06:37:24Z",
            "published": "2023-12-06T06:37:24Z",
            "summary": "This research optimizes two-pass cross-lingual transfer learning in\nlow-resource languages by enhancing phoneme recognition and phoneme-to-grapheme\ntranslation models. Our approach optimizes these two stages to improve speech\nrecognition across languages. We optimize phoneme vocabulary coverage by\nmerging phonemes based on shared articulatory characteristics, thus improving\nrecognition accuracy. Additionally, we introduce a global phoneme noise\ngenerator for realistic ASR noise during phoneme-to-grapheme training to reduce\nerror propagation. Experiments on the CommonVoice 12.0 dataset show significant\nreductions in Word Error Rate (WER) for low-resource languages, highlighting\nthe effectiveness of our approach. This research contributes to the\nadvancements of two-pass ASR systems in low-resource languages, offering the\npotential for improved cross-lingual transfer learning.",
            "author": [
                "Wonjun Lee",
                "Gary Geunbae Lee",
                "Yunsu Kim"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03312v1",
                "http://arxiv.org/pdf/2312.03312v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.SD",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03311v1",
            "title": "On the Nystrom Approximation for Preconditioning in Kernel Machines",
            "updated": "2023-12-06T06:33:25Z",
            "published": "2023-12-06T06:33:25Z",
            "summary": "Kernel methods are a popular class of nonlinear predictive models in machine\nlearning. Scalable algorithms for learning kernel models need to be iterative\nin nature, but convergence can be slow due to poor conditioning. Spectral\npreconditioning is an important tool to speed-up the convergence of such\niterative algorithms for training kernel models. However computing and storing\na spectral preconditioner can be expensive which can lead to large\ncomputational and storage overheads, precluding the application of kernel\nmethods to problems with large datasets. A Nystrom approximation of the\nspectral preconditioner is often cheaper to compute and store, and has\ndemonstrated success in practical applications. In this paper we analyze the\ntrade-offs of using such an approximated preconditioner. Specifically, we show\nthat a sample of logarithmic size (as a function of the size of the dataset)\nenables the Nystrom-based approximated preconditioner to accelerate gradient\ndescent nearly as well as the exact preconditioner, while also reducing the\ncomputational and storage overheads.",
            "author": [
                "Amirhesam Abedsoltan",
                "Mikhail Belkin",
                "Parthe Pandit",
                "Luis Rademacher"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03311v1",
                "http://arxiv.org/pdf/2312.03311v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03309v1",
            "title": "Benchmarking Continual Learning from Cognitive Perspectives",
            "updated": "2023-12-06T06:27:27Z",
            "published": "2023-12-06T06:27:27Z",
            "summary": "Continual learning addresses the problem of continuously acquiring and\ntransferring knowledge without catastrophic forgetting of old concepts. While\nhumans achieve continual learning via diverse neurocognitive mechanisms, there\nis a mismatch between cognitive properties and evaluation methods of continual\nlearning models. First, the measurement of continual learning models mostly\nrelies on evaluation metrics at a micro-level, which cannot characterize\ncognitive capacities of the model. Second, the measurement is method-specific,\nemphasizing model strengths in one aspect while obscuring potential weaknesses\nin other respects. To address these issues, we propose to integrate model\ncognitive capacities and evaluation metrics into a unified evaluation paradigm.\nWe first characterize model capacities via desiderata derived from cognitive\nproperties supporting human continual learning. The desiderata concern (1)\nadaptability in varying lengths of task sequence; (2) sensitivity to dynamic\ntask variations; and (3) efficiency in memory usage and training time\nconsumption. Then we design evaluation protocols for each desideratum to assess\ncognitive capacities of recent continual learning models. Experimental results\nshow that no method we consider has satisfied all the desiderata and is still\nfar away from realizing truly continual learning. Although some methods exhibit\nsome degree of adaptability and efficiency, no method is able to identify task\nrelationships when encountering dynamic task variations, or achieve a trade-off\nin learning similarities and differences between tasks. Inspired by these\nresults, we discuss possible factors that influence model performance in these\ndesiderata and provide guidance for the improvement of continual learning\nmodels.",
            "author": [
                "Xiaoqian Liu",
                "Junge Zhang",
                "Mingyi Zhang",
                "Peipei Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03309v1",
                "http://arxiv.org/pdf/2312.03309v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03307v1",
            "title": "Balanced Marginal and Joint Distributional Learning via Mixture\n  Cramer-Wold Distance",
            "updated": "2023-12-06T06:15:48Z",
            "published": "2023-12-06T06:15:48Z",
            "summary": "In the process of training a generative model, it becomes essential to\nmeasure the discrepancy between two high-dimensional probability distributions:\nthe generative distribution and the ground-truth distribution of the observed\ndataset. Recently, there has been growing interest in an approach that involves\nslicing high-dimensional distributions, with the Cramer-Wold distance emerging\nas a promising method. However, we have identified that the Cramer-Wold\ndistance primarily focuses on joint distributional learning, whereas\nunderstanding marginal distributional patterns is crucial for effective\nsynthetic data generation. In this paper, we introduce a novel measure of\ndissimilarity, the mixture Cramer-Wold distance. This measure enables us to\ncapture both marginal and joint distributional information simultaneously, as\nit incorporates a mixture measure with point masses on standard basis vectors.\nBuilding upon the mixture Cramer-Wold distance, we propose a new generative\nmodel called CWDAE (Cramer-Wold Distributional AutoEncoder), which shows\nremarkable performance in generating synthetic data when applied to real\ntabular datasets. Furthermore, our model offers the flexibility to adjust the\nlevel of data privacy with ease.",
            "author": [
                "Seunghwan An",
                "Sungchul Hong",
                "Jong-June Jeon"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03307v1",
                "http://arxiv.org/pdf/2312.03307v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03303v1",
            "title": "Dyport: Dynamic Importance-based Hypothesis Generation Benchmarking\n  Technique",
            "updated": "2023-12-06T06:07:50Z",
            "published": "2023-12-06T06:07:50Z",
            "summary": "This paper presents a novel benchmarking framework Dyport for evaluating\nbiomedical hypothesis generation systems. Utilizing curated datasets, our\napproach tests these systems under realistic conditions, enhancing the\nrelevance of our evaluations. We integrate knowledge from the curated databases\ninto a dynamic graph, accompanied by a method to quantify discovery importance.\nThis not only assesses hypothesis accuracy but also their potential impact in\nbiomedical research which significantly extends traditional link prediction\nbenchmarks. Applicability of our benchmarking process is demonstrated on\nseveral link prediction systems applied on biomedical semantic knowledge\ngraphs. Being flexible, our benchmarking system is designed for broad\napplication in hypothesis generation quality verification, aiming to expand the\nscope of scientific discovery within the biomedical research community.\nAvailability and implementation: Dyport framework is fully open-source. All\ncode and datasets are available at: https://github.com/IlyaTyagin/Dyport",
            "author": [
                "Ilya Tyagin",
                "Ilya Safro"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03303v1",
                "http://arxiv.org/pdf/2312.03303v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03301v1",
            "title": "Masking Behaviors in Epidemiological Networks with Cognitively-plausible\n  Reinforcement Learning",
            "updated": "2023-12-06T05:57:27Z",
            "published": "2023-12-06T05:57:27Z",
            "summary": "The COVID-19 pandemic highlighted the critical role of human behavior in\ninfluencing infectious disease transmission and the need for models capturing\nthis complex dynamic. We present an agent-based model integrating an\nepidemiological simulation of disease spread with a cognitive architecture\ndriving individual mask-wearing decisions. Agents decide whether to mask based\non a utility function weighting factors like peer conformity, personal risk\ntolerance, and mask-wearing discomfort. By conducting experiments\nsystematically varying behavioral model parameters and social network\nstructures, we demonstrate how adaptive decision-making interacts with network\nconnectivity patterns to impact population-level infection outcomes. The model\nprovides a flexible computational framework for gaining insights into how\nbehavioral interventions like mask mandates may differentially influence\ndisease spread across communities with diverse social structures. Findings\nhighlight the importance of integrating realistic human decision processes in\nepidemiological models to inform policy decisions during public health crises.",
            "author": [
                "Konstantinos Mitsopoulos",
                "Lawrence Baker",
                "Christian Lebiere",
                "Peter Pirolli",
                "Mark Orr",
                "Raffaele Vardavas"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03301v1",
                "http://arxiv.org/pdf/2312.03301v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI",
                "physics.soc-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03299v1",
            "title": "Channel-Transferable Semantic Communications for Multi-User OFDM-NOMA\n  Systems",
            "updated": "2023-12-06T05:54:20Z",
            "published": "2023-12-06T05:54:20Z",
            "summary": "Semantic communications are expected to become the core new paradigms of the\nsixth generation (6G) wireless networks. Most existing works implicitly utilize\nchannel information for codecs training, which leads to poor communications\nwhen channel type or statistical characteristics change. To tackle this issue\nposed by various channels, a novel channel-transferable semantic communications\n(CT-SemCom) framework is proposed, which adapts the codecs learned on one type\nof channel to other types of channels. Furthermore, integrating the proposed\nframework and the orthogonal frequency division multiplexing systems\nintegrating non-orthogonal multiple access technologies, i.e., OFDM-NOMA\nsystems, a power allocation problem to realize the transfer from additive white\nGaussian noise (AWGN) channels to multi-subcarrier Rayleigh fading channels is\nformulated. We then design a semantics-similar dual transformation (SSDT)\nalgorithm to derive analytical solutions with low complexity. Simulation\nresults show that the proposed CT-SemCom framework with SSDT algorithm\nsignificantly outperforms the existing work w.r.t. channel transferability,\ne.g., the peak signal-to-noise ratio (PSNR) of image transmission improves by\n4.2-7.3 dB under different variances of Rayleigh fading channels.",
            "author": [
                "Lan Lin",
                "Wenjun Xu",
                "Fengyu Wang",
                "Yimeng Zhang",
                "Wei Zhang",
                "Ping Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03299v1",
                "http://arxiv.org/pdf/2312.03299v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "eess.SP",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03298v1",
            "title": "DiffPMAE: Diffusion Masked Autoencoders for Point Cloud Reconstruction",
            "updated": "2023-12-06T05:39:00Z",
            "published": "2023-12-06T05:39:00Z",
            "summary": "Point cloud streaming is increasingly getting popular, evolving into the norm\nfor interactive service delivery and the future Metaverse. However, the\nsubstantial volume of data associated with point clouds presents numerous\nchallenges, particularly in terms of high bandwidth consumption and large\nstorage capacity. Despite various solutions proposed thus far, with a focus on\npoint cloud compression, upsampling, and completion, these\nreconstruction-related methods continue to fall short in delivering high\nfidelity point cloud output. As a solution, in DiffPMAE, we propose an\neffective point cloud reconstruction architecture. Inspired by self-supervised\nlearning concepts, we combine Masked Auto-Encoding and Diffusion Model\nmechanism to remotely reconstruct point cloud data. By the nature of this\nreconstruction process, DiffPMAE can be extended to many related downstream\ntasks including point cloud compression, upsampling and completion. Leveraging\nShapeNet-55 and ModelNet datasets with over 60000 objects, we validate the\nperformance of DiffPMAE exceeding many state-of-the-art methods in-terms of\nauto-encoding and downstream tasks considered.",
            "author": [
                "Yanlong Li",
                "Chamara Madarasingha",
                "Kanchana Thilakarathna"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03298v1",
                "http://arxiv.org/pdf/2312.03298v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03296v1",
            "title": "Cooperative Probabilistic Trajectory Forecasting under Occlusion",
            "updated": "2023-12-06T05:36:52Z",
            "published": "2023-12-06T05:36:52Z",
            "summary": "Perception and planning under occlusion is essential for safety-critical\ntasks. Occlusion-aware planning often requires communicating the information of\nthe occluded object to the ego agent for safe navigation. However,\ncommunicating rich sensor information under adverse conditions during\ncommunication loss and limited bandwidth may not be always feasible. Further,\nin GPS denied environments and indoor navigation, localizing and sharing of\noccluded objects can be challenging. To overcome this, relative pose estimation\nbetween connected agents sharing a common field of view can be a\ncomputationally effective way of communicating information about surrounding\nobjects. In this paper, we design an end-to-end network that cooperatively\nestimates the current states of occluded pedestrian in the reference frame of\nego agent and then predicts the trajectory with safety guarantees.\nExperimentally, we show that the uncertainty-aware trajectory prediction of\noccluded pedestrian by the ego agent is almost similar to the ground truth\ntrajectory assuming no occlusion. The current research holds promise for\nuncertainty-aware navigation among multiple connected agents under occlusion.",
            "author": [
                "Anshul Nayak",
                "Azim Eskandarian"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03296v1",
                "http://arxiv.org/pdf/2312.03296v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03292v1",
            "title": "Enhancing Molecular Property Prediction via Mixture of Collaborative\n  Experts",
            "updated": "2023-12-06T05:02:10Z",
            "published": "2023-12-06T05:02:10Z",
            "summary": "Molecular Property Prediction (MPP) task involves predicting biochemical\nproperties based on molecular features, such as molecular graph structures,\ncontributing to the discovery of lead compounds in drug development. To address\ndata scarcity and imbalance in MPP, some studies have adopted Graph Neural\nNetworks (GNN) as an encoder to extract commonalities from molecular graphs.\nHowever, these approaches often use a separate predictor for each task,\nneglecting the shared characteristics among predictors corresponding to\ndifferent tasks. In response to this limitation, we introduce the GNN-MoCE\narchitecture. It employs the Mixture of Collaborative Experts (MoCE) as\npredictors, exploiting task commonalities while confronting the homogeneity\nissue in the expert pool and the decision dominance dilemma within the expert\ngroup. To enhance expert diversity for collaboration among all experts, the\nExpert-Specific Projection method is proposed to assign a unique projection\nperspective to each expert. To balance decision-making influence for\ncollaboration within the expert group, the Expert-Specific Loss is presented to\nintegrate individual expert loss into the weighted decision loss of the group\nfor more equitable training. Benefiting from the enhancements of MoCE in expert\ncreation, dynamic expert group formation, and experts' collaboration, our model\ndemonstrates superior performance over traditional methods on 24 MPP datasets,\nespecially in tasks with limited data or high imbalance.",
            "author": [
                "Xu Yao",
                "Shuang Liang",
                "Songqiao Han",
                "Hailiang Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03292v1",
                "http://arxiv.org/pdf/2312.03292v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.MA",
                "q-bio.QM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03291v1",
            "title": "OMNIINPUT: A Model-centric Evaluation Framework through Output\n  Distribution",
            "updated": "2023-12-06T04:53:12Z",
            "published": "2023-12-06T04:53:12Z",
            "summary": "We propose a novel model-centric evaluation framework, OmniInput, to evaluate\nthe quality of an AI/ML model's predictions on all possible inputs (including\nhuman-unrecognizable ones), which is crucial for AI safety and reliability.\nUnlike traditional data-centric evaluation based on pre-defined test sets, the\ntest set in OmniInput is self-constructed by the model itself and the model\nquality is evaluated by investigating its output distribution. We employ an\nefficient sampler to obtain representative inputs and the output distribution\nof the trained model, which, after selective annotation, can be used to\nestimate the model's precision and recall at different output values and a\ncomprehensive precision-recall curve. Our experiments demonstrate that\nOmniInput enables a more fine-grained comparison between models, especially\nwhen their performance is almost the same on pre-defined datasets, leading to\nnew findings and insights for how to train more robust, generalizable models.",
            "author": [
                "Weitang Liu",
                "Ying Wai Li",
                "Tianle Wang",
                "Yi-Zhuang You",
                "Jingbo Shang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03291v1",
                "http://arxiv.org/pdf/2312.03291v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03289v2",
            "title": "Class Incremental Learning for Adversarial Robustness",
            "updated": "2023-12-07T04:21:33Z",
            "published": "2023-12-06T04:38:02Z",
            "summary": "Adversarial training integrates adversarial examples during model training to\nenhance robustness. However, its application in fixed dataset settings differs\nfrom real-world dynamics, where data accumulates incrementally. In this study,\nwe investigate Adversarially Robust Class Incremental Learning (ARCIL), a\nmethod that combines adversarial robustness with incremental learning. We\nobserve that combining incremental learning with naive adversarial training\neasily leads to a loss of robustness. We discover that this is attributed to\nthe disappearance of the flatness of the loss function, a characteristic of\nadversarial training. To address this issue, we propose the Flatness Preserving\nDistillation (FPD) loss that leverages the output difference between\nadversarial and clean examples. Additionally, we introduce the Logit Adjustment\nDistillation (LAD) loss, which adapts the model's knowledge to perform well on\nnew tasks. Experimental results demonstrate the superiority of our method over\napproaches that apply adversarial training to existing incremental learning\nmethods, which provides a strong baseline for incremental learning on\nadversarial robustness in the future. Our method achieves AutoAttack accuracy\nthat is 5.99\\%p, 5.27\\%p, and 3.90\\%p higher on average than the baseline on\nsplit CIFAR-10, CIFAR-100, and Tiny ImageNet, respectively. The code will be\nmade available.",
            "author": [
                "Seungju Cho",
                "Hongsin Lee",
                "Changick Kim"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03289v2",
                "http://arxiv.org/pdf/2312.03289v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03288v1",
            "title": "STEP CATFormer: Spatial-Temporal Effective Body-Part Cross Attention\n  Transformer for Skeleton-based Action Recognition",
            "updated": "2023-12-06T04:36:58Z",
            "published": "2023-12-06T04:36:58Z",
            "summary": "Graph convolutional networks (GCNs) have been widely used and achieved\nremarkable results in skeleton-based action recognition. We think the key to\nskeleton-based action recognition is a skeleton hanging in frames, so we focus\non how the Graph Convolutional Convolution networks learn different topologies\nand effectively aggregate joint features in the global temporal and local\ntemporal. In this work, we propose three Channel-wise Tolopogy Graph\nConvolution based on Channel-wise Topology Refinement Graph Convolution\n(CTR-GCN). Combining CTR-GCN with two joint cross-attention modules can capture\nthe upper-lower body part and hand-foot relationship skeleton features. After\nthat, to capture features of human skeletons changing in frames we design the\nTemporal Attention Transformers to extract skeletons effectively. The Temporal\nAttention Transformers can learn the temporal features of human skeleton\nsequences. Finally, we fuse the temporal features output scale with MLP and\nclassification. We develop a powerful graph convolutional network named Spatial\nTemporal Effective Body-part Cross Attention Transformer which notably\nhigh-performance on the NTU RGB+D, NTU RGB+D 120 datasets. Our code and models\nare available at https://github.com/maclong01/STEP-CATFormer",
            "author": [
                "Nguyen Huu Bao Long"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03288v1",
                "http://arxiv.org/pdf/2312.03288v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG",
                "I.2.10"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03277v1",
            "title": "Anomaly Detection for Scalable Task Grouping in Reinforcement\n  Learning-based RAN Optimization",
            "updated": "2023-12-06T04:05:17Z",
            "published": "2023-12-06T04:05:17Z",
            "summary": "The use of learning-based methods for optimizing cellular radio access\nnetworks (RAN) has received increasing attention in recent years. This\ncoincides with a rapid increase in the number of cell sites worldwide, driven\nlargely by dramatic growth in cellular network traffic. Training and\nmaintaining learned models that work well across a large number of cell sites\nhas thus become a pertinent problem. This paper proposes a scalable framework\nfor constructing a reinforcement learning policy bank that can perform RAN\noptimization across a large number of cell sites with varying traffic patterns.\nCentral to our framework is a novel application of anomaly detection techniques\nto assess the compatibility between sites (tasks) and the policy bank. This\nallows our framework to intelligently identify when a policy can be reused for\na task, and when a new policy needs to be trained and added to the policy bank.\nOur results show that our approach to compatibility assessment leads to an\nefficient use of computational resources, by allowing us to construct a\nperformant policy bank without exhaustively training on all tasks, which makes\nit applicable under real-world constraints.",
            "author": [
                "Jimmy Li",
                "Igor Kozlov",
                "Di Wu",
                "Xue Liu",
                "Gregory Dudek"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03277v1",
                "http://arxiv.org/pdf/2312.03277v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03274v1",
            "title": "Empirical Bayes Covariance Decomposition, and a solution to the Multiple\n  Tuning Problem in Sparse PCA",
            "updated": "2023-12-06T04:00:42Z",
            "published": "2023-12-06T04:00:42Z",
            "summary": "Sparse Principal Components Analysis (PCA) has been proposed as a way to\nimprove both interpretability and reliability of PCA. However, use of sparse\nPCA in practice is hindered by the difficulty of tuning the multiple\nhyperparameters that control the sparsity of different PCs (the \"multiple\ntuning problem\", MTP). Here we present a solution to the MTP using Empirical\nBayes methods. We first introduce a general formulation for penalized PCA of a\ndata matrix $\\mathbf{X}$, which includes some existing sparse PCA methods as\nspecial cases. We show that this formulation also leads to a penalized\ndecomposition of the covariance (or Gram) matrix, $\\mathbf{X}^T\\mathbf{X}$. We\nintroduce empirical Bayes versions of these penalized problems, in which the\npenalties are determined by prior distributions that are estimated from the\ndata by maximum likelihood rather than cross-validation. The resulting\n\"Empirical Bayes Covariance Decomposition\" provides a principled and efficient\nsolution to the MTP in sparse PCA, and one that can be immediately extended to\nincorporate other structural assumptions (e.g. non-negative PCA). We illustrate\nthe effectiveness of this approach on both simulated and real data examples.",
            "author": [
                "Joonsuk Kang",
                "Matthew Stephens"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03274v1",
                "http://arxiv.org/pdf/2312.03274v1"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03270v1",
            "title": "Geometric Deep Learning Towards the Iterative Classification of\n  Graph-Based Aircraft Thermal Management Systems",
            "updated": "2023-12-06T03:49:13Z",
            "published": "2023-12-06T03:49:13Z",
            "summary": "In this paper, we use graph-based techniques to investigate the use of\ngeometric deep learning (GDL) in the classification and down-selection of\naircraft thermal management systems (TMS). Previous work developed an\nenumerative graph generation procedure using a component catalog with network\nstructure constraints to represent novel aircraft TMSs as graphs. However, as\nwith many enumerative approaches, combinatorial explosion limits its efficacy\nin many real-world problems, particularly when simulations and optimization\nmust be performed on the many (automatically-generated) physics models.\nTherefore, we present an approach that takes the directed graphs representing\naircraft TMSs and use GDL to predict the critical characteristics of the\nremaining graphs. This paper's findings demonstrate that incorporating\nadditional graph-based features enhances performance, achieving an accuracy of\n97% for determining a graph's compilability and simulatability while using only\n5% of the data for training. By applying iterative classification methods, we\nalso successfully segmented the total set of graphs into more specific groups\nwith an average inclusion of 84.7 of the top 100 highest-performing graphs,\nachieved by training on 45% of the data.",
            "author": [
                "Anthony Sirico Jr.",
                "Daniel R Herber"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03270v1",
                "http://arxiv.org/pdf/2312.03270v1"
            ],
            "primary_category": "cs.CE",
            "category": [
                "cs.CE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03266v1",
            "title": "SO-NeRF: Active View Planning for NeRF using Surrogate Objectives",
            "updated": "2023-12-06T03:31:13Z",
            "published": "2023-12-06T03:31:13Z",
            "summary": "Despite the great success of Neural Radiance Fields (NeRF), its\ndata-gathering process remains vague with only a general rule of thumb of\nsampling as densely as possible. The lack of understanding of what actually\nconstitutes good views for NeRF makes it difficult to actively plan a sequence\nof views that yield the maximal reconstruction quality. We propose Surrogate\nObjectives for Active Radiance Fields (SOAR), which is a set of interpretable\nfunctions that evaluates the goodness of views using geometric and photometric\nvisual cues - surface coverage, geometric complexity, textural complexity, and\nray diversity. Moreover, by learning to infer the SOAR scores from a deep\nnetwork, SOARNet, we are able to effectively select views in mere seconds\ninstead of hours, without the need for prior visits to all the candidate views\nor training any radiance field during such planning. Our experiments show\nSOARNet outperforms the baselines with $\\sim$80x speed-up while achieving\nbetter or comparable reconstruction qualities. We finally show that SOAR is\nmodel-agnostic, thus it generalizes across fully neural-implicit to fully\nexplicit approaches.",
            "author": [
                "Keifer Lee",
                "Shubham Gupta",
                "Sunglyoung Kim",
                "Bhargav Makwana",
                "Chao Chen",
                "Chen Feng"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03266v1",
                "http://arxiv.org/pdf/2312.03266v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03263v1",
            "title": "Weathering Ongoing Uncertainty: Learning and Planning in a Time-Varying\n  Partially Observable Environment",
            "updated": "2023-12-06T03:20:42Z",
            "published": "2023-12-06T03:20:42Z",
            "summary": "Optimal decision-making presents a significant challenge for autonomous\nsystems operating in uncertain, stochastic and time-varying environments.\nEnvironmental variability over time can significantly impact the system's\noptimal decision making strategy for mission completion. To model such\nenvironments, our work combines the previous notion of Time-Varying Markov\nDecision Processes (TVMDP) with partial observability and introduces\nTime-Varying Partially Observable Markov Decision Processes (TV-POMDP). We\npropose a two-pronged approach to accurately estimate and plan within the\nTV-POMDP: 1) Memory Prioritized State Estimation (MPSE), which leverages\nweighted memory to provide more accurate time-varying transition estimates; and\n2) an MPSE-integrated planning strategy that optimizes long-term rewards while\naccounting for temporal constraint. We validate the proposed framework and\nalgorithms using simulations and hardware, with robots exploring a partially\nobservable, time-varying environments. Our results demonstrate superior\nperformance over standard methods, highlighting the framework's effectiveness\nin stochastic, uncertain, time-varying domains.",
            "author": [
                "Gokul Puthumanaillam",
                "Xiangyu Liu",
                "Negar Mehr",
                "Melkior Ornik"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03263v1",
                "http://arxiv.org/pdf/2312.03263v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03262v1",
            "title": "Low-Cost High-Power Membership Inference by Boosting Relativity",
            "updated": "2023-12-06T03:18:49Z",
            "published": "2023-12-06T03:18:49Z",
            "summary": "We present a robust membership inference attack (RMIA) that amplifies the\ndistinction between population data and the training data on any target model,\nby effectively leveraging both reference models and reference data in our\nlikelihood ratio test. Our algorithm exhibits superior test power\n(true-positive rate) when compared to prior methods, even at extremely low\nfalse-positive error rates (as low as 0). Also, under computation constraints,\nwhere only a limited number of reference models (as few as 1) are available,\nour method performs exceptionally well, unlike some prior attacks that approach\nrandom guessing in such scenarios. Our method lays the groundwork for\ncost-effective and practical yet powerful and robust privacy risk analysis of\nmachine learning algorithms.",
            "author": [
                "Sajjad Zarifzadeh",
                "Philippe Liu",
                "Reza Shokri"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03262v1",
                "http://arxiv.org/pdf/2312.03262v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.CR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03259v1",
            "title": "f-FERM: A Scalable Framework for Robust Fair Empirical Risk Minimization",
            "updated": "2023-12-06T03:14:16Z",
            "published": "2023-12-06T03:14:16Z",
            "summary": "Training and deploying machine learning models that meet fairness criteria\nfor protected groups are fundamental in modern artificial intelligence. While\nnumerous constraints and regularization terms have been proposed in the\nliterature to promote fairness in machine learning tasks, most of these methods\nare not amenable to stochastic optimization due to the complex and nonlinear\nstructure of constraints and regularizers. Here, the term \"stochastic\" refers\nto the ability of the algorithm to work with small mini-batches of data.\nMotivated by the limitation of existing literature, this paper presents a\nunified stochastic optimization framework for fair empirical risk minimization\nbased on f-divergence measures (f-FERM). The proposed stochastic algorithm\nenjoys theoretical convergence guarantees. In addition, our experiments\ndemonstrate the superiority of fairness-accuracy tradeoffs offered by f-FERM\nfor almost all batch sizes (ranging from full-batch to batch size of one).\nMoreover, we show that our framework can be extended to the case where there is\na distribution shift from training to the test data. Our extension is based on\na distributionally robust optimization reformulation of f-FERM objective under\n$L_p$ norms as uncertainty sets. Again, in this distributionally robust\nsetting, f-FERM not only enjoys theoretical convergence guarantees but also\noutperforms other baselines in the literature in the tasks involving\ndistribution shifts. An efficient stochastic implementation of $f$-FERM is\npublicly available.",
            "author": [
                "Sina Baharlouei",
                "Shivam Patel",
                "Meisam Razaviyayn"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03259v1",
                "http://arxiv.org/pdf/2312.03259v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03256v1",
            "title": "CAFE: Towards Compact, Adaptive, and Fast Embedding for Large-scale\n  Recommendation Models",
            "updated": "2023-12-06T03:09:19Z",
            "published": "2023-12-06T03:09:19Z",
            "summary": "Recently, the growing memory demands of embedding tables in Deep Learning\nRecommendation Models (DLRMs) pose great challenges for model training and\ndeployment. Existing embedding compression solutions cannot simultaneously meet\nthree key design requirements: memory efficiency, low latency, and adaptability\nto dynamic data distribution. This paper presents CAFE, a Compact, Adaptive,\nand Fast Embedding compression framework that addresses the above requirements.\nThe design philosophy of CAFE is to dynamically allocate more memory resources\nto important features (called hot features), and allocate less memory to\nunimportant ones. In CAFE, we propose a fast and lightweight sketch data\nstructure, named HotSketch, to capture feature importance and report hot\nfeatures in real time. For each reported hot feature, we assign it a unique\nembedding. For the non-hot features, we allow multiple features to share one\nembedding by using hash embedding technique. Guided by our design philosophy,\nwe further propose a multi-level hash embedding framework to optimize the\nembedding tables of non-hot features. We theoretically analyze the accuracy of\nHotSketch, and analyze the model convergence against deviation. Extensive\nexperiments show that CAFE significantly outperforms existing embedding\ncompression methods, yielding 3.92% and 3.68% superior testing AUC on Criteo\nKaggle dataset and CriteoTB dataset at a compression ratio of 10000x. The\nsource codes of CAFE are available at GitHub.",
            "author": [
                "Hailin Zhang",
                "Zirui Liu",
                "Boxuan Chen",
                "Yikai Zhao",
                "Tong Zhao",
                "Tong Yang",
                "Bin Cui"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03256v1",
                "http://arxiv.org/pdf/2312.03256v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03253v1",
            "title": "Seller-side Outcome Fairness in Online Marketplaces",
            "updated": "2023-12-06T02:58:49Z",
            "published": "2023-12-06T02:58:49Z",
            "summary": "This paper aims to investigate and achieve seller-side fairness within online\nmarketplaces, where many sellers and their items are not sufficiently exposed\nto customers in an e-commerce platform. This phenomenon raises concerns\nregarding the potential loss of revenue associated with less exposed items as\nwell as less marketplace diversity. We introduce the notion of seller-side\noutcome fairness and build an optimization model to balance collected\nrecommendation rewards and the fairness metric. We then propose a\ngradient-based data-driven algorithm based on the duality and bandit theory.\nOur numerical experiments on real e-commerce data sets show that our algorithm\ncan lift seller fairness measures while not hurting metrics like collected\nGross Merchandise Value (GMV) and total purchases.",
            "author": [
                "Zikun Ye",
                "Reza Yousefi Maragheh",
                "Lalitesh Morishetti",
                "Shanu Vashishtha",
                "Jason Cho",
                "Kaushiki Nag",
                "Sushant Kumar",
                "Kannan Achan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03253v1",
                "http://arxiv.org/pdf/2312.03253v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03252v1",
            "title": "Privacy-Preserving Task-Oriented Semantic Communications Against Model\n  Inversion Attacks",
            "updated": "2023-12-06T02:57:56Z",
            "published": "2023-12-06T02:57:56Z",
            "summary": "Semantic communication has been identified as a core technology for the sixth\ngeneration (6G) of wireless networks. Recently, task-oriented semantic\ncommunications have been proposed for low-latency inference with limited\nbandwidth. Although transmitting only task-related information does protect a\ncertain level of user privacy, adversaries could apply model inversion\ntechniques to reconstruct the raw data or extract useful information, thereby\ninfringing on users' privacy. To mitigate privacy infringement, this paper\nproposes an information bottleneck and adversarial learning (IBAL) approach to\nprotect users' privacy against model inversion attacks. Specifically, we\nextract task-relevant features from the input based on the information\nbottleneck (IB) theory. To overcome the difficulty in calculating the mutual\ninformation in high-dimensional space, we derive a variational upper bound to\nestimate the true mutual information. To prevent data reconstruction from\ntask-related features by adversaries, we leverage adversarial learning to train\nencoder to fool adversaries by maximizing reconstruction distortion.\nFurthermore, considering the impact of channel variations on privacy-utility\ntrade-off and the difficulty in manually tuning the weights of each loss, we\npropose an adaptive weight adjustment method. Numerical results demonstrate\nthat the proposed approaches can effectively protect privacy without\nsignificantly affecting task performance and achieve better privacy-utility\ntrade-offs than baseline methods.",
            "author": [
                "Yanhu Wang",
                "Shuaishuai Guo",
                "Yiqin Deng",
                "Haixia Zhang",
                "Yuguang Fang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03252v1",
                "http://arxiv.org/pdf/2312.03252v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03248v1",
            "title": "Customizable Combination of Parameter-Efficient Modules for Multi-Task\n  Learning",
            "updated": "2023-12-06T02:47:56Z",
            "published": "2023-12-06T02:47:56Z",
            "summary": "Modular and composable transfer learning is an emerging direction in the\nfield of Parameter Efficient Fine-Tuning, as it enables neural networks to\nbetter organize various aspects of knowledge, leading to improved cross-task\ngeneralization. In this paper, we introduce a novel approach Customized\nPolytropon C-Poly that combines task-common skills and task-specific skills,\nwhile the skill parameters being highly parameterized using low-rank\ntechniques. Each task is associated with a customizable number of exclusive\nspecialized skills and also benefits from skills shared with peer tasks. A\nskill assignment matrix is jointly learned. To evaluate our approach, we\nconducted extensive experiments on the Super-NaturalInstructions and the\nSuperGLUE benchmarks. Our findings demonstrate that C-Poly outperforms\nfully-shared, task-specific, and skill-indistinguishable baselines,\nsignificantly enhancing the sample efficiency in multi-task learning scenarios.",
            "author": [
                "Haowen Wang",
                "Tao Sun",
                "Cong Fan",
                "Jinjie Gu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03248v1",
                "http://arxiv.org/pdf/2312.03248v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03245v1",
            "title": "A Simple Framework to Enhance the Adversarial Robustness of Deep\n  Learning-based Intrusion Detection System",
            "updated": "2023-12-06T02:33:12Z",
            "published": "2023-12-06T02:33:12Z",
            "summary": "Deep learning based intrusion detection systems (DL-based IDS) have emerged\nas one of the best choices for providing security solutions against various\nnetwork intrusion attacks. However, due to the emergence and development of\nadversarial deep learning technologies, it becomes challenging for the adoption\nof DL models into IDS. In this paper, we propose a novel IDS architecture that\ncan enhance the robustness of IDS against adversarial attacks by combining\nconventional machine learning (ML) models and Deep Learning models. The\nproposed DLL-IDS consists of three components: DL-based IDS, adversarial\nexample (AE) detector, and ML-based IDS. We first develop a novel AE detector\nbased on the local intrinsic dimensionality (LID). Then, we exploit the low\nattack transferability between DL models and ML models to find a robust ML\nmodel that can assist us in determining the maliciousness of AEs. If the input\ntraffic is detected as an AE, the ML-based IDS will predict the maliciousness\nof input traffic, otherwise the DL-based IDS will work for the prediction. The\nfusion mechanism can leverage the high prediction accuracy of DL models and low\nattack transferability between DL models and ML models to improve the\nrobustness of the whole system. In our experiments, we observe a significant\nimprovement in the prediction performance of the IDS when subjected to\nadversarial attack, achieving high accuracy with low resource consumption.",
            "author": [
                "Xinwei Yuan",
                "Shu Han",
                "Wei Huang",
                "Hongliang Ye",
                "Xianglong Kong",
                "Fan Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03245v1",
                "http://arxiv.org/pdf/2312.03245v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03243v1",
            "title": "Generalizable Neural Physics Solvers by Baldwinian Evolution",
            "updated": "2023-12-06T02:31:12Z",
            "published": "2023-12-06T02:31:12Z",
            "summary": "Physics-informed neural networks (PINNs) are at the forefront of scientific\nmachine learning, making possible the creation of machine intelligence that is\ncognizant of physical laws and able to accurately simulate them. In this paper,\nthe potential of discovering PINNs that generalize over an entire family of\nphysics tasks is studied, for the first time, through a biological lens of the\nBaldwin effect. Drawing inspiration from the neurodevelopment of precocial\nspecies that have evolved to learn, predict and react quickly to their\nenvironment, we envision PINNs that are pre-wired with connection strengths\ninducing strong biases towards efficient learning of physics. To this end,\nevolutionary selection pressure (guided by proficiency over a family of tasks)\nis coupled with lifetime learning (to specialize on a smaller subset of those\ntasks) to produce PINNs that demonstrate fast and physics-compliant prediction\ncapabilities across a range of empirically challenging problem instances. The\nBaldwinian approach achieves an order of magnitude improvement in prediction\naccuracy at a fraction of the computation cost compared to state-of-the-art\nresults with PINNs meta-learned by gradient descent. This paper marks a leap\nforward in the meta-learning of PINNs as generalizable physics solvers.",
            "author": [
                "Jian Cheng Wong",
                "Chin Chun Ooi",
                "Abhishek Gupta",
                "Pao-Hsiung Chiu",
                "Joshua Shao Zheng Low",
                "My Ha Dao",
                "Yew-Soon Ong"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03243v1",
                "http://arxiv.org/pdf/2312.03243v1"
            ],
            "primary_category": "cs.NE",
            "category": [
                "cs.NE",
                "cs.CE",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03236v1",
            "title": "Multicoated and Folded Graph Neural Networks with Strong Lottery Tickets",
            "updated": "2023-12-06T02:16:44Z",
            "published": "2023-12-06T02:16:44Z",
            "summary": "The Strong Lottery Ticket Hypothesis (SLTH) demonstrates the existence of\nhigh-performing subnetworks within a randomly initialized model, discoverable\nthrough pruning a convolutional neural network (CNN) without any weight\ntraining. A recent study, called Untrained GNNs Tickets (UGT), expanded SLTH\nfrom CNNs to shallow graph neural networks (GNNs). However, discrepancies\npersist when comparing baseline models with learned dense weights.\nAdditionally, there remains an unexplored area in applying SLTH to deeper GNNs,\nwhich, despite delivering improved accuracy with additional layers, suffer from\nexcessive memory requirements. To address these challenges, this work utilizes\nMulticoated Supermasks (M-Sup), a scalar pruning mask method, and implements it\nin GNNs by proposing a strategy for setting its pruning thresholds adaptively.\nIn the context of deep GNNs, this research uncovers the existence of untrained\nrecurrent networks, which exhibit performance on par with their trained\nfeed-forward counterparts. This paper also introduces the Multi-Stage Folding\nand Unshared Masks methods to expand the search space in terms of both\narchitecture and parameters. Through the evaluation of various datasets,\nincluding the Open Graph Benchmark (OGB), this work establishes a triple-win\nscenario for SLTH-based GNNs: by achieving high sparsity, competitive\nperformance, and high memory efficiency with up to 98.7\\% reduction, it\ndemonstrates suitability for energy-efficient graph processing.",
            "author": [
                "Jiale Yan",
                "Hiroaki Ito",
                "\u00c1ngel L\u00f3pez Garc\u00eda-Arias",
                "Yasuyuki Okoshi",
                "Hikari Otsuka",
                "Kazushi Kawamura",
                "Thiem Van Chu",
                "Masato Motomura"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03236v1",
                "http://arxiv.org/pdf/2312.03236v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03235v1",
            "title": "HEET: A Heterogeneity Measure to Quantify the Difference across\n  Distributed Computing Systems",
            "updated": "2023-12-06T02:15:19Z",
            "published": "2023-12-06T02:15:19Z",
            "summary": "Although system heterogeneity has been extensively studied in the past, there\nis yet to be a study on measuring the impact of heterogeneity on system\nperformance. For this purpose, we propose a heterogeneity measure that can\ncharacterize the impact of the heterogeneity of a system on its performance\nbehavior in terms of throughput or makespan. We develop a mathematical model to\ncharacterize a heterogeneous system in terms of its task and machine\nheterogeneity dimensions and then reduce it to a single value, called\nHomogeneous Equivalent Execution Time (HEET), which represents the execution\ntime behavior of the entire system. We used AWS EC2 instances to implement a\nreal-world machine learning inference system. Performance evaluation of the\nHEET score across different heterogeneous system configurations demonstrates\nthat HEET can accurately characterize the performance behavior of these\nsystems. In particular, the results show that our proposed method is capable of\npredicting the true makespan of heterogeneous systems without online\nevaluations with an average precision of 84%. This heterogeneity measure is\ninstrumental for solution architects to configure their systems proactively to\nbe sufficiently heterogeneous to meet their desired performance objectives.",
            "author": [
                "Ali Mokhtari",
                "Saeid Ghafouri",
                "Pooyan Jamshidi",
                "Mohsen Amini Salehi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03235v1",
                "http://arxiv.org/pdf/2312.03235v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC",
                "cs.PF"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03231v1",
            "title": "Deep Multimodal Fusion for Surgical Feedback Classification",
            "updated": "2023-12-06T01:59:47Z",
            "published": "2023-12-06T01:59:47Z",
            "summary": "Quantification of real-time informal feedback delivered by an experienced\nsurgeon to a trainee during surgery is important for skill improvements in\nsurgical training. Such feedback in the live operating room is inherently\nmultimodal, consisting of verbal conversations (e.g., questions and answers) as\nwell as non-verbal elements (e.g., through visual cues like pointing to\nanatomic elements). In this work, we leverage a clinically-validated\nfive-category classification of surgical feedback: \"Anatomic\", \"Technical\",\n\"Procedural\", \"Praise\" and \"Visual Aid\". We then develop a multi-label machine\nlearning model to classify these five categories of surgical feedback from\ninputs of text, audio, and video modalities. The ultimate goal of our work is\nto help automate the annotation of real-time contextual surgical feedback at\nscale. Our automated classification of surgical feedback achieves AUCs ranging\nfrom 71.5 to 77.6 with the fusion improving performance by 3.1%. We also show\nthat high-quality manual transcriptions of feedback audio from experts improve\nAUCs to between 76.5 and 96.2, which demonstrates a clear path toward future\nimprovements. Empirically, we find that the Staged training strategy, with\nfirst pre-training each modality separately and then training them jointly, is\nmore effective than training different modalities altogether. We also present\nintuitive findings on the importance of modalities for different feedback\ncategories. This work offers an important first look at the feasibility of\nautomated classification of real-world live surgical feedback based on text,\naudio, and video modalities.",
            "author": [
                "Rafal Kocielnik",
                "Elyssa Y. Wong",
                "Timothy N. Chu",
                "Lydia Lin",
                "De-An Huang",
                "Jiayun Wang",
                "Anima Anandkumar",
                "Andrew J. Hung"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03231v1",
                "http://arxiv.org/pdf/2312.03231v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CV",
                "cs.HC",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03225v1",
            "title": "Snake Robot with Tactile Perception Navigates on Large-scale Challenging\n  Terrain",
            "updated": "2023-12-06T01:50:47Z",
            "published": "2023-12-06T01:50:47Z",
            "summary": "Along with the advancement of robot skin technology, there has been notable\nprogress in the development of snake robots featuring body-surface tactile\nperception. In this study, we proposed a locomotion control framework for snake\nrobots that integrates tactile perception to augment their adaptability to\nvarious terrains. Our approach embraces a hierarchical reinforcement learning\n(HRL) architecture, wherein the high-level orchestrates global navigation\nstrategies while the low-level uses curriculum learning for local navigation\nmaneuvers. Due to the significant computational demands of collision detection\nin whole-body tactile sensing, the efficiency of the simulator is severely\ncompromised. Thus a distributed training pattern to mitigate the efficiency\nreduction was adopted. We evaluated the navigation performance of the snake\nrobot in complex large-scale cave exploration with challenging terrains to\nexhibit improvements in motion efficiency, evidencing the efficacy of tactile\nperception in terrain-adaptive locomotion of snake robots.",
            "author": [
                "Shuo Jiang",
                "Adarsh Salagame",
                "Alireza Ramezani",
                "Lawson Wong"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03225v1",
                "http://arxiv.org/pdf/2312.03225v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03223v1",
            "title": "Hierarchical RL-Guided Large-scale Navigation of a Snake Robot",
            "updated": "2023-12-06T01:44:58Z",
            "published": "2023-12-06T01:44:58Z",
            "summary": "Classical snake robot control leverages mimicking snake-like gaits tuned for\nspecific environments. However, to operate adaptively in unstructured\nenvironments, gait generation must be dynamically scheduled. In this work, we\npresent a four-layer hierarchical control scheme to enable the snake robot to\nnavigate freely in large-scale environments. The proposed model decomposes\nnavigation into global planning, local planning, gait generation, and gait\ntracking. Using reinforcement learning (RL) and a central pattern generator\n(CPG), our method learns to navigate in complex mazes within hours and can be\ndirectly deployed to arbitrary new environments in a zero-shot fashion. We use\nthe high-fidelity model of Northeastern's slithering robot COBRA to test the\neffectiveness of the proposed hierarchical control approach.",
            "author": [
                "Shuo Jiang",
                "Adarsh Salagame",
                "Alireza Ramezani",
                "Lawson Wong"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03223v1",
                "http://arxiv.org/pdf/2312.03223v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03222v1",
            "title": "Predicting Scores of Various Aesthetic Attribute Sets by Learning from\n  Overall Score Labels",
            "updated": "2023-12-06T01:41:49Z",
            "published": "2023-12-06T01:41:49Z",
            "summary": "Now many mobile phones embed deep-learning models for evaluation or guidance\non photography. These models cannot provide detailed results like human pose\nscores or scene color scores because of the rare of corresponding aesthetic\nattribute data. However, the annotation of image aesthetic attribute scores\nrequires experienced artists and professional photographers, which hinders the\ncollection of large-scale fully-annotated datasets. In this paper, we propose\nto replace image attribute labels with feature extractors. First, a novel\naesthetic attribute evaluation framework based on attribute features is\nproposed to predict attribute scores and overall scores. We call it the F2S\n(attribute features to attribute scores) model. We use networks from different\ntasks to provide attribute features to our F2S models. Then, we define an\naesthetic attribute contribution to describe the role of aesthetic attributes\nthroughout an image and use it with the attribute scores and the overall scores\nto train our F2S model. Sufficient experiments on publicly available datasets\ndemonstrate that our F2S model achieves comparable performance with those\ntrained on the datasets with fully-annotated aesthetic attribute score labels.\nOur method makes it feasible to learn meaningful attribute scores for various\naesthetic attribute sets in different types of images with only overall\naesthetic scores.",
            "author": [
                "Heng Huang",
                "Xin Jin",
                "Yaqi Liu",
                "Hao Lou",
                "Chaoen Xiao",
                "Shuai Cui",
                "Xinning Li",
                "Dongqing Zou"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03222v1",
                "http://arxiv.org/pdf/2312.03222v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03218v1",
            "title": "Accelerated Gradient Algorithms with Adaptive Subspace Search for\n  Instance-Faster Optimization",
            "updated": "2023-12-06T01:16:10Z",
            "published": "2023-12-06T01:16:10Z",
            "summary": "Gradient-based minimax optimal algorithms have greatly promoted the\ndevelopment of continuous optimization and machine learning. One seminal work\ndue to Yurii Nesterov [Nes83a] established $\\tilde{\\mathcal{O}}(\\sqrt{L/\\mu})$\ngradient complexity for minimizing an $L$-smooth $\\mu$-strongly convex\nobjective. However, an ideal algorithm would adapt to the explicit complexity\nof a particular objective function and incur faster rates for simpler problems,\ntriggering our reconsideration of two defeats of existing optimization modeling\nand analysis. (i) The worst-case optimality is neither the instance optimality\nnor such one in reality. (ii) Traditional $L$-smoothness condition may not be\nthe primary abstraction/characterization for modern practical problems.\n  In this paper, we open up a new way to design and analyze gradient-based\nalgorithms with direct applications in machine learning, including linear\nregression and beyond. We introduce two factors $(\\alpha, \\tau_{\\alpha})$ to\nrefine the description of the degenerated condition of the optimization\nproblems based on the observation that the singular values of Hessian often\ndrop sharply. We design adaptive algorithms that solve simpler problems without\npre-known knowledge with reduced gradient or analogous oracle accesses. The\nalgorithms also improve the state-of-art complexities for several problems in\nmachine learning, thereby solving the open problem of how to design faster\nalgorithms in light of the known complexity lower bounds. Specially, with the\n$\\mathcal{O}(1)$-nuclear norm bounded, we achieve an optimal\n$\\tilde{\\mathcal{O}}(\\mu^{-1/3})$ (v.s. $\\tilde{\\mathcal{O}}(\\mu^{-1/2})$)\ngradient complexity for linear regression. We hope this work could invoke the\nrethinking for understanding the difficulty of modern problems in optimization.",
            "author": [
                "Yuanshi Liu",
                "Hanzhen Zhao",
                "Yang Xu",
                "Pengyun Yue",
                "Cong Fang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03218v1",
                "http://arxiv.org/pdf/2312.03218v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CC",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03216v1",
            "title": "SDSRA: A Skill-Driven Skill-Recombination Algorithm for Efficient Policy\n  Learning",
            "updated": "2023-12-06T01:15:34Z",
            "published": "2023-12-06T01:15:34Z",
            "summary": "In this paper, we introduce a novel algorithm - the Skill-Driven Skill\nRecombination Algorithm (SDSRA) - an innovative framework that significantly\nenhances the efficiency of achieving maximum entropy in reinforcement learning\ntasks. We find that SDSRA achieves faster convergence compared to the\ntraditional Soft Actor-Critic (SAC) algorithm and produces improved policies.\nBy integrating skill-based strategies within the robust Actor-Critic framework,\nSDSRA demonstrates remarkable adaptability and performance across a wide array\nof complex and diverse benchmarks.",
            "author": [
                "Eric H. Jiang",
                "Andrew Lizarraga"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03216v1",
                "http://arxiv.org/pdf/2312.03216v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03213v1",
            "title": "Bootstrap Your Own Variance",
            "updated": "2023-12-06T01:02:13Z",
            "published": "2023-12-06T01:02:13Z",
            "summary": "Understanding model uncertainty is important for many applications. We\npropose Bootstrap Your Own Variance (BYOV), combining Bootstrap Your Own Latent\n(BYOL), a negative-free Self-Supervised Learning (SSL) algorithm, with Bayes by\nBackprop (BBB), a Bayesian method for estimating model posteriors. We find that\nthe learned predictive std of BYOV vs. a supervised BBB model is well captured\nby a Gaussian distribution, providing preliminary evidence that the learned\nparameter posterior is useful for label free uncertainty estimation. BYOV\nimproves upon the deterministic BYOL baseline (+2.83% test ECE, +1.03% test\nBrier) and presents better calibration and reliability when tested with various\naugmentations (eg: +2.4% test ECE, +1.2% test Brier for Salt & Pepper noise).",
            "author": [
                "Polina Turishcheva",
                "Jason Ramapuram",
                "Sinead Williamson",
                "Dan Busbridge",
                "Eeshan Dhekane",
                "Russ Webb"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03213v1",
                "http://arxiv.org/pdf/2312.03213v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03212v1",
            "title": "Constrained Bayesian Optimization Under Partial Observations: Balanced\n  Improvements and Provable Convergence",
            "updated": "2023-12-06T01:00:07Z",
            "published": "2023-12-06T01:00:07Z",
            "summary": "The partially observable constrained optimization problems (POCOPs) impede\ndata-driven optimization techniques since an infeasible solution of POCOPs can\nprovide little information about the objective as well as the constraints. We\nendeavor to design an efficient and provable method for expensive POCOPs under\nthe framework of constrained Bayesian optimization. Our method consists of two\nkey components. Firstly, we present an improved design of the acquisition\nfunctions that introduces balanced exploration during optimization. We\nrigorously study the convergence properties of this design to demonstrate its\neffectiveness. Secondly, we propose a Gaussian process embedding different\nlikelihoods as the surrogate model for a partially observable constraint. This\nmodel leads to a more accurate representation of the feasible regions compared\nto traditional classification-based models. Our proposed method is empirically\nstudied on both synthetic and real-world problems. The results demonstrate the\ncompetitiveness of our method for solving POCOPs.",
            "author": [
                "Shengbo Wang",
                "Ke Li"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03212v1",
                "http://arxiv.org/pdf/2312.03212v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03207v1",
            "title": "Satellite Imagery and AI: A New Era in Ocean Conservation, from Research\n  to Deployment and Impact",
            "updated": "2023-12-06T00:48:50Z",
            "published": "2023-12-06T00:48:50Z",
            "summary": "Illegal, unreported, and unregulated (IUU) fishing poses a global threat to\nocean habitats. Publicly available satellite data offered by NASA and the\nEuropean Space Agency (ESA) provide an opportunity to actively monitor this\nactivity. Effectively leveraging satellite data for maritime conservation\nrequires highly reliable machine learning models operating globally with\nminimal latency. This paper introduces three specialized computer vision models\ndesigned for synthetic aperture radar (Sentinel-1), optical imagery\n(Sentinel-2), and nighttime lights (Suomi-NPP/NOAA-20). It also presents best\npractices for developing and delivering real-time computer vision services for\nconservation. These models have been deployed in Skylight, a real time maritime\nmonitoring platform, which is provided at no cost to users worldwide.",
            "author": [
                "Patrick Beukema",
                "Favyen Bastani",
                "Piper Wolters",
                "Henry Herzog",
                "Joe Ferdinando"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03207v1",
                "http://arxiv.org/pdf/2312.03207v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03205v1",
            "title": "Who Leaked the Model? Tracking IP Infringers in Accountable Federated\n  Learning",
            "updated": "2023-12-06T00:47:55Z",
            "published": "2023-12-06T00:47:55Z",
            "summary": "Federated learning (FL) emerges as an effective collaborative learning\nframework to coordinate data and computation resources from massive and\ndistributed clients in training. Such collaboration results in non-trivial\nintellectual property (IP) represented by the model parameters that should be\nprotected and shared by the whole party rather than an individual user.\nMeanwhile, the distributed nature of FL endorses a malicious client the\nconvenience to compromise IP through illegal model leakage to unauthorized\nthird parties. To block such IP leakage, it is essential to make the IP\nidentifiable in the shared model and locate the anonymous infringer who first\nleaks it. The collective challenges call for \\emph{accountable federated\nlearning}, which requires verifiable ownership of the model and is capable of\nrevealing the infringer's identity upon leakage. In this paper, we propose\nDecodable Unique Watermarking (DUW) for complying with the requirements of\naccountable FL. Specifically, before a global model is sent to a client in an\nFL round, DUW encodes a client-unique key into the model by leveraging a\nbackdoor-based watermark injection. To identify the infringer of a leaked\nmodel, DUW examines the model and checks if the triggers can be decoded as the\ncorresponding keys. Extensive empirical results show that DUW is highly\neffective and robust, achieving over $99\\%$ watermark success rate for Digits,\nCIFAR-10, and CIFAR-100 datasets under heterogeneous FL settings, and\nidentifying the IP infringer with $100\\%$ accuracy even after common watermark\nremoval attempts.",
            "author": [
                "Shuyang Yu",
                "Junyuan Hong",
                "Yi Zeng",
                "Fei Wang",
                "Ruoxi Jia",
                "Jiayu Zhou"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03205v1",
                "http://arxiv.org/pdf/2312.03205v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03203v1",
            "title": "Feature 3DGS: Supercharging 3D Gaussian Splatting to Enable Distilled\n  Feature Fields",
            "updated": "2023-12-06T00:46:30Z",
            "published": "2023-12-06T00:46:30Z",
            "summary": "3D scene representations have gained immense popularity in recent years.\nMethods that use Neural Radiance fields are versatile for traditional tasks\nsuch as novel view synthesis. In recent times, some work has emerged that aims\nto extend the functionality of NeRF beyond view synthesis, for semantically\naware tasks such as editing and segmentation using 3D feature field\ndistillation from 2D foundation models. However, these methods have two major\nlimitations: (a) they are limited by the rendering speed of NeRF pipelines, and\n(b) implicitly represented feature fields suffer from continuity artifacts\nreducing feature quality. Recently, 3D Gaussian Splatting has shown\nstate-of-the-art performance on real-time radiance field rendering. In this\nwork, we go one step further: in addition to radiance field rendering, we\nenable 3D Gaussian splatting on arbitrary-dimension semantic features via 2D\nfoundation model distillation. This translation is not straightforward: naively\nincorporating feature fields in the 3DGS framework leads to warp-level\ndivergence. We propose architectural and training changes to efficiently avert\nthis problem. Our proposed method is general, and our experiments showcase\nnovel view semantic segmentation, language-guided editing and segment anything\nthrough learning feature fields from state-of-the-art 2D foundation models such\nas SAM and CLIP-LSeg. Across experiments, our distillation method is able to\nprovide comparable or better results, while being significantly faster to both\ntrain and render. Additionally, to the best of our knowledge, we are the first\nmethod to enable point and bounding-box prompting for radiance field\nmanipulation, by leveraging the SAM model. Project website at:\nhttps://feature-3dgs.github.io/",
            "author": [
                "Shijie Zhou",
                "Haoran Chang",
                "Sicheng Jiang",
                "Zhiwen Fan",
                "Zehao Zhu",
                "Dejia Xu",
                "Pradyumna Chari",
                "Suya You",
                "Zhangyang Wang",
                "Achuta Kadambi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03203v1",
                "http://arxiv.org/pdf/2312.03203v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03196v2",
            "title": "Domain Invariant Representation Learning and Sleep Dynamics Modeling for\n  Automatic Sleep Staging",
            "updated": "2023-12-07T01:58:54Z",
            "published": "2023-12-06T00:28:08Z",
            "summary": "Sleep staging has become a critical task in diagnosing and treating sleep\ndisorders to prevent sleep related diseases. With rapidly growing large scale\npublic sleep databases and advances in machine learning, significant progress\nhas been made toward automatic sleep staging. However, previous studies face\nsome critical problems in sleep studies; the heterogeneity of subjects'\nphysiological signals, the inability to extract meaningful information from\nunlabeled sleep signal data to improve predictive performances, the difficulty\nin modeling correlations between sleep stages, and the lack of an effective\nmechanism to quantify predictive uncertainty. In this study, we propose a\nneural network based automatic sleep staging model, named DREAM, to learn\ndomain generalized representations from physiological signals and models sleep\ndynamics. DREAM learns sleep related and subject invariant representations from\ndiverse subjects' sleep signal segments and models sleep dynamics by capturing\ninteractions between sequential signal segments and between sleep stages. In\nthe experiments, we demonstrate that DREAM outperforms the existing sleep\nstaging methods on three datasets. The case study demonstrates that our model\ncan learn the generalized decision function resulting in good prediction\nperformances for the new subjects, especially in case there are differences\nbetween testing and training subjects. The usage of unlabeled data shows the\nbenefit of leveraging unlabeled EEG data. Further, uncertainty quantification\ndemonstrates that DREAM provides prediction uncertainty, making the model\nreliable and helping sleep experts in real world applications.",
            "author": [
                "Seungyeon Lee",
                "Thai-Hoang Pham",
                "Zhao Cheng",
                "Ping Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03196v2",
                "http://arxiv.org/pdf/2312.03196v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03194v1",
            "title": "Corporate Bankruptcy Prediction with Domain-Adapted BERT",
            "updated": "2023-12-06T00:05:25Z",
            "published": "2023-12-06T00:05:25Z",
            "summary": "This study performs BERT-based analysis, which is a representative\ncontextualized language model, on corporate disclosure data to predict\nimpending bankruptcies. Prior literature on bankruptcy prediction mainly\nfocuses on developing more sophisticated prediction methodologies with\nfinancial variables. However, in our study, we focus on improving the quality\nof input dataset. Specifically, we employ BERT model to perform sentiment\nanalysis on MD&A disclosures. We show that BERT outperforms dictionary-based\npredictions and Word2Vec-based predictions in terms of adjusted R-square in\nlogistic regression, k-nearest neighbor (kNN-5), and linear kernel support\nvector machine (SVM). Further, instead of pre-training the BERT model from\nscratch, we apply self-learning with confidence-based filtering to corporate\ndisclosure data (10-K). We achieve the accuracy rate of 91.56% and demonstrate\nthat the domain adaptation procedure brings a significant improvement in\nprediction accuracy.",
            "author": [
                "Alex Kim",
                "Sangwon Yoon"
            ],
            "link": [
                "http://dx.doi.org/10.18653/v1/2021.econlp-1.4",
                "http://arxiv.org/abs/2312.03194v1",
                "http://arxiv.org/pdf/2312.03194v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG",
                "econ.GN",
                "q-fin.EC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03187v1",
            "title": "FERGI: Automatic Annotation of User Preferences for Text-to-Image\n  Generation from Spontaneous Facial Expression Reaction",
            "updated": "2023-12-05T23:33:49Z",
            "published": "2023-12-05T23:33:49Z",
            "summary": "Researchers have proposed to use data of human preference feedback to\nfine-tune text-to-image generative models. However, the scalability of human\nfeedback collection has been limited by its reliance on manual annotation.\nTherefore, we develop and test a method to automatically annotate user\npreferences from their spontaneous facial expression reaction to the generated\nimages. We collect a dataset of Facial Expression Reaction to Generated Images\n(FERGI) and show that the activations of multiple facial action units (AUs) are\nhighly correlated with user evaluations of the generated images. Specifically,\nAU4 (brow lowerer) is most consistently reflective of negative evaluations of\nthe generated image. This can be useful in two ways. Firstly, we can\nautomatically annotate user preferences between image pairs with substantial\ndifference in AU4 responses to them with an accuracy significantly\noutperforming state-of-the-art scoring models. Secondly, directly integrating\nthe AU4 responses with the scoring models improves their consistency with human\npreferences. Additionally, the AU4 response best reflects the user's evaluation\nof the image fidelity, making it complementary to the state-of-the-art scoring\nmodels, which are generally better at reflecting image-text alignment. Finally,\nthis method of automatic annotation with facial expression analysis can be\npotentially generalized to other generation tasks. The code is available at\nhttps://github.com/ShuangquanFeng/FERGI, and the dataset is also available at\nthe same link for research purposes.",
            "author": [
                "Shuangquan Feng",
                "Junhua Ma",
                "Virginia R. de Sa"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03187v1",
                "http://arxiv.org/pdf/2312.03187v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.HC",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03186v1",
            "title": "Data-Driven Traffic Reconstruction and Kernel Methods for Identifying\n  Stop-and-Go Congestion",
            "updated": "2023-12-05T23:32:48Z",
            "published": "2023-12-05T23:32:48Z",
            "summary": "Identifying stop-and-go events (SAGs) in traffic flow presents an important\navenue for advancing data-driven research for climate change mitigation and\nsustainability, owing to their substantial impact on carbon emissions, travel\ntime, fuel consumption, and roadway safety. In fact, SAGs are estimated to\naccount for 33-50% of highway driving externalities. However, insufficient\nattention has been paid to precisely quantifying where, when, and how much\nthese SAGs take place -necessary for downstream decision making, such as\nintervention design and policy analysis. A key challenge is that the data\navailable to researchers and governments are typically sparse and aggregated to\na granularity that obscures SAGs. To overcome such data limitations, this study\nthus explores the use of traffic reconstruction techniques for SAG\nidentification. In particular, we introduce a kernel-based method for\nidentifying spatio-temporal features in traffic and leverage bootstrapping to\nquantify the uncertainty of the reconstruction process. Experimental results on\nCalifornia highway data demonstrate the promise of the method for capturing\nSAGs. This work contributes to a foundation for data-driven decision making to\nadvance sustainability of traffic systems.",
            "author": [
                "Edgar Ramirez Sanchez",
                "Shreyaa Raghavan",
                "Cathy Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03186v1",
                "http://arxiv.org/pdf/2312.03186v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03185v1",
            "title": "Lung Cancer Detection from CT Scan Images based on Genetic-Independent\n  Recurrent Deep Learning",
            "updated": "2023-12-05T23:31:05Z",
            "published": "2023-12-05T23:31:05Z",
            "summary": "Lung cancer is one of the prevalence diseases in the world which cause many\ndeaths. Detecting early stages of lung cancer is so necessary. So, modeling and\nsimulating some intelligent medical systems is an essential which can help\nspecialist to accurately determine and diagnose the disease. So this paper\ncontributes a new lung cancer detection model in CT images which use machine\nlearning methods. There are three steps in this model: noise reduction\n(pre-processing), segmentation (middle-processing) and optimize segmentation\nfor detect exact are of nodules. This article use some filters for noise\nreduction and then use Independent Recurrent Neural Networks (IndRNN) as deep\nlearning methods for segmentation which optimize and tune by Genetic Algorithm.\nThe results represented that the proposed method can detect exact area of\nnodules in CT images.",
            "author": [
                "Ehsan Sadeghi Pour",
                "Mahdi Esmaeili"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03185v1",
                "http://arxiv.org/pdf/2312.03185v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03182v1",
            "title": "Investigating Technology Usage Span by Analyzing Users' Q&A Traces in\n  Stack Overflow",
            "updated": "2023-12-05T23:17:48Z",
            "published": "2023-12-05T23:17:48Z",
            "summary": "Choosing an appropriate software development technology (e.g., programming\nlanguage) is challenging due to the proliferation of diverse options. The\nselection of inappropriate technologies for development may have a far-reaching\neffect on software developers' career growth. Switching to a different\ntechnology after working with one may lead to a complex learning curve and,\nthus, be more challenging. Therefore, it is crucial for software developers to\nfind technologies that have a high usage span. Intuitively, the usage span of a\ntechnology can be determined by the time span developers have used that\ntechnology. Existing literature focuses on the technology landscape to explore\nthe complex and implicit dependencies among technologies but lacks formal\nstudies to draw insights about their usage span. This paper investigates the\ntechnology usage span by analyzing the question and answering (Q&A) traces of\nStack Overflow (SO), the largest technical Q&A website available to date. In\nparticular, we analyze 6.7 million Q&A traces posted by about 97K active SO\nusers and see what technologies have appeared in their questions or answers\nover 15 years. According to our analysis, C# and Java programming languages\nhave a high usage span, followed by JavaScript. Besides, developers used the\n.NET framework, iOS & Windows Operating Systems (OS), and SQL query language\nfor a long time (on average). Our study also exposes the emerging (i.e., newly\ngrowing) technologies. For example, usages of technologies such as SwiftUI,\n.NET-6.0, Visual Studio 2022, and Blazor WebAssembly framework are increasing.\nThe findings from our study can assist novice developers, startup software\nindustries, and software users in determining appropriate technologies. This\nalso establishes an initial benchmark for future investigation on the use span\nof software technologies.",
            "author": [
                "Saikat Mondal",
                "Debajyoti Mondal",
                "Chanchal K. Roy"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03182v1",
                "http://arxiv.org/pdf/2312.03182v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03180v1",
            "title": "Image reconstructions using sparse dictionary representations and\n  implicit, non-negative mappings",
            "updated": "2023-12-05T23:07:21Z",
            "published": "2023-12-05T23:07:21Z",
            "summary": "Many imaging science tasks can be modeled as a discrete linear inverse\nproblem. Solving linear inverse problems is often challenging, with\nill-conditioned operators and potentially non-unique solutions. Embedding prior\nknowledge, such as smoothness, into the solution can overcome these challenges.\nIn this work, we encode prior knowledge using a non-negative patch dictionary,\nwhich effectively learns a basis from a training set of natural images. In this\ndictionary basis, we desire solutions that are non-negative and sparse (i.e.,\ncontain many zero entries). With these constraints, standard methods for\nsolving discrete linear inverse problems are not directly applicable. One such\napproach is the modified residual norm steepest descent (MRNSD), which produces\nnon-negative solutions but does not induce sparsity. In this paper, we provide\ntwo methods based on MRNSD that promote sparsity. In our first method, we add\nan $\\ell_1$-regularization term with a new, optimal step size. In our second\nmethod, we propose a new non-negative, sparsity-promoting mapping of the\nsolution. We compare the performance of our proposed methods on a number of\nnumerical experiments, including deblurring, image completion, computer\ntomography, and superresolution. Our results show that these methods\neffectively solve discrete linear inverse problems with non-negativity and\nsparsity constraints.",
            "author": [
                "Elizabeth Newman",
                "Jack Michael Solomon",
                "Matthias Chung"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03180v1",
                "http://arxiv.org/pdf/2312.03180v1"
            ],
            "primary_category": "math.NA",
            "category": [
                "math.NA",
                "cs.NA",
                "65F10, 65F22",
                "G.1.3"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03179v1",
            "title": "CaloQVAE : Simulating high-energy particle-calorimeter interactions\n  using hybrid quantum-classical generative models",
            "updated": "2023-12-05T23:05:36Z",
            "published": "2023-12-05T23:05:36Z",
            "summary": "The Large Hadron Collider's high luminosity era presents major computational\nchallenges in the analysis of collision events. Large amounts of Monte Carlo\n(MC) simulation will be required to constrain the statistical uncertainties of\nthe simulated datasets below these of the experimental data. Modelling of\nhigh-energy particles propagating through the calorimeter section of the\ndetector is the most computationally intensive MC simulation task. We introduce\na technique combining recent advancements in generative models and quantum\nannealing for fast and efficient simulation of high-energy particle-calorimeter\ninteractions.",
            "author": [
                "Sehmimul Hoque",
                "Hao Jia",
                "Abhishek Abhishek",
                "Mojde Fadaie",
                "J. Quetzalcoatl Toledo-Mar\u00edn",
                "Tiago Vale",
                "Roger G. Melko",
                "Maximilian Swiatlowski",
                "Wojciech T. Fedorko"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03179v1",
                "http://arxiv.org/pdf/2312.03179v1"
            ],
            "primary_category": "hep-ex",
            "category": [
                "hep-ex",
                "cs.LG",
                "quant-ph",
                "81P68, 68T07, 81V99"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03177v1",
            "title": "Using Curiosity for an Even Representation of Tasks in Continual Offline\n  Reinforcement Learning",
            "updated": "2023-12-05T22:53:05Z",
            "published": "2023-12-05T22:53:05Z",
            "summary": "In this work, we investigate the means of using curiosity on replay buffers\nto improve offline multi-task continual reinforcement learning when tasks,\nwhich are defined by the non-stationarity in the environment, are non labeled\nand not evenly exposed to the learner in time. In particular, we investigate\nthe use of curiosity both as a tool for task boundary detection and as a\npriority metric when it comes to retaining old transition tuples, which we\nrespectively use to propose two different buffers. Firstly, we propose a Hybrid\nReservoir Buffer with Task Separation (HRBTS), where curiosity is used to\ndetect task boundaries that are not known due to the task agnostic nature of\nthe problem. Secondly, by using curiosity as a priority metric when it comes to\nretaining old transition tuples, a Hybrid Curious Buffer (HCB) is proposed. We\nultimately show that these buffers, in conjunction with regular reinforcement\nlearning algorithms, can be used to alleviate the catastrophic forgetting issue\nsuffered by the state of the art on replay buffers when the agent's exposure to\ntasks is not equal along time. We evaluate catastrophic forgetting and the\nefficiency of our proposed buffers against the latest works such as the Hybrid\nReservoir Buffer (HRB) and the Multi-Time Scale Replay Buffer (MTR) in three\ndifferent continual reinforcement learning settings. Experiments were done on\nclassical control tasks and Metaworld environment. Experiments show that our\nproposed replay buffers display better immunity to catastrophic forgetting\ncompared to existing works in most of the settings.",
            "author": [
                "Pankayaraj Pathmanathan",
                "Natalia D\u00edaz-Rodr\u00edguez",
                "Javier Del Ser"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03177v1",
                "http://arxiv.org/pdf/2312.03177v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03176v1",
            "title": "Active Learning for Abrupt Shifts Change-point Detection via\n  Derivative-Aware Gaussian Processes",
            "updated": "2023-12-05T22:44:05Z",
            "published": "2023-12-05T22:44:05Z",
            "summary": "Change-point detection (CPD) is crucial for identifying abrupt shifts in\ndata, which influence decision-making and efficient resource allocation across\nvarious domains. To address the challenges posed by the costly and\ntime-intensive data acquisition in CPD, we introduce the Derivative-Aware\nChange Detection (DACD) method. It leverages the derivative process of a\nGaussian process (GP) for Active Learning (AL), aiming to pinpoint change-point\nlocations effectively. DACD balances the exploitation and exploration of\nderivative processes through multiple data acquisition functions (AFs). By\nutilizing GP derivative mean and variance as criteria, DACD sequentially\nselects the next sampling data point, thus enhancing algorithmic efficiency and\nensuring reliable and accurate results. We investigate the effectiveness of\nDACD method in diverse scenarios and show it outperforms other active learning\nchange-point detection approaches.",
            "author": [
                "Hao Zhao",
                "Rong Pan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03176v1",
                "http://arxiv.org/pdf/2312.03176v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03173v1",
            "title": "A Comparative Study of AI-Generated (GPT-4) and Human-crafted MCQs in\n  Programming Education",
            "updated": "2023-12-05T22:29:43Z",
            "published": "2023-12-05T22:29:43Z",
            "summary": "There is a constant need for educators to develop and maintain effective\nup-to-date assessments. While there is a growing body of research in computing\neducation on utilizing large language models (LLMs) in generation and\nengagement with coding exercises, the use of LLMs for generating programming\nMCQs has not been extensively explored. We analyzed the capability of GPT-4 to\nproduce multiple-choice questions (MCQs) aligned with specific learning\nobjectives (LOs) from Python programming classes in higher education.\nSpecifically, we developed an LLM-powered (GPT-4) system for generation of MCQs\nfrom high-level course context and module-level LOs. We evaluated 651\nLLM-generated and 449 human-crafted MCQs aligned to 246 LOs from 6 Python\ncourses. We found that GPT-4 was capable of producing MCQs with clear language,\na single correct choice, and high-quality distractors. We also observed that\nthe generated MCQs appeared to be well-aligned with the LOs. Our findings can\nbe leveraged by educators wishing to take advantage of the state-of-the-art\ngenerative models to support MCQ authoring efforts.",
            "author": [
                "Jacob Doughty",
                "Zipiao Wan",
                "Anishka Bompelli",
                "Jubahed Qayum",
                "Taozhi Wang",
                "Juran Zhang",
                "Yujia Zheng",
                "Aidan Doyle",
                "Pragnya Sridhar",
                "Arav Agarwal",
                "Christopher Bogart",
                "Eric Keylor",
                "Can Kultur",
                "Jaromir Savelka",
                "Majd Sakr"
            ],
            "link": [
                "http://dx.doi.org/10.1145/3636243.3636256",
                "http://arxiv.org/abs/2312.03173v1",
                "http://arxiv.org/pdf/2312.03173v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY",
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03167v1",
            "title": "Adaptive spectral graph wavelets for collaborative filtering",
            "updated": "2023-12-05T22:22:25Z",
            "published": "2023-12-05T22:22:25Z",
            "summary": "Collaborative filtering is a popular approach in recommender systems, whose\nobjective is to provide personalized item suggestions to potential users based\non their purchase or browsing history. However, personalized recommendations\nrequire considerable amount of behavioral data on users, which is usually\nunavailable for new users, giving rise to the cold-start problem. To help\nalleviate this challenging problem, we introduce a spectral graph wavelet\ncollaborative filtering framework for implicit feedback data, where users,\nitems and their interactions are represented as a bipartite graph.\nSpecifically, we first propose an adaptive transfer function by leveraging a\npower transform with the goal of stabilizing the variance of graph frequencies\nin the spectral domain. Then, we design a deep recommendation model for\nefficient learning of low-dimensional embeddings of users and items using\nspectral graph wavelets in an end-to-end fashion. In addition to capturing the\ngraph's local and global structures, our approach yields localization of graph\nsignals in both spatial and spectral domains, and hence not only learns\ndiscriminative representations of users and items, but also promotes the\nrecommendation quality. The effectiveness of our proposed model is demonstrated\nthrough extensive experiments on real-world benchmark datasets, achieving\nbetter recommendation performance compared with strong baseline methods.",
            "author": [
                "Osama Alshareet",
                "A. Ben Hamza"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03167v1",
                "http://arxiv.org/pdf/2312.03167v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03166v1",
            "title": "Deep Learning for Fast Inference of Mechanistic Models' Parameters",
            "updated": "2023-12-05T22:16:54Z",
            "published": "2023-12-05T22:16:54Z",
            "summary": "Inferring parameters of macro-kinetic growth models, typically represented by\nOrdinary Differential Equations (ODE), from the experimental data is a crucial\nstep in bioprocess engineering. Conventionally, estimates of the parameters are\nobtained by fitting the mechanistic model to observations. Fitting, however,\nrequires a significant computational power. Specifically, during the\ndevelopment of new bioprocesses that use previously unknown organisms or\nstrains, efficient, robust, and computationally cheap methods for parameter\nestimation are of great value. In this work, we propose using Deep Neural\nNetworks (NN) for directly predicting parameters of mechanistic models given\nobservations. The approach requires spending computational resources for\ntraining a NN, nonetheless, once trained, such a network can provide parameter\nestimates orders of magnitude faster than conventional methods. We consider a\ntraining procedure that combines Neural Networks and mechanistic models. We\ndemonstrate the performance of the proposed algorithms on data sampled from\nseveral mechanistic models used in bioengineering describing a typical\nindustrial batch process and compare the proposed method, a typical\ngradient-based fitting procedure, and the combination of the two. We find that,\nwhile Neural Network estimates are slightly improved by further fitting, these\nestimates are measurably better than the fitting procedure alone.",
            "author": [
                "Maxim Borisyak",
                "Stefan Born",
                "Peter Neubauer",
                "Mariano Nicolas Cruz-Bournazou"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03166v1",
                "http://arxiv.org/pdf/2312.03166v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "q-bio.QM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03160v1",
            "title": "HybridNeRF: Efficient Neural Rendering via Adaptive Volumetric Surfaces",
            "updated": "2023-12-05T22:04:49Z",
            "published": "2023-12-05T22:04:49Z",
            "summary": "Neural radiance fields provide state-of-the-art view synthesis quality but\ntend to be slow to render. One reason is that they make use of volume\nrendering, thus requiring many samples (and model queries) per ray at render\ntime. Although this representation is flexible and easy to optimize, most\nreal-world objects can be modeled more efficiently with surfaces instead of\nvolumes, requiring far fewer samples per ray. This observation has spurred\nconsiderable progress in surface representations such as signed distance\nfunctions, but these may struggle to model semi-opaque and thin structures. We\npropose a method, HybridNeRF, that leverages the strengths of both\nrepresentations by rendering most objects as surfaces while modeling the\n(typically) small fraction of challenging regions volumetrically. We evaluate\nHybridNeRF against the challenging Eyeful Tower dataset along with other\ncommonly used view synthesis datasets. When comparing to state-of-the-art\nbaselines, including recent rasterization-based approaches, we improve error\nrates by 15-30% while achieving real-time framerates (at least 36 FPS) for\nvirtual-reality resolutions (2Kx2K).",
            "author": [
                "Haithem Turki",
                "Vasu Agrawal",
                "Samuel Rota Bul\u00f2",
                "Lorenzo Porzi",
                "Peter Kontschieder",
                "Deva Ramanan",
                "Michael Zollh\u00f6fer",
                "Christian Richardt"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03160v1",
                "http://arxiv.org/pdf/2312.03160v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03155v1",
            "title": "Algorithmic Fairness with Feedback",
            "updated": "2023-12-05T21:42:14Z",
            "published": "2023-12-05T21:42:14Z",
            "summary": "The field of algorithmic fairness has rapidly emerged over the past 15 years\nas algorithms have become ubiquitous in everyday lives. Algorithmic fairness\ntraditionally considers statistical notions of fairness algorithms might\nsatisfy in decisions based on noisy data. We first show that these are\ntheoretically disconnected from welfare-based notions of fairness. We then\ndiscuss two individual welfare-based notions of fairness, envy freeness and\nprejudice freeness, and establish conditions under which they are equivalent to\nerror rate balance and predictive parity, respectively. We discuss the\nimplications of these findings in light of the recently discovered\nimpossibility theorem in algorithmic fairness (Kleinberg, Mullainathan, &\nRaghavan (2016), Chouldechova (2017)).",
            "author": [
                "John W. Patty",
                "Elizabeth Maggie Penn"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03155v1",
                "http://arxiv.org/pdf/2312.03155v1"
            ],
            "primary_category": "econ.TH",
            "category": [
                "econ.TH",
                "stat.AP",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03769v1",
            "title": "GPT vs Human for Scientific Reviews: A Dual Source Review on\n  Applications of ChatGPT in Science",
            "updated": "2023-12-05T21:41:52Z",
            "published": "2023-12-05T21:41:52Z",
            "summary": "The new polymath Large Language Models (LLMs) can speed-up greatly scientific\nreviews, possibly using more unbiased quantitative metrics, facilitating\ncross-disciplinary connections, and identifying emerging trends and research\ngaps by analyzing large volumes of data. However, at the present time, they\nlack the required deep understanding of complex methodologies, they have\ndifficulty in evaluating innovative claims, and they are unable to assess\nethical issues and conflicts of interest. Herein, we consider 13 GPT-related\npapers across different scientific domains, reviewed by a human reviewer and\nSciSpace, a large language model, with the reviews evaluated by three distinct\ntypes of evaluators, namely GPT-3.5, a crowd panel, and GPT-4. We found that\n50% of SciSpace's responses to objective questions align with those of a human\nreviewer, with GPT-4 (informed evaluator) often rating the human reviewer\nhigher in accuracy, and SciSpace higher in structure, clarity, and\ncompleteness. In subjective questions, the uninformed evaluators (GPT-3.5 and\ncrowd panel) showed varying preferences between SciSpace and human responses,\nwith the crowd panel showing a preference for the human responses. However,\nGPT-4 rated them equally in accuracy and structure but favored SciSpace for\ncompleteness.",
            "author": [
                "Chenxi Wu",
                "Alan John Varghese",
                "Vivek Oommen",
                "George Em Karniadakis"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03769v1",
                "http://arxiv.org/pdf/2312.03769v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03154v1",
            "title": "ViscoNet: Bridging and Harmonizing Visual and Textual Conditioning for\n  ControlNet",
            "updated": "2023-12-05T21:41:17Z",
            "published": "2023-12-05T21:41:17Z",
            "summary": "This paper introduces ViscoNet, a novel method that enhances text-to-image\nhuman generation models with visual prompting. Unlike existing methods that\nrely on lengthy text descriptions to control the image structure, ViscoNet\nallows users to specify the visual appearance of the target object with a\nreference image. ViscoNet disentangles the object's appearance from the image\nbackground and injects it into a pre-trained latent diffusion model (LDM) model\nvia a ControlNet branch. This way, ViscoNet mitigates the style mode collapse\nproblem and enables precise and flexible visual control. We demonstrate the\neffectiveness of ViscoNet on human image generation, where it can manipulate\nvisual attributes and artistic styles with text and image prompts. We also show\nthat ViscoNet can learn visual conditioning from small and specific object\ndomains while preserving the generative power of the LDM backbone.",
            "author": [
                "Soon Yau Cheong",
                "Armin Mustafa",
                "Andrew Gilbert"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03154v1",
                "http://arxiv.org/pdf/2312.03154v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03151v1",
            "title": "Multitask Learning Can Improve Worst-Group Outcomes",
            "updated": "2023-12-05T21:38:24Z",
            "published": "2023-12-05T21:38:24Z",
            "summary": "In order to create machine learning systems that serve a variety of users\nwell, it is vital to not only achieve high average performance but also ensure\nequitable outcomes across diverse groups. However, most machine learning\nmethods are designed to improve a model's average performance on a chosen end\ntask without consideration for their impact on worst group error. Multitask\nlearning (MTL) is one such widely used technique. In this paper, we seek not\nonly to understand the impact of MTL on worst-group accuracy but also to\nexplore its potential as a tool to address the challenge of group-wise\nfairness. We primarily consider the common setting of fine-tuning a pre-trained\nmodel, where, following recent work (Gururangan et al., 2020; Dery et al.,\n2023), we multitask the end task with the pre-training objective constructed\nfrom the end task data itself. In settings with few or no group annotations, we\nfind that multitasking often, but not always, achieves better worst-group\naccuracy than Just-Train-Twice (JTT; Liu et al. (2021)) -- a representative\ndistributionally robust optimization (DRO) method. Leveraging insights from\nsynthetic data experiments, we propose to modify standard MTL by regularizing\nthe joint multitask representation space. We run a large number of fine-tuning\nexperiments across computer vision and natural language and find that our\nregularized MTL approach consistently outperforms JTT on both worst and average\ngroup outcomes. Our official code can be found here:\nhttps://github.com/atharvajk98/MTL-group-robustness.",
            "author": [
                "Atharva Kulkarni",
                "Lucio Dery",
                "Amrith Setlur",
                "Aditi Raghunathan",
                "Ameet Talwalkar",
                "Graham Neubig"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03151v1",
                "http://arxiv.org/pdf/2312.03151v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03147v1",
            "title": "Neural parameter calibration and uncertainty quantification for epidemic\n  forecasting",
            "updated": "2023-12-05T21:34:59Z",
            "published": "2023-12-05T21:34:59Z",
            "summary": "The recent COVID-19 pandemic has thrown the importance of accurately\nforecasting contagion dynamics and learning infection parameters into sharp\nfocus. At the same time, effective policy-making requires knowledge of the\nuncertainty on such predictions, in order, for instance, to be able to ready\nhospitals and intensive care units for a worst-case scenario without needlessly\nwasting resources. In this work, we apply a novel and powerful computational\nmethod to the problem of learning probability densities on contagion parameters\nand providing uncertainty quantification for pandemic projections. Using a\nneural network, we calibrate an ODE model to data of the spread of COVID-19 in\nBerlin in 2020, achieving both a significantly more accurate calibration and\nprediction than Markov-Chain Monte Carlo (MCMC)-based sampling schemes. The\nuncertainties on our predictions provide meaningful confidence intervals e.g.\non infection figures and hospitalisation rates, while training and running the\nneural scheme takes minutes where MCMC takes hours. We show convergence of our\nmethod to the true posterior on a simplified SIR model of epidemics, and also\ndemonstrate our method's learning capabilities on a reduced dataset, where a\ncomplex model is learned from a small number of compartments for which data is\navailable.",
            "author": [
                "Thomas Gaskin",
                "Tim Conrad",
                "Grigorios A. Pavliotis",
                "Christof Sch\u00fctte"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03147v1",
                "http://arxiv.org/pdf/2312.03147v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "math.OC",
                "physics.soc-ph",
                "49-02, 92-02, 68-02",
                "J.3; G.1.6; I.2.1; G.3"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03146v1",
            "title": "LRMP: Layer Replication with Mixed Precision for Spatial In-memory DNN\n  Accelerators",
            "updated": "2023-12-05T21:31:20Z",
            "published": "2023-12-05T21:31:20Z",
            "summary": "In-memory computing (IMC) with non-volatile memories (NVMs) has emerged as a\npromising approach to address the rapidly growing computational demands of Deep\nNeural Networks (DNNs). Mapping DNN layers spatially onto NVM-based IMC\naccelerators achieves high degrees of parallelism. However, two challenges that\narise in this approach are the highly non-uniform distribution of layer\nprocessing times and high area requirements. We propose LRMP, a method to\njointly apply layer replication and mixed precision quantization to improve the\nperformance of DNNs when mapped to area-constrained NVM-based IMC accelerators.\nLRMP uses a combination of reinforcement learning and integer linear\nprogramming to search the replication-quantization design space using a model\nthat is closely informed by the target hardware architecture. Across five DNN\nbenchmarks, LRMP achieves 2.8-9$\\times$ latency and 11.8-19$\\times$ throughput\nimprovement at iso-accuracy.",
            "author": [
                "Abinand Nallathambi",
                "Christin David Bose",
                "Wilfried Haensch",
                "Anand Raghunathan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03146v1",
                "http://arxiv.org/pdf/2312.03146v1"
            ],
            "primary_category": "cs.AR",
            "category": [
                "cs.AR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03145v1",
            "title": "Maximum likelihood thresholds of Gaussian graphical models and graphical\n  lasso",
            "updated": "2023-12-05T21:29:37Z",
            "published": "2023-12-05T21:29:37Z",
            "summary": "Associated to each graph G is a Gaussian graphical model. Such models are\noften used in high-dimensional settings, i.e. where there are relatively few\ndata points compared to the number of variables. The maximum likelihood\nthreshold of a graph is the minimum number of data points required to fit the\ncorresponding graphical model using maximum likelihood estimation. Graphical\nlasso is a method for selecting and fitting a graphical model. In this project,\nwe ask: when graphical lasso is used to select and fit a graphical model on n\ndata points, how likely is it that n is greater than or equal to the maximum\nlikelihood threshold of the corresponding graph? Our results are a series of\ncomputational experiments.",
            "author": [
                "Daniel Irving Bernstein",
                "Hayden Outlaw"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03145v1",
                "http://arxiv.org/pdf/2312.03145v1"
            ],
            "primary_category": "math.ST",
            "category": [
                "math.ST",
                "stat.ML",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03140v1",
            "title": "FlexModel: A Framework for Interpretability of Distributed Large\n  Language Models",
            "updated": "2023-12-05T21:19:33Z",
            "published": "2023-12-05T21:19:33Z",
            "summary": "With the growth of large language models, now incorporating billions of\nparameters, the hardware prerequisites for their training and deployment have\nseen a corresponding increase. Although existing tools facilitate model\nparallelization and distributed training, deeper model interactions, crucial\nfor interpretability and responsible AI techniques, still demand thorough\nknowledge of distributed computing. This often hinders contributions from\nresearchers with machine learning expertise but limited distributed computing\nbackground. Addressing this challenge, we present FlexModel, a software package\nproviding a streamlined interface for engaging with models distributed across\nmulti-GPU and multi-node configurations. The library is compatible with\nexisting model distribution libraries and encapsulates PyTorch models. It\nexposes user-registerable HookFunctions to facilitate straightforward\ninteraction with distributed model internals, bridging the gap between\ndistributed and single-device model paradigms. Primarily, FlexModel enhances\naccessibility by democratizing model interactions and promotes more inclusive\nresearch in the domain of large-scale neural networks. The package is found at\nhttps://github.com/VectorInstitute/flex_model.",
            "author": [
                "Matthew Choi",
                "Muhammad Adil Asif",
                "John Willes",
                "David Emerson"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03140v1",
                "http://arxiv.org/pdf/2312.03140v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CL",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03134v1",
            "title": "A Hardware Evaluation Framework for Large Language Model Inference",
            "updated": "2023-12-05T21:01:33Z",
            "published": "2023-12-05T21:01:33Z",
            "summary": "The past year has witnessed the increasing popularity of Large Language\nModels (LLMs). Their unprecedented scale and associated high hardware cost have\nimpeded their broader adoption, calling for efficient hardware designs. With\nthe large hardware needed to simply run LLM inference, evaluating different\nhardware designs becomes a new bottleneck.\n  This work introduces LLMCompass, a hardware evaluation framework for LLM\ninference workloads. LLMCompass is fast, accurate, versatile, and able to\ndescribe and evaluate different hardware designs. LLMCompass includes a mapper\nto automatically find performance-optimal mapping and scheduling. It also\nincorporates an area-based cost model to help architects reason about their\ndesign choices. Compared to real-world hardware, LLMCompass' estimated latency\nachieves an average 10.4% error rate across various operators with various\ninput sizes and an average 4.1% error rate for LLM inference. With LLMCompass,\nsimulating a 4-NVIDIA A100 GPU node running GPT-3 175B inference can be done\nwithin 16 minutes on commodity hardware, including 26,400 rounds of the\nmapper's parameter search.\n  With the aid of LLMCompass, this work draws architectural implications and\nexplores new cost-effective hardware designs. By reducing the compute\ncapability or replacing High Bandwidth Memory (HBM) with traditional DRAM,\nthese new designs can achieve as much as 3.41x improvement in performance/cost\ncompared to an NVIDIA A100, making them promising choices for democratizing\nLLMs.\n  LLMCompass is planned to be fully open-source.",
            "author": [
                "Hengrui Zhang",
                "August Ning",
                "Rohan Prabhakar",
                "David Wentzlaff"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03134v1",
                "http://arxiv.org/pdf/2312.03134v1"
            ],
            "primary_category": "cs.AR",
            "category": [
                "cs.AR",
                "cs.DC",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03133v1",
            "title": "Predicting Bone Degradation Using Vision Transformer and Synthetic\n  Cellular Microstructures Dataset",
            "updated": "2023-12-05T21:00:08Z",
            "published": "2023-12-05T21:00:08Z",
            "summary": "Bone degradation, especially for astronauts in microgravity conditions, is\ncrucial for space exploration missions since the lower applied external forces\naccelerate the diminution in bone stiffness and strength substantially.\nAlthough existing computational models help us understand this phenomenon and\npossibly restrict its effect in the future, they are time-consuming to simulate\nthe changes in the bones, not just the bone microstructures, of each individual\nin detail. In this study, a robust yet fast computational method to predict and\nvisualize bone degradation has been developed. Our deep-learning method,\nTransVNet, can take in different 3D voxelized images and predict their\nevolution throughout months utilizing a hybrid 3D-CNN-VisionTransformer\nautoencoder architecture. Because of limited available experimental data and\nchallenges of obtaining new samples, a digital twin dataset of diverse and\ninitial bone-like microstructures was generated to train our TransVNet on the\nevolution of the 3D images through a previously developed degradation model for\nmicrogravity.",
            "author": [
                "Mohammad Saber Hashemi",
                "Azadeh Sheidaei"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03133v1",
                "http://arxiv.org/pdf/2312.03133v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV",
                "physics.med-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03130v1",
            "title": "The DUNE Far Detector Vertical Drift Technology, Technical Design Report",
            "updated": "2023-12-05T20:57:34Z",
            "published": "2023-12-05T20:57:34Z",
            "summary": "DUNE is an international experiment dedicated to addressing some of the\nquestions at the forefront of particle physics and astrophysics, including the\nmystifying preponderance of matter over antimatter in the early universe. The\ndual-site experiment will employ an intense neutrino beam focused on a near and\na far detector as it aims to determine the neutrino mass hierarchy and to make\nhigh-precision measurements of the PMNS matrix parameters, including the\nCP-violating phase. It will also stand ready to observe supernova neutrino\nbursts, and seeks to observe nucleon decay as a signature of a grand unified\ntheory underlying the standard model.\n  The DUNE far detector implements liquid argon time-projection chamber\n(LArTPC) technology, and combines the many tens-of-kiloton fiducial mass\nnecessary for rare event searches with the sub-centimeter spatial resolution\nrequired to image those events with high precision. The addition of a photon\ndetection system enhances physics capabilities for all DUNE physics drivers and\nopens prospects for further physics explorations. Given its size, the far\ndetector will be implemented as a set of modules, with LArTPC designs that\ndiffer from one another as newer technologies arise.\n  In the vertical drift LArTPC design, a horizontal cathode bisects the\ndetector, creating two stacked drift volumes in which ionization charges drift\ntowards anodes at either the top or bottom. The anodes are composed of\nperforated PCB layers with conductive strips, enabling reconstruction in 3D.\nLight-trap-style photon detection modules are placed both on the cryostat's\nside walls and on the central cathode where they are optically powered.\n  This Technical Design Report describes in detail the technical\nimplementations of each subsystem of this LArTPC that, together with the other\nfar detector modules and the near detector, will enable DUNE to achieve its\nphysics goals.",
            "author": [
                "DUNE Collaboration",
                "A. Abed Abud",
                "B. Abi",
                "R. Acciarri",
                "M. A. Acero",
                "M. R. Adames",
                "G. Adamov",
                "M. Adamowski",
                "D. Adams",
                "M. Adinolfi",
                "C. Adriano",
                "A. Aduszkiewicz",
                "J. Aguilar",
                "B. Aimard",
                "F. Akbar",
                "K. Allison",
                "S. Alonso Monsalve",
                "M. Alrashed",
                "A. Alton",
                "R. Alvarez",
                "H. Amar",
                "P. Amedo",
                "J. Anderson",
                "D. A. Andrade",
                "C. Andreopoulos",
                "M. Andreotti",
                "M. P. Andrews",
                "F. Andrianala",
                "S. Andringa",
                "N. Anfimov",
                "A. Ankowski",
                "M. Antoniassi",
                "M. Antonova",
                "A. Antoshkin",
                "A. Aranda-Fernandez",
                "L. Arellano",
                "E. Arrieta Diaz",
                "M. A. Arroyave",
                "J. Asaadi",
                "A. Ashkenazi",
                "L. Asquith",
                "E. Atkin",
                "D. Auguste",
                "A. Aurisano",
                "V. Aushev",
                "D. Autiero",
                "F. Azfar",
                "A. Back",
                "H. Back",
                "J. J. Back",
                "I. Bagaturia",
                "L. Bagby",
                "N. Balashov",
                "S. Balasubramanian",
                "P. Baldi",
                "W. Baldini",
                "B. Baller",
                "B. Bambah",
                "R. Banerjee",
                "F. Barao",
                "G. Barenboim",
                "P. Barham Alz\u00e1s",
                "G. J. Barker",
                "W. Barkhouse",
                "G. Barr",
                "J. Barranco Monarca",
                "A. Barros",
                "N. Barros",
                "D. Barrow",
                "J. L. Barrow",
                "A. Basharina-Freshville",
                "A. Bashyal",
                "V. Basque",
                "C. Batchelor",
                "L. Bathe-Peters",
                "J. B. R. Battat",
                "F. Battisti",
                "F. Bay",
                "M. C. Q. Bazetto",
                "J. L. L. Bazo Alba",
                "J. F. Beacom",
                "E. Bechetoille",
                "B. Behera",
                "E. Belchior",
                "G. Bell",
                "L. Bellantoni",
                "G. Bellettini",
                "V. Bellini",
                "O. Beltramello",
                "N. Benekos",
                "C. Benitez Montiel",
                "D. Benjamin",
                "F. Bento Neves",
                "J. Berger",
                "S. Berkman",
                "P. Bernardini",
                "A. Bersani",
                "S. Bertolucci",
                "M. Betancourt",
                "A. Betancur Rodr\u00edguez",
                "A. Bevan",
                "Y. Bezawada",
                "A. T. Bezerra",
                "T. J. Bezerra",
                "A. Bhat",
                "V. Bhatnagar",
                "J. Bhatt",
                "M. Bhattacharjee",
                "M. Bhattacharya",
                "S. Bhuller",
                "B. Bhuyan",
                "S. Biagi",
                "J. Bian",
                "K. Biery",
                "B. Bilki",
                "M. Bishai",
                "A. Bitadze",
                "A. Blake",
                "F. D. Blaszczyk",
                "G. C. Blazey",
                "E. Blucher",
                "J. Boissevain",
                "S. Bolognesi",
                "T. Bolton",
                "L. Bomben",
                "M. Bonesini",
                "C. Bonilla-Diaz",
                "F. Bonini",
                "A. Booth",
                "F. Boran",
                "S. Bordoni",
                "R. Borges Merlo",
                "A. Borkum",
                "N. Bostan",
                "J. Bracinik",
                "D. Braga",
                "B. Brahma",
                "D. Brailsford",
                "F. Bramati",
                "A. Branca",
                "A. Brandt",
                "J. Bremer",
                "C. Brew",
                "S. J. Brice",
                "V. Brio",
                "C. Brizzolari",
                "C. Bromberg",
                "J. Brooke",
                "A. Bross",
                "G. Brunetti",
                "M. Brunetti",
                "N. Buchanan",
                "H. Budd",
                "J. Buergi",
                "D. Burgardt",
                "S. Butchart",
                "G. Caceres V.",
                "I. Cagnoli",
                "T. Cai",
                "R. Calabrese",
                "J. Calcutt",
                "M. Calin",
                "L. Calivers",
                "E. Calvo",
                "A. Caminata",
                "W. Campanelli",
                "A. Campos Benitez",
                "N. Canci",
                "J. Cap\u00f3",
                "I. Caracas",
                "D. Caratelli",
                "D. Carber",
                "J. M. Carceller",
                "G. Carini",
                "B. Carlus",
                "M. F. Carneiro",
                "P. Carniti",
                "I. Caro Terrazas",
                "H. Carranza",
                "N. Carrara",
                "L. Carroll",
                "T. Carroll",
                "A. Carter",
                "D. Casazza",
                "J. F. Casta\u00f1o Forero",
                "F. A. Casta\u00f1o",
                "A. Castillo",
                "C. Castromonte",
                "E. Catano-Mur",
                "C. Cattadori",
                "F. Cavalier",
                "F. Cavanna",
                "S. Centro",
                "G. Cerati",
                "A. Cervelli",
                "A. Cervera Villanueva",
                "K. Chakraborty",
                "M. Chalifour",
                "A. Chappell",
                "N. Charitonidis",
                "A. Chatterjee",
                "H. Chen",
                "M. Chen",
                "W. C. Chen",
                "Y. Chen",
                "Z. Chen-Wishart",
                "D. Cherdack",
                "C. Chi",
                "R. Chirco",
                "N. Chitirasreemadam",
                "K. Cho",
                "S. Choate",
                "D. Chokheli",
                "P. S. Chong",
                "B. Chowdhury",
                "D. Christian",
                "A. Chukanov",
                "M. Chung",
                "E. Church",
                "M. F. Cicala",
                "M. Cicerchia",
                "V. Cicero",
                "R. Ciolini",
                "J. Clair",
                "P. Clarke",
                "G. Cline",
                "T. E. Coan",
                "A. G. Cocco",
                "J. A. B. Coelho",
                "A. Cohen",
                "J. Collot",
                "E. Conley",
                "J. M. Conrad",
                "M. Convery",
                "P. Cooke",
                "S. Copello",
                "P. Cova",
                "C. Cox",
                "L. Cremaldi",
                "L. Cremonesi",
                "J. I. Crespo-Anad\u00f3n",
                "M. Crisler",
                "E. Cristaldo",
                "J. Crnkovic",
                "G. Crone",
                "R. Cross",
                "A. Cudd",
                "C. Cuesta",
                "Y. Cui",
                "D. Cussans",
                "J. Dai",
                "O. Dalager",
                "R. Dallavalle",
                "H. da Motta",
                "Z. A. Dar",
                "R. Darby",
                "L. Da Silva Peres",
                "Q. David",
                "G. S. Davies",
                "S. Davini",
                "J. Dawson",
                "R. De Aguiar",
                "P. De Almeida",
                "P. Debbins",
                "I. De Bonis",
                "M. P. Decowski",
                "A. de Gouv\u00eaa",
                "P. C. De Holanda",
                "I. L. De Icaza Astiz",
                "P. De Jong",
                "A. De la Torre",
                "A. Delbart",
                "D. Delepine",
                "M. Delgado",
                "A. Dell'Acqua",
                "G. Delle Monache",
                "N. Delmonte",
                "P. De Lurgio",
                "R. Demario",
                "J. R. T. de Mello Neto",
                "D. M. DeMuth",
                "S. Dennis",
                "C. Densham",
                "P. Denton",
                "G. W. Deptuch",
                "A. De Roeck",
                "V. De Romeri",
                "J. P. Detje",
                "J. Devine",
                "R. Dharmapalan",
                "M. Dias",
                "J. S. D\u00edaz",
                "F. D\u00edaz",
                "F. Di Capua",
                "A. Di Domenico",
                "S. Di Domizio",
                "S. Di Falco",
                "L. Di Giulio",
                "P. Ding",
                "L. Di Noto",
                "E. Diociaiuti",
                "C. Distefano",
                "R. Diurba",
                "M. Diwan",
                "Z. Djurcic",
                "D. Doering",
                "S. Dolan",
                "F. Dolek",
                "M. J. Dolinski",
                "D. Domenici",
                "L. Domine",
                "S. Donati",
                "Y. Donon",
                "S. Doran",
                "D. Douglas",
                "T. A. Doyle",
                "A. Dragone",
                "F. Drielsma",
                "L. Duarte",
                "D. Duchesneau",
                "K. Duffy",
                "K. Dugas",
                "P. Dunne",
                "B. Dutta",
                "H. Duyang",
                "O. Dvornikov",
                "D. A. Dwyer",
                "A. S. Dyshkant",
                "S. Dytman",
                "M. Eads",
                "A. Earle",
                "S. Edayath",
                "D. Edmunds",
                "J. Eisch",
                "P. Englezos",
                "A. Ereditato",
                "T. Erjavec",
                "C. O. Escobar",
                "J. J. Evans",
                "E. Ewart",
                "A. C. Ezeribe",
                "K. Fahey",
                "L. Fajt",
                "A. Falcone",
                "M. Fani'",
                "C. Farnese",
                "Y. Farzan",
                "D. Fedoseev",
                "J. Felix",
                "Y. Feng",
                "E. Fernandez-Martinez",
                "F. Ferraro",
                "G. Ferry",
                "L. Fields",
                "P. Filip",
                "A. Filkins",
                "F. Filthaut",
                "R. Fine",
                "G. Fiorillo",
                "M. Fiorini",
                "S. Fogarty",
                "W. Foreman",
                "J. Fowler",
                "J. Franc",
                "K. Francis",
                "D. Franco",
                "J. Franklin",
                "J. Freeman",
                "J. Fried",
                "A. Friedland",
                "S. Fuess",
                "I. K. Furic",
                "K. Furman",
                "A. P. Furmanski",
                "A. Gabrielli",
                "A. M. Gago",
                "F. Galizzi",
                "H. Gallagher",
                "A. Gallas",
                "N. Gallice",
                "V. Galymov",
                "E. Gamberini",
                "T. Gamble",
                "F. Ganacim",
                "R. Gandhi",
                "S. Ganguly",
                "F. Gao",
                "S. Gao",
                "D. Garcia-Gamez",
                "M. \u00c1. Garc\u00eda-Peris",
                "F. Gardim",
                "S. Gardiner",
                "D. Gastler",
                "A. Gauch",
                "J. Gauvreau",
                "P. Gauzzi",
                "G. Ge",
                "N. Geffroy",
                "B. Gelli",
                "S. Gent",
                "L. Gerlach",
                "Z. Ghorbani-Moghaddam",
                "P. Giammaria",
                "T. Giammaria",
                "D. Gibin",
                "I. Gil-Botella",
                "S. Gilligan",
                "A. Gioiosa",
                "S. Giovannella",
                "C. Girerd",
                "A. K. Giri",
                "C. Giugliano",
                "V. Giusti",
                "D. Gnani",
                "O. Gogota",
                "S. Gollapinni",
                "K. Gollwitzer",
                "R. A. Gomes",
                "L. V. Gomez Bermeo",
                "L. S. Gomez Fajardo",
                "F. Gonnella",
                "D. Gonzalez-Diaz",
                "M. Gonzalez-Lopez",
                "M. C. Goodman",
                "S. Goswami",
                "C. Gotti",
                "J. Goudeau",
                "E. Goudzovski",
                "C. Grace",
                "E. Gramellini",
                "R. Gran",
                "E. Granados",
                "P. Granger",
                "C. Grant",
                "D. R. Gratieri",
                "G. Grauso",
                "P. Green",
                "S. Greenberg",
                "J. Greer",
                "W. C. Griffith",
                "F. T. Groetschla",
                "K. Grzelak",
                "W. Gu",
                "V. Guarino",
                "M. Guarise",
                "R. Guenette",
                "E. Guerard",
                "M. Guerzoni",
                "D. Guffanti",
                "A. Guglielmi",
                "B. Guo",
                "Y. Guo",
                "A. Gupta",
                "V. Gupta",
                "G. Gurung",
                "D. Gutierrez",
                "P. Guzowski",
                "M. M. Guzzo",
                "S. Gwon",
                "K. Haaf",
                "A. Habig",
                "H. Hadavand",
                "R. Haenni",
                "L. Hagaman",
                "A. Hahn",
                "J. Haiston",
                "J. Hakenmueller",
                "T. Hamernik",
                "P. Hamilton",
                "J. Hancock",
                "F. Happacher",
                "D. A. Harris",
                "J. Hartnell",
                "T. Hartnett",
                "J. Harton",
                "T. Hasegawa",
                "C. Hasnip",
                "R. Hatcher",
                "K. Hayrapetyan",
                "J. Hays",
                "E. Hazen",
                "M. He",
                "A. Heavey",
                "K. M. Heeger",
                "J. Heise",
                "S. Henry",
                "M. A. Hernandez Morquecho",
                "K. Herner",
                "V. Hewes",
                "A. Higuera",
                "C. Hilgenberg",
                "S. J. Hillier",
                "A. Himmel",
                "E. Hinkle",
                "L. R. Hirsch",
                "J. Ho",
                "J. Hoff",
                "A. Holin",
                "T. Holvey",
                "E. Hoppe",
                "G. A. Horton-Smith",
                "M. Hostert",
                "T. Houdy",
                "B. Howard",
                "R. Howell",
                "I. Hristova",
                "M. S. Hronek",
                "J. Huang",
                "R. G. Huang",
                "Z. Hulcher",
                "M. Ibrahim",
                "G. Iles",
                "N. Ilic",
                "A. M. Iliescu",
                "R. Illingworth",
                "G. Ingratta",
                "A. Ioannisian",
                "B. Irwin",
                "L. Isenhower",
                "M. Ismerio Oliveira",
                "R. Itay",
                "C. M. Jackson",
                "V. Jain",
                "E. James",
                "W. Jang",
                "B. Jargowsky",
                "D. Jena",
                "X. Ji",
                "C. Jiang",
                "J. Jiang",
                "L. Jiang",
                "A. Jipa",
                "F. R. Joaquim",
                "W. Johnson",
                "B. Jones",
                "R. Jones",
                "D. Jos\u00e9 Fern\u00e1ndez",
                "N. Jovancevic",
                "M. Judah",
                "C. K. Jung",
                "T. Junk",
                "Y. Jwa",
                "M. Kabirnezhad",
                "A. C. Kaboth",
                "I. Kadenko",
                "I. Kakorin",
                "A. Kalitkina",
                "D. Kalra",
                "F. Kamiya",
                "M. Kandemir",
                "D. M. Kaplan",
                "G. Karagiorgi",
                "G. Karaman",
                "A. Karcher",
                "Y. Karyotakis",
                "S. Kasai",
                "S. P. Kasetti",
                "L. Kashur",
                "I. Katsioulas",
                "A. Kauther",
                "N. Kazaryan",
                "L. Ke",
                "E. Kearns",
                "P. T. Keener",
                "K. J. Kelly",
                "E. Kemp",
                "O. Kemularia",
                "Y. Kermaidic",
                "W. Ketchum",
                "S. H. Kettell",
                "M. Khabibullin",
                "N. Khan",
                "A. Khotjantsev",
                "A. Khvedelidze",
                "D. Kim",
                "J. Kim",
                "B. King",
                "B. Kirby",
                "M. Kirby",
                "A. Kish",
                "J. Klein",
                "J. Kleykamp",
                "A. Klustova",
                "T. Kobilarcik",
                "L. Koch",
                "K. Koehler",
                "L. W. Koerner",
                "D. H. Koh",
                "L. Kolupaeva",
                "D. Korablev",
                "M. Kordosky",
                "T. Kosc",
                "U. Kose",
                "V. A. Kosteleck\u00fd",
                "K. Kothekar",
                "I. Kotler",
                "M. Kovalcuk",
                "V. Kozhukalov",
                "W. Krah",
                "R. Kralik",
                "M. Kramer",
                "L. Kreczko",
                "F. Krennrich",
                "I. Kreslo",
                "T. Kroupova",
                "S. Kubota",
                "M. Kubu",
                "Y. Kudenko",
                "V. A. Kudryavtsev",
                "S. Kuhlmann",
                "S. Kulagin",
                "J. Kumar",
                "P. Kumar",
                "S. Kumaran",
                "P. Kunze",
                "J. Kunzmann",
                "R. Kuravi",
                "N. Kurita",
                "C. Kuruppu",
                "V. Kus",
                "T. Kutter",
                "J. Kvasnicka",
                "T. Labree",
                "T. Lackey",
                "A. Lambert",
                "B. J. Land",
                "C. E. Lane",
                "N. Lane",
                "K. Lang",
                "T. Langford",
                "M. Langstaff",
                "F. Lanni",
                "O. Lantwin",
                "J. Larkin",
                "P. Lasorak",
                "D. Last",
                "A. Laudrain",
                "A. Laundrie",
                "G. Laurenti",
                "E. Lavaut",
                "A. Lawrence",
                "P. Laycock",
                "I. Lazanu",
                "M. Lazzaroni",
                "T. Le",
                "S. Leardini",
                "J. Learned",
                "T. LeCompte",
                "C. Lee",
                "V. Legin",
                "G. Lehmann Miotto",
                "R. Lehnert",
                "M. A. Leigui de Oliveira",
                "M. Leitner",
                "D. Leon Silverio",
                "L. M. Lepin",
                "J. -Y. Li",
                "S. W. Li",
                "Y. Li",
                "H. Liao",
                "C. S. Lin",
                "D. Lindebaum",
                "R. A. Lineros",
                "J. Ling",
                "A. Lister",
                "B. R. Littlejohn",
                "H. Liu",
                "J. Liu",
                "Y. Liu",
                "S. Lockwitz",
                "M. Lokajicek",
                "I. Lomidze",
                "K. Long",
                "T. V. Lopes",
                "J. Lopez",
                "I. L\u00f3pez de Rego",
                "N. L\u00f3pez-March",
                "T. Lord",
                "J. M. LoSecco",
                "W. C. Louis",
                "A. Lozano Sanchez",
                "X. -G. Lu",
                "K. B. Luk",
                "B. Lunday",
                "X. Luo",
                "E. Luppi",
                "J. Maalmi",
                "D. MacFarlane",
                "A. A. Machado",
                "P. Machado",
                "C. T. Macias",
                "J. R. Macier",
                "M. MacMahon",
                "A. Maddalena",
                "A. Madera",
                "P. Madigan",
                "S. Magill",
                "C. Magueur",
                "K. Mahn",
                "A. Maio",
                "A. Major",
                "K. Majumdar",
                "M. Man",
                "R. C. Mandujano",
                "J. Maneira",
                "S. Manly",
                "A. Mann",
                "K. Manolopoulos",
                "M. Manrique Plata",
                "S. Manthey Corchado",
                "V. N. Manyam",
                "M. Marchan",
                "A. Marchionni",
                "W. Marciano",
                "D. Marfatia",
                "C. Mariani",
                "J. Maricic",
                "F. Marinho",
                "A. D. Marino",
                "T. Markiewicz",
                "F. Das Chagas Marques",
                "D. Marsden",
                "M. Marshak",
                "C. M. Marshall",
                "J. Marshall",
                "J. Mart\u00edn-Albo",
                "N. Martinez",
                "D. A. Martinez Caicedo",
                "F. Mart\u00ednez L\u00f3pez",
                "P. Mart\u00ednez Mirav\u00e9",
                "S. Martynenko",
                "V. Mascagna",
                "C. Massari",
                "A. Mastbaum",
                "F. Matichard",
                "S. Matsuno",
                "G. Matteucci",
                "J. Matthews",
                "C. Mauger",
                "N. Mauri",
                "K. Mavrokoridis",
                "I. Mawby",
                "R. Mazza",
                "A. Mazzacane",
                "T. McAskill",
                "N. McConkey",
                "K. S. McFarland",
                "C. McGrew",
                "A. McNab",
                "L. Meazza",
                "V. C. N. Meddage",
                "A. Mefodiev",
                "B. Mehta",
                "P. Mehta",
                "P. Melas",
                "O. Mena",
                "H. Mendez",
                "P. Mendez",
                "D. P. M\u00e9ndez",
                "A. Menegolli",
                "G. Meng",
                "M. D. Messier",
                "S. Metallo",
                "J. Metcalf",
                "W. Metcalf",
                "M. Mewes",
                "H. Meyer",
                "T. Miao",
                "A. Miccoli",
                "G. Michna",
                "V. Mikola",
                "R. Milincic",
                "G. Miller",
                "W. Miller",
                "O. Mineev",
                "A. Minotti",
                "L. Miralles",
                "O. G. Miranda",
                "C. Mironov",
                "S. Miryala",
                "S. Miscetti",
                "C. S. Mishra",
                "S. R. Mishra",
                "A. Mislivec",
                "M. Mitchell",
                "D. Mladenov",
                "I. Mocioiu",
                "A. Mogan",
                "N. Moggi",
                "R. Mohanta",
                "T. A. Mohayai",
                "N. Mokhov",
                "J. Molina",
                "L. Molina Bueno",
                "E. Montagna",
                "A. Montanari",
                "C. Montanari",
                "D. Montanari",
                "D. Montanino",
                "L. M. Monta\u00f1o Zetina",
                "M. Mooney",
                "A. F. Moor",
                "Z. Moore",
                "D. Moreno",
                "O. Moreno-Palacios",
                "L. Morescalchi",
                "D. Moretti",
                "R. Moretti",
                "C. Morris",
                "C. Mossey",
                "M. Mote",
                "C. A. Moura",
                "G. Mouster",
                "W. Mu",
                "L. Mualem",
                "J. Mueller",
                "M. Muether",
                "F. Muheim",
                "A. Muir",
                "M. Mulhearn",
                "D. Munford",
                "L. J. Munteanu",
                "H. Muramatsu",
                "J. Muraz",
                "M. Murphy",
                "T. Murphy",
                "J. Muse",
                "A. Mytilinaki",
                "J. Nachtman",
                "Y. Nagai",
                "S. Nagu",
                "M. Nalbandyan",
                "R. Nandakumar",
                "D. Naples",
                "S. Narita",
                "A. Nath",
                "A. Navrer-Agasson",
                "N. Nayak",
                "M. Nebot-Guinot",
                "A. Nehm",
                "J. K. Nelson",
                "O. Neogi",
                "J. Nesbit",
                "M. Nessi",
                "D. Newbold",
                "M. Newcomer",
                "R. Nichol",
                "F. Nicolas-Arnaldos",
                "A. Nikolica",
                "J. Nikolov",
                "E. Niner",
                "K. Nishimura",
                "A. Norman",
                "A. Norrick",
                "P. Novella",
                "J. A. Nowak",
                "M. Oberling",
                "J. P. Ochoa-Ricoux",
                "S. Oh",
                "S. B. Oh",
                "A. Olivier",
                "A. Olshevskiy",
                "T. Olson",
                "Y. Onel",
                "Y. Onishchuk",
                "A. Oranday",
                "M. Osbiston",
                "J. A. Osorio V\u00e9lez",
                "L. Otiniano Ormachea",
                "J. Ott",
                "L. Pagani",
                "G. Palacio",
                "O. Palamara",
                "S. Palestini",
                "J. M. Paley",
                "M. Pallavicini",
                "C. Palomares",
                "S. Pan",
                "P. Panda",
                "W. Panduro Vazquez",
                "E. Pantic",
                "V. Paolone",
                "V. Papadimitriou",
                "R. Papaleo",
                "A. Papanestis",
                "D. Papoulias",
                "S. Paramesvaran",
                "A. Paris",
                "S. Parke",
                "E. Parozzi",
                "S. Parsa",
                "Z. Parsa",
                "S. Parveen",
                "M. Parvu",
                "D. Pasciuto",
                "S. Pascoli",
                "L. Pasqualini",
                "J. Pasternak",
                "C. Patrick",
                "L. Patrizii",
                "R. B. Patterson",
                "T. Patzak",
                "A. Paudel",
                "L. Paulucci",
                "Z. Pavlovic",
                "G. Pawloski",
                "D. Payne",
                "V. Pec",
                "E. Pedreschi",
                "S. J. M. Peeters",
                "W. Pellico",
                "A. Pena Perez",
                "E. Pennacchio",
                "A. Penzo",
                "O. L. G. Peres",
                "Y. F. Perez Gonzalez",
                "L. P\u00e9rez-Molina",
                "C. Pernas",
                "J. Perry",
                "D. Pershey",
                "G. Pessina",
                "G. Petrillo",
                "C. Petta",
                "R. Petti",
                "V. Pia",
                "L. Pickering",
                "F. Pietropaolo",
                "V. L. Pimentel",
                "G. Pinaroli",
                "J. Pinchault",
                "K. Plows",
                "R. Plunkett",
                "C. Pollack",
                "T. Pollman",
                "F. Pompa",
                "X. Pons",
                "N. Poonthottathil",
                "F. Poppi",
                "J. Porter",
                "M. Potekhin",
                "R. Potenza",
                "J. Pozimski",
                "M. Pozzato",
                "S. Prakash",
                "T. Prakash",
                "C. Pratt",
                "M. Prest",
                "A. Prosser",
                "F. Psihas",
                "D. Pugnere",
                "X. Qian",
                "J. L. Raaf",
                "V. Radeka",
                "J. Rademacker",
                "B. Radics",
                "A. Rafique",
                "E. Raguzin",
                "M. Rai",
                "S. Rajagopalan",
                "M. Rajaoalisoa",
                "I. Rakhno",
                "L. Rakotondravohitra",
                "L. Ralte",
                "M. A. Ramirez Delgado",
                "B. Ramson",
                "A. Rappoldi",
                "G. Raselli",
                "P. Ratoff",
                "R. Ray",
                "H. Razafinime",
                "E. M. Rea",
                "J. S. Real",
                "B. Rebel",
                "R. Rechenmacher",
                "M. Reggiani-Guzzo",
                "J. Reichenbacher",
                "S. D. Reitzner",
                "H. Rejeb Sfar",
                "E. Renner",
                "A. Renshaw",
                "S. Rescia",
                "F. Resnati",
                "D. Restrepo",
                "C. Reynolds",
                "M. Ribas",
                "S. Riboldi",
                "C. Riccio",
                "G. Riccobene",
                "J. S. Ricol",
                "M. Rigan",
                "E. V. Rinc\u00f3n",
                "A. Ritchie-Yates",
                "S. Ritter",
                "D. Rivera",
                "R. Rivera",
                "A. Robert",
                "J. L. Rocabado Rocha",
                "L. Rochester",
                "M. Roda",
                "P. Rodrigues",
                "M. J. Rodriguez Alonso",
                "J. Rodriguez Rondon",
                "S. Rosauro-Alcaraz",
                "P. Rosier",
                "D. Ross",
                "M. Rossella",
                "M. Rossi",
                "M. Ross-Lonergan",
                "N. Roy",
                "P. Roy",
                "C. Rubbia",
                "A. Ruggeri",
                "G. Ruiz Ferreira",
                "B. Russell",
                "D. Ruterbories",
                "A. Rybnikov",
                "A. Saa-Hernandez",
                "R. Saakyan",
                "S. Sacerdoti",
                "S. K. Sahoo",
                "N. Sahu",
                "K. Sakashita",
                "P. Sala",
                "N. Samios",
                "O. Samoylov",
                "M. C. Sanchez",
                "A. S\u00e1nchez Bravo",
                "P. Sanchez-Lucas",
                "V. Sandberg",
                "D. A. Sanders",
                "D. Sankey",
                "D. Santoro",
                "N. Saoulidou",
                "P. Sapienza",
                "C. Sarasty",
                "I. Sarcevic",
                "I. Sarra",
                "G. Savage",
                "V. Savinov",
                "G. Scanavini",
                "A. Scaramelli",
                "A. Scarff",
                "T. Schefke",
                "H. Schellman",
                "S. Schifano",
                "P. Schlabach",
                "D. Schmitz",
                "A. W. Schneider",
                "K. Scholberg",
                "A. Schukraft",
                "B. Schuld",
                "E. Segreto",
                "A. Selyunin",
                "C. R. Senise",
                "J. Sensenig",
                "M. H. Shaevitz",
                "P. Shanahan",
                "P. Sharma",
                "R. Kumar",
                "K. Shaw",
                "T. Shaw",
                "K. Shchablo",
                "C. Shepherd-Themistocleous",
                "A. Sheshukov",
                "W. Shi",
                "S. Shin",
                "S. Shivakoti",
                "I. Shoemaker",
                "D. Shooltz",
                "R. Shrock",
                "B. Siddi",
                "J. Silber",
                "L. Simard",
                "J. Sinclair",
                "G. Sinev",
                "Jaydip Singh",
                "J. Singh",
                "L. Singh",
                "P. Singh",
                "V. Singh",
                "S. Singh Chauhan",
                "R. Sipos",
                "C. Sironneau",
                "G. Sirri",
                "K. Siyeon",
                "K. Skarpaas",
                "J. Smedley",
                "E. Smith",
                "J. Smith",
                "P. Smith",
                "J. Smolik",
                "M. Smy",
                "M. Snape",
                "E. L. Snider",
                "P. Snopok",
                "D. Snowden-Ifft",
                "M. Soares Nunes",
                "H. Sobel",
                "M. Soderberg",
                "S. Sokolov",
                "C. J. Solano Salinas",
                "S. S\u00f6ldner-Rembold",
                "S. R. Soleti",
                "N. Solomey",
                "V. Solovov",
                "W. E. Sondheim",
                "M. Sorel",
                "A. Sotnikov",
                "J. Soto-Oton",
                "A. Sousa",
                "K. Soustruznik",
                "F. Spinella",
                "J. Spitz",
                "N. J. C. Spooner",
                "K. Spurgeon",
                "D. Stalder",
                "M. Stancari",
                "L. Stanco",
                "J. Steenis",
                "R. Stein",
                "H. M. Steiner",
                "A. F. Steklain Lisb\u00f4a",
                "A. Stepanova",
                "J. Stewart",
                "B. Stillwell",
                "J. Stock",
                "F. Stocker",
                "T. Stokes",
                "M. Strait",
                "T. Strauss",
                "L. Strigari",
                "A. Stuart",
                "J. G. Suarez",
                "J. Subash",
                "A. Surdo",
                "L. Suter",
                "C. M. Sutera",
                "K. Sutton",
                "Y. Suvorov",
                "R. Svoboda",
                "S. K. Swain",
                "B. Szczerbinska",
                "A. M. Szelc",
                "A. Sztuc",
                "A. Taffara",
                "N. Talukdar",
                "J. Tamara",
                "H. A. Tanaka",
                "S. Tang",
                "N. Taniuchi",
                "A. M. Tapia Casanova",
                "B. Tapia Oregui",
                "A. Tapper",
                "S. Tariq",
                "E. Tarpara",
                "E. Tatar",
                "R. Tayloe",
                "D. Tedeschi",
                "A. M. Teklu",
                "J. Tena Vidal",
                "P. Tennessen",
                "M. Tenti",
                "K. Terao",
                "F. Terranova",
                "G. Testera",
                "T. Thakore",
                "A. Thea",
                "A. Thiebault",
                "A. Thompson",
                "C. Thorn",
                "S. C. Timm",
                "E. Tiras",
                "V. Tishchenko",
                "N. Todorovi\u0107",
                "L. Tomassetti",
                "A. Tonazzo",
                "D. Torbunov",
                "M. Torti",
                "M. Tortola",
                "F. Tortorici",
                "N. Tosi",
                "D. Totani",
                "M. Toups",
                "C. Touramanis",
                "R. Travaglini",
                "J. Trevor",
                "E. Triller",
                "S. Trilov",
                "D. Truncali",
                "W. H. Trzaska",
                "Y. Tsai",
                "Y. -T. Tsai",
                "Z. Tsamalaidze",
                "K. V. Tsang",
                "N. Tsverava",
                "S. Z. Tu",
                "S. Tufanli",
                "J. Turner",
                "M. Tuzi",
                "J. Tyler",
                "E. Tyley",
                "M. Tzanov",
                "M. A. Uchida",
                "J. Ure\u00f1a Gonz\u00e1lez",
                "J. Urheim",
                "T. Usher",
                "H. Utaegbulam",
                "S. Uzunyan",
                "M. R. Vagins",
                "P. Vahle",
                "S. Valder",
                "G. A. Valdiviesso",
                "E. Valencia",
                "R. Valentim",
                "Z. Vallari",
                "E. Vallazza",
                "J. W. F. Valle",
                "R. Van Berg",
                "R. G. Van de Water",
                "D. V. Forero",
                "M. Van Nuland-Troost",
                "F. Varanini",
                "D. Vargas Oliva",
                "G. Varner",
                "S. Vasina",
                "N. Vaughan",
                "K. Vaziri",
                "J. Vega",
                "S. Ventura",
                "A. Verdugo",
                "S. Vergani",
                "M. Verzocchi",
                "K. Vetter",
                "M. Vicenzi",
                "H. Vieira de Souza",
                "C. Vignoli",
                "C. Vilela",
                "E. Villa",
                "B. Viren",
                "A. Vizcaya-Hernandez",
                "T. Vrba",
                "Q. Vuong",
                "A. V. Waldron",
                "M. Wallbank",
                "J. Walsh",
                "T. Walton",
                "H. Wang",
                "J. Wang",
                "L. Wang",
                "M. H. L. S. Wang",
                "X. Wang",
                "Y. Wang",
                "K. Warburton",
                "D. Warner",
                "L. Warsame",
                "M. O. Wascko",
                "D. Waters",
                "A. Watson",
                "K. Wawrowska",
                "A. Weber",
                "M. Weber",
                "H. Wei",
                "A. Weinstein",
                "H. Wenzel",
                "S. Westerdale",
                "M. Wetstein",
                "K. Whalen",
                "J. Whilhelmi",
                "A. White",
                "A. White",
                "L. H. Whitehead",
                "D. Whittington",
                "M. J. Wilking",
                "A. Wilkinson",
                "C. Wilkinson",
                "F. Wilson",
                "R. J. Wilson",
                "P. Winter",
                "W. Wisniewski",
                "J. Wolcott",
                "J. Wolfs",
                "T. Wongjirad",
                "A. Wood",
                "K. Wood",
                "E. Worcester",
                "M. Worcester",
                "M. Wospakrik",
                "K. Wresilo",
                "C. Wret",
                "S. Wu",
                "W. Wu",
                "W. Wu",
                "M. Wurm",
                "J. Wyenberg",
                "Y. Xiao",
                "I. Xiotidis",
                "B. Yaeggy",
                "N. Yahlali",
                "E. Yandel",
                "K. Yang",
                "T. Yang",
                "A. Yankelevich",
                "N. Yershov",
                "K. Yonehara",
                "T. Young",
                "B. Yu",
                "H. Yu",
                "J. Yu",
                "Y. Yu",
                "W. Yuan",
                "R. Zaki",
                "J. Zalesak",
                "L. Zambelli",
                "B. Zamorano",
                "A. Zani",
                "O. Zapata",
                "L. Zazueta",
                "G. P. Zeller",
                "J. Zennamo",
                "K. Zeug",
                "C. Zhang",
                "S. Zhang",
                "M. Zhao",
                "E. Zhivun",
                "E. D. Zimmerman",
                "S. Zucchelli",
                "J. Zuklin",
                "V. Zutshi",
                "R. Zwaska"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03130v1",
                "http://arxiv.org/pdf/2312.03130v1"
            ],
            "primary_category": "hep-ex",
            "category": [
                "hep-ex",
                "physics.ins-det"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03129v1",
            "title": "Leveraging Laryngograph Data for Robust Voicing Detection in Speech",
            "updated": "2023-12-05T20:57:00Z",
            "published": "2023-12-05T20:57:00Z",
            "summary": "Accurately detecting voiced intervals in speech signals is a critical step in\npitch tracking and has numerous applications. While conventional signal\nprocessing methods and deep learning algorithms have been proposed for this\ntask, their need to fine-tune threshold parameters for different datasets and\nlimited generalization restrict their utility in real-world applications. To\naddress these challenges, this study proposes a supervised voicing detection\nmodel that leverages recorded laryngograph data. The model is based on a\ndensely-connected convolutional recurrent neural network (DC-CRN), and trained\non data with reference voicing decisions extracted from laryngograph data sets.\nPretraining is also investigated to improve the generalization ability of the\nmodel. The proposed model produces robust voicing detection results,\noutperforming other strong baseline methods, and generalizes well to unseen\ndatasets. The source code of the proposed model with pretraining is provided\nalong with the list of used laryngograph datasets to facilitate further\nresearch in this area.",
            "author": [
                "Yixuan Zhang",
                "Heming Wang",
                "DeLiang Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03129v1",
                "http://arxiv.org/pdf/2312.03129v1"
            ],
            "primary_category": "eess.AS",
            "category": [
                "eess.AS",
                "cs.SD"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03122v1",
            "title": "Assertion Enhanced Few-Shot Learning: Instructive Technique for Large\n  Language Models to Generate Educational Explanations",
            "updated": "2023-12-05T20:41:34Z",
            "published": "2023-12-05T20:41:34Z",
            "summary": "Human educators possess an intrinsic ability to anticipate and seek\neducational explanations from students, which drives them to pose\nthought-provoking questions when students cannot articulate these explanations\nindependently. We aim to imbue Intelligent Tutoring Systems with this ability\nusing few-shot learning capability of Large Language Models. Our work proposes\na novel prompting technique, Assertion Enhanced Few-Shot Learning, to\nfacilitate the generation of accurate, detailed oriented educational\nexplanations. Our central hypothesis is that, in educational domain, few-shot\ndemonstrations are necessary but not a sufficient condition for quality\nexplanation generation. We conducted a study involving 12 in-service teachers,\ncomparing our approach to Traditional Few-Shot Learning. The results show that\nAssertion Enhanced Few-Shot Learning improves explanation accuracy by 15% and\nyields higher-quality explanations, as evaluated by teachers. We also conduct a\nqualitative ablation study to factor the impact of assertions to provide\neducator-friendly prompting guidelines for generating explanations in their\ndomain of interest.",
            "author": [
                "Tasmia Shahriar",
                "Noboru Matsuda",
                "Kelly Ramos"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03122v1",
                "http://arxiv.org/pdf/2312.03122v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03121v2",
            "title": "Evaluating Agents using Social Choice Theory",
            "updated": "2023-12-07T02:16:24Z",
            "published": "2023-12-05T20:40:37Z",
            "summary": "We argue that many general evaluation problems can be viewed through the lens\nof voting theory. Each task is interpreted as a separate voter, which requires\nonly ordinal rankings or pairwise comparisons of agents to produce an overall\nevaluation. By viewing the aggregator as a social welfare function, we are able\nto leverage centuries of research in social choice theory to derive principled\nevaluation frameworks with axiomatic foundations. These evaluations are\ninterpretable and flexible, while avoiding many of the problems currently\nfacing cross-task evaluation. We apply this Voting-as-Evaluation (VasE)\nframework across multiple settings, including reinforcement learning, large\nlanguage models, and humans. In practice, we observe that VasE can be more\nrobust than popular evaluation frameworks (Elo and Nash averaging), discovers\nproperties in the evaluation data not evident from scores alone, and can\npredict outcomes better than Elo in a complex seven-player game. We identify\none particular approach, maximal lotteries, that satisfies important\nconsistency properties relevant to evaluation, is computationally efficient\n(polynomial in the size of the evaluation data), and identifies game-theoretic\ncycles.",
            "author": [
                "Marc Lanctot",
                "Kate Larson",
                "Yoram Bachrach",
                "Luke Marris",
                "Zun Li",
                "Avishkar Bhoopchand",
                "Thomas Anthony",
                "Brian Tanner",
                "Anna Koop"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03121v2",
                "http://arxiv.org/pdf/2312.03121v2"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.GT",
                "cs.MA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03120v1",
            "title": "The Landscape of Modern Machine Learning: A Review of Machine,\n  Distributed and Federated Learning",
            "updated": "2023-12-05T20:40:05Z",
            "published": "2023-12-05T20:40:05Z",
            "summary": "With the advance of the powerful heterogeneous, parallel and distributed\ncomputing systems and ever increasing immense amount of data, machine learning\nhas become an indispensable part of cutting-edge technology, scientific\nresearch and consumer products. In this study, we present a review of modern\nmachine and deep learning. We provide a high-level overview for the latest\nadvanced machine learning algorithms, applications, and frameworks. Our\ndiscussion encompasses parallel distributed learning, deep learning as well as\nfederated learning. As a result, our work serves as an introductory text to the\nvast field of modern machine learning.",
            "author": [
                "Omer Subasi",
                "Oceane Bel",
                "Joseph Manzano",
                "Kevin Barker"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03120v1",
                "http://arxiv.org/pdf/2312.03120v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03111v1",
            "title": "Parallel Proof-of-Work with DAG-Style Voting and Targeted Reward\n  Discounting",
            "updated": "2023-12-05T20:14:33Z",
            "published": "2023-12-05T20:14:33Z",
            "summary": "We present parallel proof-of-work with DAG-style voting, a novel\nproof-of-work cryptocurrency protocol that, compared to Bitcoin, provides\nbetter consistency guarantees, higher transaction throughput, lower transaction\nconfirmation latency, and higher resilience against incentive attacks. The\nsuperior consistency guarantees follow from implementing parallel\nproof-of-work, a recent consensus scheme that enforces a configurable number of\nproof-of-work votes per block. Our work is inspired by another recent protocol,\nTailstorm, which structures the individual votes as tree and mitigates\nincentive attacks by discounting the mining rewards proportionally to the depth\nof the tree. We propose to structure the votes as a directed acyclic graph\n(DAG) instead of a tree. This allows for a more targeted punishment of\noffending miners and, as we show through a reinforcement learning based attack\nsearch, makes the protocol even more resilient to incentive attacks. An\ninteresting by-product of our analysis is that parallel proof-of-work without\nreward discounting is less resilient to incentive attacks than Bitcoin in some\nrealistic network scenarios.",
            "author": [
                "Patrik Keller"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03111v1",
                "http://arxiv.org/pdf/2312.03111v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03110v1",
            "title": "The Automated Bias Triangle Feature Extraction Framework",
            "updated": "2023-12-05T20:12:31Z",
            "published": "2023-12-05T20:12:31Z",
            "summary": "Bias triangles represent features in stability diagrams of Quantum Dot (QD)\ndevices, whose occurrence and property analysis are crucial indicators for spin\nphysics. Nevertheless, challenges associated with quality and availability of\ndata as well as the subtlety of physical phenomena of interest have hindered an\nautomatic and bespoke analysis framework, often still relying (in part) on\nhuman labelling and verification. We introduce a feature extraction framework\nfor bias triangles, built from unsupervised, segmentation-based computer vision\nmethods, which facilitates the direct identification and quantification of\nphysical properties of the former. Thereby, the need for human input or large\ntraining datasets to inform supervised learning approaches is circumvented,\nwhile additionally enabling the automation of pixelwise shape and feature\nlabeling. In particular, we demonstrate that Pauli Spin Blockade (PSB)\ndetection can be conducted effectively, efficiently and without any training\ndata as a direct result of this approach.",
            "author": [
                "Madeleine Kotzagiannidis",
                "Jonas Schuff",
                "Nathan Korda"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03110v1",
                "http://arxiv.org/pdf/2312.03110v1"
            ],
            "primary_category": "cond-mat.mes-hall",
            "category": [
                "cond-mat.mes-hall",
                "cs.CV",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03767v1",
            "title": "Unknown Sample Discovery for Source Free Open Set Domain Adaptation",
            "updated": "2023-12-05T20:07:51Z",
            "published": "2023-12-05T20:07:51Z",
            "summary": "Open Set Domain Adaptation (OSDA) aims to adapt a model trained on a source\ndomain to a target domain that undergoes distribution shift and contains\nsamples from novel classes outside the source domain. Source-free OSDA\n(SF-OSDA) techniques eliminate the need to access source domain samples, but\ncurrent SF-OSDA methods utilize only the known classes in the target domain for\nadaptation, and require access to the entire target domain even during\ninference after adaptation, to make the distinction between known and unknown\nsamples. In this paper, we introduce Unknown Sample Discovery (USD) as an\nSF-OSDA method that utilizes a temporally ensembled teacher model to conduct\nknown-unknown target sample separation and adapts the student model to the\ntarget domain over all classes using co-training and temporal consistency\nbetween the teacher and the student. USD promotes Jensen-Shannon distance (JSD)\nas an effective measure for known-unknown sample separation. Our\nteacher-student framework significantly reduces error accumulation resulting\nfrom imperfect known-unknown sample separation, while curriculum guidance helps\nto reliably learn the distinction between target known and target unknown\nsubspaces. USD appends the target model with an unknown class node, thus\nreadily classifying a target sample into any of the known or unknown classes in\nsubsequent post-adaptation inference stages. Empirical results show that USD is\nsuperior to existing SF-OSDA methods and is competitive with current OSDA\nmodels that utilize both source and target domains during adaptation.",
            "author": [
                "Chowdhury Sadman Jahan",
                "Andreas Savakis"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03767v1",
                "http://arxiv.org/pdf/2312.03767v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03105v1",
            "title": "Improving Automated Algorithm Selection by Advancing Fitness Landscape\n  Analysis",
            "updated": "2023-12-05T19:53:25Z",
            "published": "2023-12-05T19:53:25Z",
            "summary": "Optimization is ubiquitous in our daily lives. In the past, (sub-)optimal\nsolutions to any problem have been derived by trial and error, sheer luck, or\nthe expertise of knowledgeable individuals. In our contemporary age, there\nthankfully exists a plethora of different algorithms that can find solutions\nmore reliably than ever before. Yet, choosing an appropriate algorithm for any\ngiven problem is challenging in itself. The field of automated algorithm\nselection provides various approaches to tackle this latest problem. This is\ndone by delegating the selection of a suitable algorithm for a given problem to\na complex computer model. This computer model is generated through the use of\nArtificial Intelligence. Many of these computer models rely on some sort of\ninformation about the problem to make a reasonable selection. Various methods\nexist to provide this informative input to the computer model in the form of\nnumerical data.\n  In this cumulative dissertation, I propose several improvements to the\ndifferent variants of informative inputs. This in turn enhances and refines the\ncurrent state-of-the-art of automated algorithm selection. Specifically, I\nidentify and address current issues with the existing body of work to\nstrengthen the foundation that future work builds upon. Furthermore, the rise\nof deep learning offers ample opportunities for automated algorithm selection.\nIn several joint works, my colleagues and I developed and evaluated several\ndifferent methods that replace the existing methods to extract an informative\ninput. Lastly, automated algorithm selection approaches have been restricted to\ncertain types of problems. I propose a method to extend the generation of\ninformative inputs to other problem types and provide an outlook on further\npromising research directions.",
            "author": [
                "Raphael Patrick Prager"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03105v1",
                "http://arxiv.org/pdf/2312.03105v1"
            ],
            "primary_category": "cs.NE",
            "category": [
                "cs.NE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03102v1",
            "title": "Fully Convolutional Slice-to-Volume Reconstruction for Single-Stack MRI",
            "updated": "2023-12-05T19:45:44Z",
            "published": "2023-12-05T19:45:44Z",
            "summary": "In magnetic resonance imaging (MRI), slice-to-volume reconstruction (SVR)\nrefers to computational reconstruction of an unknown 3D magnetic resonance\nvolume from stacks of 2D slices corrupted by motion. While promising, current\nSVR methods require multiple slice stacks for accurate 3D reconstruction,\nleading to long scans and limiting their use in time-sensitive applications\nsuch as fetal fMRI. Here, we propose a SVR method that overcomes the\nshortcomings of previous work and produces state-of-the-art reconstructions in\nthe presence of extreme inter-slice motion. Inspired by the recent success of\nsingle-view depth estimation methods, we formulate SVR as a single-stack motion\nestimation task and train a fully convolutional network to predict a motion\nstack for a given slice stack, producing a 3D reconstruction as a byproduct of\nthe predicted motion. Extensive experiments on the SVR of adult and fetal\nbrains demonstrate that our fully convolutional method is twice as accurate as\nprevious SVR methods. Our code is available at github.com/seannz/svr.",
            "author": [
                "Sean I. Young",
                "Ya\u00ebl Balbastre",
                "Bruce Fischl",
                "Polina Golland",
                "Juan Eugenio Iglesias"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03102v1",
                "http://arxiv.org/pdf/2312.03102v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03101v1",
            "title": "Invariant derivations and trace bounds",
            "updated": "2023-12-05T19:44:04Z",
            "published": "2023-12-05T19:44:04Z",
            "summary": "Almost 20 years ago, J-P. Serre announced a bound on the trace of elements of\ncompact Lie groups under the adjoint representation together with related\nresults, provided indications of his proofs, and invited a better proof. This\nnote provides a new, general method for proving such bounds; uses that method\nto derive Serre's bounds; gives a second proof of Serre's announced results\nthat (we learned) closely follows his original argument; and provides a result\nabout the lower bound for traces of other representations.",
            "author": [
                "Skip Garibaldi",
                "Robert M. Guralnick",
                "Eric M. Rains"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03101v1",
                "http://arxiv.org/pdf/2312.03101v1"
            ],
            "primary_category": "math.RT",
            "category": [
                "math.RT",
                "math.GR",
                "22E47 (Primary), 22C05, 22G20 (Secondary)"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03096v1",
            "title": "Incidental Polysemanticity",
            "updated": "2023-12-05T19:29:54Z",
            "published": "2023-12-05T19:29:54Z",
            "summary": "Polysemantic neurons (neurons that activate for a set of unrelated features)\nhave been seen as a significant obstacle towards interpretability of\ntask-optimized deep networks, with implications for AI safety. The classic\norigin story of polysemanticity is that the data contains more \"features\" than\nneurons, such that learning to perform a task forces the network to co-allocate\nmultiple unrelated features to the same neuron, endangering our ability to\nunderstand the network's internal processing. In this work, we present a second\nand non-mutually exclusive origin story of polysemanticity. We show that\npolysemanticity can arise incidentally, even when there are ample neurons to\nrepresent all features in the data, using a combination of theory and\nexperiments. This second type of polysemanticity occurs because random\ninitialization can, by chance alone, initially assign multiple features to the\nsame neuron, and the training dynamics then strengthen such overlap. Due to its\norigin, we term this \\textit{incidental polysemanticity}.",
            "author": [
                "Victor Lecomte",
                "Kushal Thaman",
                "Trevor Chow",
                "Rylan Schaeffer",
                "Sanmi Koyejo"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03096v1",
                "http://arxiv.org/pdf/2312.03096v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.NE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03764v1",
            "title": "Similarity-based Knowledge Transfer for Cross-Domain Reinforcement\n  Learning",
            "updated": "2023-12-05T19:26:01Z",
            "published": "2023-12-05T19:26:01Z",
            "summary": "Transferring knowledge in cross-domain reinforcement learning is a\nchallenging setting in which learning is accelerated by reusing knowledge from\na task with different observation and/or action space. However, it is often\nnecessary to carefully select the source of knowledge for the receiving end to\nbenefit from the transfer process. In this article, we study how to measure the\nsimilarity between cross-domain reinforcement learning tasks to select a source\nof knowledge that will improve the performance of the learning agent. We\ndeveloped a semi-supervised alignment loss to match different spaces with a set\nof encoder-decoders, and use them to measure similarity and transfer policies\nacross tasks. In comparison to prior works, our method does not require data to\nbe aligned, paired or collected by expert policies. Experimental results, on a\nset of varied Mujoco control tasks, show the robustness of our method in\neffectively selecting and transferring knowledge, without the supervision of a\ntailored set of source tasks.",
            "author": [
                "Sergio A. Serrano",
                "Jose Martinez-Carranza",
                "L. Enrique Sucar"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03764v1",
                "http://arxiv.org/pdf/2312.03764v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "68T37, 68T42, 68T07, 68T05"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03763v1",
            "title": "Gaussian3Diff: 3D Gaussian Diffusion for 3D Full Head Synthesis and\n  Editing",
            "updated": "2023-12-05T19:05:58Z",
            "published": "2023-12-05T19:05:58Z",
            "summary": "We present a novel framework for generating photorealistic 3D human head and\nsubsequently manipulating and reposing them with remarkable flexibility. The\nproposed approach leverages an implicit function representation of 3D human\nheads, employing 3D Gaussians anchored on a parametric face model. To enhance\nrepresentational capabilities and encode spatial information, we embed a\nlightweight tri-plane payload within each Gaussian rather than directly storing\ncolor and opacity. Additionally, we parameterize the Gaussians in a 2D UV space\nvia a 3DMM, enabling effective utilization of the diffusion model for 3D head\navatar generation. Our method facilitates the creation of diverse and realistic\n3D human heads with fine-grained editing over facial features and expressions.\nExtensive experiments demonstrate the effectiveness of our method.",
            "author": [
                "Yushi Lan",
                "Feitong Tan",
                "Di Qiu",
                "Qiangeng Xu",
                "Kyle Genova",
                "Zeng Huang",
                "Sean Fanello",
                "Rohit Pandey",
                "Thomas Funkhouser",
                "Chen Change Loy",
                "Yinda Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03763v1",
                "http://arxiv.org/pdf/2312.03763v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03762v1",
            "title": "Colour versus Shape Goal Misgeneralization in Reinforcement Learning: A\n  Case Study",
            "updated": "2023-12-05T19:00:46Z",
            "published": "2023-12-05T19:00:46Z",
            "summary": "We explore colour versus shape goal misgeneralization originally demonstrated\nby Di Langosco et al. (2022) in the Procgen Maze environment, where, given an\nambiguous choice, the agents seem to prefer generalization based on colour\nrather than shape. After training over 1,000 agents in a simplified version of\nthe environment and evaluating them on over 10 million episodes, we conclude\nthat the behaviour can be attributed to the agents learning to detect the goal\nobject through a specific colour channel. This choice is arbitrary.\nAdditionally, we show how, due to underspecification, the preferences can\nchange when retraining the agents using exactly the same procedure except for\nusing a different random seed for the training run. Finally, we demonstrate the\nexistence of outliers in out-of-distribution behaviour based on training random\nseed alone.",
            "author": [
                "Karolis Ramanauskas",
                "\u00d6zg\u00fcr \u015eim\u015fek"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03762v1",
                "http://arxiv.org/pdf/2312.03762v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03074v1",
            "title": "Clumpy star formation and an obscured nuclear starburst in the luminous\n  dusty z=4 galaxy GN20 seen by MIRI/JWST",
            "updated": "2023-12-05T19:00:08Z",
            "published": "2023-12-05T19:00:08Z",
            "summary": "Dusty star-forming galaxies emit most of their light at far-IR to mm\nwavelengths as their star formation is highly obscured. Far-IR and mm\nobservations have revealed their dust, neutral and molecular gas properties.\nThe sensitivity of JWST at rest-frame optical and near-infrared wavelengths now\nallows the study of the stellar and ionized gas content. We investigate the\nspatially resolved distribution and kinematics of the ionized gas in GN20, a\ndusty star forming galaxy at $z$=4.0548. We present deep MIRI/MRS integral\nfield spectroscopy of the near-infrared rest-frame emission of GN20. We detect\nspatially resolved \\paa, out to a radius of 6 kpc, distributed in a clumpy\nmorphology. The star formation rate derived from \\paa\\ (144 $\\pm$ 9\n\\msunperyear) is only 7.7 $\\pm 0.5 $\\% of the infrared star formation rate\n(1860 $\\pm$ 90 \\msunperyear). We attribute this to very high extinction (A$_V$\n= 17.2 $\\pm$ 0.4 mag, or A$_{V,mixed}$ = 44 $\\pm$ 3 mag), especially in the\nnucleus of GN20, where only faint \\paa\\ is detected, suggesting a deeply buried\nstarburst. We identify four, spatially unresolved, clumps in the \\paa\\\nemission. Based on the double peaked \\paa\\ profile we find that each clump\nconsist of at least two sub-clumps. We find mass upper limits consistent with\nthem being formed in a gravitationally unstable gaseous disk. The UV bright\nregion of GN20 does not have any detected \\paa\\ emission, suggesting an age of\nmore than 10 Myrs for this region of the galaxy. From the rotation profile of\n\\paa\\ we conclude that the gas kinematics are rotationally dominated and the\n$v_{rot}/\\sigma_{m} = 3.8 \\pm 1.4$ is similar to low-redshift LIRGs. We\nspeculate that the clumps seen in GN20 could contribute to building up the\ninner disk and bulge of GN20.",
            "author": [
                "A. Bik",
                "J. \u00c1lvarez-M\u00e1rquez",
                "L. Colina",
                "A. Crespo G\u00f3mez",
                "F. Peissker",
                "F. Walter",
                "L. A. Boogaard",
                "G. \u00d6stlin",
                "T. R. Greve",
                "G. Wright",
                "A. Alonso-Herrero",
                "K. I. Caputi",
                "L. Costantin",
                "A. Eckart",
                "S. Gillman",
                "J. Hjorth",
                "E. Iani",
                "I. Jermann",
                "A. Labiano",
                "D. Langeroodi",
                "J. Melinder",
                "P. G. P\u00e9rez-Gonz\u00e1lez",
                "J. P. Pye",
                "P. Rinaldi",
                "T. Tikkanen",
                "P. van der Werf",
                "M. G\u00fcdel",
                "Th. Henning",
                "P. O. Lagage",
                "T. Ray",
                "E. F. van Dishoeck"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03074v1",
                "http://arxiv.org/pdf/2312.03074v1"
            ],
            "primary_category": "astro-ph.GA",
            "category": [
                "astro-ph.GA",
                "astro-ph.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03067v2",
            "title": "Semi-visible jets, energy-based models, and self-supervision",
            "updated": "2023-12-07T18:47:41Z",
            "published": "2023-12-05T19:00:03Z",
            "summary": "We present DarkCLR, a novel framework for detecting semi-visible jets at the\nLHC. DarkCLR uses a self-supervised contrastive-learning approach to create\nobservables that are approximately invariant under relevant transformations. We\nuse background-enhanced data to create a sensitive representation and evaluate\nthe representations using a normalized autoencoder as a density estimator. Our\nresults show a remarkable sensitivity for a wide range of semi-visible jets and\nare more robust than a supervised classifier trained on a specific signal.",
            "author": [
                "Luigi Favaro",
                "Michael Kr\u00e4mer",
                "Tanmoy Modak",
                "Tilman Plehn",
                "Jan R\u00fcschkamp"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03067v2",
                "http://arxiv.org/pdf/2312.03067v2"
            ],
            "primary_category": "hep-ph",
            "category": [
                "hep-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03059v1",
            "title": "A Foray on SCFT$_3$ via Super Spinor-Helicity and Grassmann Twistor\n  Variables",
            "updated": "2023-12-05T19:00:01Z",
            "published": "2023-12-05T19:00:01Z",
            "summary": "In this paper, we develop a momentum super space formalism for\n$\\mathcal{N}=1,2$ superconformal field theories in three dimensions. First, we\nsolve for super-correlators in the usual momentum superspace variables.\nHowever, we found that expressing quantities in super space spinor helicity\nvariables greatly simplifies the analysis. Further, by performing a \"half\"\nFourier transform of the Grassmann coordinates which is analogous to the\nTwistor transform, an even more remarkable simplification occurs. Using these\nformalism, we first compute all three point correlation functions involving\nconserved super-currents with arbitrary spins in $\\mathcal{N}=1,2$ theories. We\ndiscover interesting double copy relations in $\\mathcal{N}=1$\nsuper-correlators. Also, we discovered super double copy relations that take us\nfrom $\\mathcal{N}=1$ to $\\mathcal{N}=2$ super-correlators. We also comment on\nthe connection of our results with the flat space super amplitudes in one\nhigher dimension.",
            "author": [
                "Sachin Jain",
                "Dhruva K. S",
                "Deep Mazumdar",
                "Shivang Yadav"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03059v1",
                "http://arxiv.org/pdf/2312.03059v1"
            ],
            "primary_category": "hep-th",
            "category": [
                "hep-th"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03057v1",
            "title": "Advantage of Quantum Machine Learning from General Computational\n  Advantages",
            "updated": "2023-12-05T19:00:00Z",
            "published": "2023-12-05T19:00:00Z",
            "summary": "An overarching milestone of quantum machine learning (QML) is to demonstrate\nthe advantage of QML over all possible classical learning methods in\naccelerating a common type of learning task as represented by supervised\nlearning with classical data. However, the provable advantages of QML in\nsupervised learning have been known so far only for the learning tasks designed\nfor using the advantage of specific quantum algorithms, i.e., Shor's\nalgorithms. Here we explicitly construct an unprecedentedly broader family of\nsupervised learning tasks with classical data to offer the provable advantage\nof QML based on general quantum computational advantages, progressing beyond\nShor's algorithms. Our learning task is feasibly achievable by executing a\ngeneral class of functions that can be computed efficiently in polynomial time\nfor a large fraction of inputs by arbitrary quantum algorithms but not by any\nclassical algorithm. We prove the hardness of achieving this learning task for\nany possible polynomial-time classical learning method. We also clarify\nprotocols for preparing the classical data to demonstrate this learning task in\nexperiments. These results open routes to exploit a variety of quantum\nadvantages in computing functions for the experimental demonstration of the\nadvantage of QML.",
            "author": [
                "Hayata Yamasaki",
                "Natsuto Isogai",
                "Mio Murao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03057v1",
                "http://arxiv.org/pdf/2312.03057v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02976v1",
            "title": "Imitating Shortest Paths in Simulation Enables Effective Navigation and\n  Manipulation in the Real World",
            "updated": "2023-12-05T18:59:45Z",
            "published": "2023-12-05T18:59:45Z",
            "summary": "Reinforcement learning (RL) with dense rewards and imitation learning (IL)\nwith human-generated trajectories are the most widely used approaches for\ntraining modern embodied agents. RL requires extensive reward shaping and\nauxiliary losses and is often too slow and ineffective for long-horizon tasks.\nWhile IL with human supervision is effective, collecting human trajectories at\nscale is extremely expensive. In this work, we show that imitating\nshortest-path planners in simulation produces agents that, given a language\ninstruction, can proficiently navigate, explore, and manipulate objects in both\nsimulation and in the real world using only RGB sensors (no depth map or GPS\ncoordinates). This surprising result is enabled by our end-to-end,\ntransformer-based, SPOC architecture, powerful visual encoders paired with\nextensive image augmentation, and the dramatic scale and diversity of our\ntraining data: millions of frames of shortest-path-expert trajectories\ncollected inside approximately 200,000 procedurally generated houses containing\n40,000 unique 3D assets. Our models, data, training code, and newly proposed\n10-task benchmarking suite CHORES will be open-sourced.",
            "author": [
                "Kiana Ehsani",
                "Tanmay Gupta",
                "Rose Hendrix",
                "Jordi Salvador",
                "Luca Weihs",
                "Kuo-Hao Zeng",
                "Kunal Pratap Singh",
                "Yejin Kim",
                "Winson Han",
                "Alvaro Herrasti",
                "Ranjay Krishna",
                "Dustin Schwenk",
                "Eli VanderBilt",
                "Aniruddha Kembhavi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02976v1",
                "http://arxiv.org/pdf/2312.02976v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02975v1",
            "title": "Dexterous Functional Grasping",
            "updated": "2023-12-05T18:59:23Z",
            "published": "2023-12-05T18:59:23Z",
            "summary": "While there have been significant strides in dexterous manipulation, most of\nit is limited to benchmark tasks like in-hand reorientation which are of\nlimited utility in the real world. The main benefit of dexterous hands over\ntwo-fingered ones is their ability to pickup tools and other objects (including\nthin ones) and grasp them firmly to apply force. However, this task requires\nboth a complex understanding of functional affordances as well as precise\nlow-level control. While prior work obtains affordances from human data this\napproach doesn't scale to low-level control. Similarly, simulation training\ncannot give the robot an understanding of real-world semantics. In this paper,\nwe aim to combine the best of both worlds to accomplish functional grasping for\nin-the-wild objects. We use a modular approach. First, affordances are obtained\nby matching corresponding regions of different objects and then a low-level\npolicy trained in sim is run to grasp it. We propose a novel application of\neigengrasps to reduce the search space of RL using a small amount of human data\nand find that it leads to more stable and physically realistic motion. We find\nthat eigengrasp action space beats baselines in simulation and outperforms\nhardcoded grasping in real and matches or outperforms a trained human\nteleoperator. Results visualizations and videos at https://dexfunc.github.io/",
            "author": [
                "Ananye Agarwal",
                "Shagun Uppal",
                "Kenneth Shaw",
                "Deepak Pathak"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02975v1",
                "http://arxiv.org/pdf/2312.02975v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI",
                "cs.CV",
                "cs.LG",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02974v1",
            "title": "Describing Differences in Image Sets with Natural Language",
            "updated": "2023-12-05T18:59:16Z",
            "published": "2023-12-05T18:59:16Z",
            "summary": "How do two sets of images differ? Discerning set-level differences is crucial\nfor understanding model behaviors and analyzing datasets, yet manually sifting\nthrough thousands of images is impractical. To aid in this discovery process,\nwe explore the task of automatically describing the differences between two\n$\\textbf{sets}$ of images, which we term Set Difference Captioning. This task\ntakes in image sets $D_A$ and $D_B$, and outputs a description that is more\noften true on $D_A$ than $D_B$. We outline a two-stage approach that first\nproposes candidate difference descriptions from image sets and then re-ranks\nthe candidates by checking how well they can differentiate the two sets. We\nintroduce VisDiff, which first captions the images and prompts a language model\nto propose candidate descriptions, then re-ranks these descriptions using CLIP.\nTo evaluate VisDiff, we collect VisDiffBench, a dataset with 187 paired image\nsets with ground truth difference descriptions. We apply VisDiff to various\ndomains, such as comparing datasets (e.g., ImageNet vs. ImageNetV2), comparing\nclassification models (e.g., zero-shot CLIP vs. supervised ResNet), summarizing\nmodel failure modes (supervised ResNet), characterizing differences between\ngenerative models (e.g., StableDiffusionV1 and V2), and discovering what makes\nimages memorable. Using VisDiff, we are able to find interesting and previously\nunknown differences in datasets and models, demonstrating its utility in\nrevealing nuanced insights.",
            "author": [
                "Lisa Dunlap",
                "Yuhui Zhang",
                "Xiaohan Wang",
                "Ruiqi Zhong",
                "Trevor Darrell",
                "Jacob Steinhardt",
                "Joseph E. Gonzalez",
                "Serena Yeung-Levy"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02974v1",
                "http://arxiv.org/pdf/2312.02974v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.CL",
                "cs.CY",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02973v1",
            "title": "GauHuman: Articulated Gaussian Splatting from Monocular Human Videos",
            "updated": "2023-12-05T18:59:14Z",
            "published": "2023-12-05T18:59:14Z",
            "summary": "We present, GauHuman, a 3D human model with Gaussian Splatting for both fast\ntraining (1 ~ 2 minutes) and real-time rendering (up to 189 FPS), compared with\nexisting NeRF-based implicit representation modelling frameworks demanding\nhours of training and seconds of rendering per frame. Specifically, GauHuman\nencodes Gaussian Splatting in the canonical space and transforms 3D Gaussians\nfrom canonical space to posed space with linear blend skinning (LBS), in which\neffective pose and LBS refinement modules are designed to learn fine details of\n3D humans under negligible computational cost. Moreover, to enable fast\noptimization of GauHuman, we initialize and prune 3D Gaussians with 3D human\nprior, while splitting/cloning via KL divergence guidance, along with a novel\nmerge operation for further speeding up. Extensive experiments on ZJU_Mocap and\nMonoCap datasets demonstrate that GauHuman achieves state-of-the-art\nperformance quantitatively and qualitatively with fast training and real-time\nrendering speed. Notably, without sacrificing rendering quality, GauHuman can\nfast model the 3D human performer with ~13k 3D Gaussians.",
            "author": [
                "Shoukang Hu",
                "Ziwei Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02973v1",
                "http://arxiv.org/pdf/2312.02973v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03051v1",
            "title": "Generating Interpretable Networks using Hypernetworks",
            "updated": "2023-12-05T18:55:32Z",
            "published": "2023-12-05T18:55:32Z",
            "summary": "An essential goal in mechanistic interpretability to decode a network, i.e.,\nto convert a neural network's raw weights to an interpretable algorithm. Given\nthe difficulty of the decoding problem, progress has been made to understand\nthe easier encoding problem, i.e., to convert an interpretable algorithm into\nnetwork weights. Previous works focus on encoding existing algorithms into\nnetworks, which are interpretable by definition. However, focusing on encoding\nlimits the possibility of discovering new algorithms that humans have never\nstumbled upon, but that are nevertheless interpretable. In this work, we\nexplore the possibility of using hypernetworks to generate interpretable\nnetworks whose underlying algorithms are not yet known. The hypernetwork is\ncarefully designed such that it can control network complexity, leading to a\ndiverse family of interpretable algorithms ranked by their complexity. All of\nthem are interpretable in hindsight, although some of them are less intuitive\nto humans, hence providing new insights regarding how to \"think\" like a neural\nnetwork. For the task of computing L1 norms, hypernetworks find three\nalgorithms: (a) the double-sided algorithm, (b) the convexity algorithm, (c)\nthe pudding algorithm, although only the first algorithm was expected by the\nauthors before experiments. We automatically classify these algorithms and\nanalyze how these algorithmic phases develop during training, as well as how\nthey are affected by complexity control. Furthermore, we show that a trained\nhypernetwork can correctly construct models for input dimensions not seen in\ntraining, demonstrating systematic generalization.",
            "author": [
                "Isaac Liao",
                "Ziming Liu",
                "Max Tegmark"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03051v1",
                "http://arxiv.org/pdf/2312.03051v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.NE",
                "68T07",
                "I.2.6"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03761v1",
            "title": "Learning High-Dimensional Differential Graphs From Multi-Attribute Data",
            "updated": "2023-12-05T18:54:46Z",
            "published": "2023-12-05T18:54:46Z",
            "summary": "We consider the problem of estimating differences in two Gaussian graphical\nmodels (GGMs) which are known to have similar structure. The GGM structure is\nencoded in its precision (inverse covariance) matrix. In many applications one\nis interested in estimating the difference in two precision matrices to\ncharacterize underlying changes in conditional dependencies of two sets of\ndata. Existing methods for differential graph estimation are based on\nsingle-attribute (SA) models where one associates a scalar random variable with\neach node. In multi-attribute (MA) graphical models, each node represents a\nrandom vector. In this paper, we analyze a group lasso penalized D-trace loss\nfunction approach for differential graph learning from multi-attribute data. An\nalternating direction method of multipliers (ADMM) algorithm is presented to\noptimize the objective function. Theoretical analysis establishing consistency\nin support recovery and estimation in high-dimensional settings is provided.\nNumerical results based on synthetic as well as real data are presented.",
            "author": [
                "Jitendra K Tugnait"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03761v1",
                "http://arxiv.org/pdf/2312.03761v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02966v1",
            "title": "Diffusion-SS3D: Diffusion Model for Semi-supervised 3D Object Detection",
            "updated": "2023-12-05T18:54:03Z",
            "published": "2023-12-05T18:54:03Z",
            "summary": "Semi-supervised object detection is crucial for 3D scene understanding,\nefficiently addressing the limitation of acquiring large-scale 3D bounding box\nannotations. Existing methods typically employ a teacher-student framework with\npseudo-labeling to leverage unlabeled point clouds. However, producing reliable\npseudo-labels in a diverse 3D space still remains challenging. In this work, we\npropose Diffusion-SS3D, a new perspective of enhancing the quality of\npseudo-labels via the diffusion model for semi-supervised 3D object detection.\nSpecifically, we include noises to produce corrupted 3D object size and class\nlabel distributions, and then utilize the diffusion model as a denoising\nprocess to obtain bounding box outputs. Moreover, we integrate the diffusion\nmodel into the teacher-student framework, so that the denoised bounding boxes\ncan be used to improve pseudo-label generation, as well as the entire\nsemi-supervised learning process. We conduct experiments on the ScanNet and SUN\nRGB-D benchmark datasets to demonstrate that our approach achieves\nstate-of-the-art performance against existing methods. We also present\nextensive analysis to understand how our diffusion model design affects\nperformance in semi-supervised learning.",
            "author": [
                "Cheng-Ju Ho",
                "Chen-Hsuan Tai",
                "Yen-Yu Lin",
                "Ming-Hsuan Yang",
                "Yi-Hsuan Tsai"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02966v1",
                "http://arxiv.org/pdf/2312.02966v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02959v2",
            "title": "Detecting algorithmic bias in medical AI-models",
            "updated": "2023-12-06T20:57:39Z",
            "published": "2023-12-05T18:47:34Z",
            "summary": "With the growing prevalence of machine learning and artificial\nintelligence-based medical decision support systems, it is equally important to\nensure that these systems provide patient outcomes in a fair and equitable\nfashion. This paper presents an innovative framework for detecting areas of\nalgorithmic bias in medical-AI decision support systems. Our approach\nefficiently identifies potential biases in medical-AI models, specifically in\nthe context of sepsis prediction, by employing the Classification and\nRegression Trees (CART) algorithm. We verify our methodology by conducting a\nseries of synthetic data experiments, showcasing its ability to estimate areas\nof bias in controlled settings precisely. The effectiveness of the concept is\nfurther validated by experiments using electronic medical records from Grady\nMemorial Hospital in Atlanta, Georgia. These tests demonstrate the practical\nimplementation of our strategy in a clinical environment, where it can function\nas a vital instrument for guaranteeing fairness and equity in AI-based medical\ndecisions.",
            "author": [
                "Jeffrey Smith",
                "Andre Holder",
                "Rishikesan Kamaleswaran",
                "Yao Xie"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02959v2",
                "http://arxiv.org/pdf/2312.02959v2"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.CY",
                "cs.LG",
                "stat.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03050v1",
            "title": "HIG: Hierarchical Interlacement Graph Approach to Scene Graph Generation\n  in Video Understanding",
            "updated": "2023-12-05T18:47:19Z",
            "published": "2023-12-05T18:47:19Z",
            "summary": "Visual interactivity understanding within visual scenes presents a\nsignificant challenge in computer vision. Existing methods focus on complex\ninteractivities while leveraging a simple relationship model. These methods,\nhowever, struggle with a diversity of appearance, situation, position,\ninteraction, and relation in videos. This limitation hinders the ability to\nfully comprehend the interplay within the complex visual dynamics of subjects.\nIn this paper, we delve into interactivities understanding within visual\ncontent by deriving scene graph representations from dense interactivities\namong humans and objects. To achieve this goal, we first present a new dataset\ncontaining Appearance-Situation-Position-Interaction-Relation predicates, named\nASPIRe, offering an extensive collection of videos marked by a wide range of\ninteractivities. Then, we propose a new approach named Hierarchical\nInterlacement Graph (HIG), which leverages a unified layer and graph within a\nhierarchical structure to provide deep insights into scene changes across five\ndistinct tasks. Our approach demonstrates superior performance to other methods\nthrough extensive experiments conducted in various scenarios.",
            "author": [
                "Trong-Thuan Nguyen",
                "Pha Nguyen",
                "Khoa Luu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03050v1",
                "http://arxiv.org/pdf/2312.03050v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02957v1",
            "title": "Classification for everyone : Building geography agnostic models for\n  fairer recognition",
            "updated": "2023-12-05T18:41:03Z",
            "published": "2023-12-05T18:41:03Z",
            "summary": "In this paper, we analyze different methods to mitigate inherent geographical\nbiases present in state of the art image classification models. We first\nquantitatively present this bias in two datasets - The Dollar Street Dataset\nand ImageNet, using images with location information. We then present different\nmethods which can be employed to reduce this bias. Finally, we analyze the\neffectiveness of the different techniques on making these models more robust to\ngeographical locations of the images.",
            "author": [
                "Akshat Jindal",
                "Shreya Singh",
                "Soham Gadgil"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02957v1",
                "http://arxiv.org/pdf/2312.02957v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.CY",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02956v1",
            "title": "Choroidalyzer: An open-source, end-to-end pipeline for choroidal\n  analysis in optical coherence tomography",
            "updated": "2023-12-05T18:40:40Z",
            "published": "2023-12-05T18:40:40Z",
            "summary": "Purpose: To develop Choroidalyzer, an open-source, end-to-end pipeline for\nsegmenting the choroid region, vessels, and fovea, and deriving choroidal\nthickness, area, and vascular index.\n  Methods: We used 5,600 OCT B-scans (233 subjects, 6 systemic disease cohorts,\n3 device types, 2 manufacturers). To generate region and vessel ground-truths,\nwe used state-of-the-art automatic methods following manual correction of\ninaccurate segmentations, with foveal positions manually annotated. We trained\na U-Net deep-learning model to detect the region, vessels, and fovea to\ncalculate choroid thickness, area, and vascular index in a fovea-centred region\nof interest. We analysed segmentation agreement (AUC, Dice) and choroid metrics\nagreement (Pearson, Spearman, mean absolute error (MAE)) in internal and\nexternal test sets. We compared Choroidalyzer to two manual graders on a small\nsubset of external test images and examined cases of high error.\n  Results: Choroidalyzer took 0.299 seconds per image on a standard laptop and\nachieved excellent region (Dice: internal 0.9789, external 0.9749), very good\nvessel segmentation performance (Dice: internal 0.8817, external 0.8703) and\nexcellent fovea location prediction (MAE: internal 3.9 pixels, external 3.4\npixels). For thickness, area, and vascular index, Pearson correlations were\n0.9754, 0.9815, and 0.8285 (internal) / 0.9831, 0.9779, 0.7948 (external),\nrespectively (all p<0.0001). Choroidalyzer's agreement with graders was\ncomparable to the inter-grader agreement across all metrics.\n  Conclusions: Choroidalyzer is an open-source, end-to-end pipeline that\naccurately segments the choroid and reliably extracts thickness, area, and\nvascular index. Especially choroidal vessel segmentation is a difficult and\nsubjective task, and fully-automatic methods like Choroidalyzer could provide\nobjectivity and standardisation.",
            "author": [
                "Justin Engelmann",
                "Jamie Burke",
                "Charlene Hamid",
                "Megan Reid-Schachter",
                "Dan Pugh",
                "Neeraj Dhaun",
                "Diana Moukaddem",
                "Lyle Gray",
                "Niall Strang",
                "Paul McGraw",
                "Amos Storkey",
                "Paul J. Steptoe",
                "Stuart King",
                "Tom MacGillivray",
                "Miguel O. Bernabeu",
                "Ian J. C. MacCormick"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02956v1",
                "http://arxiv.org/pdf/2312.02956v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV",
                "cs.LG",
                "q-bio.QM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03048v1",
            "title": "DGInStyle: Domain-Generalizable Semantic Segmentation with Image\n  Diffusion Models and Stylized Semantic Control",
            "updated": "2023-12-05T18:34:12Z",
            "published": "2023-12-05T18:34:12Z",
            "summary": "Large, pretrained latent diffusion models (LDMs) have demonstrated an\nextraordinary ability to generate creative content, specialize to user data\nthrough few-shot fine-tuning, and condition their output on other modalities,\nsuch as semantic maps. However, are they usable as large-scale data generators,\ne.g., to improve tasks in the perception stack, like semantic segmentation? We\ninvestigate this question in the context of autonomous driving, and answer it\nwith a resounding \"yes\". We propose an efficient data generation pipeline\ntermed DGInStyle. First, we examine the problem of specializing a pretrained\nLDM to semantically-controlled generation within a narrow domain. Second, we\ndesign a Multi-resolution Latent Fusion technique to overcome the bias of LDMs\ntowards dominant objects. Third, we propose a Style Swap technique to endow the\nrich generative prior with the learned semantic control. Using DGInStyle, we\ngenerate a diverse dataset of street scenes, train a domain-agnostic semantic\nsegmentation model on it, and evaluate the model on multiple popular autonomous\ndriving datasets. Our approach consistently increases the performance of\nseveral domain generalization methods, in some cases by +2.5 mIoU compared to\nthe previous state-of-the-art method without our generative augmentation\nscheme. Source code and dataset are available at https://dginstyle.github.io .",
            "author": [
                "Yuru Jia",
                "Lukas Hoyer",
                "Shengyu Huang",
                "Tianfu Wang",
                "Luc Van Gool",
                "Konrad Schindler",
                "Anton Obukhov"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03048v1",
                "http://arxiv.org/pdf/2312.03048v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02946v1",
            "title": "Calibrating dimension reduction hyperparameters in the presence of noise",
            "updated": "2023-12-05T18:16:17Z",
            "published": "2023-12-05T18:16:17Z",
            "summary": "The goal of dimension reduction tools is to construct a low-dimensional\nrepresentation of high-dimensional data. These tools are employed for a variety\nof reasons such as noise reduction, visualization, and to lower computational\ncosts. However, there is a fundamental issue that is highly discussed in other\nmodeling problems, but almost entirely ignored in the dimension reduction\nliterature: overfitting. If we interpret data as a combination of signal and\nnoise, prior works judge dimension reduction techniques on their ability to\ncapture the entirety of the data, i.e. both the signal and the noise. In the\ncontext of other modeling problems, techniques such as feature-selection,\ncross-validation, and regularization are employed to combat overfitting, but no\nsuch precautions are taken when performing dimension reduction. In this paper,\nwe present a framework that models dimension reduction problems in the presence\nof noise and use this framework to explore the role perplexity and number of\nneighbors play in overfitting data when applying t-SNE and UMAP. We also\npresent a workflow others may use to calibrate perplexity or number of\nneighbors in the presence of noise.",
            "author": [
                "Justin Lin",
                "Julia Fukuyama"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02946v1",
                "http://arxiv.org/pdf/2312.02946v1"
            ],
            "primary_category": "stat.AP",
            "category": [
                "stat.AP",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02941v1",
            "title": "Fast CT anatomic localization algorithm",
            "updated": "2023-12-05T18:09:47Z",
            "published": "2023-12-05T18:09:47Z",
            "summary": "Automatically determining the position of every slice in a CT scan is a basic\nyet powerful capability allowing fast retrieval of region of interest for\nvisual inspection and automated analysis. Unlike conventional localization\napproaches which work at the slice level, we directly localize only a fraction\nof the slices and and then fit a linear model which maps slice index to its\nestimated axial anatomical position based on those slices. The model is then\nused to assign axial position to every slices of the scan. This approach proves\nto be both computationally efficient, with a typical processing time of less\nthan a second per scan (regardless of its size), accurate, with a typical\nmedian localization error of 1 cm, and robust to different noise sources,\nimaging protocols, metal induced artifacts, anatomical deformations etc.\nAnother key element of our approach is the introduction of a mapping confidence\nscore. This score acts as a fail safe mechanism which allows a rejection of\nunreliable localization results in rare cases of anomalous scans. Our algorithm\nsets new State Of The Art results in terms of localization accuracy. It also\noffers a decrease of two orders of magnitude in processing time with respect to\nall published processing times. It was designed to be invariant to various scan\nresolutions, scan protocols, patient orientations, strong artifacts and various\ndeformations and abnormalities. Additionally, our algorithm is the first one to\nthe best of our knowledge which supports the entire body from head to feet and\nis not confined to specific anatomical region. This algorithm was tested on\nthousands of scans and proves to be very reliable and useful as a preprocessing\nstage for many applications.",
            "author": [
                "Amit Oved"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02941v1",
                "http://arxiv.org/pdf/2312.02941v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02928v1",
            "title": "LivePhoto: Real Image Animation with Text-guided Motion Control",
            "updated": "2023-12-05T17:59:52Z",
            "published": "2023-12-05T17:59:52Z",
            "summary": "Despite the recent progress in text-to-video generation, existing studies\nusually overlook the issue that only spatial contents but not temporal motions\nin synthesized videos are under the control of text. Towards such a challenge,\nthis work presents a practical system, named LivePhoto, which allows users to\nanimate an image of their interest with text descriptions. We first establish a\nstrong baseline that helps a well-learned text-to-image generator (i.e., Stable\nDiffusion) take an image as a further input. We then equip the improved\ngenerator with a motion module for temporal modeling and propose a carefully\ndesigned training pipeline to better link texts and motions. In particular,\nconsidering the facts that (1) text can only describe motions roughly (e.g.,\nregardless of the moving speed) and (2) text may include both content and\nmotion descriptions, we introduce a motion intensity estimation module as well\nas a text re-weighting module to reduce the ambiguity of text-to-motion\nmapping. Empirical evidence suggests that our approach is capable of well\ndecoding motion-related textual instructions into videos, such as actions,\ncamera movements, or even conjuring new contents from thin air (e.g., pouring\nwater into an empty glass). Interestingly, thanks to the proposed intensity\nlearning mechanism, our system offers users an additional control signal (i.e.,\nthe motion intensity) besides text for video customization.",
            "author": [
                "Xi Chen",
                "Zhiheng Liu",
                "Mengting Chen",
                "Yutong Feng",
                "Yu Liu",
                "Yujun Shen",
                "Hengshuang Zhao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02928v1",
                "http://arxiv.org/pdf/2312.02928v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02918v1",
            "title": "Multimodal Prompt Perceiver: Empower Adaptiveness, Generalizability and\n  Fidelity for All-in-One Image Restoration",
            "updated": "2023-12-05T17:47:11Z",
            "published": "2023-12-05T17:47:11Z",
            "summary": "Despite substantial progress, all-in-one image restoration (IR) grapples with\npersistent challenges in handling intricate real-world degradations. This paper\nintroduces MPerceiver: a novel multimodal prompt learning approach that\nharnesses Stable Diffusion (SD) priors to enhance adaptiveness,\ngeneralizability and fidelity for all-in-one image restoration. Specifically,\nwe develop a dual-branch module to master two types of SD prompts: textual for\nholistic representation and visual for multiscale detail representation. Both\nprompts are dynamically adjusted by degradation predictions from the CLIP image\nencoder, enabling adaptive responses to diverse unknown degradations. Moreover,\na plug-in detail refinement module improves restoration fidelity via direct\nencoder-to-decoder information transformation. To assess our method, MPerceiver\nis trained on 9 tasks for all-in-one IR and outperforms state-of-the-art\ntask-specific methods across most tasks. Post multitask pre-training,\nMPerceiver attains a generalized representation in low-level vision, exhibiting\nremarkable zero-shot and few-shot capabilities in unseen tasks. Extensive\nexperiments on 16 IR tasks and 26 benchmarks underscore the superiority of\nMPerceiver in terms of adaptiveness, generalizability and fidelity.",
            "author": [
                "Yuang Ai",
                "Huaibo Huang",
                "Xiaoqiang Zhou",
                "Jiexiang Wang",
                "Ran He"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02918v1",
                "http://arxiv.org/pdf/2312.02918v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02916v1",
            "title": "MIND: Multi-Task Incremental Network Distillation",
            "updated": "2023-12-05T17:46:52Z",
            "published": "2023-12-05T17:46:52Z",
            "summary": "The recent surge in pervasive devices generating dynamic data streams has\nunderscored the necessity for learning systems to adapt to data distributional\nshifts continually. To tackle this challenge, the research community has put\nforth a spectrum of methodologies, including the demanding pursuit of\nclass-incremental learning without replay data. In this study, we present MIND,\na parameter isolation method that aims to significantly enhance the performance\nof replay-free solutions and achieve state-of-the-art results on several widely\nstudied datasets. Our approach introduces two main contributions: two\nalternative distillation procedures that significantly improve the efficiency\nof MIND increasing the accumulated knowledge of each sub-network, and the\noptimization of the BachNorm layers across tasks inside the sub-networks.\nOverall, MIND outperforms all the state-of-the-art methods for rehearsal-free\nClass-Incremental learning (with an increment in classification accuracy of\napprox. +6% on CIFAR-100/10 and +10% on TinyImageNet/10) reaching up to approx.\n+40% accuracy in Domain-Incremental scenarios. Moreover, we ablated each\ncontribution to demonstrate its impact on performance improvement. Our results\nshowcase the superior performance of MIND indicating its potential for\naddressing the challenges posed by Class-incremental and Domain-Incremental\nlearning in resource-constrained environments.",
            "author": [
                "Jacopo Bonato",
                "Francesco Pelosin",
                "Luigi Sabetta",
                "Alessandro Nicolosi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02916v1",
                "http://arxiv.org/pdf/2312.02916v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02914v2",
            "title": "Unsupervised Video Domain Adaptation with Masked Pre-Training and\n  Collaborative Self-Training",
            "updated": "2023-12-06T19:12:32Z",
            "published": "2023-12-05T17:39:19Z",
            "summary": "In this work, we tackle the problem of unsupervised domain adaptation (UDA)\nfor video action recognition. Our approach, which we call UNITE, uses an image\nteacher model to adapt a video student model to the target domain. UNITE first\nemploys self-supervised pre-training to promote discriminative feature learning\non target domain videos using a teacher-guided masked distillation objective.\nWe then perform self-training on masked target data, using the video student\nmodel and image teacher model together to generate improved pseudolabels for\nunlabeled target videos. Our self-training process successfully leverages the\nstrengths of both models to achieve strong transfer performance across domains.\nWe evaluate our approach on multiple video domain adaptation benchmarks and\nobserve significant improvements upon previously reported results.",
            "author": [
                "Arun Reddy",
                "William Paul",
                "Corban Rivera",
                "Ketul Shah",
                "Celso M. de Melo",
                "Rama Chellappa"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02914v2",
                "http://arxiv.org/pdf/2312.02914v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02912v1",
            "title": "Realistic Scatterer Based Adversarial Attacks on SAR Image Classifiers",
            "updated": "2023-12-05T17:36:34Z",
            "published": "2023-12-05T17:36:34Z",
            "summary": "Adversarial attacks have highlighted the vulnerability of classifiers based\non machine learning for Synthetic Aperture Radar (SAR) Automatic Target\nRecognition (ATR) tasks. An adversarial attack perturbs SAR images of on-ground\ntargets such that the classifiers are misled into making incorrect predictions.\nHowever, many existing attacking techniques rely on arbitrary manipulation of\nSAR images while overlooking the feasibility of executing the attacks on\nreal-world SAR imagery. Instead, adversarial attacks should be able to be\nimplemented by physical actions, for example, placing additional false objects\nas scatterers around the on-ground target to perturb the SAR image and fool the\nSAR ATR.\n  In this paper, we propose the On-Target Scatterer Attack (OTSA), a\nscatterer-based physical adversarial attack. To ensure the feasibility of its\nphysical execution, we enforce a constraint on the positioning of the\nscatterers. Specifically, we restrict the scatterers to be placed only on the\ntarget instead of in the shadow regions or the background. To achieve this, we\nintroduce a positioning score based on Gaussian kernels and formulate an\noptimization problem for our OTSA attack. Using a gradient ascent method to\nsolve the optimization problem, the OTSA can generate a vector of parameters\ndescribing the positions, shapes, sizes and amplitudes of the scatterers to\nguide the physical execution of the attack that will mislead SAR image\nclassifiers. The experimental results show that our attack obtains\nsignificantly higher success rates under the positioning constraint compared\nwith the existing method.",
            "author": [
                "Tian Ye",
                "Rajgopal Kannan",
                "Viktor Prasanna",
                "Carl Busart",
                "Lance Kaplan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02912v1",
                "http://arxiv.org/pdf/2312.02912v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02910v1",
            "title": "Rare Galaxy Classes Identified In Foundation Model Representations",
            "updated": "2023-12-05T17:36:04Z",
            "published": "2023-12-05T17:36:04Z",
            "summary": "We identify rare and visually distinctive galaxy populations by searching for\nstructure within the learned representations of pretrained models. We show that\nthese representations arrange galaxies by appearance in patterns beyond those\nneeded to predict the pretraining labels. We design a clustering approach to\nisolate specific local patterns, revealing groups of galaxies with rare and\nscientifically-interesting morphologies.",
            "author": [
                "Mike Walmsley",
                "Anna M. M. Scaife"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02910v1",
                "http://arxiv.org/pdf/2312.02910v1"
            ],
            "primary_category": "astro-ph.GA",
            "category": [
                "astro-ph.GA",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02908v1",
            "title": "Deep Learning Segmentation of Spiral Arms and Bars",
            "updated": "2023-12-05T17:30:16Z",
            "published": "2023-12-05T17:30:16Z",
            "summary": "We present the first deep learning model for segmenting galactic spiral arms\nand bars. In a blinded assessment by expert astronomers, our predicted spiral\narm masks are preferred over both current automated methods (99% of\nevaluations) and our original volunteer labels (79% of evaluations). Experts\nrated our spiral arm masks as `mostly good' to `perfect' in 89% of evaluations.\nBar lengths trivially derived from our predicted bar masks are in excellent\nagreement with a dedicated crowdsourcing project. The pixelwise precision of\nour masks, previously impossible at scale, will underpin new research into how\nspiral arms and bars evolve.",
            "author": [
                "Mike Walmsley",
                "Ashley Spindler"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02908v1",
                "http://arxiv.org/pdf/2312.02908v1"
            ],
            "primary_category": "astro-ph.GA",
            "category": [
                "astro-ph.GA",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03046v1",
            "title": "Diversified in-domain synthesis with efficient fine-tuning for few-shot\n  classification",
            "updated": "2023-12-05T17:18:09Z",
            "published": "2023-12-05T17:18:09Z",
            "summary": "Few-shot image classification aims to learn an image classifier using only a\nsmall set of labeled examples per class. A recent research direction for\nimproving few-shot classifiers involves augmenting the labelled samples with\nsynthetic images created by state-of-the-art text-to-image generation models.\nFollowing this trend, we propose Diversified in-domain synthesis with efficient\nfine-tuning (DISEF), a novel approach which addresses the generalization\nchallenge in few-shot learning using synthetic data. DISEF consists of two main\ncomponents. First, we propose a novel text-to-image augmentation pipeline that,\nby leveraging the real samples and their rich semantics coming from an advanced\ncaptioning model, promotes in-domain sample diversity for better\ngeneralization. Second, we emphasize the importance of effective model\nfine-tuning in few-shot recognition, proposing to use Low-Rank Adaptation\n(LoRA) for joint adaptation of the text and image encoders in a Vision Language\nModel. We validate our method in ten different benchmarks, consistently\noutperforming baselines and establishing a new state-of-the-art for few-shot\nclassification. Code is available at \\url{https://github.com/vturrisi/disef}",
            "author": [
                "Victor G. Turrisi da Costa",
                "Nicola Dall'Asen",
                "Yiming Wang",
                "Nicu Sebe",
                "Elisa Ricci"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03046v1",
                "http://arxiv.org/pdf/2312.03046v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02901v1",
            "title": "Concept Drift Adaptation in Text Stream Mining Settings: A Comprehensive\n  Review",
            "updated": "2023-12-05T17:15:16Z",
            "published": "2023-12-05T17:15:16Z",
            "summary": "Due to the advent and increase in the popularity of the Internet, people have\nbeen producing and disseminating textual data in several ways, such as reviews,\nsocial media posts, and news articles. As a result, numerous researchers have\nbeen working on discovering patterns in textual data, especially because social\nmedia posts function as social sensors, indicating peoples' opinions,\ninterests, etc. However, most tasks regarding natural language processing are\naddressed using traditional machine learning methods and static datasets. This\nsetting can lead to several problems, such as an outdated dataset, which may\nnot correspond to reality, and an outdated model, which has its performance\ndegrading over time. Concept drift is another aspect that emphasizes these\nissues, which corresponds to data distribution and pattern changes. In a text\nstream scenario, it is even more challenging due to its characteristics, such\nas the high speed and data arriving sequentially. In addition, models for this\ntype of scenario must adhere to the constraints mentioned above while learning\nfrom the stream by storing texts for a limited time and consuming low memory.\nIn this study, we performed a systematic literature review regarding concept\ndrift adaptation in text stream scenarios. Considering well-defined criteria,\nwe selected 40 papers to unravel aspects such as text drift categories, types\nof text drift detection, model update mechanism, the addressed stream mining\ntasks, types of text representations, and text representation update mechanism.\nIn addition, we discussed drift visualization and simulation and listed\nreal-world datasets used in the selected papers. Therefore, this paper\ncomprehensively reviews the concept drift adaptation in text stream mining\nscenarios.",
            "author": [
                "Cristiano Mesquita Garcia",
                "Ramon Simoes Abilio",
                "Alessandro Lameiras Koerich",
                "Alceu de Souza Britto Jr.",
                "Jean Paul Barddal"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02901v1",
                "http://arxiv.org/pdf/2312.02901v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL",
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02892v1",
            "title": "Safe Stabilization with Model Uncertainties: A Universal Formula with\n  Gaussian Process Learning",
            "updated": "2023-12-05T17:03:01Z",
            "published": "2023-12-05T17:03:01Z",
            "summary": "A combination of control Lyapunov functions (CLFs) and control barrier\nfunctions (CBFs) forms an efficient framework for addressing control challenges\nin safe stabilization. In our previous research, we developed an analytical\ncontrol strategy, namely the universal formula, that incorporates CLF and CBF\nconditions for safe stabilization. However, successful implementation of this\nuniversal formula relies on an accurate model, as any mismatch between the\nmodel and the actual system can compromise stability and safety. In this paper,\nwe propose a new universal formula that leverages Gaussian processes (GPs)\nlearning to address safe stabilization in the presence of model uncertainty. By\nutilizing the results related to bounded learning errors, we achieve a high\nprobability of stability and safety guarantees with the proposed universal\nformula. Additionally, we introduce a probabilistic compatibility condition to\nevaluate conflicts between the modified CLF and CBF conditions with GP learning\nresults. In cases where compatibility assumptions fail and control system\nlimits are present, we propose a modified universal formula that relaxes\nstability constraints and a projection-based method accommodating control\nlimits. We illustrate the effectiveness of our approach through a simulation of\nadaptive cruise control (ACC), highlighting its potential for practical\napplications in real-world scenarios.",
            "author": [
                "Ming Li",
                "Zhiyong Sun"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02892v1",
                "http://arxiv.org/pdf/2312.02892v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02882v1",
            "title": "Zero Trust for Cyber Resilience",
            "updated": "2023-12-05T16:53:20Z",
            "published": "2023-12-05T16:53:20Z",
            "summary": "The increased connectivity and potential insider threats make traditional\nnetwork defense vulnerable. Instead of assuming that everything behind the\nsecurity perimeter is safe, the zero-trust security model verifies every\nincoming request before granting access. This chapter draws attention to the\ncyber resilience within the zero-trust model. We introduce the evolution from\ntraditional perimeter-based security to zero trust and discuss their\ndifference. Two key elements of the zero-trust engine are trust evaluation (TE)\nand policy engine (PE). We introduce the design of the two components and\ndiscuss how their interplay would contribute to cyber resilience. Dynamic game\ntheory and learning are applied as quantitative approaches to achieve automated\nzero-trust cyber resilience. Several case studies and implementations are\nintroduced to illustrate the benefits of such a security model.",
            "author": [
                "Yunfei Ge",
                "Quanyan Zhu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02882v1",
                "http://arxiv.org/pdf/2312.02882v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.GT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02877v1",
            "title": "A Dynamic Network for Efficient Point Cloud Registration",
            "updated": "2023-12-05T16:47:46Z",
            "published": "2023-12-05T16:47:46Z",
            "summary": "For the point cloud registration task, a significant challenge arises from\nnon-overlapping points that consume extensive computational resources while\nnegatively affecting registration accuracy. In this paper, we introduce a\ndynamic approach, widely utilized to improve network efficiency in computer\nvision tasks, to the point cloud registration task. We employ an iterative\nregistration process on point cloud data multiple times to identify regions\nwhere matching points cluster, ultimately enabling us to remove noisy points.\nSpecifically, we begin with deep global sampling to perform coarse global\nregistration. Subsequently, we employ the proposed refined node proposal module\nto further narrow down the registration region and perform local registration.\nFurthermore, we utilize a spatial consistency-based classifier to evaluate the\nresults of each registration stage. The model terminates once it reaches\nsufficient confidence, avoiding unnecessary computations. Extended experiments\ndemonstrate that our model significantly reduces time consumption compared to\nother methods with similar results, achieving a speed improvement of over 41%\non indoor dataset (3DMatch) and 33% on outdoor datasets (KITTI) while\nmaintaining competitive registration recall requirements.",
            "author": [
                "Yang Ai",
                "Xi Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02877v1",
                "http://arxiv.org/pdf/2312.02877v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02873v1",
            "title": "Toward autocorrection of chemical process flowsheets using large\n  language models",
            "updated": "2023-12-05T16:39:41Z",
            "published": "2023-12-05T16:39:41Z",
            "summary": "The process engineering domain widely uses Process Flow Diagrams (PFDs) and\nProcess and Instrumentation Diagrams (P&IDs) to represent process flows and\nequipment configurations. However, the P&IDs and PFDs, hereafter called\nflowsheets, can contain errors causing safety hazards, inefficient operation,\nand unnecessary expenses. Correcting and verifying flowsheets is a tedious,\nmanual process. We propose a novel generative AI methodology for automatically\nidentifying errors in flowsheets and suggesting corrections to the user, i.e.,\nautocorrecting flowsheets. Inspired by the breakthrough of Large Language\nModels (LLMs) for grammatical autocorrection of human language, we investigate\nLLMs for the autocorrection of flowsheets. The input to the model is a\npotentially erroneous flowsheet and the output of the model are suggestions for\na corrected flowsheet. We train our autocorrection model on a synthetic dataset\nin a supervised manner. The model achieves a top-1 accuracy of 80% and a top-5\naccuracy of 84% on an independent test dataset of synthetically generated\nflowsheets. The results suggest that the model can learn to autocorrect the\nsynthetic flowsheets. We envision that flowsheet autocorrection will become a\nuseful tool for chemical engineers.",
            "author": [
                "Lukas Schulze Balhorn",
                "Marc Caballero",
                "Artur M. Schweidtmann"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02873v1",
                "http://arxiv.org/pdf/2312.02873v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02872v1",
            "title": "Experimental Insights Towards Explainable and Interpretable Pedestrian\n  Crossing Prediction",
            "updated": "2023-12-05T16:39:32Z",
            "published": "2023-12-05T16:39:32Z",
            "summary": "In the context of autonomous driving, pedestrian crossing prediction is a key\ncomponent for improving road safety. Presently, the focus of these predictions\nextends beyond achieving trustworthy results; it is shifting towards the\nexplainability and interpretability of these predictions. This research\nintroduces a novel neuro-symbolic approach that combines deep learning and\nfuzzy logic for an explainable and interpretable pedestrian crossing\nprediction. We have developed an explainable predictor (ExPedCross), which\nutilizes a set of explainable features and employs a fuzzy inference system to\npredict whether the pedestrian will cross or not. Our approach was evaluated on\nboth the PIE and JAAD datasets. The results offer experimental insights into\nachieving explainability and interpretability in the pedestrian crossing\nprediction task. Furthermore, the testing results yield a set of guidelines and\nrecommendations regarding the process of dataset selection, feature selection,\nand explainability.",
            "author": [
                "Angie Nataly Melo",
                "Carlota Salinas",
                "Miguel Angel Sotelo"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02872v1",
                "http://arxiv.org/pdf/2312.02872v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.NE",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02871v1",
            "title": "Attention-enhanced neural differential equations for physics-informed\n  deep learning of ion transport",
            "updated": "2023-12-05T16:39:24Z",
            "published": "2023-12-05T16:39:24Z",
            "summary": "Species transport models typically combine partial differential equations\n(PDEs) with relations from hindered transport theory to quantify\nelectromigrative, convective, and diffusive transport through complex\nnanoporous systems; however, these formulations are frequently substantial\nsimplifications of the governing dynamics, leading to the poor generalization\nperformance of PDE-based models. Given the growing interest in deep learning\nmethods for the physical sciences, we develop a machine learning-based approach\nto characterize ion transport across nanoporous membranes. Our proposed\nframework centers around attention-enhanced neural differential equations that\nincorporate electroneutrality-based inductive biases to improve generalization\nperformance relative to conventional PDE-based methods. In addition, we study\nthe role of the attention mechanism in illuminating physically-meaningful\nion-pairing relationships across diverse mixture compositions. Further, we\ninvestigate the importance of pre-training on simulated data from PDE-based\nmodels, as well as the performance benefits from hard vs. soft inductive\nbiases. Our results indicate that physics-informed deep learning solutions can\noutperform their classical PDE-based counterparts and provide promising avenues\nfor modelling complex transport phenomena across diverse applications.",
            "author": [
                "Danyal Rehman",
                "John H. Lienhard"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02871v1",
                "http://arxiv.org/pdf/2312.02871v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "math-ph",
                "math.MP",
                "physics.comp-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03044v1",
            "title": "REST: Enhancing Group Robustness in DNNs through Reweighted Sparse\n  Training",
            "updated": "2023-12-05T16:27:54Z",
            "published": "2023-12-05T16:27:54Z",
            "summary": "The deep neural network (DNN) has been proven effective in various domains.\nHowever, they often struggle to perform well on certain minority groups during\ninference, despite showing strong performance on the majority of data groups.\nThis is because over-parameterized models learned \\textit{bias attributes} from\na large number of \\textit{bias-aligned} training samples. These bias attributes\nare strongly spuriously correlated with the target variable, causing the models\nto be biased towards spurious correlations (i.e., \\textit{bias-conflicting}).\nTo tackle this issue, we propose a novel \\textbf{re}weighted \\textbf{s}parse\n\\textbf{t}raining framework, dubbed as \\textit{\\textbf{REST}}, which aims to\nenhance the performance of biased data while improving computation and memory\nefficiency. Our proposed REST framework has been experimentally validated on\nthree datasets, demonstrating its effectiveness in exploring unbiased\nsubnetworks. We found that REST reduces the reliance on spuriously correlated\nfeatures, leading to better performance across a wider range of data groups\nwith fewer training and inference resources. We highlight that the\n\\textit{REST} framework represents a promising approach for improving the\nperformance of DNNs on biased data, while simultaneously improving computation\nand memory efficiency. By reducing the reliance on spurious correlations, REST\nhas the potential to enhance the robustness of DNNs and improve their\ngeneralization capabilities. Code is released at\n\\url{https://github.com/zhao1402072392/REST}",
            "author": [
                "Jiaxu Zhao",
                "Lu Yin",
                "Shiwei Liu",
                "Meng Fang",
                "Mykola Pechenizkiy"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03044v1",
                "http://arxiv.org/pdf/2312.03044v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02867v1",
            "title": "Semi-Supervised Health Index Monitoring with Feature Generation and\n  Fusion",
            "updated": "2023-12-05T16:27:51Z",
            "published": "2023-12-05T16:27:51Z",
            "summary": "The Health Index (HI) is crucial for evaluating system health, aiding tasks\nlike anomaly detection and predicting remaining useful life for systems\ndemanding high safety and reliability. Tight monitoring is crucial for\nachieving high precision at a lower cost, with applications such as spray\ncoating. Obtaining HI labels in real-world applications is often\ncost-prohibitive, requiring continuous, precise health measurements. Therefore,\nit is more convenient to leverage run-to failure datasets that may provide\npotential indications of machine wear condition, making it necessary to apply\nsemi-supervised tools for HI construction. In this study, we adapt the Deep\nSemi-supervised Anomaly Detection (DeepSAD) method for HI construction. We use\nthe DeepSAD embedding as a condition indicators to address interpretability\nchallenges and sensitivity to system-specific factors. Then, we introduce a\ndiversity loss to enrich condition indicators. We employ an alternating\nprojection algorithm with isotonic constraints to transform the DeepSAD\nembedding into a normalized HI with an increasing trend. Validation on the PHME\n2010 milling dataset, a recognized benchmark with ground truth HIs demonstrates\nmeaningful HIs estimations. Our methodology is then applied to monitor wear\nstates of thermal spray coatings using high-frequency voltage. Our\ncontributions create opportunities for more accessible and reliable HI\nestimation, particularly in cases where obtaining ground truth HI labels is\nunfeasible.",
            "author": [
                "Ga\u00ebtan Frusque",
                "Ismail Nejjar",
                "Majid Nabavi",
                "Olga Fink"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02867v1",
                "http://arxiv.org/pdf/2312.02867v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ME"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03043v1",
            "title": "Navigating the Synthetic Realm: Harnessing Diffusion-based Models for\n  Laparoscopic Text-to-Image Generation",
            "updated": "2023-12-05T16:20:22Z",
            "published": "2023-12-05T16:20:22Z",
            "summary": "Recent advances in synthetic imaging open up opportunities for obtaining\nadditional data in the field of surgical imaging. This data can provide\nreliable supplements supporting surgical applications and decision-making\nthrough computer vision. Particularly the field of image-guided surgery, such\nas laparoscopic and robotic-assisted surgery, benefits strongly from synthetic\nimage datasets and virtual surgical training methods. Our study presents an\nintuitive approach for generating synthetic laparoscopic images from short text\nprompts using diffusion-based generative models. We demonstrate the usage of\nstate-of-the-art text-to-image architectures in the context of laparoscopic\nimaging with regard to the surgical removal of the gallbladder as an example.\nResults on fidelity and diversity demonstrate that diffusion-based models can\nacquire knowledge about the style and semantics in the field of image-guided\nsurgery. A validation study with a human assessment survey underlines the\nrealistic nature of our synthetic data, as medical personnel detects actual\nimages in a pool with generated images causing a false-positive rate of 66%. In\naddition, the investigation of a state-of-the-art machine learning model to\nrecognize surgical actions indicates enhanced results when trained with\nadditional generated images of up to 5.20%. Overall, the achieved image quality\ncontributes to the usage of computer-generated images in surgical applications\nand enhances its path to maturity.",
            "author": [
                "Simeon Allmendinger",
                "Patrick Hemmer",
                "Moritz Queisner",
                "Igor Sauer",
                "Leopold M\u00fcller",
                "Johannes Jakubik",
                "Michael V\u00f6ssing",
                "Niklas K\u00fchl"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03043v1",
                "http://arxiv.org/pdf/2312.03043v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.AI",
                "cs.CV",
                "q-bio.TO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02859v1",
            "title": "Lessons from Usable ML Deployments and Application to Wind Turbine\n  Monitoring",
            "updated": "2023-12-05T16:13:50Z",
            "published": "2023-12-05T16:13:50Z",
            "summary": "Through past experiences deploying what we call usable ML (one step beyond\nexplainable ML, including both explanations and other augmenting information)\nto real-world domains, we have learned three key lessons. First, many\norganizations are beginning to hire people who we call ``bridges'' because they\nbridge the gap between ML developers and domain experts, and these people fill\na valuable role in developing usable ML applications. Second, a configurable\nsystem that enables easily iterating on usable ML interfaces during\ncollaborations with bridges is key. Finally, there is a need for continuous,\nin-deployment evaluations to quantify the real-world impact of usable ML.\nThroughout this paper, we apply these lessons to the task of wind turbine\nmonitoring, an essential task in the renewable energy domain. Turbine engineers\nand data analysts must decide whether to perform costly in-person\ninvestigations on turbines to prevent potential cases of brakepad failure, and\nwell-tuned usable ML interfaces can aid with this decision-making process.\nThrough the applications of our lessons to this task, we hope to demonstrate\nthe potential real-world impact of usable ML in the renewable energy domain.",
            "author": [
                "Alexandra Zytek",
                "Wei-En Wang",
                "Sofia Koukoura",
                "Kalyan Veeramachaneni"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02859v1",
                "http://arxiv.org/pdf/2312.02859v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02858v2",
            "title": "Towards Causal Representations of Climate Model Data",
            "updated": "2023-12-06T15:52:07Z",
            "published": "2023-12-05T16:13:34Z",
            "summary": "Climate models, such as Earth system models (ESMs), are crucial for\nsimulating future climate change based on projected Shared Socioeconomic\nPathways (SSP) greenhouse gas emissions scenarios. While ESMs are sophisticated\nand invaluable, machine learning-based emulators trained on existing simulation\ndata can project additional climate scenarios much faster and are\ncomputationally efficient. However, they often lack generalizability and\ninterpretability. This work delves into the potential of causal representation\nlearning, specifically the \\emph{Causal Discovery with Single-parent Decoding}\n(CDSD) method, which could render climate model emulation efficient\n\\textit{and} interpretable. We evaluate CDSD on multiple climate datasets,\nfocusing on emissions, temperature, and precipitation. Our findings shed light\non the challenges, limitations, and promise of using CDSD as a stepping stone\ntowards more interpretable and robust climate model emulation.",
            "author": [
                "Julien Boussard",
                "Chandni Nagda",
                "Julia Kaltenborn",
                "Charlotte Emilie Elektra Lange",
                "Philippe Brouillard",
                "Yaniv Gurwicz",
                "Peer Nowack",
                "David Rolnick"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02858v2",
                "http://arxiv.org/pdf/2312.02858v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "physics.ao-ph",
                "stat.ME"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02856v2",
            "title": "Comparative study of quantum emitter fabrication in wide bandgap\n  materials using localized electron irradiation",
            "updated": "2023-12-07T11:13:17Z",
            "published": "2023-12-05T16:12:37Z",
            "summary": "Quantum light sources are crucial foundational components for various quantum\ntechnology applications. With the rapid development of quantum technology,\nthere has been a growing demand for materials that are capable of hosting\nquantum emitters. One such material platform are fluorescent defects in\nhexagonal boron nitride (hBN) inducing deep sub-levels within the band gap. The\nquestion arises if other layered wide bandgap (2D) materials offer similar\nsingle photon emitting defects. Here, we investigate and compare the\nfabrication of quantum emitters in exfoliated multi-layer mica flakes with hBN\nand other wide bandgap 3D crystals (silicon carbide and gallium nitride) which\nare known to host quantum emitters. We use our primary fabrication technique of\nlocalized electron irradiation using a standard scanning electron microscope.\nTo complement our experimental work, we employ density functional theory\nsimulations to study the atomic structures of intrinsic defects and their\nphotophysical properties. While our fabrication technique can create hBN\nquantum emitters with a high yield and high single photon purity, it is unable\nto fabricate emitters in the other solid-state crystals under investigation.\nThis allows us to draw conclusions on the emitter fabrication mechanism, which\ncould be relying on the activation of already present defects by charge state\nmanipulation. We therefore provide an important step toward the identification\nof hBN emitters and their formation process.",
            "author": [
                "Anand Kumar",
                "Chanaprom Cholsuk",
                "Mohammad N. Mishuk",
                "Mouli Hazra",
                "Clotilde Pillot",
                "Tjorben Matthes",
                "Tanveer A. Shaik",
                "Asli Cakan",
                "Volker Deckert",
                "Sujin Suwanna",
                "Tobias Vogl"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02856v2",
                "http://arxiv.org/pdf/2312.02856v2"
            ],
            "primary_category": "physics.app-ph",
            "category": [
                "physics.app-ph",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02855v1",
            "title": "Exploring Error Bits for Memory Failure Prediction: An In-Depth\n  Correlative Study",
            "updated": "2023-12-05T16:11:52Z",
            "published": "2023-12-05T16:11:52Z",
            "summary": "In large-scale datacenters, memory failure is a common cause of server\ncrashes, with uncorrectable errors (UEs) being a major indicator of Dual Inline\nMemory Module (DIMM) defects. Existing approaches primarily focus on predicting\nUEs using correctable errors (CEs), without fully considering the information\nprovided by error bits. However, error bit patterns have a strong correlation\nwith the occurrence of uncorrectable errors (UEs). In this paper, we present a\ncomprehensive study on the correlation between CEs and UEs, specifically\nemphasizing the importance of spatio-temporal error bit information. Our\nanalysis reveals a strong correlation between spatio-temporal error bits and UE\noccurrence. Through evaluations using real-world datasets, we demonstrate that\nour approach significantly improves prediction performance by 15% in F1-score\ncompared to the state-of-the-art algorithms. Overall, our approach effectively\nreduces the number of virtual machine interruptions caused by UEs by\napproximately 59%.",
            "author": [
                "Qiao Yu",
                "Wengui Zhang",
                "Jorge Cardoso",
                "Odej Kao"
            ],
            "link": [
                "http://dx.doi.org/10.1109/ICCAD57390.2023.10323692",
                "http://arxiv.org/abs/2312.02855v1",
                "http://arxiv.org/pdf/2312.02855v1"
            ],
            "primary_category": "cs.AR",
            "category": [
                "cs.AR",
                "cs.AI",
                "cs.DC",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02852v1",
            "title": "Expert-guided Bayesian Optimisation for Human-in-the-loop Experimental\n  Design of Known Systems",
            "updated": "2023-12-05T16:09:31Z",
            "published": "2023-12-05T16:09:31Z",
            "summary": "Domain experts often possess valuable physical insights that are overlooked\nin fully automated decision-making processes such as Bayesian optimisation. In\nthis article we apply high-throughput (batch) Bayesian optimisation alongside\nanthropological decision theory to enable domain experts to influence the\nselection of optimal experiments. Our methodology exploits the hypothesis that\nhumans are better at making discrete choices than continuous ones and enables\nexperts to influence critical early decisions. At each iteration we solve an\naugmented multi-objective optimisation problem across a number of alternate\nsolutions, maximising both the sum of their utility function values and the\ndeterminant of their covariance matrix, equivalent to their total variability.\nBy taking the solution at the knee point of the Pareto front, we return a set\nof alternate solutions at each iteration that have both high utility values and\nare reasonably distinct, from which the expert selects one for evaluation. We\ndemonstrate that even in the case of an uninformed practitioner, our algorithm\nrecovers the regret of standard Bayesian optimisation.",
            "author": [
                "Tom Savage",
                "Ehecatl Antonio del Rio Chanona"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02852v1",
                "http://arxiv.org/pdf/2312.02852v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.HC",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02850v2",
            "title": "A Kernel-Based Neural Network Test for High-dimensional Sequencing Data\n  Analysis",
            "updated": "2023-12-06T04:42:58Z",
            "published": "2023-12-05T16:06:23Z",
            "summary": "The recent development of artificial intelligence (AI) technology, especially\nthe advance of deep neural network (DNN) technology, has revolutionized many\nfields. While DNN plays a central role in modern AI technology, it has been\nrarely used in sequencing data analysis due to challenges brought by\nhigh-dimensional sequencing data (e.g., overfitting). Moreover, due to the\ncomplexity of neural networks and their unknown limiting distributions,\nbuilding association tests on neural networks for genetic association analysis\nremains a great challenge. To address these challenges and fill the important\ngap of using AI in high-dimensional sequencing data analysis, we introduce a\nnew kernel-based neural network (KNN) test for complex association analysis of\nsequencing data. The test is built on our previously developed KNN framework,\nwhich uses random effects to model the overall effects of high-dimensional\ngenetic data and adopts kernel-based neural network structures to model complex\ngenotype-phenotype relationships. Based on KNN, a Wald-type test is then\nintroduced to evaluate the joint association of high-dimensional genetic data\nwith a disease phenotype of interest, considering non-linear and non-additive\neffects (e.g., interaction effects). Through simulations, we demonstrated that\nour proposed method attained higher power compared to the sequence kernel\nassociation test (SKAT), especially in the presence of non-linear and\ninteraction effects. Finally, we apply the methods to the whole genome\nsequencing (WGS) dataset from the Alzheimer's Disease Neuroimaging Initiative\n(ADNI) study, investigating new genes associated with the hippocampal volume\nchange over time.",
            "author": [
                "Tingting Hou",
                "Chang Jiang",
                "Qing Lu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02850v2",
                "http://arxiv.org/pdf/2312.02850v2"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG",
                "stat.ME"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02849v1",
            "title": "Algorithms for mean-field variational inference via polyhedral\n  optimization in the Wasserstein space",
            "updated": "2023-12-05T16:02:04Z",
            "published": "2023-12-05T16:02:04Z",
            "summary": "We develop a theory of finite-dimensional polyhedral subsets over the\nWasserstein space and optimization of functionals over them via first-order\nmethods. Our main application is to the problem of mean-field variational\ninference, which seeks to approximate a distribution $\\pi$ over $\\mathbb{R}^d$\nby a product measure $\\pi^\\star$. When $\\pi$ is strongly log-concave and\nlog-smooth, we provide (1) approximation rates certifying that $\\pi^\\star$ is\nclose to the minimizer $\\pi^\\star_\\diamond$ of the KL divergence over a\n\\emph{polyhedral} set $\\mathcal{P}_\\diamond$, and (2) an algorithm for\nminimizing $\\text{KL}(\\cdot\\|\\pi)$ over $\\mathcal{P}_\\diamond$ with accelerated\ncomplexity $O(\\sqrt \\kappa \\log(\\kappa d/\\varepsilon^2))$, where $\\kappa$ is\nthe condition number of $\\pi$.",
            "author": [
                "Yiheng Jiang",
                "Sinho Chewi",
                "Aram-Alexandre Pooladian"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02849v1",
                "http://arxiv.org/pdf/2312.02849v1"
            ],
            "primary_category": "math.ST",
            "category": [
                "math.ST",
                "cs.LG",
                "math.OC",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03041v1",
            "title": "Transformer-Based Deep Learning Model for Bored Pile Load-Deformation\n  Prediction in Bangkok Subsoil",
            "updated": "2023-12-05T15:54:13Z",
            "published": "2023-12-05T15:54:13Z",
            "summary": "This paper presents a novel deep learning model based on the transformer\narchitecture to predict the load-deformation behavior of large bored piles in\nBangkok subsoil. The model encodes the soil profile and pile features as\ntokenization input, and generates the load-deformation curve as output. The\nmodel also incorporates the previous sequential data of load-deformation curve\ninto the decoder to improve the prediction accuracy. The model also\nincorporates the previous sequential data of load-deformation curve into the\ndecoder. The model shows a satisfactory accuracy and generalization ability for\nthe load-deformation curve prediction, with a mean absolute error of 5.72% for\nthe test data. The model could also be used for parametric analysis and design\noptimization of piles under different soil and pile conditions, pile cross\nsection, pile length and type of pile.",
            "author": [
                "Sompote Youwai",
                "Chissanupong Thongnoo"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03041v1",
                "http://arxiv.org/pdf/2312.03041v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02843v1",
            "title": "Are Vision Transformers More Data Hungry Than Newborn Visual Systems?",
            "updated": "2023-12-05T15:53:24Z",
            "published": "2023-12-05T15:53:24Z",
            "summary": "Vision transformers (ViTs) are top performing models on many computer vision\nbenchmarks and can accurately predict human behavior on object recognition\ntasks. However, researchers question the value of using ViTs as models of\nbiological learning because ViTs are thought to be more data hungry than\nbrains, with ViTs requiring more training data to reach similar levels of\nperformance. To test this assumption, we directly compared the learning\nabilities of ViTs and animals, by performing parallel controlled rearing\nexperiments on ViTs and newborn chicks. We first raised chicks in impoverished\nvisual environments containing a single object, then simulated the training\ndata available in those environments by building virtual animal chambers in a\nvideo game engine. We recorded the first-person images acquired by agents\nmoving through the virtual chambers and used those images to train self\nsupervised ViTs that leverage time as a teaching signal, akin to biological\nvisual systems. When ViTs were trained through the eyes of newborn chicks, the\nViTs solved the same view invariant object recognition tasks as the chicks.\nThus, ViTs were not more data hungry than newborn visual systems: both learned\nview invariant object representations in impoverished visual environments. The\nflexible and generic attention based learning mechanism in ViTs combined with\nthe embodied data streams available to newborn animals appears sufficient to\ndrive the development of animal-like object recognition.",
            "author": [
                "Lalit Pandey",
                "Samantha M. W. Wood",
                "Justin N. Wood"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02843v1",
                "http://arxiv.org/pdf/2312.02843v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG",
                "cs.NE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02831v1",
            "title": "Detection of Seismic Infrasonic Elephant Rumbles Using Spectrogram-Based\n  Machine Learning",
            "updated": "2023-12-05T15:26:14Z",
            "published": "2023-12-05T15:26:14Z",
            "summary": "This paper presents an effective method of identifying elephant rumbles in\ninfrasonic seismic signals. The design and implementation of electronic\ncircuitry to amplify, filter, and digitize the seismic signals captured through\ngeophones are presented. A collection of seismic infrasonic elephant rumbles\nwas collected at a free-ranging area of an elephant orphanage in Sri Lanka. The\nseismic rumbles were converted to spectrograms, and several methods were used\nfor spectral feature extraction. Using LasyPredict, the features extracted\nusing different methods were fed into their corresponding machine-learning\nalgorithms to train them for automatic seismic rumble identification. It was\nfound that the Mel frequency cepstral coefficient (MFCC) together with the\nRidge classifier machine learning algorithm produced the best performance in\nidentifying seismic elephant rumbles. A novel method for denoising the spectrum\nthat leads to enhanced accuracy in identifying seismic rumbles is also\npresented.",
            "author": [
                "A. M. J. V. Costa",
                "C. S. Pallikkonda",
                "H. H. R. Hiroshan",
                "G. R. U. Y. Gamlath",
                "S. R. Munasinghe",
                "C. U. S. Edussooriya"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02831v1",
                "http://arxiv.org/pdf/2312.02831v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY",
                "physics.geo-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02829v1",
            "title": "MIMONets: Multiple-Input-Multiple-Output Neural Networks Exploiting\n  Computation in Superposition",
            "updated": "2023-12-05T15:25:45Z",
            "published": "2023-12-05T15:25:45Z",
            "summary": "With the advent of deep learning, progressively larger neural networks have\nbeen designed to solve complex tasks. We take advantage of these capacity-rich\nmodels to lower the cost of inference by exploiting computation in\nsuperposition. To reduce the computational burden per input, we propose\nMultiple-Input-Multiple-Output Neural Networks (MIMONets) capable of handling\nmany inputs at once. MIMONets augment various deep neural network architectures\nwith variable binding mechanisms to represent an arbitrary number of inputs in\na compositional data structure via fixed-width distributed representations.\nAccordingly, MIMONets adapt nonlinear neural transformations to process the\ndata structure holistically, leading to a speedup nearly proportional to the\nnumber of superposed input items in the data structure. After processing in\nsuperposition, an unbinding mechanism recovers each transformed input of\ninterest. MIMONets also provide a dynamic trade-off between accuracy and\nthroughput by an instantaneous on-demand switching between a set of\naccuracy-throughput operating points, yet within a single set of fixed\nparameters. We apply the concept of MIMONets to both CNN and Transformer\narchitectures resulting in MIMOConv and MIMOFormer, respectively. Empirical\nevaluations show that MIMOConv achieves about 2-4 x speedup at an accuracy\ndelta within [+0.68, -3.18]% compared to WideResNet CNNs on CIFAR10 and\nCIFAR100. Similarly, MIMOFormer can handle 2-4 inputs at once while maintaining\na high average accuracy within a [-1.07, -3.43]% delta on the long range arena\nbenchmark. Finally, we provide mathematical bounds on the interference between\nsuperposition channels in MIMOFormer. Our code is available at\nhttps://github.com/IBM/multiple-input-multiple-output-nets.",
            "author": [
                "Nicolas Menet",
                "Michael Hersche",
                "Geethan Karunaratne",
                "Luca Benini",
                "Abu Sebastian",
                "Abbas Rahimi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02829v1",
                "http://arxiv.org/pdf/2312.02829v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02828v1",
            "title": "Convergence Rates for Stochastic Approximation: Biased Noise with\n  Unbounded Variance, and Applications",
            "updated": "2023-12-05T15:22:39Z",
            "published": "2023-12-05T15:22:39Z",
            "summary": "The Stochastic Approximation (SA) algorithm introduced by Robbins and Monro\nin 1951 has been a standard method for solving equations of the form\n$\\mathbf{f}({\\boldsymbol {\\theta}}) = \\mathbf{0}$, when only noisy measurements\nof $\\mathbf{f}(\\cdot)$ are available. If $\\mathbf{f}({\\boldsymbol {\\theta}}) =\n\\nabla J({\\boldsymbol {\\theta}})$ for some function $J(\\cdot)$, then SA can\nalso be used to find a stationary point of $J(\\cdot)$. In much of the\nliterature, it is assumed that the error term ${\\boldsymbol {xi}}_{t+1}$ has\nzero conditional mean, and that its conditional variance is bounded as a\nfunction of $t$ (though not necessarily with respect to ${\\boldsymbol\n{\\theta}}_t$). Also, for the most part, the emphasis has been on\n``synchronous'' SA, whereby, at each time $t$, \\textit{every} component of\n${\\boldsymbol {\\theta}}_t$ is updated. Over the years, SA has been applied to a\nvariety of areas, out of which two are the focus in this paper: Convex and\nnonconvex optimization, and Reinforcement Learning (RL). As it turns out, in\nthese applications, the above-mentioned assumptions do not always hold. In\nzero-order methods, the error neither has zero mean nor bounded conditional\nvariance. In the present paper, we extend SA theory to encompass errors with\nnonzero conditional mean and/or unbounded conditional variance, and also\nasynchronous SA. In addition, we derive estimates for the rate of convergence\nof the algorithm. Then we apply the new results to problems in nonconvex\noptimization, and to Markovian SA, a recently emerging area in RL. We prove\nthat SA converges in these situations, and compute the ``optimal step size\nsequences'' to maximize the estimated rate of convergence.",
            "author": [
                "Rajeeva L. Karandikar",
                "M. Vidyasagar"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02828v1",
                "http://arxiv.org/pdf/2312.02828v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG",
                "math.OC",
                "math.PR",
                "62L20, 60G17, 93D05"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02826v1",
            "title": "Calibrated Adaptive Teacher for Domain Adaptive Intelligent Fault\n  Diagnosis",
            "updated": "2023-12-05T15:19:29Z",
            "published": "2023-12-05T15:19:29Z",
            "summary": "Intelligent Fault Diagnosis (IFD) based on deep learning has proven to be an\neffective and flexible solution, attracting extensive research. Deep neural\nnetworks can learn rich representations from vast amounts of representative\nlabeled data for various applications. In IFD, they achieve high classification\nperformance from signals in an end-to-end manner, without requiring extensive\ndomain knowledge. However, deep learning models usually only perform well on\nthe data distribution they have been trained on. When applied to a different\ndistribution, they may experience performance drops. This is also observed in\nIFD, where assets are often operated in working conditions different from those\nin which labeled data have been collected. Unsupervised domain adaptation (UDA)\ndeals with the scenario where labeled data are available in a source domain,\nand only unlabeled data are available in a target domain, where domains may\ncorrespond to operating conditions. Recent methods rely on training with\nconfident pseudo-labels for target samples. However, the confidence-based\nselection of pseudo-labels is hindered by poorly calibrated confidence\nestimates in the target domain, primarily due to over-confident predictions,\nwhich limits the quality of pseudo-labels and leads to error accumulation. In\nthis paper, we propose a novel UDA method called Calibrated Adaptive Teacher\n(CAT), where we propose to calibrate the predictions of the teacher network\nthroughout the self-training process, leveraging post-hoc calibration\ntechniques. We evaluate CAT on domain-adaptive IFD and perform extensive\nexperiments on the Paderborn benchmark for bearing fault diagnosis under\nvarying operating conditions. Our proposed method achieves state-of-the-art\nperformance on most transfer tasks.",
            "author": [
                "Florent Forest",
                "Olga Fink"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02826v1",
                "http://arxiv.org/pdf/2312.02826v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "eess.SP",
                "stat.ML",
                "68T07, 62H30",
                "I.2.6; J.2"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03038v1",
            "title": "Sample-based Dynamic Hierarchical Transformer with Layer and Head\n  Flexibility via Contextual Bandit",
            "updated": "2023-12-05T15:04:11Z",
            "published": "2023-12-05T15:04:11Z",
            "summary": "Transformer requires a fixed number of layers and heads which makes them\ninflexible to the complexity of individual samples and expensive in training\nand inference. To address this, we propose a sample-based Dynamic Hierarchical\nTransformer (DHT) model whose layers and heads can be dynamically configured\nwith single data samples via solving contextual bandit problems. To determine\nthe number of layers and heads, we use the Uniform Confidence Bound while we\ndeploy combinatorial Thompson Sampling in order to select specific head\ncombinations given their number. Different from previous work that focuses on\ncompressing trained networks for inference only, DHT is not only advantageous\nfor adaptively optimizing the underlying network architecture during training\nbut also has a flexible network for efficient inference. To the best of our\nknowledge, this is the first comprehensive data-driven dynamic transformer\nwithout any additional auxiliary neural networks that implement the dynamic\nsystem. According to the experiment results, we achieve up to 74% computational\nsavings for both training and inference with a minimal loss of accuracy.",
            "author": [
                "Fanfei Meng",
                "Lele Zhang",
                "Yu Chen",
                "Yuxin Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03038v1",
                "http://arxiv.org/pdf/2312.03038v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.NE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02804v1",
            "title": "Score-Aware Policy-Gradient Methods and Performance Guarantees using\n  Local Lyapunov Conditions: Applications to Product-Form Stochastic Networks\n  and Queueing Systems",
            "updated": "2023-12-05T14:44:58Z",
            "published": "2023-12-05T14:44:58Z",
            "summary": "Stochastic networks and queueing systems often lead to Markov decision\nprocesses (MDPs) with large state and action spaces as well as nonconvex\nobjective functions, which hinders the convergence of many reinforcement\nlearning (RL) algorithms. Policy-gradient methods perform well on MDPs with\nlarge state and action spaces, but they sometimes experience slow convergence\ndue to the high variance of the gradient estimator. In this paper, we show that\nsome of these difficulties can be circumvented by exploiting the structure of\nthe underlying MDP. We first introduce a new family of gradient estimators\ncalled score-aware gradient estimators (SAGEs). When the stationary\ndistribution of the MDP belongs to an exponential family parametrized by the\npolicy parameters, SAGEs allow us to estimate the policy gradient without\nrelying on value-function estimation, contrary to classical policy-gradient\nmethods like actor-critic. To demonstrate their applicability, we examine two\ncommon control problems arising in stochastic networks and queueing systems\nwhose stationary distributions have a product-form, a special case of\nexponential families. As a second contribution, we show that, under appropriate\nassumptions, the policy under a SAGE-based policy-gradient method has a large\nprobability of converging to an optimal policy, provided that it starts\nsufficiently close to it, even with a nonconvex objective function and multiple\nmaximizers. Our key assumptions are that, locally around a maximizer, a\nnondegeneracy property of the Hessian of the objective function holds and a\nLyapunov function exists. Finally, we conduct a numerical comparison between a\nSAGE-based policy-gradient method and an actor-critic algorithm. The results\ndemonstrate that the SAGE-based method finds close-to-optimal policies more\nrapidly, highlighting its superior performance over the traditional\nactor-critic method.",
            "author": [
                "C\u00e9line Comte",
                "Matthieu Jonckheere",
                "Jaron Sanders",
                "Albert Senen-Cerda"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02804v1",
                "http://arxiv.org/pdf/2312.02804v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.PF",
                "math.OC",
                "math.PR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02798v1",
            "title": "Weakly Supervised Detection of Hallucinations in LLM Activations",
            "updated": "2023-12-05T14:35:11Z",
            "published": "2023-12-05T14:35:11Z",
            "summary": "We propose an auditing method to identify whether a large language model\n(LLM) encodes patterns such as hallucinations in its internal states, which may\npropagate to downstream tasks. We introduce a weakly supervised auditing\ntechnique using a subset scanning approach to detect anomalous patterns in LLM\nactivations from pre-trained models. Importantly, our method does not need\nknowledge of the type of patterns a-priori. Instead, it relies on a reference\ndataset devoid of anomalies during testing. Further, our approach enables the\nidentification of pivotal nodes responsible for encoding these patterns, which\nmay offer crucial insights for fine-tuning specific sub-networks for bias\nmitigation. We introduce two new scanning methods to handle LLM activations for\nanomalous sentences that may deviate from the expected distribution in either\ndirection. Our results confirm prior findings of BERT's limited internal\ncapacity for encoding hallucinations, while OPT appears capable of encoding\nhallucination information internally. Importantly, our scanning approach,\nwithout prior exposure to false statements, performs comparably to a fully\nsupervised out-of-distribution classifier.",
            "author": [
                "Miriam Rateike",
                "Celia Cintas",
                "John Wamburu",
                "Tanya Akumu",
                "Skyler Speakman"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02798v1",
                "http://arxiv.org/pdf/2312.02798v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02796v1",
            "title": "Materials Expert-Artificial Intelligence for Materials Discovery",
            "updated": "2023-12-05T14:29:18Z",
            "published": "2023-12-05T14:29:18Z",
            "summary": "The advent of material databases provides an unprecedented opportunity to\nuncover predictive descriptors for emergent material properties from vast data\nspace. However, common reliance on high-throughput ab initio data necessarily\ninherits limitations of such data: mismatch with experiments. On the other\nhand, experimental decisions are often guided by an expert's intuition honed\nfrom experiences that are rarely articulated. We propose using machine learning\nto \"bottle\" such operational intuition into quantifiable descriptors using\nexpertly curated measurement-based data. We introduce \"Materials\nExpert-Artificial Intelligence\" (ME-AI) to encapsulate and articulate this\nhuman intuition. As a first step towards such a program, we focus on the\ntopological semimetal (TSM) among square-net materials as the property inspired\nby the expert-identified descriptor based on structural information: the\ntolerance factor. We start by curating a dataset encompassing 12 primary\nfeatures of 879 square-net materials, using experimental data whenever\npossible. We then use Dirichlet-based Gaussian process regression using a\nspecialized kernel to reveal composite descriptors for square-net topological\nsemimetals. The ME-AI learned descriptors independently reproduce expert\nintuition and expand upon it. Specifically, new descriptors point to\nhypervalency as a critical chemical feature predicting TSM within square-net\ncompounds. Our success with a carefully defined problem points to the \"machine\nbottling human insight\" approach as promising for machine learning-aided\nmaterial discovery.",
            "author": [
                "Yanjun Liu",
                "Milena Jovanovic",
                "Krishnanand Mallayya",
                "Wesley J. Maddox",
                "Andrew Gordon Wilson",
                "Sebastian Klemenz",
                "Leslie M. Schoop",
                "Eun-Ah Kim"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02796v1",
                "http://arxiv.org/pdf/2312.02796v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci",
                "cond-mat.str-el",
                "cs.LG",
                "physics.data-an"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02791v1",
            "title": "Unsupervised learning on spontaneous retinal activity leads to efficient\n  neural representation geometry",
            "updated": "2023-12-05T14:22:46Z",
            "published": "2023-12-05T14:22:46Z",
            "summary": "Prior to the onset of vision, neurons in the developing mammalian retina\nspontaneously fire in correlated activity patterns known as retinal waves.\nExperimental evidence suggests that retinal waves strongly influence the\nemergence of sensory representations before visual experience. We aim to model\nthis early stage of functional development by using movies of neurally active\ndeveloping retinas as pre-training data for neural networks. Specifically, we\npre-train a ResNet-18 with an unsupervised contrastive learning objective\n(SimCLR) on both simulated and experimentally-obtained movies of retinal waves,\nthen evaluate its performance on image classification tasks. We find that\npre-training on retinal waves significantly improves performance on tasks that\ntest object invariance to spatial translation, while slightly improving\nperformance on more complex tasks like image classification. Notably, these\nperformance boosts are realized on held-out natural images even though the\npre-training procedure does not include any natural image data. We then propose\na geometrical explanation for the increase in network performance, namely that\nthe spatiotemporal characteristics of retinal waves facilitate the formation of\nseparable feature representations. In particular, we demonstrate that networks\npre-trained on retinal waves are more effective at separating image manifolds\nthan randomly initialized networks, especially for manifolds defined by sets of\nspatial translations. These findings indicate that the broad spatiotemporal\nproperties of retinal waves prepare networks for higher order feature\nextraction.",
            "author": [
                "Andrew Ligeralde",
                "Yilun Kuang",
                "Thomas Edward Yerxa",
                "Miah N. Pitcher",
                "Marla Feller",
                "SueYeon Chung"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02791v1",
                "http://arxiv.org/pdf/2312.02791v1"
            ],
            "primary_category": "q-bio.NC",
            "category": [
                "q-bio.NC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02786v1",
            "title": "Machine Learning Driven Sensitivity Analysis of E3SM Land Model\n  Parameters for Wetland Methane Emissions",
            "updated": "2023-12-05T14:16:13Z",
            "published": "2023-12-05T14:16:13Z",
            "summary": "Methane (CH4) is the second most critical greenhouse gas after carbon\ndioxide, contributing to 16-25% of the observed atmospheric warming. Wetlands\nare the primary natural source of methane emissions globally. However, wetland\nmethane emission estimates from biogeochemistry models contain considerable\nuncertainty. One of the main sources of this uncertainty arises from the\nnumerous uncertain model parameters within various physical, biological, and\nchemical processes that influence methane production, oxidation, and transport.\nSensitivity Analysis (SA) can help identify critical parameters for methane\nemission and achieve reduced biases and uncertainties in future projections.\nThis study performs SA for 19 selected parameters responsible for critical\nbiogeochemical processes in the methane module of the Energy Exascale Earth\nSystem Model (E3SM) land model (ELM). The impact of these parameters on various\nCH4 fluxes is examined at 14 FLUXNET- CH4 sites with diverse vegetation types.\nGiven the extensive number of model simulations needed for global\nvariance-based SA, we employ a machine learning (ML) algorithm to emulate the\ncomplex behavior of ELM methane biogeochemistry. ML enables the computational\ntime to be shortened significantly from 6 CPU hours to 0.72 milliseconds,\nachieving reduced computational costs. We found that parameters linked to CH4\nproduction and diffusion generally present the highest sensitivities despite\napparent seasonal variation. Comparing simulated emissions from perturbed\nparameter sets against FLUXNET-CH4 observations revealed that better\nperformances can be achieved at each site compared to the default parameter\nvalues. This presents a scope for further improving simulated emissions using\nparameter calibration with advanced optimization techniques like Bayesian\noptimization.",
            "author": [
                "Sandeep Chinta",
                "Xiang Gao",
                "Qing Zhu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02786v1",
                "http://arxiv.org/pdf/2312.02786v1"
            ],
            "primary_category": "physics.ao-ph",
            "category": [
                "physics.ao-ph",
                "cs.LG",
                "physics.data-an"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02783v1",
            "title": "Large Language Models on Graphs: A Comprehensive Survey",
            "updated": "2023-12-05T14:14:27Z",
            "published": "2023-12-05T14:14:27Z",
            "summary": "Large language models (LLMs), such as ChatGPT and LLaMA, are creating\nsignificant advancements in natural language processing, due to their strong\ntext encoding/decoding ability and newly found emergent capability (e.g.,\nreasoning). While LLMs are mainly designed to process pure texts, there are\nmany real-world scenarios where text data are associated with rich structure\ninformation in the form of graphs (e.g., academic networks, and e-commerce\nnetworks) or scenarios where graph data are paired with rich textual\ninformation (e.g., molecules with descriptions). Besides, although LLMs have\nshown their pure text-based reasoning ability, it is underexplored whether such\nability can be generalized to graph scenarios (i.e., graph-based reasoning). In\nthis paper, we provide a systematic review of scenarios and techniques related\nto large language models on graphs. We first summarize potential scenarios of\nadopting LLMs on graphs into three categories, namely pure graphs, text-rich\ngraphs, and text-paired graphs. We then discuss detailed techniques for\nutilizing LLMs on graphs, including LLM as Predictor, LLM as Encoder, and LLM\nas Aligner, and compare the advantages and disadvantages of different schools\nof models. Furthermore, we mention the real-world applications of such methods\nand summarize open-source codes and benchmark datasets. Finally, we conclude\nwith potential future research directions in this fast-growing field. The\nrelated source can be found at\nhttps://github.com/PeterGriffinJin/Awesome-Language-Model-on-Graphs.",
            "author": [
                "Bowen Jin",
                "Gang Liu",
                "Chi Han",
                "Meng Jiang",
                "Heng Ji",
                "Jiawei Han"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02783v1",
                "http://arxiv.org/pdf/2312.02783v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02780v1",
            "title": "Scaling Laws for Adversarial Attacks on Language Model Activations",
            "updated": "2023-12-05T14:12:15Z",
            "published": "2023-12-05T14:12:15Z",
            "summary": "We explore a class of adversarial attacks targeting the activations of\nlanguage models. By manipulating a relatively small subset of model\nactivations, $a$, we demonstrate the ability to control the exact prediction of\na significant number (in some cases up to 1000) of subsequent tokens $t$. We\nempirically verify a scaling law where the maximum number of target tokens\n$t_\\mathrm{max}$ predicted depends linearly on the number of tokens $a$ whose\nactivations the attacker controls as $t_\\mathrm{max} = \\kappa a$. We find that\nthe number of bits of control in the input space needed to control a single bit\nin the output space (what we call attack resistance $\\chi$) is remarkably\nconstant between $\\approx 16$ and $\\approx 25$ over 2 orders of magnitude of\nmodel sizes for different language models. Compared to attacks on tokens,\nattacks on activations are predictably much stronger, however, we identify a\nsurprising regularity where one bit of input steered either via activations or\nvia tokens is able to exert control over a similar amount of output bits. This\ngives support for the hypothesis that adversarial attacks are a consequence of\ndimensionality mismatch between the input and output spaces. A practical\nimplication of the ease of attacking language model activations instead of\ntokens is for multi-modal and selected retrieval models, where additional data\nsources are added as activations directly, sidestepping the tokenized input.\nThis opens up a new, broad attack surface. By using language models as a\ncontrollable test-bed to study adversarial attacks, we were able to experiment\nwith input-output dimensions that are inaccessible in computer vision,\nespecially where the output dimension dominates.",
            "author": [
                "Stanislav Fort"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02780v1",
                "http://arxiv.org/pdf/2312.02780v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02771v1",
            "title": "Scaling-up Memristor Monte Carlo with magnetic domain-wall physics",
            "updated": "2023-12-05T14:01:28Z",
            "published": "2023-12-05T14:01:28Z",
            "summary": "By exploiting the intrinsic random nature of nanoscale devices, Memristor\nMonte Carlo (MMC) is a promising enabler of edge learning systems. However, due\nto multiple algorithmic and device-level limitations, existing demonstrations\nhave been restricted to very small neural network models and datasets. We\ndiscuss these limitations, and describe how they can be overcome, by mapping\nthe stochastic gradient Langevin dynamics (SGLD) algorithm onto the physics of\nmagnetic domain-wall Memristors to scale-up MMC models by five orders of\nmagnitude. We propose the push-pull pulse programming method that realises SGLD\nin-physics, and use it to train a domain-wall based ResNet18 on the CIFAR-10\ndataset. On this task, we observe no performance degradation relative to a\nfloating point model down to an update precision of between 6 and 7-bits,\nindicating we have made a step towards a large-scale edge learning system\nleveraging noisy analogue devices.",
            "author": [
                "Thomas Dalgaty",
                "Shogo Yamada",
                "Anca Molnos",
                "Eiji Kawasaki",
                "Thomas Mesquida",
                "Fran\u00e7ois Rummens",
                "Tatsuo Shibata",
                "Yukihiro Urakawa",
                "Yukio Terasaki",
                "Tomoyuki Sasaki",
                "Marc Duranton"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02771v1",
                "http://arxiv.org/pdf/2312.02771v1"
            ],
            "primary_category": "cs.ET",
            "category": [
                "cs.ET",
                "physics.app-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02770v1",
            "title": "Learning \"Look-Ahead\" Nonlocal Traffic Dynamics in a Ring Road",
            "updated": "2023-12-05T14:00:32Z",
            "published": "2023-12-05T14:00:32Z",
            "summary": "The macroscopic traffic flow model is widely used for traffic control and\nmanagement. To incorporate drivers' anticipative behaviors and to remove\nimpractical speed discontinuity inherent in the classic\nLighthill-Whitham-Richards (LWR) traffic model, nonlocal partial differential\nequation (PDE) models with ``look-ahead\" dynamics have been proposed, which\nassume that the speed is a function of weighted downstream traffic density.\nHowever, it lacks data validation on two important questions: whether there\nexist nonlocal dynamics, and how the length and weight of the ``look-ahead\"\nwindow affect the spatial temporal propagation of traffic densities. In this\npaper, we adopt traffic trajectory data from a ring-road experiment and design\na physics-informed neural network to learn the fundamental diagram and\nlook-ahead kernel that best fit the data, and reinvent a data-enhanced nonlocal\nLWR model via minimizing the loss function combining the data discrepancy and\nthe nonlocal model discrepancy. Results show that the learned nonlocal LWR\nyields a more accurate prediction of traffic wave propagation in three\ndifferent scenarios: stop-and-go oscillations, congested, and free traffic. We\nfirst demonstrate the existence of ``look-ahead\" effect with real traffic data.\nThe optimal nonlocal kernel is found out to take a length of around 35 to 50\nmeters, and the kernel weight within 5 meters accounts for the majority of the\nnonlocal effect. Our results also underscore the importance of choosing a\npriori physics in machine learning models.",
            "author": [
                "Chenguang Zhao",
                "Huan Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02770v1",
                "http://arxiv.org/pdf/2312.02770v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02762v1",
            "title": "Learning Cortical Anomaly through Masked Encoding for Unsupervised\n  Heterogeneity Mapping",
            "updated": "2023-12-05T13:44:25Z",
            "published": "2023-12-05T13:44:25Z",
            "summary": "The detection of heterogeneous mental disorders based on brain readouts\nremains challenging due to the complexity of symptoms and the absence of\nreliable biomarkers. This paper introduces CAM (Cortical Anomaly Detection\nthrough Masked Image Modeling), a novel self-supervised framework designed for\nthe unsupervised detection of complex brain disorders using cortical surface\nfeatures. We employ this framework for the detection of individuals on the\npsychotic spectrum and demonstrate its capabilities compared to state-ofthe-art\nmethods, achieving an AUC of 0.696 for Schizoaffective and 0.769 for\nSchizophreniform, without the need for any labels. Furthermore, the analysis of\natypical cortical regions includes Pars Triangularis and several frontal areas,\noften implicated in schizophrenia, provide further confidence in our approach.\nAltogether, we demonstrate a scalable approach for anomaly detection of complex\nbrain disorders based on cortical abnormalities.",
            "author": [
                "Hao-Chun Yang",
                "Ole Andreassen",
                "Lars Tjelta Westlye",
                "Andre F. Marquand",
                "Christian F. Beckmann",
                "Thomas Wolfers"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02762v1",
                "http://arxiv.org/pdf/2312.02762v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02753v1",
            "title": "C3: High-performance and low-complexity neural compression from a single\n  image or video",
            "updated": "2023-12-05T13:28:59Z",
            "published": "2023-12-05T13:28:59Z",
            "summary": "Most neural compression models are trained on large datasets of images or\nvideos in order to generalize to unseen data. Such generalization typically\nrequires large and expressive architectures with a high decoding complexity.\nHere we introduce C3, a neural compression method with strong rate-distortion\n(RD) performance that instead overfits a small model to each image or video\nseparately. The resulting decoding complexity of C3 can be an order of\nmagnitude lower than neural baselines with similar RD performance. C3 builds on\nCOOL-CHIC (Ladune et al.) and makes several simple and effective improvements\nfor images. We further develop new methodology to apply C3 to videos. On the\nCLIC2020 image benchmark, we match the RD performance of VTM, the reference\nimplementation of the H.266 codec, with less than 3k MACs/pixel for decoding.\nOn the UVG video benchmark, we match the RD performance of the Video\nCompression Transformer (Mentzer et al.), a well-established neural video\ncodec, with less than 5k MACs/pixel for decoding.",
            "author": [
                "Hyunjik Kim",
                "Matthias Bauer",
                "Lucas Theis",
                "Jonathan Richard Schwarz",
                "Emilien Dupont"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02753v1",
                "http://arxiv.org/pdf/2312.02753v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV",
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02748v1",
            "title": "Compositional Generalization for Data-to-Text Generation",
            "updated": "2023-12-05T13:23:15Z",
            "published": "2023-12-05T13:23:15Z",
            "summary": "Data-to-text generation involves transforming structured data, often\nrepresented as predicate-argument tuples, into coherent textual descriptions.\nDespite recent advances, systems still struggle when confronted with unseen\ncombinations of predicates, producing unfaithful descriptions (e.g.\nhallucinations or omissions). We refer to this issue as compositional\ngeneralisation, and it encouraged us to create a benchmark for assessing the\nperformance of different approaches on this specific problem. Furthermore, we\npropose a novel model that addresses compositional generalization by clustering\npredicates into groups. Our model generates text in a sentence-by-sentence\nmanner, relying on one cluster of predicates at a time. This approach\nsignificantly outperforms T5~baselines across all evaluation metrics.Notably,\nit achieved a 31% improvement over T5 in terms of a metric focused on\nmaintaining faithfulness to the input.",
            "author": [
                "Xinnuo Xu",
                "Ivan Titov",
                "Mirella Lapata"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02748v1",
                "http://arxiv.org/pdf/2312.02748v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02742v1",
            "title": "Search for the edge-on galaxies using an artificial neural network",
            "updated": "2023-12-05T13:08:40Z",
            "published": "2023-12-05T13:08:40Z",
            "summary": "We present an application of an artificial neural network methodology to a\nmodern wide-field sky survey Pan-STARRS1 in order to build a high-quality\nsample of disk galaxies visible in edge-on orientation. Such galaxies play an\nimportant role in the study of the vertical distribution of stars, gas and\ndust, which is usually not available to study in other galaxies outside the\nMilky Way. We give a detailed description of the network architecture and the\nlearning process. The method demonstrates good effectiveness with detection\nrate about 97\\% and it works equally well for galaxies over a wide range of\nbrightnesses and sizes, which resulted in a creation of a catalogue of edge-on\ngalaxies with $10^5$ of objects. The catalogue is published on-line with an\nopen access.",
            "author": [
                "S. S. Savchenko",
                "D. I. Makarov",
                "A. V. Antipova",
                "I. S. Tikhonenko"
            ],
            "link": [
                "http://dx.doi.org/10.1016/j.ascom.2023.100771",
                "http://arxiv.org/abs/2312.02742v1",
                "http://arxiv.org/pdf/2312.02742v1"
            ],
            "primary_category": "astro-ph.IM",
            "category": [
                "astro-ph.IM",
                "astro-ph.GA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02739v1",
            "title": "LExCI: A Framework for Reinforcement Learning with Embedded Systems",
            "updated": "2023-12-05T13:06:25Z",
            "published": "2023-12-05T13:06:25Z",
            "summary": "Advances in artificial intelligence (AI) have led to its application in many\nareas of everyday life. In the context of control engineering, reinforcement\nlearning (RL) represents a particularly promising approach as it is centred\naround the idea of allowing an agent to freely interact with its environment to\nfind an optimal strategy. One of the challenges professionals face when\ntraining and deploying RL agents is that the latter often have to run on\ndedicated embedded devices. This could be to integrate them into an existing\ntoolchain or to satisfy certain performance criteria like real-time\nconstraints. Conventional RL libraries, however, cannot be easily utilised in\nconjunction with that kind of hardware. In this paper, we present a framework\nnamed LExCI, the Learning and Experiencing Cycle Interface, which bridges this\ngap and provides end-users with a free and open-source tool for training agents\non embedded systems using the open-source library RLlib. Its operability is\ndemonstrated with two state-of-the-art RL-algorithms and a rapid control\nprototyping system.",
            "author": [
                "Kevin Badalian",
                "Lucas Koch",
                "Tobias Brinkmann",
                "Mario Picerno",
                "Marius Wegener",
                "Sung-Yong Lee",
                "Jakob Andert"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02739v1",
                "http://arxiv.org/pdf/2312.02739v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02737v1",
            "title": "Track finding with deep neural networks",
            "updated": "2023-12-05T13:01:41Z",
            "published": "2023-12-05T13:01:41Z",
            "summary": "High-energy physics experiments require fast and efficient methods for\nreconstructing the tracks of charged particles. The commonly used algorithms\nare sequential, and the required CPU power increases rapidly with the number of\ntracks. Neural networks can speed up the process due to their capability of\nmodeling complex non-linear data dependencies and finding all tracks in\nparallel. In this paper, we describe the application of a deep neural network\nfor reconstructing straight tracks in a toy two-dimensional model. It is\nplanned to apply this method to the experimental data obtained by the MUonE\nexperiment at CERN.",
            "author": [
                "Marcin Kucharczyk",
                "Marcin Wolter"
            ],
            "link": [
                "http://dx.doi.org/10.7494/csci.2019.20.4.3376",
                "http://arxiv.org/abs/2312.02737v1",
                "http://arxiv.org/pdf/2312.02737v1"
            ],
            "primary_category": "hep-ex",
            "category": [
                "hep-ex"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02730v1",
            "title": "Towards Measuring Representational Similarity of Large Language Models",
            "updated": "2023-12-05T12:48:04Z",
            "published": "2023-12-05T12:48:04Z",
            "summary": "Understanding the similarity of the numerous released large language models\n(LLMs) has many uses, e.g., simplifying model selection, detecting illegal\nmodel reuse, and advancing our understanding of what makes LLMs perform well.\nIn this work, we measure the similarity of representations of a set of LLMs\nwith 7B parameters. Our results suggest that some LLMs are substantially\ndifferent from others. We identify challenges of using representational\nsimilarity measures that suggest the need of careful study of similarity scores\nto avoid false conclusions.",
            "author": [
                "Max Klabunde",
                "Mehdi Ben Amor",
                "Michael Granitzer",
                "Florian Lemmerich"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02730v1",
                "http://arxiv.org/pdf/2312.02730v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03033v1",
            "title": "LiDAR-based Person Re-identification",
            "updated": "2023-12-05T12:44:17Z",
            "published": "2023-12-05T12:44:17Z",
            "summary": "Camera-based person re-identification (ReID) systems have been widely applied\nin the field of public security. However, cameras often lack the perception of\n3D morphological information of human and are susceptible to various\nlimitations, such as inadequate illumination, complex background, and personal\nprivacy. In this paper, we propose a LiDAR-based ReID framework, ReID3D, that\nutilizes pre-training strategy to retrieve features of 3D body shape and\nintroduces Graph-based Complementary Enhancement Encoder for extracting\ncomprehensive features. Due to the lack of LiDAR datasets, we build LReID, the\nfirst LiDAR-based person ReID dataset, which is collected in several outdoor\nscenes with variations in natural conditions. Additionally, we introduce\nLReID-sync, a simulated pedestrian dataset designed for pre-training encoders\nwith tasks of point cloud completion and shape parameter learning. Extensive\nexperiments on LReID show that ReID3D achieves exceptional performance with a\nrank-1 accuracy of 94.0, highlighting the significant potential of LiDAR in\naddressing person ReID tasks. To the best of our knowledge, we are the first to\npropose a solution for LiDAR-based ReID. The code and datasets will be released\nsoon.",
            "author": [
                "Wenxuan Guo",
                "Zhiyu Pan",
                "Yingping Liang",
                "Ziheng Xi",
                "Zhi Chen Zhong",
                "Jianjiang Feng",
                "Jie Zhou"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03033v1",
                "http://arxiv.org/pdf/2312.03033v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02724v1",
            "title": "RankZephyr: Effective and Robust Zero-Shot Listwise Reranking is a\n  Breeze!",
            "updated": "2023-12-05T12:39:00Z",
            "published": "2023-12-05T12:39:00Z",
            "summary": "In information retrieval, proprietary large language models (LLMs) such as\nGPT-4 and open-source counterparts such as LLaMA and Vicuna have played a vital\nrole in reranking. However, the gap between open-source and closed models\npersists, with reliance on proprietary, non-transparent models constraining\nreproducibility. Addressing this gap, we introduce RankZephyr, a\nstate-of-the-art, open-source LLM for listwise zero-shot reranking. RankZephyr\nnot only bridges the effectiveness gap with GPT-4 but in some cases surpasses\nthe proprietary model. Our comprehensive evaluations across several datasets\n(TREC Deep Learning Tracks; NEWS and COVID from BEIR) showcase this ability.\nRankZephyr benefits from strategic training choices and is resilient against\nvariations in initial document ordering and the number of documents reranked.\nAdditionally, our model outperforms GPT-4 on the NovelEval test set, comprising\nqueries and passages past its training period, which addresses concerns about\ndata contamination. To foster further research in this rapidly evolving field,\nwe provide all code necessary to reproduce our results at\nhttps://github.com/castorini/rank_llm.",
            "author": [
                "Ronak Pradeep",
                "Sahel Sharifymoghaddam",
                "Jimmy Lin"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02724v1",
                "http://arxiv.org/pdf/2312.02724v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02720v1",
            "title": "Towards the Inferrence of Structural Similarity of Combinatorial\n  Landscapes",
            "updated": "2023-12-05T12:34:51Z",
            "published": "2023-12-05T12:34:51Z",
            "summary": "One of the most common problem-solving heuristics is by analogy. For a given\nproblem, a solver can be viewed as a strategic walk on its fitness landscape.\nThus if a solver works for one problem instance, we expect it will also be\neffective for other instances whose fitness landscapes essentially share\nstructural similarities with each other. However, due to the black-box nature\nof combinatorial optimization, it is far from trivial to infer such similarity\nin real-world scenarios. To bridge this gap, by using local optima network as a\nproxy of fitness landscapes, this paper proposed to leverage graph data mining\ntechniques to conduct qualitative and quantitative analyses to explore the\nlatent topological structural information embedded in those landscapes. By\nconducting large-scale empirical experiments on three classic combinatorial\noptimization problems, we gain concrete evidence to support the existence of\nstructural similarity between landscapes of the same classes within neighboring\ndimensions. We also interrogated the relationship between landscapes of\ndifferent problem classes.",
            "author": [
                "Mingyu Huang",
                "Ke Li"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02720v1",
                "http://arxiv.org/pdf/2312.02720v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02708v1",
            "title": "(Provable) Adversarial Robustness for Group Equivariant Tasks: Graphs,\n  Point Clouds, Molecules, and More",
            "updated": "2023-12-05T12:09:45Z",
            "published": "2023-12-05T12:09:45Z",
            "summary": "A machine learning model is traditionally considered robust if its prediction\nremains (almost) constant under input perturbations with small norm. However,\nreal-world tasks like molecular property prediction or point cloud segmentation\nhave inherent equivariances, such as rotation or permutation equivariance. In\nsuch tasks, even perturbations with large norm do not necessarily change an\ninput's semantic content. Furthermore, there are perturbations for which a\nmodel's prediction explicitly needs to change. For the first time, we propose a\nsound notion of adversarial robustness that accounts for task equivariance. We\nthen demonstrate that provable robustness can be achieved by (1) choosing a\nmodel that matches the task's equivariances (2) certifying traditional\nadversarial robustness. Certification methods are, however, unavailable for\nmany models, such as those with continuous equivariances. We close this gap by\ndeveloping the framework of equivariance-preserving randomized smoothing, which\nenables architecture-agnostic certification. We additionally derive the first\narchitecture-specific graph edit distance certificates, i.e. sound robustness\nguarantees for isomorphism equivariant tasks like node classification. Overall,\na sound notion of robustness is an important prerequisite for future work at\nthe intersection of robust and geometric machine learning.",
            "author": [
                "Jan Schuchardt",
                "Yan Scholten",
                "Stephan G\u00fcnnemann"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02708v1",
                "http://arxiv.org/pdf/2312.02708v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CR",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02705v1",
            "title": "Unified learning-based lossy and lossless JPEG recompression",
            "updated": "2023-12-05T12:07:27Z",
            "published": "2023-12-05T12:07:27Z",
            "summary": "JPEG is still the most widely used image compression algorithm. Most image\ncompression algorithms only consider uncompressed original image, while\nignoring a large number of already existing JPEG images. Recently, JPEG\nrecompression approaches have been proposed to further reduce the size of JPEG\nfiles. However, those methods only consider JPEG lossless recompression, which\nis just a special case of the rate-distortion theorem. In this paper, we\npropose a unified lossly and lossless JPEG recompression framework, which\nconsists of learned quantization table and Markovian hierarchical variational\nautoencoders. Experiments show that our method can achieve arbitrarily low\ndistortion when the bitrate is close to the upper bound, namely the bitrate of\nthe lossless compression model. To the best of our knowledge, this is the first\nlearned method that bridges the gap between lossy and lossless recompression of\nJPEG images.",
            "author": [
                "Jianghui Zhang",
                "Yuanyuan Wang",
                "Lina Guo",
                "Jixiang Luo",
                "Tongda Xu",
                "Yan Wang",
                "Zhi Wang",
                "Hongwei Qin"
            ],
            "link": [
                "http://dx.doi.org/10.1109/ICIP49359.2023.10222354",
                "http://arxiv.org/abs/2312.02705v1",
                "http://arxiv.org/pdf/2312.02705v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02702v1",
            "title": "Neural Sign Actors: A diffusion model for 3D sign language production\n  from text",
            "updated": "2023-12-05T12:04:34Z",
            "published": "2023-12-05T12:04:34Z",
            "summary": "Sign Languages (SL) serve as the predominant mode of communication for the\nDeaf and Hard of Hearing communities. The advent of deep learning has aided\nnumerous methods in SL recognition and translation, achieving remarkable\nresults. However, Sign Language Production (SLP) poses a challenge for the\ncomputer vision community as the motions generated must be realistic and have\nprecise semantic meanings. Most SLP methods rely on 2D data, thus impeding\ntheir ability to attain a necessary level of realism. In this work, we propose\na diffusion-based SLP model trained on a curated large-scale dataset of 4D\nsigning avatars and their corresponding text transcripts. The proposed method\ncan generate dynamic sequences of 3D avatars from an unconstrained domain of\ndiscourse using a diffusion process formed on a novel and anatomically informed\ngraph neural network defined on the SMPL-X body skeleton. Through a series of\nquantitative and qualitative experiments, we show that the proposed method\nconsiderably outperforms previous methods of SLP. We believe that this work\npresents an important and necessary step towards realistic neural sign avatars,\nbridging the communication gap between Deaf and hearing communities. The code,\nmethod and generated data will be made publicly available.",
            "author": [
                "Vasileios Baltatzis",
                "Rolandos Alexandros Potamias",
                "Evangelos Ververas",
                "Guanxiong Sun",
                "Jiankang Deng",
                "Stefanos Zafeiriou"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02702v1",
                "http://arxiv.org/pdf/2312.02702v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02699v1",
            "title": "Enhancing Vehicle Entrance and Parking Management: Deep Learning\n  Solutions for Efficiency and Security",
            "updated": "2023-12-05T12:02:53Z",
            "published": "2023-12-05T12:02:53Z",
            "summary": "The auto-management of vehicle entrance and parking in any organization is a\ncomplex challenge encompassing record-keeping, efficiency, and security\nconcerns. Manual methods for tracking vehicles and finding parking spaces are\nslow and a waste of time. To solve the problem of auto management of vehicle\nentrance and parking, we have utilized state-of-the-art deep learning models\nand automated the process of vehicle entrance and parking into any\norganization. To ensure security, our system integrated vehicle detection,\nlicense number plate verification, and face detection and recognition models to\nensure that the person and vehicle are registered with the organization. We\nhave trained multiple deep-learning models for vehicle detection, license\nnumber plate detection, face detection, and recognition, however, the YOLOv8n\nmodel outperformed all the other models. Furthermore, License plate recognition\nis facilitated by Google's Tesseract-OCR Engine. By integrating these\ntechnologies, the system offers efficient vehicle detection, precise\nidentification, streamlined record keeping, and optimized parking slot\nallocation in buildings, thereby enhancing convenience, accuracy, and security.\nFuture research opportunities lie in fine-tuning system performance for a wide\nrange of real-world applications.",
            "author": [
                "Muhammad Umer Ramzan",
                "Usman Ali",
                "Syed Haider Abbas Naqvi",
                "Zeeshan Aslam",
                "Tehseen",
                "Husnain Ali",
                "Muhammad Faheem"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02699v1",
                "http://arxiv.org/pdf/2312.02699v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02697v1",
            "title": "Hierarchical Visual Policy Learning for Long-Horizon Robot Manipulation\n  in Densely Cluttered Scenes",
            "updated": "2023-12-05T11:57:39Z",
            "published": "2023-12-05T11:57:39Z",
            "summary": "In this work, we focus on addressing the long-horizon manipulation tasks in\ndensely cluttered scenes. Such tasks require policies to effectively manage\nsevere occlusions among objects and continually produce actions based on visual\nobservations. We propose a vision-based Hierarchical policy for Cluttered-scene\nLong-horizon Manipulation (HCLM). It employs a high-level policy and three\noptions to select and instantiate three parameterized action primitives: push,\npick, and place. We first train the pick and place options by behavior cloning\n(BC). Subsequently, we use hierarchical reinforcement learning (HRL) to train\nthe high-level policy and push option. During HRL, we propose a Spatially\nExtended Q-update (SEQ) to augment the updates for the push option and a\nTwo-Stage Update Scheme (TSUS) to alleviate the non-stationary transition\nproblem in updating the high-level policy. We demonstrate that HCLM\nsignificantly outperforms baseline methods in terms of success rate and\nefficiency in diverse tasks. We also highlight our method's ability to\ngeneralize to more cluttered environments with more additional blocks.",
            "author": [
                "Hecheng Wang",
                "Lizhe Qi",
                "Bin Fang",
                "Yunquan Sun"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02697v1",
                "http://arxiv.org/pdf/2312.02697v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02696v1",
            "title": "Analyzing and Improving the Training Dynamics of Diffusion Models",
            "updated": "2023-12-05T11:55:47Z",
            "published": "2023-12-05T11:55:47Z",
            "summary": "Diffusion models currently dominate the field of data-driven image synthesis\nwith their unparalleled scaling to large datasets. In this paper, we identify\nand rectify several causes for uneven and ineffective training in the popular\nADM diffusion model architecture, without altering its high-level structure.\nObserving uncontrolled magnitude changes and imbalances in both the network\nactivations and weights over the course of training, we redesign the network\nlayers to preserve activation, weight, and update magnitudes on expectation. We\nfind that systematic application of this philosophy eliminates the observed\ndrifts and imbalances, resulting in considerably better networks at equal\ncomputational complexity. Our modifications improve the previous record FID of\n2.41 in ImageNet-512 synthesis to 1.81, achieved using fast deterministic\nsampling.\n  As an independent contribution, we present a method for setting the\nexponential moving average (EMA) parameters post-hoc, i.e., after completing\nthe training run. This allows precise tuning of EMA length without the cost of\nperforming several training runs, and reveals its surprising interactions with\nnetwork architecture, training time, and guidance.",
            "author": [
                "Tero Karras",
                "Miika Aittala",
                "Jaakko Lehtinen",
                "Janne Hellsten",
                "Timo Aila",
                "Samuli Laine"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02696v1",
                "http://arxiv.org/pdf/2312.02696v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG",
                "cs.NE",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02684v1",
            "title": "DeepPointMap: Advancing LiDAR SLAM with Unified Neural Descriptors",
            "updated": "2023-12-05T11:40:41Z",
            "published": "2023-12-05T11:40:41Z",
            "summary": "Point clouds have shown significant potential in various domains, including\nSimultaneous Localization and Mapping (SLAM). However, existing approaches\neither rely on dense point clouds to achieve high localization accuracy or use\ngeneralized descriptors to reduce map size. Unfortunately, these two aspects\nseem to conflict with each other. To address this limitation, we propose a\nunified architecture, DeepPointMap, achieving excellent preference on both\naspects. We utilize neural network to extract highly representative and sparse\nneural descriptors from point clouds, enabling memory-efficient map\nrepresentation and accurate multi-scale localization tasks (e.g., odometry and\nloop-closure). Moreover, we showcase the versatility of our framework by\nextending it to more challenging multi-agent collaborative SLAM. The promising\nresults obtained in these scenarios further emphasize the effectiveness and\npotential of our approach.",
            "author": [
                "Xiaze Zhang",
                "Ziheng Ding",
                "Qi Jing",
                "Yuejie Zhang",
                "Wenchao Ding",
                "Rui Feng"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02684v1",
                "http://arxiv.org/pdf/2312.02684v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02683v1",
            "title": "Diffusion-Based Speech Enhancement in Matched and Mismatched Conditions\n  Using a Heun-Based Sampler",
            "updated": "2023-12-05T11:40:38Z",
            "published": "2023-12-05T11:40:38Z",
            "summary": "Diffusion models are a new class of generative models that have recently been\napplied to speech enhancement successfully. Previous works have demonstrated\ntheir superior performance in mismatched conditions compared to state-of-the\nart discriminative models. However, this was investigated with a single\ndatabase for training and another one for testing, which makes the results\nhighly dependent on the particular databases. Moreover, recent developments\nfrom the image generation literature remain largely unexplored for speech\nenhancement. These include several design aspects of diffusion models, such as\nthe noise schedule or the reverse sampler. In this work, we systematically\nassess the generalization performance of a diffusion-based speech enhancement\nmodel by using multiple speech, noise and binaural room impulse response (BRIR)\ndatabases to simulate mismatched acoustic conditions. We also experiment with a\nnoise schedule and a sampler that have not been applied to speech enhancement\nbefore. We show that the proposed system substantially benefits from using\nmultiple databases for training, and achieves superior performance compared to\nstate-of-the-art discriminative models in both matched and mismatched\nconditions. We also show that a Heun-based sampler achieves superior\nperformance at a smaller computational cost compared to a sampler commonly used\nfor speech enhancement.",
            "author": [
                "Philippe Gonzalez",
                "Zheng-Hua Tan",
                "Jan \u00d8stergaard",
                "Jesper Jensen",
                "Tommy Sonne Alstr\u00f8m",
                "Tobias May"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02683v1",
                "http://arxiv.org/pdf/2312.02683v1"
            ],
            "primary_category": "eess.AS",
            "category": [
                "eess.AS",
                "cs.LG",
                "cs.SD"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02682v1",
            "title": "H-GAP: Humanoid Control with a Generalist Planner",
            "updated": "2023-12-05T11:40:24Z",
            "published": "2023-12-05T11:40:24Z",
            "summary": "Humanoid control is an important research challenge offering avenues for\nintegration into human-centric infrastructures and enabling physics-driven\nhumanoid animations. The daunting challenges in this field stem from the\ndifficulty of optimizing in high-dimensional action spaces and the instability\nintroduced by the bipedal morphology of humanoids. However, the extensive\ncollection of human motion-captured data and the derived datasets of humanoid\ntrajectories, such as MoCapAct, paves the way to tackle these challenges. In\nthis context, we present Humanoid Generalist Autoencoding Planner (H-GAP), a\nstate-action trajectory generative model trained on humanoid trajectories\nderived from human motion-captured data, capable of adeptly handling downstream\ncontrol tasks with Model Predictive Control (MPC). For 56 degrees of freedom\nhumanoid, we empirically demonstrate that H-GAP learns to represent and\ngenerate a wide range of motor behaviours. Further, without any learning from\nonline interactions, it can also flexibly transfer these behaviors to solve\nnovel downstream control tasks via planning. Notably, H-GAP excels established\nMPC baselines that have access to the ground truth dynamics model, and is\nsuperior or comparable to offline RL methods trained for individual tasks.\nFinally, we do a series of empirical studies on the scaling properties of\nH-GAP, showing the potential for performance gains via additional data but not\ncomputing. Code and videos are available at\nhttps://ycxuyingchen.github.io/hgap/.",
            "author": [
                "Zhengyao Jiang",
                "Yingchen Xu",
                "Nolan Wagener",
                "Yicheng Luo",
                "Michael Janner",
                "Edward Grefenstette",
                "Tim Rockt\u00e4schel",
                "Yuandong Tian"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02682v1",
                "http://arxiv.org/pdf/2312.02682v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03032v1",
            "title": "Zero-Shot Point Cloud Registration",
            "updated": "2023-12-05T11:33:16Z",
            "published": "2023-12-05T11:33:16Z",
            "summary": "Learning-based point cloud registration approaches have significantly\noutperformed their traditional counterparts. However, they typically require\nextensive training on specific datasets. In this paper, we propose , the first\nzero-shot point cloud registration approach that eliminates the need for\ntraining on point cloud datasets. The cornerstone of ZeroReg is the novel\ntransfer of image features from keypoints to the point cloud, enriched by\naggregating information from 3D geometric neighborhoods. Specifically, we\nextract keypoints and features from 2D image pairs using a frozen pretrained 2D\nbackbone. These features are then projected in 3D, and patches are constructed\nby searching for neighboring points. We integrate the geometric and visual\nfeatures of each point using our novel parameter-free geometric decoder.\nSubsequently, the task of determining correspondences between point clouds is\nformulated as an optimal transport problem. Extensive evaluations of ZeroReg\ndemonstrate its competitive performance against both traditional and\nlearning-based methods. On benchmarks such as 3DMatch, 3DLoMatch, and ScanNet,\nZeroReg achieves impressive Recall Ratios (RR) of over 84%, 46%, and 75%,\nrespectively.",
            "author": [
                "Weijie Wang",
                "Guofeng Mei",
                "Bin Ren",
                "Xiaoshui Huang",
                "Fabio Poiesi",
                "Luc Van Gool",
                "Nicu Sebe",
                "Bruno Lepri"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03032v1",
                "http://arxiv.org/pdf/2312.03032v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02677v1",
            "title": "Contact Energy Based Hindsight Experience Prioritization",
            "updated": "2023-12-05T11:32:25Z",
            "published": "2023-12-05T11:32:25Z",
            "summary": "Multi-goal robot manipulation tasks with sparse rewards are difficult for\nreinforcement learning (RL) algorithms due to the inefficiency in collecting\nsuccessful experiences. Recent algorithms such as Hindsight Experience Replay\n(HER) expedite learning by taking advantage of failed trajectories and\nreplacing the desired goal with one of the achieved states so that any failed\ntrajectory can be utilized as a contribution to learning. However, HER\nuniformly chooses failed trajectories, without taking into account which ones\nmight be the most valuable for learning. In this paper, we address this problem\nand propose a novel approach Contact Energy Based Prioritization~(CEBP) to\nselect the samples from the replay buffer based on rich information due to\ncontact, leveraging the touch sensors in the gripper of the robot and object\ndisplacement. Our prioritization scheme favors sampling of contact-rich\nexperiences, which are arguably the ones providing the largest amount of\ninformation. We evaluate our proposed approach on various sparse reward robotic\ntasks and compare them with the state-of-the-art methods. We show that our\nmethod surpasses or performs on par with those methods on robot manipulation\ntasks. Finally, we deploy the trained policy from our method to a real Franka\nrobot for a pick-and-place task. We observe that the robot can solve the task\nsuccessfully. The videos and code are publicly available at:\nhttps://erdiphd.github.io/HER_force",
            "author": [
                "Erdi Sayar",
                "Zhenshan Bing",
                "Carlo D'Eramo",
                "Ozgur S. Oguz",
                "Alois Knoll"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02677v1",
                "http://arxiv.org/pdf/2312.02677v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02674v1",
            "title": "Amortized Bayesian Decision Making for simulation-based models",
            "updated": "2023-12-05T11:29:54Z",
            "published": "2023-12-05T11:29:54Z",
            "summary": "Simulation-based inference (SBI) provides a powerful framework for inferring\nposterior distributions of stochastic simulators in a wide range of domains. In\nmany settings, however, the posterior distribution is not the end goal itself\n-- rather, the derived parameter values and their uncertainties are used as a\nbasis for deciding what actions to take. Unfortunately, because posterior\ndistributions provided by SBI are (potentially crude) approximations of the\ntrue posterior, the resulting decisions can be suboptimal. Here, we address the\nquestion of how to perform Bayesian decision making on stochastic simulators,\nand how one can circumvent the need to compute an explicit approximation to the\nposterior. Our method trains a neural network on simulated data and can predict\nthe expected cost given any data and action, and can, thus, be directly used to\ninfer the action with lowest cost. We apply our method to several benchmark\nproblems and demonstrate that it induces similar cost as the true posterior\ndistribution. We then apply the method to infer optimal actions in a real-world\nsimulator in the medical neurosciences, the Bayesian Virtual Epileptic Patient,\nand demonstrate that it allows to infer actions associated with low cost after\nfew simulations.",
            "author": [
                "Mila Gorecki",
                "Jakob H. Macke",
                "Michael Deistler"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02674v1",
                "http://arxiv.org/pdf/2312.02674v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02673v1",
            "title": "Robust Backdoor Detection for Deep Learning via Topological Evolution\n  Dynamics",
            "updated": "2023-12-05T11:29:12Z",
            "published": "2023-12-05T11:29:12Z",
            "summary": "A backdoor attack in deep learning inserts a hidden backdoor in the model to\ntrigger malicious behavior upon specific input patterns. Existing detection\napproaches assume a metric space (for either the original inputs or their\nlatent representations) in which normal samples and malicious samples are\nseparable. We show that this assumption has a severe limitation by introducing\na novel SSDT (Source-Specific and Dynamic-Triggers) backdoor, which obscures\nthe difference between normal samples and malicious samples.\n  To overcome this limitation, we move beyond looking for a perfect metric\nspace that would work for different deep-learning models, and instead resort to\nmore robust topological constructs. We propose TED (Topological Evolution\nDynamics) as a model-agnostic basis for robust backdoor detection. The main\nidea of TED is to view a deep-learning model as a dynamical system that evolves\ninputs to outputs. In such a dynamical system, a benign input follows a natural\nevolution trajectory similar to other benign inputs. In contrast, a malicious\nsample displays a distinct trajectory, since it starts close to benign samples\nbut eventually shifts towards the neighborhood of attacker-specified target\nsamples to activate the backdoor.\n  Extensive evaluations are conducted on vision and natural language datasets\nacross different network architectures. The results demonstrate that TED not\nonly achieves a high detection rate, but also significantly outperforms\nexisting state-of-the-art detection approaches, particularly in addressing the\nsophisticated SSDT attack. The code to reproduce the results is made public on\nGitHub.",
            "author": [
                "Xiaoxing Mo",
                "Yechao Zhang",
                "Leo Yu Zhang",
                "Wei Luo",
                "Nan Sun",
                "Shengshan Hu",
                "Shang Gao",
                "Yang Xiang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02673v1",
                "http://arxiv.org/pdf/2312.02673v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02671v1",
            "title": "Learning a Sparse Representation of Barron Functions with the Inverse\n  Scale Space Flow",
            "updated": "2023-12-05T11:26:02Z",
            "published": "2023-12-05T11:26:02Z",
            "summary": "This paper presents a method for finding a sparse representation of Barron\nfunctions. Specifically, given an $L^2$ function $f$, the inverse scale space\nflow is used to find a sparse measure $\\mu$ minimising the $L^2$ loss between\nthe Barron function associated to the measure $\\mu$ and the function $f$. The\nconvergence properties of this method are analysed in an ideal setting and in\nthe cases of measurement noise and sampling bias. In an ideal setting the\nobjective decreases strictly monotone in time to a minimizer with\n$\\mathcal{O}(1/t)$, and in the case of measurement noise or sampling bias the\noptimum is achieved up to a multiplicative or additive constant. This\nconvergence is preserved on discretization of the parameter space, and the\nminimizers on increasingly fine discretizations converge to the optimum on the\nfull parameter space.",
            "author": [
                "Tjeerd Jan Heeringa",
                "Tim Roith",
                "Christoph Brune",
                "Martin Burger"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02671v1",
                "http://arxiv.org/pdf/2312.02671v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG",
                "cs.NA",
                "math.FA",
                "math.NA",
                "47A52, 68T07, 65K10, 90C25",
                "I.2.6; F.2.1; G.1.6"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02669v1",
            "title": "Deep-learning-driven end-to-end metalens imaging",
            "updated": "2023-12-05T11:22:09Z",
            "published": "2023-12-05T11:22:09Z",
            "summary": "Recent advances in metasurface lenses (metalenses) have shown great potential\nfor opening a new era in compact imaging, photography, light detection and\nranging (LiDAR), and virtual reality/augmented reality (VR/AR) applications.\nHowever, the fundamental trade-off between broadband focusing efficiency and\noperating bandwidth limits the performance of broadband metalenses, resulting\nin chromatic aberration, angular aberration, and a relatively low efficiency.\nIn this study, a deep-learning-based image restoration framework is proposed to\novercome these limitations and realize end-to-end metalens imaging, thereby\nachieving aberration-free full-color imaging for massproduced metalenses with\n10-mm diameter. Neural network-assisted metalens imaging achieved a high\nresolution comparable to that of the ground truth image.",
            "author": [
                "Joonhyuk Seo",
                "Jaegang Jo",
                "Joohoon Kim",
                "Joonho Kang",
                "Chanik Kang",
                "Seongwon Moon",
                "Eunji Lee",
                "Jehyeong Hong",
                "Junsuk Rho",
                "Haejun Chung"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02669v1",
                "http://arxiv.org/pdf/2312.02669v1"
            ],
            "primary_category": "physics.optics",
            "category": [
                "physics.optics",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02665v1",
            "title": "Lights out: training RL agents robust to temporary blindness",
            "updated": "2023-12-05T11:10:05Z",
            "published": "2023-12-05T11:10:05Z",
            "summary": "Agents trained with DQN rely on an observation at each timestep to decide\nwhat action to take next. However, in real world applications observations can\nchange or be missing entirely. Examples of this could be a light bulb breaking\ndown, or the wallpaper in a certain room changing. While these situations\nchange the actual observation, the underlying optimal policy does not change.\nBecause of this we want our agent to continue taking actions until it receives\na (recognized) observation again. To achieve this we introduce a combination of\na neural network architecture that uses hidden representations of the\nobservations and a novel n-step loss function. Our implementation is able to\nwithstand location based blindness stretches longer than the ones it was\ntrained on, and therefore shows robustness to temporary blindness. For access\nto our implementation, please email Nathan, Marije, or Pau.",
            "author": [
                "N. Ordonez",
                "M. Tromp",
                "P. M. Julbe",
                "W. B\u00f6hmer"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02665v1",
                "http://arxiv.org/pdf/2312.02665v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03030v1",
            "title": "Generating Visually Realistic Adversarial Patch",
            "updated": "2023-12-05T11:07:39Z",
            "published": "2023-12-05T11:07:39Z",
            "summary": "Deep neural networks (DNNs) are vulnerable to various types of adversarial\nexamples, bringing huge threats to security-critical applications. Among these,\nadversarial patches have drawn increasing attention due to their good\napplicability to fool DNNs in the physical world. However, existing works often\ngenerate patches with meaningless noise or patterns, making it conspicuous to\nhumans. To address this issue, we explore how to generate visually realistic\nadversarial patches to fool DNNs. Firstly, we analyze that a high-quality\nadversarial patch should be realistic, position irrelevant, and printable to be\ndeployed in the physical world. Based on this analysis, we propose an effective\nattack called VRAP, to generate visually realistic adversarial patches.\nSpecifically, VRAP constrains the patch in the neighborhood of a real image to\nensure the visual reality, optimizes the patch at the poorest position for\nposition irrelevance, and adopts Total Variance loss as well as gamma\ntransformation to make the generated patch printable without losing\ninformation. Empirical evaluations on the ImageNet dataset demonstrate that the\nproposed VRAP exhibits outstanding attack performance in the digital world.\nMoreover, the generated adversarial patches can be disguised as the scrawl or\nlogo in the physical world to fool the deep models without being detected,\nbringing significant threats to DNNs-enabled applications.",
            "author": [
                "Xiaosen Wang",
                "Kunyu Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03030v1",
                "http://arxiv.org/pdf/2312.03030v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03029v1",
            "title": "Gaussian Head Avatar: Ultra High-fidelity Head Avatar via Dynamic\n  Gaussians",
            "updated": "2023-12-05T11:01:44Z",
            "published": "2023-12-05T11:01:44Z",
            "summary": "Creating high-fidelity 3D head avatars has always been a research hotspot,\nbut there remains a great challenge under lightweight sparse view setups. In\nthis paper, we propose Gaussian Head Avatar represented by controllable 3D\nGaussians for high-fidelity head avatar modeling. We optimize the neutral 3D\nGaussians and a fully learned MLP-based deformation field to capture complex\nexpressions. The two parts benefit each other, thereby our method can model\nfine-grained dynamic details while ensuring expression accuracy. Furthermore,\nwe devise a well-designed geometry-guided initialization strategy based on\nimplicit SDF and Deep Marching Tetrahedra for the stability and convergence of\nthe training procedure. Experiments show our approach outperforms other\nstate-of-the-art sparse-view methods, achieving ultra high-fidelity rendering\nquality at 2K resolution even under exaggerated expressions.",
            "author": [
                "Yuelang Xu",
                "Benwang Chen",
                "Zhe Li",
                "Hongwen Zhang",
                "Lizhen Wang",
                "Zerong Zheng",
                "Yebin Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03029v1",
                "http://arxiv.org/pdf/2312.03029v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02661v1",
            "title": "A Self-Commissioning Edge Computing Method for Data-Driven Anomaly\n  Detection in Power Electronic Systems",
            "updated": "2023-12-05T10:56:25Z",
            "published": "2023-12-05T10:56:25Z",
            "summary": "Ensuring the reliability of power electronic converters is a matter of great\nimportance, and data-driven condition monitoring techniques are cementing\nthemselves as an important tool for this purpose. However, translating methods\nthat work well in controlled lab environments to field applications presents\nsignificant challenges, notably because of the limited diversity and accuracy\nof the lab training data. By enabling the use of field data, online machine\nlearning can be a powerful tool to overcome this problem, but it introduces\nadditional challenges in ensuring the stability and predictability of the\ntraining processes. This work presents an edge computing method that mitigates\nthese shortcomings with minimal additional memory usage, by employing an\nautonomous algorithm that prioritizes the storage of training samples with\nlarger prediction errors. The method is demonstrated on the use case of a\nself-commissioning condition monitoring system, in the form of a thermal\nanomaly detection scheme for a variable frequency motor drive, where the\nalgorithm self-learned to distinguish normal and anomalous operation with\nminimal prior knowledge. The obtained results, based on experimental data, show\na significant improvement in prediction accuracy and training speed, when\ncompared to equivalent models trained online without the proposed data\nselection process.",
            "author": [
                "Pere Izquierdo Gomez",
                "Miguel E. Lopez Gajardo",
                "Nenad Mijatovic",
                "Tomislav Dragicevic"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02661v1",
                "http://arxiv.org/pdf/2312.02661v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02660v1",
            "title": "Uniswap Daily Transaction Indices by Network",
            "updated": "2023-12-05T10:53:46Z",
            "published": "2023-12-05T10:53:46Z",
            "summary": "DeFi is transforming financial services by removing intermediaries and\nproducing a wealth of open-source data. This transformation is propelled by\nLayer 2 (L2) solutions, aimed at boosting network efficiency and scalability\nbeyond current Layer 1 (L1) capabilities. This study addresses the lack of\ndetailed L2 impact analysis by examining over 50 million transactions from\nUniswap. Our dataset, featuring transactions from L1 and L2 across networks\nlike Ethereum and Polygon, provides daily indices revealing adoption,\nscalability, and decentralization within the DeFi space. These indices help to\nelucidate the complex relationship between DeFi and L2 technologies, advancing\nour understanding of the ecosystem. The dataset is enhanced by an open-source\nPython framework for computing decentralization indices, adaptable for various\nresearch needs. This positions the dataset as a vital resource for machine\nlearning endeavors, particularly deep learning, contributing significantly to\nthe development of Blockchain as Web3's infrastructure.",
            "author": [
                "Nir Chemaya",
                "Lin William Cong",
                "Emma Jorgensen",
                "Dingyue Liu",
                "Luyao Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02660v1",
                "http://arxiv.org/pdf/2312.02660v1"
            ],
            "primary_category": "econ.GN",
            "category": [
                "econ.GN",
                "cs.CE",
                "cs.CR",
                "cs.CY",
                "q-fin.EC",
                "stat.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03028v1",
            "title": "Double Integral Enhanced Zeroing Neural Network Optimized with ALSOA\n  fostered Lung Cancer Classification using CT Images",
            "updated": "2023-12-05T10:53:35Z",
            "published": "2023-12-05T10:53:35Z",
            "summary": "Lung cancer is one of the deadliest diseases and the leading cause of illness\nand death. Since lung cancer cannot predicted at premature stage, it able to\nonly be discovered more broadly once it has spread to other lung parts. The\nrisk grows when radiologists and other specialists determine whether lung\ncancer is current. Owing to significance of determining type of treatment and\nits depth based on severity of the illness, critical to develop smart and\nautomatic cancer prediction scheme is precise, at which stage of cancer. In\nthis paper, Double Integral Enhanced Zeroing Neural Network Optimized with\nALSOA fostered Lung Cancer Classification using CT Images (LCC-DIEZNN-ALSO-CTI)\nis proposed. Initially, input CT image is amassed from lung cancer dataset. The\ninput CT image is pre-processing via Unscented Trainable Kalman Filtering\n(UTKF) technique. In pre-processing stage unwanted noise are removed from CT\nimages. Afterwards, grayscale statistic features and Haralick texture features\nextracted by Adaptive and Concise Empirical Wavelet Transform (ACEWT). The\nproposed model is implemented on MATLAB. The performance of the proposed method\nis analyzed through existing techniques. The proposed method attains 18.32%,\n27.20%, and 34.32% higher accuracy analyzed with existing method likes Deep\nLearning Assisted Predict of Lung Cancer on Computed Tomography Images\nUtilizing AHHMM (LCC-AHHMM-CT), Convolutional neural networks based pulmonary\nnodule malignancy assessment in pipeline for classifying lung cancer\n(LCC-ICNN-CT), Automated Decision Support Scheme for Lung Cancer Identification\nwith Categorization (LCC-RFCN-MLRPN-CT) methods respectively.",
            "author": [
                "V S Priya Sumitha",
                "V. Keerthika",
                "A. Geetha"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03028v1",
                "http://arxiv.org/pdf/2312.03028v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02659v1",
            "title": "Supervised learning of spatial features with STDP and homeostasis using\n  Spiking Neural Networks on SpiNNaker",
            "updated": "2023-12-05T10:53:31Z",
            "published": "2023-12-05T10:53:31Z",
            "summary": "Artificial Neural Networks (ANN) have gained large popularity thanks to their\nability to learn using the well-known backpropagation algorithm. On the other\nhand, Spiking Neural Networks (SNNs), despite having wider abilities than ANNs,\nhave always presented a challenge in the training phase. This paper shows a new\nmethod to perform supervised learning on SNNs, using Spike Timing Dependent\nPlasticity (STDP) and homeostasis, aiming at training the network to identify\nspatial patterns. The method is tested using the SpiNNaker digital\narchitecture. A SNN is trained to recognise one or multiple patterns and\nperformance metrics are extracted to measure the performance of the network.\nSome considerations are drawn from the results showing that, in the case of a\nsingle trained pattern, the network behaves as the ideal detector, with 100%\naccuracy in detecting the trained pattern. However, as the number of trained\npatterns on a single network increases, the accuracy of the identification is\nlinked to the similarities between these patterns. This method of training an\nSNN to detect spatial patterns may be applied on pattern recognition in static\nimages or traffic analysis in computer networks, where each network packet\nrepresents a spatial pattern. It will be stipulated that the homeostatic factor\nmay enable the network to detect patterns with some degree of similarities,\nrather than only perfectly matching patterns.",
            "author": [
                "Sergio Davies",
                "Andrew Gait",
                "Andrew Rowley",
                "Alessandro Di Nuovo"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02659v1",
                "http://arxiv.org/pdf/2312.02659v1"
            ],
            "primary_category": "cs.NE",
            "category": [
                "cs.NE",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02658v1",
            "title": "Do AI models produce better weather forecasts than physics-based models?\n  A quantitative evaluation case study of Storm Ciar\u00e1n",
            "updated": "2023-12-05T10:52:33Z",
            "published": "2023-12-05T10:52:33Z",
            "summary": "There has been huge recent interest in the potential of making operational\nweather forecasts using machine learning techniques. As they become a part of\nthe weather forecasting toolbox, there is a pressing need to understand how\nwell current machine learning models can simulate high-impactweather events. We\ncompare forecasts of Storm Ciar\\'an, a European windstorm that caused sixteen\ndeaths and extensive damage in Northern Europe, made by machine learning and\nnumericalweather prediction models. The four machine learning models considered\n(FourCastNet, Pangu-Weather, GraphCast and FourCastNet-v2) produce forecasts\nthat accurately capture the synoptic-scale structure of the cyclone including\nthe position of the cloud head, shape of the warm sector and location of warm\nconveyor belt jet, and the large-scale dynamical drivers important for the\nrapid storm development such as the position of the storm relative to the\nupper-level jet exit. However, their ability to resolve the more detailed\nstructures important for issuing weather warnings is more mixed. All of the\nmachine learning models underestimate the peak amplitude of winds associated\nwith the storm, only some machine learning models resolve the warm core\nseclusion and none of the machine learning models capture the sharp bent-back\nwarm frontal gradient. Our study shows there is a great deal about the\nperformance and properties of machine learning weather forecasts that can be\nderived from case studies of high-impact weather events such as Storm Ciar\\'an.",
            "author": [
                "Andrew J. Charlton-Perez",
                "Helen F. Dacre",
                "Simon Driscoll",
                "Suzanne L. Gray",
                "Ben Harvey",
                "Natalie J. Harvey",
                "Kieran M. R. Hunt",
                "Robert W. Lee",
                "Ranjini Swaminathan",
                "Remy Vandaele",
                "Ambrogio Volont\u00e9"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02658v1",
                "http://arxiv.org/pdf/2312.02658v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "physics.ao-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02652v1",
            "title": "What Machine Learning Can Do for Focusing Aerogel Detectors",
            "updated": "2023-12-05T10:46:16Z",
            "published": "2023-12-05T10:46:16Z",
            "summary": "Particle identification at the Super Charm-Tau factory experiment will be\nprovided by a Focusing Aerogel Ring Imaging CHerenkov detector (FARICH). The\nspecifics of detector location make proper cooling difficult, therefore a\nsignificant number of ambient background hits are captured. They must be\nmitigated to reduce the data flow and improve particle velocity resolution. In\nthis work we present several approaches to filtering signal hits, inspired by\nmachine learning techniques from computer vision.",
            "author": [
                "Foma Shipilov",
                "Alexander Barnyakov",
                "Vladimir Bobrovnikov",
                "Sergey Kononov",
                "Fedor Ratnikov"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02652v1",
                "http://arxiv.org/pdf/2312.02652v1"
            ],
            "primary_category": "hep-ex",
            "category": [
                "hep-ex",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02650v1",
            "title": "Learning convex objectives to reduce the complexity of model predictive\n  control",
            "updated": "2023-12-05T10:42:10Z",
            "published": "2023-12-05T10:42:10Z",
            "summary": "For large systems that consider uncertainty the online solution of model\npredictive control problems can be computationally taxing, and even infeasible.\nThis can be offset by using a shorter horizon, however this can in turn result\nin poor controller performance. In this work we consider the task of learning a\nconvex cost-to-go to allow the use of a short, potentially single step, control\nhorizon to reduce the online computational cost. We consider two surrogates:\n(1) a convex interpolating function and (2) an input-convex neural network. We\nhighlight that irrespective of the choice of surrogate the behaviour of the\nsurrogate near the origin and ability of the surrogate to describe the feasible\nregion are key concerns for the closed loop performance of the new MPC problem.\nWe address these concerns by tailoring the design of the surrogate to ensure\ngood performance in both aspects. The paper concludes with a numerical example,\nshowing the clear and significant reduction in computational complexity through\nthe use of these convex surrogates.",
            "author": [
                "E. M. Turan",
                "Z. Mdoe",
                "J. J\u00e4schke"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02650v1",
                "http://arxiv.org/pdf/2312.02650v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02649v1",
            "title": "A Q-learning approach to the continuous control problem of robot\n  inverted pendulum balancing",
            "updated": "2023-12-05T10:40:48Z",
            "published": "2023-12-05T10:40:48Z",
            "summary": "This study evaluates the application of a discrete action space reinforcement\nlearning method (Q-learning) to the continuous control problem of robot\ninverted pendulum balancing. To speed up the learning process and to overcome\ntechnical difficulties related to the direct learning on the real robotic\nsystem, the learning phase is performed in simulation environment. A\nmathematical model of the system dynamics is implemented, deduced by curve\nfitting on data acquired from the real system. The proposed approach\ndemonstrated feasible, featuring its application on a real world robot that\nlearned to balance an inverted pendulum. This study also reinforces and\ndemonstrates the importance of an accurate representation of the physical world\nin simulation to achieve a more efficient implementation of reinforcement\nlearning algorithms in real world, even when using a discrete action space\nalgorithm to control a continuous action.",
            "author": [
                "Mohammad Safeea",
                "Pedro Neto"
            ],
            "link": [
                "http://dx.doi.org/10.1016/j.iswa.2023.200313",
                "http://arxiv.org/abs/2312.02649v1",
                "http://arxiv.org/pdf/2312.02649v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02647v1",
            "title": "TPA3D: Triplane Attention for Fast Text-to-3D Generation",
            "updated": "2023-12-05T10:39:37Z",
            "published": "2023-12-05T10:39:37Z",
            "summary": "Due to the lack of large-scale text-3D correspondence data, recent text-to-3D\ngeneration works mainly rely on utilizing 2D diffusion models for synthesizing\n3D data. Since diffusion-based methods typically require significant\noptimization time for both training and inference, the use of GAN-based models\nwould still be desirable for fast 3D generation. In this work, we propose\nTriplane Attention for text-guided 3D generation (TPA3D), an end-to-end\ntrainable GAN-based deep learning model for fast text-to-3D generation. With\nonly 3D shape data and their rendered 2D images observed during training, our\nTPA3D is designed to retrieve detailed visual descriptions for synthesizing the\ncorresponding 3D mesh data. This is achieved by the proposed attention\nmechanisms on the extracted sentence and word-level text features. In our\nexperiments, we show that TPA3D generates high-quality 3D textured shapes\naligned with fine-grained descriptions, while impressive computation efficiency\ncan be observed.",
            "author": [
                "Hong-En Chen",
                "Bin-Shih Wu",
                "Sheng-Yu Huang",
                "Yu-Chiang Frank Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02647v1",
                "http://arxiv.org/pdf/2312.02647v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02646v1",
            "title": "SAMSGL: Series-Aligned Multi-Scale Graph Learning for Spatio-Temporal\n  Forecasting",
            "updated": "2023-12-05T10:37:54Z",
            "published": "2023-12-05T10:37:54Z",
            "summary": "Spatio-temporal forecasting in various domains, like traffic prediction and\nweather forecasting, is a challenging endeavor, primarily due to the\ndifficulties in modeling propagation dynamics and capturing high-dimensional\ninteractions among nodes. Despite the significant strides made by graph-based\nnetworks in spatio-temporal forecasting, there remain two pivotal factors\nclosely related to forecasting performance that need further consideration:\ntime delays in propagation dynamics and multi-scale high-dimensional\ninteractions. In this work, we present a Series-Aligned Multi-Scale Graph\nLearning (SAMSGL) framework, aiming to enhance forecasting performance. In\norder to handle time delays in spatial interactions, we propose a\nseries-aligned graph convolution layer to facilitate the aggregation of\nnon-delayed graph signals, thereby mitigating the influence of time delays for\nthe improvement in accuracy. To understand global and local spatio-temporal\ninteractions, we develop a spatio-temporal architecture via multi-scale graph\nlearning, which encompasses two essential components: multi-scale graph\nstructure learning and graph-fully connected (Graph-FC) blocks. The multi-scale\ngraph structure learning includes a global graph structure to learn both\ndelayed and non-delayed node embeddings, as well as a local one to learn node\nvariations influenced by neighboring factors. The Graph-FC blocks\nsynergistically fuse spatial and temporal information to boost prediction\naccuracy. To evaluate the performance of SAMSGL, we conduct experiments on\nmeteorological and traffic forecasting datasets, which demonstrate its\neffectiveness and superiority.",
            "author": [
                "Xiaobei Zou",
                "Luolin Xiong",
                "Yang Tang",
                "Jurgen Kurths"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02646v1",
                "http://arxiv.org/pdf/2312.02646v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02622v1",
            "title": "On the Initialization of Graph Neural Networks",
            "updated": "2023-12-05T09:55:49Z",
            "published": "2023-12-05T09:55:49Z",
            "summary": "Graph Neural Networks (GNNs) have displayed considerable promise in graph\nrepresentation learning across various applications. The core learning process\nrequires the initialization of model weight matrices within each GNN layer,\nwhich is typically accomplished via classic initialization methods such as\nXavier initialization. However, these methods were originally motivated to\nstabilize the variance of hidden embeddings and gradients across layers of\nFeedforward Neural Networks (FNNs) and Convolutional Neural Networks (CNNs) to\navoid vanishing gradients and maintain steady information flow. In contrast,\nwithin the GNN context classical initializations disregard the impact of the\ninput graph structure and message passing on variance. In this paper, we\nanalyze the variance of forward and backward propagation across GNN layers and\nshow that the variance instability of GNN initializations comes from the\ncombined effect of the activation function, hidden dimension, graph structure\nand message passing. To better account for these influence factors, we propose\na new initialization method for Variance Instability Reduction within GNN\nOptimization (Virgo), which naturally tends to equate forward and backward\nvariances across successive layers. We conduct comprehensive experiments on 15\ndatasets to show that Virgo can lead to superior model performance and more\nstable variance at initialization on node classification, link prediction and\ngraph classification tasks. Codes are in\nhttps://github.com/LspongebobJH/virgo_icml2023.",
            "author": [
                "Jiahang Li",
                "Yakun Song",
                "Xiang Song",
                "David Paul Wipf"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02622v1",
                "http://arxiv.org/pdf/2312.02622v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02619v1",
            "title": "Rethinking and Simplifying Bootstrapped Graph Latents",
            "updated": "2023-12-05T09:49:50Z",
            "published": "2023-12-05T09:49:50Z",
            "summary": "Graph contrastive learning (GCL) has emerged as a representative paradigm in\ngraph self-supervised learning, where negative samples are commonly regarded as\nthe key to preventing model collapse and producing distinguishable\nrepresentations. Recent studies have shown that GCL without negative samples\ncan achieve state-of-the-art performance as well as scalability improvement,\nwith bootstrapped graph latent (BGRL) as a prominent step forward. However,\nBGRL relies on a complex architecture to maintain the ability to scatter\nrepresentations, and the underlying mechanisms enabling the success remain\nlargely unexplored. In this paper, we introduce an instance-level decorrelation\nperspective to tackle the aforementioned issue and leverage it as a springboard\nto reveal the potential unnecessary model complexity within BGRL. Based on our\nfindings, we present SGCL, a simple yet effective GCL framework that utilizes\nthe outputs from two consecutive iterations as positive pairs, eliminating the\nnegative samples. SGCL only requires a single graph augmentation and a single\ngraph encoder without additional parameters. Extensive experiments conducted on\nvarious graph benchmarks demonstrate that SGCL can achieve competitive\nperformance with fewer parameters, lower time and space costs, and significant\nconvergence speedup.",
            "author": [
                "Wangbin Sun",
                "Jintang Li",
                "Liang Chen",
                "Bingzhe Wu",
                "Yatao Bian",
                "Zibin Zheng"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02619v1",
                "http://arxiv.org/pdf/2312.02619v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02617v1",
            "title": "DreaMo: Articulated 3D Reconstruction From A Single Casual Video",
            "updated": "2023-12-05T09:47:37Z",
            "published": "2023-12-05T09:47:37Z",
            "summary": "Articulated 3D reconstruction has valuable applications in various domains,\nyet it remains costly and demands intensive work from domain experts. Recent\nadvancements in template-free learning methods show promising results with\nmonocular videos. Nevertheless, these approaches necessitate a comprehensive\ncoverage of all viewpoints of the subject in the input video, thus limiting\ntheir applicability to casually captured videos from online sources. In this\nwork, we study articulated 3D shape reconstruction from a single and casually\ncaptured internet video, where the subject's view coverage is incomplete. We\npropose DreaMo that jointly performs shape reconstruction while solving the\nchallenging low-coverage regions with view-conditioned diffusion prior and\nseveral tailored regularizations. In addition, we introduce a skeleton\ngeneration strategy to create human-interpretable skeletons from the learned\nneural bones and skinning weights. We conduct our study on a self-collected\ninternet video collection characterized by incomplete view coverage. DreaMo\nshows promising quality in novel-view rendering, detailed articulated shape\nreconstruction, and skeleton generation. Extensive qualitative and quantitative\nstudies validate the efficacy of each proposed component, and show existing\nmethods are unable to solve correct geometry due to the incomplete view\ncoverage.",
            "author": [
                "Tao Tu",
                "Ming-Feng Li",
                "Chieh Hubert Lin",
                "Yen-Chi Cheng",
                "Min Sun",
                "Ming-Hsuan Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02617v1",
                "http://arxiv.org/pdf/2312.02617v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02615v1",
            "title": "Projection Regret: Reducing Background Bias for Novelty Detection via\n  Diffusion Models",
            "updated": "2023-12-05T09:44:47Z",
            "published": "2023-12-05T09:44:47Z",
            "summary": "Novelty detection is a fundamental task of machine learning which aims to\ndetect abnormal ($\\textit{i.e.}$ out-of-distribution (OOD)) samples. Since\ndiffusion models have recently emerged as the de facto standard generative\nframework with surprising generation results, novelty detection via diffusion\nmodels has also gained much attention. Recent methods have mainly utilized the\nreconstruction property of in-distribution samples. However, they often suffer\nfrom detecting OOD samples that share similar background information to the\nin-distribution data. Based on our observation that diffusion models can\n\\emph{project} any sample to an in-distribution sample with similar background\ninformation, we propose \\emph{Projection Regret (PR)}, an efficient novelty\ndetection method that mitigates the bias of non-semantic information. To be\nspecific, PR computes the perceptual distance between the test image and its\ndiffusion-based projection to detect abnormality. Since the perceptual distance\noften fails to capture semantic changes when the background information is\ndominant, we cancel out the background bias by comparing it against recursive\nprojections. Extensive experiments demonstrate that PR outperforms the prior\nart of generative-model-based novelty detection methods by a significant\nmargin.",
            "author": [
                "Sungik Choi",
                "Hankook Lee",
                "Honglak Lee",
                "Moontae Lee"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02615v1",
                "http://arxiv.org/pdf/2312.02615v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02614v1",
            "title": "Prompt Optimization via Adversarial In-Context Learning",
            "updated": "2023-12-05T09:44:45Z",
            "published": "2023-12-05T09:44:45Z",
            "summary": "We propose a new method, Adversarial In-Context Learning (adv-ICL), to\noptimize prompt for in-context learning (ICL) by employing one LLM as a\ngenerator, another as a discriminator, and a third as a prompt modifier. As in\ntraditional adversarial learning, adv-ICL is implemented as a two-player game\nbetween the generator and discriminator, where the generator tries to generate\nrealistic enough output to fool the discriminator. In each round, given an\ninput prefixed by task instructions and several exemplars, the generator\nproduces an output. The discriminator is then tasked with classifying the\ngenerator input-output pair as model-generated or real data. Based on the\ndiscriminator loss, the prompt modifier proposes possible edits to the\ngenerator and discriminator prompts, and the edits that most improve the\nadversarial loss are selected. We show that adv-ICL results in significant\nimprovements over state-of-the-art prompt optimization techniques for both open\nand closed-source models on 11 generation and classification tasks including\nsummarization, arithmetic reasoning, machine translation, data-to-text\ngeneration, and the MMLU and big-bench hard benchmarks. In addition, because\nour method uses pre-trained models and updates only prompts rather than model\nparameters, it is computationally efficient, easy to extend to any LLM and\ntask, and effective in low-resource settings.",
            "author": [
                "Xuan Long Do",
                "Yiran Zhao",
                "Hannah Brown",
                "Yuxi Xie",
                "James Xu Zhao",
                "Nancy F. Chen",
                "Kenji Kawaguchi",
                "Michael Qizhe Xie",
                "Junxian He"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02614v1",
                "http://arxiv.org/pdf/2312.02614v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02613v1",
            "title": "A Unified Simulation Framework for Visual and Behavioral Fidelity in\n  Crowd Analysis",
            "updated": "2023-12-05T09:43:27Z",
            "published": "2023-12-05T09:43:27Z",
            "summary": "Simulation is a powerful tool to easily generate annotated data, and a highly\ndesirable feature, especially in those domains where learning models need large\ntraining datasets. Machine learning and deep learning solutions, have proven to\nbe extremely data-hungry and sometimes, the available real-world data are not\nsufficient to effectively model the given task. Despite the initial skepticism\nof a portion of the scientific community, the potential of simulation has been\nlargely confirmed in many application areas, and the recent developments in\nterms of rendering and virtualization engines, have shown a good ability also\nin representing complex scenes. This includes environmental factors, such as\nweather conditions and surface reflectance, as well as human-related events,\nlike human actions and behaviors. We present a human crowd simulator, called\nUniCrowd, and its associated validation pipeline. We show how the simulator can\ngenerate annotated data, suitable for computer vision tasks, in particular for\ndetection and segmentation, as well as the related applications, as crowd\ncounting, human pose estimation, trajectory analysis and prediction, and\nanomaly detection.",
            "author": [
                "Niccol\u00f2 Bisagno",
                "Nicola Garau",
                "Antonio Luigi Stefani",
                "Nicola Conci"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02613v1",
                "http://arxiv.org/pdf/2312.02613v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.MA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02611v1",
            "title": "Privacy-Aware Data Acquisition under Data Similarity in Regression\n  Markets",
            "updated": "2023-12-05T09:39:04Z",
            "published": "2023-12-05T09:39:04Z",
            "summary": "Data markets facilitate decentralized data exchange for applications such as\nprediction, learning, or inference. The design of these markets is challenged\nby varying privacy preferences as well as data similarity among data owners.\nRelated works have often overlooked how data similarity impacts pricing and\ndata value through statistical information leakage. We demonstrate that data\nsimilarity and privacy preferences are integral to market design and propose a\nquery-response protocol using local differential privacy for a two-party data\nacquisition mechanism. In our regression data market model, we analyze\nstrategic interactions between privacy-aware owners and the learner as a\nStackelberg game over the asked price and privacy factor. Finally, we\nnumerically evaluate how data similarity affects market participation and\ntraded data value.",
            "author": [
                "Shashi Raj Pandey",
                "Pierre Pinson",
                "Petar Popovski"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02611v1",
                "http://arxiv.org/pdf/2312.02611v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CR",
                "cs.GT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02608v1",
            "title": "Panoptica -- instance-wise evaluation of 3D semantic and instance\n  segmentation maps",
            "updated": "2023-12-05T09:34:56Z",
            "published": "2023-12-05T09:34:56Z",
            "summary": "This paper introduces panoptica, a versatile and performance-optimized\npackage designed for computing instance-wise segmentation quality metrics from\n2D and 3D segmentation maps. panoptica addresses the limitations of existing\nmetrics and provides a modular framework that complements the original\nintersection over union-based panoptic quality with other metrics, such as the\ndistance metric Average Symmetric Surface Distance. The package is open-source,\nimplemented in Python, and accompanied by comprehensive documentation and\ntutorials. panoptica employs a three-step metrics computation process to cover\ndiverse use cases. The efficacy of panoptica is demonstrated on various\nreal-world biomedical datasets, where an instance-wise evaluation is\ninstrumental for an accurate representation of the underlying clinical task.\nOverall, we envision panoptica as a valuable tool facilitating in-depth\nevaluation of segmentation methods.",
            "author": [
                "Florian Kofler",
                "Hendrik M\u00f6ller",
                "Josef A. Buchner",
                "Ezequiel de la Rosa",
                "Ivan Ezhov",
                "Marcel Rosier",
                "Isra Mekki",
                "Suprosanna Shit",
                "Moritz Negwer",
                "Rami Al-Maskari",
                "Ali Ert\u00fcrk",
                "Shankeeth Vinayahalingam",
                "Fabian Isensee",
                "Sarthak Pati",
                "Daniel Rueckert",
                "Jan S. Kirschke",
                "Stefan K. Ehrlich",
                "Annika Reinke",
                "Bjoern Menze",
                "Benedikt Wiestler",
                "Marie Piraud"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02608v1",
                "http://arxiv.org/pdf/2312.02608v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02605v1",
            "title": "Accelerating Learnt Video Codecs with Gradient Decay and Layer-wise\n  Distillation",
            "updated": "2023-12-05T09:26:09Z",
            "published": "2023-12-05T09:26:09Z",
            "summary": "In recent years, end-to-end learnt video codecs have demonstrated their\npotential to compete with conventional coding algorithms in term of compression\nefficiency. However, most learning-based video compression models are\nassociated with high computational complexity and latency, in particular at the\ndecoder side, which limits their deployment in practical applications. In this\npaper, we present a novel model-agnostic pruning scheme based on gradient decay\nand adaptive layer-wise distillation. Gradient decay enhances parameter\nexploration during sparsification whilst preventing runaway sparsity and is\nsuperior to the standard Straight-Through Estimation. The adaptive layer-wise\ndistillation regulates the sparse training in various stages based on the\ndistortion of intermediate features. This stage-wise design efficiently updates\nparameters with minimal computational overhead. The proposed approach has been\napplied to three popular end-to-end learnt video codecs, FVC, DCVC, and\nDCVC-HEM. Results confirm that our method yields up to 65% reduction in MACs\nand 2x speed-up with less than 0.3dB drop in BD-PSNR. Supporting code and\nsupplementary material can be downloaded from:\nhttps://jasminepp.github.io/lightweightdvc/",
            "author": [
                "Tianhao Peng",
                "Ge Gao",
                "Heming Sun",
                "Fan Zhang",
                "David Bull"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02605v1",
                "http://arxiv.org/pdf/2312.02605v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02603v1",
            "title": "Automatic Robot Path Planning for Visual Inspection from Object Shape",
            "updated": "2023-12-05T09:21:34Z",
            "published": "2023-12-05T09:21:34Z",
            "summary": "Visual inspection is a crucial yet time-consuming task across various\nindustries. Numerous established methods employ machine learning in inspection\ntasks, necessitating specific training data that includes predefined inspection\nposes and training images essential for the training of models. The acquisition\nof such data and their integration into an inspection framework is challenging\ndue to the variety in objects and scenes involved and due to additional\nbottlenecks caused by the manual collection of training data by humans, thereby\nhindering the automation of visual inspection across diverse domains. This work\nproposes a solution for automatic path planning using a single depth camera\nmounted on a robot manipulator. Point clouds obtained from the depth images are\nprocessed and filtered to extract object profiles and transformed to inspection\ntarget paths for the robot end-effector. The approach relies on the geometry of\nthe object and generates an inspection path that follows the shape normal to\nthe surface. Depending on the object size and shape, inspection paths can be\ndefined as single or multi-path plans. Results are demonstrated in both\nsimulated and real-world environments, yielding promising inspection paths for\nobjects with varying sizes and shapes. Code and video are open-source available\nat: https://github.com/CuriousLad1000/Auto-Path-Planner",
            "author": [
                "O. Tasneem",
                "R. Pieters"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02603v1",
                "http://arxiv.org/pdf/2312.02603v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02596v1",
            "title": "TSVR+: Twin support vector regression with privileged information",
            "updated": "2023-12-05T09:15:10Z",
            "published": "2023-12-05T09:15:10Z",
            "summary": "In the realm of machine learning, the data may contain additional attributes,\nknown as privileged information (PI). The main purpose of PI is to assist in\nthe training of the model and then utilize the acquired knowledge to make\npredictions for unseen samples. Support vector regression (SVR) is an effective\nregression model, however, it has a low learning speed due to solving a convex\nquadratic problem (QP) subject to a pair of constraints. In contrast, twin\nsupport vector regression (TSVR) is more efficient than SVR as it solves two\nQPs each subject to one set of constraints. However, TSVR and its variants are\ntrained only on regular features and do not use privileged features for\ntraining. To fill this gap, we introduce a fusion of TSVR with learning using\nprivileged information (LUPI) and propose a novel approach called twin support\nvector regression with privileged information (TSVR+). The regularization terms\nin the proposed TSVR+ capture the essence of statistical learning theory and\nimplement the structural risk minimization principle. We use the successive\noverrelaxation (SOR) technique to solve the optimization problem of the\nproposed TSVR+, which enhances the training efficiency. As far as our knowledge\nextends, the integration of the LUPI concept into twin variants of regression\nmodels is a novel advancement. The numerical experiments conducted on UCI,\nstock and time series data collectively demonstrate the superiority of the\nproposed model.",
            "author": [
                "Anuradha Kumari",
                "M. Tanveer"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02596v1",
                "http://arxiv.org/pdf/2312.02596v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02592v1",
            "title": "FRAPP\u00c9: A Post-Processing Framework for Group Fairness Regularization",
            "updated": "2023-12-05T09:09:21Z",
            "published": "2023-12-05T09:09:21Z",
            "summary": "Post-processing mitigation techniques for group fairness generally adjust the\ndecision threshold of a base model in order to improve fairness. Methods in\nthis family exhibit several advantages that make them appealing in practice:\npost-processing requires no access to the model training pipeline, is agnostic\nto the base model architecture, and offers a reduced computation cost compared\nto in-processing. Despite these benefits, existing methods face other\nchallenges that limit their applicability: they require knowledge of the\nsensitive attributes at inference time and are oftentimes outperformed by\nin-processing. In this paper, we propose a general framework to transform any\nin-processing method with a penalized objective into a post-processing\nprocedure. The resulting method is specifically designed to overcome the\naforementioned shortcomings of prior post-processing approaches. Furthermore,\nwe show theoretically and through extensive experiments on real-world data that\nthe resulting post-processing method matches or even surpasses the\nfairness-error trade-off offered by the in-processing counterpart.",
            "author": [
                "Alexandru \u0162ifrea",
                "Preethi Lahoti",
                "Ben Packer",
                "Yoni Halpern",
                "Ahmad Beirami",
                "Flavien Prost"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02592v1",
                "http://arxiv.org/pdf/2312.02592v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02586v1",
            "title": "Mapping the Information Journey: Unveiling the Documentation Experience\n  of Software Developers in China",
            "updated": "2023-12-05T08:57:32Z",
            "published": "2023-12-05T08:57:32Z",
            "summary": "This research delves into understanding the behaviors and characteristics of\nChinese developers in relation to their use of technical documentation, which\nis crucial for creating high-quality developer documentation. We conducted\ninterviews with 25 software developers and surveyed 177 participants, using the\npreliminary interview findings to inform the survey design. Our approach\nencompassed traditional user research methods, including persona and user\njourney mapping, to develop typical personas and information journeys based on\nthe qualitative data from the interviews and quantitative results from the\nsurvey. Our results revealed distinct characteristics and differences between\njunior and senior developers in terms of their use of technical documentation,\nbroadly categorized into personality traits, learning habits, and working\nhabits. We observed that the information journey of both groups typically\nencompasses four stages: Exploration, Understanding, Practice, and Application.\nConsequently, we created two distinct personas and information journey maps to\nrepresent these two developer groups. Our findings highlight that developers\nprioritize the content, organization, and maintenance aspects of documentation.\nIn conclusion, we recommend organizing documentation content to align with\ndevelopers' information journeys, tailoring documentation to meet the needs of\ndevelopers at various levels, and focusing on the content, organization, and\nmaintenance aspects of documentation.",
            "author": [
                "Zhijun Gao",
                "Jiangying Wang",
                "Meina Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02586v1",
                "http://arxiv.org/pdf/2312.02586v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC",
                "cs.IT",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02573v1",
            "title": "UTBoost: A Tree-boosting based System for Uplift Modeling",
            "updated": "2023-12-05T08:41:23Z",
            "published": "2023-12-05T08:41:23Z",
            "summary": "Uplift modeling refers to the set of machine learning techniques that a\nmanager may use to estimate customer uplift, that is, the net effect of an\naction on some customer outcome. By identifying the subset of customers for\nwhom a treatment will have the greatest effect, uplift models assist\ndecision-makers in optimizing resource allocations and maximizing overall\nreturns. Accurately estimating customer uplift poses practical challenges, as\nit requires assessing the difference between two mutually exclusive outcomes\nfor each individual. In this paper, we propose two innovative adaptations of\nthe well-established Gradient Boosting Decision Trees (GBDT) algorithm, which\nlearn the causal effect in a sequential way and overcome the counter-factual\nnature. Both approaches innovate existing techniques in terms of ensemble\nlearning method and learning objectives, respectively. Experiments on\nlarge-scale datasets demonstrate the usefulness of the proposed methods, which\noften yielding remarkable improvements over base models. To facilitate the\napplication, we develop the UTBoost, an end-to-end tree boosting system\nspecifically designed for uplift modeling. The package is open source and has\nbeen optimized for training speed to meet the needs of real industrial\napplications.",
            "author": [
                "Junjie Gao",
                "Xiangyu Zheng",
                "DongDong Wang",
                "Zhixiang Huang",
                "Bangqi Zheng",
                "Kai Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02573v1",
                "http://arxiv.org/pdf/2312.02573v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02567v1",
            "title": "Think Twice Before Selection: Federated Evidential Active Learning for\n  Medical Image Analysis with Domain Shifts",
            "updated": "2023-12-05T08:32:27Z",
            "published": "2023-12-05T08:32:27Z",
            "summary": "Federated learning facilitates the collaborative learning of a global model\nacross multiple distributed medical institutions without centralizing data.\nNevertheless, the expensive cost of annotation on local clients remains an\nobstacle to effectively utilizing local data. To mitigate this issue, federated\nactive learning methods suggest leveraging local and global model predictions\nto select a relatively small amount of informative local data for annotation.\nHowever, existing methods mainly focus on all local data sampled from the same\ndomain, making them unreliable in realistic medical scenarios with domain\nshifts among different clients. In this paper, we make the first attempt to\nassess the informativeness of local data derived from diverse domains and\npropose a novel methodology termed Federated Evidential Active Learning (FEAL)\nto calibrate the data evaluation under domain shift. Specifically, we introduce\na Dirichlet prior distribution in both local and global models to treat the\nprediction as a distribution over the probability simplex and capture both\naleatoric and epistemic uncertainties by using the Dirichlet-based evidential\nmodel. Then we employ the epistemic uncertainty to calibrate the aleatoric\nuncertainty. Afterward, we design a diversity relaxation strategy to reduce\ndata redundancy and maintain data diversity. Extensive experiments and analyses\nare conducted to show the superiority of FEAL over the state-of-the-art active\nlearning methods and the efficiency of FEAL under the federated active learning\nframework.",
            "author": [
                "Jiayi Chen",
                "Benteng Ma",
                "Hengfei Cui",
                "Yong Xia",
                "Kwang-Ting Cheng"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02567v1",
                "http://arxiv.org/pdf/2312.02567v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03026v1",
            "title": "Uni3DL: Unified Model for 3D and Language Understanding",
            "updated": "2023-12-05T08:30:27Z",
            "published": "2023-12-05T08:30:27Z",
            "summary": "In this work, we present Uni3DL, a unified model for 3D and Language\nunderstanding. Distinct from existing unified vision-language models in 3D\nwhich are limited in task variety and predominantly dependent on projected\nmulti-view images, Uni3DL operates directly on point clouds. This approach\nsignificantly expands the range of supported tasks in 3D, encompassing both\nvision and vision-language tasks in 3D. At the core of Uni3DL, a query\ntransformer is designed to learn task-agnostic semantic and mask outputs by\nattending to 3D visual features, and a task router is employed to selectively\ngenerate task-specific outputs required for diverse tasks. With a unified\narchitecture, our Uni3DL model enjoys seamless task decomposition and\nsubstantial parameter sharing across tasks. Uni3DL has been rigorously\nevaluated across diverse 3D vision-language understanding tasks, including\nsemantic segmentation, object detection, instance segmentation, visual\ngrounding, 3D captioning, and text-3D cross-modal retrieval. It demonstrates\nperformance on par with or surpassing state-of-the-art (SOTA) task-specific\nmodels. We hope our benchmark and Uni3DL model will serve as a solid step to\nease future research in unified models in the realm of 3D and language\nunderstanding. Project page: https://uni3dl.github.io.",
            "author": [
                "Xiang Li",
                "Jian Ding",
                "Zhaoyang Chen",
                "Mohamed Elhoseiny"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03026v1",
                "http://arxiv.org/pdf/2312.03026v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02566v1",
            "title": "Structured World Representations in Maze-Solving Transformers",
            "updated": "2023-12-05T08:24:26Z",
            "published": "2023-12-05T08:24:26Z",
            "summary": "Transformer models underpin many recent advances in practical machine\nlearning applications, yet understanding their internal behavior continues to\nelude researchers. Given the size and complexity of these models, forming a\ncomprehensive picture of their inner workings remains a significant challenge.\nTo this end, we set out to understand small transformer models in a more\ntractable setting: that of solving mazes. In this work, we focus on the\nabstractions formed by these models and find evidence for the consistent\nemergence of structured internal representations of maze topology and valid\npaths. We demonstrate this by showing that the residual stream of only a single\ntoken can be linearly decoded to faithfully reconstruct the entire maze. We\nalso find that the learned embeddings of individual tokens have spatial\nstructure. Furthermore, we take steps towards deciphering the circuity of\npath-following by identifying attention heads (dubbed $\\textit{adjacency\nheads}$), which are implicated in finding valid subsequent tokens.",
            "author": [
                "Michael Igorevich Ivanitskiy",
                "Alex F. Spies",
                "Tilman R\u00e4uker",
                "Guillaume Corlouer",
                "Chris Mathwin",
                "Lucia Quirke",
                "Can Rager",
                "Rusheb Shah",
                "Dan Valentine",
                "Cecilia Diniz Behn",
                "Katsumi Inoue",
                "Samy Wu Fung"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02566v1",
                "http://arxiv.org/pdf/2312.02566v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03025v1",
            "title": "Training on Synthetic Data Beats Real Data in Multimodal Relation\n  Extraction",
            "updated": "2023-12-05T08:11:34Z",
            "published": "2023-12-05T08:11:34Z",
            "summary": "The task of multimodal relation extraction has attracted significant research\nattention, but progress is constrained by the scarcity of available training\ndata. One natural thought is to extend existing datasets with cross-modal\ngenerative models. In this paper, we consider a novel problem setting, where\nonly unimodal data, either text or image, are available during training. We aim\nto train a multimodal classifier from synthetic data that perform well on real\nmultimodal test data. However, training with synthetic data suffers from two\nobstacles: lack of data diversity and label information loss. To alleviate the\nissues, we propose Mutual Information-aware Multimodal Iterated Relational dAta\nGEneration (MI2RAGE), which applies Chained Cross-modal Generation (CCG) to\npromote diversity in the generated data and exploits a teacher network to\nselect valuable training samples with high mutual information with the\nground-truth labels. Comparing our method to direct training on synthetic data,\nwe observed a significant improvement of 24.06% F1 with synthetic text and\n26.42% F1 with synthetic images. Notably, our best model trained on completely\nsynthetic images outperforms prior state-of-the-art models trained on real\nmultimodal data by a margin of 3.76% in F1. Our codebase will be made available\nupon acceptance.",
            "author": [
                "Zilin Du",
                "Haoxin Li",
                "Xu Guo",
                "Boyang Li"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03025v1",
                "http://arxiv.org/pdf/2312.03025v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02561v1",
            "title": "DanZero+: Dominating the GuanDan Game through Reinforcement Learning",
            "updated": "2023-12-05T08:07:32Z",
            "published": "2023-12-05T08:07:32Z",
            "summary": "The utilization of artificial intelligence (AI) in card games has been a\nwell-explored subject within AI research for an extensive period. Recent\nadvancements have propelled AI programs to showcase expertise in intricate card\ngames such as Mahjong, DouDizhu, and Texas Hold'em. In this work, we aim to\ndevelop an AI program for an exceptionally complex and popular card game called\nGuanDan. This game involves four players engaging in both competitive and\ncooperative play throughout a long process to upgrade their level, posing great\nchallenges for AI due to its expansive state and action space, long episode\nlength, and complex rules. Employing reinforcement learning techniques,\nspecifically Deep Monte Carlo (DMC), and a distributed training framework, we\nfirst put forward an AI program named DanZero for this game. Evaluation against\nbaseline AI programs based on heuristic rules highlights the outstanding\nperformance of our bot. Besides, in order to further enhance the AI's\ncapabilities, we apply policy-based reinforcement learning algorithm to\nGuanDan. To address the challenges arising from the huge action space, which\nwill significantly impact the performance of policy-based algorithms, we adopt\nthe pre-trained model to facilitate the training process and the achieved AI\nprogram manages to achieve a superior performance.",
            "author": [
                "Youpeng Zhao",
                "Yudong Lu",
                "Jian Zhao",
                "Wengang Zhou",
                "Houqiang Li"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02561v1",
                "http://arxiv.org/pdf/2312.02561v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02554v1",
            "title": "ULMA: Unified Language Model Alignment with Demonstration and Point-wise\n  Human Preference",
            "updated": "2023-12-05T07:52:12Z",
            "published": "2023-12-05T07:52:12Z",
            "summary": "Language model alignment is a cutting-edge technique in large language model\ntraining to align the model output to user's intent, e.g., being helpful and\nharmless. Recent alignment framework consists of two steps: supervised\nfine-tuning with demonstration data and preference learning with human\npreference data. Previous preference learning methods, such as RLHF and DPO,\nmainly focus on pair-wise preference data. However, in many real-world\nscenarios where human feedbacks are intrinsically point-wise, these methods\nwill suffer from information loss or even fail. To fill this gap, in this\npaper, we first develop a preference learning method called point-wise DPO to\ntackle point-wise preference data. Further revelation on the connection between\nsupervised fine-tuning and point-wise preference learning enables us to develop\na unified framework for both human demonstration and point-wise preference\ndata, which sheds new light on the construction of preference dataset.\nExtensive experiments on point-wise datasets with binary or continuous labels\ndemonstrate the superior performance and efficiency of our proposed methods. A\nnew dataset with high-quality demonstration samples on harmlessness is\nconstructed and made publicly available.",
            "author": [
                "Tianchi Cai",
                "Xierui Song",
                "Jiyan Jiang",
                "Fei Teng",
                "Jinjie Gu",
                "Guannan Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02554v1",
                "http://arxiv.org/pdf/2312.02554v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03023v1",
            "title": "A study of topological quantities of lattice QCD by a modified DCGAN\n  frame",
            "updated": "2023-12-05T07:44:21Z",
            "published": "2023-12-05T07:44:21Z",
            "summary": "A modified deep convolutional generative adversarial network (M-DCGAN) frame\nis proposed to study the N-dimensional (ND) topological quantities in lattice\nQCD based on the Monte Carlo (MC) simulations. We construct a new scaling\nstructure including fully connected layers to support the generation of\nhigh-quality high-dimensional images for the M-DCGAN. Our results show that the\nM-DCGAN scheme of the Machine learning should be helpful for us to calculate\nefficiently the 1D distribution of topological charge and the 4D topological\ncharge density compared with the case by the MC simulation alone.",
            "author": [
                "Lin Gao",
                "Heping Ying",
                "Jianbo Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03023v1",
                "http://arxiv.org/pdf/2312.03023v1"
            ],
            "primary_category": "hep-lat",
            "category": [
                "hep-lat"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02549v1",
            "title": "DemaFormer: Damped Exponential Moving Average Transformer with\n  Energy-Based Modeling for Temporal Language Grounding",
            "updated": "2023-12-05T07:37:21Z",
            "published": "2023-12-05T07:37:21Z",
            "summary": "Temporal Language Grounding seeks to localize video moments that semantically\ncorrespond to a natural language query. Recent advances employ the attention\nmechanism to learn the relations between video moments and the text query.\nHowever, naive attention might not be able to appropriately capture such\nrelations, resulting in ineffective distributions where target video moments\nare difficult to separate from the remaining ones. To resolve the issue, we\npropose an energy-based model framework to explicitly learn moment-query\ndistributions. Moreover, we propose DemaFormer, a novel Transformer-based\narchitecture that utilizes exponential moving average with a learnable damping\nfactor to effectively encode moment-query inputs. Comprehensive experiments on\nfour public temporal language grounding datasets showcase the superiority of\nour methods over the state-of-the-art baselines.",
            "author": [
                "Thong Nguyen",
                "Xiaobao Wu",
                "Xinshuai Dong",
                "Cong-Duy Nguyen",
                "See-Kiong Ng",
                "Luu Anh Tuan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02549v1",
                "http://arxiv.org/pdf/2312.02549v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02548v1",
            "title": "GeNIe: Generative Hard Negative Images Through Diffusion",
            "updated": "2023-12-05T07:34:30Z",
            "published": "2023-12-05T07:34:30Z",
            "summary": "Data augmentation is crucial in training deep models, preventing them from\noverfitting to limited data. Common data augmentation methods are effective,\nbut recent advancements in generative AI, such as diffusion models for image\ngeneration, enable more sophisticated augmentation techniques that produce data\nresembling natural images. We recognize that augmented samples closer to the\nideal decision boundary of a classifier are particularly effective and\nefficient in guiding the learning process. We introduce GeNIe which leverages a\ndiffusion model conditioned on a text prompt to merge contrasting data points\n(an image from the source category and a text prompt from the target category)\nto generate challenging samples for the target category. Inspired by recent\nimage editing methods, we limit the number of diffusion iterations and the\namount of noise. This ensures that the generated image retains low-level and\ncontextual features from the source image, potentially conflicting with the\ntarget category. Our extensive experiments, in few-shot and also long-tail\ndistribution settings, demonstrate the effectiveness of our novel augmentation\nmethod, especially benefiting categories with a limited number of examples.",
            "author": [
                "Soroush Abbasi Koohpayegani",
                "Anuj Singh",
                "K L Navaneet",
                "Hadi Jamali-Rad",
                "Hamed Pirsiavash"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02548v1",
                "http://arxiv.org/pdf/2312.02548v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02547v1",
            "title": "On Optimal Consistency-Robustness Trade-Off for Learning-Augmented\n  Multi-Option Ski Rental",
            "updated": "2023-12-05T07:33:51Z",
            "published": "2023-12-05T07:33:51Z",
            "summary": "The learning-augmented multi-option ski rental problem generalizes the\nclassical ski rental problem in two ways: the algorithm is provided with a\nprediction on the number of days we can ski, and the ski rental options now\ncome with a variety of rental periods and prices to choose from, unlike the\nclassical two-option setting. Subsequent to the initial study of the\nmulti-option ski rental problem (without learning augmentation) due to Zhang,\nPoon, and Xu, significant progress has been made for this problem recently in\nparticular. The problem is very well understood when we relinquish one of the\ntwo generalizations -- for the learning-augmented classical ski rental problem,\nalgorithms giving best-possible trade-off between consistency and robustness\nexist; for the multi-option ski rental problem without learning augmentation,\ndeterministic/randomized algorithms giving the best-possible competitiveness\nhave been found. However, in presence of both generalizations, there remained a\nhuge gap between the algorithmic and impossibility results. In fact, for\nrandomized algorithms, we did not have any nontrivial lower bounds on the\nconsistency-robustness trade-off before.\n  This paper bridges this gap for both deterministic and randomized algorithms.\nFor deterministic algorithms, we present a best-possible algorithm that\ncompletely matches the known lower bound. For randomized algorithms, we show\nthe first nontrivial lower bound on the consistency-robustness trade-off, and\nalso present an improved randomized algorithm. Our algorithm matches our lower\nbound on robustness within a factor of e/2 when the consistency is at most\n1.086.",
            "author": [
                "Yongho Shin",
                "Changyeol Lee",
                "Hyung-Chan An"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02547v1",
                "http://arxiv.org/pdf/2312.02547v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "cs.GT",
                "cs.LG",
                "68W27, 68T05",
                "F.2.2; I.2.6"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02546v1",
            "title": "Machine Vision Therapy: Multimodal Large Language Models Can Enhance\n  Visual Robustness via Denoising In-Context Learning",
            "updated": "2023-12-05T07:29:14Z",
            "published": "2023-12-05T07:29:14Z",
            "summary": "Although vision models such as Contrastive Language-Image Pre-Training (CLIP)\nshow impressive generalization performance, their zero-shot robustness is still\nlimited under Out-of-Distribution (OOD) scenarios without fine-tuning. Instead\nof undesirably providing human supervision as commonly done, it is possible to\ntake advantage of Multi-modal Large Language Models (MLLMs) that hold powerful\nvisual understanding abilities. However, MLLMs are shown to struggle with\nvision problems due to the incompatibility of tasks, thus hindering their\nutilization. In this paper, we propose to effectively leverage MLLMs to conduct\nMachine Vision Therapy which aims to rectify the noisy predictions from vision\nmodels. By fine-tuning with the denoised labels, the learning model performance\ncan be boosted in an unsupervised manner. To solve the incompatibility issue,\nwe propose a novel Denoising In-Context Learning (DICL) strategy to align\nvision tasks with MLLMs. Concretely, by estimating a transition matrix that\ncaptures the probability of one class being confused with another, an\ninstruction containing a correct exemplar and an erroneous one from the most\nprobable noisy class can be constructed. Such an instruction can help any MLLMs\nwith ICL ability to detect and rectify incorrect predictions of vision models.\nThrough extensive experiments on ImageNet, WILDS, DomainBed, and other OOD\ndatasets, we carefully validate the quantitative and qualitative effectiveness\nof our method. Our code is available at\nhttps://github.com/tmllab/Machine_Vision_Therapy.",
            "author": [
                "Zhuo Huang",
                "Chang Liu",
                "Yinpeng Dong",
                "Hang Su",
                "Shibao Zheng",
                "Tongliang Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02546v1",
                "http://arxiv.org/pdf/2312.02546v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03022v1",
            "title": "Beyond Isolation: Multi-Agent Synergy for Improving Knowledge Graph\n  Construction",
            "updated": "2023-12-05T07:27:08Z",
            "published": "2023-12-05T07:27:08Z",
            "summary": "Knowledge graph construction (KGC) is a multifaceted undertaking involving\nthe extraction of entities, relations, and events. Traditionally, large\nlanguage models (LLMs) have been viewed as solitary task-solving agents in this\ncomplex landscape. However, this paper challenges this paradigm by introducing\na novel framework, CooperKGC. Departing from the conventional approach,\nCooperKGC establishes a collaborative processing network, assembling a KGC\ncollaboration team capable of concurrently addressing entity, relation, and\nevent extraction tasks. Our experiments unequivocally demonstrate that\nfostering collaboration and information interaction among diverse agents within\nCooperKGC yields superior results compared to individual cognitive processes\noperating in isolation. Importantly, our findings reveal that the collaboration\nfacilitated by CooperKGC enhances knowledge selection, correction, and\naggregation capabilities across multiple rounds of interactions.",
            "author": [
                "Hongbin Ye",
                "Honghao Gui",
                "Aijia Zhang",
                "Tong Liu",
                "Wei Hua",
                "Weiqiang Jia"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03022v1",
                "http://arxiv.org/pdf/2312.03022v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02545v1",
            "title": "Graph Information Bottleneck for Remote Sensing Segmentation",
            "updated": "2023-12-05T07:23:22Z",
            "published": "2023-12-05T07:23:22Z",
            "summary": "Remote sensing segmentation has a wide range of applications in environmental\nprotection, and urban change detection, etc. Despite the success of deep\nlearning-based remote sensing segmentation methods (e.g., CNN and Transformer),\nthey are not flexible enough to model irregular objects. In addition, existing\ngraph contrastive learning methods usually adopt the way of maximizing mutual\ninformation to keep the node representations consistent between different graph\nviews, which may cause the model to learn task-independent redundant\ninformation. To tackle the above problems, this paper treats images as graph\nstructures and introduces a simple contrastive vision GNN (SC-ViG) architecture\nfor remote sensing segmentation. Specifically, we construct a node-masked and\nedge-masked graph view to obtain an optimal graph structure representation,\nwhich can adaptively learn whether to mask nodes and edges. Furthermore, this\npaper innovatively introduces information bottleneck theory into graph\ncontrastive learning to maximize task-related information while minimizing\ntask-independent redundant information. Finally, we replace the convolutional\nmodule in UNet with the SC-ViG module to complete the segmentation and\nclassification tasks of remote sensing images. Extensive experiments on\npublicly available real datasets demonstrate that our method outperforms\nstate-of-the-art remote sensing image segmentation methods.",
            "author": [
                "Yuntao Shou",
                "Wei Ai",
                "Tao Meng"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02545v1",
                "http://arxiv.org/pdf/2312.02545v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02544v1",
            "title": "Characterization of Locality in Spin States and Forced Moves for\n  Optimizations",
            "updated": "2023-12-05T07:21:00Z",
            "published": "2023-12-05T07:21:00Z",
            "summary": "Ising formulations are widely utilized to solve combinatorial optimization\nproblems, and a variety of quantum or semiconductor-based hardware has recently\nbeen made available. In combinatorial optimization problems, the existence of\nlocal minima in energy landscapes is problematic to use to seek the global\nminimum. We note that the aim of the optimization is not to obtain exact\nsamplings from the Boltzmann distribution, and there is thus no need to satisfy\ndetailed balance conditions. In light of this fact, we develop an algorithm to\nget out of the local minima efficiently while it does not yield the exact\nsamplings. For this purpose, we utilize a feature that characterizes locality\nin the current state, which is easy to obtain with a type of specialized\nhardware. Furthermore, as the proposed algorithm is based on a rejection-free\nalgorithm, the computational cost is low. In this work, after presenting the\ndetails of the proposed algorithm, we report the results of numerical\nexperiments that demonstrate the effectiveness of the proposed feature and\nalgorithm.",
            "author": [
                "Yoshiki Sato",
                "Makiko Konoshima",
                "Hirotaka Tamura",
                "Jun Ohkubo"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02544v1",
                "http://arxiv.org/pdf/2312.02544v1"
            ],
            "primary_category": "physics.app-ph",
            "category": [
                "physics.app-ph",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02538v1",
            "title": "A Multi-Granularity-Aware Aspect Learning Model for Multi-Aspect Dense\n  Retrieval",
            "updated": "2023-12-05T07:08:08Z",
            "published": "2023-12-05T07:08:08Z",
            "summary": "Dense retrieval methods have been mostly focused on unstructured text and\nless attention has been drawn to structured data with various aspects, e.g.,\nproducts with aspects such as category and brand. Recent work has proposed two\napproaches to incorporate the aspect information into item representations for\neffective retrieval by predicting the values associated with the item aspects.\nDespite their efficacy, they treat the values as isolated classes (e.g., \"Smart\nHomes\", \"Home, Garden & Tools\", and \"Beauty & Health\") and ignore their\nfine-grained semantic relation. Furthermore, they either enforce the learning\nof aspects into the CLS token, which could confuse it from its designated use\nfor representing the entire content semantics, or learn extra aspect embeddings\nonly with the value prediction objective, which could be insufficient\nespecially when there are no annotated values for an item aspect. Aware of\nthese limitations, we propose a MUlti-granulaRity-aware Aspect Learning model\n(MURAL) for multi-aspect dense retrieval. It leverages aspect information\nacross various granularities to capture both coarse and fine-grained semantic\nrelations between values. Moreover, MURAL incorporates separate aspect\nembeddings as input to transformer encoders so that the masked language model\nobjective can assist implicit aspect learning even without aspect-value\nannotations. Extensive experiments on two real-world datasets of products and\nmini-programs show that MURAL outperforms state-of-the-art baselines\nsignificantly.",
            "author": [
                "Xiaojie Sun",
                "Keping Bi",
                "Jiafeng Guo",
                "Sihui Yang",
                "Qishen Zhang",
                "Zhongyi Liu",
                "Guannan Zhang",
                "Xueqi Cheng"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02538v1",
                "http://arxiv.org/pdf/2312.02538v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02537v1",
            "title": "Asymmetric leader-laggard cluster synchronization for collective\n  decision-making with laser network",
            "updated": "2023-12-05T07:04:21Z",
            "published": "2023-12-05T07:04:21Z",
            "summary": "Photonic accelerators have recently attracted soaring interest, harnessing\nthe ultimate nature of light for information processing. Collective\ndecision-making with a laser network, employing the chaotic and synchronous\ndynamics of optically interconnected lasers to address the competitive\nmulti-armed bandit (CMAB) problem, is a highly compelling approach due to its\nscalability and experimental feasibility. We investigated essential network\nstructures for collective decision-making through quantitative stability\nanalysis. Moreover, we demonstrated the asymmetric preferences of players in\nthe CMAB problem, extending its functionality to more practical applications.\nOur study highlights the capability and significance of machine learning built\nupon chaotic lasers and photonic devices.",
            "author": [
                "Shun Kotoku",
                "Takatomo Mihana",
                "Andr\u00e9 R\u00f6hm",
                "Ryoichi Horisaki",
                "Makoto Naruse"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02537v1",
                "http://arxiv.org/pdf/2312.02537v1"
            ],
            "primary_category": "physics.optics",
            "category": [
                "physics.optics",
                "cs.LG",
                "nlin.CD"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03020v1",
            "title": "Enhanced Breast Cancer Tumor Classification using MobileNetV2: A\n  Detailed Exploration on Image Intensity, Error Mitigation, and\n  Streamlit-driven Real-time Deployment",
            "updated": "2023-12-05T06:58:14Z",
            "published": "2023-12-05T06:58:14Z",
            "summary": "This research introduces a sophisticated transfer learning model based on\nGoogle's MobileNetV2 for breast cancer tumor classification into normal,\nbenign, and malignant categories, utilizing a dataset of 1576 ultrasound images\n(265 normal, 891 benign, 420 malignant). The model achieves an accuracy of\n0.82, precision of 0.83, recall of 0.81, ROC-AUC of 0.94, PR-AUC of 0.88, and\nMCC of 0.74. It examines image intensity distributions and misclassification\nerrors, offering improvements for future applications. Addressing dataset\nimbalances, the study ensures a generalizable model. This work, using a dataset\nfrom Baheya Hospital, Cairo, Egypt, compiled by Walid Al-Dhabyani et al.,\nemphasizes MobileNetV2's potential in medical imaging, aiming to improve\ndiagnostic precision in oncology. Additionally, the paper explores\nStreamlit-based deployment for real-time tumor classification, demonstrating\nMobileNetV2's applicability in medical imaging and setting a benchmark for\nfuture research in oncology diagnostics.",
            "author": [
                "Aaditya Surya",
                "Aditya Shah",
                "Jarnell Kabore",
                "Subash Sasikumar"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03020v1",
                "http://arxiv.org/pdf/2312.03020v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02535v1",
            "title": "Towards Open-set Gesture Recognition via Feature Activation Enhancement\n  and Orthogonal Prototype Learning",
            "updated": "2023-12-05T06:49:15Z",
            "published": "2023-12-05T06:49:15Z",
            "summary": "Gesture recognition is a foundational task in human-machine interaction\n(HMI). While there has been significant progress in gesture recognition based\non surface electromyography (sEMG), accurate recognition of predefined gestures\nonly within a closed set is still inadequate in practice. It is essential to\neffectively discern and reject unknown gestures of disinterest in a robust\nsystem. Numerous methods based on prototype learning (PL) have been proposed to\ntackle this open set recognition (OSR) problem. However, they do not fully\nexplore the inherent distinctions between known and unknown classes. In this\npaper, we propose a more effective PL method leveraging two novel and inherent\ndistinctions, feature activation level and projection inconsistency.\nSpecifically, the Feature Activation Enhancement Mechanism (FAEM) widens the\ngap in feature activation values between known and unknown classes.\nFurthermore, we introduce Orthogonal Prototype Learning (OPL) to construct\nmultiple perspectives. OPL acts to project a sample from orthogonal directions\nto maximize the distinction between its two projections, where unknown samples\nwill be projected near the clusters of different known classes while known\nsamples still maintain intra-class similarity. Our proposed method\nsimultaneously achieves accurate closed-set classification for predefined\ngestures and effective rejection for unknown gestures. Extensive experiments\ndemonstrate its efficacy and superiority in open-set gesture recognition based\non sEMG.",
            "author": [
                "Chen Liu",
                "Can Han",
                "Chengfeng Zhou",
                "Crystal Cai",
                "Suncheng Xiang",
                "Hualiang Ni",
                "Dahong Qian"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02535v1",
                "http://arxiv.org/pdf/2312.02535v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02532v1",
            "title": "DRAFT: Dense Retrieval Augmented Few-shot Topic classifier Framework",
            "updated": "2023-12-05T06:28:45Z",
            "published": "2023-12-05T06:28:45Z",
            "summary": "With the growing volume of diverse information, the demand for classifying\narbitrary topics has become increasingly critical. To address this challenge,\nwe introduce DRAFT, a simple framework designed to train a classifier for\nfew-shot topic classification. DRAFT uses a few examples of a specific topic as\nqueries to construct Customized dataset with a dense retriever model.\nMulti-query retrieval (MQR) algorithm, which effectively handles multiple\nqueries related to a specific topic, is applied to construct the Customized\ndataset. Subsequently, we fine-tune a classifier using the Customized dataset\nto identify the topic. To demonstrate the efficacy of our proposed approach, we\nconduct evaluations on both widely used classification benchmark datasets and\nmanually constructed datasets with 291 diverse topics, which simulate diverse\ncontents encountered in real-world applications. DRAFT shows competitive or\nsuperior performance compared to baselines that use in-context learning, such\nas GPT-3 175B and InstructGPT 175B, on few-shot topic classification tasks\ndespite having 177 times fewer parameters, demonstrating its effectiveness.",
            "author": [
                "Keonwoo Kim",
                "Younggun Lee"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02532v1",
                "http://arxiv.org/pdf/2312.02532v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02531v1",
            "title": "PolyFit: A Peg-in-hole Assembly Framework for Unseen Polygon Shapes via\n  Sim-to-real Adaptation",
            "updated": "2023-12-05T06:28:33Z",
            "published": "2023-12-05T06:28:33Z",
            "summary": "The study addresses the foundational and challenging task of peg-in-hole\nassembly in robotics, where misalignments caused by sensor inaccuracies and\nmechanical errors often result in insertion failures or jamming. This research\nintroduces PolyFit, representing a paradigm shift by transitioning from a\nreinforcement learning approach to a supervised learning methodology. PolyFit\nis a Force/Torque (F/T)-based supervised learning framework designed for 5-DoF\npeg-in-hole assembly. It utilizes F/T data for accurate extrinsic pose\nestimation and adjusts the peg pose to rectify misalignments. Extensive\ntraining in a simulated environment involves a dataset encompassing a diverse\nrange of peg-hole shapes, extrinsic poses, and their corresponding contact F/T\nreadings. To enhance extrinsic pose estimation, a multi-point contact strategy\nis integrated into the model input, recognizing that identical F/T readings can\nindicate different poses. The study proposes a sim-to-real adaptation method\nfor real-world application, using a sim-real paired dataset to enable effective\ngeneralization to complex and unseen polygon shapes. PolyFit achieves\nimpressive peg-in-hole success rates of 97.3% and 96.3% for seen and unseen\nshapes in simulations, respectively. Real-world evaluations further demonstrate\nsubstantial success rates of 86.7% and 85.0%, highlighting the robustness and\nadaptability of the proposed method.",
            "author": [
                "Geonhyup Lee",
                "Joosoon Lee",
                "Sangjun Noh",
                "Minhwan Ko",
                "Kangmin Kim",
                "Kyoobin Lee"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02531v1",
                "http://arxiv.org/pdf/2312.02531v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02530v1",
            "title": "MEMTO: Memory-guided Transformer for Multivariate Time Series Anomaly\n  Detection",
            "updated": "2023-12-05T06:28:19Z",
            "published": "2023-12-05T06:28:19Z",
            "summary": "Detecting anomalies in real-world multivariate time series data is\nchallenging due to complex temporal dependencies and inter-variable\ncorrelations. Recently, reconstruction-based deep models have been widely used\nto solve the problem. However, these methods still suffer from an\nover-generalization issue and fail to deliver consistently high performance. To\naddress this issue, we propose the MEMTO, a memory-guided Transformer using a\nreconstruction-based approach. It is designed to incorporate a novel memory\nmodule that can learn the degree to which each memory item should be updated in\nresponse to the input data. To stabilize the training procedure, we use a\ntwo-phase training paradigm which involves using K-means clustering for\ninitializing memory items. Additionally, we introduce a bi-dimensional\ndeviation-based detection criterion that calculates anomaly scores considering\nboth input space and latent space. We evaluate our proposed method on five\nreal-world datasets from diverse domains, and it achieves an average anomaly\ndetection F1-score of 95.74%, significantly outperforming the previous\nstate-of-the-art methods. We also conduct extensive experiments to empirically\nvalidate the effectiveness of our proposed model's key components.",
            "author": [
                "Junho Song",
                "Keonwoo Kim",
                "Jeonglyul Oh",
                "Sungzoon Cho"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02530v1",
                "http://arxiv.org/pdf/2312.02530v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02522v1",
            "title": "MASP: Scalable GNN-based Planning for Multi-Agent Navigation",
            "updated": "2023-12-05T06:05:04Z",
            "published": "2023-12-05T06:05:04Z",
            "summary": "We investigate the problem of decentralized multi-agent navigation tasks,\nwhere multiple agents need to reach initially unassigned targets in a limited\ntime. Classical planning-based methods suffer from expensive computation\noverhead at each step and offer limited expressiveness for complex cooperation\nstrategies. In contrast, reinforcement learning (RL) has recently become a\npopular paradigm for addressing this issue. However, RL struggles with low data\nefficiency and cooperation when directly exploring (nearly) optimal policies in\nthe large search space, especially with an increased agent number (e.g., 10+\nagents) or in complex environments (e.g., 3D simulators). In this paper, we\npropose Multi-Agent Scalable GNN-based P lanner (MASP), a goal-conditioned\nhierarchical planner for navigation tasks with a substantial number of agents.\nMASP adopts a hierarchical framework to divide a large search space into\nmultiple smaller spaces, thereby reducing the space complexity and accelerating\ntraining convergence. We also leverage graph neural networks (GNN) to model the\ninteraction between agents and goals, improving goal achievement. Besides, to\nenhance generalization capabilities in scenarios with unseen team sizes, we\ndivide agents into multiple groups, each with a previously trained number of\nagents. The results demonstrate that MASP outperforms classical planning-based\ncompetitors and RL baselines, achieving a nearly 100% success rate with minimal\ntraining data in both multi-agent particle environments (MPE) with 50 agents\nand a quadrotor 3-dimensional environment (OmniDrones) with 20 agents.\nFurthermore, the learned policy showcases zero-shot generalization across\nunseen team sizes.",
            "author": [
                "Xinyi Yang",
                "Xinting Yang",
                "Chao Yu",
                "Jiayu Chen",
                "Huazhong Yang",
                "Yu Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02522v1",
                "http://arxiv.org/pdf/2312.02522v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02520v1",
            "title": "Towards More Unified In-context Visual Understanding",
            "updated": "2023-12-05T06:02:21Z",
            "published": "2023-12-05T06:02:21Z",
            "summary": "The rapid advancement of large language models (LLMs) has accelerated the\nemergence of in-context learning (ICL) as a cutting-edge approach in the\nnatural language processing domain. Recently, ICL has been employed in visual\nunderstanding tasks, such as semantic segmentation and image captioning,\nyielding promising results. However, existing visual ICL framework can not\nenable producing content across multiple modalities, which limits their\npotential usage scenarios. To address this issue, we present a new ICL\nframework for visual understanding with multi-modal output enabled. First, we\nquantize and embed both text and visual prompt into a unified representational\nspace, structured as interleaved in-context sequences. Then a decoder-only\nsparse transformer architecture is employed to perform generative modeling on\nthem, facilitating in-context learning. Thanks to this design, the model is\ncapable of handling in-context vision understanding tasks with multimodal\noutput in a unified pipeline. Experimental results demonstrate that our model\nachieves competitive performance compared with specialized models and previous\nICL baselines. Overall, our research takes a further step toward unified\nmultimodal in-context learning.",
            "author": [
                "Dianmo Sheng",
                "Dongdong Chen",
                "Zhentao Tan",
                "Qiankun Liu",
                "Qi Chu",
                "Jianmin Bao",
                "Tao Gong",
                "Bin Liu",
                "Shengwei Xu",
                "Nenghai Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02520v1",
                "http://arxiv.org/pdf/2312.02520v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02519v1",
            "title": "Creative Agents: Empowering Agents with Imagination for Creative Tasks",
            "updated": "2023-12-05T06:00:52Z",
            "published": "2023-12-05T06:00:52Z",
            "summary": "We study building embodied agents for open-ended creative tasks. While\nexisting methods build instruction-following agents that can perform diverse\nopen-ended tasks, none of them demonstrates creativity -- the ability to give\nnovel and diverse task solutions implicit in the language instructions. This\nlimitation comes from their inability to convert abstract language instructions\ninto concrete task goals in the environment and perform long-horizon planning\nfor such complicated goals. Given the observation that humans perform creative\ntasks with the help of imagination, we propose a class of solutions for\ncreative agents, where the controller is enhanced with an imaginator that\ngenerates detailed imaginations of task outcomes conditioned on language\ninstructions. We introduce several approaches to implementing the components of\ncreative agents. We implement the imaginator with either a large language model\nfor textual imagination or a diffusion model for visual imagination. The\ncontroller can either be a behavior-cloning policy learned from data or a\npre-trained foundation model generating executable codes in the environment. We\nbenchmark creative tasks with the challenging open-world game Minecraft, where\nthe agents are asked to create diverse buildings given free-form language\ninstructions. In addition, we propose novel evaluation metrics for open-ended\ncreative tasks utilizing GPT-4V, which holds many advantages over existing\nmetrics. We perform a detailed experimental analysis of creative agents,\nshowing that creative agents are the first AI agents accomplishing diverse\nbuilding creation in the survival mode of Minecraft. Our benchmark and models\nare open-source for future research on creative agents\n(https://github.com/PKU-RL/Creative-Agents).",
            "author": [
                "Chi Zhang",
                "Penglin Cai",
                "Yuhui Fu",
                "Haoqi Yuan",
                "Zongqing Lu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02519v1",
                "http://arxiv.org/pdf/2312.02519v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02517v1",
            "title": "Simplifying Neural Network Training Under Class Imbalance",
            "updated": "2023-12-05T05:52:44Z",
            "published": "2023-12-05T05:52:44Z",
            "summary": "Real-world datasets are often highly class-imbalanced, which can adversely\nimpact the performance of deep learning models. The majority of research on\ntraining neural networks under class imbalance has focused on specialized loss\nfunctions, sampling techniques, or two-stage training procedures. Notably, we\ndemonstrate that simply tuning existing components of standard deep learning\npipelines, such as the batch size, data augmentation, optimizer, and label\nsmoothing, can achieve state-of-the-art performance without any such\nspecialized class imbalance methods. We also provide key prescriptions and\nconsiderations for training under class imbalance, and an understanding of why\nimbalance methods succeed or fail.",
            "author": [
                "Ravid Shwartz-Ziv",
                "Micah Goldblum",
                "Yucen Lily Li",
                "C. Bayan Bruss",
                "Andrew Gordon Wilson"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02517v1",
                "http://arxiv.org/pdf/2312.02517v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02515v1",
            "title": "ASPEN: High-Throughput LoRA Fine-Tuning of Large Language Models with a\n  Single GPU",
            "updated": "2023-12-05T05:38:38Z",
            "published": "2023-12-05T05:38:38Z",
            "summary": "Transformer-based large language models (LLMs) have demonstrated outstanding\nperformance across diverse domains, particularly when fine-turned for specific\ndomains. Recent studies suggest that the resources required for fine-tuning\nLLMs can be economized through parameter-efficient methods such as Low-Rank\nAdaptation (LoRA). While LoRA effectively reduces computational burdens and\nresource demands, it currently supports only a single-job fine-tuning setup.\n  In this paper, we present ASPEN, a high-throughput framework for fine-tuning\nLLMs. ASPEN efficiently trains multiple jobs on a single GPU using the LoRA\nmethod, leveraging shared pre-trained model and adaptive scheduling. ASPEN is\ncompatible with transformer-based language models like LLaMA and ChatGLM, etc.\nExperiments show that ASPEN saves 53% of GPU memory when training multiple\nLLaMA-7B models on NVIDIA A100 80GB GPU and boosts training throughput by about\n17% compared to existing methods when training with various pre-trained models\non different GPUs. The adaptive scheduling algorithm reduces turnaround time by\n24%, end-to-end training latency by 12%, prioritizing jobs and preventing\nout-of-memory issues.",
            "author": [
                "Zhengmao Ye",
                "Dengchun Li",
                "Jingqi Tian",
                "Tingfeng Lan",
                "Jie Zuo",
                "Lei Duan",
                "Hui Lu",
                "Yexi Jiang",
                "Jian Sha",
                "Ke Zhang",
                "Mingjie Tang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02515v1",
                "http://arxiv.org/pdf/2312.02515v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02512v1",
            "title": "AV2AV: Direct Audio-Visual Speech to Audio-Visual Speech Translation\n  with Unified Audio-Visual Speech Representation",
            "updated": "2023-12-05T05:36:44Z",
            "published": "2023-12-05T05:36:44Z",
            "summary": "This paper proposes a novel direct Audio-Visual Speech to Audio-Visual Speech\nTranslation (AV2AV) framework, where the input and output of the system are\nmultimodal (i.e., audio and visual speech). With the proposed AV2AV, two key\nadvantages can be brought: 1) We can perform real-like conversations with\nindividuals worldwide in a virtual meeting by utilizing our own primary\nlanguages. In contrast to Speech-to-Speech Translation (A2A), which solely\ntranslates between audio modalities, the proposed AV2AV directly translates\nbetween audio-visual speech. This capability enhances the dialogue experience\nby presenting synchronized lip movements along with the translated speech. 2)\nWe can improve the robustness of the spoken language translation system. By\nemploying the complementary information of audio-visual speech, the system can\neffectively translate spoken language even in the presence of acoustic noise,\nshowcasing robust performance. To mitigate the problem of the absence of a\nparallel AV2AV translation dataset, we propose to train our spoken language\ntranslation system with the audio-only dataset of A2A. This is done by learning\nunified audio-visual speech representations through self-supervised learning in\nadvance to train the translation system. Moreover, we propose an AV-Renderer\nthat can generate raw audio and video in parallel. It is designed with\nzero-shot speaker modeling, thus the speaker in source audio-visual speech can\nbe maintained at the target translated audio-visual speech. The effectiveness\nof AV2AV is evaluated with extensive experiments in a many-to-many language\ntranslation setting. The demo page is available on\nhttps://choijeongsoo.github.io/av2av.",
            "author": [
                "Jeongsoo Choi",
                "Se Jin Park",
                "Minsu Kim",
                "Yong Man Ro"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02512v1",
                "http://arxiv.org/pdf/2312.02512v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.MM",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03446v1",
            "title": "Visual Hindsight Self-Imitation Learning for Interactive Navigation",
            "updated": "2023-12-05T05:34:12Z",
            "published": "2023-12-05T05:34:12Z",
            "summary": "Interactive visual navigation tasks, which involve following instructions to\nreach and interact with specific targets, are challenging not only because\nsuccessful experiences are very rare but also because the complex visual inputs\nrequire a substantial number of samples. Previous methods for these tasks often\nrely on intricately designed dense rewards or the use of expensive expert data\nfor imitation learning. To tackle these challenges, we propose a novel\napproach, Visual Hindsight Self-Imitation Learning (VHS) for enhancing sample\nefficiency through hindsight goal re-labeling and self-imitation. We also\nintroduce a prototypical goal embedding method derived from experienced goal\nobservations, that is particularly effective in vision-based and partially\nobservable environments. This embedding technique allows the agent to visually\nreinterpret its unsuccessful attempts, enabling vision-based goal re-labeling\nand self-imitation from enhanced successful experiences. Experimental results\nshow that VHS outperforms existing techniques in interactive visual navigation\ntasks, confirming its superior performance and sample efficiency.",
            "author": [
                "Kibeom Kim",
                "Kisung Shin",
                "Min Whoo Lee",
                "Moonhoen Lee",
                "Minsu Lee",
                "Byoung-Tak Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03446v1",
                "http://arxiv.org/pdf/2312.03446v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02509v1",
            "title": "When PETs misbehave: A Contextual Integrity analysis",
            "updated": "2023-12-05T05:27:43Z",
            "published": "2023-12-05T05:27:43Z",
            "summary": "Privacy enhancing technologies, or PETs, have been hailed as a promising\nmeans to protect privacy without compromising on the functionality of digital\nservices. At the same time, and partly because they may encode a narrow\nconceptualization of privacy as confidentiality that is popular among\npolicymakers, engineers and the public, PETs risk being co-opted to promote\nprivacy-invasive practices. In this paper, we resort to the theory of\nContextual Integrity to explain how privacy technologies may be misused to\nerode privacy. To illustrate, we consider three PETs and scenarios: anonymous\ncredentials for age verification, client-side scanning for illegal content\ndetection, and homomorphic encryption for machine learning model training.\nUsing the theory of Contextual Integrity, we reason about the notion of privacy\nthat these PETs encode, and show that CI enables us to identify and reason\nabout the limitations of PETs and their misuse, and which may ultimately lead\nto privacy violations.",
            "author": [
                "Ero Balsa",
                "Yan Shvartzshnaider"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02509v1",
                "http://arxiv.org/pdf/2312.02509v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.CY",
                "cs.IT",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02501v1",
            "title": "Inspecting Model Fairness in Ultrasound Segmentation Tasks",
            "updated": "2023-12-05T05:08:08Z",
            "published": "2023-12-05T05:08:08Z",
            "summary": "With the rapid expansion of machine learning and deep learning (DL),\nresearchers are increasingly employing learning-based algorithms to alleviate\ndiagnostic challenges across diverse medical tasks and applications. While\nadvancements in diagnostic precision are notable, some researchers have\nidentified a concerning trend: their models exhibit biased performance across\nsubgroups characterized by different sensitive attributes. This bias not only\ninfringes upon the rights of patients but also has the potential to lead to\nlife-altering consequences. In this paper, we inspect a series of DL\nsegmentation models using two ultrasound datasets, aiming to assess the\npresence of model unfairness in these specific tasks. Our findings reveal that\neven state-of-the-art DL algorithms demonstrate unfair behavior in ultrasound\nsegmentation tasks. These results serve as a crucial warning, underscoring the\nnecessity for careful model evaluation before their deployment in real-world\nscenarios. Such assessments are imperative to ensure ethical considerations and\nmitigate the risk of adverse impacts on patient outcomes.",
            "author": [
                "Zikang Xu",
                "Fenghe Tang",
                "Quan Quan",
                "Jianrui Ding",
                "Chunping Ning",
                "S. Kevin Zhou"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02501v1",
                "http://arxiv.org/pdf/2312.02501v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02498v1",
            "title": "Provable Reinforcement Learning for Networked Control Systems with\n  Stochastic Packet Disordering",
            "updated": "2023-12-05T04:59:28Z",
            "published": "2023-12-05T04:59:28Z",
            "summary": "This paper formulates a stochastic optimal control problem for linear\nnetworked control systems featuring stochastic packet disordering with a unique\nstabilizing solution certified. The problem is solved by proposing\nreinforcement learning algorithms. A measurement method is first presented to\ndeal with PD and calculate the newest control input. The NCSs with stochastic\nPD are modeled as stochastic NCSs. Then, given a cost function, a modified\nalgebraic Riccati equation is derived within the formulation. We propose\noffline policy iteration and value iteration algorithms to solve the MARE\nassociated with provable convergence. These two algorithms require knowledge of\nNCS dynamics and PD probabilities. To release that, we further design online\nmodel-free off-policy and Q-learning algorithms with an online estimation\nmethod for PD probability. Both model-free algorithms solve the optimal control\nproblem using real-time system states, control inputs, and PD probability\nestimates. Simulation results verify the proposed formulation and algorithms at\nlast.",
            "author": [
                "Wenqian Xue",
                "Yi Jiang",
                "Frank L. Lewis",
                "Bosen Lian"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02498v1",
                "http://arxiv.org/pdf/2312.02498v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02494v1",
            "title": "ReconU-Net: a direct PET image reconstruction using U-Net architecture\n  with back projection-induced skip connection",
            "updated": "2023-12-05T04:51:42Z",
            "published": "2023-12-05T04:51:42Z",
            "summary": "[Objective] This study aims to introduce a novel back projection-induced\nU-Net-shaped architecture, called ReconU-Net, for deep learning-based direct\npositron emission tomography (PET) image reconstruction. Additionally, our\nobjective is to analyze the behavior of direct PET image reconstruction and\ngain deeper insights by comparing the proposed ReconU-Net architecture with\nother encoder-decoder architectures without skip connections. [Approach] The\nproposed ReconU-Net architecture uniquely integrates the physical model of the\nback projection operation into the skip connection. This distinctive feature\nfacilitates the effective transfer of intrinsic spatial information from the\ninput sinogram to the reconstructed image via an embedded physical model. The\nproposed ReconU-Net was trained using Monte Carlo simulation data from the\nBrainweb phantom and tested on both simulated and real Hoffman brain phantom\ndata. [Main results] The proposed ReconU-Net method generated a reconstructed\nimage with a more accurate structure compared to other deep learning-based\ndirect reconstruction methods. Further analysis showed that the proposed\nReconU-Net architecture has the ability to transfer features of multiple\nresolutions, especially non-abstract high-resolution information, through skip\nconnections. Despite limited training on simulated data, the proposed\nReconU-Net successfully reconstructed the real Hoffman brain phantom, unlike\nother deep learning-based direct reconstruction methods, which failed to\nproduce a reconstructed image. [Significance] The proposed ReconU-Net can\nimprove the fidelity of direct PET image reconstruction, even when dealing with\nsmall training datasets, by leveraging the synergistic relationship between\ndata-driven modeling and the physics model of the imaging process.",
            "author": [
                "Fumio Hashimoto",
                "Kibo Ote"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02494v1",
                "http://arxiv.org/pdf/2312.02494v1"
            ],
            "primary_category": "physics.med-ph",
            "category": [
                "physics.med-ph",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02493v1",
            "title": "Flexible Communication for Optimal Distributed Learning over\n  Unpredictable Networks",
            "updated": "2023-12-05T04:51:19Z",
            "published": "2023-12-05T04:51:19Z",
            "summary": "Gradient compression alleviates expensive communication in distributed deep\nlearning by sending fewer values and its corresponding indices, typically via\nAllgather (AG). Training with high compression ratio (CR) achieves high\naccuracy like DenseSGD, but has lower parallel scaling due to high\ncommunication cost (i.e., parallel efficiency). Using lower CRs improves\nparallel efficiency by lowering synchronization cost, but degrades model\naccuracy as well (statistical efficiency). Further, speedup attained with\ndifferent models and CRs also varies with network latency, effective bandwidth\nand collective op used for aggregation. In many cases, collectives like\nAllreduce (AR) have lower cost than AG to exchange the same amount of data. In\nthis paper, we propose an AR-compatible Topk compressor that is\nbandwidth-optimal and thus performs better than AG in certain network\nconfigurations. We develop a flexible communication strategy that switches\nbetween AG and AR based on which collective is optimal in the current settings,\nand model the pareto-relationship between parallel and statistical efficiency\nas a multi-objective optimization (MOO) problem to dynamically adjust CR and\naccelerate training while still converging to high accuracy.",
            "author": [
                "Sahil Tyagi",
                "Martin Swany"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02493v1",
                "http://arxiv.org/pdf/2312.02493v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC",
                "cs.AI",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02491v1",
            "title": "Pseudo Replay-based Class Continual Learning for Online New Category\n  Anomaly Detection in Additive Manufacturing",
            "updated": "2023-12-05T04:43:23Z",
            "published": "2023-12-05T04:43:23Z",
            "summary": "The incorporation of advanced sensors and machine learning techniques has\nenabled modern manufacturing enterprises to perform data-driven in-situ quality\nmonitoring based on the sensor data collected in manufacturing processes.\nHowever, one critical challenge is that newly presented defect category may\nmanifest as the manufacturing process continues, resulting in monitoring\nperformance deterioration of previously trained machine learning models. Hence,\nthere is an increasing need for empowering machine learning model to learn\ncontinually. Among all continual learning methods, memory-based continual\nlearning has the best performance but faces the constraints of data storage\ncapacity. To address this issue, this paper develops a novel pseudo\nreplay-based continual learning by integrating class incremental learning and\noversampling-based data generation. Without storing all the data, the developed\nframework could generate high-quality data representing previous classes to\ntrain machine learning model incrementally when new category anomaly occurs. In\naddition, it could even enhance the monitoring performance since it also\neffectively improves the data quality. The effectiveness of the proposed\nframework is validated in an additive manufacturing process, which leverages\nsupervised classification problem for anomaly detection. The experimental\nresults show that the developed method is very promising in detecting novel\nanomaly while maintaining a good performance on the previous task and brings up\nmore flexibility in model architecture.",
            "author": [
                "Zhangyue Shi",
                "Tianxin Xie",
                "Chenang Liu",
                "Yuxuan Li"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02491v1",
                "http://arxiv.org/pdf/2312.02491v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02490v1",
            "title": "Constrained Twin Variational Auto-Encoder for Intrusion Detection in IoT\n  Systems",
            "updated": "2023-12-05T04:42:04Z",
            "published": "2023-12-05T04:42:04Z",
            "summary": "Intrusion detection systems (IDSs) play a critical role in protecting\nbillions of IoT devices from malicious attacks. However, the IDSs for IoT\ndevices face inherent challenges of IoT systems, including the heterogeneity of\nIoT data/devices, the high dimensionality of training data, and the imbalanced\ndata. Moreover, the deployment of IDSs on IoT systems is challenging, and\nsometimes impossible, due to the limited resources such as memory/storage and\ncomputing capability of typical IoT devices. To tackle these challenges, this\narticle proposes a novel deep neural network/architecture called Constrained\nTwin Variational Auto-Encoder (CTVAE) that can feed classifiers of IDSs with\nmore separable/distinguishable and lower-dimensional representation data.\nAdditionally, in comparison to the state-of-the-art neural networks used in\nIDSs, CTVAE requires less memory/storage and computing power, hence making it\nmore suitable for IoT IDS systems. Extensive experiments with the 11 most\npopular IoT botnet datasets show that CTVAE can boost around 1% in terms of\naccuracy and Fscore in detection attack compared to the state-of-the-art\nmachine learning and representation learning methods, whilst the running time\nfor attack detection is lower than 2E-6 seconds and the model size is lower\nthan 1 MB. We also further investigate various characteristics of CTVAE in the\nlatent space and in the reconstruction representation to demonstrate its\nefficacy compared with current well-known methods.",
            "author": [
                "Phai Vu Dinh",
                "Quang Uy Nguyen",
                "Dinh Thai Hoang",
                "Diep N. Nguyen",
                "Son Pham Bao",
                "Eryk Dutkiewicz"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02490v1",
                "http://arxiv.org/pdf/2312.02490v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02488v1",
            "title": "Uncertainty-Aware Shared Autonomy System with Hierarchical Conservative\n  Skill Inference",
            "updated": "2023-12-05T04:32:40Z",
            "published": "2023-12-05T04:32:40Z",
            "summary": "Shared autonomy imitation learning, in which robots share workspace with\nhumans for learning, enables correct actions in unvisited states and the\neffective resolution of compounding errors through expert's corrections.\nHowever, it demands continuous human attention and supervision to lead the\ndemonstrations, without considering the risks associated with human judgment\nerrors and delayed interventions. This can potentially lead to high levels of\nfatigue for the demonstrator and the additional errors. In this work, we\npropose an uncertainty-aware shared autonomy system that enables the robot to\ninfer conservative task skills considering environmental uncertainties and\nlearning from expert demonstrations and corrections. To enhance generalization\nand scalability, we introduce a hierarchical structure-based skill uncertainty\ninference framework operating at more abstract levels. We apply this to robot\nmotion to promote a more stable interaction. Although shared autonomy systems\nhave demonstrated high-level results in recent research and play a critical\nrole, specific system design details have remained elusive. This paper provides\na detailed design proposal for a shared autonomy system considering various\nrobot configurations. Furthermore, we experimentally demonstrate the system's\ncapability to learn operational skills, even in dynamic environments with\ninterference, through pouring and pick-and-place tasks. Our code will be\nreleased soon.",
            "author": [
                "Taewoo Kim",
                "Donghyung Kim",
                "Minsu Jang",
                "Jaehong Kim"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02488v1",
                "http://arxiv.org/pdf/2312.02488v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02483v1",
            "title": "EtC: Temporal Boundary Expand then Clarify for Weakly Supervised Video\n  Grounding with Multimodal Large Language Model",
            "updated": "2023-12-05T04:15:56Z",
            "published": "2023-12-05T04:15:56Z",
            "summary": "Early weakly supervised video grounding (WSVG) methods often struggle with\nincomplete boundary detection due to the absence of temporal boundary\nannotations. To bridge the gap between video-level and boundary-level\nannotation, explicit-supervision methods, i.e., generating pseudo-temporal\nboundaries for training, have achieved great success. However, data\naugmentations in these methods might disrupt critical temporal information,\nyielding poor pseudo boundaries. In this paper, we propose a new perspective\nthat maintains the integrity of the original temporal content while introducing\nmore valuable information for expanding the incomplete boundaries. To this end,\nwe propose EtC (Expand then Clarify), first use the additional information to\nexpand the initial incomplete pseudo boundaries, and subsequently refine these\nexpanded ones to achieve precise boundaries. Motivated by video continuity,\ni.e., visual similarity across adjacent frames, we use powerful multimodal\nlarge language models (MLLMs) to annotate each frame within initial pseudo\nboundaries, yielding more comprehensive descriptions for expanded boundaries.\nTo further clarify the noise of expanded boundaries, we combine mutual learning\nwith a tailored proposal-level contrastive objective to use a learnable\napproach to harmonize a balance between incomplete yet clean (initial) and\ncomprehensive yet noisy (expanded) boundaries for more precise ones.\nExperiments demonstrate the superiority of our method on two challenging WSVG\ndatasets.",
            "author": [
                "Guozhang Li",
                "Xinpeng Ding",
                "De Cheng",
                "Jie Li",
                "Nannan Wang",
                "Xinbo Gao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02483v1",
                "http://arxiv.org/pdf/2312.02483v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02481v1",
            "title": "Learning to Holistically Detect Bridges from Large-Size VHR Remote\n  Sensing Imagery",
            "updated": "2023-12-05T04:15:22Z",
            "published": "2023-12-05T04:15:22Z",
            "summary": "Bridge detection in remote sensing images (RSIs) plays a crucial role in\nvarious applications, but it poses unique challenges compared to the detection\nof other objects. In RSIs, bridges exhibit considerable variations in terms of\ntheir spatial scales and aspect ratios. Therefore, to ensure the visibility and\nintegrity of bridges, it is essential to perform holistic bridge detection in\nlarge-size very-high-resolution (VHR) RSIs. However, the lack of datasets with\nlarge-size VHR RSIs limits the deep learning algorithms' performance on bridge\ndetection. Due to the limitation of GPU memory in tackling large-size images,\ndeep learning-based object detection methods commonly adopt the cropping\nstrategy, which inevitably results in label fragmentation and discontinuous\nprediction. To ameliorate the scarcity of datasets, this paper proposes a\nlarge-scale dataset named GLH-Bridge comprising 6,000 VHR RSIs sampled from\ndiverse geographic locations across the globe. These images encompass a wide\nrange of sizes, varying from 2,048*2,048 to 16,38*16,384 pixels, and\ncollectively feature 59,737 bridges. Furthermore, we present an efficient\nnetwork for holistic bridge detection (HBD-Net) in large-size RSIs. The HBD-Net\npresents a separate detector-based feature fusion (SDFF) architecture and is\noptimized via a shape-sensitive sample re-weighting (SSRW) strategy. Based on\nthe proposed GLH-Bridge dataset, we establish a bridge detection benchmark\nincluding the OBB and HBB tasks, and validate the effectiveness of the proposed\nHBD-Net. Additionally, cross-dataset generalization experiments on two publicly\navailable datasets illustrate the strong generalization capability of the\nGLH-Bridge dataset.",
            "author": [
                "Yansheng Li",
                "Junwei Luo",
                "Yongjun Zhang",
                "Yihua Tan",
                "Jin-Gang Yu",
                "Song Bai"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02481v1",
                "http://arxiv.org/pdf/2312.02481v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02479v1",
            "title": "Applications of Domain Adversarial Neural Network in phase transition of\n  3D Potts model",
            "updated": "2023-12-05T04:12:13Z",
            "published": "2023-12-05T04:12:13Z",
            "summary": "Machine learning techniques exhibit significant performance in discriminating\ndifferent phases of matter and provide a new avenue for studying phase\ntransitions. We investigate the phase transitions of three dimensional\n$q$-state Potts model on cubic lattice by using a transfer learning approach,\nDomain Adversarial Neural Network (DANN). With the unique neural network\narchitecture, it could evaluate the high-temperature (disordered) and\nlow-temperature (ordered) phases, and identify the first and second order phase\ntransitions. Meanwhile, by training the DANN with a few labeled configurations,\nthe critical points for $q=2,3,4$ and $5$ can be predicted with high accuracy,\nwhich are consistent with those of the Monte Carlo simulations. These findings\nwould promote us to learn and explore the properties of phase transitions in\nhigh-dimensional systems.",
            "author": [
                "Xiangna Chen",
                "Feiyi Liu",
                "Weibing Deng",
                "Shiyang Chen",
                "Jianmin Shen",
                "Gabor Papp",
                "Wei Li",
                "Chunbin Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02479v1",
                "http://arxiv.org/pdf/2312.02479v1"
            ],
            "primary_category": "physics.comp-ph",
            "category": [
                "physics.comp-ph",
                "cond-mat.stat-mech"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02478v1",
            "title": "RL-Based Cargo-UAV Trajectory Planning and Cell Association for Minimum\n  Handoffs, Disconnectivity, and Energy Consumption",
            "updated": "2023-12-05T04:06:09Z",
            "published": "2023-12-05T04:06:09Z",
            "summary": "Unmanned aerial vehicle (UAV) is a promising technology for last-mile cargo\ndelivery. However, the limited on-board battery capacity, cellular\nunreliability, and frequent handoffs in the airspace are the main obstacles to\nunleash its full potential. Given that existing cellular networks were\nprimarily designed to service ground users, re-utilizing the same architecture\nfor highly mobile aerial users, e.g., cargo-UAVs, is deemed challenging.\nIndeed, to ensure a safe delivery using cargo-UAVs, it is crucial to utilize\nthe available energy efficiently, while guaranteeing reliable connectivity for\ncommand-and-control and avoiding frequent handoff. To achieve this goal, we\npropose a novel approach for joint cargo-UAV trajectory planning and cell\nassociation. Specifically, we formulate the cargo-UAV mission as a\nmulti-objective problem aiming to 1) minimize energy consumption, 2) reduce\nhandoff events, and 3) guarantee cellular reliability along the trajectory. We\nleverage reinforcement learning (RL) to jointly optimize the cargo-UAV's\ntrajectory and cell association. Simulation results demonstrate a performance\nimprovement of our proposed method, in terms of handoffs, disconnectivity, and\nenergy consumption, compared to benchmarks.",
            "author": [
                "Nesrine Cherif",
                "Wael Jaafar",
                "Halim Yanikomeroglu",
                "Abbas Yongacoglu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02478v1",
                "http://arxiv.org/pdf/2312.02478v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.LG",
                "cs.RO",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02475v2",
            "title": "Accurate Machine Learning Predictions of Coercivity in High-Performance\n  Permanent Magnets",
            "updated": "2023-12-07T03:45:40Z",
            "published": "2023-12-05T03:58:34Z",
            "summary": "Increased demand for high-performance permanent magnets in the electric\nvehicle and wind turbine industries has prompted the search for cost-effective\nalternatives. Nevertheless, the discovery of new magnetic materials with the\ndesired intrinsic and extrinsic permanent magnet properties presents a\nsignificant challenge. Traditional density functional theory (DFT) accurately\npredicts intrinsic permanent magnet properties such as magnetic moments,\nmagneto-crystalline anisotropy constants, and exchange interactions. However,\nit cannot compute extrinsic macroscopic properties, such as coercivity ($H_c$),\nwhich are influenced by factors like microscopic defects and internal grain\nstructures. Although micromagnetic simulation helps compute $H_c$, it\noverestimates the values almost by an order of magnitude due to Brown's\nparadox. To circumvent these limitations, we employ machine learning (ML)\nmethods in an extensive database obtained from experiments, DFT calculations,\nand micromagnetic modeling. Our novel ML approach is computationally much\nfaster than the micromagnetic simulation program, the mumax$^3$. We\nsuccessfully utilize it to predict $H_c$ values for materials like cerium-doped\n$\\mathrm{Nd}_2\\mathrm{Fe}_{14}\\mathrm{B}$, and subsequently compare the\npredicted values with experimental results. Remarkably, our ML model accurately\nidentifies uniaxial magnetic anisotropy as the primary contributor to $H_c$.\nWith DFT calculations, we predict the Nd-site dependent magnetic anisotropy\nbehavior in $\\mathrm{Nd}_2\\mathrm{Fe}_{14}\\mathrm{B}$, confirming $4f$-site\nplanar and $4g$-site uniaxial to crystalline $c$-direction in good agreement\nwith experiment. The Green's function atomic sphere approximation calculated a\nCurie temperature ($T_{\\rm C}$) for $\\mathrm{Nd}_2\\mathrm{Fe}_{14}\\mathrm{B}$\nthat also agrees well with experiment.",
            "author": [
                "Churna Bhandari",
                "Gavin N. Nop",
                "Jonathan D. H. Smith",
                "Durga Paudyal"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02475v2",
                "http://arxiv.org/pdf/2312.02475v2"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci",
                "physics.data-an"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02473v1",
            "title": "NeutronStream: A Dynamic GNN Training Framework with Sliding Window for\n  Graph Streams",
            "updated": "2023-12-05T03:58:05Z",
            "published": "2023-12-05T03:58:05Z",
            "summary": "Existing Graph Neural Network (GNN) training frameworks have been designed to\nhelp developers easily create performant GNN implementations. However, most\nexisting GNN frameworks assume that the input graphs are static, but ignore\nthat most real-world graphs are constantly evolving. Though many dynamic GNN\nmodels have emerged to learn from evolving graphs, the training process of\nthese dynamic GNNs is dramatically different from traditional GNNs in that it\ncaptures both the spatial and temporal dependencies of graph updates. This\nposes new challenges for designing dynamic GNN training frameworks. First, the\ntraditional batched training method fails to capture real-time structural\nevolution information. Second, the time-dependent nature makes parallel\ntraining hard to design. Third, it lacks system supports for users to\nefficiently implement dynamic GNNs. In this paper, we present NeutronStream, a\nframework for training dynamic GNN models. NeutronStream abstracts the input\ndynamic graph into a chronologically updated stream of events and processes the\nstream with an optimized sliding window to incrementally capture the\nspatial-temporal dependencies of events. Furthermore, NeutronStream provides a\nparallel execution engine to tackle the sequential event processing challenge\nto achieve high performance. NeutronStream also integrates a built-in graph\nstorage structure that supports dynamic updates and provides a set of\neasy-to-use APIs that allow users to express their dynamic GNNs. Our\nexperimental results demonstrate that, compared to state-of-the-art dynamic GNN\nimplementations, NeutronStream achieves speedups ranging from 1.48X to 5.87X\nand an average accuracy improvement of 3.97%.",
            "author": [
                "Chaoyi Chen",
                "Dechao Gao",
                "Yanfeng Zhang",
                "Qiange Wang",
                "Zhenbo Fu",
                "Xuecang Zhang",
                "Junhua Zhu",
                "Yu Gu",
                "Ge Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02473v1",
                "http://arxiv.org/pdf/2312.02473v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02471v1",
            "title": "Congestion-aware Distributed Task Offloading in Wireless Multi-hop\n  Networks Using Graph Neural Networks",
            "updated": "2023-12-05T03:46:30Z",
            "published": "2023-12-05T03:46:30Z",
            "summary": "Computational offloading has become an enabling component for edge\nintelligence in mobile and smart devices. Existing offloading schemes mainly\nfocus on mobile devices and servers, while ignoring the potential network\ncongestion caused by tasks from multiple mobile devices, especially in wireless\nmulti-hop networks. To fill this gap, we propose a low-overhead,\ncongestion-aware distributed task offloading scheme by augmenting a distributed\ngreedy framework with graph-based machine learning. In simulated wireless\nmulti-hop networks with 20-110 nodes and a resource allocation scheme based on\nshortest path routing and contention-based link scheduling, our approach is\ndemonstrated to be effective in reducing congestion or unstable queues under\nthe context-agnostic baseline, while improving the execution latency over local\ncomputing.",
            "author": [
                "Zhongyuan Zhao",
                "Jake Perazzone",
                "Gunjan Verma",
                "Santiago Segarra"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02471v1",
                "http://arxiv.org/pdf/2312.02471v1"
            ],
            "primary_category": "cs.NI",
            "category": [
                "cs.NI",
                "cs.LG",
                "eess.SP",
                "05C90",
                "C.2.1; C.2.2"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02470v1",
            "title": "Generator Born from Classifier",
            "updated": "2023-12-05T03:41:17Z",
            "published": "2023-12-05T03:41:17Z",
            "summary": "In this paper, we make a bold attempt toward an ambitious task: given a\npre-trained classifier, we aim to reconstruct an image generator, without\nrelying on any data samples. From a black-box perspective, this challenge seems\nintractable, since it inevitably involves identifying the inverse function for\na classifier, which is, by nature, an information extraction process. As such,\nwe resort to leveraging the knowledge encapsulated within the parameters of the\nneural network. Grounded on the theory of Maximum-Margin Bias of gradient\ndescent, we propose a novel learning paradigm, in which the generator is\ntrained to ensure that the convergence conditions of the network parameters are\nsatisfied over the generated distribution of the samples. Empirical validation\nfrom various image generation tasks substantiates the efficacy of our strategy.",
            "author": [
                "Runpeng Yu",
                "Xinchao Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02470v1",
                "http://arxiv.org/pdf/2312.02470v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02469v1",
            "title": "Learning Energy-based Model via Dual-MCMC Teaching",
            "updated": "2023-12-05T03:39:54Z",
            "published": "2023-12-05T03:39:54Z",
            "summary": "This paper studies the fundamental learning problem of the energy-based model\n(EBM). Learning the EBM can be achieved using the maximum likelihood estimation\n(MLE), which typically involves the Markov Chain Monte Carlo (MCMC) sampling,\nsuch as the Langevin dynamics. However, the noise-initialized Langevin dynamics\ncan be challenging in practice and hard to mix. This motivates the exploration\nof joint training with the generator model where the generator model serves as\na complementary model to bypass MCMC sampling. However, such a method can be\nless accurate than the MCMC and result in biased EBM learning. While the\ngenerator can also serve as an initializer model for better MCMC sampling, its\nlearning can be biased since it only matches the EBM and has no access to\nempirical training examples. Such biased generator learning may limit the\npotential of learning the EBM. To address this issue, we present a joint\nlearning framework that interweaves the maximum likelihood learning algorithm\nfor both the EBM and the complementary generator model. In particular, the\ngenerator model is learned by MLE to match both the EBM and the empirical data\ndistribution, making it a more informative initializer for MCMC sampling of\nEBM. Learning generator with observed examples typically requires inference of\nthe generator posterior. To ensure accurate and efficient inference, we adopt\nthe MCMC posterior sampling and introduce a complementary inference model to\ninitialize such latent MCMC sampling. We show that three separate models can be\nseamlessly integrated into our joint framework through two (dual-) MCMC\nteaching, enabling effective and efficient EBM learning.",
            "author": [
                "Jiali Cui",
                "Tian Han"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02469v1",
                "http://arxiv.org/pdf/2312.02469v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV",
                "stat.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02466v1",
            "title": "Merger Tree-based Halo/Galaxy Matching Between Cosmological Simulations\n  with Different Resolutions: Galaxy-by-galaxy Resolution Study and the Machine\n  Learning-based Correction",
            "updated": "2023-12-05T03:35:23Z",
            "published": "2023-12-05T03:35:23Z",
            "summary": "We introduce a novel halo/galaxy matching technique between two cosmological\nsimulations with different resolutions, which utilizes the positions and masses\nof halos along their subhalo merger tree. With this tool, we conduct a study of\nresolution biases through the galaxy-by-galaxy inspection of a pair of\nsimulations that have the same simulation configuration but different mass\nresolutions, utilizing a suite of IllustrisTNG simulations to assess the impact\non galaxy properties. We find that, with the subgrid physics model calibrated\nfor TNG100-1, subhalos in TNG100-1 (high resolution) have $\\lesssim0.5$ dex\nhigher stellar masses than their counterparts in the TNG100-2 (low-resolution).\nIt is also discovered that the subhalos with $M_{\\mathrm{gas}}\\sim10^{8.5}{\\rm\nM}_\\odot$ in TNG100-1 have $\\sim0.5$ dex higher gas mass than those in\nTNG100-2. The mass profiles of the subhalos reveal that the dark matter masses\nof low-resolution subhalos are $\\sim0.6$ times lower within 2 kpc, near the\nresolution limit. The differences in stellar mass and hot gas mass are most\npronounced in the central region. We exploit machine learning to build a\ncorrection mapping for the physical quantities of subhalos from low- to\nhigh-resolution simulations (TNG300-1 and TNG100-1), which enables us to find\nan efficient way to compile a high-resolution galaxy catalog even from a\nlow-resolution simulation. Our tools can easily be applied to other large\ncosmological simulations, testing and mitigating the resolution biases of their\nnumerical codes and subgrid physics models.",
            "author": [
                "Minyong Jung",
                "Ji-hoon Kim",
                "Boon Kiat Oh",
                "Sungwook E. Hong",
                "Jaehyun Lee",
                "Juhan Kim"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02466v1",
                "http://arxiv.org/pdf/2312.02466v1"
            ],
            "primary_category": "astro-ph.GA",
            "category": [
                "astro-ph.GA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02462v1",
            "title": "Dimensionality Reduction and Dynamical Mode Recognition of Circular\n  Arrays of Flame Oscillators Using Deep Neural Network",
            "updated": "2023-12-05T03:25:45Z",
            "published": "2023-12-05T03:25:45Z",
            "summary": "Oscillatory combustion in aero engines and modern gas turbines often has\nsignificant adverse effects on their operation, and accurately recognizing\nvarious oscillation modes is the prerequisite for understanding and controlling\ncombustion instability. However, the high-dimensional spatial-temporal data of\na complex combustion system typically poses considerable challenges to the\ndynamical mode recognition. Based on a two-layer bidirectional long short-term\nmemory variational autoencoder (Bi-LSTM-VAE) dimensionality reduction model and\na two-dimensional Wasserstein distance-based classifier (WDC), this study\nproposes a promising method (Bi-LSTM-VAE-WDC) for recognizing dynamical modes\nin oscillatory combustion systems. Specifically, the Bi-LSTM-VAE dimension\nreduction model was introduced to reduce the high-dimensional spatial-temporal\ndata of the combustion system to a low-dimensional phase space; Gaussian kernel\ndensity estimates (GKDE) were computed based on the distribution of phase\npoints in a grid; two-dimensional WD values were calculated from the GKDE maps\nto recognize the oscillation modes. The time-series data used in this study\nwere obtained from numerical simulations of circular arrays of laminar flame\noscillators. The results show that the novel Bi-LSTM-VAE method can produce a\nnon-overlapping distribution of phase points, indicating an effective\nunsupervised mode recognition and classification. Furthermore, the present\nmethod exhibits a more prominent performance than VAE and PCA (principal\ncomponent analysis) for distinguishing dynamical modes in complex flame\nsystems, implying its potential in studying turbulent combustion.",
            "author": [
                "Weiming Xu",
                "Tao Yang",
                "Peng Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02462v1",
                "http://arxiv.org/pdf/2312.02462v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "physics.flu-dyn"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02450v1",
            "title": "GIT-Net: Generalized Integral Transform for Operator Learning",
            "updated": "2023-12-05T03:03:54Z",
            "published": "2023-12-05T03:03:54Z",
            "summary": "This article introduces GIT-Net, a deep neural network architecture for\napproximating Partial Differential Equation (PDE) operators, inspired by\nintegral transform operators. GIT-NET harnesses the fact that differential\noperators commonly used for defining PDEs can often be represented\nparsimoniously when expressed in specialized functional bases (e.g., Fourier\nbasis). Unlike rigid integral transforms, GIT-Net parametrizes adaptive\ngeneralized integral transforms with deep neural networks. When compared to\nseveral recently proposed alternatives, GIT-Net's computational and memory\nrequirements scale gracefully with mesh discretizations, facilitating its\napplication to PDE problems on complex geometries. Numerical experiments\ndemonstrate that GIT-Net is a competitive neural network operator, exhibiting\nsmall test errors and low evaluations across a range of PDE problems. This\nstands in contrast to existing neural network operators, which typically excel\nin just one of these areas.",
            "author": [
                "Chao Wang",
                "Alexandre Hoang Thiery"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02450v1",
                "http://arxiv.org/pdf/2312.02450v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02447v1",
            "title": "Fast non-autoregressive inverse folding with discrete diffusion",
            "updated": "2023-12-05T02:57:42Z",
            "published": "2023-12-05T02:57:42Z",
            "summary": "Generating protein sequences that fold into a intended 3D structure is a\nfundamental step in de novo protein design. De facto methods utilize\nautoregressive generation, but this eschews higher order interactions that\ncould be exploited to improve inference speed. We describe a non-autoregressive\nalternative that performs inference using a constant number of calls resulting\nin a 23 times speed up without a loss in performance on the CATH benchmark.\nConditioned on the 3D structure, we fine-tune ProteinMPNN to perform discrete\ndiffusion with a purity prior over the index sampling order. Our approach gives\nthe flexibility in trading off inference speed and accuracy by modulating the\ndiffusion speed. Code: https://github.com/johnyang101/pmpnndiff",
            "author": [
                "John J. Yang",
                "Jason Yim",
                "Regina Barzilay",
                "Tommi Jaakkola"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02447v1",
                "http://arxiv.org/pdf/2312.02447v1"
            ],
            "primary_category": "q-bio.BM",
            "category": [
                "q-bio.BM",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02445v1",
            "title": "LLaRA: Aligning Large Language Models with Sequential Recommenders",
            "updated": "2023-12-05T02:53:46Z",
            "published": "2023-12-05T02:53:46Z",
            "summary": "Sequential recommendation aims to predict the subsequent items matching user\npreference based on her/his historical interactions. With the development of\nLarge Language Models (LLMs), there is growing interest in exploring the\npotential of LLMs for sequential recommendation by framing it as a language\nmodeling task. Prior works represent items in the textual prompts using either\nID indexing or text indexing and feed the prompts into LLMs, but falling short\nof either encapsulating comprehensive world knowledge or exhibiting sufficient\nsequential understanding. To harness the complementary strengths of traditional\nrecommenders (which encode user behavioral knowledge) and LLMs (which possess\nworld knowledge about items), we propose LLaRA -- a Large Language and\nRecommendation Assistant framework. Specifically, LLaRA represents items in\nLLM's input prompts using a novel hybrid approach that integrates ID-based item\nembeddings from traditional recommenders with textual item features. Viewing\nthe ``sequential behavior of the user'' as a new modality in recommendation, we\nemploy an adapter to bridge the modality gap between ID embeddings of the\ntraditional recommenders and the input space of LLMs. Furthermore, instead of\ndirectly exposing the hybrid prompt to LLMs, we apply a curriculum learning\napproach to gradually ramp up training complexity. We first warm up the LLM\nwith text-only prompting, which aligns more naturally with the LLM's language\nmodeling capabilities. Thereafter, we progressively transition to hybrid\nprompting, training the adapter to incorporate behavioral knowledge from the\ntraditional sequential recommender into the LLM. Extensive experiments\ndemonstrate the efficacy of LLaRA framework. Our code and data are available at\nhttps://github.com/ljy0ustc/LLaRA .",
            "author": [
                "Jiayi Liao",
                "Sihang Li",
                "Zhengyi Yang",
                "Jiancan Wu",
                "Yancheng Yuan",
                "Xiang Wang",
                "Xiangnan He"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02445v1",
                "http://arxiv.org/pdf/2312.02445v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02438v1",
            "title": "Adaptive Instrument Design for Indirect Experiments",
            "updated": "2023-12-05T02:38:04Z",
            "published": "2023-12-05T02:38:04Z",
            "summary": "Indirect experiments provide a valuable framework for estimating treatment\neffects in situations where conducting randomized control trials (RCTs) is\nimpractical or unethical. Unlike RCTs, indirect experiments estimate treatment\neffects by leveraging (conditional) instrumental variables, enabling estimation\nthrough encouragement and recommendation rather than strict treatment\nassignment. However, the sample efficiency of such estimators depends not only\non the inherent variability in outcomes but also on the varying compliance\nlevels of users with the instrumental variables and the choice of estimator\nbeing used, especially when dealing with numerous instrumental variables. While\nadaptive experiment design has a rich literature for direct experiments, in\nthis paper we take the initial steps towards enhancing sample efficiency for\nindirect experiments by adaptively designing a data collection policy over\ninstrumental variables. Our main contribution is a practical computational\nprocedure that utilizes influence functions to search for an optimal data\ncollection policy, minimizing the mean-squared error of the desired\n(non-linear) estimator. Through experiments conducted in various domains\ninspired by real-world applications, we showcase how our method can\nsignificantly improve the sample efficiency of indirect experiments.",
            "author": [
                "Yash Chandak",
                "Shiv Shankar",
                "Vasilis Syrgkanis",
                "Emma Brunskill"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02438v1",
                "http://arxiv.org/pdf/2312.02438v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02431v1",
            "title": "Visually Grounded Language Learning: a review of language games,\n  datasets, tasks, and models",
            "updated": "2023-12-05T02:17:29Z",
            "published": "2023-12-05T02:17:29Z",
            "summary": "In recent years, several machine learning models have been proposed. They are\ntrained with a language modelling objective on large-scale text-only data. With\nsuch pretraining, they can achieve impressive results on many Natural Language\nUnderstanding and Generation tasks. However, many facets of meaning cannot be\nlearned by ``listening to the radio\" only. In the literature, many\nVision+Language (V+L) tasks have been defined with the aim of creating models\nthat can ground symbols in the visual modality. In this work, we provide a\nsystematic literature review of several tasks and models proposed in the V+L\nfield. We rely on Wittgenstein's idea of `language games' to categorise such\ntasks into 3 different families: 1) discriminative games, 2) generative games,\nand 3) interactive games. Our analysis of the literature provides evidence that\nfuture work should be focusing on interactive games where communication in\nNatural Language is important to resolve ambiguities about object referents and\naction plans and that physical embodiment is essential to understand the\nsemantics of situations and events. Overall, these represent key requirements\nfor developing grounded meanings in neural models.",
            "author": [
                "Alessandro Suglia",
                "Ioannis Konstas",
                "Oliver Lemon"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02431v1",
                "http://arxiv.org/pdf/2312.02431v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02429v2",
            "title": "PEFA: Parameter-Free Adapters for Large-scale Embedding-based Retrieval\n  Models",
            "updated": "2023-12-06T03:08:31Z",
            "published": "2023-12-05T02:08:48Z",
            "summary": "Embedding-based Retrieval Models (ERMs) have emerged as a promising framework\nfor large-scale text retrieval problems due to powerful large language models.\nNevertheless, fine-tuning ERMs to reach state-of-the-art results can be\nexpensive due to the extreme scale of data as well as the complexity of\nmulti-stages pipelines (e.g., pre-training, fine-tuning, distillation). In this\nwork, we propose the PEFA framework, namely ParamEter-Free Adapters, for fast\ntuning of ERMs without any backward pass in the optimization. At index building\nstage, PEFA equips the ERM with a non-parametric k-nearest neighbor (kNN)\ncomponent. At inference stage, PEFA performs a convex combination of two\nscoring functions, one from the ERM and the other from the kNN. Based on the\nneighborhood definition, PEFA framework induces two realizations, namely\nPEFA-XL (i.e., extra large) using double ANN indices and PEFA-XS (i.e., extra\nsmall) using a single ANN index. Empirically, PEFA achieves significant\nimprovement on two retrieval applications. For document retrieval, regarding\nRecall@100 metric, PEFA improves not only pre-trained ERMs on Trivia-QA by an\naverage of 13.2%, but also fine-tuned ERMs on NQ-320K by an average of 5.5%,\nrespectively. For product search, PEFA improves the Recall@100 of the\nfine-tuned ERMs by an average of 5.3% and 14.5%, for PEFA-XS and PEFA-XL,\nrespectively. Our code is available at\nhttps://github.com/amzn/pecos/tree/mainline/examples/pefa-wsdm24.",
            "author": [
                "Wei-Cheng Chang",
                "Jyun-Yu Jiang",
                "Jiong Zhang",
                "Mutasem Al-Darabsah",
                "Choon Hui Teo",
                "Cho-Jui Hsieh",
                "Hsiang-Fu Yu",
                "S. V. N. Vishwanathan"
            ],
            "link": [
                "http://dx.doi.org/10.1145/3616855.3635791",
                "http://arxiv.org/abs/2312.02429v2",
                "http://arxiv.org/pdf/2312.02429v2"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03017v1",
            "title": "AI-driven emergence of frequency information non-uniform distribution\n  via THz metasurface spectrum prediction",
            "updated": "2023-12-05T01:48:58Z",
            "published": "2023-12-05T01:48:58Z",
            "summary": "Recently, artificial intelligence has been extensively deployed across\nvarious scientific disciplines, optimizing and guiding the progression of\nexperiments through the integration of abundant datasets, whilst continuously\nprobing the vast theoretical space encapsulated within the data. Particularly,\ndeep learning models, due to their end-to-end adaptive learning capabilities,\nare capable of autonomously learning intrinsic data features, thereby\ntranscending the limitations of traditional experience to a certain extent.\nHere, we unveil previously unreported information characteristics pertaining to\ndifferent frequencies emerged during our work on predicting the terahertz\nspectral modulation effects of metasurfaces based on AI-prediction. Moreover,\nwe have substantiated that our proposed methodology of simply adding\nsupplementary multi-frequency inputs to the existing dataset during the target\nspectral prediction process can significantly enhance the predictive accuracy\nof the network. This approach effectively optimizes the utilization of existing\ndatasets and paves the way for interdisciplinary research and applications in\nartificial intelligence, chemistry, composite material design, biomedicine, and\nother fields.",
            "author": [
                "Xiaohua Xing",
                "Yuqi Ren",
                "Die Zou",
                "Qiankun Zhang",
                "Bingxuan Mao",
                "Jianquan Yao",
                "Deyi Xiong",
                "Shuang Zhang",
                "Liang Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03017v1",
                "http://arxiv.org/pdf/2312.03017v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "physics.optics"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03016v1",
            "title": "Protein Language Model-Powered 3D Ligand Binding Site Prediction from\n  Protein Sequence",
            "updated": "2023-12-05T01:47:38Z",
            "published": "2023-12-05T01:47:38Z",
            "summary": "Prediction of ligand binding sites of proteins is a fundamental and important\ntask for understanding the function of proteins and screening potential drugs.\nMost existing methods require experimentally determined protein holo-structures\nas input. However, such structures can be unavailable on novel or less-studied\nproteins. To tackle this limitation, we propose LaMPSite, which only takes\nprotein sequences and ligand molecular graphs as input for ligand binding site\npredictions. The protein sequences are used to retrieve residue-level\nembeddings and contact maps from the pre-trained ESM-2 protein language model.\nThe ligand molecular graphs are fed into a graph neural network to compute\natom-level embeddings. Then we compute and update the protein-ligand\ninteraction embedding based on the protein residue-level embeddings and ligand\natom-level embeddings, and the geometric constraints in the inferred protein\ncontact map and ligand distance map. A final pooling on protein-ligand\ninteraction embedding would indicate which residues belong to the binding\nsites. Without any 3D coordinate information of proteins, our proposed model\nachieves competitive performance compared to baseline methods that require 3D\nprotein structures when predicting binding sites. Given that less than 50% of\nproteins have reliable structure information in the current stage, LaMPSite\nwill provide new opportunities for drug discovery.",
            "author": [
                "Shuo Zhang",
                "Lei Xie"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03016v1",
                "http://arxiv.org/pdf/2312.03016v1"
            ],
            "primary_category": "q-bio.QM",
            "category": [
                "q-bio.QM",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02422v1",
            "title": "ChatGPT in the Classroom: Boon or Bane for Physics Students' Academic\n  Performance?",
            "updated": "2023-12-05T01:46:28Z",
            "published": "2023-12-05T01:46:28Z",
            "summary": "This study investigates the influence of ChatGPT, an AI-based language model,\non student performance in a physics course. We conducted an experimental\nanalysis with two cohorts of students in a second-semester engineering physics\ncourse. The control group (Physics 2 2022B) used traditional teaching methods,\nwhile the experimental group (Physics 2 2023A) integrated ChatGPT as a learning\ntool. Our results indicate that the use of ChatGPT led to a significant\ndecrease in student performance, as evidenced by lower grades and negative Hake\nfactors compared to the control group. In addition, a survey of students\nrevealed conflicting perceptions of the usefulness of ChatGPT in teaching\nphysics. While most recognized its usefulness in understanding concepts and\nproviding information, concerns were raised about its potential to reduce\ncritical thinking and independent learning. These findings suggest that while\nChatGPT can be a useful tool, it should be used with caution and as a\nsupplement to traditional teaching methods, rather than as a stand-alone\nsolution. The study underlines the importance of critical and reflective use of\nAI tools in educational settings and highlights the irreplaceable role of\nteachers in providing comprehensive educational support.",
            "author": [
                "Manuel G. Forero",
                "H. J. Herrera-Su\u00e1rez"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02422v1",
                "http://arxiv.org/pdf/2312.02422v1"
            ],
            "primary_category": "physics.ed-ph",
            "category": [
                "physics.ed-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02420v1",
            "title": "Towards Granularity-adjusted Pixel-level Semantic Annotation",
            "updated": "2023-12-05T01:37:18Z",
            "published": "2023-12-05T01:37:18Z",
            "summary": "Recent advancements in computer vision predominantly rely on learning-based\nsystems, leveraging annotations as the driving force to develop specialized\nmodels. However, annotating pixel-level information, particularly in semantic\nsegmentation, presents a challenging and labor-intensive task, prompting the\nneed for autonomous processes. In this work, we propose GranSAM which\ndistinguishes itself by providing semantic segmentation at the user-defined\ngranularity level on unlabeled data without the need for any manual\nsupervision, offering a unique contribution in the realm of semantic mask\nannotation method. Specifically, we propose an approach to enable the Segment\nAnything Model (SAM) with semantic recognition capability to generate\npixel-level annotations for images without any manual supervision. For this, we\naccumulate semantic information from synthetic images generated by the Stable\nDiffusion model or web crawled images and employ this data to learn a mapping\nfunction between SAM mask embeddings and object class labels. As a result, SAM,\nenabled with granularity-adjusted mask recognition, can be used for pixel-level\nsemantic annotation purposes. We conducted experiments on the PASCAL VOC 2012\nand COCO-80 datasets and observed a +17.95% and +5.17% increase in mIoU,\nrespectively, compared to existing state-of-the-art methods when evaluated\nunder our problem setting.",
            "author": [
                "Rohit Kundu",
                "Sudipta Paul",
                "Rohit Lal",
                "Amit K. Roy-Chowdhury"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02420v1",
                "http://arxiv.org/pdf/2312.02420v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02419v1",
            "title": "Human Demonstrations are Generalizable Knowledge for Robots",
            "updated": "2023-12-05T01:35:39Z",
            "published": "2023-12-05T01:35:39Z",
            "summary": "Learning from human demonstrations is an emerging trend for designing\nintelligent robotic systems. However, previous methods typically regard videos\nas instructions, simply dividing them into action sequences for robotic\nrepetition, which poses obstacles to generalization to diverse tasks or object\ninstances. In this paper, we propose a different perspective, considering human\ndemonstration videos not as mere instructions, but as a source of knowledge for\nrobots. Motivated by this perspective and the remarkable comprehension and\ngeneralization capabilities exhibited by large language models (LLMs), we\npropose DigKnow, a method that DIstills Generalizable KNOWledge with a\nhierarchical structure. Specifically, DigKnow begins by converting human\ndemonstration video frames into observation knowledge. This knowledge is then\nsubjected to analysis to extract human action knowledge and further distilled\ninto pattern knowledge compassing task and object instances, resulting in the\nacquisition of generalizable knowledge with a hierarchical structure. In\nsettings with different tasks or object instances, DigKnow retrieves relevant\nknowledge for the current task and object instances. Subsequently, the\nLLM-based planner conducts planning based on the retrieved knowledge, and the\npolicy executes actions in line with the plan to achieve the designated task.\nUtilizing the retrieved knowledge, we validate and rectify planning and\nexecution outcomes, resulting in a substantial enhancement of the success rate.\nExperimental results across a range of tasks and scenes demonstrate the\neffectiveness of this approach in facilitating real-world robots to accomplish\ntasks with the knowledge derived from human demonstrations.",
            "author": [
                "Guangyan Chen",
                "Te Cui",
                "Tianxing Zhou",
                "Zicai Peng",
                "Mengxiao Hu",
                "Meiling Wang",
                "Yi Yang",
                "Yufeng Yue"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02419v1",
                "http://arxiv.org/pdf/2312.02419v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03015v1",
            "title": "PartSLIP++: Enhancing Low-Shot 3D Part Segmentation via Multi-View\n  Instance Segmentation and Maximum Likelihood Estimation",
            "updated": "2023-12-05T01:33:04Z",
            "published": "2023-12-05T01:33:04Z",
            "summary": "Open-world 3D part segmentation is pivotal in diverse applications such as\nrobotics and AR/VR. Traditional supervised methods often grapple with limited\n3D data availability and struggle to generalize to unseen object categories.\nPartSLIP, a recent advancement, has made significant strides in zero- and\nfew-shot 3D part segmentation. This is achieved by harnessing the capabilities\nof the 2D open-vocabulary detection module, GLIP, and introducing a heuristic\nmethod for converting and lifting multi-view 2D bounding box predictions into\n3D segmentation masks. In this paper, we introduce PartSLIP++, an enhanced\nversion designed to overcome the limitations of its predecessor. Our approach\nincorporates two major improvements. First, we utilize a pre-trained 2D\nsegmentation model, SAM, to produce pixel-wise 2D segmentations, yielding more\nprecise and accurate annotations than the 2D bounding boxes used in PartSLIP.\nSecond, PartSLIP++ replaces the heuristic 3D conversion process with an\ninnovative modified Expectation-Maximization algorithm. This algorithm\nconceptualizes 3D instance segmentation as unobserved latent variables, and\nthen iteratively refines them through an alternating process of 2D-3D matching\nand optimization with gradient descent. Through extensive evaluations, we show\nthat PartSLIP++ demonstrates better performance over PartSLIP in both low-shot\n3D semantic and instance-based object part segmentation tasks. Code released at\nhttps://github.com/zyc00/PartSLIP2.",
            "author": [
                "Yuchen Zhou",
                "Jiayuan Gu",
                "Xuanlin Li",
                "Minghua Liu",
                "Yunhao Fang",
                "Hao Su"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03015v1",
                "http://arxiv.org/pdf/2312.03015v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02418v1",
            "title": "Decoding Data Quality via Synthetic Corruptions: Embedding-guided\n  Pruning of Code Data",
            "updated": "2023-12-05T01:19:30Z",
            "published": "2023-12-05T01:19:30Z",
            "summary": "Code datasets, often collected from diverse and uncontrolled sources such as\nGitHub, potentially suffer from quality issues, thereby affecting the\nperformance and training efficiency of Large Language Models (LLMs) optimized\nfor code generation. Previous studies demonstrated the benefit of using\nembedding spaces for data pruning, but they mainly focused on duplicate removal\nor increasing variety, and in other modalities, such as images. Our work\nfocuses on using embeddings to identify and remove \"low-quality\" code data.\nFirst, we explore features of \"low-quality\" code in embedding space, through\nthe use of synthetic corruptions. Armed with this knowledge, we devise novel\npruning metrics that operate in embedding space to identify and remove\nlow-quality entries in the Stack dataset. We demonstrate the benefits of this\nsynthetic corruption informed pruning (SCIP) approach on the well-established\nHumanEval and MBPP benchmarks, outperforming existing embedding-based methods.\nImportantly, we achieve up to a 3% performance improvement over no pruning,\nthereby showing the promise of insights from synthetic corruptions for data\npruning.",
            "author": [
                "Yu Yang",
                "Aaditya K. Singh",
                "Mostafa Elhoushi",
                "Anas Mahmoud",
                "Kushal Tirumala",
                "Fabian Gloeckle",
                "Baptiste Rozi\u00e8re",
                "Carole-Jean Wu",
                "Ari S. Morcos",
                "Newsha Ardalani"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02418v1",
                "http://arxiv.org/pdf/2312.02418v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02417v1",
            "title": "Near-Optimal Mean Estimation with Unknown, Heteroskedastic Variances",
            "updated": "2023-12-05T01:13:10Z",
            "published": "2023-12-05T01:13:10Z",
            "summary": "Given data drawn from a collection of Gaussian variables with a common mean\nbut different and unknown variances, what is the best algorithm for estimating\ntheir common mean? We present an intuitive and efficient algorithm for this\ntask. As different closed-form guarantees can be hard to compare, the\nSubset-of-Signals model serves as a benchmark for heteroskedastic mean\nestimation: given $n$ Gaussian variables with an unknown subset of $m$\nvariables having variance bounded by 1, what is the optimal estimation error as\na function of $n$ and $m$? Our algorithm resolves this open question up to\nlogarithmic factors, improving upon the previous best known estimation error by\npolynomial factors when $m = n^c$ for all $0<c<1$. Of particular note, we\nobtain error $o(1)$ with $m = \\tilde{O}(n^{1/4})$ variance-bounded samples,\nwhereas previous work required $m = \\tilde{\\Omega}(n^{1/2})$. Finally, we show\nthat in the multi-dimensional setting, even for $d=2$, our techniques enable\nrates comparable to knowing the variance of each sample.",
            "author": [
                "Spencer Compton",
                "Gregory Valiant"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02417v1",
                "http://arxiv.org/pdf/2312.02417v1"
            ],
            "primary_category": "math.ST",
            "category": [
                "math.ST",
                "cs.DS",
                "stat.ML",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02416v1",
            "title": "Towards Fast and Stable Federated Learning: Confronting Heterogeneity\n  via Knowledge Anchor",
            "updated": "2023-12-05T01:12:56Z",
            "published": "2023-12-05T01:12:56Z",
            "summary": "Federated learning encounters a critical challenge of data heterogeneity,\nadversely affecting the performance and convergence of the federated model.\nVarious approaches have been proposed to address this issue, yet their\neffectiveness is still limited. Recent studies have revealed that the federated\nmodel suffers severe forgetting in local training, leading to global forgetting\nand performance degradation. Although the analysis provides valuable insights,\na comprehensive understanding of the vulnerable classes and their impact\nfactors is yet to be established. In this paper, we aim to bridge this gap by\nsystematically analyzing the forgetting degree of each class during local\ntraining across different communication rounds. Our observations are: (1) Both\nmissing and non-dominant classes suffer similar severe forgetting during local\ntraining, while dominant classes show improvement in performance. (2) When\ndynamically reducing the sample size of a dominant class, catastrophic\nforgetting occurs abruptly when the proportion of its samples is below a\ncertain threshold, indicating that the local model struggles to leverage a few\nsamples of a specific class effectively to prevent forgetting. Motivated by\nthese findings, we propose a novel and straightforward algorithm called\nFederated Knowledge Anchor (FedKA). Assuming that all clients have a single\nshared sample for each class, the knowledge anchor is constructed before each\nlocal training stage by extracting shared samples for missing classes and\nrandomly selecting one sample per class for non-dominant classes. The knowledge\nanchor is then utilized to correct the gradient of each mini-batch towards the\ndirection of preserving the knowledge of the missing and non-dominant classes.\nExtensive experimental results demonstrate that our proposed FedKA achieves\nfast and stable convergence, significantly improving accuracy on popular\nbenchmarks.",
            "author": [
                "Jinqian Chen",
                "Jihua Zhu",
                "Qinghai Zheng"
            ],
            "link": [
                "http://dx.doi.org/10.1145/3581783.3612597",
                "http://arxiv.org/abs/2312.02416v1",
                "http://arxiv.org/pdf/2312.02416v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "68T99"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03014v1",
            "title": "Foundation Models for Weather and Climate Data Understanding: A\n  Comprehensive Survey",
            "updated": "2023-12-05T01:10:54Z",
            "published": "2023-12-05T01:10:54Z",
            "summary": "As artificial intelligence (AI) continues to rapidly evolve, the realm of\nEarth and atmospheric sciences is increasingly adopting data-driven models,\npowered by progressive developments in deep learning (DL). Specifically, DL\ntechniques are extensively utilized to decode the chaotic and nonlinear aspects\nof Earth systems, and to address climate challenges via understanding weather\nand climate data. Cutting-edge performance on specific tasks within narrower\nspatio-temporal scales has been achieved recently through DL. The rise of large\nmodels, specifically large language models (LLMs), has enabled fine-tuning\nprocesses that yield remarkable outcomes across various downstream tasks,\nthereby propelling the advancement of general AI. However, we are still\nnavigating the initial stages of crafting general AI for weather and climate.\nIn this survey, we offer an exhaustive, timely overview of state-of-the-art AI\nmethodologies specifically engineered for weather and climate data, with a\nspecial focus on time series and text data. Our primary coverage encompasses\nfour critical aspects: types of weather and climate data, principal model\narchitectures, model scopes and applications, and datasets for weather and\nclimate. Furthermore, in relation to the creation and application of foundation\nmodels for weather and climate data understanding, we delve into the field's\nprevailing challenges, offer crucial insights, and propose detailed avenues for\nfuture research. This comprehensive approach equips practitioners with the\nrequisite knowledge to make substantial progress in this domain. Our survey\nencapsulates the most recent breakthroughs in research on large, data-driven\nmodels for weather and climate data understanding, emphasizing robust\nfoundations, current advancements, practical applications, crucial resources,\nand prospective research opportunities.",
            "author": [
                "Shengchao Chen",
                "Guodong Long",
                "Jing Jiang",
                "Dikai Liu",
                "Chengqi Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03014v1",
                "http://arxiv.org/pdf/2312.03014v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CV",
                "physics.ao-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02407v1",
            "title": "Robust Clustering using Hyperdimensional Computing",
            "updated": "2023-12-05T00:46:29Z",
            "published": "2023-12-05T00:46:29Z",
            "summary": "This paper addresses the clustering of data in the hyperdimensional computing\n(HDC) domain. In prior work, an HDC-based clustering framework, referred to as\nHDCluster, has been proposed. However, the performance of the existing\nHDCluster is not robust. The performance of HDCluster is degraded as the\nhypervectors for the clusters are chosen at random during the initialization\nstep. To overcome this bottleneck, we assign the initial cluster hypervectors\nby exploring the similarity of the encoded data, referred to as \\textit{query}\nhypervectors. Intra-cluster hypervectors have a higher similarity than\ninter-cluster hypervectors. Harnessing the similarity results among query\nhypervectors, this paper proposes four HDC-based clustering algorithms:\nsimilarity-based k-means, equal bin-width histogram, equal bin-height\nhistogram, and similarity-based affinity propagation. Experimental results\nillustrate that: (i) Compared to the existing HDCluster, our proposed HDC-based\nclustering algorithms can achieve better accuracy, more robust performance,\nfewer iterations, and less execution time. Similarity-based affinity\npropagation outperforms the other three HDC-based clustering algorithms on\neight datasets by 2~38% in clustering accuracy. (ii) Even for one-pass\nclustering, i.e., without any iterative update of the cluster hypervectors, our\nproposed algorithms can provide more robust clustering accuracy than HDCluster.\n(iii) Over eight datasets, five out of eight can achieve higher or comparable\naccuracy when projected onto the hyperdimensional space. Traditional clustering\nis more desirable than HDC when the number of clusters, $k$, is large.",
            "author": [
                "Lulu Ge",
                "Keshab K. Parhi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02407v1",
                "http://arxiv.org/pdf/2312.02407v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.DB",
                "cs.SC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02406v1",
            "title": "Efficient Online Data Mixing For Language Model Pre-Training",
            "updated": "2023-12-05T00:42:35Z",
            "published": "2023-12-05T00:42:35Z",
            "summary": "The data used to pretrain large language models has a decisive impact on a\nmodel's downstream performance, which has led to a large body of work on data\nselection methods that aim to automatically determine the most suitable data to\nuse for pretraining. Existing data selection methods suffer from slow and\ncomputationally expensive processes, a problem amplified by the increasing size\nof models and of pretraining datasets. Data mixing, on the other hand, reduces\nthe complexity of data selection by grouping data points together and\ndetermining sampling probabilities across entire groups. However, data mixing\nproportions are typically fixed before training and therefore cannot adapt to\nchanging training dynamics. To address these limitations, we develop an\nefficient algorithm for Online Data Mixing (ODM) that combines elements from\nboth data selection and data mixing. Based on multi-armed bandit algorithms,\nour online approach optimizes the data mixing proportions during training.\nRemarkably, our method trains a model that reaches the final perplexity of the\nnext best method with 19\\% fewer training iterations, and improves performance\non the 5-shot MMLU benchmark by 1.9% relative accuracy, while adding negligible\nwall-clock time during pretraining.",
            "author": [
                "Alon Albalak",
                "Liangming Pan",
                "Colin Raffel",
                "William Yang Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02406v1",
                "http://arxiv.org/pdf/2312.02406v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02405v1",
            "title": "BEDD: The MineRL BASALT Evaluation and Demonstrations Dataset for\n  Training and Benchmarking Agents that Solve Fuzzy Tasks",
            "updated": "2023-12-05T00:29:44Z",
            "published": "2023-12-05T00:29:44Z",
            "summary": "The MineRL BASALT competition has served to catalyze advances in learning\nfrom human feedback through four hard-to-specify tasks in Minecraft, such as\ncreate and photograph a waterfall. Given the completion of two years of BASALT\ncompetitions, we offer to the community a formalized benchmark through the\nBASALT Evaluation and Demonstrations Dataset (BEDD), which serves as a resource\nfor algorithm development and performance assessment. BEDD consists of a\ncollection of 26 million image-action pairs from nearly 14,000 videos of human\nplayers completing the BASALT tasks in Minecraft. It also includes over 3,000\ndense pairwise human evaluations of human and algorithmic agents. These\ncomparisons serve as a fixed, preliminary leaderboard for evaluating\nnewly-developed algorithms. To enable this comparison, we present a streamlined\ncodebase for benchmarking new algorithms against the leaderboard. In addition\nto presenting these datasets, we conduct a detailed analysis of the data from\nboth datasets to guide algorithm development and evaluation. The released code\nand data are available at https://github.com/minerllabs/basalt-benchmark .",
            "author": [
                "Stephanie Milani",
                "Anssi Kanervisto",
                "Karolis Ramanauskas",
                "Sander Schulhoff",
                "Brandon Houghton",
                "Rohin Shah"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02405v1",
                "http://arxiv.org/pdf/2312.02405v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03013v1",
            "title": "Breast Ultrasound Report Generation using LangChain",
            "updated": "2023-12-05T00:28:26Z",
            "published": "2023-12-05T00:28:26Z",
            "summary": "Breast ultrasound (BUS) is a critical diagnostic tool in the field of breast\nimaging, aiding in the early detection and characterization of breast\nabnormalities. Interpreting breast ultrasound images commonly involves creating\ncomprehensive medical reports, containing vital information to promptly assess\nthe patient's condition. However, the ultrasound imaging system necessitates\ncapturing multiple images of various parts to compile a single report,\npresenting a time-consuming challenge. To address this problem, we propose the\nintegration of multiple image analysis tools through a LangChain using Large\nLanguage Models (LLM), into the breast reporting process. Through a combination\nof designated tools and text generation through LangChain, our method can\naccurately extract relevant features from ultrasound images, interpret them in\na clinical context, and produce comprehensive and standardized reports. This\napproach not only reduces the burden on radiologists and healthcare\nprofessionals but also enhances the consistency and quality of reports. The\nextensive experiments shows that each tools involved in the proposed method can\noffer qualitatively and quantitatively significant results. Furthermore,\nclinical evaluation on the generated reports demonstrates that the proposed\nmethod can make report in clinically meaningful way.",
            "author": [
                "Jaeyoung Huh",
                "Hyun Jeong Park",
                "Jong Chul Ye"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03013v1",
                "http://arxiv.org/pdf/2312.03013v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.AI",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02403v1",
            "title": "Deep Neural Operator Enabled Concurrent Multitask Design for\n  Multifunctional Metamaterials under Heterogeneous Fields",
            "updated": "2023-12-05T00:12:41Z",
            "published": "2023-12-05T00:12:41Z",
            "summary": "Multifunctional metamaterials (MMM) bear promise as next-generation material\nplatforms supporting miniaturization and customization. Despite many\nproof-of-concept demonstrations and the proliferation of deep learning assisted\ndesign, grand challenges of inverse design for MMM, especially those involving\nheterogeneous fields possibly subject to either mutual meta-atom coupling or\nlong-range interactions, remain largely under-explored. To this end, we present\na data-driven design framework, which streamlines the inverse design of MMMs\ninvolving heterogeneous fields. A core enabler is implicit Fourier neural\noperator (IFNO), which predicts heterogeneous fields distributed across a\nmetamaterial array, thus in general at odds with homogenization assumptions, in\na parameter-/sample-efficient fashion. Additionally, we propose a standard\nformulation of inverse problem covering a broad class of MMMs, and\ngradient-based multitask concurrent optimization identifying a set of\nPareto-optimal architecture-stimulus (A-S) pairs. Fourier multiclass blending\nis proposed to synthesize inter-class meta-atoms anchored on a set of geometric\nmotifs, while enjoying training-free dimension reduction and built-it\nreconstruction. Interlocking the three pillars, the framework is validated for\nlight-bylight programmable plasmonic nanoantenna, whose design involves vast\nspace jointly spanned by quasi-freeform supercells, maneuverable incident phase\ndistributions, and conflicting figure-of-merits involving on-demand\nlocalization patterns. Accommodating all the challenges without a-priori\nsimplifications, our framework could propel future advancements of MMM.",
            "author": [
                "Doksoo Lee",
                "Lu Zhang",
                "Yue Yu",
                "Wei Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02403v1",
                "http://arxiv.org/pdf/2312.02403v1"
            ],
            "primary_category": "cs.CE",
            "category": [
                "cs.CE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02401v1",
            "title": "Harmonizing Global Voices: Culturally-Aware Models for Enhanced Content\n  Moderation",
            "updated": "2023-12-05T00:11:09Z",
            "published": "2023-12-05T00:11:09Z",
            "summary": "Content moderation at scale faces the challenge of considering local cultural\ndistinctions when assessing content. While global policies aim to maintain\ndecision-making consistency and prevent arbitrary rule enforcement, they often\noverlook regional variations in interpreting natural language as expressed in\ncontent. In this study, we are looking into how moderation systems can tackle\nthis issue by adapting to local comprehension nuances. We train large language\nmodels on extensive datasets of media news and articles to create culturally\nattuned models. The latter aim to capture the nuances of communication across\ngeographies with the goal of recognizing cultural and societal variations in\nwhat is considered offensive content. We further explore the capability of\nthese models to generate explanations for instances of content violation,\naiming to shed light on how policy guidelines are perceived when cultural and\nsocietal contexts change. We find that training on extensive media datasets\nsuccessfully induced cultural awareness and resulted in improvements in\nhandling content violations on a regional basis. Additionally, these\nadvancements include the ability to provide explanations that align with the\nspecific local norms and nuances as evidenced by the annotators' preference in\nour conducted study. This multifaceted success reinforces the critical role of\nan adaptable content moderation approach in keeping pace with the ever-evolving\nnature of the content it oversees.",
            "author": [
                "Alex J. Chan",
                "Jos\u00e9 Luis Redondo Garc\u00eda",
                "Fabrizio Silvestri",
                "Colm O'Donnel",
                "Konstantina Palla"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02401v1",
                "http://arxiv.org/pdf/2312.02401v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02400v1",
            "title": "Auto DP-SGD: Dual Improvements of Privacy and Accuracy via Automatic\n  Clipping Threshold and Noise Multiplier Estimation",
            "updated": "2023-12-05T00:09:57Z",
            "published": "2023-12-05T00:09:57Z",
            "summary": "DP-SGD has emerged as a popular method to protect personally identifiable\ninformation in deep learning applications. Unfortunately, DP-SGD's per-sample\ngradient clipping and uniform noise addition during training can significantly\ndegrade model utility. To enhance the model's utility, researchers proposed\nvarious adaptive DP-SGD methods. However, we examine and discover that these\ntechniques result in greater privacy leakage or lower accuracy than the\ntraditional DP-SGD method, or a lack of evaluation on a complex data set such\nas CIFAR100. To address these limitations, we propose an Auto DP-SGD. Our\nmethod automates clipping threshold estimation based on the DL model's gradient\nnorm and scales the gradients of each training sample without losing gradient\ninformation. This helps to improve the algorithm's utility while using a less\nprivacy budget. To further improve accuracy, we introduce automatic noise\nmultiplier decay mechanisms to decrease the noise multiplier after every epoch.\nFinally, we develop closed-form mathematical expressions using tCDP accountant\nfor automatic noise multiplier and automatic clipping threshold estimation.\nThrough extensive experimentation, we demonstrate that Auto DP-SGD outperforms\nexisting SOTA DP-SGD methods in privacy and accuracy on various benchmark\ndatasets. We also show that privacy can be improved by lowering the scale\nfactor and using learning rate schedulers without significantly reducing\naccuracy. Specifically, Auto DP-SGD, when used with a step noise multiplier,\nimproves accuracy by 3.20, 1.57, 6.73, and 1.42 for the MNIST, CIFAR10,\nCIFAR100, and AG News Corpus datasets, respectively. Furthermore, it obtains a\nsubstantial reduction in the privacy budget of 94.9, 79.16, 67.36, and 53.37\nfor the corresponding data sets.",
            "author": [
                "Sai Venkatesh Chilukoti",
                "Md Imran Hossen",
                "Liqun Shan",
                "Vijay Srinivas Tida",
                "Xiai Hei"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02400v1",
                "http://arxiv.org/pdf/2312.02400v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CR",
                "26, 40"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02399v1",
            "title": "Primordial fluctuations from quantum gravity: 16-cell topological model",
            "updated": "2023-12-04T23:53:59Z",
            "published": "2023-12-04T23:53:59Z",
            "summary": "We present a numerical analysis of an Hartle-Hawking state for the early\nuniverse, in the deep quantum regime, computed using the covariant Loop Quantum\nGravity formalism, in a truncation defined by 16-cell and in a simplified case\nwhere the dynamics is defined by SU(2) BF theory. We compute mean geometry,\nfluctuations and correlations. The results are consistent with the hypothesis\nthat refining the triangulation does not affect the global physical picture\nsubstantially.",
            "author": [
                "Pietropaolo Frisoni",
                "Francesco Gozzini",
                "Francesca Vidotto"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02399v1",
                "http://arxiv.org/pdf/2312.02399v1"
            ],
            "primary_category": "gr-qc",
            "category": [
                "gr-qc"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02396v1",
            "title": "Unsupervised Change Detection for Space Habitats Using 3D Point Clouds",
            "updated": "2023-12-04T23:26:12Z",
            "published": "2023-12-04T23:26:12Z",
            "summary": "This work presents an algorithm for scene change detection from point clouds\nto enable autonomous robotic caretaking in future space habitats. Autonomous\nrobotic systems will help maintain future deep-space habitats, such as the\nGateway space station, which will be uncrewed for extended periods. Existing\nscene analysis software used on the International Space Station (ISS) relies on\nmanually-labeled images for detecting changes. In contrast, the algorithm\npresented in this work uses raw, unlabeled point clouds as inputs. The\nalgorithm first applies modified Expectation-Maximization Gaussian Mixture\nModel (GMM) clustering to two input point clouds. It then performs change\ndetection by comparing the GMMs using the Earth Mover's Distance. The algorithm\nis validated quantitatively and qualitatively using a test dataset collected by\nan Astrobee robot in the NASA Ames Granite Lab comprising single frame depth\nimages taken directly by Astrobee and full-scene reconstructed maps built with\nRGB-D and pose data from Astrobee. The runtimes of the approach are also\nanalyzed in depth. The source code is publicly released to promote further\ndevelopment.",
            "author": [
                "Jamie Santos",
                "Holly Dinkel",
                "Julia Di",
                "Paulo V. K. Borges",
                "Marina Moreira",
                "Oleg Alexandrov",
                "Brian Coltin",
                "Trey Smith"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02396v1",
                "http://arxiv.org/pdf/2312.02396v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02387v1",
            "title": "Dissecting Medical Referral Mechanisms in Health Services: Role of\n  Physician Professional Networks",
            "updated": "2023-12-04T23:03:09Z",
            "published": "2023-12-04T23:03:09Z",
            "summary": "Medical referrals between primary care physicians (PC) and specialist care\n(SC) physicians profoundly impact patient care regarding quality, satisfaction,\nand cost. This paper investigates the influence of professional networks among\nmedical doctors on referring patients from PC to SC. Using five-year\nconsultation data from a Portuguese private health provider, we conducted\nexploratory data analysis and constructed both professional and referral\nnetworks among physicians. We then apply Graph Neural Network (GNN) models to\nlearn latent representations of the referral network. Our analysis supports the\nhypothesis that doctors' professional social connections can predict medical\nreferrals, potentially enhancing collaboration within organizations and\nimproving healthcare services. This research contributes to dissecting the\nunderlying mechanisms in primary-specialty referrals, thereby providing\nvaluable insights for enhancing patient care and effective healthcare\nmanagement.",
            "author": [
                "Regina de Brito Duarte",
                "Qiwei Han",
                "Claudia Soares"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02387v1",
                "http://arxiv.org/pdf/2312.02387v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY",
                "cs.LG",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02380v1",
            "title": "FaultFormer: Transformer-based Prediction of Bearing Faults",
            "updated": "2023-12-04T22:51:02Z",
            "published": "2023-12-04T22:51:02Z",
            "summary": "The growth of deep learning in the past decade has motivated important\napplications to smart manufacturing and machine health monitoring. In\nparticular, vibration data offers a rich and reliable source to provide\nmeaningful insights into machine health and predictive maintenance. In this\nwork, we present a Transformer based framework for analyzing vibration signals\nto predict different types of bearing faults (FaultFormer). In particular, we\nprocess signal data using data augmentations and extract their Fourier modes to\ntrain a transformer encoder to achieve state of the art accuracies. The\nattention mechanism as well as model outputs were analyzed to confirm the\ntransformer's ability to automatically extract features within signals and\nlearn both global and local relationships to make classifications. Lastly, two\npretraining strategies were proposed to pave the way for large, generalizable\ntransformers that could adapt to new data, situations, or machinery on the\nproduction floor.",
            "author": [
                "Anthony Zhou",
                "Amir Barati Farimani"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02380v1",
                "http://arxiv.org/pdf/2312.02380v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02375v1",
            "title": "CityTFT: Temporal Fusion Transformer for Urban Building Energy Modeling",
            "updated": "2023-12-04T22:19:03Z",
            "published": "2023-12-04T22:19:03Z",
            "summary": "Urban Building Energy Modeling (UBEM) is an emerging method to investigate\nurban design and energy systems against the increasing energy demand at urban\nand neighborhood levels. However, current UBEM methods are mostly physic-based\nand time-consuming in multiple climate change scenarios. This work proposes\nCityTFT, a data-driven UBEM framework, to accurately model the energy demands\nin urban environments. With the empowerment of the underlying TFT framework and\nan augmented loss function, CityTFT could predict heating and cooling triggers\nin unseen climate dynamics with an F1 score of 99.98 \\% while RMSE of loads of\n13.57 kWh.",
            "author": [
                "Ting-Yu Dai",
                "Dev Niyogi",
                "Zoltan Nagy"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02375v1",
                "http://arxiv.org/pdf/2312.02375v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02372v1",
            "title": "On the Trade-Off between Stability and Representational Capacity in\n  Graph Neural Networks",
            "updated": "2023-12-04T22:07:17Z",
            "published": "2023-12-04T22:07:17Z",
            "summary": "Analyzing the stability of graph neural networks (GNNs) under topological\nperturbations is key to understanding their transferability and the role of\neach architecture component. However, stability has been investigated only for\nparticular architectures, questioning whether it holds for a broader spectrum\nof GNNs or only for a few instances. To answer this question, we study the\nstability of EdgeNet: a general GNN framework that unifies more than twenty\nsolutions including the convolutional and attention-based classes, as well as\ngraph isomorphism networks and hybrid architectures. We prove that all GNNs\nwithin the EdgeNet framework are stable to topological perturbations. By\nstudying the effect of different EdgeNet categories on the stability, we show\nthat GNNs with fewer degrees of freedom in their parameter space, linked to a\nlower representational capacity, are more stable. The key factor yielding this\ntrade-off is the eigenvector misalignment between the EdgeNet parameter\nmatrices and the graph shift operator. For example, graph convolutional neural\nnetworks that assign a single scalar per signal shift (hence, with a perfect\nalignment) are more stable than the more involved node or edge-varying\ncounterparts. Extensive numerical results corroborate our theoretical findings\nand highlight the role of different architecture components in the trade-off.",
            "author": [
                "Zhan Gao",
                "Amanda Prorok",
                "Elvin Isufi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02372v1",
                "http://arxiv.org/pdf/2312.02372v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02369v1",
            "title": "Energy as a source of pre-service teachers' conceptions about\n  radioactivity",
            "updated": "2023-12-04T21:51:44Z",
            "published": "2023-12-04T21:51:44Z",
            "summary": "Although researchers have extensively studied student conceptions of\nradioactivity, the conceptions held by pre-service teachers on this subject are\nlargely absent from the literature. We conducted a qualitative content analysis\nof problem-centered interviews with pre-service teachers N = 13 to establish\nwhich conceptions are held by pre-service teachers and to examine these\nconceptions' structure in coordination classes. As has already been observed in\nstudents, some pre-service teachers inadequately differentiate between\nradioactive matter and ionizing radiation and between fission and decay. We\nalso observed that pre-service teachers tend to describe the activation of\nmaterials due to ionizing radiation despite having previously denied an\nactivation, thus showing that the conception of activation of materials can\nreemerge in particular framings. Within the interviews conducted, the concept\nof energy emerged as a central coordination class regarding radioactivity. This\ncoordination class appeared across contexts and proved fruitful in explaining\npre-service teachers' conceptions about radioactivity. We will use the results\nfrom this study to develop a teaching-learning laboratory for pre-service\nteachers in which they can actively study high school students' conceptions\nwhile reflecting on their own. In this way, these findings will contribute to\nimproving the structure of nuclear physics courses at the university.",
            "author": [
                "Axel-Thilo Prokop",
                "Ronny Nawrodt"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02369v1",
                "http://arxiv.org/pdf/2312.02369v1"
            ],
            "primary_category": "physics.ed-ph",
            "category": [
                "physics.ed-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02368v1",
            "title": "RINAS: Training with Dataset Shuffling Can Be General and Fast",
            "updated": "2023-12-04T21:50:08Z",
            "published": "2023-12-04T21:50:08Z",
            "summary": "Deep learning datasets are expanding at an unprecedented pace, creating new\nchallenges for data processing in model training pipelines. A crucial aspect of\nthese pipelines is dataset shuffling, which significantly improves unbiased\nlearning and convergence accuracy by adhering to the principles of random\nsampling. However, loading shuffled data for large datasets incurs significant\noverhead in the deep learning pipeline and severely impacts the end-to-end\ntraining throughput. To mitigate this, current deep learning systems often\nresort to partial dataset shuffling, sacrificing global randomness to maintain\nacceptable training throughput on large datasets, still leaving global\nshuffling efficiency issues not fully explored.\n  In this work, we present RINAS, a data loading framework that systematically\naddresses the performance bottleneck of loading global shuffled datasets. Our\nkey contribution is to offer an intra-batch unordered data fetching approach,\nwhich unleashes unexplored parallelism of data loading. We implement RINAS\nunder the PyTorch framework for common dataset libraries HuggingFace and\nTorchVision. Our experimental results show that RINAS improves the throughput\nof general language model training and vision model training by up to 59% and\n89%, respectively.",
            "author": [
                "Tianle Zhong",
                "Jiechen Zhao",
                "Xindi Guo",
                "Qiang Su",
                "Geoffrey Fox"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02368v1",
                "http://arxiv.org/pdf/2312.02368v1"
            ],
            "primary_category": "cs.DB",
            "category": [
                "cs.DB",
                "cs.DC",
                "cs.LG",
                "cs.PF"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02367v1",
            "title": "States as goal-directed concepts: an epistemic approach to\n  state-representation learning",
            "updated": "2023-12-04T21:48:38Z",
            "published": "2023-12-04T21:48:38Z",
            "summary": "Our goals fundamentally shape how we experience the world. For example, when\nwe are hungry, we tend to view objects in our environment according to whether\nor not they are edible (or tasty). Alternatively, when we are cold, we may view\nthe very same objects according to their ability to produce heat. Computational\ntheories of learning in cognitive systems, such as reinforcement learning, use\nthe notion of \"state-representation\" to describe how agents decide which\nfeatures of their environment are behaviorally-relevant and which can be\nignored. However, these approaches typically assume \"ground-truth\" state\nrepresentations that are known by the agent, and reward functions that need to\nbe learned. Here we suggest an alternative approach in which\nstate-representations are not assumed veridical, or even pre-defined, but\nrather emerge from the agent's goals through interaction with its environment.\nWe illustrate this novel perspective by inferring the goals driving rat\nbehavior in an odor-guided choice task and discuss its implications for\ndeveloping, from first principles, an information-theoretic account of\ngoal-directed state representation learning and behavior.",
            "author": [
                "Nadav Amir",
                "Yael Niv",
                "Angela Langdon"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02367v1",
                "http://arxiv.org/pdf/2312.02367v1"
            ],
            "primary_category": "q-bio.NC",
            "category": [
                "q-bio.NC",
                "cs.IT",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02366v1",
            "title": "Towards General Purpose Vision Foundation Models for Medical Image\n  Analysis: An Experimental Study of DINOv2 on Radiology Benchmarks",
            "updated": "2023-12-04T21:47:10Z",
            "published": "2023-12-04T21:47:10Z",
            "summary": "The integration of deep learning systems into the medical domain has been\nhindered by the resource-intensive process of data annotation and the inability\nof these systems to generalize to different data distributions. Foundation\nmodels, which are models pre-trained on large datasets, have emerged as a\nsolution to reduce reliance on annotated data and enhance model\ngeneralizability and robustness. DINOv2, an open-source foundation model\npre-trained with self-supervised learning on 142 million curated natural\nimages, excels in extracting general-purpose visual representations, exhibiting\npromising capabilities across various vision tasks. Nevertheless, a critical\nquestion remains unanswered regarding DINOv2's adaptability to radiological\nimaging, and the clarity on whether its features are sufficiently general to\nbenefit radiology image analysis is yet to be established. Therefore, this\nstudy comprehensively evaluates DINOv2 for radiology, conducting over 100\nexperiments across diverse modalities (X-ray, CT, and MRI). Tasks include\ndisease classification and organ segmentation on both 2D and 3D images,\nevaluated under different settings like kNN, few-shot learning, linear-probing,\nend-to-end fine-tuning, and parameter-efficient fine-tuning, to measure the\neffectiveness and generalizability of the DINOv2 feature embeddings.\nComparative analyses with established medical image analysis models, U-Net and\nTransUnet for segmentation, and CNN and ViT models pre-trained via supervised,\nweakly supervised, and self-supervised learning for classification, reveal\nDINOv2's superior performance in segmentation tasks and competitive results in\ndisease classification. The findings contribute insights to potential avenues\nfor optimizing pre-training strategies for medical imaging and enhancing the\nbroader understanding of DINOv2's role in bridging the gap between natural and\nradiological image analysis.",
            "author": [
                "Mohammed Baharoon",
                "Waseem Qureshi",
                "Jiahong Ouyang",
                "Yanwu Xu",
                "Kilian Phol",
                "Abdulrhman Aljouie",
                "Wei Peng"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02366v1",
                "http://arxiv.org/pdf/2312.02366v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02365v1",
            "title": "MEDPSeg: End-to-end segmentation of pulmonary structures and lesions in\n  computed tomography",
            "updated": "2023-12-04T21:46:39Z",
            "published": "2023-12-04T21:46:39Z",
            "summary": "The COVID-19 pandemic response highlighted the potential of deep learning\nmethods in facilitating the diagnosis and prognosis of lung diseases through\nautomated segmentation of normal and abnormal tissue in computed tomography\n(CT). Such methods not only have the potential to aid in clinical\ndecision-making but also contribute to the comprehension of novel diseases. In\nlight of the labor-intensive nature of manual segmentation for large chest CT\ncohorts, there is a pressing need for reliable automated approaches that enable\nefficient analysis of chest CT anatomy in vast research databases, especially\nin more scarcely annotated targets such as pneumonia consolidations. A limiting\nfactor for the development of such methods is that most current models optimize\na fixed annotation format per network output. To tackle this problem,\npolymorphic training is used to optimize a network with a fixed number of\noutput channels to represent multiple hierarchical anatomic structures,\nindirectly optimizing more complex labels with simpler annotations. We combined\nover 6000 volumetric CT scans containing varying formats of manual and\nautomated labels from different sources, and used polymorphic training along\nwith multitask learning to develop MEDPSeg, an end-to-end method for the\nsegmentation of lungs, airways, pulmonary artery, and lung lesions with\nseparation of ground glass opacities, and parenchymal consolidations, all in a\nsingle forward prediction. We achieve state-of-the-art performance in multiple\ntargets, particularly in the segmentation of ground glass opacities and\nconsolidations, a challenging problem with limited manual annotation\navailability. In addition, we provide an open-source implementation with a\ngraphical user interface at https://github.com/MICLab-Unicamp/medpseg.",
            "author": [
                "Diedre S. Carmo",
                "Jean Ribeiro",
                "Alejandro P. Comellas",
                "Joseph M. Reinhardt",
                "Sarah E. Gerard",
                "Let\u00edcia Rittner",
                "Roberto A. Lotufo"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02365v1",
                "http://arxiv.org/pdf/2312.02365v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02364v1",
            "title": "Class-Discriminative Attention Maps for Vision Transformers",
            "updated": "2023-12-04T21:46:21Z",
            "published": "2023-12-04T21:46:21Z",
            "summary": "Interpretability methods are critical components for examining and exploring\ndeep neural networks (DNN), as well as increasing our understanding of and\ntrust in them. Vision transformers (ViT), which can be trained to\nstate-of-the-art performance with a self-supervised learning (SSL) training\nmethod, provide built-in attention maps (AM). While AMs can provide\nhigh-quality semantic segmentation of input images, they do not account for any\nsignal coming from a downstream classifier. We introduce class-discriminative\nattention maps (CDAM), a novel post-hoc explanation method that is highly\nsensitive to the target class. Our method essentially scales attention scores\nby how relevant the corresponding tokens are for the predictions of a\nclassifier head. Alternative to classifier outputs, CDAM can also explain a\nuser-defined concept by targeting similarity measures in the latent space of\nthe ViT. This allows for explanations of arbitrary concepts, defined by the\nuser through a few sample images. We investigate the operating characteristics\nof CDAM in comparison with relevance propagation (RP) and token ablation maps\n(TAM), an alternative to pixel occlusion methods. CDAM is highly\nclass-discriminative and semantically relevant, while providing implicit\nregularization of relevance scores.\n  PyTorch implementation: \\url{https://github.com/lenbrocki/CDAM}\n  Web live demo: \\url{https://cdam.informatism.com/}",
            "author": [
                "Lennart Brocki",
                "Neo Christopher Chung"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02364v1",
                "http://arxiv.org/pdf/2312.02364v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02358v1",
            "title": "Peer attention enhances student learning",
            "updated": "2023-12-04T21:36:58Z",
            "published": "2023-12-04T21:36:58Z",
            "summary": "Human visual attention is susceptible to social influences. In education,\npeer effects impact student learning, but their precise role in modulating\nattention remains unclear. Our experiment (N=311) demonstrates that displaying\npeer visual attention regions when students watch online course videos enhances\ntheir focus and engagement. However, students retain adaptability in following\npeer attention cues. Overall, guided peer attention improves learning\nexperiences and outcomes. These findings elucidate how peer visual attention\nshapes students' gaze patterns, deepening understanding of peer influence on\nlearning. They also offer insights into designing adaptive online learning\ninterventions leveraging peer attention modelling to optimize student\nattentiveness and success.",
            "author": [
                "Songlin Xu",
                "Dongyin Hu",
                "Ru Wang",
                "Xinyu Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02358v1",
                "http://arxiv.org/pdf/2312.02358v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02355v1",
            "title": "When is Offline Policy Selection Sample Efficient for Reinforcement\n  Learning?",
            "updated": "2023-12-04T21:35:13Z",
            "published": "2023-12-04T21:35:13Z",
            "summary": "Offline reinforcement learning algorithms often require careful\nhyperparameter tuning. Consequently, before deployment, we need to select\namongst a set of candidate policies. As yet, however, there is little\nunderstanding about the fundamental limits of this offline policy selection\n(OPS) problem. In this work we aim to provide clarity on when sample efficient\nOPS is possible, primarily by connecting OPS to off-policy policy evaluation\n(OPE) and Bellman error (BE) estimation. We first show a hardness result, that\nin the worst case, OPS is just as hard as OPE, by proving a reduction of OPE to\nOPS. As a result, no OPS method can be more sample efficient than OPE in the\nworst case. We then propose a BE method for OPS, called Identifiable BE\nSelection (IBES), that has a straightforward method for selecting its own\nhyperparameters. We highlight that using IBES for OPS generally has more\nrequirements than OPE methods, but if satisfied, can be more sample efficient.\nWe conclude with an empirical study comparing OPE and IBES, and by showing the\ndifficulty of OPS on an offline Atari benchmark dataset.",
            "author": [
                "Vincent Liu",
                "Prabhat Nagarajan",
                "Andrew Patterson",
                "Martha White"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02355v1",
                "http://arxiv.org/pdf/2312.02355v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02352v1",
            "title": "Working Backwards: Learning to Place by Picking",
            "updated": "2023-12-04T21:32:00Z",
            "published": "2023-12-04T21:32:00Z",
            "summary": "We present Learning to Place by Picking (LPP), a method capable of\nautonomously collecting demonstrations for a family of placing tasks in which\nobjects must be manipulated to specific locations. With LPP, we approach the\nlearning of robotic object placement policies by reversing the grasping process\nand exploiting the inherent symmetry of the pick and place problems.\nSpecifically, we obtain placing demonstrations from a set of grasp sequences of\nobjects that are initially located at their target placement locations. Our\nsystem is capable of collecting hundreds of demonstrations without human\nintervention by using a combination of tactile sensing and compliant control\nfor grasps. We train a policy directly from visual observations through\nbehaviour cloning, using the autonomously-collected demonstrations. By doing\nso, the policy can generalize to object placement scenarios outside of the\ntraining environment without privileged information (e.g., placing a plate\npicked up from a table and not at the original placement location). We validate\nour approach on home robotic scenarios that include dishwasher loading and\ntable setting. Our approach yields robotic placing policies that outperform\npolicies trained with kinesthetic teaching, both in terms of performance and\ndata efficiency, while requiring no human supervision.",
            "author": [
                "Oliver Limoyo",
                "Abhisek Konar",
                "Trevor Ablett",
                "Jonathan Kelly",
                "Francois R. Hogan",
                "Gregory Dudek"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02352v1",
                "http://arxiv.org/pdf/2312.02352v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03012v1",
            "title": "A Waddington landscape for prototype learning in generalized Hopfield\n  networks",
            "updated": "2023-12-04T21:28:14Z",
            "published": "2023-12-04T21:28:14Z",
            "summary": "Networks in machine learning offer examples of complex high-dimensional\ndynamical systems reminiscent of biological systems. Here, we study the\nlearning dynamics of Generalized Hopfield networks, which permit a\nvisualization of internal memories. These networks have been shown to proceed\nthrough a 'feature-to-prototype' transition, as the strength of network\nnonlinearity is increased, wherein the learned, or terminal, states of internal\nmemories transition from mixed to pure states. Focusing on the prototype\nlearning dynamics of the internal memories we observe a strong resemblance to\nthe canalized, or low-dimensional, dynamics of cells as they differentiate\nwithin a Waddingtonian landscape. Dynamically, we demonstrate that learning in\na Generalized Hopfield Network proceeds through sequential 'splits' in memory\nspace. Furthermore, order of splitting is interpretable and reproducible. The\ndynamics between the splits are canalized in the Waddington sense -- robust to\nvariations in detailed aspects of the system. In attempting to make the analogy\na rigorous equivalence, we study smaller subsystems that exhibit similar\nproperties to the full system. We combine analytical calculations with\nnumerical simulations to study the dynamical emergence of the\nfeature-to-prototype transition, and the behaviour of splits in the landscape,\nsaddles points, visited during learning. We exhibit regimes where saddles\nappear and disappear through saddle-node bifurcations, qualitatively changing\nthe distribution of learned memories as the strength of the nonlinearity is\nvaried -- allowing us to systematically investigate the mechanisms that\nunderlie the emergence of Waddingtonian dynamics. Memories can thus\ndifferentiate in a predictive and controlled way, revealing new bridges between\nexperimental biology, dynamical systems theory, and machine learning.",
            "author": [
                "Nacer Eddine Boukacem",
                "Allen Leary",
                "Robin Th\u00e9riault",
                "Felix Gottlieb",
                "Madhav Mani",
                "Paul Fran\u00e7ois"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03012v1",
                "http://arxiv.org/pdf/2312.03012v1"
            ],
            "primary_category": "cond-mat.dis-nn",
            "category": [
                "cond-mat.dis-nn",
                "cs.LG",
                "cs.NE",
                "q-bio.QM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02344v1",
            "title": "STEREOFOG -- Computational DeFogging via Image-to-Image Translation on a\n  real-world Dataset",
            "updated": "2023-12-04T21:07:13Z",
            "published": "2023-12-04T21:07:13Z",
            "summary": "Image-to-Image translation (I2I) is a subtype of Machine Learning (ML) that\nhas tremendous potential in applications where two domains of images and the\nneed for translation between the two exist, such as the removal of fog. For\nexample, this could be useful for autonomous vehicles, which currently struggle\nwith adverse weather conditions like fog. However, datasets for I2I tasks are\nnot abundant and typically hard to acquire. Here, we introduce STEREOFOG, a\ndataset comprised of $10,067$ paired fogged and clear images, captured using a\ncustom-built device, with the purpose of exploring I2I's potential in this\ndomain. It is the only real-world dataset of this kind to the best of our\nknowledge. Furthermore, we apply and optimize the pix2pix I2I ML framework to\nthis dataset. With the final model achieving an average Complex\nWavelet-Structural Similarity (CW-SSIM) score of $0.76$, we prove the\ntechnique's suitability for the problem.",
            "author": [
                "Anton Pollak",
                "Rajesh Menon"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02344v1",
                "http://arxiv.org/pdf/2312.02344v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02343v1",
            "title": "Time-based vs. Fingerprinting-based Positioning Using Artificial Neural\n  Networks",
            "updated": "2023-12-04T21:01:51Z",
            "published": "2023-12-04T21:01:51Z",
            "summary": "High-accuracy positioning has gained significant interest for many use-cases\nacross various domains such as industrial internet of things (IIoT), healthcare\nand entertainment. Radio frequency (RF) measurements are widely utilized for\nuser localization. However, challenging radio conditions such as\nnon-line-of-sight (NLOS) and multipath propagation can deteriorate the\npositioning accuracy. Machine learning (ML)-based estimators have been proposed\nto overcome these challenges. RF measurements can be utilized for positioning\nin multiple ways resulting in time-based, angle-based and fingerprinting-based\nmethods. Different methods, however, impose different implementation\nrequirements to the system, and may perform differently in terms of accuracy\nfor a given setting. In this paper, we use artificial neural networks (ANNs) to\nrealize time-of-arrival (ToA)-based and channel impulse response (CIR)\nfingerprinting-based positioning. We compare their performance for different\nindoor environments based on real-world ultra-wideband (UWB) measurements. We\nfirst show that using ML techniques helps to improve the estimation accuracy\ncompared to conventional techniques for time-based positioning. When comparing\ntime-based and fingerprinting schemes using ANNs, we show that the favorable\nmethod in terms of positioning accuracy is different for different\nenvironments, where the accuracy is affected not only by the radio propagation\nconditions but also the density and distribution of reference user locations\nused for fingerprinting.",
            "author": [
                "Anil Kirmaz",
                "Taylan Sahin",
                "Diomidis S. Michalopoulos",
                "Wolfgang Gerstacker"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02343v1",
                "http://arxiv.org/pdf/2312.02343v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02339v1",
            "title": "Expressive Sign Equivariant Networks for Spectral Geometric Learning",
            "updated": "2023-12-04T20:48:18Z",
            "published": "2023-12-04T20:48:18Z",
            "summary": "Recent work has shown the utility of developing machine learning models that\nrespect the structure and symmetries of eigenvectors. These works promote sign\ninvariance, since for any eigenvector v the negation -v is also an eigenvector.\nHowever, we show that sign invariance is theoretically limited for tasks such\nas building orthogonally equivariant models and learning node positional\nencodings for link prediction in graphs. In this work, we demonstrate the\nbenefits of sign equivariance for these tasks. To obtain these benefits, we\ndevelop novel sign equivariant neural network architectures. Our models are\nbased on a new analytic characterization of sign equivariant polynomials and\nthus inherit provable expressiveness properties. Controlled synthetic\nexperiments show that our networks can achieve the theoretically predicted\nbenefits of sign equivariant models. Code is available at\nhttps://github.com/cptq/Sign-Equivariant-Nets.",
            "author": [
                "Derek Lim",
                "Joshua Robinson",
                "Stefanie Jegelka",
                "Haggai Maron"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02339v1",
                "http://arxiv.org/pdf/2312.02339v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02337v1",
            "title": "Measuring Distributional Shifts in Text: The Advantage of Language\n  Model-Based Embeddings",
            "updated": "2023-12-04T20:46:48Z",
            "published": "2023-12-04T20:46:48Z",
            "summary": "An essential part of monitoring machine learning models in production is\nmeasuring input and output data drift. In this paper, we present a system for\nmeasuring distributional shifts in natural language data and highlight and\ninvestigate the potential advantage of using large language models (LLMs) for\nthis problem. Recent advancements in LLMs and their successful adoption in\ndifferent domains indicate their effectiveness in capturing semantic\nrelationships for solving various natural language processing problems. The\npower of LLMs comes largely from the encodings (embeddings) generated in the\nhidden layers of the corresponding neural network. First we propose a\nclustering-based algorithm for measuring distributional shifts in text data by\nexploiting such embeddings. Then we study the effectiveness of our approach\nwhen applied to text embeddings generated by both LLMs and classical embedding\nalgorithms. Our experiments show that general-purpose LLM-based embeddings\nprovide a high sensitivity to data drift compared to other embedding methods.\nWe propose drift sensitivity as an important evaluation metric to consider when\ncomparing language models. Finally, we present insights and lessons learned\nfrom deploying our framework as part of the Fiddler ML Monitoring platform over\na period of 18 months.",
            "author": [
                "Gyandev Gupta",
                "Bashir Rastegarpanah",
                "Amalendu Iyer",
                "Joshua Rubin",
                "Krishnaram Kenthapadi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02337v1",
                "http://arxiv.org/pdf/2312.02337v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02334v1",
            "title": "An Evaluation Framework for Mapping News Headlines to Event Classes in a\n  Knowledge Graph",
            "updated": "2023-12-04T20:42:26Z",
            "published": "2023-12-04T20:42:26Z",
            "summary": "Mapping ongoing news headlines to event-related classes in a rich knowledge\nbase can be an important component in a knowledge-based event analysis and\nforecasting solution. In this paper, we present a methodology for creating a\nbenchmark dataset of news headlines mapped to event classes in Wikidata, and\nresources for the evaluation of methods that perform the mapping. We use the\ndataset to study two classes of unsupervised methods for this task: 1)\nadaptations of classic entity linking methods, and 2) methods that treat the\nproblem as a zero-shot text classification problem. For the first approach, we\nevaluate off-the-shelf entity linking systems. For the second approach, we\nexplore a) pre-trained natural language inference (NLI) models, and b)\npre-trained large generative language models. We present the results of our\nevaluation, lessons learned, and directions for future work. The dataset and\nscripts for evaluation are made publicly available.",
            "author": [
                "Steve Fonin Mbouadeu",
                "Martin Lorenzo",
                "Ken Barker",
                "Oktie Hassanzadeh"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02334v1",
                "http://arxiv.org/pdf/2312.02334v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03011v1",
            "title": "InstructBooth: Instruction-following Personalized Text-to-Image\n  Generation",
            "updated": "2023-12-04T20:34:46Z",
            "published": "2023-12-04T20:34:46Z",
            "summary": "Personalizing text-to-image models using a limited set of images for a\nspecific object has been explored in subject-specific image generation.\nHowever, existing methods often encounter challenges in aligning with text\nprompts due to overfitting to the limited training images. In this work, we\nintroduce InstructBooth, a novel method designed to enhance image-text\nalignment in personalized text-to-image models. Our approach first personalizes\ntext-to-image models with a small number of subject-specific images using a\nunique identifier. After personalization, we fine-tune personalized\ntext-to-image models using reinforcement learning to maximize a reward that\nquantifies image-text alignment. Additionally, we propose complementary\ntechniques to increase the synergy between these two processes. Our method\ndemonstrates superior image-text alignment compared to baselines while\nmaintaining personalization ability. In human evaluations, InstructBooth\noutperforms DreamBooth when considering all comprehensive factors.",
            "author": [
                "Daewon Chae",
                "Nokyung Park",
                "Jinkyu Kim",
                "Kimin Lee"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03011v1",
                "http://arxiv.org/pdf/2312.03011v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02331v1",
            "title": "Revisiting Topic-Guided Language Models",
            "updated": "2023-12-04T20:33:24Z",
            "published": "2023-12-04T20:33:24Z",
            "summary": "A recent line of work in natural language processing has aimed to combine\nlanguage models and topic models. These topic-guided language models augment\nneural language models with topic models, unsupervised learning methods that\ncan discover document-level patterns of word use. This paper compares the\neffectiveness of these methods in a standardized setting. We study four\ntopic-guided language models and two baselines, evaluating the held-out\npredictive performance of each model on four corpora. Surprisingly, we find\nthat none of these methods outperform a standard LSTM language model baseline,\nand most fail to learn good topics. Further, we train a probe of the neural\nlanguage model that shows that the baseline's hidden states already encode\ntopic information. We make public all code used for this study.",
            "author": [
                "Carolina Zheng",
                "Keyon Vafa",
                "David M. Blei"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02331v1",
                "http://arxiv.org/pdf/2312.02331v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02327v1",
            "title": "FLea: Improving federated learning on scarce and label-skewed data via\n  privacy-preserving feature augmentation",
            "updated": "2023-12-04T20:24:09Z",
            "published": "2023-12-04T20:24:09Z",
            "summary": "Learning a global model by abstracting the knowledge, distributed across\nmultiple clients, without aggregating the raw data is the primary goal of\nFederated Learning (FL). Typically, this works in rounds alternating between\nparallel local training at several clients, followed by model aggregation at a\nserver. We found that existing FL methods under-perform when local datasets are\nsmall and present severe label skew as these lead to over-fitting and local\nmodel bias. This is a realistic setting in many real-world applications. To\naddress the problem, we propose \\textit{FLea}, a unified framework that tackles\nover-fitting and local bias by encouraging clients to exchange\nprivacy-protected features to aid local training. The features refer to\nactivations from an intermediate layer of the model, which are obfuscated\nbefore being shared with other clients to protect sensitive information in the\ndata. \\textit{FLea} leverages a novel way of combining local and shared\nfeatures as augmentations to enhance local model learning. Our extensive\nexperiments demonstrate that \\textit{FLea} outperforms the start-of-the-art FL\nmethods, sharing only model parameters, by up to $17.6\\%$, and FL methods that\nshare data augmentations by up to $6.3\\%$, while reducing the privacy\nvulnerability associated with shared data augmentations.",
            "author": [
                "Tong Xia",
                "Abhirup Ghosh",
                "Cecilia Mascolo"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02327v1",
                "http://arxiv.org/pdf/2312.02327v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CR",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02315v1",
            "title": "Hydrogen induces chiral conduction channels in the topological magnet",
            "updated": "2023-12-04T19:56:44Z",
            "published": "2023-12-04T19:56:44Z",
            "summary": "Chirality, a characteristic handedness that distinguishes 'left' from\n'right', cuts widely across all of nature$^1$, from the structure of DNA$^2$ to\nopposite chirality of particles and antiparticles$^3$. In condensed matter\nchiral fermions have been identified in Weyl semimetals$^4$ through their\nunconventional electrodynamics arising from 'axial' charge imbalance between\nchiral Weyl nodes of topologically nontrivial electronic bands. Up to now it\nhas been challenging or impossible to create transport channels of Weyl\nfermions in a single material that could be easily configured for advancing\nchiral logic or spintronics$^{5,6}$. Here we generate chirality-directed\nconduction channels in inversion-symmetric Weyl ferromagnet (FM) $MnSb_2Te_4$,\nemergent from a deep connection between chirality in reciprocal and real space.\nWe alter the bandstructure on-demand with an intake and a subsequent release of\nionic hydrogen ($H^+$) $-$ a process we show to induce the tilt and rotation of\nWeyl bands. The transformed Weyl FM states feature a doubled Curie temperature\n$\\geq50K$ and an enhanced angular transport chirality synchronous with a rare\nfield-antisymmetric longitudinal resistance $-$ a low-field tunable 'chiral\nswitch' that roots in the interplay of Berry curvature$^7$, chiral anomaly$^8$\nand hydrogen-engendered mutation of Weyl nodes.",
            "author": [
                "Afrin N. Tamanna",
                "Ayesha Lakra",
                "Xiaxin Ding",
                "Entela Buzi",
                "Kyungwha Park",
                "Kamil Sobczak",
                "Haiming Deng",
                "Gargee Sharma",
                "Sumanta Tewari",
                "Lia Krusin-Elbaum"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02315v1",
                "http://arxiv.org/pdf/2312.02315v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02312v1",
            "title": "Visual Encoders for Data-Efficient Imitation Learning in Modern Video\n  Games",
            "updated": "2023-12-04T19:52:12Z",
            "published": "2023-12-04T19:52:12Z",
            "summary": "Video games have served as useful benchmarks for the decision making\ncommunity, but going beyond Atari games towards training agents in modern games\nhas been prohibitively expensive for the vast majority of the research\ncommunity. Recent progress in the research, development and open release of\nlarge vision models has the potential to amortize some of these costs across\nthe community. However, it is currently unclear which of these models have\nlearnt representations that retain information critical for sequential decision\nmaking. Towards enabling wider participation in the research of gameplaying\nagents in modern games, we present a systematic study of imitation learning\nwith publicly available visual encoders compared to the typical, task-specific,\nend-to-end training approach in Minecraft, Minecraft Dungeons and\nCounter-Strike: Global Offensive.",
            "author": [
                "Lukas Sch\u00e4fer",
                "Logan Jones",
                "Anssi Kanervisto",
                "Yuhan Cao",
                "Tabish Rashid",
                "Raluca Georgescu",
                "Dave Bignell",
                "Siddhartha Sen",
                "Andrea Trevi\u00f1o Gavito",
                "Sam Devlin"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02312v1",
                "http://arxiv.org/pdf/2312.02312v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02311v1",
            "title": "Searching for the Highest-z Dual AGN in the Deepest Chandra Surveys",
            "updated": "2023-12-04T19:49:57Z",
            "published": "2023-12-04T19:49:57Z",
            "summary": "We present an analysis searching for dual AGN among 62 high-redshift ($2.5 <\nz < 3.5$) X-ray sources selected from publicly available deep Chandra fields.\nWe aim to quantify the frequency of dual AGN in the high-redshift Universe,\nwhich holds implications for black hole merger timescales and low-frequency\ngravitational wave detection rates. We analyze each X-ray source using BAYMAX,\nan analysis tool that calculates the Bayes factor for whether a given archival\nChandra AGN is more likely a single or dual point source. We find no strong\nevidence for dual AGN in any individual source in our sample. We then increase\nour sensitivity to search for dual AGN across the sample by comparing our\nmeasured distribution of Bayes factors to that expected from a sample composed\nentirely of single point sources, and again find no evidence for dual AGN in\nthe observed sample distribution. Although our analysis utilizes one of the\nlargest Chandra catalogs of high-$z$ X-ray point sources available to study,\nthe findings remain limited by the modest number of sources observed at the\nhighest spatial resolution with Chandra and the typical count rates of the\ndetected sources. Our non-detection allows us to place an upper-limit on the\nX-ray dual AGN fraction between $2.5<z<3.5$ of 4.8\\%. Expanding substantially\non these results at X-ray wavelengths will require future surveys spanning\nlarger sky areas and extending to fainter fluxes than has been possible with\nChandra. We illustrate the potential of the AXIS mission concept in this\nregard.",
            "author": [
                "Brandon Sandoval",
                "Adi Foord",
                "Steven W. Allen",
                "Marta Volonteri",
                "Aaron Stemo",
                "Nianyi Chen",
                "Tiziana Di Matteo",
                "Kayhan Gultekin",
                "Melanie Habouzit",
                "Clara Puerto-Sanchez",
                "Edmund Hodges-Kluck",
                "Yohan Dubois"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02311v1",
                "http://arxiv.org/pdf/2312.02311v1"
            ],
            "primary_category": "astro-ph.HE",
            "category": [
                "astro-ph.HE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02310v1",
            "title": "VaQuitA: Enhancing Alignment in LLM-Assisted Video Understanding",
            "updated": "2023-12-04T19:48:02Z",
            "published": "2023-12-04T19:48:02Z",
            "summary": "Recent advancements in language-model-based video understanding have been\nprogressing at a remarkable pace, spurred by the introduction of Large Language\nModels (LLMs). However, the focus of prior research has been predominantly on\ndevising a projection layer that maps video features to tokens, an approach\nthat is both rudimentary and inefficient. In our study, we introduce a\ncutting-edge framework, VaQuitA, designed to refine the synergy between video\nand textual information. At the data level, instead of sampling frames\nuniformly, we implement a sampling method guided by CLIP-score rankings, which\nenables a more aligned selection of frames with the given question. At the\nfeature level, we integrate a trainable Video Perceiver alongside a\nVisual-Query Transformer (abbreviated as VQ-Former), which bolsters the\ninterplay between the input question and the video features. We also discover\nthat incorporating a simple prompt, \"Please be critical\", into the LLM input\ncan substantially enhance its video comprehension capabilities. Our\nexperimental results indicate that VaQuitA consistently sets a new benchmark\nfor zero-shot video question-answering tasks and is adept at producing\nhigh-quality, multi-turn video dialogues with users.",
            "author": [
                "Yizhou Wang",
                "Ruiyi Zhang",
                "Haoliang Wang",
                "Uttaran Bhattacharya",
                "Yun Fu",
                "Gang Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02310v1",
                "http://arxiv.org/pdf/2312.02310v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02309v1",
            "title": "Training Reinforcement Learning Agents and Humans With\n  Difficulty-Conditioned Generators",
            "updated": "2023-12-04T19:45:06Z",
            "published": "2023-12-04T19:45:06Z",
            "summary": "We adapt Parameterized Environment Response Model (PERM), a method for\ntraining both Reinforcement Learning (RL) Agents and human learners in\nparameterized environments by directly modeling difficulty and ability.\nInspired by Item Response Theory (IRT), PERM aligns environment difficulty with\nindividual ability, creating a Zone of Proximal Development-based curriculum.\nRemarkably, PERM operates without real-time RL updates and allows for offline\ntraining, ensuring its adaptability across diverse students. We present a\ntwo-stage training process that capitalizes on PERM's adaptability, and\ndemonstrate its effectiveness in training RL agents and humans in an empirical\nstudy.",
            "author": [
                "Sidney Tio",
                "Jimmy Ho",
                "Pradeep Varakantham"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02309v1",
                "http://arxiv.org/pdf/2312.02309v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.HC",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02308v1",
            "title": "AdsorbRL: Deep Multi-Objective Reinforcement Learning for Inverse\n  Catalysts Design",
            "updated": "2023-12-04T19:44:04Z",
            "published": "2023-12-04T19:44:04Z",
            "summary": "A central challenge of the clean energy transition is the development of\ncatalysts for low-emissions technologies. Recent advances in Machine Learning\nfor quantum chemistry drastically accelerate the computation of catalytic\nactivity descriptors such as adsorption energies. Here we introduce AdsorbRL, a\nDeep Reinforcement Learning agent aiming to identify potential catalysts given\na multi-objective binding energy target, trained using offline learning on the\nOpen Catalyst 2020 and Materials Project data sets. We experiment with Deep\nQ-Network agents to traverse the space of all ~160,000 possible unary, binary\nand ternary compounds of 55 chemical elements, with very sparse rewards based\non adsorption energy known for only between 2,000 and 3,000 catalysts per\nadsorbate. To constrain the actions space, we introduce Random Edge Traversal\nand train a single-objective DQN agent on the known states subgraph, which we\nfind strengthens target binding energy by an average of 4.1 eV. We extend this\napproach to multi-objective, goal-conditioned learning, and train a DQN agent\nto identify materials with the highest (respectively lowest) adsorption\nenergies for multiple simultaneous target adsorbates. We experiment with\nObjective Sub-Sampling, a novel training scheme aimed at encouraging\nexploration in the multi-objective setup, and demonstrate simultaneous\nadsorption energy improvement across all target adsorbates, by an average of\n0.8 eV. Overall, our results suggest strong potential for Deep Reinforcement\nLearning applied to the inverse catalysts design problem.",
            "author": [
                "Romain Lacombe",
                "Lucas Hendren",
                "Khalid El-Awady"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02308v1",
                "http://arxiv.org/pdf/2312.02308v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "physics.chem-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03756v1",
            "title": "LineConGraphs: Line Conversation Graphs for Effective Emotion\n  Recognition using Graph Neural Networks",
            "updated": "2023-12-04T19:36:58Z",
            "published": "2023-12-04T19:36:58Z",
            "summary": "Emotion Recognition in Conversations (ERC) is a critical aspect of affective\ncomputing, and it has many practical applications in healthcare, education,\nchatbots, and social media platforms. Earlier approaches for ERC analysis\ninvolved modeling both speaker and long-term contextual information using graph\nneural network architectures. However, it is ideal to deploy\nspeaker-independent models for real-world applications. Additionally, long\ncontext windows can potentially create confusion in recognizing the emotion of\nan utterance in a conversation. To overcome these limitations, we propose novel\nline conversation graph convolutional network (LineConGCN) and graph attention\n(LineConGAT) models for ERC analysis. These models are speaker-independent and\nbuilt using a graph construction strategy for conversations -- line\nconversation graphs (LineConGraphs). The conversational context in\nLineConGraphs is short-term -- limited to one previous and future utterance,\nand speaker information is not part of the graph. We evaluate the performance\nof our proposed models on two benchmark datasets, IEMOCAP and MELD, and show\nthat our LineConGAT model outperforms the state-of-the-art methods with an\nF1-score of 64.58% and 76.50%. Moreover, we demonstrate that embedding\nsentiment shift information into line conversation graphs further enhances the\nERC performance in the case of GCN models.",
            "author": [
                "Gokul S Krishnan",
                "Sarala Padi",
                "Craig S. Greenberg",
                "Balaraman Ravindran",
                "Dinesh Manoch",
                "Ram D. Sriram"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03756v1",
                "http://arxiv.org/pdf/2312.03756v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.HC",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02300v1",
            "title": "Reconsideration on evaluation of machine learning models in continuous\n  monitoring using wearables",
            "updated": "2023-12-04T19:34:08Z",
            "published": "2023-12-04T19:34:08Z",
            "summary": "This paper explores the challenges in evaluating machine learning (ML) models\nfor continuous health monitoring using wearable devices beyond conventional\nmetrics. We state the complexities posed by real-world variability, disease\ndynamics, user-specific characteristics, and the prevalence of false\nnotifications, necessitating novel evaluation strategies. Drawing insights from\nlarge-scale heart studies, the paper offers a comprehensive guideline for\nrobust ML model evaluation on continuous health monitoring.",
            "author": [
                "Cheng Ding",
                "Zhicheng Guo",
                "Cynthia Rudin",
                "Ran Xiao",
                "Fadi B Nahab",
                "Xiao Hu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02300v1",
                "http://arxiv.org/pdf/2312.02300v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02299v1",
            "title": "Cotton Yield Prediction Using Random Forest",
            "updated": "2023-12-04T19:33:29Z",
            "published": "2023-12-04T19:33:29Z",
            "summary": "The cotton industry in the United States is committed to sustainable\nproduction practices that minimize water, land, and energy use while improving\nsoil health and cotton output. Climate-smart agricultural technologies are\nbeing developed to boost yields while decreasing operating expenses. Crop yield\nprediction, on the other hand, is difficult because of the complex and\nnonlinear impacts of cultivar, soil type, management, pest and disease,\nclimate, and weather patterns on crops. To solve this issue, we employ machine\nlearning (ML) to forecast production while considering climate change, soil\ndiversity, cultivar, and inorganic nitrogen levels. From the 1980s to the\n1990s, field data were gathered across the southern cotton belt of the United\nStates. To capture the most current effects of climate change over the previous\nsix years, a second data source was produced using the process-based crop\nmodel, GOSSYM. We concentrated our efforts on three distinct areas inside each\nof the three southern states: Texas, Mississippi, and Georgia. To simplify the\namount of computations, accumulated heat units (AHU) for each set of\nexperimental data were employed as an analogy to use time-series weather data.\nThe Random Forest Regressor yielded a 97.75% accuracy rate, with a root mean\nsquare error of 55.05 kg/ha and an R2 of around 0.98. These findings\ndemonstrate how an ML technique may be developed and applied as a reliable and\neasy-to-use model to support the cotton climate-smart initiative.",
            "author": [
                "Alakananda Mitra",
                "Sahila Beegum",
                "David Fleisher",
                "Vangimalla R. Reddy",
                "Wenguang Sun",
                "Chittaranjan Ray",
                "Dennis Timlin",
                "Arindam Malakar"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02299v1",
                "http://arxiv.org/pdf/2312.02299v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CY",
                "stat.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02298v1",
            "title": "MoE-AMC: Enhancing Automatic Modulation Classification Performance Using\n  Mixture-of-Experts",
            "updated": "2023-12-04T19:31:15Z",
            "published": "2023-12-04T19:31:15Z",
            "summary": "Automatic Modulation Classification (AMC) plays a vital role in time series\nanalysis, such as signal classification and identification within wireless\ncommunications. Deep learning-based AMC models have demonstrated significant\npotential in this domain. However, current AMC models inadequately consider the\ndisparities in handling signals under conditions of low and high\nSignal-to-Noise Ratio (SNR), resulting in an unevenness in their performance.\nIn this study, we propose MoE-AMC, a novel Mixture-of-Experts (MoE) based model\nspecifically crafted to address AMC in a well-balanced manner across varying\nSNR conditions. Utilizing the MoE framework, MoE-AMC seamlessly combines the\nstrengths of LSRM (a Transformer-based model) for handling low SNR signals and\nHSRM (a ResNet-based model) for high SNR signals. This integration empowers\nMoE-AMC to achieve leading performance in modulation classification, showcasing\nits efficacy in capturing distinctive signal features under diverse SNR\nscenarios. We conducted experiments using the RML2018.01a dataset, where\nMoE-AMC achieved an average classification accuracy of 71.76% across different\nSNR levels, surpassing the performance of previous SOTA models by nearly 10%.\nThis study represents a pioneering application of MoE techniques in the realm\nof AMC, offering a promising avenue for elevating signal classification\naccuracy within wireless communication systems.",
            "author": [
                "Jiaxin Gao",
                "Qinglong Cao",
                "Yuntian Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02298v1",
                "http://arxiv.org/pdf/2312.02298v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP",
                "cs.CV",
                "cs.LG",
                "stat.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02296v1",
            "title": "LLMs Accelerate Annotation for Medical Information Extraction",
            "updated": "2023-12-04T19:26:13Z",
            "published": "2023-12-04T19:26:13Z",
            "summary": "The unstructured nature of clinical notes within electronic health records\noften conceals vital patient-related information, making it challenging to\naccess or interpret. To uncover this hidden information, specialized Natural\nLanguage Processing (NLP) models are required. However, training these models\nnecessitates large amounts of labeled data, a process that is both\ntime-consuming and costly when relying solely on human experts for annotation.\nIn this paper, we propose an approach that combines Large Language Models\n(LLMs) with human expertise to create an efficient method for generating ground\ntruth labels for medical text annotation. By utilizing LLMs in conjunction with\nhuman annotators, we significantly reduce the human annotation burden, enabling\nthe rapid creation of labeled datasets. We rigorously evaluate our method on a\nmedical information extraction task, demonstrating that our approach not only\nsubstantially cuts down on human intervention but also maintains high accuracy.\nThe results highlight the potential of using LLMs to improve the utilization of\nunstructured clinical data, allowing for the swift deployment of tailored NLP\nsolutions in healthcare.",
            "author": [
                "Akshay Goel",
                "Almog Gueta",
                "Omry Gilon",
                "Chang Liu",
                "Sofia Erell",
                "Lan Huong Nguyen",
                "Xiaohong Hao",
                "Bolous Jaber",
                "Shashir Reddy",
                "Rupesh Kartha",
                "Jean Steiner",
                "Itay Laish",
                "Amir Feder"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02296v1",
                "http://arxiv.org/pdf/2312.02296v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02290v1",
            "title": "You Can Run but not Hide: Improving Gait Recognition with Intrinsic\n  Occlusion Type Awareness",
            "updated": "2023-12-04T19:11:40Z",
            "published": "2023-12-04T19:11:40Z",
            "summary": "While gait recognition has seen many advances in recent years, the occlusion\nproblem has largely been ignored. This problem is especially important for gait\nrecognition from uncontrolled outdoor sequences at range - since any small\nobstruction can affect the recognition system. Most current methods assume the\navailability of complete body information while extracting the gait features.\nWhen parts of the body are occluded, these methods may hallucinate and output a\ncorrupted gait signature as they try to look for body parts which are not\npresent in the input at all. To address this, we exploit the learned occlusion\ntype while extracting identity features from videos. Thus, in this work, we\npropose an occlusion aware gait recognition method which can be used to model\nintrinsic occlusion awareness into potentially any state-of-the-art gait\nrecognition method. Our experiments on the challenging GREW and BRIAR datasets\nshow that networks enhanced with this occlusion awareness perform better at\nrecognition tasks than their counterparts trained on similar occlusions.",
            "author": [
                "Ayush Gupta",
                "Rama Chellappa"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02290v1",
                "http://arxiv.org/pdf/2312.02290v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03009v1",
            "title": "I-PHYRE: Interactive Physical Reasoning",
            "updated": "2023-12-04T19:01:19Z",
            "published": "2023-12-04T19:01:19Z",
            "summary": "Current evaluation protocols predominantly assess physical reasoning in\nstationary scenes, creating a gap in evaluating agents' abilities to interact\nwith dynamic events. While contemporary methods allow agents to modify initial\nscene configurations and observe consequences, they lack the capability to\ninteract with events in real time. To address this, we introduce I-PHYRE, a\nframework that challenges agents to simultaneously exhibit intuitive physical\nreasoning, multi-step planning, and in-situ intervention. Here, intuitive\nphysical reasoning refers to a quick, approximate understanding of physics to\naddress complex problems; multi-step denotes the need for extensive sequence\nplanning in I-PHYRE, considering each intervention can significantly alter\nsubsequent choices; and in-situ implies the necessity for timely object\nmanipulation within a scene, where minor timing deviations can result in task\nfailure. We formulate four game splits to scrutinize agents' learning and\ngeneralization of essential principles of interactive physical reasoning,\nfostering learning through interaction with representative scenarios. Our\nexploration involves three planning strategies and examines several supervised\nand reinforcement agents' zero-shot generalization proficiency on I-PHYRE. The\noutcomes highlight a notable gap between existing learning algorithms and human\nperformance, emphasizing the imperative for more research in enhancing agents\nwith interactive physical reasoning capabilities. The environment and baselines\nwill be made publicly available.",
            "author": [
                "Shiqian Li",
                "Kewen Wu",
                "Chi Zhang",
                "Yixin Zhu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03009v1",
                "http://arxiv.org/pdf/2312.03009v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CV",
                "cs.LG",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02278v1",
            "title": "Learning PDFs through Interpretable Latent Representations in Mellin\n  Space",
            "updated": "2023-12-04T19:00:23Z",
            "published": "2023-12-04T19:00:23Z",
            "summary": "Representing the parton distribution functions (PDFs) of the proton and other\nhadrons through flexible, high-fidelity parametrizations has been a\nlong-standing goal of particle physics phenomenology. This is particularly true\nsince the chosen parametrization methodology can play an influential role in\nthe ultimate PDF uncertainties as extracted in QCD global analyses; these, in\nturn, are often determinative of the reach of experiments at the LHC and other\nfacilities to non-standard physics, including at large $x$, where\nparametrization effects can be significant. In this study, we explore a series\nof encoder-decoder machine-learning (ML) models with various neural-network\ntopologies as efficient means of reconstructing PDFs from meaningful\ninformation stored in an interpretable latent space. Given recent effort to\npioneer synergies between QCD analyses and lattice-gauge calculations, we\nformulate a latent representation based on the behavior of PDFs in Mellin\nspace, i.e., their integrated moments, and test the ability of various models\nto decode PDFs from this information faithfully. We introduce a numerical\npackage, $\\texttt{PDFdecoder}$, which implements several encoder-decoder models\nto reconstruct PDFs with high fidelity and use this tool to explore strengths\nand pitfalls of neural-network approaches to PDF parametrization. We\nadditionally dissect patterns of learned correlations between encoded Mellin\nmoments and reconstructed PDFs which suggest opportunities for further\nimprovements to ML-based approaches to PDF parametrizations and uncertainty\nquantification.",
            "author": [
                "Brandon Kriesten",
                "T. J. Hobbs"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02278v1",
                "http://arxiv.org/pdf/2312.02278v1"
            ],
            "primary_category": "hep-ph",
            "category": [
                "hep-ph",
                "hep-lat",
                "nucl-th"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02277v1",
            "title": "ALEXR: Optimal Single-Loop Algorithms for Convex Finite-Sum Coupled\n  Compositional Stochastic Optimization",
            "updated": "2023-12-04T19:00:07Z",
            "published": "2023-12-04T19:00:07Z",
            "summary": "This paper revisits a class of convex Finite-Sum Coupled Compositional\nStochastic Optimization (cFCCO) problems with many applications, including\ngroup distributionally robust optimization (GDRO), reinforcement learning, and\nlearning to rank. To better solve these problems, we introduce a unified family\nof efficient single-loop primal-dual block-coordinate proximal algorithms,\ndubbed ALEXR. This algorithm leverages block-coordinate stochastic mirror\nascent updates for the dual variable and stochastic proximal gradient descent\nupdates for the primal variable. We establish the convergence rates of ALEXR in\nboth convex and strongly convex cases under smoothness and non-smoothness\nconditions of involved functions, which not only improve the best rates in\nprevious works on smooth cFCCO problems but also expand the realm of cFCCO for\nsolving more challenging non-smooth problems such as the dual form of GDRO.\nFinally, we present lower complexity bounds to demonstrate that the convergence\nrates of ALEXR are optimal among first-order block-coordinate stochastic\nalgorithms for the considered class of cFCCO problems.",
            "author": [
                "Bokun Wang",
                "Tianbao Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02277v1",
                "http://arxiv.org/pdf/2312.02277v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02266v1",
            "title": "Hierarchical Cross-entropy Loss for Classification of Astrophysical\n  Transients",
            "updated": "2023-12-04T19:00:01Z",
            "published": "2023-12-04T19:00:01Z",
            "summary": "Astrophysical transient phenomena are traditionally classified\nspectroscopically in a hierarchical taxonomy; however, this graph structure is\ncurrently not utilized in neural net-based photometric classifiers for\ntime-domain astrophysics. Instead, independent classifiers are trained for\ndifferent tiers of classified data, and events are excluded if they fall\noutside of these well-defined but flat classification schemes. Here, we\nintroduce a weighted hierarchical cross-entropy objective function for\nclassification of astrophysical transients. Our method allows users to directly\nbuild and use physics- or observationally-motivated tree-based taxonomies. Our\nweighted hierarchical cross-entropy loss directly uses this graph to accurately\nclassify all targets into any node of the tree, re-weighting imbalanced\nclasses. We test our novel loss on a set of variable stars and extragalactic\ntransients from the Zwicky Transient Facility, showing that we can achieve\nsimilar performance to fine-tuned classifiers with the advantage of notably\nmore flexibility in downstream classification tasks.",
            "author": [
                "V. Ashley Villar",
                "Kaylee de Soto",
                "Alex Gagliano"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02266v1",
                "http://arxiv.org/pdf/2312.02266v1"
            ],
            "primary_category": "astro-ph.IM",
            "category": [
                "astro-ph.IM",
                "astro-ph.HE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02264v1",
            "title": "Scaling Laws in Jet Classification",
            "updated": "2023-12-04T19:00:00Z",
            "published": "2023-12-04T19:00:00Z",
            "summary": "We demonstrate the emergence of scaling laws in the benchmark top versus QCD\njet classification problem in collider physics. Six distinct\nphysically-motivated classifiers exhibit power-law scaling of the binary\ncross-entropy test loss as a function of training set size, with distinct power\nlaw indices. This result highlights the importance of comparing classifiers as\na function of dataset size rather than for a fixed training set, as the optimal\nclassifier may change considerably as the dataset is scaled up. We speculate on\nthe interpretation of our results in terms of previous models of scaling laws\nobserved in natural language and image datasets.",
            "author": [
                "Joshua Batson",
                "Yonatan Kahn"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02264v1",
                "http://arxiv.org/pdf/2312.02264v1"
            ],
            "primary_category": "hep-ph",
            "category": [
                "hep-ph",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02156v1",
            "title": "Latent Feature-Guided Diffusion Models for Shadow Removal",
            "updated": "2023-12-04T18:59:55Z",
            "published": "2023-12-04T18:59:55Z",
            "summary": "Recovering textures under shadows has remained a challenging problem due to\nthe difficulty of inferring shadow-free scenes from shadow images. In this\npaper, we propose the use of diffusion models as they offer a promising\napproach to gradually refine the details of shadow regions during the diffusion\nprocess. Our method improves this process by conditioning on a learned latent\nfeature space that inherits the characteristics of shadow-free images, thus\navoiding the limitation of conventional methods that condition on degraded\nimages only. Additionally, we propose to alleviate potential local optima\nduring training by fusing noise features with the diffusion network. We\ndemonstrate the effectiveness of our approach which outperforms the previous\nbest method by 13% in terms of RMSE on the AISTD dataset. Further, we explore\ninstance-level shadow removal, where our model outperforms the previous best\nmethod by 82% in terms of RMSE on the DESOBA dataset.",
            "author": [
                "Kangfu Mei",
                "Luis Figueroa",
                "Zhe Lin",
                "Zhihong Ding",
                "Scott Cohen",
                "Vishal M. Patel"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02156v1",
                "http://arxiv.org/pdf/2312.02156v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02153v1",
            "title": "Aligning and Prompting Everything All at Once for Universal Visual\n  Perception",
            "updated": "2023-12-04T18:59:50Z",
            "published": "2023-12-04T18:59:50Z",
            "summary": "Vision foundation models have been explored recently to build general-purpose\nvision systems. However, predominant paradigms, driven by casting\ninstance-level tasks as an object-word alignment, bring heavy cross-modality\ninteraction, which is not effective in prompting object detection and visual\ngrounding. Another line of work that focuses on pixel-level tasks often\nencounters a large annotation gap of things and stuff, and suffers from mutual\ninterference between foreground-object and background-class segmentation. In\nstark contrast to the prevailing methods, we present APE, a universal visual\nperception model for aligning and prompting everything all at once in an image\nto perform diverse tasks, i.e., detection, segmentation, and grounding, as an\ninstance-level sentence-object matching paradigm. Specifically, APE advances\nthe convergence of detection and grounding by reformulating language-guided\ngrounding as open-vocabulary detection, which efficiently scales up model\nprompting to thousands of category vocabularies and region descriptions while\nmaintaining the effectiveness of cross-modality fusion. To bridge the\ngranularity gap of different pixel-level tasks, APE equalizes semantic and\npanoptic segmentation to proxy instance learning by considering any isolated\nregions as individual instances. APE aligns vision and language representation\non broad data with natural and challenging characteristics all at once without\ntask-specific fine-tuning. The extensive experiments on over 160 datasets\ndemonstrate that, with only one-suit of weights, APE outperforms (or is on par\nwith) the state-of-the-art models, proving that an effective yet universal\nperception for anything aligning and prompting is indeed feasible. Codes and\ntrained models are released at https://github.com/shenyunhang/APE.",
            "author": [
                "Yunhang Shen",
                "Chaoyou Fu",
                "Peixian Chen",
                "Mengdan Zhang",
                "Ke Li",
                "Xing Sun",
                "Yunsheng Wu",
                "Shaohui Lin",
                "Rongrong Ji"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02153v1",
                "http://arxiv.org/pdf/2312.02153v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02152v1",
            "title": "Steerers: A framework for rotation equivariant keypoint descriptors",
            "updated": "2023-12-04T18:59:44Z",
            "published": "2023-12-04T18:59:44Z",
            "summary": "Image keypoint descriptions that are discriminative and matchable over large\nchanges in viewpoint are vital for 3D reconstruction. However, descriptions\noutput by learned descriptors are typically not robust to camera rotation.\nWhile they can be made more robust by, e.g., data augmentation, this degrades\nperformance on upright images. Another approach is test-time augmentation,\nwhich incurs a significant increase in runtime. We instead learn a linear\ntransform in description space that encodes rotations of the input image. We\ncall this linear transform a steerer since it allows us to transform the\ndescriptions as if the image was rotated. From representation theory we know\nall possible steerers for the rotation group. Steerers can be optimized (A)\ngiven a fixed descriptor, (B) jointly with a descriptor or (C) we can optimize\na descriptor given a fixed steerer. We perform experiments in all of these\nthree settings and obtain state-of-the-art results on the rotation invariant\nimage matching benchmarks AIMS and Roto-360. We publish code and model weights\nat github.com/georg-bn/rotation-steerers.",
            "author": [
                "Georg B\u00f6kman",
                "Johan Edstedt",
                "Michael Felsberg",
                "Fredrik Kahl"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02152v1",
                "http://arxiv.org/pdf/2312.02152v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02151v1",
            "title": "Guarding Barlow Twins Against Overfitting with Mixed Samples",
            "updated": "2023-12-04T18:59:36Z",
            "published": "2023-12-04T18:59:36Z",
            "summary": "Self-supervised Learning (SSL) aims to learn transferable feature\nrepresentations for downstream applications without relying on labeled data.\nThe Barlow Twins algorithm, renowned for its widespread adoption and\nstraightforward implementation compared to its counterparts like contrastive\nlearning methods, minimizes feature redundancy while maximizing invariance to\ncommon corruptions. Optimizing for the above objective forces the network to\nlearn useful representations, while avoiding noisy or constant features,\nresulting in improved downstream task performance with limited adaptation.\nDespite Barlow Twins' proven effectiveness in pre-training, the underlying SSL\nobjective can inadvertently cause feature overfitting due to the lack of strong\ninteraction between the samples unlike the contrastive learning approaches.\nFrom our experiments, we observe that optimizing for the Barlow Twins objective\ndoesn't necessarily guarantee sustained improvements in representation quality\nbeyond a certain pre-training phase, and can potentially degrade downstream\nperformance on some datasets. To address this challenge, we introduce Mixed\nBarlow Twins, which aims to improve sample interaction during Barlow Twins\ntraining via linearly interpolated samples. This results in an additional\nregularization term to the original Barlow Twins objective, assuming linear\ninterpolation in the input space translates to linearly interpolated features\nin the feature space. Pre-training with this regularization effectively\nmitigates feature overfitting and further enhances the downstream performance\non CIFAR-10, CIFAR-100, TinyImageNet, STL-10, and ImageNet datasets. The code\nand checkpoints are available at: https://github.com/wgcban/mix-bt.git",
            "author": [
                "Wele Gedara Chaminda Bandara",
                "Celso M. De Melo",
                "Vishal M. Patel"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02151v1",
                "http://arxiv.org/pdf/2312.02151v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02150v1",
            "title": "Readout Guidance: Learning Control from Diffusion Features",
            "updated": "2023-12-04T18:59:32Z",
            "published": "2023-12-04T18:59:32Z",
            "summary": "We present Readout Guidance, a method for controlling text-to-image diffusion\nmodels with learned signals. Readout Guidance uses readout heads, lightweight\nnetworks trained to extract signals from the features of a pre-trained, frozen\ndiffusion model at every timestep. These readouts can encode single-image\nproperties, such as pose, depth, and edges; or higher-order properties that\nrelate multiple images, such as correspondence and appearance similarity.\nFurthermore, by comparing the readout estimates to a user-defined target, and\nback-propagating the gradient through the readout head, these estimates can be\nused to guide the sampling process. Compared to prior methods for conditional\ngeneration, Readout Guidance requires significantly fewer added parameters and\ntraining samples, and offers a convenient and simple recipe for reproducing\ndifferent forms of conditional control under a single framework, with a single\narchitecture and sampling procedure. We showcase these benefits in the\napplications of drag-based manipulation, identity-consistent generation, and\nspatially aligned control. Project page: https://readout-guidance.github.io.",
            "author": [
                "Grace Luo",
                "Trevor Darrell",
                "Oliver Wang",
                "Dan B Goldman",
                "Aleksander Holynski"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02150v1",
                "http://arxiv.org/pdf/2312.02150v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02147v1",
            "title": "Rejuvenating image-GPT as Strong Visual Representation Learners",
            "updated": "2023-12-04T18:59:20Z",
            "published": "2023-12-04T18:59:20Z",
            "summary": "This paper enhances image-GPT (iGPT), one of the pioneering works that\nintroduce autoregressive pretraining to predict next pixels for visual\nrepresentation learning. Two simple yet essential changes are made. First, we\nshift the prediction target from raw pixels to semantic tokens, enabling a\nhigher-level understanding of visual content. Second, we supplement the\nautoregressive modeling by instructing the model to predict not only the next\ntokens but also the visible tokens. This pipeline is particularly effective\nwhen semantic tokens are encoded by discriminatively trained models, such as\nCLIP. We introduce this novel approach as D-iGPT. Extensive experiments\nshowcase that D-iGPT excels as a strong learner of visual representations: A\nnotable achievement of D-iGPT is its compelling performance on the ImageNet-1K\ndataset -- by training on publicly available datasets, D-iGPT achieves 89.5\\%\ntop-1 accuracy with a vanilla ViT-Large model. This model also shows strong\ngeneralization on the downstream task and robustness on out-of-distribution\nsamples. Code is avaiable at\n\\href{https://github.com/OliverRensu/D-iGPT}{https://github.com/OliverRensu/D-iGPT}.",
            "author": [
                "Sucheng Ren",
                "Zeyu Wang",
                "Hongru Zhu",
                "Junfei Xiao",
                "Alan Yuille",
                "Cihang Xie"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02147v1",
                "http://arxiv.org/pdf/2312.02147v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02146v1",
            "title": "Learning Polynomial Problems with $SL(2,\\mathbb{R})$ Equivariance",
            "updated": "2023-12-04T18:59:19Z",
            "published": "2023-12-04T18:59:19Z",
            "summary": "Optimizing and certifying the positivity of polynomials are fundamental\nprimitives across mathematics and engineering applications, from dynamical\nsystems to operations research. However, solving these problems in practice\nrequires large semidefinite programs, with poor scaling in dimension and\ndegree. In this work, we demonstrate for the first time that neural networks\ncan effectively solve such problems in a data-driven fashion, achieving tenfold\nspeedups while retaining high accuracy. Moreover, we observe that these\npolynomial learning problems are equivariant to the non-compact group\n$SL(2,\\mathbb{R})$, which consists of area-preserving linear transformations.\nWe therefore adapt our learning pipelines to accommodate this structure,\nincluding data augmentation, a new $SL(2,\\mathbb{R})$-equivariant architecture,\nand an architecture equivariant with respect to its maximal compact subgroup,\n$SO(2, \\mathbb{R})$. Surprisingly, the most successful approaches in practice\ndo not enforce equivariance to the entire group, which we prove arises from an\nunusual lack of architecture universality for $SL(2,\\mathbb{R})$ in particular.\nA consequence of this result, which is of independent interest, is that there\nexists an equivariant function for which there is no sequence of equivariant\npolynomials multiplied by arbitrary invariants that approximates the original\nfunction. This is a rare example of a symmetric problem where data augmentation\noutperforms a fully equivariant architecture, and provides interesting lessons\nin both theory and practice for other problems with non-compact symmetries.",
            "author": [
                "Hannah Lawrence",
                "Mitchell Tong Harris"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02146v1",
                "http://arxiv.org/pdf/2312.02146v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02145v1",
            "title": "Repurposing Diffusion-Based Image Generators for Monocular Depth\n  Estimation",
            "updated": "2023-12-04T18:59:13Z",
            "published": "2023-12-04T18:59:13Z",
            "summary": "Monocular depth estimation is a fundamental computer vision task. Recovering\n3D depth from a single image is geometrically ill-posed and requires scene\nunderstanding, so it is not surprising that the rise of deep learning has led\nto a breakthrough. The impressive progress of monocular depth estimators has\nmirrored the growth in model capacity, from relatively modest CNNs to large\nTransformer architectures. Still, monocular depth estimators tend to struggle\nwhen presented with images with unfamiliar content and layout, since their\nknowledge of the visual world is restricted by the data seen during training,\nand challenged by zero-shot generalization to new domains. This motivates us to\nexplore whether the extensive priors captured in recent generative diffusion\nmodels can enable better, more generalizable depth estimation. We introduce\nMarigold, a method for affine-invariant monocular depth estimation that is\nderived from Stable Diffusion and retains its rich prior knowledge. The\nestimator can be fine-tuned in a couple of days on a single GPU using only\nsynthetic training data. It delivers state-of-the-art performance across a wide\nrange of datasets, including over 20% performance gains in specific cases.\nProject page: https://marigoldmonodepth.github.io.",
            "author": [
                "Bingxin Ke",
                "Anton Obukhov",
                "Shengyu Huang",
                "Nando Metzger",
                "Rodrigo Caye Daudt",
                "Konrad Schindler"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02145v1",
                "http://arxiv.org/pdf/2312.02145v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02144v1",
            "title": "Optimizing Camera Configurations for Multi-View Pedestrian Detection",
            "updated": "2023-12-04T18:59:02Z",
            "published": "2023-12-04T18:59:02Z",
            "summary": "Jointly considering multiple camera views (multi-view) is very effective for\npedestrian detection under occlusion. For such multi-view systems, it is\ncritical to have well-designed camera configurations, including camera\nlocations, directions, and fields-of-view (FoVs). Usually, these configurations\nare crafted based on human experience or heuristics. In this work, we present a\nnovel solution that features a transformer-based camera configuration\ngenerator. Using reinforcement learning, this generator autonomously explores\nvast combinations within the action space and searches for configurations that\ngive the highest detection accuracy according to the training dataset. The\ngenerator learns advanced techniques like maximizing coverage, minimizing\nocclusion, and promoting collaboration. Across multiple simulation scenarios,\nthe configurations generated by our transformer-based model consistently\noutperform random search, heuristic-based methods, and configurations designed\nby human experts, shedding light on future camera layout optimization.",
            "author": [
                "Yunzhong Hou",
                "Xingjian Leng",
                "Tom Gedeon",
                "Liang Zheng"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02144v1",
                "http://arxiv.org/pdf/2312.02144v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02143v2",
            "title": "Competition-Level Problems are Effective LLM Evaluators",
            "updated": "2023-12-05T03:44:19Z",
            "published": "2023-12-04T18:58:57Z",
            "summary": "Large language models (LLMs) have demonstrated impressive reasoning\ncapabilities, yet there is ongoing debate about these abilities and the\npotential data contamination problem recently. This paper aims to evaluate the\nreasoning capacities of LLMs, specifically in solving recent competition-level\nprogramming problems in Codeforces, which are expert-crafted and unique,\nrequiring deep understanding and robust reasoning skills. We first provide a\ncomprehensive evaluation of GPT-4's peiceived zero-shot performance on this\ntask, considering various aspects such as problems' release time, difficulties,\nand types of errors encountered. Surprisingly, the peiceived performance of\nGPT-4 has experienced a cliff like decline in problems after September 2021\nconsistently across all the difficulties and types of problems, which shows the\npotential data contamination, as well as the challenges for any existing LLM to\nsolve unseen complex reasoning problems. We further explore various approaches\nsuch as fine-tuning, Chain-of-Thought prompting and problem description\nsimplification, unfortunately none of them is able to consistently mitigate the\nchallenges. Through our work, we emphasis the importance of this excellent data\nsource for assessing the genuine reasoning capabilities of LLMs, and foster the\ndevelopment of LLMs with stronger reasoning abilities and better generalization\nin the future.",
            "author": [
                "Yiming Huang",
                "Zhenghao Lin",
                "Xiao Liu",
                "Yeyun Gong",
                "Shuai Lu",
                "Fangyu Lei",
                "Yaobo Liang",
                "Yelong Shen",
                "Chen Lin",
                "Nan Duan",
                "Weizhu Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02143v2",
                "http://arxiv.org/pdf/2312.02143v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02256v1",
            "title": "EMDM: Efficient Motion Diffusion Model for Fast, High-Quality Motion\n  Generation",
            "updated": "2023-12-04T18:58:38Z",
            "published": "2023-12-04T18:58:38Z",
            "summary": "We introduce Efficient Motion Diffusion Model (EMDM) for fast and\nhigh-quality human motion generation. Although previous motion diffusion models\nhave shown impressive results, they struggle to achieve fast generation while\nmaintaining high-quality human motions. Motion latent diffusion has been\nproposed for efficient motion generation. However, effectively learning a\nlatent space can be non-trivial in such a two-stage manner. Meanwhile,\naccelerating motion sampling by increasing the step size, e.g., DDIM, typically\nleads to a decline in motion quality due to the inapproximation of complex data\ndistributions when naively increasing the step size. In this paper, we propose\nEMDM that allows for much fewer sample steps for fast motion generation by\nmodeling the complex denoising distribution during multiple sampling steps.\nSpecifically, we develop a Conditional Denoising Diffusion GAN to capture\nmultimodal data distributions conditioned on both control signals, i.e.,\ntextual description and denoising time step. By modeling the complex data\ndistribution, a larger sampling step size and fewer steps are achieved during\nmotion synthesis, significantly accelerating the generation process. To\neffectively capture the human dynamics and reduce undesired artifacts, we\nemploy motion geometric loss during network training, which improves the motion\nquality and training efficiency. As a result, EMDM achieves a remarkable\nspeed-up at the generation stage while maintaining high-quality motion\ngeneration in terms of fidelity and diversity.",
            "author": [
                "Wenyang Zhou",
                "Zhiyang Dou",
                "Zeyu Cao",
                "Zhouyingcheng Liao",
                "Jingbo Wang",
                "Wenjia Wang",
                "Yuan Liu",
                "Taku Komura",
                "Wenping Wang",
                "Lingjie Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02256v1",
                "http://arxiv.org/pdf/2312.02256v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02141v1",
            "title": "iMatching: Imperative Correspondence Learning",
            "updated": "2023-12-04T18:58:20Z",
            "published": "2023-12-04T18:58:20Z",
            "summary": "Learning feature correspondence is a foundational task in computer vision,\nholding immense importance for downstream applications such as visual odometry\nand 3D reconstruction. Despite recent progress in data-driven models, feature\ncorrespondence learning is still limited by the lack of accurate per-pixel\ncorrespondence labels. To overcome this difficulty, we introduce a new\nself-supervised scheme, imperative learning (IL), for training feature\ncorrespondence. It enables correspondence learning on arbitrary uninterrupted\nvideos without any camera pose or depth labels, heralding a new era for\nself-supervised correspondence learning. Specifically, we formulated the\nproblem of correspondence learning as a bilevel optimization, which takes the\nreprojection error from bundle adjustment as a supervisory signal for the\nmodel. To avoid large memory and computation overhead, we leverage the\nstationary point to effectively back-propagate the implicit gradients through\nbundle adjustment. Through extensive experiments, we demonstrate superior\nperformance on tasks including feature matching and pose estimation, in which\nwe obtained an average of 30% accuracy gain over the state-of-the-art matching\nmodels.",
            "author": [
                "Zitong Zhan",
                "Dasong Gao",
                "Yun-Jou Lin",
                "Youjie Xia",
                "Chen Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02141v1",
                "http://arxiv.org/pdf/2312.02141v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02139v1",
            "title": "DiffiT: Diffusion Vision Transformers for Image Generation",
            "updated": "2023-12-04T18:57:01Z",
            "published": "2023-12-04T18:57:01Z",
            "summary": "Diffusion models with their powerful expressivity and high sample quality\nhave enabled many new applications and use-cases in various domains. For sample\ngeneration, these models rely on a denoising neural network that generates\nimages by iterative denoising. Yet, the role of denoising network architecture\nis not well-studied with most efforts relying on convolutional residual U-Nets.\nIn this paper, we study the effectiveness of vision transformers in\ndiffusion-based generative learning. Specifically, we propose a new model,\ndenoted as Diffusion Vision Transformers (DiffiT), which consists of a hybrid\nhierarchical architecture with a U-shaped encoder and decoder. We introduce a\nnovel time-dependent self-attention module that allows attention layers to\nadapt their behavior at different stages of the denoising process in an\nefficient manner. We also introduce latent DiffiT which consists of transformer\nmodel with the proposed self-attention layers, for high-resolution image\ngeneration. Our results show that DiffiT is surprisingly effective in\ngenerating high-fidelity images, and it achieves state-of-the-art (SOTA)\nbenchmarks on a variety of class-conditional and unconditional synthesis tasks.\nIn the latent space, DiffiT achieves a new SOTA FID score of 1.73 on\nImageNet-256 dataset. Repository: https://github.com/NVlabs/DiffiT",
            "author": [
                "Ali Hatamizadeh",
                "Jiaming Song",
                "Guilin Liu",
                "Jan Kautz",
                "Arash Vahdat"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02139v1",
                "http://arxiv.org/pdf/2312.02139v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02255v1",
            "title": "Re-Nerfing: Enforcing Geometric Constraints on Neural Radiance Fields\n  through Novel Views Synthesis",
            "updated": "2023-12-04T18:56:08Z",
            "published": "2023-12-04T18:56:08Z",
            "summary": "Neural Radiance Fields (NeRFs) have shown remarkable novel view synthesis\ncapabilities even in large-scale, unbounded scenes, albeit requiring hundreds\nof views or introducing artifacts in sparser settings. Their optimization\nsuffers from shape-radiance ambiguities wherever only a small visual overlap is\navailable. This leads to erroneous scene geometry and artifacts. In this paper,\nwe propose Re-Nerfing, a simple and general multi-stage approach that leverages\nNeRF's own view synthesis to address these limitations. With Re-Nerfing, we\nincrease the scene's coverage and enhance the geometric consistency of novel\nviews as follows: First, we train a NeRF with the available views. Then, we use\nthe optimized NeRF to synthesize pseudo-views next to the original ones to\nsimulate a stereo or trifocal setup. Finally, we train a second NeRF with both\noriginal and pseudo views while enforcing structural, epipolar constraints via\nthe newly synthesized images. Extensive experiments on the mip-NeRF 360 dataset\nshow the effectiveness of Re-Nerfing across denser and sparser input scenarios,\nbringing improvements to the state-of-the-art Zip-NeRF, even when trained with\nall views.",
            "author": [
                "Felix Tristram",
                "Stefano Gasperini",
                "Federico Tombari",
                "Nassir Navab",
                "Benjamin Busam"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02255v1",
                "http://arxiv.org/pdf/2312.02255v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02134v1",
            "title": "GaussianAvatar: Towards Realistic Human Avatar Modeling from a Single\n  Video via Animatable 3D Gaussians",
            "updated": "2023-12-04T18:55:45Z",
            "published": "2023-12-04T18:55:45Z",
            "summary": "We present GaussianAvatar, an efficient approach to creating realistic human\navatars with dynamic 3D appearances from a single video. We start by\nintroducing animatable 3D Gaussians to explicitly represent humans in various\nposes and clothing styles. Such an explicit and animatable representation can\nfuse 3D appearances more efficiently and consistently from 2D observations. Our\nrepresentation is further augmented with dynamic properties to support\npose-dependent appearance modeling, where a dynamic appearance network along\nwith an optimizable feature tensor is designed to learn the\nmotion-to-appearance mapping. Moreover, by leveraging the differentiable motion\ncondition, our method enables a joint optimization of motions and appearances\nduring avatar modeling, which helps to tackle the long-standing issue of\ninaccurate motion estimation in monocular settings. The efficacy of\nGaussianAvatar is validated on both the public dataset and our collected\ndataset, demonstrating its superior performances in terms of appearance quality\nand rendering efficiency.",
            "author": [
                "Liangxiao Hu",
                "Hongwen Zhang",
                "Yuxiang Zhang",
                "Boyao Zhou",
                "Boning Liu",
                "Shengping Zhang",
                "Liqiang Nie"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02134v1",
                "http://arxiv.org/pdf/2312.02134v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02133v1",
            "title": "Style Aligned Image Generation via Shared Attention",
            "updated": "2023-12-04T18:55:35Z",
            "published": "2023-12-04T18:55:35Z",
            "summary": "Large-scale Text-to-Image (T2I) models have rapidly gained prominence across\ncreative fields, generating visually compelling outputs from textual prompts.\nHowever, controlling these models to ensure consistent style remains\nchallenging, with existing methods necessitating fine-tuning and manual\nintervention to disentangle content and style. In this paper, we introduce\nStyleAligned, a novel technique designed to establish style alignment among a\nseries of generated images. By employing minimal `attention sharing' during the\ndiffusion process, our method maintains style consistency across images within\nT2I models. This approach allows for the creation of style-consistent images\nusing a reference style through a straightforward inversion operation. Our\nmethod's evaluation across diverse styles and text prompts demonstrates\nhigh-quality synthesis and fidelity, underscoring its efficacy in achieving\nconsistent style across various inputs.",
            "author": [
                "Amir Hertz",
                "Andrey Voynov",
                "Shlomi Fruchter",
                "Daniel Cohen-Or"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02133v1",
                "http://arxiv.org/pdf/2312.02133v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02132v1",
            "title": "Hot PATE: Private Aggregation of Distributions for Diverse Task",
            "updated": "2023-12-04T18:54:34Z",
            "published": "2023-12-04T18:54:34Z",
            "summary": "The Private Aggregation of Teacher Ensembles (PATE)\nframework~\\cite{PapernotAEGT:ICLR2017} is a versatile approach to\nprivacy-preserving machine learning. In PATE, teacher models are trained on\ndistinct portions of sensitive data, and their predictions are privately\naggregated to label new training examples for a student model.\n  Until now, PATE has primarily been explored with classification-like tasks,\nwhere each example possesses a ground-truth label, and knowledge is transferred\nto the student by labeling public examples. Generative AI models, however,\nexcel in open ended \\emph{diverse} tasks with multiple valid responses and\nscenarios that may not align with traditional labeled examples. Furthermore,\nthe knowledge of models is often encapsulated in the response distribution\nitself and may be transferred from teachers to student in a more fluid way. We\npropose \\emph{hot PATE}, tailored for the diverse setting. In hot PATE, each\nteacher model produces a response distribution and the aggregation method must\npreserve both privacy and diversity of responses. We demonstrate, analytically\nand empirically, that hot PATE achieves privacy-utility tradeoffs that are\ncomparable to, and in diverse settings, significantly surpass, the baseline\n``cold'' PATE.",
            "author": [
                "Edith Cohen",
                "Xin Lyu",
                "Jelani Nelson",
                "Tamas Sarlos",
                "Uri Stemmer"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02132v1",
                "http://arxiv.org/pdf/2312.02132v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CR",
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02125v2",
            "title": "TPPoet: Transformer-Based Persian Poem Generation using Minimal Data and\n  Advanced Decoding Techniques",
            "updated": "2023-12-06T05:19:11Z",
            "published": "2023-12-04T18:52:26Z",
            "summary": "Recent advances in language models (LMs), have demonstrated significant\nefficacy in tasks related to the arts and humanities. While LMs have exhibited\nexceptional performance across a wide range of natural language processing\ntasks, there are notable challenges associated with their utilization on small\ndatasets and their ability to replicate more creative human capacities. In this\nstudy, we aim to address these challenges by training a Persian classical\npoetry generation model using a transformer architecture on a specialized\ndataset with no pretraining. Additionally, we propose a novel decoding method\nto enhance coherence and meaningfulness in the generated poetry, effectively\nmanaging the tradeoff between diversity and quality. Furthermore, the results\nof our training approach and the proposed decoding method are evaluated through\ncomprehensive set of automatic and human evaluations and showed its superior\ncapability to generate coherent and meaningful poetry in compare to other\ndecoding methods and an existing Persian large language model (LLM).",
            "author": [
                "Amir Panahandeh",
                "Hanie Asemi",
                "Esmaeil Nourani"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02125v2",
                "http://arxiv.org/pdf/2312.02125v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02124v1",
            "title": "VerA: Versatile Anonymization Fit for Clinical Facial Images",
            "updated": "2023-12-04T18:51:44Z",
            "published": "2023-12-04T18:51:44Z",
            "summary": "The escalating legislative demand for data privacy in facial image\ndissemination has underscored the significance of image anonymization. Recent\nadvancements in the field surpass traditional pixelation or blur methods, yet\nthey predominantly address regular single images. This leaves clinical image\nanonymization -- a necessity for illustrating medical interventions -- largely\nunaddressed. We present VerA, a versatile facial image anonymization that is\nfit for clinical facial images where: (1) certain semantic areas must be\npreserved to show medical intervention results, and (2) anonymizing image pairs\nis crucial for showing before-and-after results. VerA outperforms or is on par\nwith state-of-the-art methods in de-identification and photorealism for regular\nimages. In addition, we validate our results on paired anonymization, and on\nthe anonymization of both single and paired clinical images with extensive\nquantitative and qualitative evaluation.",
            "author": [
                "Majed El Helou",
                "Doruk Cetin",
                "Petar Stamenkovic",
                "Fabio Zund"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02124v1",
                "http://arxiv.org/pdf/2312.02124v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02119v1",
            "title": "Tree of Attacks: Jailbreaking Black-Box LLMs Automatically",
            "updated": "2023-12-04T18:49:23Z",
            "published": "2023-12-04T18:49:23Z",
            "summary": "While Large Language Models (LLMs) display versatile functionality, they\ncontinue to generate harmful, biased, and toxic content, as demonstrated by the\nprevalence of human-designed jailbreaks. In this work, we present Tree of\nAttacks with Pruning (TAP), an automated method for generating jailbreaks that\nonly requires black-box access to the target LLM. TAP utilizes an LLM to\niteratively refine candidate (attack) prompts using tree-of-thoughts reasoning\nuntil one of the generated prompts jailbreaks the target. Crucially, before\nsending prompts to the target, TAP assesses them and prunes the ones unlikely\nto result in jailbreaks. Using tree-of-thought reasoning allows TAP to navigate\na large search space of prompts and pruning reduces the total number of queries\nsent to the target. In empirical evaluations, we observe that TAP generates\nprompts that jailbreak state-of-the-art LLMs (including GPT4 and GPT4-Turbo)\nfor more than 80% of the prompts using only a small number of queries. This\nsignificantly improves upon the previous state-of-the-art black-box method for\ngenerating jailbreaks.",
            "author": [
                "Anay Mehrotra",
                "Manolis Zampetakis",
                "Paul Kassianik",
                "Blaine Nelson",
                "Hyrum Anderson",
                "Yaron Singer",
                "Amin Karbasi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02119v1",
                "http://arxiv.org/pdf/2312.02119v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CL",
                "cs.CR",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02254v1",
            "title": "Innovations in Agricultural Forecasting: A Multivariate Regression Study\n  on Global Crop Yield Prediction",
            "updated": "2023-12-04T18:45:28Z",
            "published": "2023-12-04T18:45:28Z",
            "summary": "The prediction of crop yields internationally is a crucial objective in\nagricultural research. Thus, this study implements 6 regression models (Linear,\nTree, Gradient Descent, Gradient Boosting, K- Nearest Neighbors, and Random\nForest) to predict crop yields in 196 countries. Given 4 key training\nparameters, pesticides (tonnes), rainfall (mm), temperature (Celsius), and\nyield (hg/ha), it was found that our Random Forest Regression model achieved a\ndetermination coefficient (r^2) of 0.94, with a margin of error (ME) of .03.\nThe models were trained and tested using the Food and Agricultural Organization\nof the United Nations data, along with the World Bank Climate Change Data\nCatalog. Furthermore, each parameter was analyzed to understand how varying\nfactors could impact overall yield. We used unconventional models, contrary to\ngenerally used Deep Learning (DL) and Machine Learning (ML) models, combined\nwith recently collected data to implement a unique approach in our research.\nExisting scholarship would benefit from understanding the most optimal model\nfor agricultural research, specifically using the United Nations data.",
            "author": [
                "Ishaan Gupta",
                "Samyutha Ayalasomayajula",
                "Yashas Shashidhara",
                "Anish Kataria",
                "Shreyas Shashidhara",
                "Krishita Kataria",
                "Aditya Undurti"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02254v1",
                "http://arxiv.org/pdf/2312.02254v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "68W03"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02112v1",
            "title": "Distributed Optimization with Feasible Set Privacy",
            "updated": "2023-12-04T18:45:04Z",
            "published": "2023-12-04T18:45:04Z",
            "summary": "We consider the setup of a constrained optimization problem with two agents\n$E_1$ and $E_2$ who jointly wish to learn the optimal solution set while\nkeeping their feasible sets $\\mathcal{P}_1$ and $\\mathcal{P}_2$ private from\neach other. The objective function $f$ is globally known and each feasible set\nis a collection of points from a global alphabet. We adopt a sequential\nsymmetric private information retrieval (SPIR) framework where one of the\nagents (say $E_1$) privately checks in $\\mathcal{P}_2$, the presence of\ncandidate solutions of the problem constrained to $\\mathcal{P}_1$ only, while\nlearning no further information on $\\mathcal{P}_2$ than the solution alone.\nFurther, we extract an information theoretically private threshold PSI (ThPSI)\nprotocol from our scheme and characterize its download cost. We show that,\ncompared to privately acquiring the feasible set $\\mathcal{P}_1\\cap\n\\mathcal{P}_2$ using an SPIR-based private set intersection (PSI) protocol, and\nfinding the optimum, our scheme is better as it incurs less information leakage\nand less download cost than the former. Over all possible uniform mappings of\n$f$ to a fixed range of values, our scheme outperforms the former with a high\nprobability.",
            "author": [
                "Shreya Meel",
                "Sennur Ulukus"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02112v1",
                "http://arxiv.org/pdf/2312.02112v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "cs.CR",
                "cs.NI",
                "eess.SP",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02111v2",
            "title": "TriDeNT: Triple Deep Network Training for Privileged Knowledge\n  Distillation in Histopathology",
            "updated": "2023-12-05T12:18:25Z",
            "published": "2023-12-04T18:43:45Z",
            "summary": "Computational pathology models rarely utilise data that will not be available\nfor inference. This means most models cannot learn from highly informative data\nsuch as additional immunohistochemical (IHC) stains and spatial\ntranscriptomics. We present TriDeNT, a novel self-supervised method for\nutilising privileged data that is not available during inference to improve\nperformance. We demonstrate the efficacy of this method for a range of\ndifferent paired data including immunohistochemistry, spatial transcriptomics\nand expert nuclei annotations. In all settings, TriDeNT outperforms other\nstate-of-the-art methods in downstream tasks, with observed improvements of up\nto 101%. Furthermore, we provide qualitative and quantitative measurements of\nthe features learned by these models and how they differ from baselines.\nTriDeNT offers a novel method to distil knowledge from scarce or costly data\nduring training, to create significantly better models for routine inputs.",
            "author": [
                "Lucas Farndale",
                "Robert Insall",
                "Ke Yuan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02111v2",
                "http://arxiv.org/pdf/2312.02111v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG",
                "q-bio.TO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02253v1",
            "title": "Diversify, Don't Fine-Tune: Scaling Up Visual Recognition Training with\n  Synthetic Images",
            "updated": "2023-12-04T18:35:27Z",
            "published": "2023-12-04T18:35:27Z",
            "summary": "Recent advances in generative deep learning have enabled the creation of\nhigh-quality synthetic images in text-to-image generation. Prior work shows\nthat fine-tuning a pretrained diffusion model on ImageNet and generating\nsynthetic training images from the finetuned model can enhance an ImageNet\nclassifier's performance. However, performance degrades as synthetic images\noutnumber real ones. In this paper, we explore whether generative fine-tuning\nis essential for this improvement and whether it is possible to further scale\nup training using more synthetic data. We present a new framework leveraging\noff-the-shelf generative models to generate synthetic training images,\naddressing multiple challenges: class name ambiguity, lack of diversity in\nnaive prompts, and domain shifts. Specifically, we leverage large language\nmodels (LLMs) and CLIP to resolve class name ambiguity. To diversify images, we\npropose contextualized diversification (CD) and stylized diversification (SD)\nmethods, also prompted by LLMs. Finally, to mitigate domain shifts, we leverage\ndomain adaptation techniques with auxiliary batch normalization for synthetic\nimages. Our framework consistently enhances recognition model performance with\nmore synthetic data, up to 6x of original ImageNet size showcasing the\npotential of synthetic data for improved recognition models and strong\nout-of-domain generalization.",
            "author": [
                "Zhuoran Yu",
                "Chenchen Zhu",
                "Sean Culatana",
                "Raghuraman Krishnamoorthi",
                "Fanyi Xiao",
                "Yong Jae Lee"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02253v1",
                "http://arxiv.org/pdf/2312.02253v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02105v1",
            "title": "Authoring Worked Examples for Java Programming with Human-AI\n  Collaboration",
            "updated": "2023-12-04T18:32:55Z",
            "published": "2023-12-04T18:32:55Z",
            "summary": "Worked examples (solutions to typical programming problems presented as a\nsource code in a certain language and are used to explain the topics from a\nprogramming class) are among the most popular types of learning content in\nprogramming classes. Most approaches and tools for presenting these examples to\nstudents are based on line-by-line explanations of the example code. However,\ninstructors rarely have time to provide line-by-line explanations for a large\nnumber of examples typically used in a programming class. In this paper, we\nexplore and assess a human-AI collaboration approach to authoring worked\nexamples for Java programming. We introduce an authoring system for creating\nJava worked examples that generates a starting version of code explanations and\npresents it to the instructor to edit if necessary. We also present a study\nthat assesses the quality of explanations created with this approach.",
            "author": [
                "Mohammad Hassany",
                "Peter Brusilovsky",
                "Jiaze Ke",
                "Kamil Akhuseyinoglu",
                "Arun Balajiee Lekshmi Narayanan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02105v1",
                "http://arxiv.org/pdf/2312.02105v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02103v1",
            "title": "Learning Pseudo-Labeler beyond Noun Concepts for Open-Vocabulary Object\n  Detection",
            "updated": "2023-12-04T18:29:03Z",
            "published": "2023-12-04T18:29:03Z",
            "summary": "Open-vocabulary object detection (OVOD) has recently gained significant\nattention as a crucial step toward achieving human-like visual intelligence.\nExisting OVOD methods extend target vocabulary from pre-defined categories to\nopen-world by transferring knowledge of arbitrary concepts from vision-language\npre-training models to the detectors. While previous methods have shown\nremarkable successes, they suffer from indirect supervision or limited\ntransferable concepts. In this paper, we propose a simple yet effective method\nto directly learn region-text alignment for arbitrary concepts. Specifically,\nthe proposed method aims to learn arbitrary image-to-text mapping for\npseudo-labeling of arbitrary concepts, named Pseudo-Labeling for Arbitrary\nConcepts (PLAC). The proposed method shows competitive performance on the\nstandard OVOD benchmark for noun concepts and a large improvement on referring\nexpression comprehension benchmark for arbitrary concepts.",
            "author": [
                "Sunghun Kang",
                "Junbum Cha",
                "Jonghwan Mun",
                "Byungseok Roh",
                "Chang D. Yoo"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02103v1",
                "http://arxiv.org/pdf/2312.02103v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02102v1",
            "title": "Mitigating Data Injection Attacks on Federated Learning",
            "updated": "2023-12-04T18:26:31Z",
            "published": "2023-12-04T18:26:31Z",
            "summary": "Federated learning is a technique that allows multiple entities to\ncollaboratively train models using their data without compromising data\nprivacy. However, despite its advantages, federated learning can be susceptible\nto false data injection attacks. In these scenarios, a malicious entity with\ncontrol over specific agents in the network can manipulate the learning\nprocess, leading to a suboptimal model. Consequently, addressing these data\ninjection attacks presents a significant research challenge in federated\nlearning systems. In this paper, we propose a novel technique to detect and\nmitigate data injection attacks on federated learning systems. Our mitigation\nmethod is a local scheme, performed during a single instance of training by the\ncoordinating node, allowing the mitigation during the convergence of the\nalgorithm. Whenever an agent is suspected to be an attacker, its data will be\nignored for a certain period, this decision will often be re-evaluated. We\nprove that with probability 1, after a finite time, all attackers will be\nignored while the probability of ignoring a trustful agent becomes 0, provided\nthat there is a majority of truthful agents. Simulations show that when the\ncoordinating node detects and isolates all the attackers, the model recovers\nand converges to the truthful model.",
            "author": [
                "Or Shalom",
                "Amir Leshem",
                "Waheed U. Bajwa"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02102v1",
                "http://arxiv.org/pdf/2312.02102v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CR",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02095v1",
            "title": "Single-sample versus case-control sampling scheme for Positive Unlabeled\n  data: the story of two scenarios",
            "updated": "2023-12-04T18:13:58Z",
            "published": "2023-12-04T18:13:58Z",
            "summary": "In the paper we argue that performance of the classifiers based on Empirical\nRisk Minimization (ERM) for positive unlabeled data, which are designed for\ncase-control sampling scheme may significantly deteriorate when applied to a\nsingle-sample scenario. We reveal why their behavior depends, in all but very\nspecific cases, on the scenario. Also, we introduce a single-sample case\nanalogue of the popular non-negative risk classifier designed for case-control\ndata and compare its performance with the original proposal. We show that the\nsignificant differences occur between them, especiall when half or more\npositive of observations are labeled. The opposite case when ERM minimizer\ndesigned for the case-control case is applied for single-sample data is also\nconsidered and similar conclusions are drawn. Taking into account difference of\nscenarios requires a sole, but crucial, change in the definition of the\nEmpirical Risk.",
            "author": [
                "Jan Mielniczuk",
                "Adam Wawrze\u0144czyk"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02095v1",
                "http://arxiv.org/pdf/2312.02095v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02093v1",
            "title": "Cultural Differences in Students' Privacy Concerns in Learning Analytics\n  across Germany, South Korea, Spain, Sweden, and the United States",
            "updated": "2023-12-04T18:10:20Z",
            "published": "2023-12-04T18:10:20Z",
            "summary": "Applications of learning analytics (LA) can raise concerns from students\nabout their privacy in higher education contexts. Developing effective\nprivacy-enhancing practices requires a systematic understanding of students'\nprivacy concerns and how they vary across national and cultural dimensions. We\nconducted a survey study with established instruments to measure privacy\nconcerns and cultural values for university students in five countries\n(Germany, South Korea, Spain, Sweden, and the United States; N = 762). The\nresults show that students generally trusted institutions with their data and\ndisclosed information as they perceived the risks to be manageable even though\nthey felt somewhat limited in their ability to control their privacy. Across\nthe five countries, German and Swedish students stood out as the most trusting\nand least concerned, especially compared to US students who reported greater\nperceived risk and less control. Students in South Korea and Spain responded\nsimilarly on all five privacy dimensions (perceived privacy risk, perceived\nprivacy control, privacy concerns, trusting beliefs, and non-self-disclosure\nbehavior), despite their significant cultural differences. Culture measured at\nthe individual level affected the antecedents and outcomes of privacy concerns\nmore than country-level culture. Perceived privacy risk and privacy control\nincrease with power distance. Trusting beliefs increase with a desire for\nuncertainty avoidance and lower masculinity. Non-self-disclosure behaviors rise\nwith power distance and masculinity, and decrease with more uncertainty\navoidance. Thus, cultural values related to trust in institutions, social\nequality and risk-taking should be considered when developing privacy-enhancing\npractices and policies in higher education.",
            "author": [
                "Olga Viberg",
                "Ren\u00e9 F. Kizilcec",
                "Ioana Jivet",
                "Alejandra Mart\u00ednez Mon\u00e9s",
                "Alice Oh",
                "Chantal Mutimukwe",
                "Stefan Hrastinski",
                "Maren Scheffel"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02093v1",
                "http://arxiv.org/pdf/2312.02093v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02251v1",
            "title": "Fine-Tuning Language Models for Context-Specific SQL Query Generation",
            "updated": "2023-12-04T18:04:27Z",
            "published": "2023-12-04T18:04:27Z",
            "summary": "The ability to generate SQL queries from natural language has significant\nimplications for making data accessible to non-specialists. This paper presents\na novel approach to fine-tuning open-source large language models (LLMs) for\nthe task of transforming natural language into SQL queries within the retail\ndomain. We introduce models specialized in generating SQL queries, trained on\nsynthetic datasets tailored to the Snowflake SQL and GoogleSQL dialects. Our\nmethodology involves generating a context-specific dataset using GPT-4, then\nfine-tuning three open-source LLMs(Starcoder Plus, Code-Llama, and Mistral)\nemploying the LoRa technique to optimize for resource constraints. The\nfine-tuned models demonstrate superior performance in zero-shot settings\ncompared to the baseline GPT-4, with Code-Llama achieving the highest accuracy\nrates, at 81.58% for Snowflake SQL and 82.66% for GoogleSQL. These results\nunderscore the effectiveness of fine-tuning LLMs on domain-specific tasks and\nsuggest a promising direction for enhancing the accessibility of relational\ndatabases through natural language interfaces.",
            "author": [
                "Amine Rebei"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02251v1",
                "http://arxiv.org/pdf/2312.02251v1"
            ],
            "primary_category": "cs.DB",
            "category": [
                "cs.DB",
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02082v1",
            "title": "Joint State and Input Estimation for Linear Dynamical Systems with\n  Sparse Control",
            "updated": "2023-12-04T17:49:21Z",
            "published": "2023-12-04T17:49:21Z",
            "summary": "Sparsity constraints on the control inputs of a linear dynamical system\nnaturally arise in several practical applications such as networked control,\ncomputer vision, seismic signal processing, and cyber-physical systems. In this\nwork, we consider the problem of jointly estimating the states and sparse\ninputs of such systems from low-dimensional (compressive) measurements. Due to\nthe low-dimensional measurements, conventional Kalman filtering and smoothing\nalgorithms fail to accurately estimate the states and inputs. We present a\nBayesian approach that exploits the input sparsity to significantly improve\nestimation accuracy. Sparsity in the input estimates is promoted by using\ndifferent prior distributions on the input. We investigate two main approaches:\nregularizer-based MAP, and {Bayesian learning-based estimation}. We also extend\nthe approaches to handle control inputs with common support and analyze the\ntime and memory complexities of the presented algorithms. Finally, using\nnumerical simulations, we show that our algorithms outperform the\nstate-of-the-art methods in terms of accuracy and time/memory complexities,\nespecially in the low-dimensional measurement regime.",
            "author": [
                "Rupam Kalyan Chakraborty",
                "Geethu Joseph",
                "Chandra R. Murthy"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02082v1",
                "http://arxiv.org/pdf/2312.02082v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02079v2",
            "title": "Deep Set Neural Networks for forecasting asynchronous bioprocess\n  timeseries",
            "updated": "2023-12-05T22:20:50Z",
            "published": "2023-12-04T17:46:57Z",
            "summary": "Cultivation experiments often produce sparse and irregular time series.\nClassical approaches based on mechanistic models, like Maximum Likelihood\nfitting or Monte-Carlo Markov chain sampling, can easily account for sparsity\nand time-grid irregularities, but most statistical and Machine Learning tools\nare not designed for handling sparse data out-of-the-box. Among popular\napproaches there are various schemes for filling missing values (imputation)\nand interpolation into a regular grid (alignment). However, such methods\ntransfer the biases of the interpolation or imputation models to the target\nmodel. We show that Deep Set Neural Networks equipped with triplet encoding of\nthe input data can successfully handle bio-process data without any need for\nimputation or alignment procedures. The method is agnostic to the particular\nnature of the time series and can be adapted for any task, for example, online\nmonitoring, predictive control, design of experiments, etc. In this work, we\nfocus on forecasting. We argue that such an approach is especially suitable for\ntypical cultivation processes, demonstrate the performance of the method on\nseveral forecasting tasks using data generated from macrokinetic growth models\nunder realistic conditions, and compare the method to a conventional fitting\nprocedure and methods based on imputation and alignment.",
            "author": [
                "Maxim Borisyak",
                "Stefan Born",
                "Peter Neubauer",
                "Mariano Nicolas Cruz-Bournazou"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02079v2",
                "http://arxiv.org/pdf/2312.02079v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02078v1",
            "title": "Integrating AI into CCTV Systems: A Comprehensive Evaluation of Smart\n  Video Surveillance in Community Space",
            "updated": "2023-12-04T17:41:52Z",
            "published": "2023-12-04T17:41:52Z",
            "summary": "This article presents an AI-enabled Smart Video Surveillance (SVS) designed\nto enhance safety in community spaces such as educational and recreational\nareas, and small businesses. The proposed system innovatively integrates with\nexisting CCTV and wired camera networks, simplifying its adoption across\nvarious community cases to leverage recent AI advancements. Our SVS system,\nfocusing on privacy, uses metadata instead of pixel data for activity\nrecognition, aligning with ethical standards. It features cloud-based\ninfrastructure and a mobile app for real-time, privacy-conscious alerts in\ncommunities.\n  This article notably pioneers a comprehensive real-world evaluation of the\nSVS system, covering AI-driven visual processing, statistical analysis,\ndatabase management, cloud communication, and user notifications. It's also the\nfirst to assess an end-to-end anomaly detection system's performance, vital for\nidentifying potential public safety incidents.\n  For our evaluation, we implemented the system in a community college, serving\nas an ideal model to exemplify the proposed system's capabilities. Our findings\nin this setting demonstrate the system's robustness, with throughput, latency,\nand scalability effectively managing 16 CCTV cameras. The system maintained a\nconsistent 16.5 frames per second (FPS) over a 21-hour operation. The average\nend-to-end latency for detecting behavioral anomalies and alerting users was\n26.76 seconds.",
            "author": [
                "Shanle Yao",
                "Babak Rahimi Ardabili",
                "Armin Danesh Pazho",
                "Ghazal Alinezhad Noghre",
                "Christopher Neff",
                "Hamed Tabkhi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02078v1",
                "http://arxiv.org/pdf/2312.02078v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02074v1",
            "title": "Federated Learning is Better with Non-Homomorphic Encryption",
            "updated": "2023-12-04T17:37:41Z",
            "published": "2023-12-04T17:37:41Z",
            "summary": "Traditional AI methodologies necessitate centralized data collection, which\nbecomes impractical when facing problems with network communication, data\nprivacy, or storage capacity. Federated Learning (FL) offers a paradigm that\nempowers distributed AI model training without collecting raw data. There are\ndifferent choices for providing privacy during FL training. One of the popular\nmethodologies is employing Homomorphic Encryption (HE) - a breakthrough in\nprivacy-preserving computation from Cryptography. However, these methods have a\nprice in the form of extra computation and memory footprint. To resolve these\nissues, we propose an innovative framework that synergizes permutation-based\ncompressors with Classical Cryptography, even though employing Classical\nCryptography was assumed to be impossible in the past in the context of FL. Our\nframework offers a way to replace HE with cheaper Classical Cryptography\nprimitives which provides security for the training process. It fosters\nasynchronous communication and provides flexible deployment options in various\ncommunication topologies.",
            "author": [
                "Konstantin Burlachenko",
                "Abdulmajeed Alrowithi",
                "Fahad Ali Albalawi",
                "Peter Richtarik"
            ],
            "link": [
                "http://dx.doi.org/10.1145/3630048.3630182",
                "http://arxiv.org/abs/2312.02074v1",
                "http://arxiv.org/pdf/2312.02074v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.LG",
                "math.OC",
                "G.1.6; E.3"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02067v1",
            "title": "Learning Feynman integrals from differential equations with neural\n  networks",
            "updated": "2023-12-04T17:26:49Z",
            "published": "2023-12-04T17:26:49Z",
            "summary": "We present a new approach for evaluating Feynman integrals numerically. We\napply the recently-proposed framework of physics-informed deep learning to\ntrain neural networks to approximate the solution to the differential equations\nsatisfied by the Feynman integrals. This approach relies neither on a canonical\nform of the differential equations, which is often a bottleneck for the\nanalytical techniques, nor on the availability of a large dataset, and after\ntraining yields essentially instantaneous evaluation times. We provide a\nproof-of-concept implementation within the PyTorch framework, and apply it to a\nnumber of one- and two-loop examples, achieving a mean magnitude of relative\ndifference of around 1% at two loops in the physical phase space with network\ntraining times on the order of an hour on a laptop GPU.",
            "author": [
                "Francesco Calisto",
                "Ryan Moodie",
                "Simone Zoia"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02067v1",
                "http://arxiv.org/pdf/2312.02067v1"
            ],
            "primary_category": "hep-ph",
            "category": [
                "hep-ph",
                "hep-th"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02063v1",
            "title": "The GPU Phase Folding and Deep Learning Method for Detecting Exoplanet\n  Transits",
            "updated": "2023-12-04T17:19:37Z",
            "published": "2023-12-04T17:19:37Z",
            "summary": "This paper presents GPFC, a novel Graphics Processing Unit (GPU) Phase\nFolding and Convolutional Neural Network (CNN) system to detect exoplanets\nusing the transit method. We devise a fast folding algorithm parallelized on a\nGPU to amplify low signal-to-noise ratio transit signals, allowing a search at\nhigh precision and speed. A CNN trained on two million synthetic light curves\nreports a score indicating the likelihood of a planetary signal at each period.\nGPFC improves on speed by three orders of magnitude over the predominant\nBox-fitting Least Squares (BLS) method. Our simulation results show GPFC\nachieves 97% training accuracy, higher true positive rate at the same false\npositive rate of detection, and higher precision at the same recall rate when\ncompared to BLS. GPFC recovers 100% of known ultra-short-period planets in\nKepler light curves from a blind search. These results highlight the promise of\nGPFC as an alternative approach to the traditional BLS algorithm for finding\nnew transiting exoplanets in data taken with Kepler and other space transit\nmissions such as K2, TESS and future PLATO and Earth 2.0.",
            "author": [
                "Kaitlyn Wang",
                "Kevin Wang",
                "Jian Ge",
                "Yinan Zhao",
                "Kevin Willis"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02063v1",
                "http://arxiv.org/pdf/2312.02063v1"
            ],
            "primary_category": "astro-ph.EP",
            "category": [
                "astro-ph.EP",
                "astro-ph.IM",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02057v1",
            "title": "An improved bound on sums of square roots via the subspace theorem",
            "updated": "2023-12-04T17:15:12Z",
            "published": "2023-12-04T17:15:12Z",
            "summary": "The sum of square roots is as follows: Given $x_1,\\dots,x_n \\in \\mathbb{Z}$\nand $a_1,\\dots,a_n \\in \\mathbb{N}$ decide whether $ E=\\sum_{i=1}^n x_i\n\\sqrt{a_i} \\geq 0$. It is a prominent open problem (Problem 33 of the Open\nProblems Project), whether this can be decided in polynomial time. The\nstate-of-the-art methods rely on separation bounds, which are lower bounds on\nthe minimum nonzero absolute value of $E$. The current best bound shows that\n$|E| \\geq \\left(n \\cdot \\max_i (|x_i| \\cdot \\sqrt{a_i})\\right)^{-2^n} $, which\nis doubly exponentially small.\n  We provide a new bound of the form $|E| \\geq \\gamma \\cdot (n \\cdot\n\\max_i|x_i|)^{-2n}$ where $\\gamma $ is a constant depending on $a_1,\\dots,a_n$.\nThis is singly exponential in $n$ for fixed $a_1,\\dots,a_n$. The constant\n$\\gamma$ is not explicit and stems from the subspace theorem, a deep result in\nthe geometry of numbers.",
            "author": [
                "Friedrich Eisenbrand",
                "Matthieu Haeberle",
                "Neta Singer"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02057v1",
                "http://arxiv.org/pdf/2312.02057v1"
            ],
            "primary_category": "cs.CG",
            "category": [
                "cs.CG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02052v1",
            "title": "DUCK: Distance-based Unlearning via Centroid Kinematics",
            "updated": "2023-12-04T17:10:25Z",
            "published": "2023-12-04T17:10:25Z",
            "summary": "Machine Unlearning is rising as a new field, driven by the pressing necessity\nof ensuring privacy in modern artificial intelligence models. This technique\nprimarily aims to eradicate any residual influence of a specific subset of data\nfrom the knowledge acquired by a neural model during its training. This work\nintroduces a novel unlearning algorithm, denoted as Distance-based Unlearning\nvia Centroid Kinematics (DUCK), which employs metric learning to guide the\nremoval of samples matching the nearest incorrect centroid in the embedding\nspace. Evaluation of the algorithm's performance is conducted across various\nbenchmark datasets in two distinct scenarios, class removal, and homogeneous\nsampling removal, obtaining state-of-the-art performance. We introduce a novel\nmetric, called Adaptive Unlearning Score (AUS), encompassing not only the\nefficacy of the unlearning process in forgetting target data but also\nquantifying the performance loss relative to the original model. Moreover, we\npropose a novel membership inference attack to assess the algorithm's capacity\nto erase previously acquired knowledge, designed to be adaptable to future\nmethodologies.",
            "author": [
                "Marco Cotogni",
                "Jacopo Bonato",
                "Luigi Sabetta",
                "Francesco Pelosin",
                "Alessandro Nicolosi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02052v1",
                "http://arxiv.org/pdf/2312.02052v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03755v1",
            "title": "Near-real-time Earthquake-induced Fatality Estimation using Crowdsourced\n  Data and Large-Language Models",
            "updated": "2023-12-04T17:09:58Z",
            "published": "2023-12-04T17:09:58Z",
            "summary": "When a damaging earthquake occurs, immediate information about casualties is\ncritical for time-sensitive decision-making by emergency response and aid\nagencies in the first hours and days. Systems such as Prompt Assessment of\nGlobal Earthquakes for Response (PAGER) by the U.S. Geological Survey (USGS)\nwere developed to provide a forecast within about 30 minutes of any significant\nearthquake globally. Traditional systems for estimating human loss in disasters\noften depend on manually collected early casualty reports from global media, a\nprocess that's labor-intensive and slow with notable time delays. Recently,\nsome systems have employed keyword matching and topic modeling to extract\nrelevant information from social media. However, these methods struggle with\nthe complex semantics in multilingual texts and the challenge of interpreting\never-changing, often conflicting reports of death and injury numbers from\nvarious unverified sources on social media platforms. In this work, we\nintroduce an end-to-end framework to significantly improve the timeliness and\naccuracy of global earthquake-induced human loss forecasting using\nmulti-lingual, crowdsourced social media. Our framework integrates (1) a\nhierarchical casualty extraction model built upon large language models, prompt\ndesign, and few-shot learning to retrieve quantitative human loss claims from\nsocial media, (2) a physical constraint-aware, dynamic-truth discovery model\nthat discovers the truthful human loss from massive noisy and potentially\nconflicting human loss claims, and (3) a Bayesian updating loss projection\nmodel that dynamically updates the final loss estimation using discovered\ntruths. We test the framework in real-time on a series of global earthquake\nevents in 2021 and 2022 and show that our framework streamlines casualty data\nretrieval, achieving speed and accuracy comparable to manual methods by USGS.",
            "author": [
                "Chenguang Wang",
                "Davis Engler",
                "Xuechun Li",
                "James Hou",
                "David J. Wald",
                "Kishor Jaiswal",
                "Susu Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03755v1",
                "http://arxiv.org/pdf/2312.03755v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.CY",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02046v1",
            "title": "Stellar mass is not the best predictor of galaxy metallicity. The\n  gravitational potential-metallicity relation $\u03a6\\rm ZR$",
            "updated": "2023-12-04T17:02:03Z",
            "published": "2023-12-04T17:02:03Z",
            "summary": "Interpreting the scaling relations followed by galaxies is a fundamental tool\nfor assessing how well we understand galaxy formation and evolution. Several\nscaling relations involving the galaxy metallicity have been discovered through\nthe years, the foremost of which is the scaling with stellar mass. This\nso-called mass-metallicity relation is thought to be fundamental and has been\nsubject to many studies in the literature. We study the dependence of the\ngas-phase metallicity on many different galaxy properties to assess which of\nthem determines the metallicity of a galaxy. We applied a random forest\nregressor algorithm on a sample of more than 3000 nearby galaxies from the\nSDSS-IV MaNGA survey. Using this machine-learning technique, we explored the\neffect of 148 parameters on the global oxygen abundance as an indicator of the\ngas metallicity. $M_{\\rm \\star}$/$R_e$, as a proxy for the baryonic\ngravitational potential of the galaxy, is found to be the primary factor\ndetermining the average gas-phase metallicity of the galaxy ($Z_g$). It\noutweighs stellar mass. A subsequent analysis provides the strongest dependence\nof $Z_g$ on $M_\\star / R_e^{\\,0.6}$. We argue that this parameter traces the\ntotal gravitational potential, and the exponent $\\alpha\\simeq 0.6$ accounts for\nthe inclusion of the dark matter component. Our results reveal the importance\nof the relation between the total gravitational potential of the galaxy and the\ngas metallicity. This relation is tighter and likely more primordial than the\nwidely known mass-metallicity relation.",
            "author": [
                "Laura S\u00e1nchez-Menguiano",
                "Jorge S\u00e1nchez Almeida",
                "Sebasti\u00e1n F. S\u00e1nchez",
                "Casiana Mu\u00f1oz-Tu\u00f1\u00f3n"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02046v1",
                "http://arxiv.org/pdf/2312.02046v1"
            ],
            "primary_category": "astro-ph.GA",
            "category": [
                "astro-ph.GA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02045v1",
            "title": "Fast Posterior Probability Sampling with Normalizing Flows and Its\n  Applicability in Bayesian analysis in Particle Physics",
            "updated": "2023-12-04T17:01:30Z",
            "published": "2023-12-04T17:01:30Z",
            "summary": "In this study, we use Rational-Quadratic Neural Spline Flows, a sophisticated\nparametrization of Normalizing Flows, for inferring posterior probability\ndistributions in scenarios where direct evaluation of the likelihood is\nchallenging at inference time. We exemplify this approach using the T2K near\ndetector as a working example, focusing on learning the posterior probability\ndistribution of neutrino flux binned in neutrino energy. The predictions of the\ntrained model are conditioned at inference time by the momentum and angle of\nthe outgoing muons released after neutrino-nuclei interaction. This\nconditioning allows for the generation of personalized posterior distributions,\ntailored to the muon observables, all without necessitating a full retraining\nof the model for each new dataset. The performances of the model are studied\nfor different shapes of the posterior distributions.",
            "author": [
                "Mathias El Baz",
                "Federico S\u00e1nchez"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02045v1",
                "http://arxiv.org/pdf/2312.02045v1"
            ],
            "primary_category": "physics.data-an",
            "category": [
                "physics.data-an",
                "hep-ex"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02037v1",
            "title": "GFS: Graph-based Feature Synthesis for Prediction over Relational\n  Databases",
            "updated": "2023-12-04T16:54:40Z",
            "published": "2023-12-04T16:54:40Z",
            "summary": "Relational databases are extensively utilized in a variety of modern\ninformation system applications, and they always carry valuable data patterns.\nThere are a huge number of data mining or machine learning tasks conducted on\nrelational databases. However, it is worth noting that there are limited\nmachine learning models specifically designed for relational databases, as most\nmodels are primarily tailored for single table settings. Consequently, the\nprevalent approach for training machine learning models on data stored in\nrelational databases involves performing feature engineering to merge the data\nfrom multiple tables into a single table and subsequently applying single table\nmodels. This approach not only requires significant effort in feature\nengineering but also destroys the inherent relational structure present in the\ndata. To address these challenges, we propose a novel framework called\nGraph-based Feature Synthesis (GFS). GFS formulates the relational database as\na heterogeneous graph, thereby preserving the relational structure within the\ndata. By leveraging the inductive bias from single table models, GFS\neffectively captures the intricate relationships inherent in each table.\nAdditionally, the whole framework eliminates the need for manual feature\nengineering. In the extensive experiment over four real-world multi-table\nrelational databases, GFS outperforms previous methods designed for relational\ndatabases, demonstrating its superior performance.",
            "author": [
                "Han Zhang",
                "Quan Gan",
                "David Wipf",
                "Weinan Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02037v1",
                "http://arxiv.org/pdf/2312.02037v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.DB"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02034v1",
            "title": "Trust, distrust, and appropriate reliance in (X)AI: a survey of\n  empirical evaluation of user trust",
            "updated": "2023-12-04T16:53:11Z",
            "published": "2023-12-04T16:53:11Z",
            "summary": "A current concern in the field of Artificial Intelligence (AI) is to ensure\nthe trustworthiness of AI systems. The development of explainability methods is\none prominent way to address this, which has often resulted in the assumption\nthat the use of explainability will lead to an increase in the trust of users\nand wider society. However, the dynamics between explainability and trust are\nnot well established and empirical investigations of their relation remain\nmixed or inconclusive. In this paper we provide a detailed description of the\nconcepts of user trust and distrust in AI and their relation to appropriate\nreliance. For that we draw from the fields of machine learning, human-computer\ninteraction, and the social sciences. Furthermore, we have created a survey of\nexisting empirical studies that investigate the effects of AI systems and XAI\nmethods on user (dis)trust. With clarifying the concepts and summarizing the\nempirical investigations, we aim to provide researchers, who examine user trust\nin AI, with an improved starting point for developing user studies to measure\nand evaluate the user's attitude towards and reliance on AI systems.",
            "author": [
                "Roel Visser",
                "Tobias M. Peters",
                "Ingrid Scharlau",
                "Barbara Hammer"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02034v1",
                "http://arxiv.org/pdf/2312.02034v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02029v1",
            "title": "Implicit Learning of Scene Geometry from Poses for Global Localization",
            "updated": "2023-12-04T16:51:23Z",
            "published": "2023-12-04T16:51:23Z",
            "summary": "Global visual localization estimates the absolute pose of a camera using a\nsingle image, in a previously mapped area. Obtaining the pose from a single\nimage enables many robotics and augmented/virtual reality applications.\nInspired by latest advances in deep learning, many existing approaches directly\nlearn and regress 6 DoF pose from an input image. However, these methods do not\nfully utilize the underlying scene geometry for pose regression. The challenge\nin monocular relocalization is the minimal availability of supervised training\ndata, which is just the corresponding 6 DoF poses of the images. In this paper,\nwe propose to utilize these minimal available labels (.i.e, poses) to learn the\nunderlying 3D geometry of the scene and use the geometry to estimate the 6 DoF\ncamera pose. We present a learning method that uses these pose labels and rigid\nalignment to learn two 3D geometric representations (\\textit{X, Y, Z\ncoordinates}) of the scene, one in camera coordinate frame and the other in\nglobal coordinate frame. Given a single image, it estimates these two 3D scene\nrepresentations, which are then aligned to estimate a pose that matches the\npose label. This formulation allows for the active inclusion of additional\nlearning constraints to minimize 3D alignment errors between the two 3D scene\nrepresentations, and 2D re-projection errors between the 3D global scene\nrepresentation and 2D image pixels, resulting in improved localization\naccuracy. During inference, our model estimates the 3D scene geometry in camera\nand global frames and aligns them rigidly to obtain pose in real-time. We\nevaluate our work on three common visual localization datasets, conduct\nablation studies, and show that our method exceeds state-of-the-art regression\nmethods' pose accuracy on all datasets.",
            "author": [
                "Mohammad Altillawi",
                "Shile Li",
                "Sai Manoj Prakhya",
                "Ziyuan Liu",
                "Joan Serrat"
            ],
            "link": [
                "http://dx.doi.org/10.1109/LRA.2023.3337699",
                "http://arxiv.org/abs/2312.02029v1",
                "http://arxiv.org/pdf/2312.02029v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02027v1",
            "title": "Stochastic Optimal Control Matching",
            "updated": "2023-12-04T16:49:43Z",
            "published": "2023-12-04T16:49:43Z",
            "summary": "Stochastic optimal control, which has the goal of driving the behavior of\nnoisy systems, is broadly applicable in science, engineering and artificial\nintelligence. Our work introduces Stochastic Optimal Control Matching (SOCM), a\nnovel Iterative Diffusion Optimization (IDO) technique for stochastic optimal\ncontrol that stems from the same philosophy as the conditional score matching\nloss for diffusion models. That is, the control is learned via a least squares\nproblem by trying to fit a matching vector field. The training loss, which is\nclosely connected to the cross-entropy loss, is optimized with respect to both\nthe control function and a family of reparameterization matrices which appear\nin the matching vector field. The optimization with respect to the\nreparameterization matrices aims at minimizing the variance of the matching\nvector field. Experimentally, our algorithm achieves lower error than all the\nexisting IDO techniques for stochastic optimal control for four different\ncontrol settings. The key idea underlying SOCM is the path-wise\nreparameterization trick, a novel technique that is of independent interest,\ne.g., for generative modeling.",
            "author": [
                "Carles Domingo-Enrich",
                "Jiequn Han",
                "Brandon Amos",
                "Joan Bruna",
                "Ricky T. Q. Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02027v1",
                "http://arxiv.org/pdf/2312.02027v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "cs.LG",
                "cs.NA",
                "math.NA",
                "math.PR",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02022v1",
            "title": "Neutrino emission of neutron-star superbursts",
            "updated": "2023-12-04T16:47:15Z",
            "published": "2023-12-04T16:47:15Z",
            "summary": "Superbursts of neutron stars are rare but powerful events explained by the\nexplosive burning of carbon in the deep layers of the outer envelope of the\nstar. In this paper we perform a simulation of superbursts and propose a simple\nmethod for describing the neutrino stage of their cooling, as well as a method\nfor describing the evolution of the burst energy on a scale of several months.\nWe note a universal relation for the temperature distribution in the burnt\nlayer at its neutrino cooling stage, as well as the unification of bolometric\nlight curves and neutrino heat loss rates for deep and powerful bursts. We\npoint out the possibility of long-term retention of the burst energy in the\nstar's envelope. The results can be useful for interpretation of superburst\nobservations.",
            "author": [
                "A. D. Kaminker",
                "A. Y. Potekhin",
                "D. G. Yakovlev"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02022v1",
                "http://arxiv.org/pdf/2312.02022v1"
            ],
            "primary_category": "astro-ph.HE",
            "category": [
                "astro-ph.HE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02021v1",
            "title": "VLTSeg: Simple Transfer of CLIP-Based Vision-Language Representations\n  for Domain Generalized Semantic Segmentation",
            "updated": "2023-12-04T16:46:38Z",
            "published": "2023-12-04T16:46:38Z",
            "summary": "Domain generalization (DG) remains a significant challenge for perception\nbased on deep neural networks (DNN), where domain shifts occur due to lighting,\nweather, or geolocation changes. In this work, we propose VLTSeg to enhance\ndomain generalization in semantic segmentation, where the network is solely\ntrained on the source domain and evaluated on unseen target domains. Our method\nleverages the inherent semantic robustness of vision-language models. First, by\nsubstituting traditional vision-only backbones with pre-trained encoders from\nCLIP and EVA-CLIP as transfer learning setting we find that in the field of DG,\nvision-language pre-training significantly outperforms supervised and\nself-supervised vision pre-training. We thus propose a new vision-language\napproach for domain generalized segmentation, which improves the domain\ngeneralization SOTA by 7.6% mIoU when training on the synthetic GTA5 dataset.\nWe further show the superior generalization capabilities of vision-language\nsegmentation models by reaching 76.48% mIoU on the popular Cityscapes-to-ACDC\nbenchmark, outperforming the previous SOTA approach by 6.9% mIoU on the test\nset at the time of writing. Additionally, our approach shows strong in-domain\ngeneralization capabilities indicated by 86.1% mIoU on the Cityscapes test set,\nresulting in a shared first place with the previous SOTA on the current\nleaderboard at the time of submission.",
            "author": [
                "Christoph H\u00fcmmer",
                "Manuel Schwonberg",
                "Liangwei Zhong",
                "Hu Cao",
                "Alois Knoll",
                "Hanno Gottschalk"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02021v1",
                "http://arxiv.org/pdf/2312.02021v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02020v1",
            "title": "H\u00fcckel Molecular Orbital Theory on a Quantum Computer: A Scalable\n  System-Agnostic Variational Implementation with Compact Encoding",
            "updated": "2023-12-04T16:44:53Z",
            "published": "2023-12-04T16:44:53Z",
            "summary": "H\\\"uckel molecular orbital (HMO) theory provides a semi-empirical treatment\nof the electronic structure in conjugated {\\pi}-electronic systems. A scalable\nsystem-agnostic execution of HMO theory on a quantum computer is reported here\nbased on a variational quantum deflation (VQD) algorithm for excited state\nquantum simulation. A compact encoding scheme is proposed here that provides an\nexponential advantage over direct mapping and allows quantum simulation of the\nHMO model for systems with up to 2^N conjugated centers in N qubits. The\ntransformation of the H\\\"uckel Hamiltonian to qubit space is achieved by two\ndifferent strategies: a machine-learning-assisted transformation and the\nFrobenius-inner-product-based transformation. These methods are tested on a\nseries of linear, cyclic, and hetero-nuclear conjugated {\\pi}-electronic\nsystems. The molecular orbital energy levels and wavefunctions from the quantum\nsimulation are in excellent agreement with the exact classical results. The\nhigher excited states of large systems, however, are found to suffer from error\naccumulation in the VQD simulation. This is mitigated by formulating a variant\nof VQD that exploits the symmetry of the Hamiltonian. This strategy has been\nsuccessfully demonstrated for the quantum simulation of C_{60} fullerene\ncontaining 680 Pauli strings encoded on six qubits. The methods developed in\nthis work are system-agnostic and hence are easily adaptable to similar\nproblems of different complexity in other fields of research.",
            "author": [
                "Harshdeep Singh",
                "Sonjoy Majumder",
                "Sabyashachi Mishra"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02020v1",
                "http://arxiv.org/pdf/2312.02020v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02019v1",
            "title": "Action Inference by Maximising Evidence: Zero-Shot Imitation from\n  Observation with World Models",
            "updated": "2023-12-04T16:43:36Z",
            "published": "2023-12-04T16:43:36Z",
            "summary": "Unlike most reinforcement learning agents which require an unrealistic amount\nof environment interactions to learn a new behaviour, humans excel at learning\nquickly by merely observing and imitating others. This ability highly depends\non the fact that humans have a model of their own embodiment that allows them\nto infer the most likely actions that led to the observed behaviour. In this\npaper, we propose Action Inference by Maximising Evidence (AIME) to replicate\nthis behaviour using world models. AIME consists of two distinct phases. In the\nfirst phase, the agent learns a world model from its past experience to\nunderstand its own body by maximising the ELBO. While in the second phase, the\nagent is given some observation-only demonstrations of an expert performing a\nnovel task and tries to imitate the expert's behaviour. AIME achieves this by\ndefining a policy as an inference model and maximising the evidence of the\ndemonstration under the policy and world model. Our method is \"zero-shot\" in\nthe sense that it does not require further training for the world model or\nonline interactions with the environment after given the demonstration. We\nempirically validate the zero-shot imitation performance of our method on the\nWalker and Cheetah embodiment of the DeepMind Control Suite and find it\noutperforms the state-of-the-art baselines. Code is available at:\nhttps://github.com/argmax-ai/aime.",
            "author": [
                "Xingyuan Zhang",
                "Philip Becker-Ehmck",
                "Patrick van der Smagt",
                "Maximilian Karl"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02019v1",
                "http://arxiv.org/pdf/2312.02019v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02015v1",
            "title": "ColonNeRF: Neural Radiance Fields for High-Fidelity Long-Sequence\n  Colonoscopy Reconstruction",
            "updated": "2023-12-04T16:38:16Z",
            "published": "2023-12-04T16:38:16Z",
            "summary": "Colonoscopy reconstruction is pivotal for diagnosing colorectal cancer.\nHowever, accurate long-sequence colonoscopy reconstruction faces three major\nchallenges: (1) dissimilarity among segments of the colon due to its meandering\nand convoluted shape; (2) co-existence of simple and intricately folded\ngeometry structures; (3) sparse viewpoints due to constrained camera\ntrajectories. To tackle these challenges, we introduce a new reconstruction\nframework based on neural radiance field (NeRF), named ColonNeRF, which\nleverages neural rendering for novel view synthesis of long-sequence\ncolonoscopy. Specifically, to reconstruct the entire colon in a piecewise\nmanner, our ColonNeRF introduces a region division and integration module,\neffectively reducing shape dissimilarity and ensuring geometric consistency in\neach segment. To learn both the simple and complex geometry in a unified\nframework, our ColonNeRF incorporates a multi-level fusion module that\nprogressively models the colon regions from easy to hard. Additionally, to\novercome the challenges from sparse views, we devise a DensiNet module for\ndensifying camera poses under the guidance of semantic consistency. We conduct\nextensive experiments on both synthetic and real-world datasets to evaluate our\nColonNeRF. Quantitatively, our ColonNeRF outperforms existing methods on two\nbenchmarks over four evaluation metrics. Notably, our LPIPS-ALEX scores exhibit\na substantial increase of about 67%-85% on the SimCol-to-3D dataset.\nQualitatively, our reconstruction visualizations show much clearer textures and\nmore accurate geometric details. These sufficiently demonstrate our superior\nperformance over the state-of-the-art methods.",
            "author": [
                "Yufei Shi",
                "Beijia Lu",
                "Jia-Wei Liu",
                "Ming Li",
                "Mike Zheng Shou"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02015v1",
                "http://arxiv.org/pdf/2312.02015v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02012v1",
            "title": "Optimal Data Generation in Multi-Dimensional Parameter Spaces, using\n  Bayesian Optimization",
            "updated": "2023-12-04T16:36:29Z",
            "published": "2023-12-04T16:36:29Z",
            "summary": "Acquiring a substantial number of data points for training accurate machine\nlearning (ML) models is a big challenge in scientific fields where data\ncollection is resource-intensive. Here, we propose a novel approach for\nconstructing a minimal yet highly informative database for training ML models\nin complex multi-dimensional parameter spaces. To achieve this, we mimic the\nunderlying relation between the output and input parameters using Gaussian\nprocess regression (GPR). Using a set of known data, GPR provides predictive\nmeans and standard deviation for the unknown data. Given the predicted standard\ndeviation by GPR, we select data points using Bayesian optimization to obtain\nan efficient database for training ML models. We compare the performance of ML\nmodels trained on databases obtained through this method, with databases\nobtained using traditional approaches. Our results demonstrate that the ML\nmodels trained on the database obtained using Bayesian optimization approach\nconsistently outperform the other two databases, achieving high accuracy with a\nsignificantly smaller number of data points. Our work contributes to the\nresource-efficient collection of data in high-dimensional complex parameter\nspaces, to achieve high precision machine learning predictions.",
            "author": [
                "M. R. Mahani",
                "Igor A. Nechepurenko",
                "Yasmin Rahimof",
                "Andreas Wicht"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02012v1",
                "http://arxiv.org/pdf/2312.02012v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "physics.app-ph",
                "physics.comp-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02010v2",
            "title": "Towards Learning a Generalist Model for Embodied Navigation",
            "updated": "2023-12-06T08:13:28Z",
            "published": "2023-12-04T16:32:51Z",
            "summary": "Building a generalist agent that can interact with the world is the\nintriguing target of AI systems, thus spurring the research for embodied\nnavigation, where an agent is required to navigate according to instructions or\nrespond to queries. Despite the major progress attained, previous works\nprimarily focus on task-specific agents and lack generalizability to unseen\nscenarios. Recently, LLMs have presented remarkable capabilities across various\nfields, and provided a promising opportunity for embodied navigation. Drawing\non this, we propose the first generalist model for embodied navigation,\nNaviLLM. It adapts LLMs to embodied navigation by introducing schema-based\ninstruction. The schema-based instruction flexibly casts various tasks into\ngeneration problems, thereby unifying a wide range of tasks. This approach\nallows us to integrate diverse data sources from various datasets into the\ntraining, equipping NaviLLM with a wide range of capabilities required by\nembodied navigation. We conduct extensive experiments to evaluate the\nperformance and generalizability of our model. The experimental results\ndemonstrate that our unified model achieves state-of-the-art performance on\nCVDN, SOON, and ScanQA. Specifically, it surpasses the previous\nstats-of-the-art method by a significant margin of 29% in goal progress on\nCVDN. Moreover, our model also demonstrates strong generalizability and\npresents impressive results on unseen tasks, e.g., embodied question answering\nand 3D captioning.",
            "author": [
                "Duo Zheng",
                "Shijia Huang",
                "Lin Zhao",
                "Yiwu Zhong",
                "Liwei Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02010v2",
                "http://arxiv.org/pdf/2312.02010v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02008v1",
            "title": "Multi-Agent Behavior Retrieval",
            "updated": "2023-12-04T16:30:19Z",
            "published": "2023-12-04T16:30:19Z",
            "summary": "This paper aims to enable multi-agent systems to effectively utilize past\nmemories to adapt to novel collaborative tasks in a data-efficient fashion. We\npropose the Multi-Agent Coordination Skill Database, a repository for storing a\ncollection of coordinated behaviors associated with the key vector distinctive\nto them. Our Transformer-based skill encoder effectively captures\nspatio-temporal interactions that contribute to coordination and provide a\nskill representation unique to each coordinated behavior. By leveraging a small\nnumber of demonstrations of the target task, the database allows us to train\nthe policy using a dataset augmented with the retrieved demonstrations.\nExperimental evaluations clearly demonstrate that our method achieves a\nsignificantly higher success rate in push manipulation tasks compared to\nbaseline methods like few-shot imitation learning. Furthermore, we validate the\neffectiveness of our retrieve-and-learn framework in a real environment using a\nteam of wheeled robots.",
            "author": [
                "So Kuroki",
                "Mai Nishimura",
                "Tadashi Kozuno"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02008v1",
                "http://arxiv.org/pdf/2312.02008v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02003v1",
            "title": "A Survey on Large Language Model (LLM) Security and Privacy: The Good,\n  the Bad, and the Ugly",
            "updated": "2023-12-04T16:25:18Z",
            "published": "2023-12-04T16:25:18Z",
            "summary": "Large Language Models (LLMs), such as GPT-3 and BERT, have revolutionized\nnatural language understanding and generation. They possess deep language\ncomprehension, human-like text generation capabilities, contextual awareness,\nand robust problem-solving skills, making them invaluable in various domains\n(e.g., search engines, customer support, translation). In the meantime, LLMs\nhave also gained traction in the security community, revealing security\nvulnerabilities and showcasing their potential in security-related tasks. This\npaper explores the intersection of LLMs with security and privacy.\nSpecifically, we investigate how LLMs positively impact security and privacy,\npotential risks and threats associated with their use, and inherent\nvulnerabilities within LLMs. Through a comprehensive literature review, the\npaper categorizes findings into \"The Good\" (beneficial LLM applications), \"The\nBad\" (offensive applications), and \"The Ugly\" (vulnerabilities and their\ndefenses). We have some interesting findings. For example, LLMs have proven to\nenhance code and data security, outperforming traditional methods. However,\nthey can also be harnessed for various attacks (particularly user-level\nattacks) due to their human-like reasoning abilities. We have identified areas\nthat require further research efforts. For example, research on model and\nparameter extraction attacks is limited and often theoretical, hindered by LLM\nparameter scale and confidentiality. Safe instruction tuning, a recent\ndevelopment, requires more exploration. We hope that our work can shed light on\nthe LLMs' potential to both bolster and jeopardize cybersecurity.",
            "author": [
                "Yifan Yao",
                "Jinhao Duan",
                "Kaidi Xu",
                "Yuanfang Cai",
                "Eric Sun",
                "Yue Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02003v1",
                "http://arxiv.org/pdf/2312.02003v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01999v1",
            "title": "SRTransGAN: Image Super-Resolution using Transformer based Generative\n  Adversarial Network",
            "updated": "2023-12-04T16:22:39Z",
            "published": "2023-12-04T16:22:39Z",
            "summary": "Image super-resolution aims to synthesize high-resolution image from a\nlow-resolution image. It is an active area to overcome the resolution\nlimitations in several applications like low-resolution object-recognition,\nmedical image enhancement, etc. The generative adversarial network (GAN) based\nmethods have been the state-of-the-art for image super-resolution by utilizing\nthe convolutional neural networks (CNNs) based generator and discriminator\nnetworks. However, the CNNs are not able to exploit the global information very\neffectively in contrast to the transformers, which are the recent breakthrough\nin deep learning by exploiting the self-attention mechanism. Motivated from the\nsuccess of transformers in language and vision applications, we propose a\nSRTransGAN for image super-resolution using transformer based GAN.\nSpecifically, we propose a novel transformer-based encoder-decoder network as a\ngenerator to generate 2x images and 4x images. We design the discriminator\nnetwork using vision transformer which uses the image as sequence of patches\nand hence useful for binary classification between synthesized and real\nhigh-resolution images. The proposed SRTransGAN outperforms the existing\nmethods by 4.38 % on an average of PSNR and SSIM scores. We also analyze the\nsaliency map to understand the learning ability of the proposed method.",
            "author": [
                "Neeraj Baghel",
                "Shiv Ram Dubey",
                "Satish Kumar Singh"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01999v1",
                "http://arxiv.org/pdf/2312.01999v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01994v1",
            "title": "A Generative Self-Supervised Framework using Functional Connectivity in\n  fMRI Data",
            "updated": "2023-12-04T16:14:43Z",
            "published": "2023-12-04T16:14:43Z",
            "summary": "Deep neural networks trained on Functional Connectivity (FC) networks\nextracted from functional Magnetic Resonance Imaging (fMRI) data have gained\npopularity due to the increasing availability of data and advances in model\narchitectures, including Graph Neural Network (GNN). Recent research on the\napplication of GNN to FC suggests that exploiting the time-varying properties\nof the FC could significantly improve the accuracy and interpretability of the\nmodel prediction. However, the high cost of acquiring high-quality fMRI data\nand corresponding phenotypic labels poses a hurdle to their application in\nreal-world settings, such that a model na\\\"ively trained in a supervised\nfashion can suffer from insufficient performance or a lack of generalization on\na small number of data. In addition, most Self-Supervised Learning (SSL)\napproaches for GNNs to date adopt a contrastive strategy, which tends to lose\nappropriate semantic information when the graph structure is perturbed or does\nnot leverage both spatial and temporal information simultaneously. In light of\nthese challenges, we propose a generative SSL approach that is tailored to\neffectively harness spatio-temporal information within dynamic FC. Our\nempirical results, experimented with large-scale (>50,000) fMRI datasets,\ndemonstrate that our approach learns valuable representations and enables the\nconstruction of accurate and robust models when fine-tuned for downstream\ntasks.",
            "author": [
                "Jungwon Choi",
                "Seongho Keum",
                "EungGu Yun",
                "Byung-Hoon Kim",
                "Juho Lee"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01994v1",
                "http://arxiv.org/pdf/2312.01994v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV",
                "eess.IV",
                "q-bio.NC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01991v1",
            "title": "Information Modified K-Nearest Neighbor",
            "updated": "2023-12-04T16:10:34Z",
            "published": "2023-12-04T16:10:34Z",
            "summary": "In this research paper, we introduce a novel classification method aimed at\nimproving the performance of the K-Nearest Neighbors (KNN) algorithm. Our\napproach leverages Mutual Information (MI) to enhance the significance of\nweights and draw inspiration from Shapley values, a concept originating from\ncooperative game theory, to refine value allocation. The fundamental concept\nunderlying KNN is the classification of samples based on the majority thorough\ntheir k-nearest neighbors. While both the distances and labels of these\nneighbors are crucial, traditional KNN assigns equal weight to all samples and\nprevance considering the varying importance of each neighbor based on their\ndistances and labels.\n  In the proposed method, known as Information-Modified KNN (IMKNN), we address\nthis issue by introducing a straightforward algorithm. To evaluate the\neffectiveness of our approach, it is compared with 7 contemporary variants of\nKNN, as well as the traditional KNN. Each of these variants exhibits its unique\nadvantages and limitations. We conduct experiments on 12 widely-used datasets,\nassessing the methods' performance in terms of accuracy, precision and recall.\n  Our study demonstrates that IMKNN consistently outperforms other methods\nacross different datasets and criteria by highlighting its superior performance\nin various classification tasks. These findings underscore the potential of\nIMKNN as a valuable tool for enhancing the capabilities of the KNN algorithm in\ndiverse applications.",
            "author": [
                "Mohammad Ali Vahedifar",
                "Azim Akhtarshenas",
                "Mariam Sabbaghian",
                "Mohammad Rafatpanah"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01991v1",
                "http://arxiv.org/pdf/2312.01991v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.IT",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02248v2",
            "title": "Towards early diagnosis of Alzheimer's disease: Advances in\n  immune-related blood biomarkers and computational modeling approaches",
            "updated": "2023-12-06T10:05:42Z",
            "published": "2023-12-04T16:05:45Z",
            "summary": "Alzheimer's disease has an increasing prevalence in the population\nworld-wide, yet current diagnostic methods based on recommended biomarkers are\nonly available in specialized clinics. Due to these circumstances, Alzheimer's\ndisease is usually diagnosed late, which contrasts with the currently available\ntreatment options that are only effective for patients at an early stage.\nBlood-based biomarkers could fill in the gap of easily accessible and low-cost\nmethods for early diagnosis of the disease. In particular, immune-based\nblood-biomarkers might be a promising option, given the recently discovered\ncross-talk of immune cells of the central nervous system with those in the\nperipheral immune system. With the help of machine learning algorithms and\nmechanistic modeling approaches, such as agent-based modeling, an in-depth\nanalysis of the simulation of cell dynamics is possible as well as of\nhigh-dimensional omics resources indicative of pathway signaling changes. Here,\nwe give a background on advances in research on brain-immune system cross-talk\nin Alzheimer's disease and review recent machine learning and mechanistic\nmodeling approaches which leverage modern omics technologies for blood-based\nimmune system-related biomarker discovery.",
            "author": [
                "Sophia Krix",
                "Ella Wilczynski",
                "Neus Falg\u00e0s",
                "Raquel S\u00e1nchez-Valle",
                "Eti Yoles",
                "Uri Nevo",
                "Kuti Baruch",
                "Holger Fr\u00f6hlich"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02248v2",
                "http://arxiv.org/pdf/2312.02248v2"
            ],
            "primary_category": "q-bio.QM",
            "category": [
                "q-bio.QM",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01970v1",
            "title": "CaRL: Cascade Reinforcement Learning with State Space Splitting for\n  O-RAN based Traffic Steering",
            "updated": "2023-12-04T15:33:00Z",
            "published": "2023-12-04T15:33:00Z",
            "summary": "The Open Radio Access Network (O-RAN) architecture empowers intelligent and\nautomated optimization of the RAN through applications deployed on the RAN\nIntelligent Controller (RIC) platform, enabling capabilities beyond what is\nachievable with traditional RAN solutions. Within this paradigm, Traffic\nSteering (TS) emerges as a pivotal RIC application that focuses on optimizing\ncell-level mobility settings in near-real-time, aiming to significantly improve\nnetwork spectral efficiency. In this paper, we design a novel TS algorithm\nbased on a Cascade Reinforcement Learning (CaRL) framework. We propose state\nspace factorization and policy decomposition to reduce the need for large\nmodels and well-labeled datasets. For each sub-state space, an RL sub-policy\nwill be trained to learn an optimized mapping onto the action space. To apply\nCaRL on new network regions, we propose a knowledge transfer approach to\ninitialize a new sub-policy based on knowledge learned by the trained policies.\nTo evaluate CaRL, we build a data-driven and scalable RIC digital twin (DT)\nthat is modeled using important real-world data, including network\nconfiguration, user geo-distribution, and traffic demand, among others, from a\ntier-1 mobile operator in the US. We evaluate CaRL on two DT scenarios\nrepresenting two network clusters in two different cities and compare its\nperformance with the business-as-usual (BAU) policy and other competing\noptimization approaches using heuristic and Q-table algorithms. Benchmarking\nresults show that CaRL performs the best and improves the average\ncluster-aggregated downlink throughput over the BAU policy by 24% and 18% in\nthese two scenarios, respectively.",
            "author": [
                "Chuanneng Sun",
                "Yu Zhou",
                "Gueyoung Jung",
                "Tuyen Xuan Tran",
                "Dario Pompili"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01970v1",
                "http://arxiv.org/pdf/2312.01970v1"
            ],
            "primary_category": "cs.NI",
            "category": [
                "cs.NI",
                "cs.SY",
                "eess.SY",
                "C.2.3; I.2.8"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01966v1",
            "title": "Distance function for spike prediction",
            "updated": "2023-12-04T15:24:48Z",
            "published": "2023-12-04T15:24:48Z",
            "summary": "Approaches to predicting neuronal spike responses commonly use a Poisson\nlearning objective. This objective quantizes responses into spike counts within\na fixed summation interval, typically on the order of 10 to 100 milliseconds in\nduration; however, neuronal responses are often time accurate down to a few\nmilliseconds, and at these timescales, Poisson models typically perform poorly.\nTo overcome this limitation, we propose the concept of a spike distance\nfunction that maps points in time to the temporal distance to the nearest\nspike. We show that neural networks can be trained to approximate spike\ndistance functions, and we present an efficient algorithm for inferring spike\ntrains from the outputs of these models. Using recordings of chicken and frog\nretinal ganglion cells responding to visual stimuli, we compare the performance\nof our approach to Poisson models trained with various summation intervals. We\nshow that our approach outperforms the standard Poisson approach at spike train\ninference.",
            "author": [
                "Kevin Doran",
                "Marvin Seifert",
                "Carola A. M. Yovanovich",
                "Tom Baden"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01966v1",
                "http://arxiv.org/pdf/2312.01966v1"
            ],
            "primary_category": "q-bio.NC",
            "category": [
                "q-bio.NC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01959v1",
            "title": "Learning-Based Approaches to Predictive Monitoring with Conformal\n  Statistical Guarantees",
            "updated": "2023-12-04T15:16:42Z",
            "published": "2023-12-04T15:16:42Z",
            "summary": "This tutorial focuses on efficient methods to predictive monitoring (PM), the\nproblem of detecting at runtime future violations of a given requirement from\nthe current state of a system. While performing model checking at runtime would\noffer a precise solution to the PM problem, it is generally computationally\nexpensive. To address this scalability issue, several lightweight approaches\nbased on machine learning have recently been proposed. These approaches work by\nlearning an approximate yet efficient surrogate (deep learning) model of the\nexpensive model checker. A key challenge remains to ensure reliable\npredictions, especially in safety-critical applications. We review our recent\nwork on predictive monitoring, one of the first to propose learning-based\napproximations for CPS verification of temporal logic specifications and the\nfirst in this context to apply conformal prediction (CP) for rigorous\nuncertainty quantification. These CP-based uncertainty estimators offer\nstatistical guarantees regarding the generalization error of the learning\nmodel, and they can be used to determine unreliable predictions that should be\nrejected. In this tutorial, we present a general and comprehensive framework\nsummarizing our approach to the predictive monitoring of CPSs, examining in\ndetail several variants determined by three main dimensions: system dynamics\n(deterministic, non-deterministic, stochastic), state observability, and\nsemantics of requirements' satisfaction (Boolean or quantitative).",
            "author": [
                "Francesca Cairoli",
                "Luca Bortolussi",
                "Nicola Paoletti"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01959v1",
                "http://arxiv.org/pdf/2312.01959v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01957v1",
            "title": "Distilled Self-Critique of LLMs with Synthetic Data: a Bayesian\n  Perspective",
            "updated": "2023-12-04T15:16:12Z",
            "published": "2023-12-04T15:16:12Z",
            "summary": "This paper proposes an interpretation of RLAIF as Bayesian inference by\nintroducing distilled Self-Critique (dSC), which refines the outputs of a LLM\nthrough a Gibbs sampler that is later distilled into a fine-tuned model. Only\nrequiring synthetic data, dSC is exercised in experiments regarding safety,\nsentiment, and privacy control, showing it can be a viable and cheap\nalternative to align LLMs. Code released at\n\\url{https://github.com/vicgalle/distilled-self-critique}.",
            "author": [
                "Victor Gallego"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01957v1",
                "http://arxiv.org/pdf/2312.01957v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01947v1",
            "title": "Maximising Quantum-Computing Expressive Power through Randomised\n  Circuits",
            "updated": "2023-12-04T15:04:42Z",
            "published": "2023-12-04T15:04:42Z",
            "summary": "In the noisy intermediate-scale quantum era, variational quantum algorithms\n(VQAs) have emerged as a promising avenue to obtain quantum advantage. However,\nthe success of VQAs depends on the expressive power of parameterised quantum\ncircuits, which is constrained by the limited gate number and the presence of\nbarren plateaus. In this work, we propose and numerically demonstrate a novel\napproach for VQAs, utilizing randomised quantum circuits to generate the\nvariational wavefunction. We parameterize the distribution function of these\nrandom circuits using artificial neural networks and optimize it to find the\nsolution. This random-circuit approach presents a trade-off between the\nexpressive power of the variational wavefunction and time cost, in terms of the\nsampling cost of quantum circuits. Given a fixed gate number, we can\nsystematically increase the expressive power by extending the quantum-computing\ntime. With a sufficiently large permissible time cost, the variational\nwavefunction can approximate any quantum state with arbitrary accuracy.\nFurthermore, we establish explicit relationships between expressive power, time\ncost, and gate number for variational quantum eigensolvers. These results\nhighlight the promising potential of the random-circuit approach in achieving a\nhigh expressive power in quantum computing.",
            "author": [
                "Yingli Yang",
                "Zongkang Zhang",
                "Anbang Wang",
                "Xiaosi Xu",
                "Xiaoting Wang",
                "Ying Li"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01947v1",
                "http://arxiv.org/pdf/2312.01947v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01941v1",
            "title": "Intrusion Detection System with Machine Learning and Multiple Datasets",
            "updated": "2023-12-04T14:58:19Z",
            "published": "2023-12-04T14:58:19Z",
            "summary": "As Artificial Intelligence (AI) technologies continue to gain traction in the\nmodern-day world, they ultimately pose an immediate threat to current\ncybersecurity systems via exploitative methods. Prompt engineering is a\nrelatively new field that explores various prompt designs that can hijack large\nlanguage models (LLMs). If used by an unethical attacker, it can enable an AI\nsystem to offer malicious insights and code to them. In this paper, an enhanced\nintrusion detection system (IDS) that utilizes machine learning (ML) and\nhyperparameter tuning is explored, which can improve a model's performance in\nterms of accuracy and efficacy. Ultimately, this improved system can be used to\ncombat the attacks made by unethical hackers. A standard IDS is solely\nconfigured with pre-configured rules and patterns; however, with the\nutilization of machine learning, implicit and different patterns can be\ngenerated through the models' hyperparameter settings and parameters. In\naddition, the IDS will be equipped with multiple datasets so that the accuracy\nof the models improves. We evaluate the performance of multiple ML models and\ntheir respective hyperparameter settings through various metrics to compare\ntheir results to other models and past research work. The results of the\nproposed multi-dataset integration method yielded an accuracy score of 99.9%\nwhen equipped with the XGBoost and random forest classifiers and\nRandomizedSearchCV hyperparameter technique.",
            "author": [
                "Haiyan Xuan",
                "Mohith Manohar"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01941v1",
                "http://arxiv.org/pdf/2312.01941v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.LG",
                "cs.NI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01939v1",
            "title": "Foundations for Transfer in Reinforcement Learning: A Taxonomy of\n  Knowledge Modalities",
            "updated": "2023-12-04T14:55:58Z",
            "published": "2023-12-04T14:55:58Z",
            "summary": "Contemporary artificial intelligence systems exhibit rapidly growing\nabilities accompanied by the growth of required resources, expansive datasets\nand corresponding investments into computing infrastructure. Although earlier\nsuccesses predominantly focus on constrained settings, recent strides in\nfundamental research and applications aspire to create increasingly general\nsystems. This evolving landscape presents a dual panorama of opportunities and\nchallenges in refining the generalisation and transfer of knowledge - the\nextraction from existing sources and adaptation as a comprehensive foundation\nfor tackling new problems. Within the domain of reinforcement learning (RL),\nthe representation of knowledge manifests through various modalities, including\ndynamics and reward models, value functions, policies, and the original data.\nThis taxonomy systematically targets these modalities and frames its discussion\nbased on their inherent properties and alignment with different objectives and\nmechanisms for transfer. Where possible, we aim to provide coarse guidance\ndelineating approaches which address requirements such as limiting environment\ninteractions, maximising computational efficiency, and enhancing generalisation\nacross varying axes of change. Finally, we analyse reasons contributing to the\nprevalence or scarcity of specific forms of transfer, the inherent potential\nbehind pushing these frontiers, and underscore the significance of\ntransitioning from designed to learned transfer.",
            "author": [
                "Markus Wulfmeier",
                "Arunkumar Byravan",
                "Sarah Bechtle",
                "Karol Hausman",
                "Nicolas Heess"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01939v1",
                "http://arxiv.org/pdf/2312.01939v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.RO",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02247v1",
            "title": "Federated Active Learning for Target Domain Generalisation",
            "updated": "2023-12-04T14:50:23Z",
            "published": "2023-12-04T14:50:23Z",
            "summary": "In this paper, we introduce Active Learning framework in Federated Learning\nfor Target Domain Generalisation, harnessing the strength from both learning\nparadigms. Our framework, FEDALV, composed of Active Learning (AL) and\nFederated Domain Generalisation (FDG), enables generalisation of an image\nclassification model trained from limited source domain client's data without\nsharing images to an unseen target domain. To this end, our FDG, FEDA, consists\nof two optimisation updates during training, one at the client and another at\nthe server level. For the client, the introduced losses aim to reduce feature\ncomplexity and condition alignment, while in the server, the regularisation\nlimits free energy biases between source and target obtained by the global\nmodel. The remaining component of FEDAL is AL with variable budgets, which\nqueries the server to retrieve and sample the most informative local data for\nthe targeted client. We performed multiple experiments on FDG w/ and w/o AL and\ncompared with both conventional FDG baselines and Federated Active Learning\nbaselines. Our extensive quantitative experiments demonstrate the superiority\nof our method in accuracy and efficiency compared to the multiple contemporary\nmethods. FEDALV manages to obtain the performance of the full training target\naccuracy while sampling as little as 5% of the source client's data.",
            "author": [
                "Razvan Caramalau",
                "Binod Bhattarai",
                "Danail Stoyanov"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02247v1",
                "http://arxiv.org/pdf/2312.02247v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02246v1",
            "title": "Conditional Variational Diffusion Models",
            "updated": "2023-12-04T14:45:56Z",
            "published": "2023-12-04T14:45:56Z",
            "summary": "Inverse problems aim to determine parameters from observations, a crucial\ntask in engineering and science. Lately, generative models, especially\ndiffusion models, have gained popularity in this area for their ability to\nproduce realistic solutions and their good mathematical properties. Despite\ntheir success, an important drawback of diffusion models is their sensitivity\nto the choice of variance schedule, which controls the dynamics of the\ndiffusion process. Fine-tuning this schedule for specific applications is\ncrucial but time-costly and does not guarantee an optimal result. We propose a\nnovel approach for learning the schedule as part of the training process. Our\nmethod supports probabilistic conditioning on data, provides high-quality\nsolutions, and is flexible, proving able to adapt to different applications\nwith minimum overhead. This approach is tested in two unrelated inverse\nproblems: super-resolution microscopy and quantitative phase imaging, yielding\ncomparable or superior results to previous methods and fine-tuned diffusion\nmodels. We conclude that fine-tuning the schedule by experimentation should be\navoided because it can be learned during training in a stable way that yields\nbetter results.",
            "author": [
                "Gabriel della Maggiora",
                "Luis Alberto Croquevielle",
                "Nikita Desphande",
                "Harry Horsley",
                "Thomas Heinis",
                "Artur Yakimovich"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02246v1",
                "http://arxiv.org/pdf/2312.02246v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG",
                "stat.ML",
                "I.2.6"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03037v1",
            "title": "Analysis and mining of low-carbon and energy-saving tourism data\n  characteristics based on machine learning algorithm",
            "updated": "2023-12-04T14:32:54Z",
            "published": "2023-12-04T14:32:54Z",
            "summary": "In order to study the formation mechanism of residents' low-carbon awareness\nand provide an important basis for traffic managers to guide urban residents to\nchoose low-carbon travel mode, this paper proposes a low-carbon energy-saving\ntravel data feature analysis and mining based on machine learning algorithm.\nThis paper uses data mining technology to analyze the data of low-carbon travel\nquestionnaire, and regards the 15-dimensional problem under the framework of\nplanned behavior theory as the internal cause variable that characterizes\nresidents' low-carbon travel willingness. The author uses K-means clustering\nalgorithm to classify the intensity of residents' low-carbon travel\nwillingness, and applies the results as the explanatory variables to the random\nforest model to explore the mechanism of residents' social attribute\ncharacteristics, travel characteristics, etc. on their low-carbon travel\nwillingness. The experimental results show that based on the Silhouette index\ntest and t-SNE dimensionality reduction, residents' low-carbon travel\nwillingness can be divided into three categories: strong, neutral, and not\nstrong; Based on the importance index, the four most significant factors are\nthe occupation, residence, family composition and commuting time of residents.\nConclusion: This method provides policy recommendations for the development and\nmanagement of urban traffic low-carbon from multiple perspectives.",
            "author": [
                "Lukasz Wierzbinski"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03037v1",
                "http://arxiv.org/pdf/2312.03037v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01925v1",
            "title": "Coefficient Shape Alignment in Multivariate Functional Regression",
            "updated": "2023-12-04T14:32:19Z",
            "published": "2023-12-04T14:32:19Z",
            "summary": "In multivariate functional data analysis, different functional covariates can\nbe homogeneous in some sense. The hidden homogeneity structure is informative\nabout the connectivity or association of different covariates. The covariates\nwith pronounced homogeneity can be analyzed jointly in the same group and this\ngives rise to a way of parsimoniously modeling multivariate functional data. In\nthis paper, we develop a multivariate functional regression technique by a new\nregularization approach termed \"coefficient shape alignment\" to tackle the\npotential homogeneity of different functional covariates. The modeling\nprocedure includes two main steps: first the unknown grouping structure is\ndetected with a new regularization approach to aggregate covariates into\ndisjoint groups; and then a grouped multivariate functional regression model is\nestablished based on the detected grouping structure. In this new grouped\nmodel, the coefficient functions of covariates in the same homogeneous group\nshare the same shape invariant to scaling. The new regularization approach\nbuilds on penalizing the discrepancy of coefficient shape. The consistency\nproperty of the detected grouping structure is thoroughly investigated, and the\nconditions that guarantee uncovering the underlying true grouping structure are\ndeveloped. The asymptotic properties of the model estimates are also developed.\nExtensive simulation studies are conducted to investigate the finite-sample\nproperties of the developed methods. The practical utility of the proposed\nmethods is illustrated in an analysis on sugar quality evaluation. This work\nprovides a novel means for analyzing the underlying homogeneity of functional\ncovariates and developing parsimonious model structures for multivariate\nfunctional data.",
            "author": [
                "Shuhao Jiao",
                "Ngai-Hang Chan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01925v1",
                "http://arxiv.org/pdf/2312.01925v1"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01921v1",
            "title": "A Machine Learning Approach Towards SKILL Code Autocompletion",
            "updated": "2023-12-04T14:29:28Z",
            "published": "2023-12-04T14:29:28Z",
            "summary": "As Moore's Law continues to increase the complexity of electronic systems,\nElectronic Design Automation (EDA) must advance to meet global demand. An\nimportant example of an EDA technology is SKILL, a scripting language used to\ncustomize and extend EDA software. Recently, code generation models using the\ntransformer architecture have achieved impressive results in academic settings\nand have even been used in commercial developer tools to improve developer\nproductivity. To the best of our knowledge, this study is the first to apply\ntransformers to SKILL code autocompletion towards improving the productivity of\nhardware design engineers. In this study, a novel, data-efficient methodology\nfor generating SKILL code is proposed and experimentally validated. More\nspecifically, we propose a novel methodology for (i) creating a high-quality\nSKILL dataset with both unlabeled and labeled data, (ii) a training strategy\nwhere T5 models pre-trained on general programming language code are fine-tuned\non our custom SKILL dataset using unsupervised and supervised learning, and\n(iii) evaluating synthesized SKILL code. We show that models trained using the\nproposed methodology outperform baselines in terms of human-judgment score and\nBLEU score. A major challenge faced was the extremely small amount of available\nSKILL code data that can be used to train a transformer model to generate SKILL\ncode. Despite our validated improvements, the extremely small dataset available\nto us was still not enough to train a model that can reliably autocomplete\nSKILL code. We discuss this and other limitations as well as future work that\ncould address these limitations.",
            "author": [
                "Enrique Dehaerne",
                "Bappaditya Dey",
                "Wannes Meert"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01921v1",
                "http://arxiv.org/pdf/2312.01921v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE",
                "cs.CL",
                "cs.PL",
                "I.2.2"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01916v1",
            "title": "PEACE: Prototype lEarning Augmented transferable framework for\n  Cross-domain rEcommendation",
            "updated": "2023-12-04T14:20:16Z",
            "published": "2023-12-04T14:20:16Z",
            "summary": "To help merchants/customers to provide/access a variety of services through\nminiapps, online service platforms have occupied a critical position in the\neffective content delivery, in which how to recommend items in the new domain\nlaunched by the service provider for customers has become more urgent. However,\nthe non-negligible gap between the source and diversified target domains poses\na considerable challenge to cross-domain recommendation systems, which often\nleads to performance bottlenecks in industrial settings. While entity graphs\nhave the potential to serve as a bridge between domains, rudimentary\nutilization still fail to distill useful knowledge and even induce the negative\ntransfer issue. To this end, we propose PEACE, a Prototype lEarning Augmented\ntransferable framework for Cross-domain rEcommendation. For domain gap\nbridging, PEACE is built upon a multi-interest and entity-oriented pre-training\narchitecture which could not only benefit the learning of generalized knowledge\nin a multi-granularity manner, but also help leverage more structural\ninformation in the entity graph. Then, we bring the prototype learning into the\npre-training over source domains, so that representations of users and items\nare greatly improved by the contrastive prototype learning module and the\nprototype enhanced attention mechanism for adaptive knowledge utilization. To\nease the pressure of online serving, PEACE is carefully deployed in a\nlightweight manner, and significant performance improvements are observed in\nboth online and offline environments.",
            "author": [
                "Chunjing Gan",
                "Bo Huang",
                "Binbin Hu",
                "Jian Ma",
                "Ziqi Liu",
                "Zhiqiang Zhang",
                "Jun Zhou",
                "Guannan Zhang",
                "Wenliang Zhong"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01916v1",
                "http://arxiv.org/pdf/2312.01916v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01915v1",
            "title": "A Reliable Representation with Bidirectional Transition Model for Visual\n  Reinforcement Learning Generalization",
            "updated": "2023-12-04T14:19:36Z",
            "published": "2023-12-04T14:19:36Z",
            "summary": "Visual reinforcement learning has proven effective in solving control tasks\nwith high-dimensional observations. However, extracting reliable and\ngeneralizable representations from vision-based observations remains a central\nchallenge. Inspired by the human thought process, when the representation\nextracted from the observation can predict the future and trace history, the\nrepresentation is reliable and accurate in comprehending the environment. Based\non this concept, we introduce a Bidirectional Transition (BiT) model, which\nleverages the ability to bidirectionally predict environmental transitions both\nforward and backward to extract reliable representations. Our model\ndemonstrates competitive generalization performance and sample efficiency on\ntwo settings of the DeepMind Control suite. Additionally, we utilize robotic\nmanipulation and CARLA simulators to demonstrate the wide applicability of our\nmethod.",
            "author": [
                "Xiaobo Hu",
                "Youfang Lin",
                "Yue Liu",
                "Jinwen Wang",
                "Shuo Wang",
                "Hehe Fan",
                "Kai Lv"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01915v1",
                "http://arxiv.org/pdf/2312.01915v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01904v1",
            "title": "Unsupervised Anomaly Detection using Aggregated Normative Diffusion",
            "updated": "2023-12-04T14:02:56Z",
            "published": "2023-12-04T14:02:56Z",
            "summary": "Early detection of anomalies in medical images such as brain MRI is highly\nrelevant for diagnosis and treatment of many conditions. Supervised machine\nlearning methods are limited to a small number of pathologies where there is\ngood availability of labeled data. In contrast, unsupervised anomaly detection\n(UAD) has the potential to identify a broader spectrum of anomalies by spotting\ndeviations from normal patterns. Our research demonstrates that existing\nstate-of-the-art UAD approaches do not generalise well to diverse types of\nanomalies in realistic multi-modal MR data. To overcome this, we introduce a\nnew UAD method named Aggregated Normative Diffusion (ANDi). ANDi operates by\naggregating differences between predicted denoising steps and ground truth\nbackwards transitions in Denoising Diffusion Probabilistic Models (DDPMs) that\nhave been trained on pyramidal Gaussian noise. We validate ANDi against three\nrecent UAD baselines, and across three diverse brain MRI datasets. We show that\nANDi, in some cases, substantially surpasses these baselines and shows\nincreased robustness to varying types of anomalies. Particularly in detecting\nmultiple sclerosis (MS) lesions, ANDi achieves improvements of up to 178% in\nterms of AUPRC.",
            "author": [
                "Alexander Frotscher",
                "Jaivardhan Kapoor",
                "Thomas Wolfers",
                "Christian F. Baumgartner"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01904v1",
                "http://arxiv.org/pdf/2312.01904v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01898v1",
            "title": "Unlocking optimal batch size schedules using continuous-time control and\n  perturbation theory",
            "updated": "2023-12-04T13:54:05Z",
            "published": "2023-12-04T13:54:05Z",
            "summary": "Stochastic Gradient Descent (SGD) and its variants are almost universally\nused to train neural networks and to fit a variety of other parametric models.\nAn important hyperparameter in this context is the batch size, which determines\nhow many samples are processed before an update of the parameters occurs.\nPrevious studies have demonstrated the benefits of using variable batch sizes.\nIn this work, we will theoretically derive optimal batch size schedules for SGD\nand similar algorithms, up to an error that is quadratic in the learning rate.\nTo achieve this, we approximate the discrete process of parameter updates using\na family of stochastic differential equations indexed by the learning rate. To\nbetter handle the state-dependent diffusion coefficient, we further expand the\nsolution of this family into a series with respect to the learning rate. Using\nthis setup, we derive a continuous-time optimal batch size schedule for a large\nfamily of diffusion coefficients and then apply the results in the setting of\nlinear regression.",
            "author": [
                "Stefan Perko"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01898v1",
                "http://arxiv.org/pdf/2312.01898v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01891v1",
            "title": "A Christmas Story about Quantum Teleportation",
            "updated": "2023-12-04T13:46:49Z",
            "published": "2023-12-04T13:46:49Z",
            "summary": "Quantum teleportation is a concept that fascinates and confuses many people,\nin particular given that it combines quantum physics and the concept of\nteleportation. With quantum teleportation likely to play a key role in several\ncommunication technologies and the quantum internet in the future, it is\nimperative to create learning tools and approaches that can accurately and\neffectively communicate the concept. Recent research has indicated the\nimportance of teachers enthusing students about the topic of quantum physics.\nTherefore, educators at both high school and early university level need to\nfind engaging and perhaps unorthodox ways of teaching complex, yet interesting\ntopics such as quantum teleportation. In this paper, we present a paradigm to\nteach about the concept of quantum teleportation using the Christmas\ngift-bringer Santa Claus. Using the example of Santa Claus, we use an unusual\ncontext to explore the key aspects of quantum teleportation, and all without\nbeing overly abstract. In addition, we outline a worksheet designed for use in\nthe classroom setting which is based on common misconceptions from quantum\nphysics.",
            "author": [
                "Barry W. Fitzgerald",
                "Patrick Emonts",
                "Jordi Tura"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01891v1",
                "http://arxiv.org/pdf/2312.01891v1"
            ],
            "primary_category": "physics.ed-ph",
            "category": [
                "physics.ed-ph",
                "physics.pop-ph",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03008v1",
            "title": "Deep Reinforcement Learning for Community Battery Scheduling under\n  Uncertainties of Load, PV Generation, and Energy Prices",
            "updated": "2023-12-04T13:45:17Z",
            "published": "2023-12-04T13:45:17Z",
            "summary": "In response to the growing uptake of distributed energy resources (DERs),\ncommunity batteries have emerged as a promising solution to support renewable\nenergy integration, reduce peak load, and enhance grid reliability. This paper\npresents a deep reinforcement learning (RL) strategy, centered around the soft\nactor-critic (SAC) algorithm, to schedule a community battery system in the\npresence of uncertainties, such as solar photovoltaic (PV) generation, local\ndemand, and real-time energy prices. We position the community battery to play\na versatile role, in integrating local PV energy, reducing peak load, and\nexploiting energy price fluctuations for arbitrage, thereby minimizing the\nsystem cost. To improve exploration and convergence during RL training, we\nutilize the noisy network technique. This paper conducts a comparative study of\ndifferent RL algorithms, including proximal policy optimization (PPO) and deep\ndeterministic policy gradient (DDPG) algorithms, to evaluate their\neffectiveness in the community battery scheduling problem. The results\ndemonstrate the potential of RL in addressing community battery scheduling\nchallenges and show that the SAC algorithm achieves the best performance\ncompared to RL and optimization benchmarks.",
            "author": [
                "Jiarong Fan",
                "Hao Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03008v1",
                "http://arxiv.org/pdf/2312.03008v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01890v1",
            "title": "Optical anisotropy and nonlinearity in deep ultraviolet fluorooxoborates",
            "updated": "2023-12-04T13:44:16Z",
            "published": "2023-12-04T13:44:16Z",
            "summary": "Optical anisotropy and nonlinearity are two tantalizingly important and\nenticing properties of an optical crystal. Combining these two features will\nhave a miraculous effect. The up conversion can extend solid state laser\nsources to the ultraviolet and deep ultraviolet (DUV) ranges through harmonic\ngeneration and for down conversion needed for quantum information technology,\nbut only a few suitable materials are known as the medium because of the\ncombination of properties that are required. These include suitable band gaps,\nmoderate optical anisotropy for phase matching and strong nonlinear optical\n(NLO) response. Fluorooxoborates are a new ideal platform for this effect in\nDUV. Here we demonstrate that fluorooxoborate is the optimal framework for DUV\nNLO material and show that the significance of the incorporation of fluorine in\nborates. The NLO performance of fluorooxoborates is strongly improved in terms\nof local crystal structure and distribution of electronic states. Importantly,\nthe role of fluorine is to control the structure, while maintaining high band\ngaps but does not directly provide large contributions to birefringence and the\nsecond harmonic generation as the conventional assumptions. This is a\nconsequence of the microscopic electron distribution and the energy position of\nthe fluorine states well below the valence band maxima. Based on our\nunderstandings, we constructed two artificial structure and they all behave as\nanticipated.",
            "author": [
                "Bing-Hua Lei",
                "Fuming Li",
                "Wenqi Jin",
                "Mirding Mutailipu",
                "Chao Cao",
                "Shilie Pan",
                "Zhihua Yang",
                "David J. Singh"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01890v1",
                "http://arxiv.org/pdf/2312.01890v1"
            ],
            "primary_category": "physics.optics",
            "category": [
                "physics.optics",
                "cond-mat.mtrl-sci"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01887v1",
            "title": "Non-Intrusive Load Monitoring for Feeder-Level EV Charging Detection:\n  Sliding Window-based Approaches to Offline and Online Detection",
            "updated": "2023-12-04T13:40:22Z",
            "published": "2023-12-04T13:40:22Z",
            "summary": "Understanding electric vehicle (EV) charging on the distribution network is\nkey to effective EV charging management and aiding decarbonization across the\nenergy and transport sectors. Advanced metering infrastructure has allowed\ndistribution system operators and utility companies to collect high-resolution\nload data from their networks. These advancements enable the non-intrusive load\nmonitoring (NILM) technique to detect EV charging using load measurement data.\nWhile existing studies primarily focused on NILM for EV charging detection in\nindividual households, there is a research gap on EV charging detection at the\nfeeder level, presenting unique challenges due to the combined load measurement\nfrom multiple households. In this paper, we develop a novel and effective\napproach for EV detection at the feeder level, involving sliding-window feature\nextraction and classical machine learning techniques, specifically models like\nXGBoost and Random Forest. Our developed method offers a lightweight and\nefficient solution, capable of quick training. Moreover, our developed method\nis versatile, supporting both offline and online EV charging detection. Our\nexperimental results demonstrate high-accuracy EV charging detection at the\nfeeder level, achieving an F-Score of 98.88% in offline detection and 93.01% in\nonline detection.",
            "author": [
                "Cameron Martin",
                "Fucai Ke",
                "Hao Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01887v1",
                "http://arxiv.org/pdf/2312.01887v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01884v1",
            "title": "Correlation and Unintended Biases on Univariate and Multivariate\n  Decision Trees",
            "updated": "2023-12-04T13:33:51Z",
            "published": "2023-12-04T13:33:51Z",
            "summary": "Decision Trees are accessible, interpretable, and well-performing\nclassification models. A plethora of variants with increasing expressiveness\nhas been proposed in the last forty years. We contrast the two families of\nunivariate DTs, whose split functions partition data through axis-parallel\nhyperplanes, and multivariate DTs, whose splits instead partition data through\noblique hyperplanes. The latter include the former, hence multivariate DTs are\nin principle more powerful. Surprisingly enough, however, univariate DTs\nconsistently show comparable performances in the literature. We analyze the\nreasons behind this, both with synthetic and real-world benchmark datasets. Our\nresearch questions test whether the pre-processing phase of removing\ncorrelation among features in datasets has an impact on the relative\nperformances of univariate vs multivariate DTs. We find that existing benchmark\ndatasets are likely biased towards favoring univariate DTs.",
            "author": [
                "Mattia Setzu",
                "Salvatore Ruggieri"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01884v1",
                "http://arxiv.org/pdf/2312.01884v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01881v1",
            "title": "Bayesian Nonlinear Regression using Sums of Simple Functions",
            "updated": "2023-12-04T13:24:46Z",
            "published": "2023-12-04T13:24:46Z",
            "summary": "This paper proposes a new Bayesian machine learning model that can be applied\nto large datasets arising in macroeconomics. Our framework sums over many\nsimple two-component location mixtures. The transition between components is\ndetermined by a logistic function that depends on a single threshold variable\nand two hyperparameters. Each of these individual models only accounts for a\nminor portion of the variation in the endogenous variables. But many of them\nare capable of capturing arbitrary nonlinear conditional mean relations.\nConjugate priors enable fast and efficient inference. In simulations, we show\nthat our approach produces accurate point and density forecasts. In a real-data\nexercise, we forecast US macroeconomic aggregates and consider the nonlinear\neffects of financial shocks in a large-scale nonlinear VAR.",
            "author": [
                "Florian Huber"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01881v1",
                "http://arxiv.org/pdf/2312.01881v1"
            ],
            "primary_category": "econ.EM",
            "category": [
                "econ.EM",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01878v1",
            "title": "HGPROMPT: Bridging Homogeneous and Heterogeneous Graphs for Few-shot\n  Prompt Learning",
            "updated": "2023-12-04T13:20:15Z",
            "published": "2023-12-04T13:20:15Z",
            "summary": "Graph neural networks (GNNs) and heterogeneous graph neural networks (HGNNs)\nare prominent techniques for homogeneous and heterogeneous graph representation\nlearning, yet their performance in an end-to-end supervised framework greatly\ndepends on the availability of task-specific supervision. To reduce the\nlabeling cost, pre-training on self-supervised pretext tasks has become a\npopular paradigm,but there is often a gap between the pre-trained model and\ndownstream tasks, stemming from the divergence in their objectives. To bridge\nthe gap, prompt learning has risen as a promising direction especially in\nfew-shot settings, without the need to fully fine-tune the pre-trained model.\nWhile there has been some early exploration of prompt-based learning on graphs,\nthey primarily deal with homogeneous graphs, ignoring the heterogeneous graphs\nthat are prevalent in downstream applications. In this paper, we propose\nHGPROMPT, a novel pre-training and prompting framework to unify not only\npre-training and downstream tasks but also homogeneous and heterogeneous graphs\nvia a dual-template design. Moreover, we propose dual-prompt in HGPROMPT to\nassist a downstream task in locating the most relevant prior to bridge the gaps\ncaused by not only feature variations but also heterogeneity differences across\ntasks. Finally, we thoroughly evaluate and analyze HGPROMPT through extensive\nexperiments on three public datasets.",
            "author": [
                "Xingtong Yu",
                "Zemin Liu",
                "Yuan Fang",
                "Xinming Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01878v1",
                "http://arxiv.org/pdf/2312.01878v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01871v1",
            "title": "FeaInfNet: Diagnosis in Medical Image with Feature-Driven Inference and\n  Visual Explanations",
            "updated": "2023-12-04T13:09:00Z",
            "published": "2023-12-04T13:09:00Z",
            "summary": "Interpretable deep learning models have received widespread attention in the\nfield of image recognition. Due to the unique multi-instance learning of\nmedical images and the difficulty in identifying decision-making regions, many\ninterpretability models that have been proposed still have problems of\ninsufficient accuracy and interpretability in medical image disease diagnosis.\nTo solve these problems, we propose feature-driven inference network\n(FeaInfNet). Our first key innovation involves proposing a feature-based\nnetwork reasoning structure, which is applied to FeaInfNet. The network of this\nstructure compares the similarity of each sub-region image patch with the\ndisease templates and normal templates that may appear in the region, and\nfinally combines the comparison of each sub-region to make the final diagnosis.\nIt simulates the diagnosis process of doctors to make the model interpretable\nin the reasoning process, while avoiding the misleading caused by the\nparticipation of normal areas in reasoning. Secondly, we propose local feature\nmasks (LFM) to extract feature vectors in order to provide global information\nfor these vectors, thus enhancing the expressive ability of the FeaInfNet.\nFinally, we propose adaptive dynamic masks (Adaptive-DM) to interpret feature\nvectors and prototypes into human-understandable image patches to provide\naccurate visual interpretation. We conducted qualitative and quantitative\nexperiments on multiple publicly available medical datasets, including RSNA,\niChallenge-PM, Covid-19, ChinaCXRSet, and MontgomerySet. The results of our\nexperiments validate that our method achieves state-of-the-art performance in\nterms of classification accuracy and interpretability compared to baseline\nmethods in medical image diagnosis. Additional ablation studies verify the\neffectiveness of each of our proposed components.",
            "author": [
                "Yitao Peng",
                "Lianghua He",
                "Die Hu",
                "Yihang Liu",
                "Longzhen Yang",
                "Shaohua Shang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01871v1",
                "http://arxiv.org/pdf/2312.01871v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03007v1",
            "title": "Modelling JWST mid-infrared counts: excellent consistency with models\n  derived for IRAS, ISO and Spitzer",
            "updated": "2023-12-04T12:50:29Z",
            "published": "2023-12-04T12:50:29Z",
            "summary": "Models derived in 2009 to fit mid-infrared (8-24 micron) source counts from\nthe IRAS, ISO and Spitzer missions, provide an excellent fit to deep counts\nwith JWST, demonstrating that the evolution of dusty star-forming galaxies is\nwell understood. The evolution of dust in galaxies at high redshifts is\ndiscussed and a simple prescription is proposed to model this. This allows more\nrealistic models for source-counts at submillimetre wavelength. A reasonable\nfit to 250, 500, 850 and 1100 micron counts is obtained. This paper therefore\ndraws together the IRAS, ISO, Spitzer, Akari, Herschel, submillimetre\nground-based, and JWST surveys into a single picture.",
            "author": [
                "Michael Rowan-Robinson"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03007v1",
                "http://arxiv.org/pdf/2312.03007v1"
            ],
            "primary_category": "astro-ph.GA",
            "category": [
                "astro-ph.GA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01858v1",
            "title": "Evaluating Dependencies in Fact Editing for Language Models: Specificity\n  and Implication Awareness",
            "updated": "2023-12-04T12:45:30Z",
            "published": "2023-12-04T12:45:30Z",
            "summary": "The potential of using a large language model (LLM) as a knowledge base (KB)\nhas sparked significant interest. To manage the knowledge acquired by LLMs, we\nneed to ensure that the editing of learned facts respects internal logical\nconstraints, which are known as dependency of knowledge. Existing work on\nediting LLMs has partially addressed the issue of dependency, when the editing\nof a fact should apply to its lexical variations without disrupting irrelevant\nones. However, they neglect the dependency between a fact and its logical\nimplications. We propose an evaluation protocol with an accompanying\nquestion-answering dataset, DepEdit, that provides a comprehensive assessment\nof the editing process considering the above notions of dependency. Our\nprotocol involves setting up a controlled environment in which we edit facts\nand monitor their impact on LLMs, along with their implications based on\nIf-Then rules. Extensive experiments on DepEdit show that existing knowledge\nediting methods are sensitive to the surface form of knowledge, and that they\nhave limited performance in inferring the implications of edited facts.",
            "author": [
                "Zichao Li",
                "Ines Arous",
                "Siva Reddy",
                "Jackie C. K. Cheung"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01858v1",
                "http://arxiv.org/pdf/2312.01858v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01855v1",
            "title": "Modular Control Architecture for Safe Marine Navigation: Reinforcement\n  Learning and Predictive Safety Filters",
            "updated": "2023-12-04T12:37:54Z",
            "published": "2023-12-04T12:37:54Z",
            "summary": "Many autonomous systems face safety challenges, requiring robust closed-loop\ncontrol to handle physical limitations and safety constraints. Real-world\nsystems, like autonomous ships, encounter nonlinear dynamics and environmental\ndisturbances. Reinforcement learning is increasingly used to adapt to complex\nscenarios, but standard frameworks ensuring safety and stability are lacking.\nPredictive Safety Filters (PSF) offer a promising solution, ensuring constraint\nsatisfaction in learning-based control without explicit constraint handling.\nThis modular approach allows using arbitrary control policies, with the safety\nfilter optimizing proposed actions to meet physical and safety constraints. We\napply this approach to marine navigation, combining RL with PSF on a simulated\nCybership II model. The RL agent is trained on path following and collision\navpodance, while the PSF monitors and modifies control actions for safety.\nResults demonstrate the PSF's effectiveness in maintaining safety without\nhindering the RL agent's learning rate and performance, evaluated against a\nstandard RL agent without PSF.",
            "author": [
                "Aksel Vaaler",
                "Svein Jostein Husa",
                "Daniel Menges",
                "Thomas Nakken Larsen",
                "Adil Rasheed"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01855v1",
                "http://arxiv.org/pdf/2312.01855v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01853v1",
            "title": "Robot Synesthesia: In-Hand Manipulation with Visuotactile Sensing",
            "updated": "2023-12-04T12:35:43Z",
            "published": "2023-12-04T12:35:43Z",
            "summary": "Executing contact-rich manipulation tasks necessitates the fusion of tactile\nand visual feedback. However, the distinct nature of these modalities poses\nsignificant challenges. In this paper, we introduce a system that leverages\nvisual and tactile sensory inputs to enable dexterous in-hand manipulation.\nSpecifically, we propose Robot Synesthesia, a novel point cloud-based tactile\nrepresentation inspired by human tactile-visual synesthesia. This approach\nallows for the simultaneous and seamless integration of both sensory inputs,\noffering richer spatial information and facilitating better reasoning about\nrobot actions. The method, trained in a simulated environment and then deployed\nto a real robot, is applicable to various in-hand object rotation tasks.\nComprehensive ablations are performed on how the integration of vision and\ntouch can improve reinforcement learning and Sim2Real performance. Our project\npage is available at https://yingyuan0414.github.io/visuotactile/ .",
            "author": [
                "Ying Yuan",
                "Haichuan Che",
                "Yuzhe Qin",
                "Binghao Huang",
                "Zhao-Heng Yin",
                "Kang-Won Lee",
                "Yi Wu",
                "Soo-Chul Lim",
                "Xiaolong Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01853v1",
                "http://arxiv.org/pdf/2312.01853v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01850v1",
            "title": "Generalization by Adaptation: Diffusion-Based Domain Extension for\n  Domain-Generalized Semantic Segmentation",
            "updated": "2023-12-04T12:31:45Z",
            "published": "2023-12-04T12:31:45Z",
            "summary": "When models, e.g., for semantic segmentation, are applied to images that are\nvastly different from training data, the performance will drop significantly.\nDomain adaptation methods try to overcome this issue, but need samples from the\ntarget domain. However, this might not always be feasible for various reasons\nand therefore domain generalization methods are useful as they do not require\nany target data. We present a new diffusion-based domain extension (DIDEX)\nmethod and employ a diffusion model to generate a pseudo-target domain with\ndiverse text prompts. In contrast to existing methods, this allows to control\nthe style and content of the generated images and to introduce a high\ndiversity. In a second step, we train a generalizing model by adapting towards\nthis pseudo-target domain. We outperform previous approaches by a large margin\nacross various datasets and architectures without using any real data. For the\ngeneralization from GTA5, we improve state-of-the-art mIoU performance by 3.8%\nabsolute on average and for SYNTHIA by 11.8% absolute, marking a big step for\nthe generalization performance on these benchmarks. Code is available at\nhttps://github.com/JNiemeijer/DIDEX",
            "author": [
                "Joshua Niemeijer",
                "Manuel Schwonberg",
                "Jan-Aike Term\u00f6hlen",
                "Nico M. Schmidt",
                "Tim Fingscheidt"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01850v1",
                "http://arxiv.org/pdf/2312.01850v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01843v1",
            "title": "Light Curves For Ten R Coronae Borealis Stars For Longer Than a Century:\n  Secular Evolution, Dip Statistics, and a General Model for the Shape of\n  Isolated Light Curve Dips",
            "updated": "2023-12-04T12:25:48Z",
            "published": "2023-12-04T12:25:48Z",
            "summary": "R Coronae Borealis stars (RCBs) are cool supergiants that display\nnon-periodic deep dips in brightness. Recently, a group of `Hot RCB stars` has\nbeen discovered to be fast evolving across the HR diagram, as these stars leave\nthe RCB region, with brightness changes at the rate of $\\sim$1 mag/century.\nPerhaps cool RCB stars can also be seen evolving, either increasing in\ntemperature as they evolve to become Hot RCB stars, or perhaps increasing in\nluminosity as the stars arrive at the RCB region. To seek these changes, the\nonly possible method is to extract archival data going back more than a\ncentury, looking for the brightness changes associated with the evolution. I\nhave measured and extracted 323,464 magnitudes (mostly from the Harvard plates\nand from the AAVSO) for ten cool RCB stars, all with over a century for the\nlight curves, all consistently calibrated to a modern magnitude system. For\ntimes away from any dips, these light curves are flat to within the typical\nuncertainty of $\\pm$0.10 mag/century. That is, I see no significant evolution.\nI also have collected a large database of light curve dips and their\nproperties. From this, the light curves for all the well-observed isolated dips\nhave the same shape, featuring a flat slope for the few days immediately after\nthe minima. Further, I derive a general model for the shape of the light curve\nfor all isolated RCB dips, with a simple equation accurately describing the\nobserved recovery to maximum light.",
            "author": [
                "Bradley E. Schaefer"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01843v1",
                "http://arxiv.org/pdf/2312.01843v1"
            ],
            "primary_category": "astro-ph.SR",
            "category": [
                "astro-ph.SR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01841v2",
            "title": "VividTalk: One-Shot Audio-Driven Talking Head Generation Based on 3D\n  Hybrid Prior",
            "updated": "2023-12-07T03:14:22Z",
            "published": "2023-12-04T12:25:37Z",
            "summary": "Audio-driven talking head generation has drawn much attention in recent\nyears, and many efforts have been made in lip-sync, expressive facial\nexpressions, natural head pose generation, and high video quality. However, no\nmodel has yet led or tied on all these metrics due to the one-to-many mapping\nbetween audio and motion. In this paper, we propose VividTalk, a two-stage\ngeneric framework that supports generating high-visual quality talking head\nvideos with all the above properties. Specifically, in the first stage, we map\nthe audio to mesh by learning two motions, including non-rigid expression\nmotion and rigid head motion. For expression motion, both blendshape and vertex\nare adopted as the intermediate representation to maximize the representation\nability of the model. For natural head motion, a novel learnable head pose\ncodebook with a two-phase training mechanism is proposed. In the second stage,\nwe proposed a dual branch motion-vae and a generator to transform the meshes\ninto dense motion and synthesize high-quality video frame-by-frame. Extensive\nexperiments show that the proposed VividTalk can generate high-visual quality\ntalking head videos with lip-sync and realistic enhanced by a large margin, and\noutperforms previous state-of-the-art works in objective and subjective\ncomparisons.",
            "author": [
                "Xusen Sun",
                "Longhao Zhang",
                "Hao Zhu",
                "Peng Zhang",
                "Bang Zhang",
                "Xinya Ji",
                "Kangneng Zhou",
                "Daiheng Gao",
                "Liefeng Bo",
                "Xun Cao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01841v2",
                "http://arxiv.org/pdf/2312.01841v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01837v1",
            "title": "Prompting Disentangled Embeddings for Knowledge Graph Completion with\n  Pre-trained Language Model",
            "updated": "2023-12-04T12:20:25Z",
            "published": "2023-12-04T12:20:25Z",
            "summary": "Both graph structures and textual information play a critical role in\nKnowledge Graph Completion (KGC). With the success of Pre-trained Language\nModels (PLMs) such as BERT, they have been applied for text encoding for KGC.\nHowever, the current methods mostly prefer to fine-tune PLMs, leading to huge\ntraining costs and limited scalability to larger PLMs. In contrast, we propose\nto utilize prompts and perform KGC on a frozen PLM with only the prompts\ntrained. Accordingly, we propose a new KGC method named PDKGC with two prompts\n-- a hard task prompt which is to adapt the KGC task to the PLM pre-training\ntask of token prediction, and a disentangled structure prompt which learns\ndisentangled graph representation so as to enable the PLM to combine more\nrelevant structure knowledge with the text information. With the two prompts,\nPDKGC builds a textual predictor and a structural predictor, respectively, and\ntheir combination leads to more comprehensive entity prediction. Solid\nevaluation on two widely used KGC datasets has shown that PDKGC often\noutperforms the baselines including the state-of-the-art, and its components\nare all effective. Our codes and data are available at\nhttps://github.com/genggengcss/PDKGC.",
            "author": [
                "Yuxia Geng",
                "Jiaoyan Chen",
                "Yuhang Zeng",
                "Zhuo Chen",
                "Wen Zhang",
                "Jeff Z. Pan",
                "Yuxiang Wang",
                "Xiaoliang Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01837v1",
                "http://arxiv.org/pdf/2312.01837v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01835v1",
            "title": "Few Clicks Suffice: Active Test-Time Adaptation for Semantic\n  Segmentation",
            "updated": "2023-12-04T12:16:02Z",
            "published": "2023-12-04T12:16:02Z",
            "summary": "Test-time adaptation (TTA) adapts the pre-trained models during inference\nusing unlabeled test data and has received a lot of research attention due to\nits potential practical value. Unfortunately, without any label supervision,\nexisting TTA methods rely heavily on heuristic or empirical studies. Where to\nupdate the model always falls into suboptimal or brings more computational\nresource consumption. Meanwhile, there is still a significant performance gap\nbetween the TTA approaches and their supervised counterparts. Motivated by\nactive learning, in this work, we propose the active test-time adaptation for\nsemantic segmentation setup. Specifically, we introduce the human-in-the-loop\npattern during the testing phase, which queries very few labels to facilitate\npredictions and model updates in an online manner. To do so, we propose a\nsimple but effective ATASeg framework, which consists of two parts, i.e., model\nadapter and label annotator. Extensive experiments demonstrate that ATASeg\nbridges the performance gap between TTA methods and their supervised\ncounterparts with only extremely few annotations, even one click for labeling\nsurpasses known SOTA TTA methods by 2.6% average mIoU on ACDC benchmark.\nEmpirical results imply that progress in either the model adapter or the label\nannotator will bring improvements to the ATASeg framework, giving it large\nresearch and reality potential.",
            "author": [
                "Longhui Yuan",
                "Shuang Li",
                "Zhuo He",
                "Binhui Xie"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01835v1",
                "http://arxiv.org/pdf/2312.01835v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01836v1",
            "title": "Integrated Drill Boom Hole-Seeking Control via Reinforcement Learning",
            "updated": "2023-12-04T12:16:02Z",
            "published": "2023-12-04T12:16:02Z",
            "summary": "Intelligent drill boom hole-seeking is a promising technology for enhancing\ndrilling efficiency, mitigating potential safety hazards, and relieving human\noperators. Most existing intelligent drill boom control methods rely on a\nhierarchical control framework based on inverse kinematics. However, these\nmethods are generally time-consuming due to the computational complexity of\ninverse kinematics and the inefficiency of the sequential execution of multiple\njoints. To tackle these challenges, this study proposes an integrated drill\nboom control method based on Reinforcement Learning (RL). We develop an\nintegrated drill boom control framework that utilizes a parameterized policy to\ndirectly generate control inputs for all joints at each time step, taking\nadvantage of joint posture and target hole information. By formulating the\nhole-seeking task as a Markov decision process, contemporary mainstream RL\nalgorithms can be directly employed to learn a hole-seeking policy, thus\neliminating the need for inverse kinematics solutions and promoting cooperative\nmulti-joint control. To enhance the drilling accuracy throughout the entire\ndrilling process, we devise a state representation that combines\nDenavit-Hartenberg joint information and preview hole-seeking discrepancy data.\nSimulation results show that the proposed method significantly outperforms\ntraditional methods in terms of hole-seeking accuracy and time efficiency.",
            "author": [
                "Haoqi Yan",
                "Haoyuan Xu",
                "Hongbo Gao",
                "Fei Ma",
                "Shengbo Eben Li",
                "Jingliang Duan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01836v1",
                "http://arxiv.org/pdf/2312.01836v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01826v1",
            "title": "Terrain-based Coverage Manifold Estimation: Machine Learning, Stochastic\n  Geometry, or Simulation?",
            "updated": "2023-12-04T11:59:19Z",
            "published": "2023-12-04T11:59:19Z",
            "summary": "Given the necessity of connecting the unconnected, covering blind spots has\nemerged as a critical task in the next-generation wireless communication\nnetwork. A direct solution involves obtaining a coverage manifold that visually\nshowcases network coverage performance at each position. Our goal is to devise\ndifferent methods that minimize the absolute error between the estimated\ncoverage manifold and the actual coverage manifold (referred to as accuracy),\nwhile simultaneously maximizing the reduction in computational complexity\n(measured by computational latency). Simulation is a common method for\nacquiring coverage manifolds. Although accurate, it is computationally\nexpensive, making it challenging to extend to large-scale networks. In this\npaper, we expedite traditional simulation methods by introducing a statistical\nmodel termed line-of-sight probability-based accelerated simulation. Stochastic\ngeometry is suitable for evaluating the performance of large-scale networks,\nalbeit in a coarse-grained manner. Therefore, we propose a second method\nwherein a model training approach is applied to the stochastic geometry\nframework to enhance accuracy and reduce complexity. Additionally, we propose a\nmachine learning-based method that ensures both low complexity and high\naccuracy, albeit with a significant demand for the size and quality of the\ndataset. Furthermore, we describe the relationships between these three\nmethods, compare their complexity and accuracy as performance verification, and\ndiscuss their application scenarios.",
            "author": [
                "Ruibo Wang",
                "Washim Uddin Mondal",
                "Mustafa A. Kishk",
                "Vaneet Aggarwal",
                "Mohamed-Slim Alouini"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01826v1",
                "http://arxiv.org/pdf/2312.01826v1"
            ],
            "primary_category": "cs.NI",
            "category": [
                "cs.NI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02243v1",
            "title": "FlowHON: Representing Flow Fields Using Higher-Order Networks",
            "updated": "2023-12-04T11:50:25Z",
            "published": "2023-12-04T11:50:25Z",
            "summary": "Flow fields are often partitioned into data blocks for massively parallel\ncomputation and analysis based on blockwise relationships. However, most of the\nprevious techniques only consider the first-order dependencies among blocks,\nwhich is insufficient in describing complex flow patterns. In this work, we\npresent FlowHON, an approach to construct higher-order networks (HONs) from\nflow fields. FlowHON captures the inherent higher-order dependencies in flow\nfields as nodes and estimates the transitions among them as edges. We formulate\nthe HON construction as an optimization problem with three linear\ntransformations. The first two layers correspond to the node generation and the\nthird one corresponds to edge estimation. Our formulation allows the node\ngeneration and edge estimation to be solved in a unified framework. With\nFlowHON, the rich set of traditional graph algorithms can be applied without\nany modification to analyze flow fields, while leveraging the higher-order\ninformation to understand the inherent structure and manage flow data for\nefficiency. We demonstrate the effectiveness of FlowHON using a series of\ndownstream tasks, including estimating the density of particles during tracing,\npartitioning flow fields for data management, and understanding flow fields\nusing the node-link diagram representation of networks.",
            "author": [
                "Nan Chen",
                "Zhihong Li",
                "Jun Tao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02243v1",
                "http://arxiv.org/pdf/2312.02243v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01818v1",
            "title": "Learning Machine Morality through Experience and Interaction",
            "updated": "2023-12-04T11:46:34Z",
            "published": "2023-12-04T11:46:34Z",
            "summary": "Increasing interest in ensuring safety of next-generation Artificial\nIntelligence (AI) systems calls for novel approaches to embedding morality into\nautonomous agents. Traditionally, this has been done by imposing explicit\ntop-down rules or hard constraints on systems, for example by filtering system\noutputs through pre-defined ethical rules. Recently, instead, entirely\nbottom-up methods for learning implicit preferences from human behavior have\nbecome increasingly popular, such as those for training and fine-tuning Large\nLanguage Models. In this paper, we provide a systematization of existing\napproaches to the problem of introducing morality in machines - modeled as a\ncontinuum, and argue that the majority of popular techniques lie at the\nextremes - either being fully hard-coded, or entirely learned, where no\nexplicit statement of any moral principle is required. Given the relative\nstrengths and weaknesses of each type of methodology, we argue that more hybrid\nsolutions are needed to create adaptable and robust, yet more controllable and\ninterpretable agents.\n  In particular, we present three case studies of recent works which use\nlearning from experience (i.e., Reinforcement Learning) to explicitly provide\nmoral principles to learning agents - either as intrinsic rewards, moral\nlogical constraints or textual principles for language models. For example,\nusing intrinsic rewards in Social Dilemma games, we demonstrate how it is\npossible to represent classical moral frameworks for agents. We also present an\noverview of the existing work in this area in order to provide empirical\nevidence for the potential of this hybrid approach. We then discuss strategies\nfor evaluating the effectiveness of moral learning agents. Finally, we present\nopen research questions and implications for the future of AI safety and ethics\nwhich are emerging from this framework.",
            "author": [
                "Elizaveta Tennant",
                "Stephen Hailes",
                "Mirco Musolesi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01818v1",
                "http://arxiv.org/pdf/2312.01818v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CY",
                "cs.LG",
                "cs.MA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01816v1",
            "title": "Class Symbolic Regression: Gotta Fit 'Em All",
            "updated": "2023-12-04T11:45:44Z",
            "published": "2023-12-04T11:45:44Z",
            "summary": "We introduce \"Class Symbolic Regression\" a first framework for automatically\nfinding a single analytical functional form that accurately fits multiple\ndatasets - each governed by its own (possibly) unique set of fitting\nparameters. This hierarchical framework leverages the common constraint that\nall the members of a single class of physical phenomena follow a common\ngoverning law. Our approach extends the capabilities of our earlier Physical\nSymbolic Optimization ($\\Phi$-SO) framework for Symbolic Regression, which\nintegrates dimensional analysis constraints and deep reinforcement learning for\nsymbolic analytical function discovery from data. We demonstrate the efficacy\nof this novel approach by applying it to a panel of synthetic toy case datasets\nand showcase its practical utility for astrophysics by successfully extracting\nan analytic galaxy potential from a set of simulated orbits approximating\nstellar streams.",
            "author": [
                "Wassim Tenachi",
                "Rodrigo Ibata",
                "Thibaut L. Fran\u00e7ois",
                "Foivos I. Diakogiannis"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01816v1",
                "http://arxiv.org/pdf/2312.01816v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "astro-ph.GA",
                "astro-ph.IM",
                "physics.comp-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01811v1",
            "title": "Energy-based Potential Games for Joint Motion Forecasting and Control",
            "updated": "2023-12-04T11:30:26Z",
            "published": "2023-12-04T11:30:26Z",
            "summary": "This work uses game theory as a mathematical framework to address interaction\nmodeling in multi-agent motion forecasting and control. Despite its\ninterpretability, applying game theory to real-world robotics, like automated\ndriving, faces challenges such as unknown game parameters. To tackle these, we\nestablish a connection between differential games, optimal control, and\nenergy-based models, demonstrating how existing approaches can be unified under\nour proposed Energy-based Potential Game formulation. Building upon this, we\nintroduce a new end-to-end learning application that combines neural networks\nfor game-parameter inference with a differentiable game-theoretic optimization\nlayer, acting as an inductive bias. The analysis provides empirical evidence\nthat the game-theoretic layer adds interpretability and improves the predictive\nperformance of various neural network backbones using two simulations and two\nreal-world driving datasets.",
            "author": [
                "Christopher Diehl",
                "Tobias Klosek",
                "Martin Kr\u00fcger",
                "Nils Murzyn",
                "Timo Osterburg",
                "Torsten Bertram"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01811v1",
                "http://arxiv.org/pdf/2312.01811v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.GT",
                "cs.MA",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03006v1",
            "title": "Cone Ranking for Multi-Criteria Decision Making",
            "updated": "2023-12-04T11:13:42Z",
            "published": "2023-12-04T11:13:42Z",
            "summary": "Recently introduced cone distribution functions from statistics are turned\ninto multi-criteria decision making (MCDM) tools. It is demonstrated that this\nprocedure can be considered as an upgrade of the weighted sum scalarization\ninsofar as it absorbs a whole collection of weighted sum scalarizations at once\ninstead of fixing a particular one in advance. Moreover, situations are\ncharacterized in which different types of rank reversal occur, and it is\nexplained why this might even be useful for analyzing the ranking procedure. A\nfew examples will be discussed and a potential application in machine learning\nis outlined.",
            "author": [
                "Andreas H Hamel",
                "Daniel Kostner"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03006v1",
                "http://arxiv.org/pdf/2312.03006v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.LG",
                "math.ST",
                "stat.TH",
                "62G30, 90C29"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01800v1",
            "title": "Collaborative Neural Painting",
            "updated": "2023-12-04T10:45:12Z",
            "published": "2023-12-04T10:45:12Z",
            "summary": "The process of painting fosters creativity and rational planning. However,\nexisting generative AI mostly focuses on producing visually pleasant artworks,\nwithout emphasizing the painting process. We introduce a novel task,\nCollaborative Neural Painting (CNP), to facilitate collaborative art painting\ngeneration between humans and machines. Given any number of user-input\nbrushstrokes as the context or just the desired object class, CNP should\nproduce a sequence of strokes supporting the completion of a coherent painting.\nImportantly, the process can be gradual and iterative, so allowing users'\nmodifications at any phase until the completion. Moreover, we propose to solve\nthis task using a painting representation based on a sequence of parametrized\nstrokes, which makes it easy both editing and composition operations. These\nparametrized strokes are processed by a Transformer-based architecture with a\nnovel attention mechanism to model the relationship between the input strokes\nand the strokes to complete. We also propose a new masking scheme to reflect\nthe interactive nature of CNP and adopt diffusion models as the basic learning\nprocess for its effectiveness and diversity in the generative field. Finally,\nto develop and validate methods on the novel task, we introduce a new dataset\nof painted objects and an evaluation protocol to benchmark CNP both\nquantitatively and qualitatively. We demonstrate the effectiveness of our\napproach and the potential of the CNP task as a promising avenue for future\nresearch.",
            "author": [
                "Nicola Dall'Asen",
                "Willi Menapace",
                "Elia Peruzzo",
                "Enver Sangineto",
                "Yiming Wang",
                "Elisa Ricci"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01800v1",
                "http://arxiv.org/pdf/2312.01800v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01797v1",
            "title": "LLM A*: Human in the Loop Large Language Models Enabled A* Search for\n  Robotics",
            "updated": "2023-12-04T10:37:58Z",
            "published": "2023-12-04T10:37:58Z",
            "summary": "This research focuses on how Large Language Models (LLMs) can help with path\nplanning for mobile embodied agents such as robots, in a human-in-the-loop and\ninteractive manner. A novel framework named LLM A*, aims to leverage the\ncommonsense of LLMs, and the utility-optimal A* is proposed to facilitate\nfew-shot near-optimal path planning. Prompts are used to 1) provide LLMs with\nessential information like environment, cost, heuristics, etc.; 2) communicate\nhuman feedback to LLMs on intermediate planning results. This makes the whole\npath planning process a `white box' and human feedback guides LLM A* to\nconverge quickly compared to other data-driven methods such as reinforcement\nlearning-based (RL) path planning. In addition, it makes code-free path\nplanning practical, henceforth promoting the inclusiveness of artificial\nintelligence techniques. Comparative analysis against A* and RL shows that LLM\nA* is more efficient in terms of search space and achieves an on-a-par path\nwith A* and a better path than RL. The interactive nature of LLM A* also makes\nit a promising tool for deployment in collaborative human-robot tasks.",
            "author": [
                "Hengjia Xiao",
                "Peng Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01797v1",
                "http://arxiv.org/pdf/2312.01797v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI",
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01795v1",
            "title": "Distributed Continual Learning with CoCoA in High-dimensional Linear\n  Regression",
            "updated": "2023-12-04T10:35:46Z",
            "published": "2023-12-04T10:35:46Z",
            "summary": "We consider estimation under scenarios where the signals of interest exhibit\nchange of characteristics over time. In particular, we consider the continual\nlearning problem where different tasks, e.g., data with different\ndistributions, arrive sequentially and the aim is to perform well on the newly\narrived task without performance degradation on the previously seen tasks. In\ncontrast to the continual learning literature focusing on the centralized\nsetting, we investigate the problem from a distributed estimation perspective.\nWe consider the well-established distributed learning algorithm COCOA, which\ndistributes the model parameters and the corresponding features over the\nnetwork. We provide exact analytical characterization for the generalization\nerror of COCOA under continual learning for linear regression in a range of\nscenarios, where overparameterization is of particular interest. These\nanalytical results characterize how the generalization error depends on the\nnetwork structure, the task similarity and the number of tasks, and show how\nthese dependencies are intertwined. In particular, our results show that the\ngeneralization error can be significantly reduced by adjusting the network\nsize, where the most favorable network size depends on task similarity and the\nnumber of tasks. We present numerical results verifying the theoretical\nanalysis and illustrate the continual learning performance of COCOA with a\ndigit classification task.",
            "author": [
                "Martin Hellkvist",
                "Ay\u00e7a \u00d6z\u00e7elikkale",
                "Anders Ahl\u00e9n"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01795v1",
                "http://arxiv.org/pdf/2312.01795v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01792v1",
            "title": "Wild-Tab: A Benchmark For Out-Of-Distribution Generalization In Tabular\n  Regression",
            "updated": "2023-12-04T10:27:38Z",
            "published": "2023-12-04T10:27:38Z",
            "summary": "Out-of-Distribution (OOD) generalization, a cornerstone for building robust\nmachine learning models capable of handling data diverging from the training\nset's distribution, is an ongoing challenge in deep learning. While significant\nprogress has been observed in computer vision and natural language processing,\nits exploration in tabular data, ubiquitous in many industrial applications,\nremains nascent. To bridge this gap, we present Wild-Tab, a large-scale\nbenchmark tailored for OOD generalization in tabular regression tasks. The\nbenchmark incorporates 3 industrial datasets sourced from fields like weather\nprediction and power consumption estimation, providing a challenging testbed\nfor evaluating OOD performance under real-world conditions. Our extensive\nexperiments, evaluating 10 distinct OOD generalization methods on Wild-Tab,\nreveal nuanced insights. We observe that many of these methods often struggle\nto maintain high-performance levels on unseen data, with OOD performance\nshowing a marked drop compared to in-distribution performance. At the same\ntime, Empirical Risk Minimization (ERM), despite its simplicity, delivers\nrobust performance across all evaluations, rivaling the results of\nstate-of-the-art methods. Looking forward, we hope that the release of Wild-Tab\nwill facilitate further research on OOD generalization and aid in the\ndeployment of machine learning models in various real-world contexts where\nhandling distribution shifts is a crucial requirement.",
            "author": [
                "Sergey Kolesnikov"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01792v1",
                "http://arxiv.org/pdf/2312.01792v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02240v1",
            "title": "Contrastive Learning-Based Spectral Knowledge Distillation for\n  Multi-Modality and Missing Modality Scenarios in Semantic Segmentation",
            "updated": "2023-12-04T10:27:09Z",
            "published": "2023-12-04T10:27:09Z",
            "summary": "Improving the performance of semantic segmentation models using multispectral\ninformation is crucial, especially for environments with low-light and adverse\nconditions. Multi-modal fusion techniques pursue either the learning of\ncross-modality features to generate a fused image or engage in knowledge\ndistillation but address multimodal and missing modality scenarios as distinct\nissues, which is not an optimal approach for multi-sensor models. To address\nthis, a novel multi-modal fusion approach called CSK-Net is proposed, which\nuses a contrastive learning-based spectral knowledge distillation technique\nalong with an automatic mixed feature exchange mechanism for semantic\nsegmentation in optical (EO) and infrared (IR) images. The distillation scheme\nextracts detailed textures from the optical images and distills them into the\noptical branch of CSK-Net. The model encoder consists of shared convolution\nweights with separate batch norm (BN) layers for both modalities, to capture\nthe multi-spectral information from different modalities of the same objects. A\nNovel Gated Spectral Unit (GSU) and mixed feature exchange strategy are\nproposed to increase the correlation of modality-shared information and\ndecrease the modality-specific information during the distillation process.\nComprehensive experiments show that CSK-Net surpasses state-of-the-art models\nin multi-modal tasks and for missing modalities when exclusively utilizing IR\ndata for inference across three public benchmarking datasets. For missing\nmodality scenarios, the performance increase is achieved without additional\ncomputational costs compared to the baseline segmentation models.",
            "author": [
                "Aniruddh Sikdar",
                "Jayant Teotia",
                "Suresh Sundaram"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02240v1",
                "http://arxiv.org/pdf/2312.02240v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01787v1",
            "title": "Developing Linguistic Patterns to Mitigate Inherent Human Bias in\n  Offensive Language Detection",
            "updated": "2023-12-04T10:20:36Z",
            "published": "2023-12-04T10:20:36Z",
            "summary": "With the proliferation of social media, there has been a sharp increase in\noffensive content, particularly targeting vulnerable groups, exacerbating\nsocial problems such as hatred, racism, and sexism. Detecting offensive\nlanguage use is crucial to prevent offensive language from being widely shared\non social media. However, the accurate detection of irony, implication, and\nvarious forms of hate speech on social media remains a challenge. Natural\nlanguage-based deep learning models require extensive training with large,\ncomprehensive, and labeled datasets. Unfortunately, manually creating such\ndatasets is both costly and error-prone. Additionally, the presence of\nhuman-bias in offensive language datasets is a major concern for deep learning\nmodels. In this paper, we propose a linguistic data augmentation approach to\nreduce bias in labeling processes, which aims to mitigate the influence of\nhuman bias by leveraging the power of machines to improve the accuracy and\nfairness of labeling processes. This approach has the potential to improve\noffensive language classification tasks across multiple languages and reduce\nthe prevalence of offensive content on social media.",
            "author": [
                "Toygar Tanyel",
                "Besher Alkurdi",
                "Serkan Ayvaz"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01787v1",
                "http://arxiv.org/pdf/2312.01787v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01771v1",
            "title": "IMProv: Inpainting-based Multimodal Prompting for Computer Vision Tasks",
            "updated": "2023-12-04T09:48:29Z",
            "published": "2023-12-04T09:48:29Z",
            "summary": "In-context learning allows adapting a model to new tasks given a task\ndescription at test time. In this paper, we present IMProv - a generative model\nthat is able to in-context learn visual tasks from multimodal prompts. Given a\ntextual description of a visual task (e.g. \"Left: input image, Right:\nforeground segmentation\"), a few input-output visual examples, or both, the\nmodel in-context learns to solve it for a new test input. We train a masked\ngenerative transformer on a new dataset of figures from computer vision papers\nand their associated captions, together with a captioned large-scale image-text\ndataset. During inference time, we prompt the model with text and/or image task\nexample(s) and have the model inpaint the corresponding output. We show that\ntraining our model with text conditioning and scaling the dataset size improves\nin-context learning for computer vision tasks by over +10\\% AP for Foreground\nSegmentation, over +5\\% gains in AP for Single Object Detection, and almost\n20\\% lower LPIPS in Colorization. Our empirical results suggest that vision and\nlanguage prompts are complementary and it is advantageous to use both to\nachieve better in-context learning performance. Project page is available at\nhttps://jerryxu.net/IMProv .",
            "author": [
                "Jiarui Xu",
                "Yossi Gandelsman",
                "Amir Bar",
                "Jianwei Yang",
                "Jianfeng Gao",
                "Trevor Darrell",
                "Xiaolong Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01771v1",
                "http://arxiv.org/pdf/2312.01771v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03005v1",
            "title": "Few-Shot Anomaly Detection with Adversarial Loss for Robust Feature\n  Representations",
            "updated": "2023-12-04T09:45:02Z",
            "published": "2023-12-04T09:45:02Z",
            "summary": "Anomaly detection is a critical and challenging task that aims to identify\ndata points deviating from normal patterns and distributions within a dataset.\nVarious methods have been proposed using a one-class-one-model approach, but\nthese techniques often face practical problems such as memory inefficiency and\nthe requirement of sufficient data for training. In particular, few-shot\nanomaly detection presents significant challenges in industrial applications,\nwhere limited samples are available before mass production. In this paper, we\npropose a few-shot anomaly detection method that integrates adversarial\ntraining loss to obtain more robust and generalized feature representations. We\nutilize the adversarial loss previously employed in domain adaptation to align\nfeature distributions between source and target domains, to enhance feature\nrobustness and generalization in few-shot anomaly detection tasks. We\nhypothesize that adversarial loss is effective when applied to features that\nshould have similar characteristics, such as those from the same layer in a\nSiamese network's parallel branches or input-output pairs of\nreconstruction-based methods. Experimental results demonstrate that the\nproposed method generally achieves better performance when utilizing the\nadversarial loss.",
            "author": [
                "Jae Young Lee",
                "Wonjun Lee",
                "Jaehyun Choi",
                "Yongkwi Lee",
                "Young Seog Yoon"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03005v1",
                "http://arxiv.org/pdf/2312.03005v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01764v1",
            "title": "Dynamic Erasing Network Based on Multi-Scale Temporal Features for\n  Weakly Supervised Video Anomaly Detection",
            "updated": "2023-12-04T09:40:11Z",
            "published": "2023-12-04T09:40:11Z",
            "summary": "The goal of weakly supervised video anomaly detection is to learn a detection\nmodel using only video-level labeled data. However, prior studies typically\ndivide videos into fixed-length segments without considering the complexity or\nduration of anomalies. Moreover, these studies usually just detect the most\nabnormal segments, potentially overlooking the completeness of anomalies. To\naddress these limitations, we propose a Dynamic Erasing Network (DE-Net) for\nweakly supervised video anomaly detection, which learns multi-scale temporal\nfeatures. Specifically, to handle duration variations of abnormal events, we\nfirst propose a multi-scale temporal modeling module, capable of extracting\nfeatures from segments of varying lengths and capturing both local and global\nvisual information across different temporal scales. Then, we design a dynamic\nerasing strategy, which dynamically assesses the completeness of the detected\nanomalies and erases prominent abnormal segments in order to encourage the\nmodel to discover gentle abnormal segments in a video. The proposed method\nobtains favorable performance compared to several state-of-the-art approaches\non three datasets: XD-Violence, TAD, and UCF-Crime. Code will be made available\nat https://github.com/ArielZc/DE-Net.",
            "author": [
                "Chen Zhang",
                "Guorong Li",
                "Yuankai Qi",
                "Hanhua Ye",
                "Laiyun Qing",
                "Ming-Hsuan Yang",
                "Qingming Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01764v1",
                "http://arxiv.org/pdf/2312.01764v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01758v1",
            "title": "CZL-CIAE: CLIP-driven Zero-shot Learning for Correcting Inverse Age\n  Estimation",
            "updated": "2023-12-04T09:35:36Z",
            "published": "2023-12-04T09:35:36Z",
            "summary": "Zero-shot age estimation aims to learn feature information about age from\ninput images and make inferences about a given person's image or video frame\nwithout specific sample data. The development of zero-shot age estimation can\nimprove the efficiency and accuracy of various applications (e.g., age\nverification and secure access control, etc.), while also promoting research on\nmulti-modal and zero-shot learning in the social media field. For example,\nzero-sample age estimation can be used to create social networks focused on\nspecific age groups. However, existing methods mainly focus on supervised,\nlabeled age estimation learning, and the prediction effect of zero-shot\nlearning is very poor. To tackle the above issues, we propose a novel\nCLIP-driven Zero-shot Learning for Correcting Inverse Age Estimation\n(CZL-CIAE). Specifically, we first introduce the CLIP model to extract image\nfeatures and text semantic information respectively, and map them into a highly\nsemantically aligned high-dimensional feature space. Next, we designed a new\nTransformer architecture (i.e., FourierFormer) to achieve channel evolution and\nspatial interaction of images, and to fuse image and text semantic information.\nFinally, we introduce reversible age estimation, which uses end-to-end error\nfeedback to reduce the error rate of age predictions. Through extensive\nexperiments on multiple data sets, CZL-CIAE has achieved better age prediction\nresults.",
            "author": [
                "Yuntao Shou",
                "Wei Ai",
                "Tao Meng",
                "Keqin Li"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01758v1",
                "http://arxiv.org/pdf/2312.01758v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01756v1",
            "title": "A Comprehensive Literature Review on Sweet Orange Leaf Diseases",
            "updated": "2023-12-04T09:35:21Z",
            "published": "2023-12-04T09:35:21Z",
            "summary": "Sweet orange leaf diseases are significant to agricultural productivity. Leaf\ndiseases impact fruit quality in the citrus industry. The apparition of machine\nlearning makes the development of disease finder. Early detection and diagnosis\nare necessary for leaf management. Sweet orange leaf disease-predicting\nautomated systems have already been developed using different image-processing\ntechniques. This comprehensive literature review is systematically based on\nleaf disease and machine learning methodologies applied to the detection of\ndamaged leaves via image classification. The benefits and limitations of\ndifferent machine learning models, including Vision Transformer (ViT), Neural\nNetwork (CNN), CNN with SoftMax and RBF SVM, Hybrid CNN-SVM, HLB-ConvMLP,\nEfficientNet-b0, YOLOv5, YOLOv7, Convolutional, Deep CNN. These machine\nlearning models tested on various datasets and detected the disease. This\ncomprehensive review study related to leaf disease compares the performance of\nthe models; those models' accuracy, precision, recall, etc., were used in the\nsubsisting studies",
            "author": [
                "Yousuf Rayhan Emon",
                "Md Golam Rabbani",
                "Dr. Md. Taimur Ahad",
                "Faruk Ahmed"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01756v1",
                "http://arxiv.org/pdf/2312.01756v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02239v1",
            "title": "Model-based Deep Learning for Beam Prediction based on a Channel Chart",
            "updated": "2023-12-04T09:31:17Z",
            "published": "2023-12-04T09:31:17Z",
            "summary": "Channel charting builds a map of the radio environment in an unsupervised\nway. The obtained chart locations can be seen as low-dimensional compressed\nversions of channel state information that can be used for a wide variety of\napplications, including beam prediction. In non-standalone or cell-free\nsystems, chart locations computed at a given base station can be transmitted to\nseveral other base stations (possibly operating at different frequency bands)\nfor them to predict which beams to use. This potentially yields a dramatic\nreduction of the overhead due to channel estimation or beam management, since\nonly the base station performing charting requires channel state information,\nthe others directly predicting the beam from the chart location. In this paper,\nadvanced model-based neural network architectures are proposed for both channel\ncharting and beam prediction. The proposed methods are assessed on realistic\nsynthetic channels, yielding promising results.",
            "author": [
                "Taha Yassine",
                "Baptiste Chatelier",
                "Vincent Corlay",
                "Matthieu Crussi\u00e8re",
                "Stephane Paquelet",
                "Olav Tirkkonen",
                "Luc Le Magoarou"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02239v1",
                "http://arxiv.org/pdf/2312.02239v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "cs.AI",
                "cs.LG",
                "eess.SP",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01753v1",
            "title": "Long-Tail Learning with Rebalanced Contrastive Loss",
            "updated": "2023-12-04T09:27:03Z",
            "published": "2023-12-04T09:27:03Z",
            "summary": "Integrating supervised contrastive loss to cross entropy-based communication\nhas recently been proposed as a solution to address the long-tail learning\nproblem. However, when the class imbalance ratio is high, it requires adjusting\nthe supervised contrastive loss to support the tail classes, as the\nconventional contrastive learning is biased towards head classes by default. To\nthis end, we present Rebalanced Contrastive Learning (RCL), an efficient means\nto increase the long tail classification accuracy by addressing three main\naspects: 1. Feature space balancedness - Equal division of the feature space\namong all the classes, 2. Intra-Class compactness - Reducing the distance\nbetween same-class embeddings, 3. Regularization - Enforcing larger margins for\ntail classes to reduce overfitting. RCL adopts class frequency-based SoftMax\nloss balancing to supervised contrastive learning loss and exploits scalar\nmultiplied features fed to the contrastive learning loss to enforce\ncompactness. We implement RCL on the Balanced Contrastive Learning (BCL)\nFramework, which has the SOTA performance. Our experiments on three benchmark\ndatasets demonstrate the richness of the learnt embeddings and increased top-1\nbalanced accuracy RCL provides to the BCL framework. We further demonstrate\nthat the performance of RCL as a standalone loss also achieves state-of-the-art\nlevel accuracy.",
            "author": [
                "Charika De Alvis",
                "Dishanika Denipitiyage",
                "Suranga Seneviratne"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01753v1",
                "http://arxiv.org/pdf/2312.01753v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01751v1",
            "title": "Joint Task Partitioning and Parallel Scheduling in Device-Assisted\n  Mobile Edge Networks",
            "updated": "2023-12-04T09:22:42Z",
            "published": "2023-12-04T09:22:42Z",
            "summary": "With the development of the Internet of Things (IoT), certain IoT devices\nhave the capability to not only accomplish their own tasks but also\nsimultaneously assist other resource-constrained devices. Therefore, this paper\nconsiders a device-assisted mobile edge computing system that leverages\nauxiliary IoT devices to alleviate the computational burden on the edge\ncomputing server and enhance the overall system performance. In this study,\ncomputationally intensive tasks are decomposed into multiple partitions, and\neach task partition can be processed in parallel on an IoT device or the edge\nserver. The objective of this research is to develop an efficient online\nalgorithm that addresses the joint optimization of task partitioning and\nparallel scheduling under time-varying system states, posing challenges to\nconventional numerical optimization methods. To address these challenges, a\nframework called online task partitioning action and parallel scheduling policy\ngeneration (OTPPS) is proposed, which is based on deep reinforcement learning\n(DRL). Specifically, the framework leverages a deep neural network (DNN) to\nlearn the optimal partitioning action for each task by mapping input states.\nFurthermore, it is demonstrated that the remaining parallel scheduling problem\nexhibits NP-hard complexity when considering a specific task partitioning\naction. To address this subproblem, a fair and delay-minimized task scheduling\n(FDMTS) algorithm is designed. Extensive evaluation results demonstrate that\nOTPPS achieves near-optimal average delay performance and consistently high\nfairness levels in various environmental states compared to other baseline\nschemes.",
            "author": [
                "Yang Li",
                "Xinlei Ge",
                "Bo Lei",
                "Xing Zhang",
                "Wenbo Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01751v1",
                "http://arxiv.org/pdf/2312.01751v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01748v1",
            "title": "Deep CNN for Coherent Seismic Noise Removal: A Perspective",
            "updated": "2023-12-04T09:11:06Z",
            "published": "2023-12-04T09:11:06Z",
            "summary": "Seismic denoising is an important processing step before subsequent imaging\nand interpretation, which consumes a significant amount of time, whether it is\nfor Quality control or for the associated computations. We present results of\nour work in training convolutional neural networks for denoising seismic data,\nspecifically attenuation of surface related multiples and removal of overlap of\nshot energies during simultaneous-shooting survey. The proposed methodology is\nbeing explored not only for its ability to minimize human involvement but also\nbecause of the trained filter's ability to accelerate the process, hence,\nreduce processing time.",
            "author": [
                "Rohit Shrivastava",
                "Ashish Asgekar",
                "Evert Kramer"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01748v1",
                "http://arxiv.org/pdf/2312.01748v1"
            ],
            "primary_category": "cs.CE",
            "category": [
                "cs.CE",
                "physics.geo-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01747v1",
            "title": "Autonomous and Adaptive Role Selection for Multi-robot Collaborative\n  Area Search Based on Deep Reinforcement Learning",
            "updated": "2023-12-04T09:10:44Z",
            "published": "2023-12-04T09:10:44Z",
            "summary": "In the tasks of multi-robot collaborative area search, we propose the unified\napproach for simultaneous mapping for sensing more targets (exploration) while\nsearching and locating the targets (coverage). Specifically, we implement a\nhierarchical multi-agent reinforcement learning algorithm to decouple task\nplanning from task execution. The role concept is integrated into the\nupper-level task planning for role selection, which enables robots to learn the\nrole based on the state status from the upper-view. Besides, an intelligent\nrole switching mechanism enables the role selection module to function between\ntwo timesteps, promoting both exploration and coverage interchangeably. Then\nthe primitive policy learns how to plan based on their assigned roles and local\nobservation for sub-task execution. The well-designed experiments show the\nscalability and generalization of our method compared with state-of-the-art\napproaches in the scenes with varying complexity and number of robots.",
            "author": [
                "Lina Zhu",
                "Jiyu Cheng",
                "Hao Zhang",
                "Zhichao Cui",
                "Wei Zhang",
                "Yuehu Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01747v1",
                "http://arxiv.org/pdf/2312.01747v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01745v1",
            "title": "Cross-Modal Adaptive Dual Association for Text-to-Image Person Retrieval",
            "updated": "2023-12-04T09:10:24Z",
            "published": "2023-12-04T09:10:24Z",
            "summary": "Text-to-image person re-identification (ReID) aims to retrieve images of a\nperson based on a given textual description. The key challenge is to learn the\nrelations between detailed information from visual and textual modalities.\nExisting works focus on learning a latent space to narrow the modality gap and\nfurther build local correspondences between two modalities. However, these\nmethods assume that image-to-text and text-to-image associations are\nmodality-agnostic, resulting in suboptimal associations. In this work, we show\nthe discrepancy between image-to-text association and text-to-image association\nand propose CADA: Cross-Modal Adaptive Dual Association that finely builds\nbidirectional image-text detailed associations. Our approach features a\ndecoder-based adaptive dual association module that enables full interaction\nbetween visual and textual modalities, allowing for bidirectional and adaptive\ncross-modal correspondence associations. Specifically, the paper proposes a\nbidirectional association mechanism: Association of text Tokens to image\nPatches (ATP) and Association of image Regions to text Attributes (ARA). We\nadaptively model the ATP based on the fact that aggregating cross-modal\nfeatures based on mistaken associations will lead to feature distortion. For\nmodeling the ARA, since the attributes are typically the first distinguishing\ncues of a person, we propose to explore the attribute-level association by\npredicting the masked text phrase using the related image region. Finally, we\nlearn the dual associations between texts and images, and the experimental\nresults demonstrate the superiority of our dual formulation. Codes will be made\npublicly available.",
            "author": [
                "Dixuan Lin",
                "Yixing Peng",
                "Jingke Meng",
                "Wei-Shi Zheng"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01745v1",
                "http://arxiv.org/pdf/2312.01745v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01744v1",
            "title": "SEFGAN: Harvesting the Power of Normalizing Flows and GANs for Efficient\n  High-Quality Speech Enhancement",
            "updated": "2023-12-04T09:10:08Z",
            "published": "2023-12-04T09:10:08Z",
            "summary": "This paper proposes SEFGAN, a Deep Neural Network (DNN) combining maximum\nlikelihood training and Generative Adversarial Networks (GANs) for efficient\nspeech enhancement (SE). For this, a DNN is trained to synthesize the enhanced\nspeech conditioned on noisy speech using a Normalizing Flow (NF) as generator\nin a GAN framework. While the combination of likelihood models and GANs is not\ntrivial, SEFGAN demonstrates that a hybrid adversarial and maximum likelihood\ntraining approach enables the model to maintain high quality audio generation\nand log-likelihood estimation. Our experiments indicate that this approach\nstrongly outperforms the baseline NF-based model without introducing additional\ncomplexity to the enhancement network. A comparison using computational metrics\nand a listening experiment reveals that SEFGAN is competitive with other\nstate-of-the-art models.",
            "author": [
                "Martin Strauss",
                "Nicola Pia",
                "Nagashree K. S. Rao",
                "Bernd Edler"
            ],
            "link": [
                "http://dx.doi.org/10.1109/WASPAA58266.2023.10248144",
                "http://arxiv.org/abs/2312.01744v1",
                "http://arxiv.org/pdf/2312.01744v1"
            ],
            "primary_category": "eess.AS",
            "category": [
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02237v1",
            "title": "Singular Regularization with Information Bottleneck Improves Model's\n  Adversarial Robustness",
            "updated": "2023-12-04T09:07:30Z",
            "published": "2023-12-04T09:07:30Z",
            "summary": "Adversarial examples are one of the most severe threats to deep learning\nmodels. Numerous works have been proposed to study and defend adversarial\nexamples. However, these works lack analysis of adversarial information or\nperturbation, which cannot reveal the mystery of adversarial examples and lose\nproper interpretation. In this paper, we aim to fill this gap by studying\nadversarial information as unstructured noise, which does not have a clear\npattern. Specifically, we provide some empirical studies with singular value\ndecomposition, by decomposing images into several matrices, to analyze\nadversarial information for different attacks. Based on the analysis, we\npropose a new module to regularize adversarial information and combine\ninformation bottleneck theory, which is proposed to theoretically restrict\nintermediate representations. Therefore, our method is interpretable. Moreover,\nthe fashion of our design is a novel principle that is general and unified.\nEquipped with our new module, we evaluate two popular model structures on two\nmainstream datasets with various adversarial attacks. The results indicate that\nthe improvement in robust accuracy is significant. On the other hand, we prove\nthat our method is efficient with only a few additional parameters and able to\nbe explained under regional faithfulness analysis.",
            "author": [
                "Guanlin Li",
                "Naishan Zheng",
                "Man Zhou",
                "Jie Zhang",
                "Tianwei Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02237v1",
                "http://arxiv.org/pdf/2312.02237v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01742v1",
            "title": "Fully Spiking Denoising Diffusion Implicit Models",
            "updated": "2023-12-04T09:07:09Z",
            "published": "2023-12-04T09:07:09Z",
            "summary": "Spiking neural networks (SNNs) have garnered considerable attention owing to\ntheir ability to run on neuromorphic devices with super-high speeds and\nremarkable energy efficiencies. SNNs can be used in conventional neural\nnetwork-based time- and energy-consuming applications. However, research on\ngenerative models within SNNs remains limited, despite their advantages. In\nparticular, diffusion models are a powerful class of generative models, whose\nimage generation quality surpass that of the other generative models, such as\nGANs. However, diffusion models are characterized by high computational costs\nand long inference times owing to their iterative denoising feature. Therefore,\nwe propose a novel approach fully spiking denoising diffusion implicit model\n(FSDDIM) to construct a diffusion model within SNNs and leverage the high speed\nand low energy consumption features of SNNs via synaptic current learning\n(SCL). SCL fills the gap in that diffusion models use a neural network to\nestimate real-valued parameters of a predefined probabilistic distribution,\nwhereas SNNs output binary spike trains. The SCL enables us to complete the\nentire generative process of diffusion models exclusively using SNNs. We\ndemonstrate that the proposed method outperforms the state-of-the-art fully\nspiking generative model.",
            "author": [
                "Ryo Watanabe",
                "Yusuke Mukuta",
                "Tatsuya Harada"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01742v1",
                "http://arxiv.org/pdf/2312.01742v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01741v1",
            "title": "SRSNetwork: Siamese Reconstruction-Segmentation Networks based on\n  Dynamic-Parameter Convolution",
            "updated": "2023-12-04T09:06:41Z",
            "published": "2023-12-04T09:06:41Z",
            "summary": "In this paper, we present a high-performance deep neural network for weak\ntarget image segmentation, including medical image segmentation and infrared\nimage segmentation. To this end, this work analyzes the existing dynamic\nconvolutions and proposes dynamic parameter convolution (DPConv). Furthermore,\nit reevaluates the relationship between reconstruction tasks and segmentation\ntasks from the perspective of DPConv, leading to the proposal of a dual-network\nmodel called the Siamese Reconstruction-Segmentation Network (SRSNet). The\nproposed model is not only a universal network but also enhances the\nsegmentation performance without altering its structure, leveraging the\nreconstruction task. Additionally, as the amount of training data for the\nreconstruction network increases, the performance of the segmentation network\nalso improves synchronously. On seven datasets including five medical datasets\nand two infrared image datasets, our SRSNet consistently achieves the best\nsegmentation results. The code is released at https://github.com/fidshu/SRSNet.",
            "author": [
                "Bingkun Nian",
                "Fenghe Tang",
                "Jianrui Ding",
                "Pingping Zhang",
                "Jie Yang",
                "S. Kevin Zhou",
                "Wei Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01741v1",
                "http://arxiv.org/pdf/2312.01741v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "I.4.6"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01739v1",
            "title": "Divide-and-Conquer Strategy for Large-Scale Dynamic Bayesian Network\n  Structure Learning",
            "updated": "2023-12-04T09:03:06Z",
            "published": "2023-12-04T09:03:06Z",
            "summary": "Dynamic Bayesian Networks (DBNs), renowned for their interpretability, have\nbecome increasingly vital in representing complex stochastic processes in\nvarious domains such as gene expression analysis, healthcare, and traffic\nprediction. Structure learning of DBNs from data is challenging, particularly\nfor datasets with thousands of variables. Most current algorithms for DBN\nstructure learning are adaptations from those used in static Bayesian Networks\n(BNs), and are typically focused on small-scale problems. In order to solve\nlarge-scale problems while taking full advantage of existing algorithms, this\npaper introduces a novel divide-and-conquer strategy, originally developed for\nstatic BNs, and adapts it for large-scale DBN structure learning. In this work,\nwe specifically concentrate on 2 Time-sliced Bayesian Networks (2-TBNs), a\nspecial class of DBNs. Furthermore, we leverage the prior knowledge of 2-TBNs\nto enhance the performance of the strategy we introduce. Our approach\nsignificantly improves the scalability and accuracy of 2-TBN structure\nlearning. Experimental results demonstrate the effectiveness of our method,\nshowing substantial improvements over existing algorithms in both computational\nefficiency and structure learning accuracy. On problem instances with more than\n1,000 variables, our approach improves two accuracy metrics by 74.45% and\n110.94% on average , respectively, while reducing runtime by 93.65% on average.",
            "author": [
                "Hui Ouyang",
                "Cheng Chen",
                "Ke Tang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01739v1",
                "http://arxiv.org/pdf/2312.01739v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01738v1",
            "title": "Generalizing Political Leaning Inference to Multi-Party Systems:\n  Insights from the UK Political Landscape",
            "updated": "2023-12-04T09:02:17Z",
            "published": "2023-12-04T09:02:17Z",
            "summary": "An ability to infer the political leaning of social media users can help in\ngathering opinion polls thereby leading to a better understanding of public\nopinion. While there has been a body of research attempting to infer the\npolitical leaning of social media users, this has been typically simplified as\na binary classification problem (e.g. left vs right) and has been limited to a\nsingle location, leading to a dearth of investigation into more complex,\nmulticlass classification and its generalizability to different locations,\nparticularly those with multi-party systems. Our work performs the first such\neffort by studying political leaning inference in three of the UK's nations\n(Scotland, Wales and Northern Ireland), each of which has a different political\nlandscape composed of multiple parties. To do so, we collect and release a\ndataset comprising users labelled by their political leaning as well as\ninteractions with one another. We investigate the ability to predict the\npolitical leaning of users by leveraging these interactions in challenging\nscenarios such as few-shot learning, where training data is scarce, as well as\nassessing the applicability to users with different levels of political\nengagement. We show that interactions in the form of retweets between users can\nbe a very powerful feature to enable political leaning inference, leading to\nconsistent and robust results across different regions with multi-party\nsystems. However, we also see that there is room for improvement in predicting\nthe political leaning of users who are less engaged in politics.",
            "author": [
                "Joseba Fernandez de Landa",
                "Arkaitz Zubiaga",
                "Rodrigo Agerri"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01738v1",
                "http://arxiv.org/pdf/2312.01738v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01733v1",
            "title": "Metastability and anharmonicity enhance defect-assisted nonradiative\n  recombination in low-symmetry semiconductors",
            "updated": "2023-12-04T08:55:03Z",
            "published": "2023-12-04T08:55:03Z",
            "summary": "Strong nonradiative recombination has been observed in quasi-one-dimensional\nantimony selenide, which runs counter to the simple intuition that claims high\ndefect tolerance exists in semiconductors with antibonding state in the valence\nband and bonding state in the conduction band. Here we reveal such a defect\nintolerance actually stems from the richness of structural metastability and\nvibrational anharmonicity owing to the low-symmetry atomic structure. Taking\nthe deep defect V$_{\\rm Se}$ as a benchmark, we show the defect with its\nground-state configuration alone does not act as a recombination center.\nInstead, we identify three different configurations with different formation\nenergies, such richness of metastability offers a higher probability to\naccomplish a rapid recombination cycle. Another contributing factor is the\nanharmonicity in the potential energy surfaces that is caused by the large\natomic relaxation, which elevates the total capture coefficient by 2-3 orders\nof magnitude compared with harmonic approximation. Therefore, the unique\nproperties from both crystals and phonons in quasi-one-dimensional system\nenhance the nonradiative recombination, making the traditional intuition of\ndefect tolerance invalid. These results highlight the importance of the correct\nidentification of metastable defects and phonon anharmonicity in the\nnonradiative recombination in low-symmetry semiconductors.",
            "author": [
                "Menglin Huang",
                "Shanshan Wang",
                "Shiyou Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01733v1",
                "http://arxiv.org/pdf/2312.01733v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01729v1",
            "title": "EdgeConvFormer: Dynamic Graph CNN and Transformer based Anomaly\n  Detection in Multivariate Time Series",
            "updated": "2023-12-04T08:38:54Z",
            "published": "2023-12-04T08:38:54Z",
            "summary": "Transformer-based models for anomaly detection in multivariate time series\ncan benefit from the self-attention mechanism due to its advantage in modeling\nlong-term dependencies. However, Transformer-based anomaly detection models\nhave problems such as a large amount of data being required for training,\nstandard positional encoding is not suitable for multivariate time series data,\nand the interdependence between time series is not considered. To address these\nlimitations, we propose a novel anomaly detection method, named EdgeConvFormer,\nwhich integrates Time2vec embedding, stacked dynamic graph CNN, and Transformer\nto extract global and local spatial-time information. This design of\nEdgeConvFormer empowers it with decomposition capacities for complex time\nseries, progressive spatiotemporal correlation discovery between time series,\nand representation aggregation of multi-scale features. Experiments demonstrate\nthat EdgeConvFormer can learn the spatial-temporal correlations from\nmultivariate time series data and achieve better anomaly detection performance\nthan the state-of-the-art approaches on many real-world datasets of different\nscales.",
            "author": [
                "Jie Liu",
                "Qilin Li",
                "Senjian An",
                "Bradley Ezard",
                "Ling Li"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01729v1",
                "http://arxiv.org/pdf/2312.01729v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01728v1",
            "title": "ImputeFormer: Graph Transformers for Generalizable Spatiotemporal\n  Imputation",
            "updated": "2023-12-04T08:35:31Z",
            "published": "2023-12-04T08:35:31Z",
            "summary": "This paper focuses on the multivariate time series imputation problem using\ndeep neural architectures. The ubiquitous issue of missing data in both\nscientific and engineering tasks necessitates the development of an effective\nand general imputation model. Leveraging the wisdom and expertise garnered from\nlow-rank imputation methods, we power the canonical Transformers with three key\nknowledge-driven enhancements, including projected temporal attention, global\nadaptive graph convolution, and Fourier imputation loss. These task-agnostic\ninductive biases exploit the inherent structures of incomplete time series, and\nthus make our model versatile for a variety of imputation problems. We\ndemonstrate its superiority in terms of accuracy, efficiency, and flexibility\non heterogeneous datasets, including traffic speed, traffic volume, solar\nenergy, smart metering, and air quality. Comprehensive case studies are\nperformed to further strengthen the interpretability. Promising empirical\nresults provide strong conviction that incorporating time series primitives,\nsuch as low-rank properties, can substantially facilitate the development of a\ngeneralizable model to approach a wide range of spatiotemporal imputation\nproblems.",
            "author": [
                "Tong Nie",
                "Guoyang Qin",
                "Yuewen Mei",
                "Jian Sun"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01728v1",
                "http://arxiv.org/pdf/2312.01728v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01727v2",
            "title": "Deep learning acceleration of iterative model-based light fluence\n  correction for photoacoustic tomography",
            "updated": "2023-12-05T10:15:45Z",
            "published": "2023-12-04T08:34:19Z",
            "summary": "Photoacoustic tomography (PAT) is a promising imaging technique that can\nvisualize the distribution of chromophores within biological tissue. However,\nthe accuracy of PAT imaging is compromised by light fluence (LF), which hinders\nthe quantification of light absorbers. Currently, model-based iterative methods\nare used for LF correction, but they require significant computational\nresources due to repeated LF estimation based on differential light transport\nmodels. To improve LF correction efficiency, we propose to use Fourier neural\noperator (FNO), a neural network specially designed for solving differential\nequations, to learn the forward projection of light transport in PAT. Trained\nusing paired finite-element-based LF simulation data, our FNO model replaces\nthe traditional computational heavy LF estimator during iterative correction,\nsuch that the correction procedure is significantly accelerated. Simulation and\nexperimental results demonstrate that our method achieves comparable LF\ncorrection quality to traditional iterative methods while reducing the\ncorrection time by over 30 times.",
            "author": [
                "Zhaoyong Liang",
                "Shuangyang Zhang",
                "Zhichao Liang",
                "Zhongxin Mo",
                "Xiaoming Zhang",
                "Yutian Zhong",
                "Wufan Chen",
                "Li Qi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01727v2",
                "http://arxiv.org/pdf/2312.01727v2"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "physics.bio-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01726v1",
            "title": "Simultaneous Alignment and Surface Regression Using Hybrid 2D-3D\n  Networks for 3D Coherent Layer Segmentation of Retinal OCT Images with Full\n  and Sparse Annotations",
            "updated": "2023-12-04T08:32:31Z",
            "published": "2023-12-04T08:32:31Z",
            "summary": "Layer segmentation is important to quantitative analysis of retinal optical\ncoherence tomography (OCT). Recently, deep learning based methods have been\ndeveloped to automate this task and yield remarkable performance. However, due\nto the large spatial gap and potential mismatch between the B-scans of an OCT\nvolume, all of them were based on 2D segmentation of individual B-scans, which\nmay lose the continuity and diagnostic information of the retinal layers in 3D\nspace. Besides, most of these methods required dense annotation of the OCT\nvolumes, which is labor-intensive and expertise-demanding. This work presents a\nnovel framework based on hybrid 2D-3D convolutional neural networks (CNNs) to\nobtain continuous 3D retinal layer surfaces from OCT volumes, which works well\nwith both full and sparse annotations. The 2D features of individual B-scans\nare extracted by an encoder consisting of 2D convolutions. These 2D features\nare then used to produce the alignment displacement vectors and layer\nsegmentation by two 3D decoders coupled via a spatial transformer module. Two\nlosses are proposed to utilize the retinal layers' natural property of being\nsmooth for B-scan alignment and layer segmentation, respectively, and are the\nkey to the semi-supervised learning with sparse annotation. The entire\nframework is trained end-to-end. To the best of our knowledge, this is the\nfirst work that attempts 3D retinal layer segmentation in volumetric OCT images\nbased on CNNs. Experiments on a synthetic dataset and three public clinical\ndatasets show that our framework can effectively align the B-scans for\npotential motion correction, and achieves superior performance to\nstate-of-the-art 2D deep learning methods in terms of both layer segmentation\naccuracy and cross-B-scan 3D continuity in both fully and semi-supervised\nsettings, thus offering more clinical values than previous works.",
            "author": [
                "Hong Liu",
                "Dong Wei",
                "Donghuan Lu",
                "Xiaoying Tang",
                "Liansheng Wang",
                "Yefeng Zheng"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01726v1",
                "http://arxiv.org/pdf/2312.01726v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01725v1",
            "title": "StableVITON: Learning Semantic Correspondence with Latent Diffusion\n  Model for Virtual Try-On",
            "updated": "2023-12-04T08:27:59Z",
            "published": "2023-12-04T08:27:59Z",
            "summary": "Given a clothing image and a person image, an image-based virtual try-on aims\nto generate a customized image that appears natural and accurately reflects the\ncharacteristics of the clothing image. In this work, we aim to expand the\napplicability of the pre-trained diffusion model so that it can be utilized\nindependently for the virtual try-on task.The main challenge is to preserve the\nclothing details while effectively utilizing the robust generative capability\nof the pre-trained model. In order to tackle these issues, we propose\nStableVITON, learning the semantic correspondence between the clothing and the\nhuman body within the latent space of the pre-trained diffusion model in an\nend-to-end manner. Our proposed zero cross-attention blocks not only preserve\nthe clothing details by learning the semantic correspondence but also generate\nhigh-fidelity images by utilizing the inherent knowledge of the pre-trained\nmodel in the warping process. Through our proposed novel attention total\nvariation loss and applying augmentation, we achieve the sharp attention map,\nresulting in a more precise representation of clothing details. StableVITON\noutperforms the baselines in qualitative and quantitative evaluation, showing\npromising quality in arbitrary person images. Our code is available at\nhttps://github.com/rlawjdghek/StableVITON.",
            "author": [
                "Jeongho Kim",
                "Gyojung Gu",
                "Minho Park",
                "Sunghyun Park",
                "Jaegul Choo"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01725v1",
                "http://arxiv.org/pdf/2312.01725v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03004v1",
            "title": "Learning Multi-graph Structure for Temporal Knowledge Graph Reasoning",
            "updated": "2023-12-04T08:23:09Z",
            "published": "2023-12-04T08:23:09Z",
            "summary": "Temporal Knowledge Graph (TKG) reasoning that forecasts future events based\non historical snapshots distributed over timestamps is denoted as extrapolation\nand has gained significant attention. Owing to its extreme versatility and\nvariation in spatial and temporal correlations, TKG reasoning presents a\nchallenging task, demanding efficient capture of concurrent structures and\nevolutional interactions among facts. While existing methods have made strides\nin this direction, they still fall short of harnessing the diverse forms of\nintrinsic expressive semantics of TKGs, which encompass entity correlations\nacross multiple timestamps and periodicity of temporal information. This\nlimitation constrains their ability to thoroughly reflect historical\ndependencies and future trends. In response to these drawbacks, this paper\nproposes an innovative reasoning approach that focuses on Learning Multi-graph\nStructure (LMS). Concretely, it comprises three distinct modules concentrating\non multiple aspects of graph structure knowledge within TKGs, including\nconcurrent and evolutional patterns along timestamps, query-specific\ncorrelations across timestamps, and semantic dependencies of timestamps, which\ncapture TKG features from various perspectives. Besides, LMS incorporates an\nadaptive gate for merging entity representations both along and across\ntimestamps effectively. Moreover, it integrates timestamp semantics into graph\nattention calculations and time-aware decoders, in order to impose temporal\nconstraints on events and narrow down prediction scopes with historical\nstatistics. Extensive experimental results on five event-based benchmark\ndatasets demonstrate that LMS outperforms state-of-the-art extrapolation\nmodels, indicating the superiority of modeling a multi-graph perspective for\nTKG reasoning.",
            "author": [
                "Jinchuan Zhang",
                "Bei Hui",
                "Chong Mu",
                "Ling Tian"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03004v1",
                "http://arxiv.org/pdf/2312.03004v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01721v1",
            "title": "The Self-Loop Paradox: Investigating the Impact of Self-Loops on Graph\n  Neural Networks",
            "updated": "2023-12-04T08:23:00Z",
            "published": "2023-12-04T08:23:00Z",
            "summary": "Many Graph Neural Networks (GNNs) add self-loops to a graph to include\nfeature information about a node itself at each layer. However, if the GNN\nconsists of more than one layer, this information can return to its origin via\ncycles in the graph topology. Intuition suggests that this \"backflow\" of\ninformation should be larger in graphs with self-loops compared to graphs\nwithout. In this work, we counter this intuition and show that for certain GNN\narchitectures, the information a node gains from itself can be smaller in\ngraphs with self-loops compared to the same graphs without. We adopt an\nanalytical approach for the study of statistical graph ensembles with a given\ndegree sequence and show that this phenomenon, which we call the self-loop\nparadox, can depend both on the number of GNN layers $k$ and whether $k$ is\neven or odd. We experimentally validate our theoretical findings in a synthetic\nnode classification task and investigate its practical relevance in 23\nreal-world graphs.",
            "author": [
                "Moritz Lampert",
                "Ingo Scholtes"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01721v1",
                "http://arxiv.org/pdf/2312.01721v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02236v1",
            "title": "Rethinking Adversarial Training with Neural Tangent Kernel",
            "updated": "2023-12-04T08:06:59Z",
            "published": "2023-12-04T08:06:59Z",
            "summary": "Adversarial training (AT) is an important and attractive topic in deep\nlearning security, exhibiting mysteries and odd properties. Recent studies of\nneural network training dynamics based on Neural Tangent Kernel (NTK) make it\npossible to reacquaint AT and deeply analyze its properties. In this paper, we\nperform an in-depth investigation of AT process and properties with NTK, such\nas NTK evolution. We uncover three new findings that are missed in previous\nworks. First, we disclose the impact of data normalization on AT and the\nimportance of unbiased estimators in batch normalization layers. Second, we\nexperimentally explore the kernel dynamics and propose more time-saving AT\nmethods. Third, we study the spectrum feature inside the kernel to address the\ncatastrophic overfitting problem. To the best of our knowledge, it is the first\nwork leveraging the observations of kernel dynamics to improve existing AT\nmethods.",
            "author": [
                "Guanlin Li",
                "Han Qiu",
                "Shangwei Guo",
                "Jiwei Li",
                "Tianwei Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02236v1",
                "http://arxiv.org/pdf/2312.02236v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01713v1",
            "title": "Disentangled Interaction Representation for One-Stage Human-Object\n  Interaction Detection",
            "updated": "2023-12-04T08:02:59Z",
            "published": "2023-12-04T08:02:59Z",
            "summary": "Human-Object Interaction (HOI) detection is a core task for human-centric\nimage understanding. Recent one-stage methods adopt a transformer decoder to\ncollect image-wide cues that are useful for interaction prediction; however,\nthe interaction representations obtained using this method are entangled and\nlack interpretability. In contrast, traditional two-stage methods benefit\nsignificantly from their ability to compose interaction features in a\ndisentangled and explainable manner. In this paper, we improve the performance\nof one-stage methods by enabling them to extract disentangled interaction\nrepresentations. First, we propose Shunted Cross-Attention (SCA) to extract\nhuman appearance, object appearance, and global context features using\ndifferent cross-attention heads. This is achieved by imposing different masks\non the cross-attention maps produced by the different heads. Second, we\nintroduce the Interaction-aware Pose Estimation (IPE) task to learn\ninteraction-relevant human pose features using a disentangled decoder. This is\nachieved with a novel attention module that accurately captures the human\nkeypoints relevant to the current interaction category. Finally, our approach\nfuses the appearance feature and pose feature via element-wise addition to form\nthe interaction representation. Experimental results show that our approach can\nbe readily applied to existing one-stage HOI detectors. Moreover, we achieve\nstate-of-the-art performance on two benchmarks: HICO-DET and V-COCO.",
            "author": [
                "Xubin Zhong",
                "Changxing Ding",
                "Yupeng Hu",
                "Dacheng Tao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01713v1",
                "http://arxiv.org/pdf/2312.01713v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01711v1",
            "title": "Regressor-Segmenter Mutual Prompt Learning for Crowd Counting",
            "updated": "2023-12-04T07:53:59Z",
            "published": "2023-12-04T07:53:59Z",
            "summary": "Crowd counting has achieved significant progress by training regressors to\npredict instance positions. In heavily crowded scenarios, however, regressors\nare challenged by uncontrollable annotation variance, which causes density map\nbias and context information inaccuracy. In this study, we propose mutual\nprompt learning (mPrompt), which leverages a regressor and a segmenter as\nguidance for each other, solving bias and inaccuracy caused by annotation\nvariance while distinguishing foreground from background. In specific, mPrompt\nleverages point annotations to tune the segmenter and predict pseudo head masks\nin a way of point prompt learning. It then uses the predicted segmentation\nmasks, which serve as spatial constraint, to rectify biased point annotations\nas context prompt learning. mPrompt defines a way of mutual information\nmaximization from prompt learning, mitigating the impact of annotation variance\nwhile improving model accuracy. Experiments show that mPrompt significantly\nreduces the Mean Average Error (MAE), demonstrating the potential to be general\nframework for down-stream vision tasks.",
            "author": [
                "Mingyue Guo",
                "Li Yuan",
                "Zhaoyi Yan",
                "Binghui Chen",
                "Yaowei Wang",
                "Qixiang Ye"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01711v1",
                "http://arxiv.org/pdf/2312.01711v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02235v1",
            "title": "GenEM: Physics-Informed Generative Cryo-Electron Microscopy",
            "updated": "2023-12-04T07:52:56Z",
            "published": "2023-12-04T07:52:56Z",
            "summary": "In the past decade, deep conditional generative models have revolutionized\nthe generation of realistic images, extending their application from\nentertainment to scientific domains. Single-particle cryo-electron microscopy\n(cryo-EM) is crucial in resolving near-atomic resolution 3D structures of\nproteins, such as the SARS-COV-2 spike protein. To achieve high-resolution\nreconstruction, AI models for particle picking and pose estimation have been\nadopted. However, their performance is still limited as they lack high-quality\nannotated datasets. To address this, we introduce physics-informed generative\ncryo-electron microscopy (GenEM), which for the first time integrates\nphysical-based cryo-EM simulation with a generative unpaired noise translation\nto generate physically correct synthetic cryo-EM datasets with realistic\nnoises. Initially, GenEM simulates the cryo-EM imaging process based on a\nvirtual specimen. To generate realistic noises, we leverage an unpaired noise\ntranslation via contrastive learning with a novel mask-guided sampling scheme.\nExtensive experiments show that GenEM is capable of generating realistic\ncryo-EM images. The generated dataset can further enhance particle picking and\npose estimation models, eventually improving the reconstruction resolution. We\nwill release our code and annotated synthetic datasets.",
            "author": [
                "Jiakai Zhang",
                "Qihe Chen",
                "Yan Zeng",
                "Wenyuan Gao",
                "Xuming He",
                "Zhijie Liu",
                "Jingyi Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02235v1",
                "http://arxiv.org/pdf/2312.02235v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01699v1",
            "title": "Rethinking Urban Mobility Prediction: A Super-Multivariate Time Series\n  Forecasting Approach",
            "updated": "2023-12-04T07:39:05Z",
            "published": "2023-12-04T07:39:05Z",
            "summary": "Long-term urban mobility predictions play a crucial role in the effective\nmanagement of urban facilities and services. Conventionally, urban mobility\ndata has been structured as spatiotemporal videos, treating longitude and\nlatitude grids as fundamental pixels. Consequently, video prediction methods,\nrelying on Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs),\nhave been instrumental in this domain. In our research, we introduce a fresh\nperspective on urban mobility prediction. Instead of oversimplifying urban\nmobility data as traditional video data, we regard it as a complex multivariate\ntime series. This perspective involves treating the time-varying values of each\ngrid in each channel as individual time series, necessitating a thorough\nexamination of temporal dynamics, cross-variable correlations, and\nfrequency-domain insights for precise and reliable predictions. To address this\nchallenge, we present the Super-Multivariate Urban Mobility Transformer\n(SUMformer), which utilizes a specially designed attention mechanism to\ncalculate temporal and cross-variable correlations and reduce computational\ncosts stemming from a large number of time series. SUMformer also employs\nlow-frequency filters to extract essential information for long-term\npredictions. Furthermore, SUMformer is structured with a temporal patch merge\nmechanism, forming a hierarchical framework that enables the capture of\nmulti-scale correlations. Consequently, it excels in urban mobility pattern\nmodeling and long-term prediction, outperforming current state-of-the-art\nmethods across three real-world datasets.",
            "author": [
                "Jinguo Cheng",
                "Ke Li",
                "Yuxuan Liang",
                "Lijun Sun",
                "Junchi Yan",
                "Yuankai Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01699v1",
                "http://arxiv.org/pdf/2312.01699v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01692v1",
            "title": "Risk-Controlling Model Selection via Guided Bayesian Optimization",
            "updated": "2023-12-04T07:29:44Z",
            "published": "2023-12-04T07:29:44Z",
            "summary": "Adjustable hyperparameters of machine learning models typically impact\nvarious key trade-offs such as accuracy, fairness, robustness, or inference\ncost. Our goal in this paper is to find a configuration that adheres to\nuser-specified limits on certain risks while being useful with respect to other\nconflicting metrics. We solve this by combining Bayesian Optimization (BO) with\nrigorous risk-controlling procedures, where our core idea is to steer BO\ntowards an efficient testing strategy. Our BO method identifies a set of Pareto\noptimal configurations residing in a designated region of interest. The\nresulting candidates are statistically verified and the best-performing\nconfiguration is selected with guaranteed risk levels. We demonstrate the\neffectiveness of our approach on a range of tasks with multiple desiderata,\nincluding low error rates, equitable predictions, handling spurious\ncorrelations, managing rate and distortion in generative models, and reducing\ncomputational costs.",
            "author": [
                "Bracha Laufer-Goldshtein",
                "Adam Fisch",
                "Regina Barzilay",
                "Tommi Jaakkola"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01692v1",
                "http://arxiv.org/pdf/2312.01692v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "stat.ME",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01691v1",
            "title": "Estimating Coronal Mass Ejection Mass and Kinetic Energy by Fusion of\n  Multiple Deep-learning Models",
            "updated": "2023-12-04T07:25:55Z",
            "published": "2023-12-04T07:25:55Z",
            "summary": "Coronal mass ejections (CMEs) are massive solar eruptions, which have a\nsignificant impact on Earth. In this paper, we propose a new method, called\nDeepCME, to estimate two properties of CMEs, namely, CME mass and kinetic\nenergy. Being able to estimate these properties helps better understand CME\ndynamics. Our study is based on the CME catalog maintained at the Coordinated\nData Analysis Workshops (CDAW) Data Center, which contains all CMEs manually\nidentified since 1996 using the Large Angle and Spectrometric Coronagraph\n(LASCO) on board the Solar and Heliospheric Observatory (SOHO). We use LASCO C2\ndata in the period between January 1996 and December 2020 to train, validate\nand test DeepCME through 10-fold cross validation. The DeepCME method is a\nfusion of three deep learning models, including ResNet, InceptionNet, and\nInceptionResNet. Our fusion model extracts features from LASCO C2 images,\neffectively combining the learning capabilities of the three component models\nto jointly estimate the mass and kinetic energy of CMEs. Experimental results\nshow that the fusion model yields a mean relative error (MRE) of 0.013 (0.009,\nrespectively) compared to the MRE of 0.019 (0.017, respectively) of the best\ncomponent model InceptionResNet (InceptionNet, respectively) in estimating the\nCME mass (kinetic energy, respectively). To our knowledge, this is the first\ntime that deep learning has been used for CME mass and kinetic energy\nestimations.",
            "author": [
                "Khalid A. Alobaid",
                "Yasser Abduallah",
                "Jason T. L. Wang",
                "Haimin Wang",
                "Shen Fan",
                "Jialiang Li",
                "Huseyin Cavus",
                "Vasyl Yurchyshyn"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01691v1",
                "http://arxiv.org/pdf/2312.01691v1"
            ],
            "primary_category": "astro-ph.SR",
            "category": [
                "astro-ph.SR",
                "cs.LG",
                "physics.space-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01689v1",
            "title": "Fast and accurate sparse-view CBCT reconstruction using meta-learned\n  neural attenuation field and hash-encoding regularization",
            "updated": "2023-12-04T07:23:44Z",
            "published": "2023-12-04T07:23:44Z",
            "summary": "Cone beam computed tomography (CBCT) is an emerging medical imaging technique\nto visualize the internal anatomical structures of patients. During a CBCT\nscan, several projection images of different angles or views are collectively\nutilized to reconstruct a tomographic image. However, reducing the number of\nprojections in a CBCT scan while preserving the quality of a reconstructed\nimage is challenging due to the nature of an ill-posed inverse problem.\nRecently, a neural attenuation field (NAF) method was proposed by adopting a\nneural radiance field algorithm as a new way for CBCT reconstruction,\ndemonstrating fast and promising results using only 50 views. However,\ndecreasing the number of projections is still preferable to reduce potential\nradiation exposure, and a faster reconstruction time is required considering a\ntypical scan time. In this work, we propose a fast and accurate sparse-view\nCBCT reconstruction (FACT) method to provide better reconstruction quality and\nfaster optimization speed in the minimal number of view acquisitions ($<$ 50\nviews). In the FACT method, we meta-trained a neural network and a hash-encoder\nusing a few scans (= 15), and a new regularization technique is utilized to\nreconstruct the details of an anatomical structure. In conclusion, we have\nshown that the FACT method produced better, and faster reconstruction results\nover the other conventional algorithms based on CBCT scans of different body\nparts (chest, head, and abdomen) and CT vendors (Siemens, Phillips, and GE).",
            "author": [
                "Heejun Shin",
                "Taehee Kim",
                "Jongho Lee",
                "Seyoung Chun",
                "Seungryung Cho",
                "Dongmyung Shin"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01689v1",
                "http://arxiv.org/pdf/2312.01689v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01688v1",
            "title": "Tab-Attention: Self-Attention-based Stacked Generalization for\n  Imbalanced Credit Default Prediction",
            "updated": "2023-12-04T07:23:21Z",
            "published": "2023-12-04T07:23:21Z",
            "summary": "Accurately credit default prediction faces challenges due to imbalanced data\nand low correlation between features and labels. Existing default prediction\nstudies on the basis of gradient boosting decision trees (GBDT), deep learning\ntechniques, and feature selection strategies can have varying degrees of\nsuccess depending on the specific task. Motivated by this, we propose\nTab-Attention, a novel self-attention-based stacked generalization method for\ncredit default prediction. This approach ensembles the potential proprietary\nknowledge contributions from multi-view feature spaces, to cope with low\nfeature correlation and imbalance. We organize multi-view feature spaces\naccording to the latent linear or nonlinear strengths between features and\nlabels. Meanwhile, the f1 score assists the model in imbalance training to find\nthe optimal state for identifying minority default samples. Our Tab-Attention\nachieves superior Recall_1 and f1_1 of default intention recognition than\nexisting GBDT-based models and advanced deep learning by about 32.92% and\n16.05% on average, respectively, while maintaining outstanding overall\nperformance and prediction performance for non-default samples. The proposed\nmethod could ensemble essential knowledge through the self-attention mechanism,\nwhich is of great significance for a more robust future prediction system.",
            "author": [
                "Yandan Tan",
                "Hongbin Zhu",
                "JieWu",
                "Hongfeng Chai"
            ],
            "link": [
                "http://dx.doi.org/10.3233/FAIA230532",
                "http://arxiv.org/abs/2312.01688v1",
                "http://arxiv.org/pdf/2312.01688v1"
            ],
            "primary_category": "cs.CE",
            "category": [
                "cs.CE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01687v1",
            "title": "Optimizing Bus Travel: A Novel Approach to Feature Mining with P-KMEANS\n  and P-LDA Algorithms",
            "updated": "2023-12-04T07:21:27Z",
            "published": "2023-12-04T07:21:27Z",
            "summary": "Customizing services for bus travel can bolster its attractiveness, optimize\nusage, alleviate traffic congestion, and diminish carbon emissions. This\npotential is realized by harnessing recent advancements in positioning\ncommunication facilities, the Internet of Things, and artificial intelligence\nfor feature mining in public transportation. However, the inherent complexities\nof disorganized and unstructured public transportation data introduce\nsubstantial challenges to travel feature extraction. This study presents a bus\ntravel feature extraction method rooted in Point of Interest (POI) data,\nemploying enhanced P-KMENAS and P-LDA algorithms to overcome these limitations.\nWhile the KMEANS algorithm adeptly segments passenger travel paths into\ndistinct clusters, its outcomes can be influenced by the initial K value. On\nthe other hand, Latent Dirichlet Allocation (LDA) excels at feature\nidentification and probabilistic interpretations yet encounters difficulties\nwith feature intermingling and nuanced sub-feature interactions. Incorporating\nthe POI dimension enhances our understanding of travel behavior, aligning it\nmore closely with passenger attributes and facilitating easier data analysis.\nBy incorporating POI data, our refined P-KMENAS and P-LDA algorithms grant a\nholistic insight into travel behaviors and attributes, effectively mitigating\nthe limitations above. Consequently, this POI-centric algorithm effectively\namalgamates diverse POI attributes, delineates varied travel contexts, and\nimparts probabilistic metrics to feature properties. Our method successfully\nmines the diverse aspects of bus travel, such as age, occupation, gender,\nsports, cost, safety, and personality traits. It effectively calculates\nrelationships between individual travel behaviors and assigns explanatory and\nevaluative probabilities to POI labels, thereby enhancing bus travel\noptimization.",
            "author": [
                "Hongjie Liu",
                "Haotian Shi",
                "Sicheng Fu",
                "Tengfei Yuan",
                "Xinhuan Zhang",
                "Hongzhe Xu",
                "Bin Ran"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01687v1",
                "http://arxiv.org/pdf/2312.01687v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01682v1",
            "title": "ResEnsemble-DDPM: Residual Denoising Diffusion Probabilistic Models for\n  Ensemble Learning",
            "updated": "2023-12-04T07:14:20Z",
            "published": "2023-12-04T07:14:20Z",
            "summary": "Nowadays, denoising diffusion probabilistic models have been adapted for many\nimage segmentation tasks. However, existing end-to-end models have already\ndemonstrated remarkable capabilities. Rather than using denoising diffusion\nprobabilistic models alone, integrating the abilities of both denoising\ndiffusion probabilistic models and existing end-to-end models can better\nimprove the performance of image segmentation. Based on this, we implicitly\nintroduce residual term into the diffusion process and propose\nResEnsemble-DDPM, which seamlessly integrates the diffusion model and the\nend-to-end model through ensemble learning. The output distributions of these\ntwo models are strictly symmetric with respect to the ground truth\ndistribution, allowing us to integrate the two models by reducing the residual\nterm. Experimental results demonstrate that our ResEnsemble-DDPM can further\nimprove the capabilities of existing models. Furthermore, its ensemble learning\nstrategy can be generalized to other downstream tasks in image generation and\nget strong competitiveness.",
            "author": [
                "Shi Zhenning",
                "Dong Changsheng",
                "Xie Xueshuo",
                "Pan Bin",
                "He Along",
                "Li Tao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01682v1",
                "http://arxiv.org/pdf/2312.01682v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01680v1",
            "title": "With Great Humor Comes Great Developer Engagement",
            "updated": "2023-12-04T07:06:02Z",
            "published": "2023-12-04T07:06:02Z",
            "summary": "The worldwide collaborative effort for the creation of software is\ntechnically and socially demanding. The more engaged developers are, the more\nvalue they impart to the software they create. Engaged developers, such as\nMargaret Hamilton programming Apollo 11, can succeed in tackling the most\ndifficult engineering tasks. In this paper, we dive deep into an original\nvector of engagement - humor - and study how it fuels developer engagement.\nFirst, we collect qualitative and quantitative data about the humorous elements\npresent within three significant, real-world software projects: faker, which\nhelps developers introduce humor within their tests; lolcommits, which captures\na photograph after each contribution made by a developer; and volkswagen, an\nexercise in satire, which accidentally led to the invention of an impactful\nsoftware tool. Second, through a developer survey, we receive unique insights\nfrom 125 developers, who share their real-life experiences with humor in\nsoftware. Our analysis of the three case studies highlights the prevalence of\nhumor in software, and unveils the worldwide community of developers who are\nenthusiastic about both software and humor. We also learn about the caveats of\nhumor in software through the valuable insights shared by our survey\nrespondents. We report clear evidence that, when practiced responsibly, humor\nincreases developer engagement and supports them in addressing hard engineering\nand cognitive tasks. The most actionable highlight of our work is that software\ntests and documentation are the best locations in code to practice humor.",
            "author": [
                "Deepika Tiwari",
                "Tim Toady",
                "Martin Monperrus",
                "Benoit Baudry"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01680v1",
                "http://arxiv.org/pdf/2312.01680v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01679v1",
            "title": "Adversarial Medical Image with Hierarchical Feature Hiding",
            "updated": "2023-12-04T07:04:20Z",
            "published": "2023-12-04T07:04:20Z",
            "summary": "Deep learning based methods for medical images can be easily compromised by\nadversarial examples (AEs), posing a great security flaw in clinical\ndecision-making. It has been discovered that conventional adversarial attacks\nlike PGD which optimize the classification logits, are easy to distinguish in\nthe feature space, resulting in accurate reactive defenses. To better\nunderstand this phenomenon and reassess the reliability of the reactive\ndefenses for medical AEs, we thoroughly investigate the characteristic of\nconventional medical AEs. Specifically, we first theoretically prove that\nconventional adversarial attacks change the outputs by continuously optimizing\nvulnerable features in a fixed direction, thereby leading to outlier\nrepresentations in the feature space. Then, a stress test is conducted to\nreveal the vulnerability of medical images, by comparing with natural images.\nInterestingly, this vulnerability is a double-edged sword, which can be\nexploited to hide AEs. We then propose a simple-yet-effective hierarchical\nfeature constraint (HFC), a novel add-on to conventional white-box attacks,\nwhich assists to hide the adversarial feature in the target feature\ndistribution. The proposed method is evaluated on three medical datasets, both\n2D and 3D, with different modalities. The experimental results demonstrate the\nsuperiority of HFC, \\emph{i.e.,} it bypasses an array of state-of-the-art\nadversarial medical AE detectors more efficiently than competing adaptive\nattacks, which reveals the deficiencies of medical reactive defense and allows\nto develop more robust defenses in future.",
            "author": [
                "Qingsong Yao",
                "Zecheng He",
                "Yuexiang Li",
                "Yi Lin",
                "Kai Ma",
                "Yefeng Zheng",
                "S. Kevin Zhou"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01679v1",
                "http://arxiv.org/pdf/2312.01679v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01678v2",
            "title": "Jellyfish: A Large Language Model for Data Preprocessing",
            "updated": "2023-12-05T18:02:46Z",
            "published": "2023-12-04T07:01:54Z",
            "summary": "In this paper, we present Jellyfish, an open-source LLM as a universal task\nsolver for DP. Built on the Llama 2 13B model, Jellyfish is instruction-tuned\nwith the datasets of several typical DP tasks including error detection, data\nimputation, schema matching, and entity matching, and delivers generalizability\nto other tasks. Remarkably, Jellyfish can operate on a local, single, and\nlow-priced GPU with its 13 billion parameters, ensuring data security and\nenabling further tuning. Its proficiency in understanding natural language\nallows users to manually craft instructions for DP tasks. Unlike many existing\nmethods that heavily rely on prior knowledge, Jellyfish acquires domain\nknowledge during its tuning process and integrates optional knowledge injection\nduring inference. A distinctive feature of Jellyfish is its interpreter, which\nelucidates its output decisions. To construct Jellyfish, we develop a series of\npre-tuning and DP-tuning techniques. Jellyfish is equipped with an instance\nserializer, which automatically translates raw data into model prompts, and a\nknowledge injector, which optionally introduces task- and dataset-specific\nknowledge to enhance DP performance. Our evaluation of Jellyfish, using a range\nof real datasets, shows its competitiveness compared to state-of-the-art\nmethods and its strong generalizability to unseen tasks. Jellyfish's\nperformance rivals that of GPT series models, and its interpreter offers\nenhanced reasoning capabilities compared to GPT-3.5. Furthermore, our\nevaluation highlights the effectiveness of the techniques employed in\nconstructing Jellyfish. Our model is available at Hugging Face:\nhttps://huggingface.co/NECOUDBFM/Jellyfish .",
            "author": [
                "Haochen Zhang",
                "Yuyang Dong",
                "Chuan Xiao",
                "Masafumi Oyamada"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01678v2",
                "http://arxiv.org/pdf/2312.01678v2"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL",
                "cs.DB",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01677v2",
            "title": "Multi-task Image Restoration Guided By Robust DINO Features",
            "updated": "2023-12-05T17:46:12Z",
            "published": "2023-12-04T06:59:55Z",
            "summary": "Multi-task image restoration has gained significant interest due to its\ninherent versatility and efficiency compared to its single-task counterpart.\nDespite its potential, performance degradation is observed with an increase in\nthe number of tasks, primarily attributed to the distinct nature of each\nrestoration task. Addressing this challenge, we introduce\n\\mbox{\\textbf{DINO-IR}}, a novel multi-task image restoration approach\nleveraging robust features extracted from DINOv2. Our empirical analysis shows\nthat while shallow features of DINOv2 capture rich low-level image\ncharacteristics, the deep features ensure a robust semantic representation\ninsensitive to degradations while preserving high-frequency contour details.\nBuilding on these features, we devise specialized components, including\nmulti-layer semantic fusion module, DINO-Restore adaption and fusion module,\nand DINO perception contrastive loss, to integrate DINOv2 features into the\nrestoration paradigm. Equipped with the aforementioned components, our DINO-IR\nperforms favorably against existing multi-task image restoration approaches in\nvarious tasks by a large margin, indicating the superiority and necessity of\nreinforcing the robust features for multi-task image restoration.",
            "author": [
                "Xin Lin",
                "Chao Ren",
                "Kelvin C. K. Chan",
                "Lu Qi",
                "Jinshan Pan",
                "Ming-Hsuan Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01677v2",
                "http://arxiv.org/pdf/2312.01677v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01674v1",
            "title": "EDALearn: A Comprehensive RTL-to-Signoff EDA Benchmark for Democratized\n  and Reproducible ML for EDA Research",
            "updated": "2023-12-04T06:51:46Z",
            "published": "2023-12-04T06:51:46Z",
            "summary": "The application of Machine Learning (ML) in Electronic Design Automation\n(EDA) for Very Large-Scale Integration (VLSI) design has garnered significant\nresearch attention. Despite the requirement for extensive datasets to build\neffective ML models, most studies are limited to smaller, internally generated\ndatasets due to the lack of comprehensive public resources. In response, we\nintroduce EDALearn, the first holistic, open-source benchmark suite\nspecifically for ML tasks in EDA. This benchmark suite presents an end-to-end\nflow from synthesis to physical implementation, enriching data collection\nacross various stages. It fosters reproducibility and promotes research into ML\ntransferability across different technology nodes. Accommodating a wide range\nof VLSI design instances and sizes, our benchmark aptly represents the\ncomplexity of contemporary VLSI designs. Additionally, we provide an in-depth\ndata analysis, enabling users to fully comprehend the attributes and\ndistribution of our data, which is essential for creating efficient ML models.\nOur contributions aim to encourage further advances in the ML-EDA domain.",
            "author": [
                "Jingyu Pan",
                "Chen-Chia Chang",
                "Zhiyao Xie",
                "Yiran Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01674v1",
                "http://arxiv.org/pdf/2312.01674v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01672v1",
            "title": "STADEE: STAtistics-based DEEp Detection of Machine Generated Text",
            "updated": "2023-12-04T06:45:47Z",
            "published": "2023-12-04T06:45:47Z",
            "summary": "We present STADEE, a \\textbf{STA}tistics-based \\textbf{DEE}p detection method\nto identify machine-generated text, addressing the limitations of current\nmethods that rely heavily on fine-tuning pre-trained language models (PLMs).\nSTADEE integrates key statistical text features with a deep classifier,\nfocusing on aspects like token probability and cumulative probability, crucial\nfor handling nucleus sampling. Tested across diverse datasets and scenarios\n(in-domain, out-of-domain, and in-the-wild), STADEE demonstrates superior\nperformance, achieving an 87.05% F1 score in-domain and outperforming both\ntraditional statistical methods and fine-tuned PLMs, especially in\nout-of-domain and in-the-wild settings, highlighting its effectiveness and\ngeneralizability.",
            "author": [
                "Zheng Chen",
                "Huming Liu"
            ],
            "link": [
                "http://dx.doi.org/10.1007/978-981-99-4752-2_60",
                "http://arxiv.org/abs/2312.01672v1",
                "http://arxiv.org/pdf/2312.01672v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01662v1",
            "title": "Universal Deoxidation of Semiconductor Substrates Assisted by\n  Machine-Learning and Real-Time-Feedback-Control",
            "updated": "2023-12-04T06:24:49Z",
            "published": "2023-12-04T06:24:49Z",
            "summary": "Thin film deposition is an essential step in the semiconductor process.\nDuring preparation or loading, the substrate is exposed to the air unavoidably,\nwhich has motivated studies of the process control to remove the surface oxide\nbefore thin film deposition. Optimizing the deoxidation process in molecular\nbeam epitaxy (MBE) for a random substrate is a multidimensional challenge and\nsometimes controversial. Due to variations in semiconductor materials and\ngrowth processes, the determination of substrate deoxidation temperature is\nhighly dependent on the grower's expertise; the same substrate may yield\ninconsistent results when evaluated by different growers. Here, we employ a\nmachine learning (ML) hybrid convolution and vision transformer (CNN-ViT)\nmodel. This model utilizes reflection high-energy electron diffraction (RHEED)\nvideo as input to determine the deoxidation status of the substrate as output,\nenabling automated substrate deoxidation under a controlled architecture. This\nalso extends to the successful application of deoxidation processes on other\nsubstrates. Furthermore, we showcase the potential of models trained on data\nfrom a single MBE equipment to achieve high-accuracy deployment on other\nequipment. In contrast to traditional methods, our approach holds exceptional\npractical value. It standardizes deoxidation temperatures across various\nequipment and substrate materials, advancing the standardization research\nprocess in semiconductor preparation, a significant milestone in thin film\ngrowth technology. The concepts and methods demonstrated in this work are\nanticipated to revolutionize semiconductor manufacturing in optoelectronics and\nmicroelectronics industries by applying them to diverse material growth\nprocesses.",
            "author": [
                "Chao Shen",
                "Wenkang Zhan",
                "Jian Tang",
                "Zhaofeng Wu",
                "Bo Xu",
                "Chao Zhao",
                "Zhanguo Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01662v1",
                "http://arxiv.org/pdf/2312.01662v1"
            ],
            "primary_category": "cond-mat.mes-hall",
            "category": [
                "cond-mat.mes-hall",
                "cs.LG",
                "cs.SY",
                "eess.IV",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01659v1",
            "title": "RiskBench: A Scenario-based Benchmark for Risk Identification",
            "updated": "2023-12-04T06:21:22Z",
            "published": "2023-12-04T06:21:22Z",
            "summary": "Intelligent driving systems aim to achieve a zero-collision mobility\nexperience, requiring interdisciplinary efforts to enhance safety performance.\nThis work focuses on risk identification, the process of identifying and\nanalyzing risks stemming from dynamic traffic participants and unexpected\nevents. While significant advances have been made in the community, the current\nevaluation of different risk identification algorithms uses independent\ndatasets, leading to difficulty in direct comparison and hindering collective\nprogress toward safety performance enhancement. To address this limitation, we\nintroduce \\textbf{RiskBench}, a large-scale scenario-based benchmark for risk\nidentification. We design a scenario taxonomy and augmentation pipeline to\nenable a systematic collection of ground truth risks under diverse scenarios.\nWe assess the ability of ten algorithms to (1) detect and locate risks, (2)\nanticipate risks, and (3) facilitate decision-making. We conduct extensive\nexperiments and summarize future research on risk identification. Our aim is to\nencourage collaborative endeavors in achieving a society with zero collisions.\nWe have made our dataset and benchmark toolkit publicly on the project page:\nhttps://hcis-lab.github.io/RiskBench/",
            "author": [
                "Chi-Hsi Kung",
                "Chieh-Chi Yang",
                "Pang-Yuan Pao",
                "Shu-Wei Lu",
                "Pin-Lun Chen",
                "Hsin-Cheng Lu",
                "Yi-Ting Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01659v1",
                "http://arxiv.org/pdf/2312.01659v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01658v1",
            "title": "AGD: an Auto-switchable Optimizer using Stepwise Gradient Difference for\n  Preconditioning Matrix",
            "updated": "2023-12-04T06:20:14Z",
            "published": "2023-12-04T06:20:14Z",
            "summary": "Adaptive optimizers, such as Adam, have achieved remarkable success in deep\nlearning. A key component of these optimizers is the so-called preconditioning\nmatrix, providing enhanced gradient information and regulating the step size of\neach gradient direction. In this paper, we propose a novel approach to\ndesigning the preconditioning matrix by utilizing the gradient difference\nbetween two successive steps as the diagonal elements. These diagonal elements\nare closely related to the Hessian and can be perceived as an approximation of\nthe inner product between the Hessian row vectors and difference of the\nadjacent parameter vectors. Additionally, we introduce an auto-switching\nfunction that enables the preconditioning matrix to switch dynamically between\nStochastic Gradient Descent (SGD) and the adaptive optimizer. Based on these\ntwo techniques, we develop a new optimizer named AGD that enhances the\ngeneralization performance. We evaluate AGD on public datasets of Natural\nLanguage Processing (NLP), Computer Vision (CV), and Recommendation Systems\n(RecSys). Our experimental results demonstrate that AGD outperforms the\nstate-of-the-art (SOTA) optimizers, achieving highly competitive or\nsignificantly better predictive performance. Furthermore, we analyze how AGD is\nable to switch automatically between SGD and the adaptive optimizer and its\nactual effects on various scenarios. The code is available at\nhttps://github.com/intelligent-machine-learning/dlrover/tree/master/atorch/atorch/optimizers.",
            "author": [
                "Yun Yue",
                "Zhiling Ye",
                "Jiadi Jiang",
                "Yongchao Liu",
                "Ke Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01658v1",
                "http://arxiv.org/pdf/2312.01658v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.DC",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01657v1",
            "title": "On Tuning Neural ODE for Stability, Consistency and Faster Convergence",
            "updated": "2023-12-04T06:18:10Z",
            "published": "2023-12-04T06:18:10Z",
            "summary": "Neural-ODE parameterize a differential equation using continuous depth neural\nnetwork and solve it using numerical ODE-integrator. These models offer a\nconstant memory cost compared to models with discrete sequence of hidden layers\nin which memory cost increases linearly with the number of layers. In addition\nto memory efficiency, other benefits of neural-ode include adaptability of\nevaluation approach to input, and flexibility to choose numerical precision or\nfast training. However, despite having all these benefits, it still has some\nlimitations. We identify the ODE-integrator (also called ODE-solver) as the\nweakest link in the chain as it may have stability, consistency and convergence\n(CCS) issues and may suffer from slower convergence or may not converge at all.\nWe propose a first-order Nesterov's accelerated gradient (NAG) based ODE-solver\nwhich is proven to be tuned vis-a-vis CCS conditions. We empirically\ndemonstrate the efficacy of our approach by training faster, while achieving\nbetter or comparable performance against neural-ode employing other fixed-step\nexplicit ODE-solvers as well discrete depth models such as ResNet in three\ndifferent tasks including supervised classification, density estimation, and\ntime-series modelling.",
            "author": [
                "Sheikh Waqas Akhtar"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01657v1",
                "http://arxiv.org/pdf/2312.01657v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01655v1",
            "title": "Quantum Polar Metric Learning: Efficient Classically Learned Quantum\n  Embeddings",
            "updated": "2023-12-04T06:13:53Z",
            "published": "2023-12-04T06:13:53Z",
            "summary": "Deep metric learning has recently shown extremely promising results in the\nclassical data domain, creating well-separated feature spaces. This idea was\nalso adapted to quantum computers via Quantum Metric Learning(QMeL). QMeL\nconsists of a 2 step process with a classical model to compress the data to fit\ninto the limited number of qubits, then train a Parameterized Quantum\nCircuit(PQC) to create better separation in Hilbert Space. However, on Noisy\nIntermediate Scale Quantum (NISQ) devices. QMeL solutions result in high\ncircuit width and depth, both of which limit scalability. We propose Quantum\nPolar Metric Learning (QPMeL) that uses a classical model to learn the\nparameters of the polar form of a qubit. We then utilize a shallow PQC with\n$R_y$ and $R_z$ gates to create the state and a trainable layer of\n$ZZ(\\theta)$-gates to learn entanglement. The circuit also computes fidelity\nvia a SWAP Test for our proposed Fidelity Triplet Loss function, used to train\nboth classical and quantum components. When compared to QMeL approaches, QPMeL\nachieves 3X better multi-class separation, while using only 1/2 the number of\ngates and depth. We also demonstrate that QPMeL outperforms classical networks\nwith similar configurations, presenting a promising avenue for future research\non fully classical models with quantum loss functions.",
            "author": [
                "Vinayak Sharma",
                "Aviral Shrivastava"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01655v1",
                "http://arxiv.org/pdf/2312.01655v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.AI",
                "I.2.6; E.4"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03003v1",
            "title": "Explore, Select, Derive, and Recall: Augmenting LLM with Human-like\n  Memory for Mobile Task Automation",
            "updated": "2023-12-04T06:13:35Z",
            "published": "2023-12-04T06:13:35Z",
            "summary": "The advent of large language models (LLMs) has opened up new opportunities in\nthe field of mobile task automation. Their superior language understanding and\nreasoning capabilities allow users to automate complex and repetitive tasks.\nHowever, due to the inherent unreliability and high operational cost of LLMs,\ntheir practical applicability is quite limited. To address these issues, this\npaper introduces MemoDroid, an innovative LLM-based mobile task automator\nenhanced with a unique app memory. MemoDroid emulates the cognitive process of\nhumans interacting with a mobile app -- explore, select, derive, and recall.\nThis approach allows for a more precise and efficient learning of a task's\nprocedure by breaking it down into smaller, modular components that can be\nre-used, re-arranged, and adapted for various objectives. We implement\nMemoDroid using online LLMs services (GPT-3.5 and GPT-4) and evaluate its\nperformance on 50 unique mobile tasks across 5 widely used mobile apps. The\nresults indicate that MemoDroid can adapt learned tasks to varying contexts\nwith 100% accuracy and reduces their latency and cost by 69.22% and 77.36%\ncompared to a GPT-4 powered baseline.",
            "author": [
                "Sunjae Lee",
                "Junyoung Choi",
                "Jungjae Lee",
                "Hojun Choi",
                "Steven Y. Ko",
                "Sangeun Oh",
                "Insik Shin"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03003v1",
                "http://arxiv.org/pdf/2312.03003v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC",
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01653v1",
            "title": "An End-to-End Network Pruning Pipeline with Sparsity Enforcement",
            "updated": "2023-12-04T06:11:39Z",
            "published": "2023-12-04T06:11:39Z",
            "summary": "Neural networks have emerged as a powerful tool for solving complex tasks\nacross various domains, but their increasing size and computational\nrequirements have posed significant challenges in deploying them on\nresource-constrained devices. Neural network sparsification, and in particular\npruning, has emerged as an effective technique to alleviate these challenges by\nreducing model size, computational complexity, and memory footprint while\nmaintaining competitive performance. However, many pruning pipelines modify the\nstandard training pipeline at only a single stage, if at all. In this work, we\nlook to develop an end-to-end training pipeline that befits neural network\npruning and sparsification at all stages of training. To do so, we make use of\nnonstandard model parameter initialization, pre-pruning training methodologies,\nand post-pruning training optimizations. We conduct experiments utilizing\ncombinations of these methods, in addition to different techniques used in the\npruning step, and find that our combined pipeline can achieve significant gains\nover current state of the art approaches to neural network sparsification.",
            "author": [
                "Evan Dogariu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01653v1",
                "http://arxiv.org/pdf/2312.01653v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01652v1",
            "title": "On the Expressive Power of Behavior Structure",
            "updated": "2023-12-04T06:11:21Z",
            "published": "2023-12-04T06:11:21Z",
            "summary": "Efforts toward a comprehensive description of behavior have indeed\nfacilitated the development of representation-based approaches that utilize\ndeep learning to capture behavioral information. As behavior complexity\nincreases, the expressive power of these models reaches a bottleneck. We coin\nthe term ``behavioral molecular structure\" and propose a new model called the\nBehavioral Molecular Structure (BMS). The model characterizes behaviors at the\natomic level, analogizes behavioral attributes to atoms, and concretizes\ninterrelations at the granularity of atoms using graphs. Here, we design three\ndifferent downstream tasks to test the performance of the BMS model on public\ndatasets. Additionally, we provide a preliminary theoretical analysis\ndemonstrating that the BMS model can offer effective expressiveness for complex\nbehaviors.",
            "author": [
                "Cheng Wang",
                "Hangyu Zhu",
                "Yuhang Lin",
                "Changjun Jiang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01652v1",
                "http://arxiv.org/pdf/2312.01652v1"
            ],
            "primary_category": "cs.CE",
            "category": [
                "cs.CE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01649v1",
            "title": "A simple stacked ensemble machine learning model to predict naturalized\n  catchment hydrology and allocation status",
            "updated": "2023-12-04T06:01:43Z",
            "published": "2023-12-04T06:01:43Z",
            "summary": "New Zealand legislation requires that Regional Councils set limits for water\nresource usage to manage the effects of abstractions in over-allocated\ncatchments. We propose a simple stacked ensemble machine learning model to\npredict the probable naturalized hydrology and allocation status across 317\nanthropogenically stressed gauged catchments and across 18,612 ungauged river\nreaches in Otago. The training and testing of ensemble machine learning models\nprovides unbiased results characterized as very good (R2 > 0.8) to extremely\ngood (R2 > 0.9) when predicting naturalized mean annual low flow and Mean flow.\nStatistical 5-fold stacking identifies varying levels of risk for managing\nwater-resource sustainability in over-allocated catchments; for example, at the\nrespective 5th, 25th, 50th, 75th, and 95th percentiles the number of\noverallocated catchments are 73, 57, 44, 23, and 22. The proposed model can be\napplied to inform sustainable stream management in other regional catchments\nacross New Zealand and worldwide.",
            "author": [
                "Michael J. Friedel",
                "Dave Stewart",
                "Xiao Feng Lu",
                "Pete Stevenson",
                "Helen Manly",
                "Tom Dyer"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01649v1",
                "http://arxiv.org/pdf/2312.01649v1"
            ],
            "primary_category": "physics.geo-ph",
            "category": [
                "physics.geo-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01648v1",
            "title": "Characterizing Large Language Model Geometry Solves Toxicity Detection\n  and Generation",
            "updated": "2023-12-04T06:01:32Z",
            "published": "2023-12-04T06:01:32Z",
            "summary": "Large Language Models~(LLMs) drive current AI breakthroughs despite very\nlittle being known about their internal representations, e.g., how to extract a\nfew informative features to solve various downstream tasks. To provide a\npractical and principled answer, we propose to characterize LLMs from a\ngeometric perspective. We obtain in closed form (i) the intrinsic dimension in\nwhich the Multi-Head Attention embeddings are constrained to exist and (ii) the\npartition and per-region affine mappings of the per-layer feedforward networks.\nOur results are informative, do not rely on approximations, and are actionable.\nFirst, we show that, motivated by our geometric interpretation, we can bypass\nLlama$2$'s RLHF by controlling its embedding's intrinsic dimension through\ninformed prompt manipulation. Second, we derive $7$ interpretable spline\nfeatures that can be extracted from any (pre-trained) LLM layer, providing a\nrich abstract representation of their inputs. Those features alone ($224$ for\nMistral-7B and Llama$2$-7B) are sufficient to help solve toxicity detection,\ninfer the domain of the prompt, and even tackle the Jigsaw challenge, which\naims at characterizing the type of toxicity of various prompts. Our results\ndemonstrate how, even in large-scale regimes, exact theoretical results can\nanswer practical questions in language models. Code:\n\\url{https://github.com/RandallBalestriero/SplineLLM}.",
            "author": [
                "Randall Balestriero",
                "Romain Cosentino",
                "Sarath Shekkizhar"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01648v1",
                "http://arxiv.org/pdf/2312.01648v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01644v1",
            "title": "TMSR: Tiny Multi-path CNNs for Super Resolution",
            "updated": "2023-12-04T05:52:49Z",
            "published": "2023-12-04T05:52:49Z",
            "summary": "In this paper, we proposed a tiny multi-path CNN-based Super-Resolution (SR)\nmethod, called TMSR. We mainly refer to some tiny CNN-based SR methods, under\n5k parameters. The main contribution of the proposed method is the improved\nmulti-path learning and self-defined activated function. The experimental\nresults show that TMSR obtains competitive image quality (i.e. PSNR and SSIM)\ncompared to the related works under 5k parameters.",
            "author": [
                "Chia-Hung Liu",
                "Tzu-Hsin Hsieh",
                "Kuan-Yu Huang",
                "Pei-Yin Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01644v1",
                "http://arxiv.org/pdf/2312.01644v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01640v1",
            "title": "SequencePAR: Understanding Pedestrian Attributes via A Sequence\n  Generation Paradigm",
            "updated": "2023-12-04T05:42:56Z",
            "published": "2023-12-04T05:42:56Z",
            "summary": "Current pedestrian attribute recognition (PAR) algorithms are developed based\non multi-label or multi-task learning frameworks, which aim to discriminate the\nattributes using specific classification heads. However, these discriminative\nmodels are easily influenced by imbalanced data or noisy samples. Inspired by\nthe success of generative models, we rethink the pedestrian attribute\nrecognition scheme and believe the generative models may perform better on\nmodeling dependencies and complexity between human attributes. In this paper,\nwe propose a novel sequence generation paradigm for pedestrian attribute\nrecognition, termed SequencePAR. It extracts the pedestrian features using a\npre-trained CLIP model and embeds the attribute set into query tokens under the\nguidance of text prompts. Then, a Transformer decoder is proposed to generate\nthe human attributes by incorporating the visual features and attribute query\ntokens. The masked multi-head attention layer is introduced into the decoder\nmodule to prevent the model from remembering the next attribute while making\nattribute predictions during training. Extensive experiments on multiple widely\nused pedestrian attribute recognition datasets fully validated the\neffectiveness of our proposed SequencePAR. The source code and pre-trained\nmodels will be released at https://github.com/Event-AHU/OpenPAR.",
            "author": [
                "Jiandong Jin",
                "Xiao Wang",
                "Chenglong Li",
                "Lili Huang",
                "Jin Tang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01640v1",
                "http://arxiv.org/pdf/2312.01640v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.MM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01638v1",
            "title": "J-Net: Improved U-Net for Terahertz Image Super-Resolution",
            "updated": "2023-12-04T05:39:51Z",
            "published": "2023-12-04T05:39:51Z",
            "summary": "Terahertz (THz) waves are electromagnetic waves in the 0.1 to 10 THz\nfrequency range, and THz imaging is utilized in a range of applications,\nincluding security inspections, biomedical fields, and the non-destructive\nexamination of materials. However, THz images have low resolution due to the\nlong wavelength of THz waves. Therefore, improving the resolution of THz images\nis one of the current hot research topics. We propose a novel network\narchitecture called J-Net which is improved version of U-Net to solve the THz\nimage super-resolution. It employs the simple baseline blocks which can extract\nlow resolution (LR) image features and learn the mapping of LR images to\nhighresolution (HR) images efficiently. All training was conducted using the\nDIV2K+Flickr2K dataset, and we employed the peak signal-to-noise ratio (PSNR)\nfor quantitative comparison. In our comparisons with other THz image\nsuper-resolution methods, JNet achieved a PSNR of 32.52 dB, surpassing other\ntechniques by more than 1 dB. J-Net also demonstrates superior performance on\nreal THz images compared to other methods. Experiments show that the proposed\nJ-Net achieves better PSNR and visual improvement compared with other THz image\nsuper-resolution methods.",
            "author": [
                "Woon-Ha Yeo",
                "Seung-Hwan Jung",
                "Seung Jae Oh",
                "Inhee Maeng",
                "Eui Su Lee",
                "Han-Cheol Ryu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01638v1",
                "http://arxiv.org/pdf/2312.01638v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01637v1",
            "title": "Near-real-time monitoring of global ocean carbon sink",
            "updated": "2023-12-04T05:37:29Z",
            "published": "2023-12-04T05:37:29Z",
            "summary": "Mitigation of climate change will highly rely on a carbon emission trajectory\nthat achieves carbon neutrality by the 2050s. The ocean plays a critical role\nin modulating climate change by sequestering CO2 from the atmosphere. Relying\non the multidisciplinary cutting-edge methodologies and technologies, the\nnear-real-time monitoring of global ocean carbon sinks from January 2022 to\nJuly 2023 aims to provide the world's latest assessment of monthly and gridded\nglobal ocean carbon sinks based on machine learning and other data science\ntechnologies. The project will help us find a robust route to deal with climate\nchange, which will significantly promote the ocean carbon sinks research and\nwill be of great interest for policy makers, researchers, and the public. This\nresearch aims to build up an integrated machine learning framework and\nmethodology for assessing global ocean carbon neutral process; development of\nnear-real-time dataset; development of visualization platform; research papers\npublished in international prestigious journals; an executive report openly\naccessible to policy makers and the public.",
            "author": [
                "Piyu Ke",
                "Xiaofan Gui",
                "Wei Cao",
                "Dezhi Wang",
                "Ce Hou",
                "Lixing Wang",
                "Xuanren Song",
                "Yun Li",
                "Biqing Zhu",
                "Jiang Bian",
                "Stephen Sitch",
                "Philippe Ciais",
                "Pierre Friedlingstein",
                "Zhu Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01637v1",
                "http://arxiv.org/pdf/2312.01637v1"
            ],
            "primary_category": "physics.ao-ph",
            "category": [
                "physics.ao-ph",
                "physics.geo-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01634v1",
            "title": "Robust Streaming, Sampling, and a Perspective on Online Learning",
            "updated": "2023-12-04T05:29:28Z",
            "published": "2023-12-04T05:29:28Z",
            "summary": "In this work we present an overview of statistical learning, followed by a\nsurvey of robust streaming techniques and challenges, culminating in several\nrigorous results proving the relationship that we motivate and hint at\nthroughout the journey. Furthermore, we unify often disjoint theorems in a\nshared framework and notation to clarify the deep connections that are\ndiscovered. We hope that by approaching these results from a shared\nperspective, already aware of the technical connections that exist, we can\nenlighten the study of both fields and perhaps motivate new and previously\nunconsidered directions of research.",
            "author": [
                "Evan Dogariu",
                "Jiatong Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01634v1",
                "http://arxiv.org/pdf/2312.01634v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.GT",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02231v1",
            "title": "Quality Diversity in the Amorphous Fortress (QD-AF): Evolving for\n  Complexity in 0-Player Games",
            "updated": "2023-12-04T05:16:53Z",
            "published": "2023-12-04T05:16:53Z",
            "summary": "We explore the generation of diverse environments using the Amorphous\nFortress (AF) simulation framework. AF defines a set of Finite State Machine\n(FSM) nodes and edges that can be recombined to control the behavior of agents\nin the `fortress' grid-world. The behaviors and conditions of the agents within\nthe framework are designed to capture the common building blocks of multi-agent\nartificial life and reinforcement learning environments. Using quality\ndiversity evolutionary search, we generate diverse sets of environments. These\nenvironments exhibit certain types of complexity according to measures of\nagents' FSM architectures and activations, and collective behaviors. Our\napproach, Quality Diversity in Amorphous Fortress (QD-AF) generates families of\n0-player games akin to simplistic ecological models, and we identify the\nemergence of both competitive and co-operative multi-agent and multi-species\nsurvival dynamics. We argue that these generated worlds can collectively serve\nas training and testing grounds for learning algorithms.",
            "author": [
                "Sam Earle",
                "M Charity",
                "Dipika Rajesh",
                "Mayu Wilson",
                "Julian Togelius"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02231v1",
                "http://arxiv.org/pdf/2312.02231v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.MA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01629v1",
            "title": "CLAMP: Contrastive LAnguage Model Prompt-tuning",
            "updated": "2023-12-04T05:13:59Z",
            "published": "2023-12-04T05:13:59Z",
            "summary": "Large language models (LLMs) have emerged as powerful general-purpose\ninterfaces for many machine learning problems. Recent work has adapted LLMs to\ngenerative visual tasks like image captioning, visual question answering, and\nvisual chat, using a relatively small amount of instruction-tuning data. In\nthis paper, we explore whether modern LLMs can also be adapted to classifying\nan image into a set of categories. First, we evaluate multimodal LLMs that are\ntuned for generative tasks on zero-shot image classification and find that\ntheir performance is far below that of specialized models like CLIP. We then\npropose an approach for light fine-tuning of LLMs using the same contrastive\nimage-caption matching objective as CLIP. Our results show that LLMs can,\nindeed, achieve good image classification performance when adapted this way.\nOur approach beats state-of-the-art mLLMs by 13% and slightly outperforms\ncontrastive learning with a custom text model, while also retaining the LLM's\ngenerative abilities. LLM initialization appears to particularly help\nclassification in domains under-represented in the visual pre-training data.",
            "author": [
                "Piotr Teterwak",
                "Ximeng Sun",
                "Bryan A. Plummer",
                "Kate Saenko",
                "Ser-Nam Lim"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01629v1",
                "http://arxiv.org/pdf/2312.01629v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01627v1",
            "title": "Teachers' trust and perceptions of AI in education: The role of culture\n  and AI self-efficacy in six countries",
            "updated": "2023-12-04T05:07:23Z",
            "published": "2023-12-04T05:07:23Z",
            "summary": "AI-based educational technology (AI-EdTech) is increasingly adopted in K-12\neducation. Teachers play a critical role in this process as they are expected\nto use AI-EdTech in ways that support their teaching practice and students'\nlearning outcomes. Teachers' willingness to meaningfully integrate these\ntechnologies into their everyday educational activities depends on their\nattitudes toward AI-EdTech. We surveyed 508 K-12 teachers in six countries\nacross four continents (Brazil, Israel, Japan, Norway, Sweden, USA) about the\nperceived benefits of, concerns about, and trust in AI-EdTech. We examine\ndemographic, geo-cultural, professional, and psychological factors that might\ninfluence teachers' attitudes. Our results showed that teachers with higher AI\nunderstanding and self-efficacy perceive more benefits, fewer concerns, and\nstronger trust. We also found geographic and cultural differences in teachers'\nattitudes, including their trust in AI-EdTech, but no demographic differences\nemerged based on their age, gender, or level of education. The findings provide\na comprehensive, international account of factors influencing teachers'\nattitudes toward AI-EdTech. Efforts to raise teachers' understanding of, and\ntrust in AI-EdTech, while considering their cultural values are encouraged to\nsupport its adoption in K-12 education.",
            "author": [
                "Olga Viberg",
                "Mutlu Cukurova",
                "Yael Feldman-Maggor",
                "Giora Alexandron",
                "Shizuka Shirai",
                "Susumu Kanemune",
                "Barbara Wasson",
                "Cathrine T\u00f8mte",
                "Daniel Spikol",
                "Marcelo Milrad",
                "Raquel Coelho",
                "Ren\u00e9 F. Kizilcec"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01627v1",
                "http://arxiv.org/pdf/2312.01627v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY",
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01624v1",
            "title": "GVFs in the Real World: Making Predictions Online for Water Treatment",
            "updated": "2023-12-04T04:49:10Z",
            "published": "2023-12-04T04:49:10Z",
            "summary": "In this paper we investigate the use of reinforcement-learning based\nprediction approaches for a real drinking-water treatment plant. Developing\nsuch a prediction system is a critical step on the path to optimizing and\nautomating water treatment. Before that, there are many questions to answer\nabout the predictability of the data, suitable neural network architectures,\nhow to overcome partial observability and more. We first describe this dataset,\nand highlight challenges with seasonality, nonstationarity, partial\nobservability, and heterogeneity across sensors and operation modes of the\nplant. We then describe General Value Function (GVF) predictions -- discounted\ncumulative sums of observations -- and highlight why they might be preferable\nto classical n-step predictions common in time series prediction. We discuss\nhow to use offline data to appropriately pre-train our temporal difference\nlearning (TD) agents that learn these GVF predictions, including how to select\nhyperparameters for online fine-tuning in deployment. We find that the\nTD-prediction agent obtains an overall lower normalized mean-squared error than\nthe n-step prediction agent. Finally, we show the importance of learning in\ndeployment, by comparing a TD agent trained purely offline with no online\nupdating to a TD agent that learns online. This final result is one of the\nfirst to motivate the importance of adapting predictions in real-time, for\nnon-stationary high-volume systems in the real world.",
            "author": [
                "Muhammad Kamran Janjua",
                "Haseeb Shah",
                "Martha White",
                "Erfan Miahi",
                "Marlos C. Machado",
                "Adam White"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01624v1",
                "http://arxiv.org/pdf/2312.01624v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01619v2",
            "title": "How Many Validation Labels Do You Need? Exploring the Design Space of\n  Label-Efficient Model Ranking",
            "updated": "2023-12-06T22:42:27Z",
            "published": "2023-12-04T04:20:38Z",
            "summary": "The paper introduces LEMR, a framework that reduces annotation costs for\nmodel selection tasks. Our approach leverages ensemble methods to generate\npseudo-labels, employs uncertainty sampling for target acquisition, and\nutilizes a Z-score mechanism for iterative committee reelection to refine model\nranks. We present a systematic study across various selection metrics,\ndemonstrating that LEMR achieves comparable results to fully labeled datasets\nwith a fraction of the labeling budget. Our findings indicate that LEMR not\nonly economizes the labeling effort in weak supervision and semi-supervised\nlearning settings but also effectively guides prompt selection for large\nlanguage models. With extensive experiments across 23 tasks, we reveal that our\nframework can dramatically decrease the labeling cost without compromising the\naccuracy of model selection, thereby offering a cost-effective alternative to\ntraditional practices.",
            "author": [
                "Zhengyu Hu",
                "Jieyu Zhang",
                "Yue Yu",
                "Yuchen Zhuang",
                "Hui Xiong"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01619v2",
                "http://arxiv.org/pdf/2312.01619v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01617v1",
            "title": "Heroes: Lightweight Federated Learning with Neural Composition and\n  Adaptive Local Update in Heterogeneous Edge Networks",
            "updated": "2023-12-04T04:14:50Z",
            "published": "2023-12-04T04:14:50Z",
            "summary": "Federated Learning (FL) enables distributed clients to collaboratively train\nmodels without exposing their private data. However, it is difficult to\nimplement efficient FL due to limited resources. Most existing works compress\nthe transmitted gradients or prune the global model to reduce the resource\ncost, but leave the compressed or pruned parameters under-optimized, which\ndegrades the training performance. To address this issue, the neural\ncomposition technique constructs size-adjustable models by composing low-rank\ntensors, allowing every parameter in the global model to learn the knowledge\nfrom all clients. Nevertheless, some tensors can only be optimized by a small\nfraction of clients, thus the global model may get insufficient training,\nleading to a long completion time, especially in heterogeneous edge scenarios.\nTo this end, we enhance the neural composition technique, enabling all\nparameters to be fully trained. Further, we propose a lightweight FL framework,\ncalled Heroes, with enhanced neural composition and adaptive local update. A\ngreedy-based algorithm is designed to adaptively assign the proper tensors and\nlocal update frequencies for participating clients according to their\nheterogeneous capabilities and resource budgets. Extensive experiments\ndemonstrate that Heroes can reduce traffic consumption by about 72.05\\% and\nprovide up to 2.97$\\times$ speedup compared to the baselines.",
            "author": [
                "Jiaming Yan",
                "Jianchun Liu",
                "Shilong Wang",
                "Hongli Xu",
                "Haifeng Liu",
                "Jianhua Zhou"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01617v1",
                "http://arxiv.org/pdf/2312.01617v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01612v1",
            "title": "xNeuSM: Explainable Neural Subgraph Matching with Graph Learnable\n  Multi-hop Attention Networks",
            "updated": "2023-12-04T04:03:30Z",
            "published": "2023-12-04T04:03:30Z",
            "summary": "Subgraph matching is a challenging problem with a wide range of applications\nin database systems, biochemistry, and cognitive science. It involves\ndetermining whether a given query graph is present within a larger target\ngraph. Traditional graph-matching algorithms provide precise results but face\nchallenges in large graph instances due to the NP-complete problem, limiting\ntheir practical applicability. In contrast, recent neural network-based\napproximations offer more scalable solutions, but often lack interpretable node\ncorrespondences. To address these limitations, this article presents xNeuSM:\nExplainable Neural Subgraph Matching which introduces Graph Learnable Multi-hop\nAttention Networks (GLeMA) that adaptively learns the parameters governing the\nattention factor decay for each node across hops rather than relying on fixed\nhyperparameters. We provide a theoretical analysis establishing error bounds\nfor GLeMA's approximation of multi-hop attention as a function of the number of\nhops. Additionally, we prove that learning distinct attention decay factors for\neach node leads to a correct approximation of multi-hop attention. Empirical\nevaluation on real-world datasets shows that xNeuSM achieves substantial\nimprovements in prediction accuracy of up to 34% compared to approximate\nbaselines and, notably, at least a seven-fold faster query time than exact\nalgorithms. The source code of our implementation is available at\nhttps://github.com/martinakaduc/xNeuSM.",
            "author": [
                "Duc Q. Nguyen",
                "Thanh Toan Nguyen",
                "Tho quan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01612v1",
                "http://arxiv.org/pdf/2312.01612v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02230v1",
            "title": "A Simple and Scalable Representation for Graph Generation",
            "updated": "2023-12-04T03:43:26Z",
            "published": "2023-12-04T03:43:26Z",
            "summary": "Recently, there has been a surge of interest in employing neural networks for\ngraph generation, a fundamental statistical learning problem with critical\napplications like molecule design and community analysis. However, most\napproaches encounter significant limitations when generating large-scale\ngraphs. This is due to their requirement to output the full adjacency matrices\nwhose size grows quadratically with the number of nodes. In response to this\nchallenge, we introduce a new, simple, and scalable graph representation named\ngap encoded edge list (GEEL) that has a small representation size that aligns\nwith the number of edges. In addition, GEEL significantly reduces the\nvocabulary size by incorporating the gap encoding and bandwidth restriction\nschemes. GEEL can be autoregressively generated with the incorporation of node\npositional encoding, and we further extend GEEL to deal with attributed graphs\nby designing a new grammar. Our findings reveal that the adoption of this\ncompact representation not only enhances scalability but also bolsters\nperformance by simplifying the graph generation process. We conduct a\ncomprehensive evaluation across ten non-attributed and two molecular graph\ngeneration tasks, demonstrating the effectiveness of GEEL.",
            "author": [
                "Yunhui Jang",
                "Seul Lee",
                "Sungsoo Ahn"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02230v1",
                "http://arxiv.org/pdf/2312.02230v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01606v2",
            "title": "Deep Learning-Driven Enhancement of Welding Quality Control: Predicting\n  Welding Depth and Pore Volume in Hairpin Welding",
            "updated": "2023-12-05T05:43:31Z",
            "published": "2023-12-04T03:38:17Z",
            "summary": "To advance quality assurance in the welding process, this study presents a\nrobust deep learning model that enables the prediction of two critical welds\nKey Performance Characteristics (KPCs): welding depth and average pore volume.\nIn the proposed approach, a comprehensive range of laser welding Key Input\nCharacteristics (KICs) is utilized, including welding beam geometries, welding\nfeed rates, path repetitions for weld beam geometries, and bright light weld\nratios for all paths, all of which were obtained from hairpin welding\nexperiments. Two deep learning networks are employed with multiple hidden dense\nlayers and linear activation functions to showcase the capabilities of deep\nneural networks in capturing the intricate nonlinear connections inherent\nwithin welding KPCs and KICs. Applying deep learning networks to the small\nnumerical experimental hairpin welding dataset has shown promising results,\nachieving Mean Absolute Error (MAE) values as low as 0.1079 for predicting\nwelding depth and 0.0641 for average pore volume. Additionally, the validity\nverification demonstrates the reliability of the proposed method. This, in\nturn, promises significant advantages in controlling welding outcomes, moving\nbeyond the current trend of relying merely on monitoring for defect\nclassification.",
            "author": [
                "Amena Darwish",
                "Stefan Ericson",
                "Rohollah Ghasemi",
                "Tobias Andersson",
                "Dan L\u00f6nn",
                "Andreas Andersson Lassila",
                "Kent Salomonsson"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01606v2",
                "http://arxiv.org/pdf/2312.01606v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01605v1",
            "title": "TextAug: Test time Text Augmentation for Multimodal Person\n  Re-identification",
            "updated": "2023-12-04T03:38:04Z",
            "published": "2023-12-04T03:38:04Z",
            "summary": "Multimodal Person Reidentification is gaining popularity in the research\ncommunity due to its effectiveness compared to counter-part unimodal\nframeworks. However, the bottleneck for multimodal deep learning is the need\nfor a large volume of multimodal training examples. Data augmentation\ntechniques such as cropping, flipping, rotation, etc. are often employed in the\nimage domain to improve the generalization of deep learning models. Augmenting\nin other modalities than images, such as text, is challenging and requires\nsignificant computational resources and external data sources. In this study,\nwe investigate the effectiveness of two computer vision data augmentation\ntechniques: cutout and cutmix, for text augmentation in multi-modal person\nre-identification. Our approach merges these two augmentation strategies into\none strategy called CutMixOut which involves randomly removing words or\nsub-phrases from a sentence (Cutout) and blending parts of two or more\nsentences to create diverse examples (CutMix) with a certain probability\nassigned to each operation. This augmentation was implemented at inference time\nwithout any prior training. Our results demonstrate that the proposed technique\nis simple and effective in improving the performance on multiple multimodal\nperson re-identification benchmarks.",
            "author": [
                "Mulham Fawakherji",
                "Eduard Vazquez",
                "Pasquale Giampa",
                "Binod Bhattarai"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01605v1",
                "http://arxiv.org/pdf/2312.01605v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01601v1",
            "title": "Local-Global History-aware Contrastive Learning for Temporal Knowledge\n  Graph Reasoning",
            "updated": "2023-12-04T03:27:01Z",
            "published": "2023-12-04T03:27:01Z",
            "summary": "Temporal knowledge graphs (TKGs) have been identified as a promising approach\nto represent the dynamics of facts along the timeline. The extrapolation of TKG\nis to predict unknowable facts happening in the future, holding significant\npractical value across diverse fields. Most extrapolation studies in TKGs focus\non modeling global historical fact repeating and cyclic patterns, as well as\nlocal historical adjacent fact evolution patterns, showing promising\nperformance in predicting future unknown facts. Yet, existing methods still\nface two major challenges: (1) They usually neglect the importance of\nhistorical information in KG snapshots related to the queries when encoding the\nlocal and global historical information; (2) They exhibit weak anti-noise\ncapabilities, which hinders their performance when the inputs are contaminated\nwith noise.To this end, we propose a novel \\blue{Lo}cal-\\blue{g}lobal\nhistory-aware \\blue{C}ontrastive \\blue{L}earning model (\\blue{LogCL}) for TKG\nreasoning, which adopts contrastive learning to better guide the fusion of\nlocal and global historical information and enhance the ability to resist\ninterference. Specifically, for the first challenge, LogCL proposes an\nentity-aware attention mechanism applied to the local and global historical\nfacts encoder, which captures the key historical information related to\nqueries. For the latter issue, LogCL designs four historical query contrast\npatterns, effectively improving the robustness of the model. The experimental\nresults on four benchmark datasets demonstrate that LogCL delivers better and\nmore robust performance than the state-of-the-art baselines.",
            "author": [
                "Wei Chen",
                "Huaiyu Wan",
                "Yuting Wu",
                "Shuyuan Zhao",
                "Jiayaqi Cheng",
                "Yuxin Li",
                "Youfang Lin"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01601v1",
                "http://arxiv.org/pdf/2312.01601v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01592v1",
            "title": "Expand BERT Representation with Visual Information via Grounded Language\n  Learning with Multimodal Partial Alignment",
            "updated": "2023-12-04T03:16:48Z",
            "published": "2023-12-04T03:16:48Z",
            "summary": "Language models have been supervised with both language-only objective and\nvisual grounding in existing studies of visual-grounded language learning.\nHowever, due to differences in the distribution and scale of visual-grounded\ndatasets and language corpora, the language model tends to mix up the context\nof the tokens that occurred in the grounded data with those that do not. As a\nresult, during representation learning, there is a mismatch between the visual\ninformation and the contextual meaning of the sentence. To overcome this\nlimitation, we propose GroundedBERT - a grounded language learning method that\nenhances the BERT representation with visually grounded information.\nGroundedBERT comprises two components: (i) the original BERT which captures the\ncontextual representation of words learned from the language corpora, and (ii)\na visual grounding module which captures visual information learned from\nvisual-grounded datasets. Moreover, we employ Optimal Transport (OT),\nspecifically its partial variant, to solve the fractional alignment problem\nbetween the two modalities. Our proposed method significantly outperforms the\nbaseline language models on various language tasks of the GLUE and SQuAD\ndatasets.",
            "author": [
                "Cong-Duy Nguyen",
                "The-Anh Vu-Le",
                "Thong Nguyen",
                "Tho Quan",
                "Luu Anh Tuan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01592v1",
                "http://arxiv.org/pdf/2312.01592v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02229v1",
            "title": "Synthetic Data Generation Techniques for Developing AI-based Speech\n  Assessments for Parkinson's Disease (A Comparative Study)",
            "updated": "2023-12-04T03:12:09Z",
            "published": "2023-12-04T03:12:09Z",
            "summary": "Changes in speech and language are among the first signs of Parkinson's\ndisease (PD). Thus, clinicians have tried to identify individuals with PD from\ntheir voices for years. Doctors can leverage AI-based speech assessments to\nspot PD thanks to advancements in artificial intelligence (AI). Such AI systems\ncan be developed using machine learning classifiers that have been trained\nusing individuals' voices. Although several studies have shown reasonable\nresults in developing such AI systems, these systems would need more data\nsamples to achieve promising performance. This paper explores using deep\nlearning-based data generation techniques on the accuracy of machine learning\nclassifiers that are the core of such systems.",
            "author": [
                "Mahboobeh Parsapoor"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02229v1",
                "http://arxiv.org/pdf/2312.02229v1"
            ],
            "primary_category": "cs.SD",
            "category": [
                "cs.SD",
                "cs.AI",
                "cs.LG",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01588v1",
            "title": "ActiveClean: Generating Line-Level Vulnerability Data via Active\n  Learning",
            "updated": "2023-12-04T03:09:31Z",
            "published": "2023-12-04T03:09:31Z",
            "summary": "Deep learning vulnerability detection tools are increasing in popularity and\nhave been shown to be effective. These tools rely on large volume of high\nquality training data, which are very hard to get. Most of the currently\navailable datasets provide function-level labels, reporting whether a function\nis vulnerable or not vulnerable. However, for a vulnerability detection to be\nuseful, we need to also know the lines that are relevant to the vulnerability.\nThis paper makes efforts towards developing systematic tools and proposes.\nActiveClean to generate the large volume of line-level vulnerability data from\ncommits. That is, in addition to function-level labels, it also reports which\nlines in the function are likely responsible for vulnerability detection. In\nthe past, static analysis has been applied to clean commits to generate\nline-level data. Our approach based on active learning, which is easy to use\nand scalable, provide a complementary approach to static analysis. We designed\nsemantic and syntactic properties from commit lines and use them to train the\nmodel. We evaluated our approach on both Java and C datasets processing more\nthan 4.3K commits and 119K commit lines. AcitveClean achieved an F1 score\nbetween 70-74. Further, we also show that active learning is effective by using\njust 400 training data to reach F1 score of 70.23. Using ActiveClean, we\ngenerate the line-level labels for the entire FFMpeg project in the Devign\ndataset, including 5K functions, and also detected incorrect function-level\nlabels. We demonstrated that using our cleaned data, LineVul, a SOTA line-level\nvulnerability detection tool, detected 70 more vulnerable lines and 18 more\nvulnerable functions, and improved Top 10 accuracy from 66% to 73%.",
            "author": [
                "Ashwin Kallingal Joshy",
                "Mirza Sanjida Alam",
                "Shaila Sharmin",
                "Qi Li",
                "Wei Le"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01588v1",
                "http://arxiv.org/pdf/2312.01588v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01587v1",
            "title": "Scalable and Independent Learning of Nash Equilibrium Policies in\n  $n$-Player Stochastic Games with Unknown Independent Chains",
            "updated": "2023-12-04T03:04:09Z",
            "published": "2023-12-04T03:04:09Z",
            "summary": "We study a subclass of $n$-player stochastic games, namely, stochastic games\nwith independent chains and unknown transition matrices. In this class of\ngames, players control their own internal Markov chains whose transitions do\nnot depend on the states/actions of other players. However, players' decisions\nare coupled through their payoff functions. We assume players can receive only\nrealizations of their payoffs, and that the players can not observe the states\nand actions of other players, nor do they know the transition probability\nmatrices of their own Markov chain. Relying on a compact dual formulation of\nthe game based on occupancy measures and the technique of confidence set to\nmaintain high-probability estimates of the unknown transition matrices, we\npropose a fully decentralized mirror descent algorithm to learn an\n$\\epsilon$-NE for this class of games. The proposed algorithm has the desired\nproperties of independence, scalability, and convergence. Specifically, under\nno assumptions on the reward functions, we show the proposed algorithm\nconverges in polynomial time in a weaker distance (namely, the averaged\nNikaido-Isoda gap) to the set of $\\epsilon$-NE policies with arbitrarily high\nprobability. Moreover, assuming the existence of a variationally stable Nash\nequilibrium policy, we show that the proposed algorithm converges\nasymptotically to the stable $\\epsilon$-NE policy with arbitrarily high\nprobability. In addition to Markov potential games and linear-quadratic\nstochastic games, this work provides another subclass of $n$-player stochastic\ngames that, under some mild assumptions, admit polynomial-time learning\nalgorithms for finding their stationary $\\epsilon$-NE policies.",
            "author": [
                "Tiancheng Qin",
                "S. Rasoul Etesami"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01587v1",
                "http://arxiv.org/pdf/2312.01587v1"
            ],
            "primary_category": "cs.GT",
            "category": [
                "cs.GT",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02227v1",
            "title": "Improving Multimodal Sentiment Analysis: Supervised Angular Margin-based\n  Contrastive Learning for Enhanced Fusion Representation",
            "updated": "2023-12-04T02:58:19Z",
            "published": "2023-12-04T02:58:19Z",
            "summary": "The effectiveness of a model is heavily reliant on the quality of the fusion\nrepresentation of multiple modalities in multimodal sentiment analysis.\nMoreover, each modality is extracted from raw input and integrated with the\nrest to construct a multimodal representation. Although previous methods have\nproposed multimodal representations and achieved promising results, most of\nthem focus on forming positive and negative pairs, neglecting the variation in\nsentiment scores within the same class. Additionally, they fail to capture the\nsignificance of unimodal representations in the fusion vector. To address these\nlimitations, we introduce a framework called Supervised Angular-based\nContrastive Learning for Multimodal Sentiment Analysis. This framework aims to\nenhance discrimination and generalizability of the multimodal representation\nand overcome biases in the fusion vector's modality. Our experimental results,\nalong with visualizations on two widely used datasets, demonstrate the\neffectiveness of our approach.",
            "author": [
                "Cong-Duy Nguyen",
                "Thong Nguyen",
                "Duc Anh Vu",
                "Luu Anh Tuan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02227v1",
                "http://arxiv.org/pdf/2312.02227v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01585v1",
            "title": "OCGEC: One-class Graph Embedding Classification for DNN Backdoor\n  Detection",
            "updated": "2023-12-04T02:48:40Z",
            "published": "2023-12-04T02:48:40Z",
            "summary": "Deep neural networks (DNNs) have been found vulnerable to backdoor attacks,\nraising security concerns about their deployment in mission-critical\napplications. There are various approaches to detect backdoor attacks, however\nthey all make certain assumptions about the target attack to be detected and\nrequire equal and huge numbers of clean and backdoor samples for training,\nwhich renders these detection methods quite limiting in real-world\ncircumstances.\n  This study proposes a novel one-class classification framework called\nOne-class Graph Embedding Classification (OCGEC) that uses GNNs for model-level\nbackdoor detection with only a little amount of clean data. First, we train\nthousands of tiny models as raw datasets from a small number of clean datasets.\nFollowing that, we design a ingenious model-to-graph method for converting the\nmodel's structural details and weight features into graph data. We then\npre-train a generative self-supervised graph autoencoder (GAE) to better learn\nthe features of benign models in order to detect backdoor models without\nknowing the attack strategy. After that, we dynamically combine the GAE and\none-class classifier optimization goals to form classification boundaries that\ndistinguish backdoor models from benign models.\n  Our OCGEC combines the powerful representation capabilities of graph neural\nnetworks with the utility of one-class classification techniques in the field\nof anomaly detection. In comparison to other baselines, it achieves AUC scores\nof more than 98% on a number of tasks, which far exceeds existing methods for\ndetection even when they rely on a huge number of positive and negative\nsamples. Our pioneering application of graphic scenarios for generic backdoor\ndetection can provide new insights that can be used to improve other backdoor\ndefense tasks. Code is available at https://github.com/jhy549/OCGEC.",
            "author": [
                "Haoyu Jiang",
                "Haiyang Yu",
                "Nan Li",
                "Ping Yi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01585v1",
                "http://arxiv.org/pdf/2312.01585v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01581v1",
            "title": "Signed Binarization: Unlocking Efficiency Through Repetition-Sparsity\n  Trade-Off",
            "updated": "2023-12-04T02:33:53Z",
            "published": "2023-12-04T02:33:53Z",
            "summary": "Efficient inference of Deep Neural Networks (DNNs) on resource-constrained\nedge devices is essential. Quantization and sparsity are key algorithmic\ntechniques that translate to repetition and sparsity within tensors at the\nhardware-software interface. This paper introduces the concept of\nrepetition-sparsity trade-off that helps explain computational efficiency\nduring inference. We propose Signed Binarization, a unified co-design framework\nthat synergistically integrates hardware-software systems, quantization\nfunctions, and representation learning techniques to address this trade-off.\nOur results demonstrate that Signed Binarization is more accurate than\nbinarization with the same number of non-zero weights. Detailed analysis\nindicates that signed binarization generates a smaller distribution of\neffectual (non-zero) parameters nested within a larger distribution of total\nparameters, both of the same type, for a DNN block. Finally, our approach\nachieves a 26% speedup on real hardware, doubles energy efficiency, and reduces\ndensity by 2.8x compared to binary methods for ResNet 18, presenting an\nalternative solution for deploying efficient models in resource-limited\nenvironments.",
            "author": [
                "Sachit Kuhar",
                "Yash Jain",
                "Alexey Tumanov"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01581v1",
                "http://arxiv.org/pdf/2312.01581v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01577v1",
            "title": "RJHMC-Tree for Exploration of the Bayesian Decision Tree Posterior",
            "updated": "2023-12-04T02:23:32Z",
            "published": "2023-12-04T02:23:32Z",
            "summary": "Decision trees have found widespread application within the machine learning\ncommunity due to their flexibility and interpretability. This paper is directed\ntowards learning decision trees from data using a Bayesian approach, which is\nchallenging due to the potentially enormous parameter space required to span\nall tree models. Several approaches have been proposed to combat this\nchallenge, with one of the more successful being Markov chain Monte Carlo\n(MCMC) methods. The efficacy and efficiency of MCMC methods fundamentally rely\non the quality of the so-called proposals, which is the focus of this paper. In\nparticular, this paper investigates using a Hamiltonian Monte Carlo (HMC)\napproach to explore the posterior of Bayesian decision trees more efficiently\nby exploiting the geometry of the likelihood within a global update scheme. Two\nimplementations of the novel algorithm are developed and compared to existing\nmethods by testing against standard datasets in the machine learning and\nBayesian decision tree literature. HMC-based methods are shown to perform\nfavourably with respect to predictive test accuracy, acceptance rate, and tree\ncomplexity.",
            "author": [
                "Jodie A. Cochrane",
                "Adrian G. Wills",
                "Sarah J. Johnson"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01577v1",
                "http://arxiv.org/pdf/2312.01577v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.CO",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01576v1",
            "title": "Learning Efficient Unsupervised Satellite Image-based Building Damage\n  Detection",
            "updated": "2023-12-04T02:20:35Z",
            "published": "2023-12-04T02:20:35Z",
            "summary": "Existing Building Damage Detection (BDD) methods always require\nlabour-intensive pixel-level annotations of buildings and their conditions,\nhence largely limiting their applications. In this paper, we investigate a\nchallenging yet practical scenario of BDD, Unsupervised Building Damage\nDetection (U-BDD), where only unlabelled pre- and post-disaster satellite image\npairs are provided. As a pilot study, we have first proposed an advanced U-BDD\nbaseline that leverages pre-trained vision-language foundation models (i.e.,\nGrounding DINO, SAM and CLIP) to address the U-BDD task. However, the apparent\ndomain gap between satellite and generic images causes low confidence in the\nfoundation models used to identify buildings and their damages. In response, we\nfurther present a novel self-supervised framework, U-BDD++, which improves upon\nthe U-BDD baseline by addressing domain-specific issues associated with\nsatellite imagery. Furthermore, the new Building Proposal Generation (BPG)\nmodule and the CLIP-enabled noisy Building Proposal Selection (CLIP-BPS) module\nin U-BDD++ ensure high-quality self-training. Extensive experiments on the\nwidely used building damage assessment benchmark demonstrate the effectiveness\nof the proposed method for unsupervised building damage detection. The\npresented annotation-free and foundation model-based paradigm ensures an\nefficient learning phase. This study opens a new direction for real-world BDD\nand sets a strong baseline for future research.",
            "author": [
                "Yiyun Zhang",
                "Zijian Wang",
                "Yadan Luo",
                "Xin Yu",
                "Zi Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01576v1",
                "http://arxiv.org/pdf/2312.01576v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.MM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01573v1",
            "title": "Survey on deep learning in multimodal medical imaging for cancer\n  detection",
            "updated": "2023-12-04T02:07:47Z",
            "published": "2023-12-04T02:07:47Z",
            "summary": "The task of multimodal cancer detection is to determine the locations and\ncategories of lesions by using different imaging techniques, which is one of\nthe key research methods for cancer diagnosis. Recently, deep learning-based\nobject detection has made significant developments due to its strength in\nsemantic feature extraction and nonlinear function fitting. However, multimodal\ncancer detection remains challenging due to morphological differences in\nlesions, interpatient variability, difficulty in annotation, and imaging\nartifacts. In this survey, we mainly investigate over 150 papers in recent\nyears with respect to multimodal cancer detection using deep learning, with a\nfocus on datasets and solutions to various challenges such as data annotation,\nvariance between classes, small-scale lesions, and occlusion. We also provide\nan overview of the advantages and drawbacks of each approach. Finally, we\ndiscuss the current scope of work and provide directions for the future\ndevelopment of multimodal cancer detection.",
            "author": [
                "Yan Tian",
                "Zhaocheng Xu",
                "Yujun Ma",
                "Weiping Ding",
                "Ruili Wang",
                "Zhihong Gao",
                "Guohua Cheng",
                "Linyang He",
                "Xuran Zhao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01573v1",
                "http://arxiv.org/pdf/2312.01573v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01571v1",
            "title": "How to Configure Good In-Context Sequence for Visual Question Answering",
            "updated": "2023-12-04T02:03:23Z",
            "published": "2023-12-04T02:03:23Z",
            "summary": "Inspired by the success of Large Language Models in dealing with new tasks\nvia In-Context Learning (ICL) in NLP, researchers have also developed Large\nVision-Language Models (LVLMs) with ICL capabilities. However, when\nimplementing ICL using these LVLMs, researchers usually resort to the simplest\nway like random sampling to configure the in-context sequence, thus leading to\nsub-optimal results. To enhance the ICL performance, in this study, we use\nVisual Question Answering (VQA) as case study to explore diverse in-context\nconfigurations to find the powerful ones. Additionally, through observing the\nchanges of the LVLM outputs by altering the in-context sequence, we gain\ninsights into the inner properties of LVLMs, improving our understanding of\nthem. Specifically, to explore in-context configurations, we design diverse\nretrieval methods and employ different strategies to manipulate the retrieved\ndemonstrations. Through exhaustive experiments on three VQA datasets: VQAv2,\nVizWiz, and OK-VQA, we uncover three important inner properties of the applied\nLVLM and demonstrate which strategies can consistently improve the ICL VQA\nperformance. Our code is provided in:\nhttps://github.com/GaryJiajia/OFv2_ICL_VQA.",
            "author": [
                "Li Li",
                "Jiawei Peng",
                "Huiyi Chen",
                "Chongyang Gao",
                "Xu Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01571v1",
                "http://arxiv.org/pdf/2312.01571v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01568v1",
            "title": "Multimodal Speech Emotion Recognition Using Modality-specific\n  Self-Supervised Frameworks",
            "updated": "2023-12-04T01:49:24Z",
            "published": "2023-12-04T01:49:24Z",
            "summary": "Emotion recognition is a topic of significant interest in assistive robotics\ndue to the need to equip robots with the ability to comprehend human behavior,\nfacilitating their effective interaction in our society. Consequently,\nefficient and dependable emotion recognition systems supporting optimal\nhuman-machine communication are required. Multi-modality (including speech,\naudio, text, images, and videos) is typically exploited in emotion recognition\ntasks. Much relevant research is based on merging multiple data modalities and\ntraining deep learning models utilizing low-level data representations.\nHowever, most existing emotion databases are not large (or complex) enough to\nallow machine learning approaches to learn detailed representations. This paper\nexplores modalityspecific pre-trained transformer frameworks for\nself-supervised learning of speech and text representations for data-efficient\nemotion recognition while achieving state-of-the-art performance in recognizing\nemotions. This model applies feature-level fusion using nonverbal cue data\npoints from motion capture to provide multimodal speech emotion recognition.\nThe model was trained using the publicly available IEMOCAP dataset, achieving\nan overall accuracy of 77.58% for four emotions, outperforming state-of-the-art\napproaches",
            "author": [
                "Rutherford Agbeshi Patamia",
                "Paulo E. Santos",
                "Kingsley Nketia Acheampong",
                "Favour Ekong",
                "Kwabena Sarpong",
                "She Kun"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01568v1",
                "http://arxiv.org/pdf/2312.01568v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC",
                "cs.SD",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01567v1",
            "title": "Toward Automated Quantum Variational Machine Learning",
            "updated": "2023-12-04T01:47:05Z",
            "published": "2023-12-04T01:47:05Z",
            "summary": "In this work, we address the problem of automating quantum variational\nmachine learning. We develop a multi-locality parallelizable search algorithm,\ncalled MUSE, to find the initial points and the sets of parameters that achieve\nthe best performance for quantum variational circuit learning. Simulations with\nfive real-world classification datasets indicate that on average, MUSE improves\nthe detection accuracy of quantum variational classifiers 2.3 times with\nrespect to the observed lowest scores. Moreover, when applied to two real-world\nregression datasets, MUSE improves the quality of the predictions from negative\ncoefficients of determination to positive ones. Furthermore, the classification\nand regression scores of the quantum variational models trained with MUSE are\non par with the classical counterparts.",
            "author": [
                "Omer Subasi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01567v1",
                "http://arxiv.org/pdf/2312.01567v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.ET",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01566v1",
            "title": "Coronary Atherosclerotic Plaque Characterization with Photon-counting\n  CT: a Simulation-based Feasibility Study",
            "updated": "2023-12-04T01:47:00Z",
            "published": "2023-12-04T01:47:00Z",
            "summary": "Recent development of photon-counting CT (PCCT) brings great opportunities\nfor plaque characterization with much-improved spatial resolution and spectral\nimaging capability. While existing coronary plaque PCCT imaging results are\nbased on detectors made of CZT or CdTe materials, deep-silicon photon-counting\ndetectors have unique performance characteristics and promise distinct imaging\ncapabilities. In this work, we report a systematic simulation study of a\ndeep-silicon PCCT scanner with a new clinically-relevant digital plaque phantom\nwith realistic geometrical parameters and chemical compositions. This work\ninvestigates the effects of spatial resolution, noise, motion artifacts,\nradiation dose, and spectral characterization. Our simulation results suggest\nthat the deep-silicon PCCT design provides adequate spatial resolution for\nvisualizing a necrotic core and quantitation of key plaque features. Advanced\ndenoising techniques and aggressive bowtie filter designs can keep image noise\nto acceptable levels at this resolution while keeping radiation dose comparable\nto that of a conventional CT scan. The ultrahigh resolution of PCCT also means\nan elevated sensitivity to motion artifacts. It is found that a tolerance of\nless than 0.4 mm residual movement range requires the application of accurate\nmotion correction methods for best plaque imaging quality with PCCT.",
            "author": [
                "Mengzhou Li",
                "Mingye Wu",
                "Jed Pack",
                "Pengwei Wu",
                "Bruno De Man",
                "Adam Wang",
                "Koen Nieman",
                "Ge Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01566v1",
                "http://arxiv.org/pdf/2312.01566v1"
            ],
            "primary_category": "physics.med-ph",
            "category": [
                "physics.med-ph",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01564v1",
            "title": "APoLLo: Unified Adapter and Prompt Learning for Vision Language Models",
            "updated": "2023-12-04T01:42:09Z",
            "published": "2023-12-04T01:42:09Z",
            "summary": "The choice of input text prompt plays a critical role in the performance of\nVision-Language Pretrained (VLP) models such as CLIP. We present APoLLo, a\nunified multi-modal approach that combines Adapter and Prompt learning for\nVision-Language models. Our method is designed to substantially improve the\ngeneralization capabilities of VLP models when they are fine-tuned in a\nfew-shot setting. We introduce trainable cross-attention-based adapter layers\nin conjunction with vision and language encoders to strengthen the alignment\nbetween the two modalities. We enforce consistency between the respective\nencoder branches (receiving augmented inputs) to prevent overfitting in\ndownstream tasks. Our method is evaluated on three representative tasks:\ngeneralization to novel classes, cross-dataset evaluation, and unseen domain\nshifts. In practice, APoLLo achieves a relative gain up to 6.03% over MaPLe\n(SOTA) on novel classes for 10 diverse image recognition datasets.",
            "author": [
                "Sanjoy Chowdhury",
                "Sayan Nag",
                "Dinesh Manocha"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01564v1",
                "http://arxiv.org/pdf/2312.01564v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CL",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01562v1",
            "title": "Kernel Alignment for Quantum Support Vector Machines Using Genetic\n  Algorithms",
            "updated": "2023-12-04T01:36:26Z",
            "published": "2023-12-04T01:36:26Z",
            "summary": "The data encoding circuits used in quantum support vector machine (QSVM)\nkernels play a crucial role in their classification accuracy. However, manually\ndesigning these circuits poses significant challenges in terms of time and\nperformance. To address this, we leverage the GASP (Genetic Algorithm for State\nPreparation) framework for gate sequence selection in QSVM kernel circuits. We\nexplore supervised and unsupervised kernel loss functions' impact on encoding\ncircuit optimisation and evaluate them on diverse datasets for binary and\nmultiple-class scenarios. Benchmarking against classical and quantum kernels\nreveals GA-generated circuits matching or surpassing standard techniques. We\nanalyse the relationship between test accuracy and quantum kernel entropy, with\nresults indicating a positive correlation. Our automated framework reduces\ntrial and error, and enables improved QSVM based machine learning performance\nfor finance, healthcare, and materials science applications.",
            "author": [
                "Floyd M. Creevey",
                "Jamie A. Heredge",
                "Martin E. Sevior",
                "Lloyd C. L. Hollenberg"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01562v1",
                "http://arxiv.org/pdf/2312.01562v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01560v1",
            "title": "RaftGP: Random Fast Graph Partitioning",
            "updated": "2023-12-04T01:16:14Z",
            "published": "2023-12-04T01:16:14Z",
            "summary": "Graph partitioning (GP), a.k.a. community detection, is a classic problem\nthat divides the node set of a graph into densely-connected blocks. Following\nprior work on the IEEE HPEC Graph Challenge benchmark and recent advances in\ngraph machine learning, we propose a novel RAndom FasT Graph Partitioning\n(RaftGP) method based on an efficient graph embedding scheme. It uses the\nGaussian random projection to extract community-preserving features from\nclassic GP objectives. These features are fed into a graph neural network (GNN)\nto derive low-dimensional node embeddings. Surprisingly, our experiments\ndemonstrate that a randomly initialized GNN even without training is enough for\nRaftGP to derive informative community-preserving embeddings and support\nhigh-quality GP. To enable the derived embeddings to tackle GP, we introduce a\nhierarchical model selection algorithm that simultaneously determines the\nnumber of blocks and the corresponding GP result. We evaluate RaftGP on the\nGraph Challenge benchmark and compare the performance with five baselines,\nwhere our method can achieve a better trade-off between quality and efficiency.\nIn particular, compared to the baseline algorithm of the IEEE HPEC Graph\nChallenge, our method is 6.68x -- 23.9x faster on graphs with 1E3 -- 5E4 nodes\nand at least 64.5x faster on larger (1E5 node) graphs on which the baseline\ntakes more than 1E4 seconds. Our method achieves better accuracy on all test\ncases. We also develop a new graph generator to address some limitations of the\noriginal generator in the benchmark.",
            "author": [
                "Yu Gao",
                "Meng Qin",
                "Yibin Ding",
                "Li Zeng",
                "Chaorui Zhang",
                "Weixi Zhang",
                "Wei Han",
                "Rongqian Zhao",
                "Bo Bai"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01560v1",
                "http://arxiv.org/pdf/2312.01560v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01558v1",
            "title": "Hyperspectral Image Compression Using Sampling and Implicit Neural\n  Representations",
            "updated": "2023-12-04T01:10:04Z",
            "published": "2023-12-04T01:10:04Z",
            "summary": "Hyperspectral images, which record the electromagnetic spectrum for a pixel\nin the image of a scene, often store hundreds of channels per pixel and contain\nan order of magnitude more information than a similarly-sized RBG color image.\nConsequently, concomitant with the decreasing cost of capturing these images,\nthere is a need to develop efficient techniques for storing, transmitting, and\nanalyzing hyperspectral images. This paper develops a method for hyperspectral\nimage compression using implicit neural representations where a multilayer\nperceptron network F with sinusoidal activation functions \"learns\" to map pixel\nlocations to pixel intensities for a given hyperspectral image I. F thus acts\nas a compressed encoding of this image, and the original image is reconstructed\nby evaluating F at each pixel location. We use a sampling method with two\nfactors: window size and sampling rate to reduce the compression time. We have\nevaluated our method on four benchmarks -- Indian Pines, Jasper Ridge, Pavia\nUniversity, and Cuprite using PSNR and SSIM -- and we show that the proposed\nmethod achieves better compression than JPEG, JPEG2000, and PCA-DCT at low\nbitrates. Besides, we compare our results with the learning-based methods like\nPCA+JPEG2000, FPCA+JPEG2000, 3D DCT, 3D DWT+SVR, and WSRC and show the\ncorresponding results in the \"Compression Results\" section. We also show that\nour methods with sampling achieve better speed and performance than our method\nwithout sampling.",
            "author": [
                "Shima Rezasoltani",
                "Faisal Z. Qureshi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01558v1",
                "http://arxiv.org/pdf/2312.01558v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01555v1",
            "title": "Explainable AI is Responsible AI: How Explainability Creates Trustworthy\n  and Socially Responsible Artificial Intelligence",
            "updated": "2023-12-04T00:54:04Z",
            "published": "2023-12-04T00:54:04Z",
            "summary": "Artificial intelligence (AI) has been clearly established as a technology\nwith the potential to revolutionize fields from healthcare to finance - if\ndeveloped and deployed responsibly. This is the topic of responsible AI, which\nemphasizes the need to develop trustworthy AI systems that minimize bias,\nprotect privacy, support security, and enhance transparency and accountability.\nExplainable AI (XAI) has been broadly considered as a building block for\nresponsible AI (RAI), with most of the literature considering it as a solution\nfor improved transparency. This work proposes that XAI and responsible AI are\nsignificantly more deeply entwined. In this work, we explore state-of-the-art\nliterature on RAI and XAI technologies. Based on our findings, we demonstrate\nthat XAI can be utilized to ensure fairness, robustness, privacy, security, and\ntransparency in a wide range of contexts. Our findings lead us to conclude that\nXAI is an essential foundation for every pillar of RAI.",
            "author": [
                "Stephanie Baker",
                "Wei Xiang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01555v1",
                "http://arxiv.org/pdf/2312.01555v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CY",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01552v1",
            "title": "The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context\n  Learning",
            "updated": "2023-12-04T00:46:11Z",
            "published": "2023-12-04T00:46:11Z",
            "summary": "The alignment tuning process of large language models (LLMs) typically\ninvolves instruction learning through supervised fine-tuning (SFT) and\npreference tuning via reinforcement learning from human feedback (RLHF). A\nrecent study, LIMA (Zhou et al. 2023), shows that using merely 1K examples for\nSFT can achieve significant alignment performance as well, suggesting that the\neffect of alignment tuning might be \"superficial.\" This raises questions about\nhow exactly the alignment tuning transforms a base LLM.\n  We analyze the effect of alignment tuning by examining the token distribution\nshift between base LLMs and their aligned counterpart. Our findings reveal that\nbase LLMs and their alignment-tuned versions perform nearly identically in\ndecoding on the majority of token positions. Most distribution shifts occur\nwith stylistic tokens. These direct evidence strongly supports the Superficial\nAlignment Hypothesis suggested by LIMA.\n  Based on these findings, we rethink the alignment of LLMs by posing the\nresearch question: how effectively can we align base LLMs without SFT or RLHF?\nTo address this, we introduce a simple, tuning-free alignment method, URIAL.\nURIAL achieves effective alignment purely through in-context learning (ICL)\nwith base LLMs, requiring as few as three constant stylistic examples and a\nsystem prompt. We conduct a fine-grained and interpretable evaluation on a\ndiverse set of examples, named JUST-EVAL-INSTRUCT. Results demonstrate that\nbase LLMs with URIAL can match or even surpass the performance of LLMs aligned\nwith SFT or SFT+RLHF. We show that the gap between tuning-free and tuning-based\nalignment methods can be significantly reduced through strategic prompting and\nICL. Our findings on the superficial nature of alignment tuning and results\nwith URIAL suggest that deeper analysis and theoretical understanding of\nalignment is crucial to future LLM research.",
            "author": [
                "Bill Yuchen Lin",
                "Abhilasha Ravichander",
                "Ximing Lu",
                "Nouha Dziri",
                "Melanie Sclar",
                "Khyathi Chandu",
                "Chandra Bhagavatula",
                "Yejin Choi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01552v1",
                "http://arxiv.org/pdf/2312.01552v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01550v2",
            "title": "Using human and robot synthetic data for training smart hand tools",
            "updated": "2023-12-05T16:44:06Z",
            "published": "2023-12-04T00:42:25Z",
            "summary": "The future of work does not require a choice between human and robot. Aside\nfrom explicit human-robot collaboration, robotics can play an increasingly\nimportant role in helping train workers as well as the tools they may use,\nespecially in complex tasks that may be difficult to automate or effectively\nroboticize. This paper introduces a form of smart tool for use by human workers\nand shows how training the tool for task recognition, one of the key\nrequirements, can be accomplished. Machine learning (ML) with purely\nhuman-based data can be extremely laborious and time-consuming. First, we show\nhow data synthetically-generated by a robot can be leveraged in the ML training\nprocess. Later, we demonstrate how fine-tuning ML models for individual\nphysical tasks and workers can significantly scale up the benefits of using ML\nto provide this feedback. Experimental results show the effectiveness and\nscalability of our approach, as we test data size versus accuracy. Smart hand\ntools of the type introduced here can provide insights and real-time analytics\non efficient and safe tool usage and operation, thereby enhancing human\nparticipation and skill in a wide range of work environments. Using robotic\nplatforms to help train smart tools will be essential, particularly given the\ndiverse types of applications for which smart hand tools are envisioned for\nhuman use.",
            "author": [
                "Jose Bendana",
                "Sundar Sripada V. S.",
                "Carlos D. Salazar",
                "Sandeep Chinchali",
                "Raul G. Longoria"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01550v2",
                "http://arxiv.org/pdf/2312.01550v2"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01547v1",
            "title": "Near-Optimal Algorithms for Gaussians with Huber Contamination: Mean\n  Estimation and Linear Regression",
            "updated": "2023-12-04T00:31:16Z",
            "published": "2023-12-04T00:31:16Z",
            "summary": "We study the fundamental problems of Gaussian mean estimation and linear\nregression with Gaussian covariates in the presence of Huber contamination. Our\nmain contribution is the design of the first sample near-optimal and almost\nlinear-time algorithms with optimal error guarantees for both of these\nproblems. Specifically, for Gaussian robust mean estimation on $\\mathbb{R}^d$\nwith contamination parameter $\\epsilon \\in (0, \\epsilon_0)$ for a small\nabsolute constant $\\epsilon_0$, we give an algorithm with sample complexity $n\n= \\tilde{O}(d/\\epsilon^2)$ and almost linear runtime that approximates the\ntarget mean within $\\ell_2$-error $O(\\epsilon)$. This improves on prior work\nthat achieved this error guarantee with polynomially suboptimal sample and time\ncomplexity. For robust linear regression, we give the first algorithm with\nsample complexity $n = \\tilde{O}(d/\\epsilon^2)$ and almost linear runtime that\napproximates the target regressor within $\\ell_2$-error $O(\\epsilon)$. This is\nthe first polynomial sample and time algorithm achieving the optimal error\nguarantee, answering an open question in the literature. At the technical\nlevel, we develop a methodology that yields almost-linear time algorithms for\nmulti-directional filtering that may be of broader interest.",
            "author": [
                "Ilias Diakonikolas",
                "Daniel M. Kane",
                "Ankit Pensia",
                "Thanasis Pittas"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01547v1",
                "http://arxiv.org/pdf/2312.01547v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01546v1",
            "title": "Learning Channel Capacity with Neural Mutual Information Estimator Based\n  on Message Importance Measure",
            "updated": "2023-12-04T00:26:00Z",
            "published": "2023-12-04T00:26:00Z",
            "summary": "Channel capacity estimation plays a crucial role in beyond 5G intelligent\ncommunications. Despite its significance, this task is challenging for a\nmajority of channels, especially for the complex channels not modeled as the\nwell-known typical ones. Recently, neural networks have been used in mutual\ninformation estimation and optimization. They are particularly considered as\nefficient tools for learning channel capacity. In this paper, we propose a\ncooperative framework to simultaneously estimate channel capacity and design\nthe optimal codebook. First, we will leverage MIM-based GAN, a novel form of\ngenerative adversarial network (GAN) using message importance measure (MIM) as\nthe information distance, into mutual information estimation, and develop a\nnovel method, named MIM-based mutual information estimator (MMIE). Then, we\ndesign a generalized cooperative framework for channel capacity learning, in\nwhich a generator is regarded as an encoder producing the channel input, while\na discriminator is the mutual information estimator that assesses the\nperformance of the generator. Through the adversarial training, the generator\nautomatically learns the optimal codebook and the discriminator estimates the\nchannel capacity. Numerical experiments will demonstrate that compared with\nseveral conventional estimators, the MMIE achieves state-of-the-art performance\nin terms of accuracy and stability.",
            "author": [
                "Zhefan Li",
                "Rui She",
                "Pingyi Fan",
                "Chenghui Peng",
                "Khaled B. Letaief"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01546v1",
                "http://arxiv.org/pdf/2312.01546v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "eess.SP",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02225v1",
            "title": "Digital Histopathology with Graph Neural Networks: Concepts and\n  Explanations for Clinicians",
            "updated": "2023-12-04T00:20:50Z",
            "published": "2023-12-04T00:20:50Z",
            "summary": "To address the challenge of the ``black-box\" nature of deep learning in\nmedical settings, we combine GCExplainer - an automated concept discovery\nsolution - along with Logic Explained Networks to provide global explanations\nfor Graph Neural Networks. We demonstrate this using a generally applicable\ngraph construction and classification pipeline, involving panoptic segmentation\nwith HoVer-Net and cancer prediction with Graph Convolution Networks. By\ntraining on H&E slides of breast cancer, we show promising results in offering\nexplainable and trustworthy AI tools for clinicians.",
            "author": [
                "Alessandro Farace di Villaforesta",
                "Lucie Charlotte Magister",
                "Pietro Barbiero",
                "Pietro Li\u00f2"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02225v1",
                "http://arxiv.org/pdf/2312.02225v1"
            ],
            "primary_category": "physics.med-ph",
            "category": [
                "physics.med-ph",
                "cs.CV",
                "cs.LG",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01544v1",
            "title": "KEEC: Embed to Control on An Equivariant Geometry",
            "updated": "2023-12-04T00:11:27Z",
            "published": "2023-12-04T00:11:27Z",
            "summary": "This paper investigates how representation learning can enable optimal\ncontrol in unknown and complex dynamics, such as chaotic and non-linear\nsystems, without relying on prior domain knowledge of the dynamics. The core\nidea is to establish an equivariant geometry that is diffeomorphic to the\nmanifold defined by a dynamical system and to perform optimal control within\nthis corresponding geometry, which is a non-trivial task. To address this\nchallenge, Koopman Embed to Equivariant Control (KEEC) is introduced for model\nlearning and control. Inspired by Lie theory, KEEC begins by learning a\nnon-linear dynamical system defined on a manifold and embedding trajectories\ninto a Lie group. Subsequently, KEEC formulates an equivariant value function\nequation in reinforcement learning on the equivariant geometry, ensuring an\ninvariant effect as the value function on the original manifold. By deriving\nanalytical-form optimal actions on the equivariant value function, KEEC\ntheoretically achieves quadratic convergence for the optimal equivariant value\nfunction by leveraging the differential information on the equivariant\ngeometry. The effectiveness of KEEC is demonstrated in challenging dynamical\nsystems, including chaotic ones like Lorenz-63. Notably, our findings indicate\nthat isometric and isomorphic loss functions, ensuring the compactness and\nsmoothness of geometry, outperform loss functions without these properties.",
            "author": [
                "Xiaoyuan Cheng",
                "Yiming Yang",
                "Wei Jiang",
                "Yukun Hu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01544v1",
                "http://arxiv.org/pdf/2312.01544v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01541v1",
            "title": "Revisiting Non-separable Binary Classification and its Applications in\n  Anomaly Detection",
            "updated": "2023-12-03T23:59:03Z",
            "published": "2023-12-03T23:59:03Z",
            "summary": "The inability to linearly classify XOR has motivated much of deep learning.\nWe revisit this age-old problem and show that linear classification of XOR is\nindeed possible. Instead of separating data between halfspaces, we propose a\nslightly different paradigm, equality separation, that adapts the SVM objective\nto distinguish data within or outside the margin. Our classifier can then be\nintegrated into neural network pipelines with a smooth approximation. From its\nproperties, we intuit that equality separation is suitable for anomaly\ndetection. To formalize this notion, we introduce closing numbers, a\nquantitative measure on the capacity for classifiers to form closed decision\nregions for anomaly detection. Springboarding from this theoretical connection\nbetween binary classification and anomaly detection, we test our hypothesis on\nsupervised anomaly detection experiments, showing that equality separation can\ndetect both seen and unseen anomalies.",
            "author": [
                "Matthew Lau",
                "Ismaila Seck",
                "Athanasios P Meliopoulos",
                "Wenke Lee",
                "Eugene Ndiaye"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01541v1",
                "http://arxiv.org/pdf/2312.01541v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "stat.ML",
                "68T37 (Primary), 68T07 (Secondary)",
                "I.2.6; I.5.1"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01540v1",
            "title": "Robust Computer Vision in an Ever-Changing World: A Survey of Techniques\n  for Tackling Distribution Shifts",
            "updated": "2023-12-03T23:40:12Z",
            "published": "2023-12-03T23:40:12Z",
            "summary": "AI applications are becoming increasingly visible to the general public.\nThere is a notable gap between the theoretical assumptions researchers make\nabout computer vision models and the reality those models face when deployed in\nthe real world. One of the critical reasons for this gap is a challenging\nproblem known as distribution shift. Distribution shifts tend to vary with\ncomplexity of the data, dataset size, and application type. In our paper, we\ndiscuss the identification of such a prominent gap, exploring the concept of\ndistribution shift and its critical significance. We provide an in-depth\noverview of various types of distribution shifts, elucidate their distinctions,\nand explore techniques within the realm of the data-centric domain employed to\naddress them. Distribution shifts can occur during every phase of the machine\nlearning pipeline, from the data collection stage to the stage of training a\nmachine learning model to the stage of final model deployment. As a result, it\nraises concerns about the overall robustness of the machine learning techniques\nfor computer vision applications that are deployed publicly for consumers.\nDifferent deep learning models each tailored for specific type of data and\ntasks, architectural pipelines; highlighting how variations in data\npreprocessing and feature extraction can impact robustness., data augmentation\nstrategies (e.g. geometric, synthetic and learning-based); demonstrating their\nrole in enhancing model generalization, and training mechanisms (e.g. transfer\nlearning, zero-shot) fall under the umbrella of data-centric methods. Each of\nthese components form an integral part of the neural-network we analyze\ncontributing uniquely to strengthening model robustness against distribution\nshifts. We compare and contrast numerous AI models that are built for\nmitigating shifts in hidden stratification and spurious correlations, ...",
            "author": [
                "Eashan Adhikarla",
                "Kai Zhang",
                "Jun Yu",
                "Lichao Sun",
                "John Nicholson",
                "Brian D. Davison"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01540v1",
                "http://arxiv.org/pdf/2312.01540v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01538v1",
            "title": "Recurrent Distance-Encoding Neural Networks for Graph Representation\n  Learning",
            "updated": "2023-12-03T23:36:16Z",
            "published": "2023-12-03T23:36:16Z",
            "summary": "Graph neural networks based on iterative one-hop message passing have been\nshown to struggle in harnessing information from distant nodes effectively.\nConversely, graph transformers allow each node to attend to all other nodes\ndirectly, but suffer from high computational complexity and have to rely on\nad-hoc positional encoding to bake in the graph inductive bias. In this paper,\nwe propose a new architecture to reconcile these challenges. Our approach stems\nfrom the recent breakthroughs in long-range modeling provided by deep\nstate-space models on sequential data: for a given target node, our model\naggregates other nodes by their shortest distances to the target and uses a\nparallelizable linear recurrent network over the chain of distances to provide\na natural encoding of its neighborhood structure. With no need for positional\nencoding, we empirically show that the performance of our model is highly\ncompetitive compared with that of state-of-the-art graph transformers on\nvarious benchmarks, at a drastically reduced computational complexity. In\naddition, we show that our model is theoretically more expressive than one-hop\nmessage passing neural networks.",
            "author": [
                "Yuhui Ding",
                "Antonio Orvieto",
                "Bobby He",
                "Thomas Hofmann"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01538v1",
                "http://arxiv.org/pdf/2312.01538v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.NE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01537v1",
            "title": "Unlocking the Potential of Federated Learning: The Symphony of Dataset\n  Distillation via Deep Generative Latents",
            "updated": "2023-12-03T23:30:48Z",
            "published": "2023-12-03T23:30:48Z",
            "summary": "Data heterogeneity presents significant challenges for federated learning\n(FL). Recently, dataset distillation techniques have been introduced, and\nperformed at the client level, to attempt to mitigate some of these challenges.\nIn this paper, we propose a highly efficient FL dataset distillation framework\non the server side, significantly reducing both the computational and\ncommunication demands on local devices while enhancing the clients' privacy.\nUnlike previous strategies that perform dataset distillation on local devices\nand upload synthetic data to the server, our technique enables the server to\nleverage prior knowledge from pre-trained deep generative models to synthesize\nessential data representations from a heterogeneous model architecture. This\nprocess allows local devices to train smaller surrogate models while enabling\nthe training of a larger global model on the server, effectively minimizing\nresource utilization. We substantiate our claim with a theoretical analysis,\ndemonstrating the asymptotic resemblance of the process to the hypothetical\nideal of completely centralized training on a heterogeneous dataset. Empirical\nevidence from our comprehensive experiments indicates our method's superiority,\ndelivering an accuracy enhancement of up to 40% over non-dataset-distillation\ntechniques in highly heterogeneous FL contexts, and surpassing existing\ndataset-distillation methods by 18%. In addition to the high accuracy, our\nframework converges faster than the baselines because rather than the server\ntrains on several sets of heterogeneous data distributions, it trains on a\nmulti-modal distribution. Our code is available at\nhttps://github.com/FedDG23/FedDG-main.git",
            "author": [
                "Yuqi Jia",
                "Saeed Vahidian",
                "Jingwei Sun",
                "Jianyi Zhang",
                "Vyacheslav Kungurtsev",
                "Neil Zhenqiang Gong",
                "Yiran Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01537v1",
                "http://arxiv.org/pdf/2312.01537v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01536v1",
            "title": "CalliPaint: Chinese Calligraphy Inpainting with Diffusion Model",
            "updated": "2023-12-03T23:29:59Z",
            "published": "2023-12-03T23:29:59Z",
            "summary": "Chinese calligraphy can be viewed as a unique form of visual art. Recent\nadvancements in computer vision hold significant potential for the future\ndevelopment of generative models in the realm of Chinese calligraphy.\nNevertheless, methods of Chinese calligraphy inpainting, which can be\neffectively used in the art and education fields, remain relatively unexplored.\nIn this paper, we introduce a new model that harnesses recent advancements in\nboth Chinese calligraphy generation and image inpainting. We demonstrate that\nour proposed model CalliPaint can produce convincing Chinese calligraphy.",
            "author": [
                "Qisheng Liao",
                "Zhinuo Wang",
                "Muhammad Abdul-Mageed",
                "Gus Xia"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01536v1",
                "http://arxiv.org/pdf/2312.01536v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01530v2",
            "title": "Evaluation of Active Feature Acquisition Methods for Time-varying\n  Feature Settings",
            "updated": "2023-12-07T18:47:53Z",
            "published": "2023-12-03T23:08:29Z",
            "summary": "Machine learning methods often assume input features are available at no\ncost. However, in domains like healthcare, where acquiring features could be\nexpensive or harmful, it is necessary to balance a feature's acquisition cost\nagainst its predictive value. The task of training an AI agent to decide which\nfeatures to acquire is called active feature acquisition (AFA). By deploying an\nAFA agent, we effectively alter the acquisition strategy and trigger a\ndistribution shift. To safely deploy AFA agents under this distribution shift,\nwe present the problem of active feature acquisition performance evaluation\n(AFAPE). We examine AFAPE under i) a no direct effect (NDE) assumption, stating\nthat acquisitions don't affect the underlying feature values; and ii) a no\nunobserved confounding (NUC) assumption, stating that retrospective feature\nacquisition decisions were only based on observed features. We show that one\ncan apply offline reinforcement learning under the NUC assumption and missing\ndata methods under the NDE assumption. When NUC and NDE hold, we propose a\nnovel semi-offline reinforcement learning framework, which requires a weaker\npositivity assumption and yields more data-efficient estimators. We introduce\nthree novel estimators: a direct method (DM), an inverse probability weighting\n(IPW), and a double reinforcement learning (DRL) estimator.",
            "author": [
                "Henrik von Kleist",
                "Alireza Zamanian",
                "Ilya Shpitser",
                "Narges Ahmidi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01530v2",
                "http://arxiv.org/pdf/2312.01530v2"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01529v2",
            "title": "T3D: Towards 3D Medical Image Understanding through Vision-Language\n  Pre-training",
            "updated": "2023-12-05T09:01:07Z",
            "published": "2023-12-03T23:03:22Z",
            "summary": "Expert annotation of 3D medical image for downstream analysis is\nresource-intensive, posing challenges in clinical applications. Visual\nself-supervised learning (vSSL), though effective for learning visual\ninvariance, neglects the incorporation of domain knowledge from medicine. To\nincorporate medical knowledge into visual representation learning,\nvision-language pre-training (VLP) has shown promising results in 2D image.\nHowever, existing VLP approaches become generally impractical when applied to\nhigh-resolution 3D medical images due to GPU hardware constraints and the\npotential loss of critical details caused by downsampling, which is the\nintuitive solution to hardware constraints. To address the above limitations,\nwe introduce T3D, the first VLP framework designed for high-resolution 3D\nmedical images. T3D incorporates two text-informed pretext tasks:\n(\\lowerromannumeral{1}) text-informed contrastive learning;\n(\\lowerromannumeral{2}) text-informed image restoration. These tasks focus on\nlearning 3D visual representations from high-resolution 3D medical images and\nintegrating clinical knowledge from radiology reports, without distorting\ninformation through forced alignment of downsampled volumes with detailed\nanatomical text. Trained on a newly curated large-scale dataset of 3D medical\nimages and radiology reports, T3D significantly outperforms current vSSL\nmethods in tasks like organ and tumor segmentation, as well as disease\nclassification. This underlines T3D's potential in representation learning for\n3D medical image analysis. All data and code will be available upon acceptance.",
            "author": [
                "Che Liu",
                "Cheng Ouyang",
                "Yinda Chen",
                "Cesar C\u00e9sar Quilodr\u00e1n-Casas",
                "Lei Ma",
                "Jie Fu",
                "Yike Guo",
                "Anand Shah",
                "Wenjia Bai",
                "Rossella Arcucci"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01529v2",
                "http://arxiv.org/pdf/2312.01529v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.CL",
                "cs.LG",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01523v1",
            "title": "SymNoise: Advancing Language Model Fine-tuning with Symmetric Noise",
            "updated": "2023-12-03T22:44:58Z",
            "published": "2023-12-03T22:44:58Z",
            "summary": "In this paper, we introduce a novel fine-tuning technique for language\nmodels, which involves incorporating symmetric noise into the embedding\nprocess. This method aims to enhance the model's function by more stringently\nregulating its local curvature, demonstrating superior performance over the\ncurrent method, NEFTune. When fine-tuning the LLaMA-2-7B model using Alpaca,\nstandard techniques yield a 29.79% score on AlpacaEval. However, our approach,\nSymNoise, increases this score significantly to 69.04%, using symmetric noisy\nembeddings. This is a 6.7% improvement over the state-of-the-art method,\nNEFTune~(64.69%). Furthermore, when tested on various models and stronger\nbaseline instruction datasets, such as Evol-Instruct, ShareGPT, OpenPlatypus,\nSymNoise consistently outperforms NEFTune. The current literature, including\nNEFTune, has underscored the importance of more in-depth research into the\napplication of noise-based strategies in the fine-tuning of language models.\nOur approach, SymNoise, is another significant step towards this direction,\nshowing notable improvement over the existing state-of-the-art method.",
            "author": [
                "Arjun Singh",
                "Abhay Kumar Yadav"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01523v1",
                "http://arxiv.org/pdf/2312.01523v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01522v1",
            "title": "G2D: From Global to Dense Radiography Representation Learning via\n  Vision-Language Pre-training",
            "updated": "2023-12-03T22:44:04Z",
            "published": "2023-12-03T22:44:04Z",
            "summary": "Recently, medical vision-language pre-training (VLP) has reached substantial\nprogress to learn global visual representation from medical images and their\npaired radiology reports. However, medical imaging tasks in real world usually\nrequire finer granularity in visual features. These tasks include visual\nlocalization tasks (e.g., semantic segmentation, object detection) and visual\ngrounding task. Yet, current medical VLP methods face challenges in learning\nthese fine-grained features, as they primarily focus on brute-force alignment\nbetween image patches and individual text tokens for local visual feature\nlearning, which is suboptimal for downstream dense prediction tasks. In this\nwork, we propose a new VLP framework, named \\textbf{G}lobal to \\textbf{D}ense\nlevel representation learning (G2D) that achieves significantly improved\ngranularity and more accurate grounding for the learned features, compared to\nexisting medical VLP approaches. In particular, G2D learns dense and\nsemantically-grounded image representations via a pseudo segmentation task\nparallel with the global vision-language alignment. Notably, generating pseudo\nsegmentation targets does not incur extra trainable parameters: they are\nobtained on the fly during VLP with a parameter-free processor. G2D achieves\nsuperior performance across 6 medical imaging tasks and 25 diseases,\nparticularly in semantic segmentation, which necessitates fine-grained,\nsemantically-grounded image features. In this task, G2D surpasses peer models\neven when fine-tuned with just 1\\% of the training data, compared to the 100\\%\nused by these models. The code will be released upon acceptance.",
            "author": [
                "Che Liu",
                "Cheng Ouyang",
                "Sibo Cheng",
                "Anand Shah",
                "Wenjia Bai",
                "Rossella Arcucci"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01522v1",
                "http://arxiv.org/pdf/2312.01522v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01518v1",
            "title": "Analyzing State-Level Longevity Trends with the U.S. Mortality Database",
            "updated": "2023-12-03T22:23:09Z",
            "published": "2023-12-03T22:23:09Z",
            "summary": "We investigate state-level age-specific mortality trends based on the United\nStates Mortality Database (USMDB) published by the Human Mortality Database. In\ntandem with looking at the longevity experience across the 51 states, we also\nconsider a collection of socio-demographic, economic and educational covariates\nthat correlate with mortality trends. To obtain smoothed mortality surfaces for\neach state, we implement the machine learning framework of Multi-Output\nGaussian Process regression (Huynh \\& Ludkovski 2021) on targeted groupings of\n3--6 states. Our detailed exploratory analysis shows that the mortality\nexperience is highly inhomogeneous across states in terms of respective Age\nstructures. We moreover document multiple divergent trends between best and\nworst states, between Females and Males, and between younger and older Ages.\nThe comparisons across the 50+ fitted models offer opportunities for rich\ninsights about drivers of mortality in the U.S. and are visualized through\nnumerous figures and an online interactive dashboard.",
            "author": [
                "Mike Ludkovski",
                "Doris Padilla"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01518v1",
                "http://arxiv.org/pdf/2312.01518v1"
            ],
            "primary_category": "stat.AP",
            "category": [
                "stat.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01515v1",
            "title": "Bigger is not Always Better: The Effect of Context Size on Speech\n  Pre-Training",
            "updated": "2023-12-03T22:08:54Z",
            "published": "2023-12-03T22:08:54Z",
            "summary": "It has been generally assumed in the automatic speech recognition (ASR)\nliterature that it is better for models to have access to wider context\nwindows. Yet, many of the potential reasons this might be true in the\nsupervised setting do not necessarily transfer over to the case of unsupervised\nlearning. We investigate how much context is necessary to achieve high-quality\npre-trained acoustic models using self-supervised learning. We principally\ninvestigate contrastive predictive coding (CPC), which we adapt to be able to\nprecisely control the amount of context visible to the model during training\nand inference. We find that phone discriminability in the resulting model\nrepresentations peaks at around 40~ms of preceding context, and that having too\nmuch context (beyond around 320 ms) substantially degrades the quality of the\nrepresentations. Surprisingly, we find that this pattern also transfers to\nsupervised ASR when the pre-trained representations are used as frozen input\nfeatures. Our results point to potential changes in the design of current\nupstream architectures to better facilitate a variety of downstream tasks.",
            "author": [
                "Sean Robertson",
                "Ewan Dunbar"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01515v1",
                "http://arxiv.org/pdf/2312.01515v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.SD",
                "eess.AS",
                "I.2.7"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02224v1",
            "title": "Tracing Hyperparameter Dependencies for Model Parsing via Learnable\n  Graph Pooling Network",
            "updated": "2023-12-03T22:05:05Z",
            "published": "2023-12-03T22:05:05Z",
            "summary": "Model Parsing defines the research task of predicting hyperparameters of the\ngenerative model (GM), given a generated image as input. Since a diverse set of\nhyperparameters is jointly employed by the generative model, and dependencies\noften exist among them, it is crucial to learn these hyperparameter\ndependencies for the improved model parsing performance. To explore such\nimportant dependencies, we propose a novel model parsing method called\nLearnable Graph Pooling Network (LGPN). Specifically, we transform model\nparsing into a graph node classification task, using graph nodes and edges to\nrepresent hyperparameters and their dependencies, respectively. Furthermore,\nLGPN incorporates a learnable pooling-unpooling mechanism tailored to model\nparsing, which adaptively learns hyperparameter dependencies of GMs used to\ngenerate the input image. We also extend our proposed method to CNN-generated\nimage detection and coordinate attacks detection. Empirically, we achieve\nstate-of-the-art results in model parsing and its extended applications,\nshowing the effectiveness of our method. Our source code are available.",
            "author": [
                "Xiao Guo",
                "Vishal Asnani",
                "Sijia Liu",
                "Xiaoming Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02224v1",
                "http://arxiv.org/pdf/2312.02224v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01507v1",
            "title": "Learn2Extend: Extending sequences by retaining their statistical\n  properties with mixture models",
            "updated": "2023-12-03T21:05:50Z",
            "published": "2023-12-03T21:05:50Z",
            "summary": "This paper addresses the challenge of extending general finite sequences of\nreal numbers within a subinterval of the real line, maintaining their inherent\nstatistical properties by employing machine learning. Our focus lies on\npreserving the gap distribution and pair correlation function of these point\nsets. Leveraging advancements in deep learning applied to point processes, this\npaper explores the use of an auto-regressive \\textit{Sequence Extension Mixture\nModel} (SEMM) for extending finite sequences, by estimating directly the\nconditional density, instead of the intensity function. We perform comparative\nexperiments on multiple types of point processes, including Poisson, locally\nattractive, and locally repelling sequences, and we perform a case study on the\nprediction of Riemann $\\zeta$ function zeroes. The results indicate that the\nproposed mixture model outperforms traditional neural network architectures in\nsequence extension with the retention of statistical properties. Given this\nmotivation, we showcase the capabilities of a mixture model to extend\nsequences, maintaining specific statistical properties, i.e. the gap\ndistribution, and pair correlation indicators.",
            "author": [
                "Dimitris Vartziotis",
                "George Dasoulas",
                "Florian Pausinger"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01507v1",
                "http://arxiv.org/pdf/2312.01507v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03002v1",
            "title": "The mechanistic basis of data dependence and abrupt learning in an\n  in-context classification task",
            "updated": "2023-12-03T20:53:41Z",
            "published": "2023-12-03T20:53:41Z",
            "summary": "Transformer models exhibit in-context learning: the ability to accurately\npredict the response to a novel query based on illustrative examples in the\ninput sequence. In-context learning contrasts with traditional in-weights\nlearning of query-output relationships. What aspects of the training data\ndistribution and architecture favor in-context vs in-weights learning? Recent\nwork has shown that specific distributional properties inherent in language,\nsuch as burstiness, large dictionaries and skewed rank-frequency distributions,\ncontrol the trade-off or simultaneous appearance of these two forms of\nlearning. We first show that these results are recapitulated in a minimal\nattention-only network trained on a simplified dataset. In-context learning\n(ICL) is driven by the abrupt emergence of an induction head, which\nsubsequently competes with in-weights learning. By identifying progress\nmeasures that precede in-context learning and targeted experiments, we\nconstruct a two-parameter model of an induction head which emulates the full\ndata distributional dependencies displayed by the attention-based network. A\nphenomenological model of induction head formation traces its abrupt emergence\nto the sequential learning of three nested logits enabled by an intrinsic\ncurriculum. We propose that the sharp transitions in attention-based networks\narise due to a specific chain of multi-layer operations necessary to achieve\nICL, which is implemented by nested nonlinearities sequentially learned during\ntraining.",
            "author": [
                "Gautam Reddy"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03002v1",
                "http://arxiv.org/pdf/2312.03002v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01504v1",
            "title": "Effectively Fine-tune to Improve Large Multimodal Models for Radiology\n  Report Generation",
            "updated": "2023-12-03T20:42:38Z",
            "published": "2023-12-03T20:42:38Z",
            "summary": "Writing radiology reports from medical images requires a high level of domain\nexpertise. It is time-consuming even for trained radiologists and can be\nerror-prone for inexperienced radiologists. It would be appealing to automate\nthis task by leveraging generative AI, which has shown drastic progress in\nvision and language understanding. In particular, Large Language Models (LLM)\nhave demonstrated impressive capabilities recently and continued to set new\nstate-of-the-art performance on almost all natural language tasks. While many\nhave proposed architectures to combine vision models with LLMs for multimodal\ntasks, few have explored practical fine-tuning strategies. In this work, we\nproposed a simple yet effective two-stage fine-tuning protocol to align visual\nfeatures to LLM's text embedding space as soft visual prompts. Our framework\nwith OpenLLaMA-7B achieved state-of-the-art level performance without\ndomain-specific pretraining. Moreover, we provide detailed analyses of soft\nvisual prompts and attention mechanisms, shedding light on future research\ndirections.",
            "author": [
                "Yuzhe Lu",
                "Sungmin Hong",
                "Yash Shah",
                "Panpan Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01504v1",
                "http://arxiv.org/pdf/2312.01504v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01502v1",
            "title": "Normed Spaces for Graph Embedding",
            "updated": "2023-12-03T20:21:08Z",
            "published": "2023-12-03T20:21:08Z",
            "summary": "Theoretical results from discrete geometry suggest that normed spaces can\nabstractly embed finite metric spaces with surprisingly low theoretical bounds\non distortion in low dimensions. In this paper, inspired by this theoretical\ninsight, we highlight normed spaces as a more flexible and computationally\nefficient alternative to several popular Riemannian manifolds for learning\ngraph embeddings. Normed space embeddings significantly outperform several\npopular manifolds on a large range of synthetic and real-world graph\nreconstruction benchmark datasets while requiring significantly fewer\ncomputational resources. We also empirically verify the superiority of normed\nspace embeddings on growing families of graphs associated with negative, zero,\nand positive curvature, further reinforcing the flexibility of normed spaces in\ncapturing diverse graph structures as graph sizes increase. Lastly, we\ndemonstrate the utility of normed space embeddings on two applied graph\nembedding tasks, namely, link prediction and recommender systems. Our work\nhighlights the potential of normed spaces for geometric graph representation\nlearning, raises new research questions, and offers a valuable tool for\nexperimental mathematics in the field of finite metric space embeddings. We\nmake our code and data publically available.",
            "author": [
                "Diaaeldin Taha",
                "Wei Zhao",
                "J. Maxwell Riestenberg",
                "Michael Strube"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01502v1",
                "http://arxiv.org/pdf/2312.01502v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01499v1",
            "title": "Towards Decentralized Task Offloading and Resource Allocation in\n  User-Centric Mobile Edge Computing",
            "updated": "2023-12-03T20:07:22Z",
            "published": "2023-12-03T20:07:22Z",
            "summary": "In the traditional cellular-based mobile edge computing (MEC), users at the\nedge of the cell are prone to suffer severe inter-cell interference and signal\nattenuation, leading to low throughput even transmission interruptions. Such\nedge effect severely obstructs offloading of tasks to MEC servers. To address\nthis issue, we propose user-centric mobile edge computing (UCMEC), a novel MEC\narchitecture integrating user-centric transmission, which can ensure high\nthroughput and reliable communication for task offloading. Then, we formulate\nan optimization problem with joint consideration of task offloading, power\ncontrol, and computing resource allocation in UCMEC, aiming at obtaining the\noptimal performance in terms of long-term average total delay. To solve the\nintractable problem, we propose two decentralized joint optimization schemes\nbased on multi-agent deep reinforcement learning (MADRL) and convex\noptimization, which consider both cooperation and non-cooperation among network\nnodes. Simulation results demonstrate that the proposed schemes in UCMEC can\nsignificantly improve the uplink transmission rate by at most 343.56% and\nreduce the long-term average total delay by at most 45.57% compared to\ntraditional cellular-based MEC.",
            "author": [
                "Langtian Qin",
                "Hancheng Lu",
                "Yuang Chen",
                "Baolin Chong",
                "Feng Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01499v1",
                "http://arxiv.org/pdf/2312.01499v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.DC",
                "cs.SY",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01498v1",
            "title": "Learning Neural Traffic Rules",
            "updated": "2023-12-03T20:06:43Z",
            "published": "2023-12-03T20:06:43Z",
            "summary": "Extensive research has been devoted to the field of multi-agent navigation.\nRecently, there has been remarkable progress attributed to the emergence of\nlearning-based techniques with substantially elevated intelligence and realism.\nNonetheless, prevailing learned models face limitations in terms of scalability\nand effectiveness, primarily due to their agent-centric nature, i.e., the\nlearned neural policy is individually deployed on each agent. Inspired by the\nefficiency observed in real-world traffic networks, we present an\nenvironment-centric navigation policy. Our method learns a set of traffic rules\nto coordinate a vast group of unintelligent agents that possess only basic\ncollision-avoidance capabilities. Our method segments the environment into\ndistinct blocks and parameterizes the traffic rule using a Graph Recurrent\nNeural Network (GRNN) over the block network. Each GRNN node is trained to\nmodulate the velocities of agents as they traverse through. Using either\nImitation Learning (IL) or Reinforcement Learning (RL) schemes, we demonstrate\nthe efficacy of our neural traffic rules in resolving agent congestion, closely\nresembling real-world traffic regulations. Our method handles up to $240$\nagents at real-time and generalizes across diverse agent and environment\nconfigurations.",
            "author": [
                "Xuan Zhang",
                "Xifeng Gao",
                "Kui Wu",
                "Zherong Pan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01498v1",
                "http://arxiv.org/pdf/2312.01498v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01490v1",
            "title": "GAPS: Geometry-Aware, Physics-Based, Self-Supervised Neural Garment\n  Draping",
            "updated": "2023-12-03T19:21:53Z",
            "published": "2023-12-03T19:21:53Z",
            "summary": "Recent neural, physics-based modeling of garment deformations allows faster\nand visually aesthetic results as opposed to the existing methods.\nMaterial-specific parameters are used by the formulation to control the garment\ninextensibility. This delivers unrealistic results with physically implausible\nstretching. Oftentimes, the draped garment is pushed inside the body which is\neither corrected by an expensive post-processing, thus adding to further\ninconsistent stretching; or by deploying a separate training regime for each\nbody type, restricting its scalability. Additionally, the flawed skinning\nprocess deployed by existing methods produces incorrect results on loose\ngarments.\n  In this paper, we introduce a geometrical constraint to the existing\nformulation that is collision-aware and imposes garment inextensibility\nwherever possible. Thus, we obtain realistic results where draped clothes\nstretch only while covering bigger body regions. Furthermore, we propose a\ngeometry-aware garment skinning method by defining a body-garment closeness\nmeasure which works for all garment types, especially the loose ones.",
            "author": [
                "Ruochen Chen",
                "Liming Chen",
                "Shaifali Parashar"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01490v1",
                "http://arxiv.org/pdf/2312.01490v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01488v1",
            "title": "ADT: Agent-based Dynamic Thresholding for Anomaly Detection",
            "updated": "2023-12-03T19:07:30Z",
            "published": "2023-12-03T19:07:30Z",
            "summary": "The complexity and scale of IT systems are increasing dramatically, posing\nmany challenges to real-world anomaly detection. Deep learning anomaly\ndetection has emerged, aiming at feature learning and anomaly scoring, which\nhas gained tremendous success. However, little work has been done on the\nthresholding problem despite it being a critical factor for the effectiveness\nof anomaly detection. In this paper, we model thresholding in anomaly detection\nas a Markov Decision Process and propose an agent-based dynamic thresholding\n(ADT) framework based on a deep Q-network. The proposed method can be\nintegrated into many systems that require dynamic thresholding. An auto-encoder\nis utilized in this study to obtain feature representations and produce anomaly\nscores for complex input data. ADT can adjust thresholds adaptively by\nutilizing the anomaly scores from the auto-encoder and significantly improve\nanomaly detection performance. The properties of ADT are studied through\nexperiments on three real-world datasets and compared with benchmarks, hence\ndemonstrating its thresholding capability, data-efficient learning, stability,\nand robustness. Our study validates the effectiveness of reinforcement learning\nin optimal thresholding control in anomaly detection.",
            "author": [
                "Xue Yang",
                "Enda Howley",
                "Micheal Schukat"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01488v1",
                "http://arxiv.org/pdf/2312.01488v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03001v1",
            "title": "Computer Vision for Increased Operative Efficiency via Identification of\n  Instruments in the Neurosurgical Operating Room: A Proof-of-Concept Study",
            "updated": "2023-12-03T19:01:50Z",
            "published": "2023-12-03T19:01:50Z",
            "summary": "Objectives Computer vision (CV) is a field of artificial intelligence that\nenables machines to interpret and understand images and videos. CV has the\npotential to be of assistance in the operating room (OR) to track surgical\ninstruments. We built a CV algorithm for identifying surgical instruments in\nthe neurosurgical operating room as a potential solution for surgical\ninstrument tracking and management to decrease surgical waste and opening of\nunnecessary tools. Methods We collected 1660 images of 27 commonly used\nneurosurgical instruments. Images were labeled using the VGG Image Annotator\nand split into 80% training and 20% testing sets in order to train a U-Net\nConvolutional Neural Network using 5-fold cross validation. Results Our U-Net\nachieved a tool identification accuracy of 80-100% when distinguishing 25\nclasses of instruments, with 19/25 classes having accuracy over 90%. The model\nperformance was not adequate for sub classifying Adson, Gerald, and Debakey\nforceps, which had accuracies of 60-80%. Conclusions We demonstrated the\nviability of using machine learning to accurately identify surgical\ninstruments. Instrument identification could help optimize surgical tray\npacking, decrease tool usage and waste, decrease incidence of instrument\nmisplacement events, and assist in timing of routine instrument maintenance.\nMore training data will be needed to increase accuracy across all surgical\ninstruments that would appear in a neurosurgical operating room. Such\ntechnology has the potential to be used as a method to be used for proving what\ntools are truly needed in each type of operation allowing surgeons across the\nworld to do more with less.",
            "author": [
                "Tanner J. Zachem",
                "Sully F. Chen",
                "Vishal Venkatraman",
                "David AW Sykes",
                "Ravi Prakash",
                "Samantha Spellicy",
                "Alexander D Suarez",
                "Weston Ross",
                "Patrick J. Codd"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03001v1",
                "http://arxiv.org/pdf/2312.03001v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02222v1",
            "title": "InvertAvatar: Incremental GAN Inversion for Generalized Head Avatars",
            "updated": "2023-12-03T18:59:15Z",
            "published": "2023-12-03T18:59:15Z",
            "summary": "While high fidelity and efficiency are central to the creation of digital\nhead avatars, recent methods relying on 2D or 3D generative models often\nexperience limitations such as shape distortion, expression inaccuracy, and\nidentity flickering. Additionally, existing one-shot inversion techniques fail\nto fully leverage multiple input images for detailed feature extraction. We\npropose a novel framework, \\textbf{Incremental 3D GAN Inversion}, that enhances\navatar reconstruction performance using an algorithm designed to increase the\nfidelity from multiple frames, resulting in improved reconstruction quality\nproportional to frame count. Our method introduces a unique animatable 3D GAN\nprior with two crucial modifications for enhanced expression controllability\nalongside an innovative neural texture encoder that categorizes texture feature\nspaces based on UV parameterization. Differentiating from traditional\ntechniques, our architecture emphasizes pixel-aligned image-to-image\ntranslation, mitigating the need to learn correspondences between observation\nand canonical spaces. Furthermore, we incorporate ConvGRU-based recurrent\nnetworks for temporal data aggregation from multiple frames, boosting geometry\nand texture detail reconstruction. The proposed paradigm demonstrates\nstate-of-the-art performance on one-shot and few-shot avatar animation tasks.",
            "author": [
                "Xiaochen Zhao",
                "Jingxiang Sun",
                "Lizhen Wang",
                "Yebin Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02222v1",
                "http://arxiv.org/pdf/2312.02222v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03000v1",
            "title": "VidereX: A Navigational Application inspired by ants",
            "updated": "2023-12-03T18:59:01Z",
            "published": "2023-12-03T18:59:01Z",
            "summary": "Navigation is a crucial element in any person's life, whether for work,\neducation, social living or any other miscellaneous reason; naturally, the\nimportance of it is universally recognised and valued. One of the critical\ncomponents of navigation is vision, which facilitates movement from one place\nto another. Navigating unfamiliar settings, especially for the blind or\nvisually impaired, can pose significant challenges, impacting their\nindependence and quality of life. Current assistive travel solutions have\nshortcomings, including GPS limitations and a demand for an efficient,\nuser-friendly, and portable model. Addressing these concerns, this paper\npresents VidereX: a smartphone-based solution using an ant-inspired navigation\nalgorithm. Emulating ants' ability to learn a route between nest and feeding\ngrounds after a single traversal, VidereX enables users to rapidly acquire\nnavigational data using a one/few-shot learning strategy. A key component of\nVidereX is its emphasis on active user engagement. Like ants with a scanning\nbehaviour to actively investigate their environment, users wield the camera,\nactively exploring the visual landscape. Far from the passive reception of\ndata, this process constitutes a dynamic exploration, echoing nature's\nnavigational mechanisms.",
            "author": [
                "Nam Ho Koh",
                "Doran Amos",
                "Paul Graham",
                "Andrew Philippides"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03000v1",
                "http://arxiv.org/pdf/2312.03000v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01479v1",
            "title": "OpenVoice: Versatile Instant Voice Cloning",
            "updated": "2023-12-03T18:41:54Z",
            "published": "2023-12-03T18:41:54Z",
            "summary": "We introduce OpenVoice, a versatile voice cloning approach that requires only\na short audio clip from the reference speaker to replicate their voice and\ngenerate speech in multiple languages. OpenVoice represents a significant\nadvancement in addressing the following open challenges in the field: 1)\nFlexible Voice Style Control. OpenVoice enables granular control over voice\nstyles, including emotion, accent, rhythm, pauses, and intonation, in addition\nto replicating the tone color of the reference speaker. The voice styles are\nnot directly copied from and constrained by the style of the reference speaker.\nPrevious approaches lacked the ability to flexibly manipulate voice styles\nafter cloning. 2) Zero-Shot Cross-Lingual Voice Cloning. OpenVoice achieves\nzero-shot cross-lingual voice cloning for languages not included in the\nmassive-speaker training set. Unlike previous approaches, which typically\nrequire extensive massive-speaker multi-lingual (MSML) dataset for all\nlanguages, OpenVoice can clone voices into a new language without any\nmassive-speaker training data for that language. OpenVoice is also\ncomputationally efficient, costing tens of times less than commercially\navailable APIs that offer even inferior performance. To foster further research\nin the field, we have made the source code and trained model publicly\naccessible. We also provide qualitative results in our demo website. Prior to\nits public release, our internal version of OpenVoice was used tens of millions\nof times by users worldwide between May and October 2023, serving as the\nbackend of MyShell.ai.",
            "author": [
                "Zengyi Qin",
                "Wenliang Zhao",
                "Xumin Yu",
                "Xin Sun"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01479v1",
                "http://arxiv.org/pdf/2312.01479v1"
            ],
            "primary_category": "cs.SD",
            "category": [
                "cs.SD",
                "cs.LG",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02220v1",
            "title": "QuantAttack: Exploiting Dynamic Quantization to Attack Vision\n  Transformers",
            "updated": "2023-12-03T18:31:19Z",
            "published": "2023-12-03T18:31:19Z",
            "summary": "In recent years, there has been a significant trend in deep neural networks\n(DNNs), particularly transformer-based models, of developing ever-larger and\nmore capable models. While they demonstrate state-of-the-art performance, their\ngrowing scale requires increased computational resources (e.g., GPUs with\ngreater memory capacity). To address this problem, quantization techniques\n(i.e., low-bit-precision representation and matrix multiplication) have been\nproposed. Most quantization techniques employ a static strategy in which the\nmodel parameters are quantized, either during training or inference, without\nconsidering the test-time sample. In contrast, dynamic quantization techniques,\nwhich have become increasingly popular, adapt during inference based on the\ninput provided, while maintaining full-precision performance. However, their\ndynamic behavior and average-case performance assumption makes them vulnerable\nto a novel threat vector -- adversarial attacks that target the model's\nefficiency and availability. In this paper, we present QuantAttack, a novel\nattack that targets the availability of quantized models, slowing down the\ninference, and increasing memory usage and energy consumption. We show that\ncarefully crafted adversarial examples, which are designed to exhaust the\nresources of the operating system, can trigger worst-case performance. In our\nexperiments, we demonstrate the effectiveness of our attack on vision\ntransformers on a wide range of tasks, both uni-modal and multi-modal. We also\nexamine the effect of different attack variants (e.g., a universal\nperturbation) and the transferability between different models.",
            "author": [
                "Amit Baras",
                "Alon Zolfi",
                "Yuval Elovici",
                "Asaf Shabtai"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02220v1",
                "http://arxiv.org/pdf/2312.02220v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.CR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01476v1",
            "title": "Context-Enhanced Relational Operators with Vector Embeddings",
            "updated": "2023-12-03T18:23:48Z",
            "published": "2023-12-03T18:23:48Z",
            "summary": "Collecting data, extracting value, and combining insights from relational and\ncontext-rich multi-modal sources in data processing pipelines presents a\nchallenge for traditional relational DBMS. While relational operators allow\ndeclarative and optimizable query specification, they are limited to data\ntransformations unsuitable for capturing or analyzing context. On the other\nhand, representation learning models can map context-rich data into embeddings,\nallowing machine-automated context processing but requiring imperative data\ntransformation integration with the analytical query.\n  To bridge this dichotomy, we present a context-enhanced relational join and\nintroduce an embedding operator composable with relational operators. This\nenables hybrid relational and context-rich vector data processing, with\nalgebraic equivalences compatible with relational algebra and corresponding\nlogical and physical optimizations. We investigate model-operator interaction\nwith vector data processing and study the characteristics of the E-join\noperator. Using an example of string embeddings, we demonstrate enabling hybrid\ncontext-enhanced processing on relational join operators with vector\nembeddings. The importance of holistic optimization, from logical to physical,\nis demonstrated in an order of magnitude execution time improvement.",
            "author": [
                "Viktor Sanca",
                "Manos Chatzakis",
                "Anastasia Ailamaki"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01476v1",
                "http://arxiv.org/pdf/2312.01476v1"
            ],
            "primary_category": "cs.DB",
            "category": [
                "cs.DB",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01474v1",
            "title": "Distilling Functional Rearrangement Priors from Large Models",
            "updated": "2023-12-03T18:18:51Z",
            "published": "2023-12-03T18:18:51Z",
            "summary": "Object rearrangement, a fundamental challenge in robotics, demands versatile\nstrategies to handle diverse objects, configurations, and functional needs. To\nachieve this, the AI robot needs to learn functional rearrangement priors in\norder to specify precise goals that meet the functional requirements. Previous\nmethods typically learn such priors from either laborious human annotations or\nmanually designed heuristics, which limits scalability and generalization. In\nthis work, we propose a novel approach that leverages large models to distill\nfunctional rearrangement priors. Specifically, our approach collects diverse\narrangement examples using both LLMs and VLMs and then distills the examples\ninto a diffusion model. During test time, the learned diffusion model is\nconditioned on the initial configuration and guides the positioning of objects\nto meet functional requirements. In this manner, we create a handshaking point\nthat combines the strengths of conditional generative models and large models.\nExtensive experiments on multiple domains, including real-world scenarios,\ndemonstrate the effectiveness of our approach in generating compatible goals\nfor object rearrangement tasks, significantly outperforming baseline methods.",
            "author": [
                "Yiming Zeng",
                "Mingdong Wu",
                "Long Yang",
                "Jiyao Zhang",
                "Hao Ding",
                "Hui Cheng",
                "Hao Dong"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01474v1",
                "http://arxiv.org/pdf/2312.01474v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01473v1",
            "title": "Regularity as Intrinsic Reward for Free Play",
            "updated": "2023-12-03T18:18:44Z",
            "published": "2023-12-03T18:18:44Z",
            "summary": "We propose regularity as a novel reward signal for intrinsically-motivated\nreinforcement learning. Taking inspiration from child development, we postulate\nthat striving for structure and order helps guide exploration towards a\nsubspace of tasks that are not favored by naive uncertainty-based intrinsic\nrewards. Our generalized formulation of Regularity as Intrinsic Reward (RaIR)\nallows us to operationalize it within model-based reinforcement learning. In a\nsynthetic environment, we showcase the plethora of structured patterns that can\nemerge from pursuing this regularity objective. We also demonstrate the\nstrength of our method in a multi-object robotic manipulation environment. We\nincorporate RaIR into free play and use it to complement the model's epistemic\nuncertainty as an intrinsic reward. Doing so, we witness the autonomous\nconstruction of towers and other regular structures during free play, which\nleads to a substantial improvement in zero-shot downstream task performance on\nassembly tasks.",
            "author": [
                "Cansu Sancaktar",
                "Justus Piater",
                "Georg Martius"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01473v1",
                "http://arxiv.org/pdf/2312.01473v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01472v1",
            "title": "BenchMARL: Benchmarking Multi-Agent Reinforcement Learning",
            "updated": "2023-12-03T18:15:58Z",
            "published": "2023-12-03T18:15:58Z",
            "summary": "The field of Multi-Agent Reinforcement Learning (MARL) is currently facing a\nreproducibility crisis. While solutions for standardized reporting have been\nproposed to address the issue, we still lack a benchmarking tool that enables\nstandardization and reproducibility, while leveraging cutting-edge\nReinforcement Learning (RL) implementations. In this paper, we introduce\nBenchMARL, the first MARL training library created to enable standardized\nbenchmarking across different algorithms, models, and environments. BenchMARL\nuses TorchRL as its backend, granting it high performance and maintained\nstate-of-the-art implementations while addressing the broad community of MARL\nPyTorch users. Its design enables systematic configuration and reporting, thus\nallowing users to create and run complex benchmarks from simple one-line\ninputs. BenchMARL is open-sourced on GitHub:\nhttps://github.com/facebookresearch/BenchMARL",
            "author": [
                "Matteo Bettini",
                "Amanda Prorok",
                "Vincent Moens"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01472v1",
                "http://arxiv.org/pdf/2312.01472v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.MA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01464v1",
            "title": "Diffusion Posterior Sampling for Nonlinear CT Reconstruction",
            "updated": "2023-12-03T17:20:11Z",
            "published": "2023-12-03T17:20:11Z",
            "summary": "Diffusion models have been demonstrated as powerful deep learning tools for\nimage generation in CT reconstruction and restoration. Recently, diffusion\nposterior sampling, where a score-based diffusion prior is combined with a\nlikelihood model, has been used to produce high quality CT images given\nlow-quality measurements. This technique is attractive since it permits a\none-time, unsupervised training of a CT prior; which can then be incorporated\nwith an arbitrary data model. However, current methods only rely on a linear\nmodel of x-ray CT physics to reconstruct or restore images. While it is common\nto linearize the transmission tomography reconstruction problem, this is an\napproximation to the true and inherently nonlinear forward model. We propose a\nnew method that solves the inverse problem of nonlinear CT image reconstruction\nvia diffusion posterior sampling. We implement a traditional unconditional\ndiffusion model by training a prior score function estimator, and apply Bayes\nrule to combine this prior with a measurement likelihood score function derived\nfrom the nonlinear physical model to arrive at a posterior score function that\ncan be used to sample the reverse-time diffusion process. This plug-and-play\nmethod allows incorporation of a diffusion-based prior with generalized\nnonlinear CT image reconstruction into multiple CT system designs with\ndifferent forward models, without the need for any additional training. We\ndevelop the algorithm that performs this reconstruction, including an\nordered-subsets variant for accelerated processing and demonstrate the\ntechnique in both fully sampled low dose data and sparse-view geometries using\na single unsupervised training of the prior.",
            "author": [
                "Shudong Li",
                "Matthew Tivnan",
                "Yuan Shen",
                "J. Webster Stayman"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01464v1",
                "http://arxiv.org/pdf/2312.01464v1"
            ],
            "primary_category": "physics.med-ph",
            "category": [
                "physics.med-ph",
                "cs.CV",
                "eess.IV",
                "physics.comp-ph",
                "J.3; I.4.4; I.4.5"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01457v1",
            "title": "Marginal Density Ratio for Off-Policy Evaluation in Contextual Bandits",
            "updated": "2023-12-03T17:04:57Z",
            "published": "2023-12-03T17:04:57Z",
            "summary": "Off-Policy Evaluation (OPE) in contextual bandits is crucial for assessing\nnew policies using existing data without costly experimentation. However,\ncurrent OPE methods, such as Inverse Probability Weighting (IPW) and Doubly\nRobust (DR) estimators, suffer from high variance, particularly in cases of low\noverlap between target and behavior policies or large action and context\nspaces. In this paper, we introduce a new OPE estimator for contextual bandits,\nthe Marginal Ratio (MR) estimator, which focuses on the shift in the marginal\ndistribution of outcomes $Y$ instead of the policies themselves. Through\nrigorous theoretical analysis, we demonstrate the benefits of the MR estimator\ncompared to conventional methods like IPW and DR in terms of variance\nreduction. Additionally, we establish a connection between the MR estimator and\nthe state-of-the-art Marginalized Inverse Propensity Score (MIPS) estimator,\nproving that MR achieves lower variance among a generalized family of MIPS\nestimators. We further illustrate the utility of the MR estimator in causal\ninference settings, where it exhibits enhanced performance in estimating\nAverage Treatment Effects (ATE). Our experiments on synthetic and real-world\ndatasets corroborate our theoretical findings and highlight the practical\nadvantages of the MR estimator in OPE for contextual bandits.",
            "author": [
                "Muhammad Faaiz Taufiq",
                "Arnaud Doucet",
                "Rob Cornish",
                "Jean-Francois Ton"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01457v1",
                "http://arxiv.org/pdf/2312.01457v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG",
                "stat.ME"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01456v1",
            "title": "Compositional Policy Learning in Stochastic Control Systems with Formal\n  Guarantees",
            "updated": "2023-12-03T17:04:18Z",
            "published": "2023-12-03T17:04:18Z",
            "summary": "Reinforcement learning has shown promising results in learning neural network\npolicies for complicated control tasks. However, the lack of formal guarantees\nabout the behavior of such policies remains an impediment to their deployment.\nWe propose a novel method for learning a composition of neural network policies\nin stochastic environments, along with a formal certificate which guarantees\nthat a specification over the policy's behavior is satisfied with the desired\nprobability. Unlike prior work on verifiable RL, our approach leverages the\ncompositional nature of logical specifications provided in SpectRL, to learn\nover graphs of probabilistic reach-avoid specifications. The formal guarantees\nare provided by learning neural network policies together with reach-avoid\nsupermartingales (RASM) for the graph's sub-tasks and then composing them into\na global policy. We also derive a tighter lower bound compared to previous work\non the probability of reach-avoidance implied by a RASM, which is required to\nfind a compositional policy with an acceptable probabilistic threshold for\ncomplex tasks with multiple edge policies. We implement a prototype of our\napproach and evaluate it on a Stochastic Nine Rooms environment.",
            "author": [
                "\u0110or\u0111e \u017dikeli\u0107",
                "Mathias Lechner",
                "Abhinav Verma",
                "Krishnendu Chatterjee",
                "Thomas A. Henzinger"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01456v1",
                "http://arxiv.org/pdf/2312.01456v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01454v2",
            "title": "D-Bot: Database Diagnosis System using Large Language Models",
            "updated": "2023-12-06T02:53:11Z",
            "published": "2023-12-03T16:58:10Z",
            "summary": "Database administrators (DBAs) play an important role in managing,\nmaintaining and optimizing database systems. However, it is hard and tedious\nfor DBAs to manage a large number of databases and give timely response\n(waiting for hours is intolerable in many online cases). In addition, existing\nempirical methods only support limited diagnosis scenarios, which are also\nlabor-intensive to update the diagnosis rules for database version updates.\nRecently large language models (LLMs) have shown great potential in various\nfields. Thus, we propose D-Bot, an LLM-based database diagnosis system that can\nautomatically acquire knowledge from diagnosis documents, and generate\nreasonable and well-founded diagnosis report (i.e., identifying the root causes\nand solutions) within acceptable time (e.g., under 10 minutes compared to hours\nby a DBA). The techniques in D-Bot include (i) offline knowledge extraction\nfrom documents, (ii) automatic prompt generation (e.g., knowledge matching,\ntool retrieval), (iii) root cause analysis using tree search algorithm, and\n(iv) collaborative mechanism for complex anomalies with multiple root causes.\nWe verify D-Bot on real benchmarks (including 539 anomalies of six typical\napplications), and the results show that D-Bot can effectively analyze the root\ncauses of unseen anomalies and significantly outperforms traditional methods\nand vanilla models like GPT-4.",
            "author": [
                "Xuanhe Zhou",
                "Guoliang Li",
                "Zhaoyan Sun",
                "Zhiyuan Liu",
                "Weize Chen",
                "Jianming Wu",
                "Jiesi Liu",
                "Ruohang Feng",
                "Guoyang Zeng"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01454v2",
                "http://arxiv.org/pdf/2312.01454v2"
            ],
            "primary_category": "cs.DB",
            "category": [
                "cs.DB",
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01450v1",
            "title": "Foveation in the Era of Deep Learning",
            "updated": "2023-12-03T16:48:09Z",
            "published": "2023-12-03T16:48:09Z",
            "summary": "In this paper, we tackle the challenge of actively attending to visual scenes\nusing a foveated sensor. We introduce an end-to-end differentiable foveated\nactive vision architecture that leverages a graph convolutional network to\nprocess foveated images, and a simple yet effective formulation for foveated\nimage sampling. Our model learns to iteratively attend to regions of the image\nrelevant for classification. We conduct detailed experiments on a variety of\nimage datasets, comparing the performance of our method with previous\napproaches to foveated vision while measuring how the impact of different\nchoices, such as the degree of foveation, and the number of fixations the\nnetwork performs, affect object recognition performance. We find that our model\noutperforms a state-of-the-art CNN and foveated vision architectures of\ncomparable parameters and a given pixel or computation budget",
            "author": [
                "George Killick",
                "Paul Henderson",
                "Paul Siebert",
                "Gerardo Aragon-Camarasa"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01450v1",
                "http://arxiv.org/pdf/2312.01450v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG",
                "I.2.10; I.5.1; I.4.8"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03126v1",
            "title": "Learning Curricula in Open-Ended Worlds",
            "updated": "2023-12-03T16:44:00Z",
            "published": "2023-12-03T16:44:00Z",
            "summary": "Deep reinforcement learning (RL) provides powerful methods for training\noptimal sequential decision-making agents. As collecting real-world\ninteractions can entail additional costs and safety risks, the common paradigm\nof sim2real conducts training in a simulator, followed by real-world\ndeployment. Unfortunately, RL agents easily overfit to the choice of simulated\ntraining environments, and worse still, learning ends when the agent masters\nthe specific set of simulated environments. In contrast, the real world is\nhighly open-ended, featuring endlessly evolving environments and challenges,\nmaking such RL approaches unsuitable. Simply randomizing over simulated\nenvironments is insufficient, as it requires making arbitrary distributional\nassumptions and can be combinatorially less likely to sample specific\nenvironment instances that are useful for learning. An ideal learning process\nshould automatically adapt the training environment to maximize the learning\npotential of the agent over an open-ended task space that matches or surpasses\nthe complexity of the real world. This thesis develops a class of methods\ncalled Unsupervised Environment Design (UED), which aim to produce such\nopen-ended processes. Given an environment design space, UED automatically\ngenerates an infinite sequence or curriculum of training environments at the\nfrontier of the learning agent's capabilities. Through extensive empirical\nstudies and theoretical arguments founded on minimax-regret decision theory and\ngame theory, the findings in this thesis show that UED autocurricula can\nproduce RL agents exhibiting significantly improved robustness and\ngeneralization to previously unseen environment instances. Such autocurricula\nare promising paths toward open-ended learning systems that achieve more\ngeneral intelligence by continually generating and mastering additional\nchallenges of their own design.",
            "author": [
                "Minqi Jiang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03126v1",
                "http://arxiv.org/pdf/2312.03126v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01446v1",
            "title": "Ultrafast polarization switching in BaTiO$_3$ by photoactivation of its\n  ferroelectric and central modes",
            "updated": "2023-12-03T16:27:46Z",
            "published": "2023-12-03T16:27:46Z",
            "summary": "We use molecular dynamics simulations with machine-learned atomistic force\nfields to simulate photoexcitation of BaTiO3 by a femtosecond laser pulse whose\nphoton energy exceeds the optical gap. We demonstrate selective displacive\nexcitation of coherent zone-center ferroelectric mode phonons and of the\nstrongly anharmonic central mode. We show that the direction of P can either be\nreversed by a pulse in hundreds of femtoseconds or, on a longer time scale and\nwhen combined with a weak field, switched to any one of its symmetry-equivalent\ndirections.",
            "author": [
                "Fangyuan Gu",
                "Paul Tangney"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01446v1",
                "http://arxiv.org/pdf/2312.01446v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci",
                "physics.comp-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01445v1",
            "title": "Classification of Home Network Problems with Transformers",
            "updated": "2023-12-03T16:27:06Z",
            "published": "2023-12-03T16:27:06Z",
            "summary": "We propose a classifier that can identify ten common home network problems\nbased on the raw textual output of networking tools such as ping, dig, and ip.\nOur deep learning model uses an encoder-only transformer architecture with a\nparticular pre-tokenizer that we propose for splitting the tool output into\ntoken sequences. The use of transformers distinguishes our approach from\nrelated work on network problem classification, which still primarily relies on\nnon-deep-learning methods. Our model achieves high accuracy in our experiments,\ndemonstrating the high potential of transformer-based problem classification\nfor the home network.",
            "author": [
                "Jeremias D\u00f6tterl",
                "Zahra Hemmati Fard"
            ],
            "link": [
                "http://dx.doi.org/10.1145/3605098.3635938",
                "http://arxiv.org/abs/2312.01445v1",
                "http://arxiv.org/pdf/2312.01445v1"
            ],
            "primary_category": "cs.NI",
            "category": [
                "cs.NI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01435v1",
            "title": "Automatic Report Generation for Histopathology images using pre-trained\n  Vision Transformers and BERT",
            "updated": "2023-12-03T15:56:09Z",
            "published": "2023-12-03T15:56:09Z",
            "summary": "Deep learning for histopathology has been successfully used for disease\nclassification, image segmentation and more. However, combining image and text\nmodalities using current state-of-the-art methods has been a challenge due to\nthe high resolution of histopathology images. Automatic report generation for\nhistopathology images is one such challenge. In this work, we show that using\nan existing pre-trained Vision Transformer in a two-step process of first using\nit to encode 4096x4096 sized patches of the Whole Slide Image (WSI) and then\nusing it as the encoder and a pre-trained Bidirectional Encoder Representations\nfrom Transformers (BERT) model for language modeling-based decoder for report\ngeneration, we can build a fairly performant and portable report generation\nmechanism that takes into account the whole of the high resolution image,\ninstead of just the patches. Our method allows us to not only generate and\nevaluate captions that describe the image, but also helps us classify the image\ninto tissue types and the gender of the patient as well. Our best performing\nmodel achieves a 79.98% accuracy in Tissue Type classification and 66.36%\naccuracy in classifying the sex of the patient the tissue came from, with a\nBLEU-4 score of 0.5818 in our caption generation task.",
            "author": [
                "Saurav Sengupta",
                "Donald E. Brown"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01435v1",
                "http://arxiv.org/pdf/2312.01435v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01432v1",
            "title": "Fast Dual Subgradient Optimization of the Integrated Transportation\n  Distance Between Stochastic Kernels",
            "updated": "2023-12-03T15:44:17Z",
            "published": "2023-12-03T15:44:17Z",
            "summary": "A generalization of the Wasserstein metric, the integrated transportation\ndistance, establishes a novel distance between probability kernels of Markov\nsystems. This metric serves as the foundation for an efficient approximation\ntechnique, enabling the replacement of the original system's kernel with a\nkernel with a discrete support of limited cardinality. To facilitate practical\nimplementation, we present a specialized dual algorithm capable of constructing\nthese approximate kernels quickly and efficiently, without requiring\ncomputationally expensive matrix operations. Finally, we demonstrate the\nefficacy of our method through several illustrative examples, showcasing its\nutility in practical scenarios. This advancement offers new possibilities for\nthe streamlined analysis and manipulation of stochastic systems represented by\nkernels.",
            "author": [
                "Zhengqi Lin",
                "Andrzej Ruszczynski"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01432v1",
                "http://arxiv.org/pdf/2312.01432v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01431v1",
            "title": "D$^2$ST-Adapter: Disentangled-and-Deformable Spatio-Temporal Adapter for\n  Few-shot Action Recognition",
            "updated": "2023-12-03T15:40:10Z",
            "published": "2023-12-03T15:40:10Z",
            "summary": "Adapting large pre-trained image models to few-shot action recognition has\nproven to be an effective and efficient strategy for learning robust feature\nextractors, which is essential for few-shot learning. Typical fine-tuning based\nadaptation paradigm is prone to overfitting in the few-shot learning scenarios\nand offers little modeling flexibility for learning temporal features in video\ndata. In this work we present the Disentangled-and-Deformable Spatio-Temporal\nAdapter (D$^2$ST-Adapter), a novel adapter tuning framework for few-shot action\nrecognition, which is designed in a dual-pathway architecture to encode spatial\nand temporal features in a disentangled manner. Furthermore, we devise the\nDeformable Spatio-Temporal Attention module as the core component of\nD$^2$ST-Adapter, which can be tailored to model both spatial and temporal\nfeatures in corresponding pathways, allowing our D$^2$ST-Adapter to encode\nfeatures in a global view in 3D spatio-temporal space while maintaining a\nlightweight design. Extensive experiments with instantiations of our method on\nboth pre-trained ResNet and ViT demonstrate the superiority of our method over\nstate-of-the-art methods for few-shot action recognition. Our method is\nparticularly well-suited to challenging scenarios where temporal dynamics are\ncritical for action recognition.",
            "author": [
                "Wenjie Pei",
                "Qizhong Tan",
                "Guangming Lu",
                "Jiandong Tian"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01431v1",
                "http://arxiv.org/pdf/2312.01431v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01429v1",
            "title": "Transformers are uninterpretable with myopic methods: a case study with\n  bounded Dyck grammars",
            "updated": "2023-12-03T15:34:46Z",
            "published": "2023-12-03T15:34:46Z",
            "summary": "Interpretability methods aim to understand the algorithm implemented by a\ntrained model (e.g., a Transofmer) by examining various aspects of the model,\nsuch as the weight matrices or the attention patterns. In this work, through a\ncombination of theoretical results and carefully controlled experiments on\nsynthetic data, we take a critical view of methods that exclusively focus on\nindividual parts of the model, rather than consider the network as a whole. We\nconsider a simple synthetic setup of learning a (bounded) Dyck language.\nTheoretically, we show that the set of models that (exactly or approximately)\nsolve this task satisfy a structural characterization derived from ideas in\nformal languages (the pumping lemma). We use this characterization to show that\nthe set of optima is qualitatively rich; in particular, the attention pattern\nof a single layer can be ``nearly randomized'', while preserving the\nfunctionality of the network. We also show via extensive experiments that these\nconstructions are not merely a theoretical artifact: even after severely\nconstraining the architecture of the model, vastly different solutions can be\nreached via standard training. Thus, interpretability claims based on\ninspecting individual heads or weight matrices in the Transformer can be\nmisleading.",
            "author": [
                "Kaiyue Wen",
                "Yuchen Li",
                "Bingbin Liu",
                "Andrej Risteski"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01429v1",
                "http://arxiv.org/pdf/2312.01429v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01423v1",
            "title": "Self-Critical Alternate Learning based Semantic Broadcast Communication",
            "updated": "2023-12-03T15:01:11Z",
            "published": "2023-12-03T15:01:11Z",
            "summary": "Semantic communication (SemCom) has been deemed as a promising communication\nparadigm to break through the bottleneck of traditional communications.\nNonetheless, most of the existing works focus more on point-to-point\ncommunication scenarios and its extension to multi-user scenarios is not that\nstraightforward due to its cost-inefficiencies to directly scale the JSCC\nframework to the multi-user communication system. Meanwhile, previous methods\noptimize the system by differentiable bit-level supervision, easily leading to\na \"semantic gap\". Therefore, we delve into multi-user broadcast communication\n(BC) based on the universal transformer (UT) and propose a reinforcement\nlearning (RL) based self-critical alternate learning (SCAL) algorithm, named\nSemanticBC-SCAL, to capably adapt to the different BC channels from one\ntransmitter (TX) to multiple receivers (RXs) for sentence generation task. In\nparticular, to enable stable optimization via a nondifferentiable semantic\nmetric, we regard sentence similarity as a reward and formulate this learning\nprocess as an RL problem. Considering the huge decision space, we adopt a\nlightweight but efficient self-critical supervision to guide the learning\nprocess. Meanwhile, an alternate learning mechanism is developed to provide\ncost-effective learning, in which the encoder and decoders are updated\nasynchronously with different iterations. Notably, the incorporation of RL\nmakes SemanticBC-SCAL compliant with any user-defined semantic similarity\nmetric and simultaneously addresses the channel non-differentiability issue by\nalternate learning. Besides, the convergence of SemanticBC-SCAL is also\ntheoretically established. Extensive simulation results have been conducted to\nverify the effectiveness and superiorness of our approach, especially in low\nSNRs.",
            "author": [
                "Zhilin Lu",
                "Rongpeng Li",
                "Ming Lei",
                "Chan Wang",
                "Zhifeng Zhao",
                "Honggang Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01423v1",
                "http://arxiv.org/pdf/2312.01423v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01421v1",
            "title": "RobotGPT: Robot Manipulation Learning from ChatGPT",
            "updated": "2023-12-03T14:59:28Z",
            "published": "2023-12-03T14:59:28Z",
            "summary": "We present RobotGPT, an innovative decision framework for robotic\nmanipulation that prioritizes stability and safety. The execution code\ngenerated by ChatGPT cannot guarantee the stability and safety of the system.\nChatGPT may provide different answers for the same task, leading to\nunpredictability. This instability prevents the direct integration of ChatGPT\ninto the robot manipulation loop. Although setting the temperature to 0 can\ngenerate more consistent outputs, it may cause ChatGPT to lose diversity and\ncreativity. Our objective is to leverage ChatGPT's problem-solving capabilities\nin robot manipulation and train a reliable agent. The framework includes an\neffective prompt structure and a robust learning model. Additionally, we\nintroduce a metric for measuring task difficulty to evaluate ChatGPT's\nperformance in robot manipulation. Furthermore, we evaluate RobotGPT in both\nsimulation and real-world environments. Compared to directly using ChatGPT to\ngenerate code, our framework significantly improves task success rates, with an\naverage increase from 38.5% to 91.5%. Therefore, training a RobotGPT by\nutilizing ChatGPT as an expert is a more stable approach compared to directly\nusing ChatGPT as a task planner.",
            "author": [
                "Yixiang Jin",
                "Dingzhe Li",
                "Yong A",
                "Jun Shi",
                "Peng Hao",
                "Fuchun Sun",
                "Jianwei Zhang",
                "Bin Fang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01421v1",
                "http://arxiv.org/pdf/2312.01421v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01416v1",
            "title": "Uncertainty-biased molecular dynamics for learning uniformly accurate\n  interatomic potentials",
            "updated": "2023-12-03T14:39:14Z",
            "published": "2023-12-03T14:39:14Z",
            "summary": "Efficiently creating a concise but comprehensive data set for training\nmachine-learned interatomic potentials (MLIPs) is an under-explored problem.\nActive learning (AL), which uses either biased or unbiased molecular dynamics\n(MD) simulations to generate candidate pools, aims to address this objective.\nExisting biased and unbiased MD simulations, however, are prone to miss either\nrare events or extrapolative regions -- areas of the configurational space\nwhere unreliable predictions are made. Simultaneously exploring both regions is\nnecessary for developing uniformly accurate MLIPs. In this work, we demonstrate\nthat MD simulations, when biased by the MLIP's energy uncertainty, effectively\ncapture extrapolative regions and rare events without the need to know\n\\textit{a priori} the system's transition temperatures and pressures.\nExploiting automatic differentiation, we enhance bias-forces-driven MD\nsimulations by introducing the concept of bias stress. We also employ\ncalibrated ensemble-free uncertainties derived from sketched gradient features\nto yield MLIPs with similar or better accuracy than ensemble-based uncertainty\nmethods at a lower computational cost. We use the proposed uncertainty-driven\nAL approach to develop MLIPs for two benchmark systems: alanine dipeptide and\nMIL-53(Al). Compared to MLIPs trained with conventional MD simulations, MLIPs\ntrained with the proposed data-generation method more accurately represent the\nrelevant configurational space for both atomic systems.",
            "author": [
                "Viktor Zaverkin",
                "David Holzm\u00fcller",
                "Henrik Christiansen",
                "Federico Errica",
                "Francesco Alesiani",
                "Makoto Takamoto",
                "Mathias Niepert",
                "Johannes K\u00e4stner"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01416v1",
                "http://arxiv.org/pdf/2312.01416v1"
            ],
            "primary_category": "physics.comp-ph",
            "category": [
                "physics.comp-ph",
                "cond-mat.mtrl-sci",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01415v1",
            "title": "Thermally Averaged Magnetic Anisotropy Tensors via Machine Learning\n  Based on Gaussian Moments",
            "updated": "2023-12-03T14:37:57Z",
            "published": "2023-12-03T14:37:57Z",
            "summary": "We propose a machine learning method to model molecular tensorial quantities,\nnamely the magnetic anisotropy tensor, based on the Gaussian-moment\nneural-network approach. We demonstrate that the proposed methodology can\nachieve an accuracy of 0.3--0.4 cm$^{-1}$ and has excellent generalization\ncapability for out-of-sample configurations. Moreover, in combination with\nmachine-learned interatomic potential energies based on Gaussian moments, our\napproach can be applied to study the dynamic behavior of magnetic anisotropy\ntensors and provide a unique insight into spin-phonon relaxation.",
            "author": [
                "Viktor Zaverkin",
                "Julia Netz",
                "Fabian Zills",
                "Andreas K\u00f6hn",
                "Johannes K\u00e4stner"
            ],
            "link": [
                "http://dx.doi.org/10.1021/acs.jctc.1c00853",
                "http://arxiv.org/abs/2312.01415v1",
                "http://arxiv.org/pdf/2312.01415v1"
            ],
            "primary_category": "physics.comp-ph",
            "category": [
                "physics.comp-ph",
                "cond-mat.mtrl-sci",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01414v1",
            "title": "Predicting Properties of Periodic Systems from Cluster Data: A Case\n  Study of Liquid Water",
            "updated": "2023-12-03T14:37:27Z",
            "published": "2023-12-03T14:37:27Z",
            "summary": "The accuracy of the training data limits the accuracy of bulk properties from\nmachine-learned potentials. For example, hybrid functionals or\nwave-function-based quantum chemical methods are readily available for cluster\ndata but effectively out-of-scope for periodic structures. We show that local,\natom-centred descriptors for machine-learned potentials enable the prediction\nof bulk properties from cluster model training data, agreeing reasonably well\nwith predictions from bulk training data. We demonstrate such transferability\nby studying structural and dynamical properties of bulk liquid water with\ndensity functional theory and have found an excellent agreement with\nexperimental as well as theoretical counterparts.",
            "author": [
                "Viktor Zaverkin",
                "David Holzm\u00fcller",
                "Robin Schuldt",
                "Johannes K\u00e4stner"
            ],
            "link": [
                "http://dx.doi.org/10.1063/5.0078983",
                "http://arxiv.org/abs/2312.01414v1",
                "http://arxiv.org/pdf/2312.01414v1"
            ],
            "primary_category": "physics.comp-ph",
            "category": [
                "physics.comp-ph",
                "cond-mat.mtrl-sci",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01408v1",
            "title": "Improving In-Context Learning in Diffusion Models with Visual\n  Context-Modulated Prompts",
            "updated": "2023-12-03T14:15:52Z",
            "published": "2023-12-03T14:15:52Z",
            "summary": "In light of the remarkable success of in-context learning in large language\nmodels, its potential extension to the vision domain, particularly with visual\nfoundation models like Stable Diffusion, has sparked considerable interest.\nExisting approaches in visual in-context learning frequently face hurdles such\nas expensive pretraining, limiting frameworks, inadequate visual comprehension,\nand limited adaptability to new tasks. In response to these challenges, we\nintroduce improved Prompt Diffusion (iPromptDiff) in this study. iPromptDiff\nintegrates an end-to-end trained vision encoder that converts visual context\ninto an embedding vector. This vector is subsequently used to modulate the\ntoken embeddings of text prompts. We show that a diffusion-based vision\nfoundation model, when equipped with this visual context-modulated text\nguidance and a standard ControlNet structure, exhibits versatility and\nrobustness across a variety of training tasks and excels in in-context learning\nfor novel vision tasks, such as normal-to-image or image-to-line\ntransformations. The effectiveness of these capabilities relies heavily on a\ndeep visual understanding, which is achieved through relevant visual\ndemonstrations processed by our proposed in-context learning architecture.",
            "author": [
                "Tianqi Chen",
                "Yongfei Liu",
                "Zhendong Wang",
                "Jianbo Yuan",
                "Quanzeng You",
                "Hongxia Yang",
                "Mingyuan Zhou"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01408v1",
                "http://arxiv.org/pdf/2312.01408v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01403v1",
            "title": "OplixNet: Towards Area-Efficient Optical Split-Complex Networks with\n  Real-to-Complex Data Assignment and Knowledge Distillation",
            "updated": "2023-12-03T14:06:20Z",
            "published": "2023-12-03T14:06:20Z",
            "summary": "Having the potential for high speed, high throughput, and low energy cost,\noptical neural networks (ONNs) have emerged as a promising candidate for\naccelerating deep learning tasks. In conventional ONNs, light amplitudes are\nmodulated at the input and detected at the output. However, the light phases\nare still ignored in conventional structures, although they can also carry\ninformation for computing. To address this issue, in this paper, we propose a\nframework called OplixNet to compress the areas of ONNs by modulating input\nimage data into the amplitudes and phase parts of light signals. The input and\noutput parts of the ONNs are redesigned to make full use of both amplitude and\nphase information. Moreover, mutual learning across different ONN structures is\nintroduced to maintain the accuracy. Experimental results demonstrate that the\nproposed framework significantly reduces the areas of ONNs with the accuracy\nwithin an acceptable range. For instance, 75.03% area is reduced with a 0.33%\naccuracy decrease on fully connected neural network (FCNN) and 74.88% area is\nreduced with a 2.38% accuracy decrease on ResNet-32.",
            "author": [
                "Ruidi Qiu",
                "Amro Eldebiky",
                "Grace Li Zhang",
                "Xunzhao Yin",
                "Cheng Zhuo",
                "Ulf Schlichtmann",
                "Bing Li"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01403v1",
                "http://arxiv.org/pdf/2312.01403v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01398v1",
            "title": "Towards Mitigating Perceived Unfairness in Contracts from a Non-Legal\n  Stakeholder's Perspective",
            "updated": "2023-12-03T13:52:32Z",
            "published": "2023-12-03T13:52:32Z",
            "summary": "Commercial contracts are known to be a valuable source for deriving\nproject-specific requirements. However, contract negotiations mainly occur\namong the legal counsel of the parties involved. The participation of non-legal\nstakeholders, including requirement analysts, engineers, and solution\narchitects, whose primary responsibility lies in ensuring the seamless\nimplementation of contractual terms, is often indirect and inadequate.\nConsequently, a significant number of sentences in contractual clauses, though\nlegally accurate, can appear unfair from an implementation perspective to\nnon-legal stakeholders. This perception poses a problem since requirements\nindicated in the clauses are obligatory and can involve punitive measures and\npenalties if not implemented as committed in the contract. Therefore, the\nidentification of potentially unfair clauses in contracts becomes crucial. In\nthis work, we conduct an empirical study to analyze the perspectives of\ndifferent stakeholders regarding contractual fairness. We then investigate the\nability of Pre-trained Language Models (PLMs) to identify unfairness in\ncontractual sentences by comparing chain of thought prompting and\nsemi-supervised fine-tuning approaches. Using BERT-based fine-tuning, we\nachieved an accuracy of 84% on a dataset consisting of proprietary contracts.\nIt outperformed chain of thought prompting using Vicuna-13B by a margin of 9%.",
            "author": [
                "Anmol Singhal",
                "Preethu Rose Anish",
                "Shirish Karande",
                "Smita Ghaisas"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01398v1",
                "http://arxiv.org/pdf/2312.01398v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01397v1",
            "title": "Visual Prompting Upgrades Neural Network Sparsification: A Data-Model\n  Perspective",
            "updated": "2023-12-03T13:50:24Z",
            "published": "2023-12-03T13:50:24Z",
            "summary": "The rapid development of large-scale deep learning models questions the\naffordability of hardware platforms, which necessitates the pruning to reduce\ntheir computational and memory footprints. Sparse neural networks as the\nproduct, have demonstrated numerous favorable benefits like low complexity,\nundamaged generalization, etc. Most of the prominent pruning strategies are\ninvented from a model-centric perspective, focusing on searching and preserving\ncrucial weights by analyzing network topologies. However, the role of data and\nits interplay with model-centric pruning has remained relatively unexplored. In\nthis research, we introduce a novel data-model co-design perspective: to\npromote superior weight sparsity by learning important model topology and\nadequate input data in a synergetic manner. Specifically, customized Visual\nPrompts are mounted to upgrade neural Network sparsification in our proposed\nVPNs framework. As a pioneering effort, this paper conducts systematic\ninvestigations about the impact of different visual prompts on model pruning\nand suggests an effective joint optimization approach. Extensive experiments\nwith 3 network architectures and 8 datasets evidence the substantial\nperformance improvements from VPNs over existing start-of-the-art pruning\nalgorithms. Furthermore, we find that subnetworks discovered by VPNs from\npre-trained models enjoy better transferability across diverse downstream\nscenarios. These insights shed light on new promising possibilities of\ndata-model co-designs for vision model sparsification.",
            "author": [
                "Can Jin",
                "Tianjin Huang",
                "Yihua Zhang",
                "Mykola Pechenizkiy",
                "Sijia Liu",
                "Shiwei Liu",
                "Tianlong Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01397v1",
                "http://arxiv.org/pdf/2312.01397v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01392v1",
            "title": "Neural Network Characterization and Entropy Regulated Data Balancing\n  through Principal Component Analysis",
            "updated": "2023-12-03T13:39:36Z",
            "published": "2023-12-03T13:39:36Z",
            "summary": "This paper examines the relationship between the behavior of a neural network\nand the distribution formed from the projections of the data records into the\nspace spanned by the low-order principal components of the training data. For\nexample, in a benchmark calculation involving rotated and unrotated MNIST\ndigits, classes (digits) that are mapped far from the origin in a\nlow-dimensional principal component space and that overlap minimally with other\ndigits converge rapidly and exhibit high degrees of accuracy in neural network\ncalculations that employ the associated components of each data record as\ninputs. Further, if the space spanned by these low-order principal components\nis divided into bins and the input data records that are mapped into a given\nbin averaged, the resulting pattern can be distinguished by its geometric\nfeatures which interpolate between those of adjacent bins in an analogous\nmanner to variational autoencoders. Based on this observation, a simply\nrealized data balancing procedure can be realized by evaluating the entropy\nassociated with each histogram bin and subsequently repeating the original\nimage data associated with the bin by a number of times that is determined from\nthis entropy.",
            "author": [
                "David Yevick",
                "Karolina Hutchison"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01392v1",
                "http://arxiv.org/pdf/2312.01392v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01389v1",
            "title": "Deep Learning Assisted Raman Spectroscopy for Rapid Identification of 2D\n  Materials",
            "updated": "2023-12-03T13:26:27Z",
            "published": "2023-12-03T13:26:27Z",
            "summary": "Two-dimensional (2D) materials have attracted extensive attention due to\ntheir unique characteristics and application potentials. Raman spectroscopy, as\na rapid and non-destructive probe, exhibits distinct features and holds notable\nadvantages in the structural characterization of 2D materials. However,\ntraditional data analysis of Raman spectra relies on manual interpretation and\nfeature extraction, which are both time-consuming and subjective. In this work,\nwe employ deep learning techniques, including classificatory and generative\ndeep learning, to assist the analysis of Raman spectra of typical 2D materials.\nFor the limited and unevenly distributed Raman spectral data, we propose a data\naugmentation approach based on Denoising Diffusion Probabilistic Models (DDPM)\nto augment the training dataset and construct a four-layer Convolutional Neural\nNetwork (CNN) for 2D material classification. Experimental results illustrate\nthe effectiveness of DDPM in addressing data limitations and significantly\nimproved classification model performance. The proposed DDPM-CNN method shows\nhigh reliability, with 100%classification accuracy. Our work demonstrates the\npracticality of deep learning-assisted Raman spectroscopy for high-precision\nrecognition and classification of 2D materials, offering a promising avenue for\nrapid and automated spectral analysis.",
            "author": [
                "Yaping Qi",
                "Dan Hu",
                "Zhenping Wu",
                "Ming Zheng",
                "Guanghui Cheng",
                "Yucheng Jiang",
                "Yong P. Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01389v1",
                "http://arxiv.org/pdf/2312.01389v1"
            ],
            "primary_category": "physics.app-ph",
            "category": [
                "physics.app-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01386v1",
            "title": "Regret Optimality of GP-UCB",
            "updated": "2023-12-03T13:20:08Z",
            "published": "2023-12-03T13:20:08Z",
            "summary": "Gaussian Process Upper Confidence Bound (GP-UCB) is one of the most popular\nmethods for optimizing black-box functions with noisy observations, due to its\nsimple structure and superior performance. Its empirical successes lead to a\nnatural, yet unresolved question: Is GP-UCB regret optimal? In this paper, we\noffer the first generally affirmative answer to this important open question in\nthe Bayesian optimization literature. We establish new upper bounds on both the\nsimple and cumulative regret of GP-UCB when the objective function to optimize\nadmits certain smoothness property. These upper bounds match the known minimax\nlower bounds (up to logarithmic factors independent of the feasible region's\ndimensionality) for optimizing functions with the same smoothness.\nIntriguingly, our findings indicate that, with the same level of exploration,\nGP-UCB can simultaneously achieve optimality in both simple and cumulative\nregret. The crux of our analysis hinges on a refined uniform error bound for\nonline estimation of functions in reproducing kernel Hilbert spaces. This error\nbound, which we derive from empirical process theory, is of independent\ninterest, and its potential applications may reach beyond the scope of this\nstudy.",
            "author": [
                "Wenjia Wang",
                "Xiaowei Zhang",
                "Lu Zou"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01386v1",
                "http://arxiv.org/pdf/2312.01386v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01385v1",
            "title": "Optimization Strategies for Beam Direction and Dose Distribution\n  Selection in Radiotherapy Planning",
            "updated": "2023-12-03T13:15:24Z",
            "published": "2023-12-03T13:15:24Z",
            "summary": "Radiotherapy planning is a critical aspect of cancer treatment, where the\noptimal selection of beam directions and dose distributions significantly\nimpacts treatment efficacy and patient outcomes. Traditionally, this process\ninvolves time-consuming manual trial-and-error methods, leading to suboptimal\ntreatment plans. To address this challenge, optimization strategies based on\nadvanced artificial intelligence (AI) techniques have been explored. This paper\npresents an investigation into the application of AI-driven optimization\nmethods for beam direction and dose distribution selection in radiotherapy\nplanning. The study proposes an approach utilizing Convolutional Neural\nNetworks (CNN) to learn the relationship between patient anatomy and optimal\nbeam orientations. The CNN model is trained on a dataset comprising anatomical\nfeatures and corresponding beam orientations, derived from a column generation\n(CG) algorithm. Additionally, Particle Swarm Optimization (PSO) and Grey Wolf\nOptimization (GWO) algorithms are employed to optimize the CNN's weights and\nbiases to attain the Fluence Map Optimization (FMO) objective function.\nExperiments are conducted using data from 70 clinical prostate cancer patients.\nThe results demonstrate the effectiveness of the CNN-PSO and CNN-GWO approaches\nin generating beam orientations that yield treatment plans with dose\ndistributions comparable to those obtained through traditional CG. DVH analysis\nof the resulting plans for different anatomical structures validates the\naccuracy and feasibility of the CNN-GWO model in radiotherapy planning. The\nfindings of this study highlight the potential of AI-driven optimization\nstrategies to revolutionize radiotherapy planning by significantly reducing\nplanning time and enhancing treatment plan quality.",
            "author": [
                "Keshav Kumar K.",
                "NVSL Narasimham",
                "A. Ramakrishna Prasad"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01385v1",
                "http://arxiv.org/pdf/2312.01385v1"
            ],
            "primary_category": "physics.med-ph",
            "category": [
                "physics.med-ph",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01381v1",
            "title": "Language-driven All-in-one Adverse Weather Removal",
            "updated": "2023-12-03T13:05:54Z",
            "published": "2023-12-03T13:05:54Z",
            "summary": "All-in-one (AiO) frameworks restore various adverse weather degradations with\na single set of networks jointly. To handle various weather conditions, an AiO\nframework is expected to adaptively learn weather-specific knowledge for\ndifferent degradations and shared knowledge for common patterns. However,\nexisting methods: 1) rely on extra supervision signals, which are usually\nunknown in real-world applications; 2) employ fixed network structures, which\nrestrict the diversity of weather-specific knowledge. In this paper, we propose\na Language-driven Restoration framework (LDR) to alleviate the aforementioned\nissues. First, we leverage the power of pre-trained vision-language (PVL)\nmodels to enrich the diversity of weather-specific knowledge by reasoning about\nthe occurrence, type, and severity of degradation, generating description-based\ndegradation priors. Then, with the guidance of degradation prior, we sparsely\nselect restoration experts from a candidate list dynamically based on a\nMixture-of-Experts (MoE) structure. This enables us to adaptively learn the\nweather-specific and shared knowledge to handle various weather conditions\n(e.g., unknown or mixed weather). Experiments on extensive restoration\nscenarios show our superior performance (see Fig. 1). The source code will be\nmade available.",
            "author": [
                "Hao Yang",
                "Liyuan Pan",
                "Yan Yang",
                "Wei Liang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01381v1",
                "http://arxiv.org/pdf/2312.01381v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01379v1",
            "title": "Relation between PLS and OLS regression in terms of the eigenvalue\n  distribution of the regressor covariance matrix",
            "updated": "2023-12-03T13:00:03Z",
            "published": "2023-12-03T13:00:03Z",
            "summary": "Partial least squares (PLS) is a dimensionality reduction technique\nintroduced in the field of chemometrics and successfully employed in many other\nareas. The PLS components are obtained by maximizing the covariance between\nlinear combinations of the regressors and of the target variables. In this\nwork, we focus on its application to scalar regression problems. PLS regression\nconsists in finding the least squares predictor that is a linear combination of\na subset of the PLS components. Alternatively, PLS regression can be formulated\nas a least squares problem restricted to a Krylov subspace. This equivalent\nformulation is employed to analyze the distance between\n${\\hat{\\boldsymbol\\beta}\\;}_{\\mathrm{PLS}}^{\\scriptscriptstyle {(L)}}$, the PLS\nestimator of the vector of coefficients of the linear regression model based on\n$L$ PLS components, and $\\hat{\\boldsymbol \\beta}_{\\mathrm{OLS}}$, the one\nobtained by ordinary least squares (OLS), as a function of $L$. Specifically,\n${\\hat{\\boldsymbol\\beta}\\;}_{\\mathrm{PLS}}^{\\scriptscriptstyle {(L)}}$ is the\nvector of coefficients in the aforementioned Krylov subspace that is closest to\n$\\hat{\\boldsymbol \\beta}_{\\mathrm{OLS}}$ in terms of the Mahalanobis distance\nwith respect to the covariance matrix of the OLS estimate. We provide a bound\non this distance that depends only on the distribution of the eigenvalues of\nthe regressor covariance matrix. Numerical examples on synthetic and real-world\ndata are used to illustrate how the distance between\n${\\hat{\\boldsymbol\\beta}\\;}_{\\mathrm{PLS}}^{\\scriptscriptstyle {(L)}}$ and\n$\\hat{\\boldsymbol \\beta}_{\\mathrm{OLS}}$ depends on the number of clusters in\nwhich the eigenvalues of the regressor covariance matrix are grouped.",
            "author": [
                "David del Val",
                "Jos\u00e9 R. Berrendero",
                "Alberto Su\u00e1rez"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01379v1",
                "http://arxiv.org/pdf/2312.01379v1"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME",
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02719v1",
            "title": "A Conditional Denoising Diffusion Probabilistic Model for Point Cloud\n  Upsampling",
            "updated": "2023-12-03T12:41:41Z",
            "published": "2023-12-03T12:41:41Z",
            "summary": "Point cloud upsampling (PCU) enriches the representation of raw point clouds,\nsignificantly improving the performance in downstream tasks such as\nclassification and reconstruction. Most of the existing point cloud upsampling\nmethods focus on sparse point cloud feature extraction and upsampling module\ndesign. In a different way, we dive deeper into directly modelling the gradient\nof data distribution from dense point clouds. In this paper, we proposed a\nconditional denoising diffusion probability model (DDPM) for point cloud\nupsampling, called PUDM. Specifically, PUDM treats the sparse point cloud as a\ncondition, and iteratively learns the transformation relationship between the\ndense point cloud and the noise. Simultaneously, PUDM aligns with a dual\nmapping paradigm to further improve the discernment of point features. In this\ncontext, PUDM enables learning complex geometry details in the ground truth\nthrough the dominant features, while avoiding an additional upsampling module\ndesign. Furthermore, to generate high-quality arbitrary-scale point clouds\nduring inference, PUDM exploits the prior knowledge of the scale between sparse\npoint clouds and dense point clouds during training by parameterizing a rate\nfactor. Moreover, PUDM exhibits strong noise robustness in experimental\nresults. In the quantitative and qualitative evaluations on PU1K and PUGAN,\nPUDM significantly outperformed existing methods in terms of Chamfer Distance\n(CD) and Hausdorff Distance (HD), achieving state of the art (SOTA)\nperformance.",
            "author": [
                "Wentao Qu",
                "Yuantian Shao",
                "Lingwu Meng",
                "Xiaoshui Huang",
                "Liang Xiao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02719v1",
                "http://arxiv.org/pdf/2312.02719v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02997v1",
            "title": "Simulation-Based Inference of Surface Accumulation and Basal Melt Rates\n  of an Antarctic Ice Shelf from Isochronal Layers",
            "updated": "2023-12-03T12:22:45Z",
            "published": "2023-12-03T12:22:45Z",
            "summary": "The ice shelves buttressing the Antarctic ice sheet determine the rate of\nice-discharge into the surrounding oceans. The geometry of ice shelves, and\nhence their buttressing strength, is determined by ice flow as well as by the\nlocal surface accumulation and basal melt rates, governed by atmospheric and\noceanic conditions. Contemporary methods resolve one of these rates, but\ntypically not both. Moreover, there is little information of how they changed\nin time. We present a new method to simultaneously infer the surface\naccumulation and basal melt rates averaged over decadal and centennial\ntimescales. We infer the spatial dependence of these rates along flow line\ntransects using internal stratigraphy observed by radars, using a kinematic\nforward model of internal stratigraphy. We solve the inverse problem using\nsimulation-based inference (SBI). SBI performs Bayesian inference by training\nneural networks on simulations of the forward model to approximate the\nposterior distribution, allowing us to also quantify uncertainties over the\ninferred parameters. We demonstrate the validity of our method on a synthetic\nexample, and apply it to Ekstr\\\"om Ice Shelf, Antarctica, for which newly\nacquired radar measurements are available. We obtain posterior distributions of\nsurface accumulation and basal melt averaging over 42, 84, 146, and 188 years\nbefore 2022. Our results suggest stable atmospheric and oceanographic\nconditions over this period in this catchment of Antarctica. Use of observed\ninternal stratigraphy can separate the effects of surface accumulation and\nbasal melt, allowing them to be interpreted in a historical context of the last\ncenturies and beyond.",
            "author": [
                "Guy Moss",
                "Vjeran Vi\u0161njevi\u0107",
                "Olaf Eisen",
                "Falk M. Oraschewski",
                "Cornelius Schr\u00f6der",
                "Jakob H. Macke",
                "Reinhard Drews"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02997v1",
                "http://arxiv.org/pdf/2312.02997v1"
            ],
            "primary_category": "physics.ao-ph",
            "category": [
                "physics.ao-ph",
                "cs.LG",
                "physics.geo-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01361v1",
            "title": "MoEC: Mixture of Experts Implicit Neural Compression",
            "updated": "2023-12-03T12:02:23Z",
            "published": "2023-12-03T12:02:23Z",
            "summary": "Emerging Implicit Neural Representation (INR) is a promising data compression\ntechnique, which represents the data using the parameters of a Deep Neural\nNetwork (DNN). Existing methods manually partition a complex scene into local\nregions and overfit the INRs into those regions. However, manually designing\nthe partition scheme for a complex scene is very challenging and fails to\njointly learn the partition and INRs. To solve the problem, we propose MoEC, a\nnovel implicit neural compression method based on the theory of mixture of\nexperts. Specifically, we use a gating network to automatically assign a\nspecific INR to a 3D point in the scene. The gating network is trained jointly\nwith the INRs of different local regions. Compared with block-wise and\ntree-structured partitions, our learnable partition can adaptively find the\noptimal partition in an end-to-end manner. We conduct detailed experiments on\nmassive and diverse biomedical data to demonstrate the advantages of MoEC\nagainst existing approaches. In most of experiment settings, we have achieved\nstate-of-the-art results. Especially in cases of extreme compression ratios,\nsuch as 6000x, we are able to uphold the PSNR of 48.16.",
            "author": [
                "Jianchen Zhao",
                "Cheng-Ching Tseng",
                "Ming Lu",
                "Ruichuan An",
                "Xiaobao Wei",
                "He Sun",
                "Shanghang Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01361v1",
                "http://arxiv.org/pdf/2312.01361v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01357v1",
            "title": "Analyze the robustness of three NMF algorithms (Robust NMF with L1 norm,\n  L2-1 norm NMF, L2 NMF)",
            "updated": "2023-12-03T11:39:04Z",
            "published": "2023-12-03T11:39:04Z",
            "summary": "Non-negative matrix factorization (NMF) and its variants have been widely\nemployed in clustering and classification tasks (Long, & Jian , 2021). However,\nnoises can seriously affect the results of our experiments. Our research is\ndedicated to investigating the noise robustness of non-negative matrix\nfactorization (NMF) in the face of different types of noise. Specifically, we\nadopt three different NMF algorithms, namely L1 NMF, L2 NMF, and L21 NMF, and\nuse the ORL and YaleB data sets to simulate a series of experiments with\nsalt-and-pepper noise and Block-occlusion noise separately. In the experiment,\nwe use a variety of evaluation indicators, including root mean square error\n(RMSE), accuracy (ACC), and normalized mutual information (NMI), to evaluate\nthe performance of different NMF algorithms in noisy environments. Through\nthese indicators, we quantify the resistance of NMF algorithms to noise and\ngain insights into their feasibility in practical applications.",
            "author": [
                "Cheng Zeng",
                "Jiaqi Tian",
                "Yixuan Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01357v1",
                "http://arxiv.org/pdf/2312.01357v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01351v1",
            "title": "Deep learning and traditional-based CAD schemes for the pulmonary\n  embolism diagnosis: A survey",
            "updated": "2023-12-03T11:15:07Z",
            "published": "2023-12-03T11:15:07Z",
            "summary": "Nowadays, pulmonary Computed Tomography Angiography (CTA) is the main tool\nfor detecting Pulmonary Embolism (PE). However, manual interpretation of CTA\nvolume requires a radiologist, which is time-consuming and error-prone due to\nthe specific conditions of lung tissue, large volume of data, lack of\nexperience, and eye fatigue. Therefore, Computer-Aided Design (CAD) systems are\nused as a second opinion for the diagnosis of PE. The purpose of this article\nis to review, evaluate, and compare the performance of deep learning and\ntraditional-based CAD system for diagnosis PE and to help physicians and\nresearchers in this field. In this study, all articles available in databases\nsuch as IEEE, ScienceDirect, Wiley, Springer, Nature, and Wolters Kluwer in the\nfield of PE diagnosis were examined using traditional and deep learning\nmethods. From 2002 to 2023, 23 papers were studied to extract the articles with\nthe considered limitations. Each paper presents an automatic PE detection\nsystem that we evaluate using criteria such as sensitivity, False Positives\n(FP), and the number of datasets. This research work includes recent studies,\nstate-of-the-art research works, and a more comprehensive overview compared to\npreviously published review articles in this research area.",
            "author": [
                "Seyed Hesamoddin Hosseini",
                "Amir Hossein Taherinia",
                "Mahdi Saadatmand"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01351v1",
                "http://arxiv.org/pdf/2312.01351v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01350v1",
            "title": "Honesty Is the Best Policy: Defining and Mitigating AI Deception",
            "updated": "2023-12-03T11:11:57Z",
            "published": "2023-12-03T11:11:57Z",
            "summary": "Deceptive agents are a challenge for the safety, trustworthiness, and\ncooperation of AI systems. We focus on the problem that agents might deceive in\norder to achieve their goals (for instance, in our experiments with language\nmodels, the goal of being evaluated as truthful). There are a number of\nexisting definitions of deception in the literature on game theory and symbolic\nAI, but there is no overarching theory of deception for learning agents in\ngames. We introduce a formal definition of deception in structural causal\ngames, grounded in the philosophy literature, and applicable to real-world\nmachine learning systems. Several examples and results illustrate that our\nformal definition aligns with the philosophical and commonsense meaning of\ndeception. Our main technical result is to provide graphical criteria for\ndeception. We show, experimentally, that these results can be used to mitigate\ndeception in reinforcement learning agents and language models.",
            "author": [
                "Francis Rhys Ward",
                "Francesco Belardinelli",
                "Francesca Toni",
                "Tom Everitt"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01350v1",
                "http://arxiv.org/pdf/2312.01350v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01344v1",
            "title": "tsMorph: generation of semi-synthetic time series to understand\n  algorithm performance",
            "updated": "2023-12-03T10:40:07Z",
            "published": "2023-12-03T10:40:07Z",
            "summary": "Time series forecasting is a subject of significant scientific and industrial\nimportance. Despite the widespread utilization of forecasting methods, there is\na dearth of research aimed at comprehending the conditions under which these\nmethods yield favorable or unfavorable performances. Empirical studies,\nalthough common, encounter challenges due to the limited availability of\ndatasets, impeding the extraction of reliable insights. To address this, we\npresent tsMorph, a straightforward approach for generating semi-synthetic time\nseries through dataset morphing. tsMorph operates by creating a sequence of\ndatasets derived from two original datasets. These newly generated datasets\nexhibit a progressive departure from the characteristics of one dataset and a\nconvergence toward the attributes of the other. This method provides a valuable\nalternative for obtaining substantial datasets. In this paper, we demonstrate\nthe utility of tsMorph by assessing the performance of the Long Short-Term\nMemory Network forecasting algorithm. The time series under examination are\nsourced from the NN5 Competition. The findings reveal compelling insights.\nNotably, the performance of the Long Short-Term Memory Network improves\nproportionally with the frequency of the time series. These experiments affirm\nthat tsMorph serves as an effective tool for gaining an understanding of\nforecasting algorithm behaviors, offering a pathway to overcome the limitations\nposed by empirical studies and enabling more extensive and reliable\nexperimentation.",
            "author": [
                "Mois\u00e9s Santos",
                "Andr\u00e9 de Carvalho",
                "Carlos Soares"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01344v1",
                "http://arxiv.org/pdf/2312.01344v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01342v1",
            "title": "Graph Coordinates and Conventional Neural Networks -- An Alternative for\n  Graph Neural Networks",
            "updated": "2023-12-03T10:14:10Z",
            "published": "2023-12-03T10:14:10Z",
            "summary": "Graph-based data present unique challenges and opportunities for machine\nlearning. Graph Neural Networks (GNNs), and especially those algorithms that\ncapture graph topology through message passing for neighborhood aggregation,\nhave been a leading solution. However, these networks often require substantial\ncomputational resources and may not optimally leverage the information\ncontained in the graph's topology, particularly for large-scale or complex\ngraphs. We propose Topology Coordinate Neural Network (TCNN) and Directional\nVirtual Coordinate Neural Network (DVCNN) as novel and efficient alternatives\nto message passing GNNs, that directly leverage the graph's topology,\nsidestepping the computational challenges presented by competing algorithms.\nOur proposed methods can be viewed as a reprise of classic techniques for graph\nembedding for neural network feature engineering, but they are novel in that\nour embedding techniques leverage ideas in Graph Coordinates (GC) that are\nlacking in current practice. Experimental results, benchmarked against the Open\nGraph Benchmark Leaderboard, demonstrate that TCNN and DVCNN achieve\ncompetitive or superior performance to message passing GNNs. For similar levels\nof accuracy and ROC-AUC, TCNN and DVCNN need far fewer trainable parameters\nthan contenders of the OGBN Leaderboard. The proposed TCNN architecture\nrequires fewer parameters than any neural network method currently listed in\nthe OGBN Leaderboard for both OGBN-Proteins and OGBN-Products datasets.\nConversely, our methods achieve higher performance for a similar number of\ntrainable parameters. By providing an efficient and effective alternative to\nmessage passing GNNs, our work expands the toolbox of techniques for\ngraph-based machine learning.",
            "author": [
                "Zheyi Qin",
                "Randy Paffenroth",
                "Anura P. Jayasumana"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01342v1",
                "http://arxiv.org/pdf/2312.01342v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01340v1",
            "title": "A quasi-one-dimensional hydrogen-bonded monolayer ice phase",
            "updated": "2023-12-03T10:08:24Z",
            "published": "2023-12-03T10:08:24Z",
            "summary": "Bernal-Fowler ice rules govern the phase behaviors of crystalline bulk water\nby stipulating that each water molecule forms four hydrogen bonds. However, in\nextreme or constrained conditions, the arrangement of water molecules deviates\nfrom conventional ice rules, resulting in properties significantly different\nfrom bulk water. In this study, we employ machine learning-driven\nfirst-principles simulations to observe a unique violation of the ice rules in\na monolayer of water confined within a hydrophobic channel. We observe a\nquasi-one-dimensional hydrogen-bonded structure in a flat-rhombic phase. This\nphase consists of strongly hydrogen-bonded linear chains of water molecules\nthat zig-zag along one dimension, stabilized by van der Waals interactions that\nstack these chains along the other dimension. This arrangement of strong and\nweak bonds bears similarities with a new class of functional materials called\nquasi-one-dimensional van der Waals materials. The unusual interplay of\nhydrogen bonding and van der Waals interactions in flat-rhombic ice results in\natypical proton disorder, including long-range proton ordering and coherent\nproton dynamics. Our work sets the stage for discovering new\nconfinement-induced low-dimensional hydrogen-bonded materials and exploiting\ntheir electronic, vibronic, and optical properties in ways analogous to\nquasi-one-dimensional van der Waals materials.",
            "author": [
                "Pavan Ravindra",
                "Xavier R. Advincula",
                "Christoph Schran",
                "Angelos Michaelides",
                "Venkat Kapil"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01340v1",
                "http://arxiv.org/pdf/2312.01340v1"
            ],
            "primary_category": "cond-mat.stat-mech",
            "category": [
                "cond-mat.stat-mech",
                "cond-mat.mes-hall",
                "cond-mat.mtrl-sci",
                "cond-mat.soft"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01339v1",
            "title": "AI-Powered Arabic Crossword Puzzle Generation for Educational\n  Applications",
            "updated": "2023-12-03T10:03:50Z",
            "published": "2023-12-03T10:03:50Z",
            "summary": "This paper presents the first Arabic crossword puzzle generator driven by\nadvanced AI technology. Leveraging cutting-edge large language models including\nGPT4, GPT3-Davinci, GPT3-Curie, GPT3-Babbage, GPT3-Ada, and BERT, the system\ngenerates distinctive and challenging clues. Based on a dataset comprising over\n50,000 clue-answer pairs, the generator employs fine-tuning, few/zero-shot\nlearning strategies, and rigorous quality-checking protocols to enforce the\ngeneration of high-quality clue-answer pairs. Importantly, educational\ncrosswords contribute to enhancing memory, expanding vocabulary, and promoting\nproblem-solving skills, thereby augmenting the learning experience through a\nfun and engaging approach, reshaping the landscape of traditional learning\nmethods. The overall system can be exploited as a powerful educational tool\nthat amalgamates AI and innovative learning techniques, heralding a\ntransformative era for Arabic crossword puzzles and the intersection of\ntechnology and education.",
            "author": [
                "Kamyar Zeinalipour",
                "Mohamed Zaky Saad",
                "Marco Maggini",
                "Marco Gori"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01339v1",
                "http://arxiv.org/pdf/2312.01339v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01338v1",
            "title": "Enhancing and Adapting in the Clinic: Source-free Unsupervised Domain\n  Adaptation for Medical Image Enhancement",
            "updated": "2023-12-03T10:01:59Z",
            "published": "2023-12-03T10:01:59Z",
            "summary": "Medical imaging provides many valuable clues involving anatomical structure\nand pathological characteristics. However, image degradation is a common issue\nin clinical practice, which can adversely impact the observation and diagnosis\nby physicians and algorithms. Although extensive enhancement models have been\ndeveloped, these models require a well pre-training before deployment, while\nfailing to take advantage of the potential value of inference data after\ndeployment. In this paper, we raise an algorithm for source-free unsupervised\ndomain adaptive medical image enhancement (SAME), which adapts and optimizes\nenhancement models using test data in the inference phase. A\nstructure-preserving enhancement network is first constructed to learn a robust\nsource model from synthesized training data. Then a teacher-student model is\ninitialized with the source model and conducts source-free unsupervised domain\nadaptation (SFUDA) by knowledge distillation with the test data. Additionally,\na pseudo-label picker is developed to boost the knowledge distillation of\nenhancement tasks. Experiments were implemented on ten datasets from three\nmedical image modalities to validate the advantage of the proposed algorithm,\nand setting analysis and ablation studies were also carried out to interpret\nthe effectiveness of SAME. The remarkable enhancement performance and benefits\nfor downstream tasks demonstrate the potential and generalizability of SAME.\nThe code is available at\nhttps://github.com/liamheng/Annotation-free-Medical-Image-Enhancement.",
            "author": [
                "Heng Li",
                "Ziqin Lin",
                "Zhongxi Qiu",
                "Zinan Li",
                "Huazhu Fu",
                "Yan Hu",
                "Jiang Liu"
            ],
            "link": [
                "http://dx.doi.org/10.1109/TMI.2023.3335651",
                "http://arxiv.org/abs/2312.01338v1",
                "http://arxiv.org/pdf/2312.01338v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01335v1",
            "title": "Facial Emotion Recognition Under Mask Coverage Using a Data Augmentation\n  Technique",
            "updated": "2023-12-03T09:50:46Z",
            "published": "2023-12-03T09:50:46Z",
            "summary": "Identifying human emotions using AI-based computer vision systems, when\nindividuals wear face masks, presents a new challenge in the current Covid-19\npandemic. In this study, we propose a facial emotion recognition system capable\nof recognizing emotions from individuals wearing different face masks. A novel\ndata augmentation technique was utilized to improve the performance of our\nmodel using four mask types for each face image. We evaluated the effectiveness\nof four convolutional neural networks, Alexnet, Squeezenet, Resnet50 and\nVGGFace2 that were trained using transfer learning. The experimental findings\nrevealed that our model works effectively in multi-mask mode compared to\nsingle-mask mode. The VGGFace2 network achieved the highest accuracy rate, with\n97.82% for the person-dependent mode and 74.21% for the person-independent mode\nusing the JAFFE dataset. However, we evaluated our proposed model using the\nUIBVFED dataset. The Resnet50 has demonstrated superior performance, with\naccuracies of 73.68% for the person-dependent mode and 59.57% for the\nperson-independent mode. Moreover, we employed metrics such as precision,\nsensitivity, specificity, AUC, F1 score, and confusion matrix to measure our\nsystem's efficiency in detail. Additionally, the LIME algorithm was used to\nvisualize CNN's decision-making strategy.",
            "author": [
                "Aref Farhadipour",
                "Pouya Taghipour"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01335v1",
                "http://arxiv.org/pdf/2312.01335v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01324v1",
            "title": "MABViT -- Modified Attention Block Enhances Vision Transformers",
            "updated": "2023-12-03T09:00:31Z",
            "published": "2023-12-03T09:00:31Z",
            "summary": "Recent studies have demonstrated the effectiveness of Gated Linear Units\n(GLU) in enhancing transformer models, particularly in Large Language Models\n(LLMs). Additionally, utilizing a parallel configuration within each\nTransformer block rather than the conventional serialized method has been\nrevealed to accelerate the training of LLMs without significantly impacting\nperformance. However, when the MLP and attention block were run in parallel for\nthe image classification task, we observed a noticeable decline in performance.\nWe propose a novel transformer variant that integrates non-linearity within the\nattention block to tackle this problem. We implemented the GLU-based activation\nfunction on the Value tensor, and this new technique surpasses the current\nstate-of-the-art S/16 variant of Vision Transformers by 0.6% on the ImageNet-1K\ndataset while utilizing fewer parameters. It also supersedes the B/16 variant\nwhile using only half the parameters. Furthermore, we provide results with the\nGELU activation function variant to confirm our assertions. Lastly, we showcase\nthat the MABViT variants exhibit greater potential when utilized in deep\ntransformers compared to the standard architecture.",
            "author": [
                "Mahesh Ramesh",
                "Aswinkumar Ramkumar"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01324v1",
                "http://arxiv.org/pdf/2312.01324v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01315v1",
            "title": "Few-shot Shape Recognition by Learning Deep Shape-aware Features",
            "updated": "2023-12-03T08:12:23Z",
            "published": "2023-12-03T08:12:23Z",
            "summary": "Traditional shape descriptors have been gradually replaced by convolutional\nneural networks due to their superior performance in feature extraction and\nclassification. The state-of-the-art methods recognize object shapes via image\nreconstruction or pixel classification. However , these methods are biased\ntoward texture information and overlook the essential shape descriptions, thus,\nthey fail to generalize to unseen shapes. We are the first to propose a fewshot\nshape descriptor (FSSD) to recognize object shapes given only one or a few\nsamples. We employ an embedding module for FSSD to extract\ntransformation-invariant shape features. Secondly, we develop a dual attention\nmechanism to decompose and reconstruct the shape features via learnable shape\nprimitives. In this way, any shape can be formed through a finite set basis,\nand the learned representation model is highly interpretable and extendable to\nunseen shapes. Thirdly, we propose a decoding module to include the supervision\nof shape masks and edges and align the original and reconstructed shape\nfeatures, enforcing the learned features to be more shape-aware. Lastly, all\nthe proposed modules are assembled into a few-shot shape recognition scheme.\nExperiments on five datasets show that our FSSD significantly improves the\nshape classification compared to the state-of-the-art under the few-shot\nsetting.",
            "author": [
                "Wenlong Shi",
                "Changsheng Lu",
                "Ming Shao",
                "Yinjie Zhang",
                "Siyu Xia",
                "Piotr Koniusz"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01315v1",
                "http://arxiv.org/pdf/2312.01315v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02214v1",
            "title": "FlashAvatar: High-Fidelity Digital Avatar Rendering at 300FPS",
            "updated": "2023-12-03T07:23:53Z",
            "published": "2023-12-03T07:23:53Z",
            "summary": "We propose FlashAvatar, a novel and lightweight 3D animatable avatar\nrepresentation that could reconstruct a digital avatar from a short monocular\nvideo sequence in minutes and render high-fidelity photo-realistic images at\n300FPS on a consumer-grade GPU. To achieve this, we maintain a uniform 3D\nGaussian field embedded in the surface of a parametric face model and learn\nextra spatial offset to model non-surface regions and subtle facial details.\nWhile full use of geometric priors can capture high-frequency facial details\nand preserve exaggerated expressions, proper initialization can help reduce the\nnumber of Gaussians, thus enabling super-fast rendering speed. Extensive\nexperimental results demonstrate that FlashAvatar outperforms existing works\nregarding visual quality and personalized details and is almost an order of\nmagnitude faster in rendering speed. Project page:\nhttps://ustc3dv.github.io/FlashAvatar/",
            "author": [
                "Jun Xiang",
                "Xuan Gao",
                "Yudong Guo",
                "Juyong Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02214v1",
                "http://arxiv.org/pdf/2312.02214v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02213v1",
            "title": "JarviX: A LLM No code Platform for Tabular Data Analysis and\n  Optimization",
            "updated": "2023-12-03T07:03:04Z",
            "published": "2023-12-03T07:03:04Z",
            "summary": "In this study, we introduce JarviX, a sophisticated data analytics framework.\nJarviX is designed to employ Large Language Models (LLMs) to facilitate an\nautomated guide and execute high-precision data analyzes on tabular datasets.\nThis framework emphasizes the significance of varying column types,\ncapitalizing on state-of-the-art LLMs to generate concise data insight\nsummaries, propose relevant analysis inquiries, visualize data effectively, and\nprovide comprehensive explanations for results drawn from an extensive data\nanalysis pipeline. Moreover, JarviX incorporates an automated machine learning\n(AutoML) pipeline for predictive modeling. This integration forms a\ncomprehensive and automated optimization cycle, which proves particularly\nadvantageous for optimizing machine configuration. The efficacy and\nadaptability of JarviX are substantiated through a series of practical use case\nstudies.",
            "author": [
                "Shang-Ching Liu",
                "ShengKun Wang",
                "Wenqi Lin",
                "Chung-Wei Hsiung",
                "Yi-Chen Hsieh",
                "Yu-Ping Cheng",
                "Sian-Hong Luo",
                "Tsungyao Chang",
                "Jianwei Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02213v1",
                "http://arxiv.org/pdf/2312.02213v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.DB",
                "stat.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01306v1",
            "title": "On Significance of Subword tokenization for Low Resource and Efficient\n  Named Entity Recognition: A case study in Marathi",
            "updated": "2023-12-03T06:53:53Z",
            "published": "2023-12-03T06:53:53Z",
            "summary": "Named Entity Recognition (NER) systems play a vital role in NLP applications\nsuch as machine translation, summarization, and question-answering. These\nsystems identify named entities, which encompass real-world concepts like\nlocations, persons, and organizations. Despite extensive research on NER\nsystems for the English language, they have not received adequate attention in\nthe context of low resource languages. In this work, we focus on NER for\nlow-resource language and present our case study in the context of the Indian\nlanguage Marathi. The advancement of NLP research revolves around the\nutilization of pre-trained transformer models such as BERT for the development\nof NER models. However, we focus on improving the performance of shallow models\nbased on CNN, and LSTM by combining the best of both worlds. In the era of\ntransformers, these traditional deep learning models are still relevant because\nof their high computational efficiency. We propose a hybrid approach for\nefficient NER by integrating a BERT-based subword tokenizer into vanilla\nCNN/LSTM models. We show that this simple approach of replacing a traditional\nword-based tokenizer with a BERT-tokenizer brings the accuracy of vanilla\nsingle-layer models closer to that of deep pre-trained models like BERT. We\nshow the importance of using sub-word tokenization for NER and present our\nstudy toward building efficient NLP systems. The evaluation is performed on\nL3Cube-MahaNER dataset using tokenizers from MahaBERT, MahaGPT, IndicBERT, and\nmBERT.",
            "author": [
                "Harsh Chaudhari",
                "Anuja Patil",
                "Dhanashree Lavekar",
                "Pranav Khairnar",
                "Raviraj Joshi",
                "Sachin Pande"
            ],
            "link": [
                "http://dx.doi.org/10.1007/978-981-99-6550-2_37",
                "http://arxiv.org/abs/2312.01306v1",
                "http://arxiv.org/pdf/2312.01306v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01305v1",
            "title": "ViVid-1-to-3: Novel View Synthesis with Video Diffusion Models",
            "updated": "2023-12-03T06:50:15Z",
            "published": "2023-12-03T06:50:15Z",
            "summary": "Generating novel views of an object from a single image is a challenging\ntask. It requires an understanding of the underlying 3D structure of the object\nfrom an image and rendering high-quality, spatially consistent new views. While\nrecent methods for view synthesis based on diffusion have shown great progress,\nachieving consistency among various view estimates and at the same time abiding\nby the desired camera pose remains a critical problem yet to be solved. In this\nwork, we demonstrate a strikingly simple method, where we utilize a pre-trained\nvideo diffusion model to solve this problem. Our key idea is that synthesizing\na novel view could be reformulated as synthesizing a video of a camera going\naround the object of interest -- a scanning video -- which then allows us to\nleverage the powerful priors that a video diffusion model would have learned.\nThus, to perform novel-view synthesis, we create a smooth camera trajectory to\nthe target view that we wish to render, and denoise using both a\nview-conditioned diffusion model and a video diffusion model. By doing so, we\nobtain a highly consistent novel view synthesis, outperforming the state of the\nart.",
            "author": [
                "Jeong-gi Kwak",
                "Erqun Dong",
                "Yuhe Jin",
                "Hanseok Ko",
                "Shweta Mahajan",
                "Kwang Moo Yi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01305v1",
                "http://arxiv.org/pdf/2312.01305v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01301v1",
            "title": "Churn Prediction via Multimodal Fusion Learning:Integrating Customer\n  Financial Literacy, Voice, and Behavioral Data",
            "updated": "2023-12-03T06:28:55Z",
            "published": "2023-12-03T06:28:55Z",
            "summary": "In todays competitive landscape, businesses grapple with customer retention.\nChurn prediction models, although beneficial, often lack accuracy due to the\nreliance on a single data source. The intricate nature of human behavior and\nhigh dimensional customer data further complicate these efforts. To address\nthese concerns, this paper proposes a multimodal fusion learning model for\nidentifying customer churn risk levels in financial service providers. Our\nmultimodal approach integrates customer sentiments financial literacy (FL)\nlevel, and financial behavioral data, enabling more accurate and bias-free\nchurn prediction models. The proposed FL model utilizes a SMOGN COREG\nsupervised model to gauge customer FL levels from their financial data. The\nbaseline churn model applies an ensemble artificial neural network and\noversampling techniques to predict churn propensity in high-dimensional\nfinancial data. We also incorporate a speech emotion recognition model\nemploying a pre-trained CNN-VGG16 to recognize customer emotions based on\npitch, energy, and tone. To integrate these diverse features while retaining\nunique insights, we introduced late and hybrid fusion techniques that\ncomplementary boost coordinated multimodal co learning. Robust metrics were\nutilized to evaluate the proposed multimodal fusion model and hence the\napproach validity, including mean average precision and macro-averaged F1\nscore. Our novel approach demonstrates a marked improvement in churn\nprediction, achieving a test accuracy of 91.2%, a Mean Average Precision (MAP)\nscore of 66, and a Macro-Averaged F1 score of 54 through the proposed hybrid\nfusion learning technique compared with late fusion and baseline models.\nFurthermore, the analysis demonstrates a positive correlation between negative\nemotions, low FL scores, and high-risk customers.",
            "author": [
                "David Hason Rudd",
                "Huan Huo",
                "Md Rafiqul Islam",
                "Guandong Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01301v1",
                "http://arxiv.org/pdf/2312.01301v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CE",
                "cs.CV",
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01299v1",
            "title": "Robust Non-parametric Knowledge-based Diffusion Least Mean Squares over\n  Adaptive Networks",
            "updated": "2023-12-03T06:18:59Z",
            "published": "2023-12-03T06:18:59Z",
            "summary": "The present study proposes incorporating non-parametric knowledge into the\ndiffusion least-mean-squares algorithm in the framework of a maximum a\nposteriori (MAP) estimation. The proposed algorithm leads to a robust\nestimation of an unknown parameter vector in a group of cooperative estimators.\nUtilizing kernel density estimation and buffering some intermediate\nestimations, the prior distribution and conditional likelihood of the\nparameters vector in each node are calculated. Pseudo Huber loss function is\nused for designing the likelihood function. Also, an error thresholding\nfunction is defined to reduce the computational overhead as well as more\nrelaxation against noise, which stops the update every time an error is less\nthan a predefined threshold. The performance of the proposed algorithm is\nexamined in the stationary and non-stationary scenarios in the presence of\nGaussian and non-Gaussian noise. Results show the robustness of the proposed\nalgorithm in the presence of different noise types.",
            "author": [
                "Soheil Ashkezari-Toussi",
                "Hadi sadoghi-Yazdi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01299v1",
                "http://arxiv.org/pdf/2312.01299v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01296v1",
            "title": "Anomaly Detection Under Uncertainty Using Distributionally Robust\n  Optimization Approach",
            "updated": "2023-12-03T06:13:22Z",
            "published": "2023-12-03T06:13:22Z",
            "summary": "Anomaly detection is defined as the problem of finding data points that do\nnot follow the patterns of the majority. Among the various proposed methods for\nsolving this problem, classification-based methods, including one-class Support\nVector Machines (SVM) are considered effective and state-of-the-art. The\none-class SVM method aims to find a decision boundary to distinguish between\nnormal data points and anomalies using only the normal data. On the other hand,\nmost real-world problems involve some degree of uncertainty, where the true\nprobability distribution of each data point is unknown, and estimating it is\noften difficult and costly. Assuming partial distribution information such as\nthe first and second-order moments is known, a distributionally robust\nchance-constrained model is proposed in which the probability of\nmisclassification is low. By utilizing a mapping function to a higher\ndimensional space, the proposed model will be capable of classifying\norigin-inseparable datasets. Also, by adopting the kernel idea, the need for\nexplicitly knowing the mapping is eliminated, computations can be performed in\nthe input space, and computational complexity is reduced. Computational results\nvalidate the robustness of the proposed model under different probability\ndistributions and also the superiority of the proposed model compared to the\nstandard one-class SVM in terms of various evaluation metrics.",
            "author": [
                "Amir Hossein Noormohammadia",
                "Seyed Ali MirHassania",
                "Farnaz Hooshmand Khaligh"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01296v1",
                "http://arxiv.org/pdf/2312.01296v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01295v1",
            "title": "Two-stage dynamic creative optimization under sparse ambiguous samples\n  for e-commerce advertising",
            "updated": "2023-12-03T06:02:27Z",
            "published": "2023-12-03T06:02:27Z",
            "summary": "Ad creative is one of the main mediums for e-commerce advertising. In our\napproach we decouple this dynamic creative optimization into two stages, a\ncascaded structure that can trade off between effectiveness and efficiency. In\nthe first stage, we train an automatic creative optimization architecture based\non autoco to simulate complex interactions between creative elements. Although\nwe obtained the ranking of different creatives under a sku, because we bucketed\nand merged historical data according to periods, this confuses the ctr\ndiversity of the same ad creatives on different days and weakens the ability to\nseparate ambiguous samples. Therefore, we propose a transformer-based rerank\nmodel. With the help of the rank model, we propose a distillation method to\nlearn the relative order of ideas and extract the ranking knowledge to guide\nthe rerank learning. The creative order soft labels under each sku are\ngenerated by the rank model to alleviate the dilemma that a large number of\nunder-represented creatives cannot obtain real labels. Through the knowledge\ndiffusion of rerank, the ambiguous samples are associated with the positive and\nnegative samples. Cascade rerank and autoco to output the estimated value of\nthe synthetic ad image. In the second stage, we designed a bandit model, and\nthe bandit selected one of the output ad of the first stage for timely\ndelivery. Experimental results show that our method can outperform competing\nbaselines in terms of sctr. Online A/B testing shows that our method improves\nctr by 10% compared to the baseline.",
            "author": [
                "Guandong Li",
                "Xian Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01295v1",
                "http://arxiv.org/pdf/2312.01295v1"
            ],
            "primary_category": "cs.MM",
            "category": [
                "cs.MM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01294v1",
            "title": "Deep Ensembles Meets Quantile Regression: Uncertainty-aware Imputation\n  for Time Series",
            "updated": "2023-12-03T05:52:30Z",
            "published": "2023-12-03T05:52:30Z",
            "summary": "Multivariate time series are everywhere. Nevertheless, real-world time series\ndata often exhibit numerous missing values, which is the time series imputation\ntask. Although previous deep learning methods have been shown to be effective\nfor time series imputation, they are shown to produce overconfident\nimputations, which might be a potentially overlooked threat to the reliability\nof the intelligence system. Score-based diffusion method(i.e., CSDI) is\neffective for the time series imputation task but computationally expensive due\nto the nature of the generative diffusion model framework. In this paper, we\npropose a non-generative time series imputation method that produces accurate\nimputations with inherent uncertainty and meanwhile is computationally\nefficient. Specifically, we incorporate deep ensembles into quantile regression\nwith a shared model backbone and a series of quantile discrimination\nfunctions.This framework combines the merits of accurate uncertainty estimation\nof deep ensembles and quantile regression and above all, the shared model\nbackbone tremendously reduces most of the computation overhead of the multiple\nensembles. We examine the performance of the proposed method on two real-world\ndatasets: air quality and health-care datasets and conduct extensive\nexperiments to show that our method excels at making deterministic and\nprobabilistic predictions. Compared with the score-based diffusion method:\nCSDI, we can obtain comparable forecasting results and is better when more data\nis missing. Furthermore, as a non-generative model compared with CSDI, the\nproposed method consumes a much smaller computation overhead, yielding much\nfaster training speed and fewer model parameters.",
            "author": [
                "Ying Liu",
                "Peng Cui",
                "Wenbo Hu",
                "Richang Hong"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01294v1",
                "http://arxiv.org/pdf/2312.01294v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01288v1",
            "title": "Task-Oriented Edge Networks: Decentralized Learning Over Wireless\n  Fronthaul",
            "updated": "2023-12-03T05:24:28Z",
            "published": "2023-12-03T05:24:28Z",
            "summary": "This paper studies task-oriented edge networks where multiple edge\ninternet-of-things nodes execute machine learning tasks with the help of\npowerful deep neural networks (DNNs) at a network cloud. Separate edge nodes\n(ENs) result in a partially observable system where they can only get\npartitioned features of the global network states. These local observations\nneed to be forwarded to the cloud via resource-constrained wireless fronthual\nlinks. Individual ENs compress their local observations into uplink fronthaul\nmessages using task-oriented encoder DNNs. Then, the cloud carries out a remote\ninference task by leveraging received signals. Such a distributed topology\nrequests a decentralized training and decentralized execution (DTDE) learning\nframework for designing edge-cloud cooperative inference rules and their\ndecentralized training strategies. First, we develop fronthaul-cooperative DNN\narchitecture along with proper uplink coordination protocols suitable for\nwireless fronthaul interconnection. Inspired by the nomographic function, an\nefficient cloud inference model becomes an integration of a number of shallow\nDNNs. This modulized architecture brings versatile calculations that are\nindependent of the number of ENs. Next, we present a decentralized training\nalgorithm of separate edge-cloud DNNs over downlink wireless fronthaul\nchannels. An appropriate downlink coordination protocol is proposed, which\nbackpropagates gradient vectors wirelessly from the cloud to the ENs.",
            "author": [
                "Hoon Lee",
                "Seung-Wook Kim"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01288v1",
                "http://arxiv.org/pdf/2312.01288v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "cs.LG",
                "eess.SP",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01286v1",
            "title": "Continuous Convolutional Neural Networks for Disruption Prediction in\n  Nuclear Fusion Plasmas",
            "updated": "2023-12-03T05:09:36Z",
            "published": "2023-12-03T05:09:36Z",
            "summary": "Grid decarbonization for climate change requires dispatchable carbon-free\nenergy like nuclear fusion. The tokamak concept offers a promising path for\nfusion, but one of the foremost challenges in implementation is the occurrence\nof energetic plasma disruptions. In this study, we delve into Machine Learning\napproaches to predict plasma state outcomes. Our contributions are twofold: (1)\nWe present a novel application of Continuous Convolutional Neural Networks for\ndisruption prediction and (2) We examine the advantages and disadvantages of\ncontinuous models over discrete models for disruption prediction by comparing\nour model with the previous, discrete state of the art, and show that\ncontinuous models offer significantly better performance (Area Under the\nReceiver Operating Characteristic Curve = 0.974 v.s. 0.799) with fewer\nparameters",
            "author": [
                "William F Arnold",
                "Lucas Spangher",
                "Christina Rea"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01286v1",
                "http://arxiv.org/pdf/2312.01286v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "physics.plasm-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01285v1",
            "title": "A Literature Review on the Smart Wheelchair Systems",
            "updated": "2023-12-03T05:05:31Z",
            "published": "2023-12-03T05:05:31Z",
            "summary": "This study offers an in-depth analysis of smart wheelchair (SW) systems,\ncharting their progression from early developments to future innovations. It\ndelves into various Brain-Computer Interface (BCI) systems, including mu\nrhythm, event-related potential, and steady-state visual evoked potential. The\npaper addresses challenges in signal categorization, proposing the sparse\nBayesian extreme learning machine as an innovative solution. Additionally, it\nexplores the integration of emotional states in BCI systems, the application of\nalternative control methods such as EMG-based systems, and the deployment of\nintelligent adaptive interfaces utilizing recurrent quantum neural networks.\nThe study also covers advancements in autonomous navigation, assistance, and\nmapping, emphasizing their importance in SW systems. The human aspect of SW\ninteraction receives considerable attention, specifically in terms of privacy,\nphysiological factors, and the refinement of control mechanisms. The paper\nacknowledges the commercial challenges faced, like the limitations of indoor\nusage and the necessity for user training. For future applications, the\nresearch explores the potential of autonomous systems adept at adapting to\nchanging environments and user needs. This exploration includes reinforcement\nlearning and various control methods, such as eye and voice control, to improve\nadaptability and interaction. The potential integration with smart home\ntechnologies, including advanced features such as robotic arms, is also\nconsidered, aiming to further enhance user accessibility and independence.\nUltimately, this study seeks to provide a thorough overview of SW systems,\npresenting extensive research to detail their historical evolution, current\nstate, and future prospects.",
            "author": [
                "Yane Kim",
                "Bharath Velamala",
                "Youngseo Choi",
                "Yujin Kim",
                "Hyunkin Kim",
                "Nishad Kulkarni",
                "Eung-Joo Lee"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01285v1",
                "http://arxiv.org/pdf/2312.01285v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01284v1",
            "title": "Stable Messenger: Steganography for Message-Concealed Image Generation",
            "updated": "2023-12-03T05:02:43Z",
            "published": "2023-12-03T05:02:43Z",
            "summary": "In the ever-expanding digital landscape, safeguarding sensitive information\nremains paramount. This paper delves deep into digital protection, specifically\nfocusing on steganography. While prior research predominantly fixated on\nindividual bit decoding, we address this limitation by introducing ``message\naccuracy'', a novel metric evaluating the entirety of decoded messages for a\nmore holistic evaluation. In addition, we propose an adaptive universal loss\ntailored to enhance message accuracy, named Log-Sum-Exponential (LSE) loss,\nthereby significantly improving the message accuracy of recent approaches.\nFurthermore, we also introduce a new latent-aware encoding technique in our\nframework named \\Approach, harnessing pretrained Stable Diffusion for advanced\nsteganographic image generation, giving rise to a better trade-off between\nimage quality and message recovery. Throughout experimental results, we have\ndemonstrated the superior performance of the new LSE loss and latent-aware\nencoding technique. This comprehensive approach marks a significant step in\nevolving evaluation metrics, refining loss functions, and innovating image\nconcealment techniques, aiming for more robust and dependable information\nprotection.",
            "author": [
                "Quang Nguyen",
                "Truong Vu",
                "Cuong Pham",
                "Anh Tran",
                "Khoi Nguyen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01284v1",
                "http://arxiv.org/pdf/2312.01284v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01283v1",
            "title": "Deeper into Self-Supervised Monocular Indoor Depth Estimation",
            "updated": "2023-12-03T04:55:32Z",
            "published": "2023-12-03T04:55:32Z",
            "summary": "Monocular depth estimation using Convolutional Neural Networks (CNNs) has\nshown impressive performance in outdoor driving scenes. However,\nself-supervised learning of indoor depth from monocular sequences is quite\nchallenging for researchers because of the following two main reasons. One is\nthe large areas of low-texture regions and the other is the complex ego-motion\non indoor training datasets. In this work, our proposed method, named\nIndoorDepth, consists of two innovations. In particular, we first propose a\nnovel photometric loss with improved structural similarity (SSIM) function to\ntackle the challenge from low-texture regions. Moreover, in order to further\nmitigate the issue of inaccurate ego-motion prediction, multiple photometric\nlosses at different stages are used to train a deeper pose network with two\nresidual pose blocks. Subsequent ablation study can validate the effectiveness\nof each new idea. Experiments on the NYUv2 benchmark demonstrate that our\nIndoorDepth outperforms the previous state-of-the-art methods by a large\nmargin. In addition, we also validate the generalization ability of our method\non ScanNet dataset. Code is availabe at https://github.com/fcntes/IndoorDepth.",
            "author": [
                "Chao Fan",
                "Zhenyu Yin",
                "Yue Li",
                "Feiqing Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01283v1",
                "http://arxiv.org/pdf/2312.01283v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01281v1",
            "title": "Mendata: A Framework to Purify Manipulated Training Data",
            "updated": "2023-12-03T04:40:08Z",
            "published": "2023-12-03T04:40:08Z",
            "summary": "Untrusted data used to train a model might have been manipulated to endow the\nlearned model with hidden properties that the data contributor might later\nexploit. Data purification aims to remove such manipulations prior to training\nthe model. We propose Mendata, a novel framework to purify manipulated training\ndata. Starting from a small reference dataset in which a large majority of the\ninputs are clean, Mendata perturbs the training inputs so that they retain\ntheir utility but are distributed similarly (as measured by Wasserstein\ndistance) to the reference data, thereby eliminating hidden properties from the\nlearned model. A key challenge is how to find such perturbations, which we\naddress by formulating a min-max optimization problem and developing a two-step\nmethod to iteratively solve it. We demonstrate the effectiveness of Mendata by\napplying it to defeat state-of-the-art data poisoning and data tracing\ntechniques.",
            "author": [
                "Zonghao Huang",
                "Neil Gong",
                "Michael K. Reiter"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01281v1",
                "http://arxiv.org/pdf/2312.01281v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02211v1",
            "title": "Cycle-consistent Generative Adversarial Network Synthetic CT for MR-only\n  Adaptive Radiation Therapy on MR-Linac",
            "updated": "2023-12-03T04:38:17Z",
            "published": "2023-12-03T04:38:17Z",
            "summary": "Purpose: This study assesses the effectiveness of Deep Learning (DL) for\ncreating synthetic CT (sCT) images in MR-guided adaptive radiation therapy\n(MRgART).\n  Methods: A Cycle-GAN model was trained with MRI and CT scan slices from\nMR-LINAC treatments, generating sCT volumes. The analysis involved\nretrospective treatment plan data from patients with various tumors. sCT images\nwere compared with standard CT scans using mean absolute error in Hounsfield\nUnits (HU) and image similarity metrics (SSIM, PSNR, NCC). sCT volumes were\nintegrated into a clinical treatment system for dosimetric re-evaluation.\n  Results: The model, trained on 8405 frames from 57 patients and tested on 357\nsCT frames from 17 patients, showed sCTs comparable to dCTs in electron density\nand structural similarity with MRI scans. The MAE between sCT and dCT was 49.2\n+/- 13.2 HU, with sCT NCC exceeding dCT by 0.06, and SSIM and PSNR at 0.97 +/-\n0.01 and 19.9 +/- 1.6 respectively. Dosimetric evaluations indicated minimal\ndifferences between sCTs and dCTs, with sCTs showing better air-bubble\nreconstruction.\n  Conclusions: DL-based sCT generation on MR-Linacs is accurate for dose\ncalculation and optimization in MRgART. This could facilitate MR-only treatment\nplanning, enhancing simulation and adaptive planning efficiency on MR-Linacs.",
            "author": [
                "Gabriel L. Asher",
                "Bassem I. Zaki",
                "Gregory A. Russo",
                "Gobind S. Gill",
                "Charles R. Thomas",
                "Temiloluwa O. Prioleau",
                "Rongxiao Zhang",
                "Brady Hunt"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02211v1",
                "http://arxiv.org/pdf/2312.02211v1"
            ],
            "primary_category": "physics.med-ph",
            "category": [
                "physics.med-ph",
                "cs.CV",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01280v1",
            "title": "Brain Decodes Deep Nets",
            "updated": "2023-12-03T04:36:04Z",
            "published": "2023-12-03T04:36:04Z",
            "summary": "We developed a tool for visualizing and analyzing large pre-trained vision\nmodels by mapping them onto the brain, thus exposing their hidden inside. Our\ninnovation arises from a surprising usage of brain encoding: predicting brain\nfMRI measurements in response to images. We report two findings. First,\nexplicit mapping between the brain and deep-network features across dimensions\nof space, layers, scales, and channels is crucial. This mapping method,\nFactorTopy, is plug-and-play for any deep-network; with it, one can paint a\npicture of the network onto the brain (literally!). Second, our visualization\nshows how different training methods matter: they lead to remarkable\ndifferences in hierarchical organization and scaling behavior, growing with\nmore data or network capacity. It also provides insight into finetuning: how\npre-trained models change when adapting to small datasets. Our method is\npractical: only 3K images are enough to learn a network-to-brain mapping.",
            "author": [
                "Huzheng Yang",
                "James Gee",
                "Jianbo Shi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01280v1",
                "http://arxiv.org/pdf/2312.01280v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01279v1",
            "title": "TextGenSHAP: Scalable Post-hoc Explanations in Text Generation with Long\n  Documents",
            "updated": "2023-12-03T04:35:04Z",
            "published": "2023-12-03T04:35:04Z",
            "summary": "Large language models (LLMs) have attracted huge interest in practical\napplications given their increasingly accurate responses and coherent reasoning\nabilities. Given their nature as black-boxes using complex reasoning processes\non their inputs, it is inevitable that the demand for scalable and faithful\nexplanations for LLMs' generated content will continue to grow. There have been\nmajor developments in the explainability of neural network models over the past\ndecade. Among them, post-hoc explainability methods, especially Shapley values,\nhave proven effective for interpreting deep learning models. However, there are\nmajor challenges in scaling up Shapley values for LLMs, particularly when\ndealing with long input contexts containing thousands of tokens and\nautoregressively generated output sequences. Furthermore, it is often unclear\nhow to effectively utilize generated explanations to improve the performance of\nLLMs. In this paper, we introduce TextGenSHAP, an efficient post-hoc\nexplanation method incorporating LM-specific techniques. We demonstrate that\nthis leads to significant increases in speed compared to conventional Shapley\nvalue computations, reducing processing times from hours to minutes for\ntoken-level explanations, and to just seconds for document-level explanations.\nIn addition, we demonstrate how real-time Shapley values can be utilized in two\nimportant scenarios, providing better understanding of long-document question\nanswering by localizing important words and sentences; and improving existing\ndocument retrieval systems through enhancing the accuracy of selected passages\nand ultimately the final responses.",
            "author": [
                "James Enouen",
                "Hootan Nakhost",
                "Sayna Ebrahimi",
                "Sercan O Arik",
                "Yan Liu",
                "Tomas Pfister"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01279v1",
                "http://arxiv.org/pdf/2312.01279v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01275v1",
            "title": "A Review of Link Prediction Applications in Network Biology",
            "updated": "2023-12-03T04:23:51Z",
            "published": "2023-12-03T04:23:51Z",
            "summary": "In the domain of network biology, the interactions among heterogeneous\ngenomic and molecular entities are represented through networks. Link\nprediction (LP) methodologies are instrumental in inferring missing or\nprospective associations within these biological networks. In this review, we\nsystematically dissect the attributes of local, centrality, and embedding-based\nLP approaches, applied to static and dynamic biological networks. We undertake\nan examination of the current applications of LP metrics for predicting links\nbetween diseases, genes, proteins, RNA, microbiomes, drugs, and neurons. We\ncarry out comprehensive performance evaluations on established biological\nnetwork datasets to show the practical applications of standard LP models.\nMoreover, we compare the similarity in prediction trends among the models and\nthe specific network attributes that contribute to effective link prediction,\nbefore underscoring the role of LP in addressing the formidable challenges\nprevalent in biological systems, ranging from noise, bias, and data sparseness\nto interpretability. We conclude the review with an exploration of the\nessential characteristics expected from future LP models, poised to advance our\ncomprehension of the intricate interactions governing biological systems.",
            "author": [
                "Ahmad F. Al Musawi",
                "Satyaki Roy",
                "Preetam Ghosh"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01275v1",
                "http://arxiv.org/pdf/2312.01275v1"
            ],
            "primary_category": "q-bio.MN",
            "category": [
                "q-bio.MN",
                "cs.LG",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02210v1",
            "title": "Low-Precision Mixed-Computation Models for Inference on Edge",
            "updated": "2023-12-03T04:22:29Z",
            "published": "2023-12-03T04:22:29Z",
            "summary": "This paper presents a mixed-computation neural network processing approach\nfor edge applications that incorporates low-precision (low-width) Posit and\nlow-precision fixed point (FixP) number systems. This mixed-computation\napproach employs 4-bit Posit (Posit4), which has higher precision around zero,\nfor representing weights with high sensitivity, while it uses 4-bit FixP\n(FixP4) for representing other weights. A heuristic for analyzing the\nimportance and the quantization error of the weights is presented to assign the\nproper number system to different weights. Additionally, a gradient\napproximation for Posit representation is introduced to improve the quality of\nweight updates in the backpropagation process. Due to the high energy\nconsumption of the fully Posit-based computations, neural network operations\nare carried out in FixP or Posit/FixP. An efficient hardware implementation of\na MAC operation with a first Posit operand and FixP for a second operand and\naccumulator is presented. The efficacy of the proposed low-precision\nmixed-computation approach is extensively assessed on vision and language\nmodels. The results show that, on average, the accuracy of the\nmixed-computation is about 1.5% higher than that of FixP with a cost of 0.19%\nenergy overhead.",
            "author": [
                "Seyedarmin Azizi",
                "Mahdi Nazemi",
                "Mehdi Kamal",
                "Massoud Pedram"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02210v1",
                "http://arxiv.org/pdf/2312.02210v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01274v1",
            "title": "Learning to Compose SuperWeights for Neural Parameter Allocation Search",
            "updated": "2023-12-03T04:20:02Z",
            "published": "2023-12-03T04:20:02Z",
            "summary": "Neural parameter allocation search (NPAS) automates parameter sharing by\nobtaining weights for a network given an arbitrary, fixed parameter budget.\nPrior work has two major drawbacks we aim to address. First, there is a\ndisconnect in the sharing pattern between the search and training steps, where\nweights are warped for layers of different sizes during the search to measure\nsimilarity, but not during training, resulting in reduced performance. To\naddress this, we generate layer weights by learning to compose sets of\nSuperWeights, which represent a group of trainable parameters. These\nSuperWeights are created to be large enough so they can be used to represent\nany layer in the network, but small enough that they are computationally\nefficient. The second drawback we address is the method of measuring similarity\nbetween shared parameters. Whereas prior work compared the weights themselves,\nwe argue this does not take into account the amount of conflict between the\nshared weights. Instead, we use gradient information to identify layers with\nshared weights that wish to diverge from each other. We demonstrate that our\nSuperWeight Networks consistently boost performance over the state-of-the-art\non the ImageNet and CIFAR datasets in the NPAS setting. We further show that\nour approach can generate parameters for many network architectures using the\nsame set of weights. This enables us to support tasks like efficient ensembling\nand anytime prediction, outperforming fully-parameterized ensembles with 17%\nfewer parameters.",
            "author": [
                "Piotr Teterwak",
                "Soren Nelson",
                "Nikoli Dryden",
                "Dina Bashkirova",
                "Kate Saenko",
                "Bryan A. Plummer"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01274v1",
                "http://arxiv.org/pdf/2312.01274v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01272v1",
            "title": "Multiscale Topology in Interactomic Network: From Transcriptome to\n  Antiaddiction Drug Repurposing",
            "updated": "2023-12-03T04:01:38Z",
            "published": "2023-12-03T04:01:38Z",
            "summary": "The escalating drug addiction crisis in the United States underscores the\nurgent need for innovative therapeutic strategies. This study embarked on an\ninnovative and rigorous strategy to unearth potential drug repurposing\ncandidates for opioid and cocaine addiction treatment, bridging the gap between\ntranscriptomic data analysis and drug discovery. We initiated our approach by\nconducting differential gene expression analysis on addiction-related\ntranscriptomic data to identify key genes. We propose a novel topological\ndifferentiation to identify key genes from a protein-protein interaction (PPI)\nnetwork derived from DEGs. This method utilizes persistent Laplacians to\naccurately single out pivotal nodes within the network, conducting this\nanalysis in a multiscale manner to ensure high reliability. Through rigorous\nliterature validation, pathway analysis, and data-availability scrutiny, we\nidentified three pivotal molecular targets, mTOR, mGluR5, and NMDAR, for drug\nrepurposing from DrugBank. We crafted machine learning models employing two\nnatural language processing (NLP)-based embeddings and a traditional 2D\nfingerprint, which demonstrated robust predictive ability in gauging binding\naffinities of DrugBank compounds to selected targets. Furthermore, we\nelucidated the interactions of promising drugs with the targets and evaluated\ntheir drug-likeness. This study delineates a multi-faceted and comprehensive\nanalytical framework, amalgamating bioinformatics, topological data analysis\nand machine learning, for drug repurposing in addiction treatment, setting the\nstage for subsequent experimental validation. The versatility of the methods we\ndeveloped allows for applications across a range of diseases and transcriptomic\ndatasets.",
            "author": [
                "Hongyan Du",
                "Guo-Wei Wei",
                "Tingjun Hou"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01272v1",
                "http://arxiv.org/pdf/2312.01272v1"
            ],
            "primary_category": "q-bio.BM",
            "category": [
                "q-bio.BM",
                "cs.LG",
                "q-bio.GN"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01267v1",
            "title": "Distributed Reinforcement Learning for Molecular Design: Antioxidant\n  case",
            "updated": "2023-12-03T03:23:13Z",
            "published": "2023-12-03T03:23:13Z",
            "summary": "Deep reinforcement learning has successfully been applied for molecular\ndiscovery as shown by the Molecule Deep Q-network (MolDQN) algorithm. This\nalgorithm has challenges when applied to optimizing new molecules: training\nsuch a model is limited in terms of scalability to larger datasets and the\ntrained model cannot be generalized to different molecules in the same dataset.\nIn this paper, a distributed reinforcement learning algorithm for antioxidants,\ncalled DA-MolDQN is proposed to address these problems. State-of-the-art bond\ndissociation energy (BDE) and ionization potential (IP) predictors are\nintegrated into DA-MolDQN, which are critical chemical properties while\noptimizing antioxidants. Training time is reduced by algorithmic improvements\nfor molecular modifications. The algorithm is distributed, scalable for up to\n512 molecules, and generalizes the model to a diverse set of molecules. The\nproposed models are trained with a proprietary antioxidant dataset. The results\nhave been reproduced with both proprietary and public datasets. The proposed\nmolecules have been validated with DFT simulations and a subset of them\nconfirmed in public \"unseen\" datasets. In summary, DA-MolDQN is up to 100x\nfaster than previous algorithms and can discover new optimized molecules from\nproprietary and public antioxidants.",
            "author": [
                "Huanyi Qin",
                "Denis Akhiyarov",
                "Sophie Loehle",
                "Kenneth Chiu",
                "Mauricio Araya-Polo"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01267v1",
                "http://arxiv.org/pdf/2312.01267v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.DC",
                "q-bio.BM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01266v1",
            "title": "A unified framework for covariate adjustment under stratified\n  randomization",
            "updated": "2023-12-03T03:20:39Z",
            "published": "2023-12-03T03:20:39Z",
            "summary": "Randomization, as a key technique in clinical trials, can eliminate sources\nof bias and produce comparable treatment groups. In randomized experiments, the\ntreatment effect is a parameter of general interest. Researchers have explored\nthe validity of using linear models to estimate the treatment effect and\nperform covariate adjustment and thus improve the estimation efficiency.\nHowever, the relationship between covariates and outcomes is not necessarily\nlinear, and is often intricate. Advances in statistical theory and related\ncomputer technology allow us to use nonparametric and machine learning methods\nto better estimate the relationship between covariates and outcomes and thus\nobtain further efficiency gains. However, theoretical studies on how to draw\nvalid inferences when using nonparametric and machine learning methods under\nstratified randomization are yet to be conducted. In this paper, we discuss a\nunified framework for covariate adjustment and corresponding statistical\ninference under stratified randomization and present a detailed proof of the\nvalidity of using local linear kernel-weighted least squares regression for\ncovariate adjustment in treatment effect estimators as a special case. In the\ncase of high-dimensional data, we additionally propose an algorithm for\nstatistical inference using machine learning methods under stratified\nrandomization, which makes use of sample splitting to alleviate the\nrequirements on the asymptotic properties of machine learning methods. Finally,\nwe compare the performances of treatment effect estimators using different\nmachine learning methods by considering various data generation scenarios, to\nguide practical research.",
            "author": [
                "Fuyi Tu",
                "Wei Ma",
                "Hanzhong Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01266v1",
                "http://arxiv.org/pdf/2312.01266v1"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME",
                "math.ST",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01262v1",
            "title": "A Review and A Robust Framework of Data-Efficient 3D Scene Parsing with\n  Traditional/Learned 3D Descriptors",
            "updated": "2023-12-03T02:51:54Z",
            "published": "2023-12-03T02:51:54Z",
            "summary": "Existing state-of-the-art 3D point cloud understanding methods merely perform\nwell in a fully supervised manner. To the best of our knowledge, there exists\nno unified framework that simultaneously solves the downstream high-level\nunderstanding tasks including both segmentation and detection, especially when\nlabels are extremely limited. This work presents a general and simple framework\nto tackle point cloud understanding when labels are limited. The first\ncontribution is that we have done extensive methodology comparisons of\ntraditional and learned 3D descriptors for the task of weakly supervised 3D\nscene understanding, and validated that our adapted traditional PFH-based 3D\ndescriptors show excellent generalization ability across different domains. The\nsecond contribution is that we proposed a learning-based region merging\nstrategy based on the affinity provided by both the traditional/learned 3D\ndescriptors and learned semantics. The merging process takes both low-level\ngeometric and high-level semantic feature correlations into consideration.\nExperimental results demonstrate that our framework has the best performance\namong the three most important weakly supervised point clouds understanding\ntasks including semantic segmentation, instance segmentation, and object\ndetection even when very limited number of points are labeled. Our method,\ntermed Region Merging 3D (RM3D), has superior performance on ScanNet\ndata-efficient learning online benchmarks and other four large-scale 3D\nunderstanding benchmarks under various experimental settings, outperforming\ncurrent arts by a margin for various 3D understanding tasks without complicated\nlearning strategies such as active learning.",
            "author": [
                "Kangcheng Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01262v1",
                "http://arxiv.org/pdf/2312.01262v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02208v1",
            "title": "A Data-efficient Framework for Robotics Large-scale LiDAR Scene Parsing",
            "updated": "2023-12-03T02:38:51Z",
            "published": "2023-12-03T02:38:51Z",
            "summary": "Existing state-of-the-art 3D point clouds understanding methods only perform\nwell in a fully supervised manner. To the best of our knowledge, there exists\nno unified framework which simultaneously solves the downstream high-level\nunderstanding tasks, especially when labels are extremely limited. This work\npresents a general and simple framework to tackle point clouds understanding\nwhen labels are limited. We propose a novel unsupervised region expansion based\nclustering method for generating clusters. More importantly, we innovatively\npropose to learn to merge the over-divided clusters based on the local\nlow-level geometric property similarities and the learned high-level feature\nsimilarities supervised by weak labels. Hence, the true weak labels guide\npseudo labels merging taking both geometric and semantic feature correlations\ninto consideration. Finally, the self-supervised reconstruction and data\naugmentation optimization modules are proposed to guide the propagation of\nlabels among semantically similar points within a scene. Experimental Results\ndemonstrate that our framework has the best performance among the three most\nimportant weakly supervised point clouds understanding tasks including semantic\nsegmentation, instance segmentation, and object detection even when limited\npoints are labeled, under the data-efficient settings for the large-scale 3D\nsemantic scene parsing. The developed techniques have postentials to be applied\nto downstream tasks for better representations in robotic manipulation and\nrobotic autonomous navigation. Codes and models are publicly available at:\nhttps://github.com/KangchengLiu.",
            "author": [
                "Kangcheng Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02208v1",
                "http://arxiv.org/pdf/2312.02208v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01260v1",
            "title": "Rethinking PGD Attack: Is Sign Function Necessary?",
            "updated": "2023-12-03T02:26:58Z",
            "published": "2023-12-03T02:26:58Z",
            "summary": "Neural networks have demonstrated success in various domains, yet their\nperformance can be significantly degraded by even a small input perturbation.\nConsequently, the construction of such perturbations, known as adversarial\nattacks, has gained significant attention, many of which fall within\n\"white-box\" scenarios where we have full access to the neural network. Existing\nattack algorithms, such as the projected gradient descent (PGD), commonly take\nthe sign function on the raw gradient before updating adversarial inputs,\nthereby neglecting gradient magnitude information. In this paper, we present a\ntheoretical analysis of how such sign-based update algorithm influences\nstep-wise attack performance, as well as its caveat. We also interpret why\nprevious attempts of directly using raw gradients failed. Based on that, we\nfurther propose a new raw gradient descent (RGD) algorithm that eliminates the\nuse of sign. Specifically, we convert the constrained optimization problem into\nan unconstrained one, by introducing a new hidden variable of non-clipped\nperturbation that can move beyond the constraint. The effectiveness of the\nproposed RGD algorithm has been demonstrated extensively in experiments,\noutperforming PGD and other competitors in various settings, without incurring\nany additional computational overhead. The codes is available in\nhttps://github.com/JunjieYang97/RGD.",
            "author": [
                "Junjie Yang",
                "Tianlong Chen",
                "Xuxi Chen",
                "Zhangyang Wang",
                "Yingbin Liang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01260v1",
                "http://arxiv.org/pdf/2312.01260v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CR",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01256v1",
            "title": "Breaking XOR Arbiter PUFs without Reliability Information",
            "updated": "2023-12-03T01:39:09Z",
            "published": "2023-12-03T01:39:09Z",
            "summary": "Unreliable XOR Arbiter PUFs were broken by a machine learning attack, which\ntargets the underlying Arbiter PUFs individually. However, reliability\ninformation from the PUF was required for this attack.\n  We show that, for the first time, a perfectly reliable XOR Arbiter PUF, where\nno reliability information is accessible, can be efficiently attacked in the\nsame divide-and-conquer manner. Our key insight is that the responses of\ncorrelated challenges also reveal their distance to the decision boundary. This\nleads to a chosen challenge attack on XOR Arbiter PUFs. The effectiveness of\nour attack is confirmed through PUF simulation and FPGA implementation.",
            "author": [
                "Niloufar Sayadi",
                "Phuong Ha Nguyen",
                "Marten van Dijk",
                "Chenglu Jin"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01256v1",
                "http://arxiv.org/pdf/2312.01256v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01255v1",
            "title": "Meta ControlNet: Enhancing Task Adaptation via Meta Learning",
            "updated": "2023-12-03T01:36:45Z",
            "published": "2023-12-03T01:36:45Z",
            "summary": "Diffusion-based image synthesis has attracted extensive attention recently.\nIn particular, ControlNet that uses image-based prompts exhibits powerful\ncapability in image tasks such as canny edge detection and generates images\nwell aligned with these prompts. However, vanilla ControlNet generally requires\nextensive training of around 5000 steps to achieve a desirable control for a\nsingle task. Recent context-learning approaches have improved its adaptability,\nbut mainly for edge-based tasks, and rely on paired examples. Thus, two\nimportant open issues are yet to be addressed to reach the full potential of\nControlNet: (i) zero-shot control for certain tasks and (ii) faster adaptation\nfor non-edge-based tasks. In this paper, we introduce a novel Meta ControlNet\nmethod, which adopts the task-agnostic meta learning technique and features a\nnew layer freezing design. Meta ControlNet significantly reduces learning steps\nto attain control ability from 5000 to 1000. Further, Meta ControlNet exhibits\ndirect zero-shot adaptability in edge-based tasks without any finetuning, and\nachieves control within only 100 finetuning steps in more complex non-edge\ntasks such as Human Pose, outperforming all existing methods. The codes is\navailable in https://github.com/JunjieYang97/Meta-ControlNet.",
            "author": [
                "Junjie Yang",
                "Jinze Zhao",
                "Peihao Wang",
                "Zhangyang Wang",
                "Yingbin Liang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01255v1",
                "http://arxiv.org/pdf/2312.01255v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01249v1",
            "title": "A Multifidelity Sim-to-Real Pipeline for Verifiable and Compositional\n  Reinforcement Learning",
            "updated": "2023-12-02T23:46:27Z",
            "published": "2023-12-02T23:46:27Z",
            "summary": "We propose and demonstrate a compositional framework for training and\nverifying reinforcement learning (RL) systems within a multifidelity\nsim-to-real pipeline, in order to deploy reliable and adaptable RL policies on\nphysical hardware. By decomposing complex robotic tasks into component subtasks\nand defining mathematical interfaces between them, the framework allows for the\nindependent training and testing of the corresponding subtask policies, while\nsimultaneously providing guarantees on the overall behavior that results from\ntheir composition. By verifying the performance of these subtask policies using\na multifidelity simulation pipeline, the framework not only allows for\nefficient RL training, but also for a refinement of the subtasks and their\ninterfaces in response to challenges arising from discrepancies between\nsimulation and reality. In an experimental case study we apply the framework to\ntrain and deploy a compositional RL system that successfully pilots a Warthog\nunmanned ground robot.",
            "author": [
                "Cyrus Neary",
                "Christian Ellis",
                "Aryaman Singh Samyal",
                "Craig Lennon",
                "Ufuk Topcu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01249v1",
                "http://arxiv.org/pdf/2312.01249v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02206v1",
            "title": "Axiomatic Preference Modeling for Longform Question Answering",
            "updated": "2023-12-02T23:11:41Z",
            "published": "2023-12-02T23:11:41Z",
            "summary": "The remarkable abilities of large language models (LLMs) like GPT-4 partially\nstem from post-training processes like Reinforcement Learning from Human\nFeedback (RLHF) involving human preferences encoded in a reward model. However,\nthese reward models (RMs) often lack direct knowledge of why, or under what\nprinciples, the preferences annotations were made. In this study, we identify\nprinciples that guide RMs to better align with human preferences, and then\ndevelop an axiomatic framework to generate a rich variety of preference signals\nto uphold them. We use these axiomatic signals to train a model for scoring\nanswers to longform questions. Our approach yields a Preference Model with only\nabout 220M parameters that agrees with gold human-annotated preference labels\nmore often than GPT-4. The contributions of this work include: training a\nstandalone preference model that can score human- and LLM-generated answers on\nthe same scale; developing an axiomatic framework for generating training data\npairs tailored to certain principles; and showing that a small amount of\naxiomatic signals can help small models outperform GPT-4 in preference scoring.\nWe release our model on huggingface:\nhttps://huggingface.co/corbyrosset/axiomatic_preference_model",
            "author": [
                "Corby Rosset",
                "Guoqing Zheng",
                "Victor Dibia",
                "Ahmed Awadallah",
                "Paul Bennett"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02206v1",
                "http://arxiv.org/pdf/2312.02206v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01242v1",
            "title": "DDxT: Deep Generative Transformer Models for Differential Diagnosis",
            "updated": "2023-12-02T22:57:25Z",
            "published": "2023-12-02T22:57:25Z",
            "summary": "Differential Diagnosis (DDx) is the process of identifying the most likely\nmedical condition among the possible pathologies through the process of\nelimination based on evidence. An automated process that narrows a large set of\npathologies down to the most likely pathologies will be of great importance.\nThe primary prior works have relied on the Reinforcement Learning (RL) paradigm\nunder the intuition that it aligns better with how physicians perform DDx. In\nthis paper, we show that a generative approach trained with simpler supervised\nand self-supervised learning signals can achieve superior results on the\ncurrent benchmark. The proposed Transformer-based generative network, named\nDDxT, autoregressively produces a set of possible pathologies, i.e., DDx, and\npredicts the actual pathology using a neural network. Experiments are performed\nusing the DDXPlus dataset. In the case of DDx, the proposed network has\nachieved a mean accuracy of 99.82% and a mean F1 score of 0.9472. Additionally,\nmean accuracy reaches 99.98% with a mean F1 score of 0.9949 while predicting\nground truth pathology. The proposed DDxT outperformed the previous RL-based\napproaches by a big margin. Overall, the automated Transformer-based DDx\ngenerative model has the potential to become a useful tool for a physician in\ntimes of urgency.",
            "author": [
                "Mohammad Mahmudul Alam",
                "Edward Raff",
                "Tim Oates",
                "Cynthia Matuszek"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01242v1",
                "http://arxiv.org/pdf/2312.01242v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01241v1",
            "title": "Just-in-Time Security Patch Detection -- LLM At the Rescue for Data\n  Augmentation",
            "updated": "2023-12-02T22:53:26Z",
            "published": "2023-12-02T22:53:26Z",
            "summary": "In the face of growing vulnerabilities found in open-source software, the\nneed to identify {discreet} security patches has become paramount. The lack of\nconsistency in how software providers handle maintenance often leads to the\nrelease of security patches without comprehensive advisories, leaving users\nvulnerable to unaddressed security risks. To address this pressing issue, we\nintroduce a novel security patch detection system, LLMDA, which capitalizes on\nLarge Language Models (LLMs) and code-text alignment methodologies for patch\nreview, data enhancement, and feature combination. Within LLMDA, we initially\nutilize LLMs for examining patches and expanding data of PatchDB and SPI-DB,\ntwo security patch datasets from recent literature. We then use labeled\ninstructions to direct our LLMDA, differentiating patches based on security\nrelevance. Following this, we apply a PTFormer to merge patches with code,\nformulating hybrid attributes that encompass both the innate details and the\ninterconnections between the patches and the code. This distinctive combination\nmethod allows our system to capture more insights from the combined context of\npatches and code, hence improving detection precision. Finally, we devise a\nprobabilistic batch contrastive learning mechanism within batches to augment\nthe capability of the our LLMDA in discerning security patches. The results\nreveal that LLMDA significantly surpasses the start of the art techniques in\ndetecting security patches, underscoring its promise in fortifying software\nmaintenance.",
            "author": [
                "Xunzhu Tang",
                "Zhenghan Chen",
                "Kisub Kim",
                "Haoye Tian",
                "Saad Ezzini",
                "Jacques Klein"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01241v1",
                "http://arxiv.org/pdf/2312.01241v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02205v1",
            "title": "Disentangling the Effects of Data Augmentation and Format Transform in\n  Self-Supervised Learning of Image Representations",
            "updated": "2023-12-02T22:45:35Z",
            "published": "2023-12-02T22:45:35Z",
            "summary": "Self-Supervised Learning (SSL) enables training performant models using\nlimited labeled data. One of the pillars underlying vision SSL is the use of\ndata augmentations/perturbations of the input which do not significantly alter\nits semantic content. For audio and other temporal signals, augmentations are\ncommonly used alongside format transforms such as Fourier transforms or wavelet\ntransforms. Unlike augmentations, format transforms do not change the\ninformation contained in the data; rather, they express the same information in\ndifferent coordinates. In this paper, we study the effects of format transforms\nand augmentations both separately and together on vision SSL. We define\naugmentations in frequency space called Fourier Domain Augmentations (FDA) and\nshow that training SSL models on a combination of these and image augmentations\ncan improve the downstream classification accuracy by up to 1.3% on\nImageNet-1K. We also show improvements against SSL baselines in few-shot and\ntransfer learning setups using FDA. Surprisingly, we also observe that format\ntransforms can improve the quality of learned representations even without\naugmentations; however, the combination of the two techniques yields better\nquality.",
            "author": [
                "Neha Kalibhat",
                "Warren Morningstar",
                "Alex Bijamov",
                "Luyang Liu",
                "Karan Singhal",
                "Philip Mansfield"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02205v1",
                "http://arxiv.org/pdf/2312.02205v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01239v2",
            "title": "Motion Informed Needle Segmentation in Ultrasound Images",
            "updated": "2023-12-05T04:14:18Z",
            "published": "2023-12-02T22:25:24Z",
            "summary": "Segmenting a moving needle in ultrasound images is challenging due to the\npresence of artifacts, noise, and needle occlusion. This task becomes even more\ndemanding in scenarios where data availability is limited. Convolutional Neural\nNetworks (CNNs) have been successful in many computer vision applications, but\nstruggle to accurately segment needles without considering their motion. In\nthis paper, we present a novel approach for needle segmentation that combines\nclassical Kalman Filter (KF) techniques with data-driven learning,\nincorporating both needle features and needle motion. Our method offers two key\ncontributions. First, we propose a compatible framework that seamlessly\nintegrates into commonly used encoder-decoder style architectures. Second, we\ndemonstrate superior performance compared to recent state-of-the-art needle\nsegmentation models using our novel convolutional neural network (CNN) based\nKF-inspired block, achieving a 15\\% reduction in pixel-wise needle tip error\nand an 8\\% reduction in length error. Third, to our knowledge we are the first\nto implement a learnable filter to incorporate non-linear needle motion for\nimproving needle segmentation.",
            "author": [
                "Raghavv Goel",
                "Cecilia Morales",
                "Manpreet Singh",
                "Artur Dubrawski",
                "John Galeotti",
                "Howie Choset"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01239v2",
                "http://arxiv.org/pdf/2312.01239v2"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01238v1",
            "title": "A deep learning pipeline for cross-sectional and longitudinal multiview\n  data integration",
            "updated": "2023-12-02T22:24:35Z",
            "published": "2023-12-02T22:24:35Z",
            "summary": "Biomedical research now commonly integrates diverse data types or views from\nthe same individuals to better understand the pathobiology of complex diseases,\nbut the challenge lies in meaningfully integrating these diverse views.\nExisting methods often require the same type of data from all views\n(cross-sectional data only or longitudinal data only) or do not consider any\nclass outcome in the integration method, presenting limitations. To overcome\nthese limitations, we have developed a pipeline that harnesses the power of\nstatistical and deep learning methods to integrate cross-sectional and\nlongitudinal data from multiple sources. Additionally, it identifies key\nvariables contributing to the association between views and the separation\namong classes, providing deeper biological insights. This pipeline includes\nvariable selection/ranking using linear and nonlinear methods, feature\nextraction using functional principal component analysis and Euler\ncharacteristics, and joint integration and classification using dense\nfeed-forward networks and recurrent neural networks. We applied this pipeline\nto cross-sectional and longitudinal multi-omics data (metagenomics,\ntranscriptomics, and metabolomics) from an inflammatory bowel disease (IBD)\nstudy and we identified microbial pathways, metabolites, and genes that\ndiscriminate by IBD status, providing information on the etiology of IBD. We\nconducted simulations to compare the two feature extraction methods. The\nproposed pipeline is available from the following GitHub repository:\nhttps://github.com/lasandrall/DeepIDA-GRU.",
            "author": [
                "Sarthak Jain",
                "Sandra E. Safo"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01238v1",
                "http://arxiv.org/pdf/2312.01238v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.AP",
                "stat.CO",
                "stat.ME",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01236v1",
            "title": "Evetac: An Event-based Optical Tactile Sensor for Robotic Manipulation",
            "updated": "2023-12-02T22:01:49Z",
            "published": "2023-12-02T22:01:49Z",
            "summary": "Optical tactile sensors have recently become popular. They provide high\nspatial resolution, but struggle to offer fine temporal resolutions. To\novercome this shortcoming, we study the idea of replacing the RGB camera with\nan event-based camera and introduce a new event-based optical tactile sensor\ncalled Evetac. Along with hardware design, we develop touch processing\nalgorithms to process its measurements online at 1000 Hz. We devise an\nefficient algorithm to track the elastomer's deformation through the imprinted\nmarkers despite the sensor's sparse output. Benchmarking experiments\ndemonstrate Evetac's capabilities of sensing vibrations up to 498 Hz,\nreconstructing shear forces, and significantly reducing data rates compared to\nRGB optical tactile sensors. Moreover, Evetac's output and the marker tracking\nprovide meaningful features for learning data-driven slip detection and\nprediction models. The learned models form the basis for a robust and adaptive\nclosed-loop grasp controller capable of handling a wide range of objects. We\nbelieve that fast and efficient event-based tactile sensors like Evetac will be\nessential for bringing human-like manipulation capabilities to robotics. The\nsensor design is open-sourced at https://sites.google.com/view/evetac .",
            "author": [
                "Niklas Funk",
                "Erik Helmut",
                "Georgia Chalvatzaki",
                "Roberto Calandra",
                "Jan Peters"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01236v1",
                "http://arxiv.org/pdf/2312.01236v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01235v2",
            "title": "Strategic Data Revocation in Federated Unlearning",
            "updated": "2023-12-06T23:03:23Z",
            "published": "2023-12-02T21:57:44Z",
            "summary": "By allowing users to erase their data's impact on federated learning models,\nfederated unlearning protects users' right to be forgotten and data privacy.\nDespite a burgeoning body of research on federated unlearning's technical\nfeasibility, there is a paucity of literature investigating the considerations\nbehind users' requests for data revocation. This paper proposes a\nnon-cooperative game framework to study users' data revocation strategies in\nfederated unlearning. We prove the existence of a Nash equilibrium. However,\nusers' best response strategies are coupled via model performance and\nunlearning costs, which makes the equilibrium computation challenging. We\nobtain the Nash equilibrium by establishing its equivalence with a much simpler\nauxiliary optimization problem. We also summarize users' multi-dimensional\nattributes into a single-dimensional metric and derive the closed-form\ncharacterization of an equilibrium, when users' unlearning costs are\nnegligible. Moreover, we compare the cases of allowing and forbidding partial\ndata revocation in federated unlearning. Interestingly, the results reveal that\nallowing partial revocation does not necessarily increase users' data\ncontributions or payoffs due to the game structure. Additionally, we\ndemonstrate that positive externalities may exist between users' data\nrevocation decisions when users incur unlearning costs, while this is not the\ncase when their unlearning costs are negligible.",
            "author": [
                "Ningning Ding",
                "Ermin Wei",
                "Randall Berry"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01235v2",
                "http://arxiv.org/pdf/2312.01235v2"
            ],
            "primary_category": "cs.GT",
            "category": [
                "cs.GT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02204v1",
            "title": "Can We Learn Communication-Efficient Optimizers?",
            "updated": "2023-12-02T21:51:12Z",
            "published": "2023-12-02T21:51:12Z",
            "summary": "Communication-efficient variants of SGD, specifically local SGD, have\nreceived a great deal of interest in recent years. These approaches compute\nmultiple gradient steps locally, that is on each worker, before averaging model\nparameters, helping relieve the critical communication bottleneck in\ndistributed deep learning training. Although many variants of these approaches\nhave been proposed, they can sometimes lag behind state-of-the-art adaptive\noptimizers for deep learning. In this work, we investigate if the recent\nprogress in the emerging area of learned optimizers can potentially close this\ngap while remaining communication-efficient. Specifically, we meta-learn how to\nperform global updates given an update from local SGD iterations. Our results\ndemonstrate that learned optimizers can substantially outperform local SGD and\nits sophisticated variants while maintaining their communication efficiency.\nLearned optimizers can even generalize to unseen and much larger datasets and\narchitectures, including ImageNet and ViTs, and to unseen modalities such as\nlanguage modeling. We therefore demonstrate the potential of learned optimizers\nfor improving communication-efficient distributed learning.",
            "author": [
                "Charles-\u00c9tienne Joseph",
                "Benjamin Th\u00e9rien",
                "Abhinav Moudgil",
                "Boris Knyazev",
                "Eugene Belilovsky"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02204v1",
                "http://arxiv.org/pdf/2312.02204v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02203v1",
            "title": "Learning High-Order Relationships of Brain Regions",
            "updated": "2023-12-02T21:39:05Z",
            "published": "2023-12-02T21:39:05Z",
            "summary": "Discovering reliable and informative interactions among brain regions from\nfunctional magnetic resonance imaging (fMRI) signals is essential in\nneuroscientific predictions of cognition. Most of the current methods fail to\naccurately characterize those interactions because they only focus on pairwise\nconnections and overlook the high-order relationships of brain regions. We\ndelve into this problem and argue that these high-order relationships should be\nmaximally informative and minimally redundant (MIMR). However, identifying such\nhigh-order relationships is challenging and highly under-explored. Methods that\ncan be tailored to our context are also non-existent. In response to this gap,\nwe propose a novel method named HyBRiD that aims to extract MIMR high-order\nrelationships from fMRI data. HyBRiD employs a Constructor to identify\nhyperedge structures, and a Weighter to compute a weight for each hyperedge.\nHyBRiD achieves the MIMR objective through an innovative information bottleneck\nframework named multi-head drop-bottleneck with theoretical guarantees. Our\ncomprehensive experiments demonstrate the effectiveness of our model. Our model\noutperforms the state-of-the-art predictive model by an average of 12.1%,\nregarding the quality of hyperedges measured by CPM, a standard protocol for\nstudying brain connections.",
            "author": [
                "Weikang Qiu",
                "Huangrui Chu",
                "Selena Wang",
                "Haolan Zuo",
                "Xiaoxiao Li",
                "Yize Zhao",
                "Rex Ying"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02203v1",
                "http://arxiv.org/pdf/2312.02203v1"
            ],
            "primary_category": "q-bio.NC",
            "category": [
                "q-bio.NC",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01232v2",
            "title": "A Comprehensive Study of Vision Transformers in Image Classification\n  Tasks",
            "updated": "2023-12-05T03:46:27Z",
            "published": "2023-12-02T21:38:16Z",
            "summary": "Image Classification is a fundamental task in the field of computer vision\nthat frequently serves as a benchmark for gauging advancements in Computer\nVision. Over the past few years, significant progress has been made in image\nclassification due to the emergence of deep learning. However, challenges still\nexist, such as modeling fine-grained visual information, high computation\ncosts, the parallelism of the model, and inconsistent evaluation protocols\nacross datasets. In this paper, we conduct a comprehensive survey of existing\npapers on Vision Transformers for image classification. We first introduce the\npopular image classification datasets that influenced the design of models.\nThen, we present Vision Transformers models in chronological order, starting\nwith early attempts at adapting attention mechanism to vision tasks followed by\nthe adoption of vision transformers, as they have demonstrated success in\ncapturing intricate patterns and long-range dependencies within images.\nFinally, we discuss open problems and shed light on opportunities for image\nclassification to facilitate new research ideas.",
            "author": [
                "Mahmoud Khalil",
                "Ahmad Khalil",
                "Alioune Ngom"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01232v2",
                "http://arxiv.org/pdf/2312.01232v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01228v1",
            "title": "Mixed-Integer Optimisation of Graph Neural Networks for Computer-Aided\n  Molecular Design",
            "updated": "2023-12-02T21:10:18Z",
            "published": "2023-12-02T21:10:18Z",
            "summary": "ReLU neural networks have been modelled as constraints in mixed integer\nlinear programming (MILP), enabling surrogate-based optimisation in various\ndomains and efficient solution of machine learning certification problems.\nHowever, previous works are mostly limited to MLPs. Graph neural networks\n(GNNs) can learn from non-euclidean data structures such as molecular\nstructures efficiently and are thus highly relevant to computer-aided molecular\ndesign (CAMD). We propose a bilinear formulation for ReLU Graph Convolutional\nNeural Networks and a MILP formulation for ReLU GraphSAGE models. These\nformulations enable solving optimisation problems with trained GNNs embedded to\nglobal optimality. We apply our optimization approach to an illustrative CAMD\ncase study where the formulations of the trained GNNs are used to design\nmolecules with optimal boiling points.",
            "author": [
                "Tom McDonald",
                "Calvin Tsay",
                "Artur M. Schweidtmann",
                "Neil Yorke-Smith"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01228v1",
                "http://arxiv.org/pdf/2312.01228v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "cs.NE",
                "90C11",
                "G.1.6; I.2.6; J.2"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01227v2",
            "title": "Distributed Bayesian Estimation in Sensor Networks: Consensus on\n  Marginal Densities",
            "updated": "2023-12-07T17:55:02Z",
            "published": "2023-12-02T21:10:06Z",
            "summary": "In this paper, we aim to design and analyze distributed Bayesian estimation\nalgorithms for sensor networks. The challenges we address are to (i) derive a\ndistributed provably-correct algorithm in the functional space of probability\ndistributions over continuous variables, and (ii) leverage these results to\nobtain new distributed estimators restricted to subsets of variables observed\nby individual agents. This relates to applications such as cooperative\nlocalization and federated learning, where the data collected at any agent\ndepends on a subset of all variables of interest. We present Bayesian density\nestimation algorithms using data from non-linear likelihoods at agents in\ncentralized, distributed, and marginal distributed settings. After setting up a\ndistributed estimation objective, we prove almost-sure convergence to the\noptimal set of pdfs at each agent. Then, we prove the same for a storage-aware\nalgorithm estimating densities only over relevant variables at each agent.\nFinally, we present a Gaussian version of these algorithms and implement it in\na mapping problem using variational inference to handle non-linear likelihood\nmodels associated with LiDAR sensing.",
            "author": [
                "Parth Paritosh",
                "Nikolay Atanasov",
                "Sonia Martinez"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01227v2",
                "http://arxiv.org/pdf/2312.01227v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.MA",
                "cs.RO",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01225v1",
            "title": "UCE-FID: Using Large Unlabeled, Medium Crowdsourced-Labeled, and Small\n  Expert-Labeled Tweets for Foodborne Illness Detection",
            "updated": "2023-12-02T21:03:23Z",
            "published": "2023-12-02T21:03:23Z",
            "summary": "Foodborne illnesses significantly impact public health. Deep learning\nsurveillance applications using social media data aim to detect early warning\nsignals. However, labeling foodborne illness-related tweets for model training\nrequires extensive human resources, making it challenging to collect a\nsufficient number of high-quality labels for tweets within a limited budget.\nThe severe class imbalance resulting from the scarcity of foodborne\nillness-related tweets among the vast volume of social media further\nexacerbates the problem. Classifiers trained on a class-imbalanced dataset are\nbiased towards the majority class, making accurate detection difficult. To\novercome these challenges, we propose EGAL, a deep learning framework for\nfoodborne illness detection that uses small expert-labeled tweets augmented by\ncrowdsourced-labeled and massive unlabeled data. Specifically, by leveraging\ntweets labeled by experts as a reward set, EGAL learns to assign a weight of\nzero to incorrectly labeled tweets to mitigate their negative influence. Other\ntweets receive proportionate weights to counter-balance the unbalanced class\ndistribution. Extensive experiments on real-world \\textit{TWEET-FID} data show\nthat EGAL outperforms strong baseline models across different settings,\nincluding varying expert-labeled set sizes and class imbalance ratios. A case\nstudy on a multistate outbreak of Salmonella Typhimurium infection linked to\npackaged salad greens demonstrates how the trained model captures relevant\ntweets offering valuable outbreak insights. EGAL, funded by the U.S. Department\nof Agriculture (USDA), has the potential to be deployed for real-time analysis\nof tweet streaming, contributing to foodborne illness outbreak surveillance\nefforts.",
            "author": [
                "Ruofan Hu",
                "Dongyu Zhang",
                "Dandan Tao",
                "Huayi Zhang",
                "Hao Feng",
                "Elke Rundensteiner"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01225v1",
                "http://arxiv.org/pdf/2312.01225v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01220v1",
            "title": "Boosting Object Detection with Zero-Shot Day-Night Domain Adaptation",
            "updated": "2023-12-02T20:11:48Z",
            "published": "2023-12-02T20:11:48Z",
            "summary": "Detecting objects in low-light scenarios presents a persistent challenge, as\ndetectors trained on well-lit data exhibit significant performance degradation\non low-light data due to the low visibility. Previous methods mitigate this\nissue by investigating image enhancement or object detection techniques using\nlow-light image datasets. However, the progress is impeded by the inherent\ndifficulties associated with collecting and annotating low-light images. To\naddress this challenge, we propose to boost low-light object detection with\nzero-shot day-night domain adaptation, which aims to generalize a detector from\nwell-lit scenarios to low-light ones without requiring real low-light data. We\nfirst design a reflectance representation learning module to learn\nRetinex-based illumination invariance in images with a carefully designed\nillumination invariance reinforcement strategy. Next, an\ninterchange-redecomposition-coherence procedure is introduced to improve over\nthe vanilla Retinex image decomposition process by performing two sequential\nimage decompositions and introducing a redecomposition cohering loss. Extensive\nexperiments on ExDark, DARK FACE and CODaN datasets show strong low-light\ngeneralizability of our method.",
            "author": [
                "Zhipeng Du",
                "Miaojing Shi",
                "Jiankang Deng"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01220v1",
                "http://arxiv.org/pdf/2312.01220v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01217v1",
            "title": "Understanding Opinions Towards Climate Change on Social Media",
            "updated": "2023-12-02T20:02:34Z",
            "published": "2023-12-02T20:02:34Z",
            "summary": "Social media platforms such as Twitter (now known as X) have revolutionized\nhow the public engage with important societal and political topics. Recently,\nclimate change discussions on social media became a catalyst for political\npolarization and the spreading of misinformation. In this work, we aim to\nunderstand how real world events influence the opinions of individuals towards\nclimate change related topics on social media. To this end, we extracted and\nanalyzed a dataset of 13.6 millions tweets sent by 3.6 million users from 2006\nto 2019. Then, we construct a temporal graph from the user-user mentions\nnetwork and utilize the Louvain community detection algorithm to analyze the\nchanges in community structure around Conference of the Parties on Climate\nChange~(COP) events. Next, we also apply tools from the Natural Language\nProcessing literature to perform sentiment analysis and topic modeling on the\ntweets. Our work acts as a first step towards understanding the evolution of\npro-climate change communities around COP events. Answering these questions\nhelps us understand how to raise people's awareness towards climate change thus\nhopefully calling on more individuals to join the collaborative effort in\nslowing down climate change.",
            "author": [
                "Yashaswi Pupneja",
                "Joseph Zou",
                "Sacha L\u00e9vy",
                "Shenyang Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01217v1",
                "http://arxiv.org/pdf/2312.01217v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01213v1",
            "title": "Recent Advances in Scalable Energy-Efficient and Trustworthy Spiking\n  Neural networks: from Algorithms to Technology",
            "updated": "2023-12-02T19:47:00Z",
            "published": "2023-12-02T19:47:00Z",
            "summary": "Neuromorphic computing and, in particular, spiking neural networks (SNNs)\nhave become an attractive alternative to deep neural networks for a broad range\nof signal processing applications, processing static and/or temporal inputs\nfrom different sensory modalities, including audio and vision sensors. In this\npaper, we start with a description of recent advances in algorithmic and\noptimization innovations to efficiently train and scale low-latency, and\nenergy-efficient spiking neural networks (SNNs) for complex machine learning\napplications. We then discuss the recent efforts in algorithm-architecture\nco-design that explores the inherent trade-offs between achieving high\nenergy-efficiency and low latency while still providing high accuracy and\ntrustworthiness. We then describe the underlying hardware that has been\ndeveloped to leverage such algorithmic innovations in an efficient way. In\nparticular, we describe a hybrid method to integrate significant portions of\nthe model's computation within both memory components as well as the sensor\nitself. Finally, we discuss the potential path forward for research in building\ndeployable SNN systems identifying key challenges in the\nalgorithm-hardware-application co-design space with an emphasis on\ntrustworthiness.",
            "author": [
                "Souvik Kundu",
                "Rui-Jie Zhu",
                "Akhilesh Jaiswal",
                "Peter A. Beerel"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01213v1",
                "http://arxiv.org/pdf/2312.01213v1"
            ],
            "primary_category": "cs.AR",
            "category": [
                "cs.AR",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01212v1",
            "title": "A Comparative Analysis Towards Melanoma Classification Using Transfer\n  Learning by Analyzing Dermoscopic Images",
            "updated": "2023-12-02T19:46:48Z",
            "published": "2023-12-02T19:46:48Z",
            "summary": "Melanoma is a sort of skin cancer that starts in the cells known as\nmelanocytes. It is more dangerous than other types of skin cancer because it\ncan spread to other organs. Melanoma can be fatal if it spreads to other parts\nof the body. Early detection is the key to cure, but it requires the skills of\nskilled doctors to diagnose it. This paper presents a system that combines deep\nlearning techniques with established transfer learning methods to enable skin\nlesions classification and diagnosis of melanoma skin lesions. Using\nConvolutional Neural Networks, it presents a method for categorizing melanoma\nimages into benign and malignant images in this research (CNNs). Researchers\nused 'Deep Learning' techniques to train an expansive number of photos &\nessentially to get the expected result deep neural networks to need to be\ntrained with a huge number of parameters as dermoscopic images are sensitive &\nvery hard to classify. This paper, has been emphasized building models with\nless complexity and comparatively better accuracy with limited datasets &\npartially fewer deep networks so that the system can predict Melanoma at ease\nfrom input dermoscopic images as correctly as possible within devices with less\ncomputational power. The dataset has been obtained from ISIC Archive. Multiple\npre-trained models ResNet101, DenseNet, EfficientNet, InceptionV3 have been\nimplemented using transfer learning techniques to complete the comparative\nanalysis & every model achieved good accuracy. Before training the models, the\ndata has been augmented by multiple parameters to improve the accuracy.\nMoreover, the results are better than the previous state-of-the-art approaches\n& adequate to predict melanoma. Among these architectures, DenseNet performed\nbetter than the others which gives a validation accuracy of 96.64%, validation\nloss of 9.43% & test set accuracy of 99.63%.",
            "author": [
                "Md. Fahim Uddin",
                "Nafisa Tafshir",
                "Mohammad Monirujjaman Khan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01212v1",
                "http://arxiv.org/pdf/2312.01212v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01210v2",
            "title": "When accurate prediction models yield harmful self-fulfilling prophecies",
            "updated": "2023-12-06T10:08:47Z",
            "published": "2023-12-02T19:39:50Z",
            "summary": "Prediction models are popular in medical research and practice. By predicting\nan outcome of interest for specific patients, these models may help inform\ndifficult treatment decisions, and are often hailed as the poster children for\npersonalized, data-driven healthcare.\n  We show however, that using prediction models for decision making can lead to\nharmful decisions, even when the predictions exhibit good discrimination after\ndeployment. These models are harmful self-fulfilling prophecies: their\ndeployment harms a group of patients but the worse outcome of these patients\ndoes not invalidate the predictive power of the model. Our main result is a\nformal characterization of a set of such prediction models. Next we show that\nmodels that are well calibrated before and after deployment are useless for\ndecision making as they made no change in the data distribution. These results\npoint to the need to revise standard practices for validation, deployment and\nevaluation of prediction models that are used in medical decisions.",
            "author": [
                "Wouter A. C. van Amsterdam",
                "Nan van Geloven",
                "Jesse H. Krijthe",
                "Rajesh Ranganath",
                "Giovanni Cin\u00e1"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01210v2",
                "http://arxiv.org/pdf/2312.01210v2"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME",
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02199v1",
            "title": "USat: A Unified Self-Supervised Encoder for Multi-Sensor Satellite\n  Imagery",
            "updated": "2023-12-02T19:17:04Z",
            "published": "2023-12-02T19:17:04Z",
            "summary": "Large, self-supervised vision models have led to substantial advancements for\nautomatically interpreting natural images. Recent works have begun tailoring\nthese methods to remote sensing data which has rich structure with\nmulti-sensor, multi-spectral, and temporal information providing massive\namounts of self-labeled data that can be used for self-supervised pre-training.\nIn this work, we develop a new encoder architecture called USat that can input\nmulti-spectral data from multiple sensors for self-supervised pre-training.\nUSat is a vision transformer with modified patch projection layers and\npositional encodings to model spectral bands with varying spatial scales from\nmultiple sensors. We integrate USat into a Masked Autoencoder (MAE)\nself-supervised pre-training procedure and find that a pre-trained USat\noutperforms state-of-the-art self-supervised MAE models trained on remote\nsensing data on multiple remote sensing benchmark datasets (up to 8%) and leads\nto improvements in low data regimes (up to 7%). Code and pre-trained weights\nare available at https://github.com/stanfordmlgroup/USat .",
            "author": [
                "Jeremy Irvin",
                "Lucas Tao",
                "Joanne Zhou",
                "Yuntao Ma",
                "Langston Nashold",
                "Benjamin Liu",
                "Andrew Y. Ng"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02199v1",
                "http://arxiv.org/pdf/2312.02199v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG",
                "eess.IV",
                "stat.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01203v2",
            "title": "Harnessing Discrete Representations For Continual Reinforcement Learning",
            "updated": "2023-12-05T18:45:24Z",
            "published": "2023-12-02T18:55:26Z",
            "summary": "Reinforcement learning (RL) agents make decisions using nothing but\nobservations from the environment, and consequently, heavily rely on the\nrepresentations of those observations. Though some recent breakthroughs have\nused vector-based categorical representations of observations, often referred\nto as discrete representations, there is little work explicitly assessing the\nsignificance of such a choice. In this work, we provide a thorough empirical\ninvestigation of the advantages of representing observations as vectors of\ncategorical values within the context of reinforcement learning. We perform\nevaluations on world-model learning, model-free RL, and ultimately continual RL\nproblems, where the benefits best align with the needs of the problem setting.\nWe find that, when compared to traditional continuous representations, world\nmodels learned over discrete representations accurately model more of the world\nwith less capacity, and that agents trained with discrete representations learn\nbetter policies with less data. In the context of continual RL, these benefits\ntranslate into faster adapting agents. Additionally, our analysis suggests that\nthe observed performance improvements can be attributed to the information\ncontained within the latent vectors and potentially the encoding of the\ndiscrete representation itself.",
            "author": [
                "Edan Meyer",
                "Adam White",
                "Marlos C. Machado"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01203v2",
                "http://arxiv.org/pdf/2312.01203v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01201v1",
            "title": "PAC Privacy Preserving Diffusion Models",
            "updated": "2023-12-02T18:42:52Z",
            "published": "2023-12-02T18:42:52Z",
            "summary": "Data privacy protection is garnering increased attention among researchers.\nDiffusion models (DMs), particularly with strict differential privacy, can\npotentially produce images with both high privacy and visual quality. However,\nchallenges arise in ensuring robust protection in privatizing specific data\nattributes, areas where current models often fall short. To address these\nchallenges, we introduce the PAC Privacy Preserving Diffusion Model, a model\nleverages diffusion principles and ensure Probably Approximately Correct (PAC)\nprivacy. We enhance privacy protection by integrating a private classifier\nguidance into the Langevin Sampling Process. Additionally, recognizing the gap\nin measuring the privacy of models, we have developed a novel metric to gauge\nprivacy levels. Our model, assessed with this new metric and supported by\nGaussian matrix computations for the PAC bound, has shown superior performance\nin privacy protection over existing leading private generative models according\nto benchmark tests.",
            "author": [
                "Qipan Xu",
                "Youlong Ding",
                "Jie Gao",
                "Hao Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01201v1",
                "http://arxiv.org/pdf/2312.01201v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01200v1",
            "title": "FRAUDability: Estimating Users' Susceptibility to Financial Fraud Using\n  Adversarial Machine Learning",
            "updated": "2023-12-02T18:33:05Z",
            "published": "2023-12-02T18:33:05Z",
            "summary": "In recent years, financial fraud detection systems have become very efficient\nat detecting fraud, which is a major threat faced by e-commerce platforms. Such\nsystems often include machine learning-based algorithms aimed at detecting and\nreporting fraudulent activity. In this paper, we examine the application of\nadversarial learning based ranking techniques in the fraud detection domain and\npropose FRAUDability, a method for the estimation of a financial fraud\ndetection system's performance for every user. We are motivated by the\nassumption that \"not all users are created equal\" -- while some users are well\nprotected by fraud detection algorithms, others tend to pose a challenge to\nsuch systems. The proposed method produces scores, namely \"fraudability\nscores,\" which are numerical estimations of a fraud detection system's ability\nto detect financial fraud for a specific user, given his/her unique activity in\nthe financial system. Our fraudability scores enable those tasked with\ndefending users in a financial platform to focus their attention and resources\non users with high fraudability scores to better protect them. We validate our\nmethod using a real e-commerce platform's dataset and demonstrate the\napplication of fraudability scores from the attacker's perspective, on the\nplatform, and more specifically, on the fraud detection systems used by the\ne-commerce enterprise. We show that the scores can also help attackers increase\ntheir financial profit by 54%, by engaging solely with users with high\nfraudability scores, avoiding those users whose spending habits enable more\naccurate fraud detection.",
            "author": [
                "Chen Doytshman",
                "Satoru Momiyama",
                "Inderjeet Singh",
                "Yuval Elovici",
                "Asaf Shabtai"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01200v1",
                "http://arxiv.org/pdf/2312.01200v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01197v1",
            "title": "Short-term Precipitation Forecasting in The Netherlands: An Application\n  of Convolutional LSTM neural networks to weather radar data",
            "updated": "2023-12-02T18:13:45Z",
            "published": "2023-12-02T18:13:45Z",
            "summary": "This work addresses the challenge of short-term precipitation forecasting by\napplying Convolutional Long Short-Term Memory (ConvLSTM) neural networks to\nweather radar data from the Royal Netherlands Meteorological Institute (KNMI).\nThe research exploits the combination of Convolutional Neural Networks (CNNs)\nlayers for spatial pattern recognition and LSTM network layers for modelling\ntemporal sequences, integrating these strengths into a ConvLSTM architecture.\nThe model was trained and validated on weather radar data from the Netherlands.\nThe model is an autoencoder consisting of nine layers, uniquely combining\nconvolutional operations with LSTMs temporal processing, enabling it to capture\nthe movement and intensity of precipitation systems. The training set comprised\nof sequences of radar images, with the model being tasked to predict\nprecipitation patterns 1.5 hours ahead using the preceding data. Results\nindicate high accuracy in predicting the direction and intensity of\nprecipitation movements. The findings of this study underscore the significant\npotential of ConvLSTM networks in meteorological forecasting, particularly in\nregions with complex weather patterns. It contributes to the field by offering\na more accurate, data-driven approach to weather prediction, highlighting the\nbroader applicability of ConvLSTM networks in meteorological tasks.",
            "author": [
                "Petros Demetrakopoulos"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01197v1",
                "http://arxiv.org/pdf/2312.01197v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01196v1",
            "title": "Neural Parametric Gaussians for Monocular Non-Rigid Object\n  Reconstruction",
            "updated": "2023-12-02T18:06:24Z",
            "published": "2023-12-02T18:06:24Z",
            "summary": "Reconstructing dynamic objects from monocular videos is a severely\nunderconstrained and challenging problem, and recent work has approached it in\nvarious directions. However, owing to the ill-posed nature of this problem,\nthere has been no solution that can provide consistent, high-quality novel\nviews from camera positions that are significantly different from the training\nviews. In this work, we introduce Neural Parametric Gaussians (NPGs) to take on\nthis challenge by imposing a two-stage approach: first, we fit a low-rank\nneural deformation model, which then is used as regularization for non-rigid\nreconstruction in the second stage. The first stage learns the object's\ndeformations such that it preserves consistency in novel views. The second\nstage obtains high reconstruction quality by optimizing 3D Gaussians that are\ndriven by the coarse model. To this end, we introduce a local 3D Gaussian\nrepresentation, where temporally shared Gaussians are anchored in and deformed\nby local oriented volumes. The resulting combined model can be rendered as\nradiance fields, resulting in high-quality photo-realistic reconstructions of\nthe non-rigidly deforming objects, maintaining 3D consistency across novel\nviews. We demonstrate that NPGs achieve superior results compared to previous\nworks, especially in challenging scenarios with few multi-view cues.",
            "author": [
                "Devikalyan Das",
                "Christopher Wewer",
                "Raza Yunus",
                "Eddy Ilg",
                "Jan Eric Lenssen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01196v1",
                "http://arxiv.org/pdf/2312.01196v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01191v1",
            "title": "Bootstrapping Interactive Image-Text Alignment for Remote Sensing Image\n  Captioning",
            "updated": "2023-12-02T17:32:17Z",
            "published": "2023-12-02T17:32:17Z",
            "summary": "Recently, remote sensing image captioning has gained significant attention in\nthe remote sensing community. Due to the significant differences in spatial\nresolution of remote sensing images, existing methods in this field have\npredominantly concentrated on the fine-grained extraction of remote sensing\nimage features, but they cannot effectively handle the semantic consistency\nbetween visual features and textual features. To efficiently align the\nimage-text, we propose a novel two-stage vision-language pre-training-based\napproach to bootstrap interactive image-text alignment for remote sensing image\ncaptioning, called BITA, which relies on the design of a lightweight\ninteractive Fourier Transformer to better align remote sensing image-text\nfeatures. The Fourier layer in the interactive Fourier Transformer is capable\nof extracting multi-scale features of remote sensing images in the frequency\ndomain, thereby reducing the redundancy of remote sensing visual features.\nSpecifically, the first stage involves preliminary alignment through image-text\ncontrastive learning, which aligns the learned multi-scale remote sensing\nfeatures from the interactive Fourier Transformer with textual features. In the\nsecond stage, the interactive Fourier Transformer connects the frozen image\nencoder with a large language model. Then, prefix causal language modeling is\nutilized to guide the text generation process using visual features.\nUltimately, across the UCM-caption, RSICD, and NWPU-caption datasets, the\nexperimental results clearly demonstrate that BITA outperforms other advanced\ncomparative approaches. The code is available at\nhttps://github.com/yangcong356/BITA.",
            "author": [
                "Cong Yang",
                "Zuchao Li",
                "Lefei Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01191v1",
                "http://arxiv.org/pdf/2312.01191v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01188v1",
            "title": "Efficient Expansion and Gradient Based Task Inference for Replay Free\n  Incremental Learning",
            "updated": "2023-12-02T17:28:52Z",
            "published": "2023-12-02T17:28:52Z",
            "summary": "This paper proposes a simple but highly efficient expansion-based model for\ncontinual learning. The recent feature transformation, masking and\nfactorization-based methods are efficient, but they grow the model only over\nthe global or shared parameter. Therefore, these approaches do not fully\nutilize the previously learned information because the same task-specific\nparameter forgets the earlier knowledge. Thus, these approaches show limited\ntransfer learning ability. Moreover, most of these models have constant\nparameter growth for all tasks, irrespective of the task complexity. Our work\nproposes a simple filter and channel expansion based method that grows the\nmodel over the previous task parameters and not just over the global parameter.\nTherefore, it fully utilizes all the previously learned information without\nforgetting, which results in better knowledge transfer. The growth rate in our\nproposed model is a function of task complexity; therefore for a simple task,\nthe model has a smaller parameter growth while for complex tasks, the model\nrequires more parameters to adapt to the current task. Recent expansion based\nmodels show promising results for task incremental learning (TIL). However, for\nclass incremental learning (CIL), prediction of task id is a crucial challenge;\nhence, their results degrade rapidly as the number of tasks increase. In this\nwork, we propose a robust task prediction method that leverages entropy\nweighted data augmentations and the models gradient using pseudo labels. We\nevaluate our model on various datasets and architectures in the TIL, CIL and\ngenerative continual learning settings. The proposed approach shows\nstate-of-the-art results in all these settings. Our extensive ablation studies\nshow the efficacy of the proposed components.",
            "author": [
                "Soumya Roy",
                "Vinay K Verma",
                "Deepak Gupta"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01188v1",
                "http://arxiv.org/pdf/2312.01188v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01187v1",
            "title": "SASSL: Enhancing Self-Supervised Learning via Neural Style Transfer",
            "updated": "2023-12-02T17:25:30Z",
            "published": "2023-12-02T17:25:30Z",
            "summary": "Self-supervised learning relies heavily on data augmentation to extract\nmeaningful representations from unlabeled images. While existing\nstate-of-the-art augmentation pipelines incorporate a wide range of primitive\ntransformations, these often disregard natural image structure. Thus, augmented\nsamples can exhibit degraded semantic information and low stylistic diversity,\naffecting downstream performance of self-supervised representations. To\novercome this, we propose SASSL: Style Augmentations for Self Supervised\nLearning, a novel augmentation technique based on Neural Style Transfer. The\nmethod decouples semantic and stylistic attributes in images and applies\ntransformations exclusively to the style while preserving content, generating\ndiverse augmented samples that better retain their semantic properties.\nExperimental results show our technique achieves a top-1 classification\nperformance improvement of more than 2% on ImageNet compared to the\nwell-established MoCo v2. We also measure transfer learning performance across\nfive diverse datasets, observing significant improvements of up to 3.75%. Our\nexperiments indicate that decoupling style from content information and\ntransferring style across datasets to diversify augmentations can significantly\nimprove downstream performance of self-supervised representations.",
            "author": [
                "Renan A. Rojas-Gomez",
                "Karan Singhal",
                "Ali Etemad",
                "Alex Bijamov",
                "Warren R. Morningstar",
                "Philip Andrew Mansfield"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01187v1",
                "http://arxiv.org/pdf/2312.01187v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01186v1",
            "title": "Linker-Tuning: Optimizing Continuous Prompts for Heterodimeric Protein\n  Prediction",
            "updated": "2023-12-02T17:24:45Z",
            "published": "2023-12-02T17:24:45Z",
            "summary": "Predicting the structure of interacting chains is crucial for understanding\nbiological systems and developing new drugs. Large-scale pre-trained Protein\nLanguage Models (PLMs), such as ESM2, have shown impressive abilities in\nextracting biologically meaningful representations for protein structure\nprediction. In this paper, we show that ESMFold, which has been successful in\ncomputing accurate atomic structures for single-chain proteins, can be adapted\nto predict the heterodimer structures in a lightweight manner. We propose\nLinker-tuning, which learns a continuous prompt to connect the two chains in a\ndimer before running it as a single sequence in ESMFold. Experiment results\nshow that our method successfully predicts 56.98% of interfaces on the i.i.d.\nheterodimer test set, with an absolute improvement of +12.79% over the\nESMFold-Linker baseline. Furthermore, our model can generalize well to the\nout-of-distribution (OOD) test set HeteroTest2 and two antibody test sets Fab\nand Fv while being $9\\times$ faster than AF-Multimer.",
            "author": [
                "Shuxian Zou",
                "Hui Li",
                "Shentong Mo",
                "Xingyi Cheng",
                "Eric Xing",
                "Le Song"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01186v1",
                "http://arxiv.org/pdf/2312.01186v1"
            ],
            "primary_category": "q-bio.BM",
            "category": [
                "q-bio.BM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01185v1",
            "title": "A ripple in time: a discontinuity in American history",
            "updated": "2023-12-02T17:24:17Z",
            "published": "2023-12-02T17:24:17Z",
            "summary": "In this note we use the State of the Union Address dataset from Kaggle to\nmake some surprising (and some not so surprising) observations pertaining to\nthe general timeline of American history, and the character and nature of the\naddresses themselves. Our main approach is using vector embeddings, such as\nBERT (DistilBERT) and GPT-2. While it is widely believed that BERT (and its\nvariations) is most suitable for NLP classification tasks, we find out that\nGPT-2 in conjunction with nonlinear dimension reduction methods such as UMAP\nprovide better separation and stronger clustering. This makes GPT-2 + UMAP an\ninteresting alternative. In our case, no model fine-tuning is required, and the\npre-trained out-of-the-box GPT-2 model is enough. We also used a fine-tuned\nDistilBERT model for classification (detecting which president delivered which\naddress), with very good results (accuracy 93% - 95% depending on the run). All\ncomputations can be replicated by using the accompanying code on GitHub.",
            "author": [
                "Alexander Kolpakov",
                "Igor Rivin"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01185v1",
                "http://arxiv.org/pdf/2312.01185v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG",
                "cs.SI",
                "I.2.7; I.5.4; H.3.1; H.3.3"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01183v1",
            "title": "Comprehensive Robotic Cholecystectomy Dataset (CRCD): Integrating\n  Kinematics, Pedal Signals, and Endoscopic Videos",
            "updated": "2023-12-02T17:16:56Z",
            "published": "2023-12-02T17:16:56Z",
            "summary": "In recent years, the potential applications of machine learning to Minimally\nInvasive Surgery (MIS) have spurred interest in data sets that can be used to\ndevelop data-driven tools. This paper introduces a novel dataset recorded\nduring ex vivo pseudo-cholecystectomy procedures on pig livers, utilizing the\nda Vinci Research Kit (dVRK). Unlike current datasets, ours bridges a critical\ngap by offering not only full kinematic data but also capturing all pedal\ninputs used during the procedure and providing a time-stamped record of the\nendoscope's movements. Contributed by seven surgeons, this data set introduces\na new dimension to surgical robotics research, allowing the creation of\nadvanced models for automating console functionalities. Our work addresses the\nexisting limitation of incomplete recordings and imprecise kinematic data,\ncommon in other datasets. By introducing two models, dedicated to predicting\nclutch usage and camera activation, we highlight the dataset's potential for\nadvancing automation in surgical robotics. The comparison of methodologies and\ntime windows provides insights into the models' boundaries and limitations.",
            "author": [
                "Ki-Hwan Oh",
                "Leonardo Borgioli",
                "Alberto Mangano",
                "Valentina Valle",
                "Marco Di Pangrazio",
                "Francesco Toti",
                "Gioia Pozza",
                "Luciano Ambrosini",
                "Alvaro Ducas",
                "Milos Zefran",
                "Liaohai Chen",
                "Pier Cristoforo Giulianotti"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01183v1",
                "http://arxiv.org/pdf/2312.01183v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01178v1",
            "title": "Exploring a Hybrid Deep Learning Framework to Automatically Discover\n  Topic and Sentiment in COVID-19 Tweets",
            "updated": "2023-12-02T16:58:17Z",
            "published": "2023-12-02T16:58:17Z",
            "summary": "COVID-19 has created a major public health problem worldwide and other\nproblems such as economic crisis, unemployment, mental distress, etc. The\npandemic is deadly in the world and involves many people not only with\ninfection but also with problems, stress, wonder, fear, resentment, and hatred.\nTwitter is a highly influential social media platform and a significant source\nof health-related information, news, opinion and public sentiment where\ninformation is shared by both citizens and government sources. Therefore an\neffective analysis of COVID-19 tweets is essential for policymakers to make\nwise decisions. However, it is challenging to identify interesting and useful\ncontent from major streams of text to understand people's feelings about the\nimportant topics of the COVID-19 tweets. In this paper, we propose a new\n\\textit{framework} for analyzing topic-based sentiments by extracting key\ntopics with significant labels and classifying positive, negative, or neutral\ntweets on each topic to quickly find common topics of public opinion and\nCOVID-19-related attitudes. While building our model, we take into account\nhybridization of BiLSTM and GRU structures for sentiment analysis to achieve\nour goal. The experimental results show that our topic identification method\nextracts better topic labels and the sentiment analysis approach using our\nproposed hybrid deep learning model achieves the highest accuracy compared to\ntraditional models.",
            "author": [
                "Khandaker Tayef Shahriar",
                "Iqbal H. Sarker"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01178v1",
                "http://arxiv.org/pdf/2312.01178v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01177v1",
            "title": "IDPL-PFOD2: A New Large-Scale Dataset for Printed Farsi Optical\n  Character Recognition",
            "updated": "2023-12-02T16:56:57Z",
            "published": "2023-12-02T16:56:57Z",
            "summary": "Optical Character Recognition is a technique that converts document images\ninto searchable and editable text, making it a valuable tool for processing\nscanned documents. While the Farsi language stands as a prominent and official\nlanguage in Asia, efforts to develop efficient methods for recognizing Farsi\nprinted text have been relatively limited. This is primarily attributed to the\nlanguages distinctive features, such as cursive form, the resemblance between\ncertain alphabet characters, and the presence of numerous diacritics and dot\nplacement. On the other hand, given the substantial training sample\nrequirements of deep-based architectures for effective performance, the\ndevelopment of such datasets holds paramount significance. In light of these\nconcerns, this paper aims to present a novel large-scale dataset, IDPL-PFOD2,\ntailored for Farsi printed text recognition. The dataset comprises 2003541\nimages featuring a wide variety of fonts, styles, and sizes. This dataset is an\nextension of the previously introduced IDPL-PFOD dataset, offering a\nsubstantial increase in both volume and diversity. Furthermore, the datasets\neffectiveness is assessed through the utilization of both CRNN-based and Vision\nTransformer architectures. The CRNN-based model achieves a baseline accuracy\nrate of 78.49% and a normalized edit distance of 97.72%, while the Vision\nTransformer architecture attains an accuracy of 81.32% and a normalized edit\ndistance of 98.74%.",
            "author": [
                "Fatemeh Asadi-zeydabadi",
                "Ali Afkari-Fahandari",
                "Amin Faraji",
                "Elham Shabaninia",
                "Hossein Nezamabadi-pour"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01177v1",
                "http://arxiv.org/pdf/2312.01177v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.DB"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01172v1",
            "title": "On-sensor Printed Machine Learning Classification via Bespoke ADC and\n  Decision Tree Co-Design",
            "updated": "2023-12-02T16:28:09Z",
            "published": "2023-12-02T16:28:09Z",
            "summary": "Printed electronics (PE) technology provides cost-effective hardware with\nunmet customization, due to their low non-recurring engineering and fabrication\ncosts. PE exhibit features such as flexibility, stretchability, porosity, and\nconformality, which make them a prominent candidate for enabling ubiquitous\ncomputing. Still, the large feature sizes in PE limit the realization of\ncomplex printed circuits, such as machine learning classifiers, especially when\nprocessing sensor inputs is necessary, mainly due to the costly\nanalog-to-digital converters (ADCs). To this end, we propose the design of\nfully customized ADCs and present, for the first time, a co-design framework\nfor generating bespoke Decision Tree classifiers. Our comprehensive evaluation\nshows that our co-design enables self-powered operation of on-sensor printed\nclassifiers in all benchmark cases.",
            "author": [
                "Giorgos Armeniakos",
                "Paula L. Duarte",
                "Priyanjana Pal",
                "Georgios Zervakis",
                "Mehdi B. Tahoori",
                "Dimitrios Soudris"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01172v1",
                "http://arxiv.org/pdf/2312.01172v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01169v1",
            "title": "Virtual Category Learning: A Semi-Supervised Learning Method for Dense\n  Prediction with Extremely Limited Labels",
            "updated": "2023-12-02T16:23:52Z",
            "published": "2023-12-02T16:23:52Z",
            "summary": "Due to the costliness of labelled data in real-world applications,\nsemi-supervised learning, underpinned by pseudo labelling, is an appealing\nsolution. However, handling confusing samples is nontrivial: discarding\nvaluable confusing samples would compromise the model generalisation while\nusing them for training would exacerbate the issue of confirmation bias caused\nby the resulting inevitable mislabelling. To solve this problem, this paper\nproposes to use confusing samples proactively without label correction.\nSpecifically, a Virtual Category (VC) is assigned to each confusing sample in\nsuch a way that it can safely contribute to the model optimisation even without\na concrete label. This provides an upper bound for inter-class information\nsharing capacity, which eventually leads to a better embedding space. Extensive\nexperiments on two mainstream dense prediction tasks -- semantic segmentation\nand object detection, demonstrate that the proposed VC learning significantly\nsurpasses the state-of-the-art, especially when only very few labels are\navailable. Our intriguing findings highlight the usage of VC learning in dense\nvision tasks.",
            "author": [
                "Changrui Chen",
                "Jungong Han",
                "Kurt Debattista"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01169v1",
                "http://arxiv.org/pdf/2312.01169v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01167v1",
            "title": "Meta-Learned Attribute Self-Interaction Network for Continual and\n  Generalized Zero-Shot Learning",
            "updated": "2023-12-02T16:23:01Z",
            "published": "2023-12-02T16:23:01Z",
            "summary": "Zero-shot learning (ZSL) is a promising approach to generalizing a model to\ncategories unseen during training by leveraging class attributes, but\nchallenges remain. Recently, methods using generative models to combat bias\ntowards classes seen during training have pushed state of the art, but these\ngenerative models can be slow or computationally expensive to train. Also,\nthese generative models assume that the attribute vector of each unseen class\nis available a priori at training, which is not always practical. Additionally,\nwhile many previous ZSL methods assume a one-time adaptation to unseen classes,\nin reality, the world is always changing, necessitating a constant adjustment\nof deployed models. Models unprepared to handle a sequential stream of data are\nlikely to experience catastrophic forgetting. We propose a Meta-learned\nAttribute self-Interaction Network (MAIN) for continual ZSL. By pairing\nattribute self-interaction trained using meta-learning with inverse\nregularization of the attribute encoder, we are able to outperform\nstate-of-the-art results without leveraging the unseen class attributes while\nalso being able to train our models substantially faster (>100x) than expensive\ngenerative-based approaches. We demonstrate this with experiments on five\nstandard ZSL datasets (CUB, aPY, AWA1, AWA2, and SUN) in the generalized\nzero-shot learning and continual (fixed/dynamic) zero-shot learning settings.\nExtensive ablations and analyses demonstrate the efficacy of various components\nproposed.",
            "author": [
                "Vinay K Verma",
                "Nikhil Mehta",
                "Kevin J Liang",
                "Aakansha Mishra",
                "Lawrence Carin"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01167v1",
                "http://arxiv.org/pdf/2312.01167v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01165v1",
            "title": "Data-driven optimal control with neural network modeling of gradient\n  flows",
            "updated": "2023-12-02T16:01:00Z",
            "published": "2023-12-02T16:01:00Z",
            "summary": "Extracting physical laws from observation data is a central challenge in many\ndiverse areas of science and engineering. We propose Optimal Control Neural\nNetworks (OCN) to learn the laws of vector fields in dynamical systems, with no\nassumption on their analytical form, given data consisting of sampled\ntrajectories. The OCN framework consists of a neural network representation and\nan optimal control formulation. We provide error bounds for both the solution\nand the vector field. The bounds are shown to depend on both the training error\nand the time step between the observation data. We also demonstrate the\neffectiveness of OCN, as well as its generalization ability, by testing on\nseveral canonical systems, including the chaotic Lorenz system.",
            "author": [
                "Xuping Tian",
                "Baskar Ganapathysubramanian",
                "Hailiang Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01165v1",
                "http://arxiv.org/pdf/2312.01165v1"
            ],
            "primary_category": "math.DS",
            "category": [
                "math.DS",
                "math.OC",
                "93C15, 49K15"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01163v1",
            "title": "A New Learning Paradigm for Foundation Model-based Remote Sensing Change\n  Detection",
            "updated": "2023-12-02T15:57:17Z",
            "published": "2023-12-02T15:57:17Z",
            "summary": "Change detection (CD) is a critical task to observe and analyze dynamic\nprocesses of land cover. Although numerous deep learning-based CD models have\nperformed excellently, their further performance improvements are constrained\nby the limited knowledge extracted from the given labelled data. On the other\nhand, the foundation models that emerged recently contain a huge amount of\nknowledge by scaling up across data modalities and proxy tasks. In this paper,\nwe propose a Bi-Temporal Adapter Network (BAN), which is a universal foundation\nmodel-based CD adaptation framework aiming to extract the knowledge of\nfoundation models for CD. The proposed BAN contains three parts, i.e. frozen\nfoundation model (e.g., CLIP), bitemporal adapter branch (Bi-TAB), and bridging\nmodules between them. Specifically, the Bi-TAB can be either an existing\narbitrary CD model or some hand-crafted stacked blocks. The bridging modules\nare designed to align the general features with the task/domain-specific\nfeatures and inject the selected general knowledge into the Bi-TAB. To our\nknowledge, this is the first universal framework to adapt the foundation model\nto the CD task. Extensive experiments show the effectiveness of our BAN in\nimproving the performance of existing CD methods (e.g., up to 4.08\\% IoU\nimprovement) with only a few additional learnable parameters. More importantly,\nthese successful practices show us the potential of foundation models for\nremote sensing CD. The code is available at \\url{https://github.com/likyoo/BAN}\nand will be supported in our Open-CD \\url{https://github.com/likyoo/open-cd}.",
            "author": [
                "Kaiyu Li",
                "Xiangyong Cao",
                "Deyu Meng"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01163v1",
                "http://arxiv.org/pdf/2312.01163v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01156v1",
            "title": "Efficient Light Source Placement using Quantum Computing",
            "updated": "2023-12-02T15:28:59Z",
            "published": "2023-12-02T15:28:59Z",
            "summary": "NP-hard problems regularly come up in video games, with interesting\nconnections to real-world problems. In the game Minecraft, players place\ntorches on the ground to light up dark areas. Placing them in a way that\nminimizes the total number of torches to save resources is far from trivial. In\nthis paper, we use Quantum Computing to approach this problem. To this end, we\nderive a QUBO formulation of the torch placement problem, which we uncover to\nbe very similar to another NP-hard problem. We employ a solution strategy that\ninvolves learning Lagrangian weights in an iterative process, adding to the\never growing toolbox of QUBO formulations. Finally, we perform experiments on\nreal quantum hardware using real game data to demonstrate that our approach\nyields good torch placements.",
            "author": [
                "Sascha M\u00fccke",
                "Thore Gerlach"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01156v1",
                "http://arxiv.org/pdf/2312.01156v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01150v2",
            "title": "Pointer Networks Trained Better via Evolutionary Algorithms",
            "updated": "2023-12-06T13:22:23Z",
            "published": "2023-12-02T14:38:58Z",
            "summary": "Pointer Network (PtrNet) is a specific neural network for solving\nCombinatorial Optimization Problems (COPs). While PtrNets offer real-time\nfeed-forward inference for complex COPs instances, its quality of the results\ntends to be less satisfactory. One possible reason is that such issue suffers\nfrom the lack of global search ability of the gradient descent, which is\nfrequently employed in traditional PtrNet training methods including both\nsupervised learning and reinforcement learning. To improve the performance of\nPtrNet, this paper delves deeply into the advantages of training PtrNet with\nEvolutionary Algorithms (EAs), which have been widely acknowledged for not\neasily getting trapped by local optima. Extensive empirical studies based on\nthe Travelling Salesman Problem (TSP) have been conducted. Results demonstrate\nthat PtrNet trained with EA can consistently perform much better inference\nresults than eight state-of-the-art methods on various problem scales. Compared\nwith gradient descent based PtrNet training methods, EA achieves up to 30.21\\%\nimprovement in quality of the solution with the same computational time. With\nthis advantage, this paper is able to at the first time report the results of\nsolving 1000-dimensional TSPs by training a PtrNet on the same dimensionality,\nwhich strongly suggests that scaling up the training instances is in need to\nimprove the performance of PtrNet on solving higher-dimensional COPs.",
            "author": [
                "Muyao Zhong",
                "Shengcai Liu",
                "Bingdong Li",
                "Haobo Fu",
                "Chao Qian",
                "Ke Tang",
                "Peng Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01150v2",
                "http://arxiv.org/pdf/2312.01150v2"
            ],
            "primary_category": "cs.NE",
            "category": [
                "cs.NE",
                "68T07"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01137v1",
            "title": "Fast and Robust Sparsity-Aware Block Diagonal Representation",
            "updated": "2023-12-02T13:44:27Z",
            "published": "2023-12-02T13:44:27Z",
            "summary": "The block diagonal structure of an affinity matrix is a commonly desired\nproperty in cluster analysis because it represents clusters of feature vectors\nby non-zero coefficients that are concentrated in blocks. However, recovering a\nblock diagonal affinity matrix is challenging in real-world applications, in\nwhich the data may be subject to outliers and heavy-tailed noise that obscure\nthe hidden cluster structure. To address this issue, we first analyze the\neffect of different fundamental outlier types in graph-based cluster analysis.\nA key idea that simplifies the analysis is to introduce a vector that\nrepresents a block diagonal matrix as a piece-wise linear function of the\nsimilarity coefficients that form the affinity matrix. We reformulate the\nproblem as a robust piece-wise linear fitting problem and propose a Fast and\nRobust Sparsity-Aware Block Diagonal Representation (FRS-BDR) method, which\njointly estimates cluster memberships and the number of blocks. Comprehensive\nexperiments on a variety of real-world applications demonstrate the\neffectiveness of FRS-BDR in terms of clustering accuracy, robustness against\ncorrupted features, computation time and cluster enumeration performance.",
            "author": [
                "Aylin Tastan",
                "Michael Muma",
                "Abdelhak M. Zoubir"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01137v1",
                "http://arxiv.org/pdf/2312.01137v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02197v1",
            "title": "Exploiting Diffusion Priors for All-in-One Image Restoration",
            "updated": "2023-12-02T13:35:48Z",
            "published": "2023-12-02T13:35:48Z",
            "summary": "All-in-one aims to solve various tasks of image restoration in a single\nmodel. To this end, we present a feasible way of exploiting the image priors\ncaptured by the pretrained diffusion model, through addressing the two\nchallenges, i.e., degradation modeling and diffusion guidance. The former aims\nto simulate the process of the clean image degenerated by certain degradations,\nand the latter aims at guiding the diffusion model to generate the\ncorresponding clean image. With the motivations, we propose a zero-shot\nframework for all-in-one image restoration, termed ZeroAIR, which alternatively\nperforms the test-time degradation modeling (TDM) and the three-stage diffusion\nguidance (TDG) at each timestep of the reverse sampling. To be specific, TDM\nexploits the diffusion priors to learn a degradation model from a given\ndegraded image, and TDG divides the timesteps into three stages for taking full\nadvantage of the varying diffusion priors. Thanks to their degradation-agnostic\nproperty, the all-in-one image restoration could be achieved in a zero-shot way\nby ZeroAIR. Through extensive experiments, we show that our ZeroAIR achieves\ncomparable even better performance than those task-specific methods. The code\nwill be available on Github.",
            "author": [
                "Yuanbiao Gou",
                "Haiyu Zhao",
                "Boyun Li",
                "Xinyan Xiao",
                "Xi Peng"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02197v1",
                "http://arxiv.org/pdf/2312.02197v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02196v1",
            "title": "Dynamic Inertial Poser (DynaIP): Part-Based Motion Dynamics Learning for\n  Enhanced Human Pose Estimation with Sparse Inertial Sensors",
            "updated": "2023-12-02T13:17:10Z",
            "published": "2023-12-02T13:17:10Z",
            "summary": "This paper introduces a novel human pose estimation approach using sparse\ninertial sensors, addressing the shortcomings of previous methods reliant on\nsynthetic data. It leverages a diverse array of real inertial motion capture\ndata from different skeleton formats to improve motion diversity and model\ngeneralization. This method features two innovative components: a\npseudo-velocity regression model for dynamic motion capture with inertial\nsensors, and a part-based model dividing the body and sensor data into three\nregions, each focusing on their unique characteristics. The approach\ndemonstrates superior performance over state-of-the-art models across five\npublic datasets, notably reducing pose error by 19\\% on the DIP-IMU dataset,\nthus representing a significant improvement in inertial sensor-based human pose\nestimation. We will make the implementation of our model available for public\nuse.",
            "author": [
                "Yu Zhang",
                "Songpengcheng Xia",
                "Lei Chu",
                "Jiarui Yang",
                "Qi Wu",
                "Ling Pei"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02196v1",
                "http://arxiv.org/pdf/2312.02196v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01133v1",
            "title": "$t^3$-Variational Autoencoder: Learning Heavy-tailed Data with Student's\n  t and Power Divergence",
            "updated": "2023-12-02T13:14:28Z",
            "published": "2023-12-02T13:14:28Z",
            "summary": "The variational autoencoder (VAE) typically employs a standard normal prior\nas a regularizer for the probabilistic latent encoder. However, the Gaussian\ntail often decays too quickly to effectively accommodate the encoded points,\nfailing to preserve crucial structures hidden in the data. In this paper, we\nexplore the use of heavy-tailed models to combat over-regularization. Drawing\nupon insights from information geometry, we propose $t^3$VAE, a modified VAE\nframework that incorporates Student's t-distributions for the prior, encoder,\nand decoder. This results in a joint model distribution of a power form which\nwe argue can better fit real-world datasets. We derive a new objective by\nreformulating the evidence lower bound as joint optimization of KL divergence\nbetween two statistical manifolds and replacing with $\\gamma$-power divergence,\na natural alternative for power families. $t^3$VAE demonstrates superior\ngeneration of low-density regions when trained on heavy-tailed synthetic data.\nFurthermore, we show that $t^3$VAE significantly outperforms other models on\nCelebA and imbalanced CIFAR-100 datasets.",
            "author": [
                "Juno Kim",
                "Jaehyuk Kwon",
                "Mincheol Cho",
                "Hyunjong Lee",
                "Joong-Ho Won"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01133v1",
                "http://arxiv.org/pdf/2312.01133v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01128v1",
            "title": "SPEEDNet: Salient Pyramidal Enhancement Encoder-Decoder Network for\n  Colonoscopy Images",
            "updated": "2023-12-02T13:03:08Z",
            "published": "2023-12-02T13:03:08Z",
            "summary": "Accurate identification and precise delineation of regions of significance,\nsuch as tumors or lesions, is a pivotal goal in medical imaging analysis. This\npaper proposes SPEEDNet, a novel architecture for precisely segmenting lesions\nwithin colonoscopy images. SPEEDNet uses a novel block named\nDilated-Involutional Pyramidal Convolution Fusion (DIPC). A DIPC block combines\nthe dilated involution layers pairwise into a pyramidal structure to convert\nthe feature maps into a compact space. This lowers the total number of\nparameters while improving the learning of representations across an optimal\nreceptive field, thereby reducing the blurring effect. On the EBHISeg dataset,\nSPEEDNet outperforms three previous networks: UNet, FeedNet, and AttesResDUNet.\nSpecifically, SPEEDNet attains an average dice score of 0.952 and a recall of\n0.971. Qualitative results and ablation studies provide additional insights\ninto the effectiveness of SPEEDNet. The model size of SPEEDNet is 9.81 MB,\nsignificantly smaller than that of UNet (22.84 MB), FeedNet(185.58 MB), and\nAttesResDUNet (140.09 MB).",
            "author": [
                "Tushir Sahu",
                "Vidhi Bhatt",
                "Sai Chandra Teja R",
                "Sparsh Mittal",
                "Nagesh Kumar S"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01128v1",
                "http://arxiv.org/pdf/2312.01128v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01127v1",
            "title": "Symmetric Mean-field Langevin Dynamics for Distributional Minimax\n  Problems",
            "updated": "2023-12-02T13:01:29Z",
            "published": "2023-12-02T13:01:29Z",
            "summary": "In this paper, we extend mean-field Langevin dynamics to minimax optimization\nover probability distributions for the first time with symmetric and provably\nconvergent updates. We propose mean-field Langevin averaged gradient (MFL-AG),\na single-loop algorithm that implements gradient descent ascent in the\ndistribution spaces with a novel weighted averaging, and establish\naverage-iterate convergence to the mixed Nash equilibrium. We also study both\ntime and particle discretization regimes and prove a new uniform-in-time\npropagation of chaos result which accounts for the dependency of the particle\ninteractions on all previous distributions. Furthermore, we propose mean-field\nLangevin anchored best response (MFL-ABR), a symmetric double-loop algorithm\nbased on best response dynamics with linear last-iterate convergence. Finally,\nwe study applications to zero-sum Markov games and conduct simulations\ndemonstrating long-term optimality.",
            "author": [
                "Juno Kim",
                "Kakei Yamamoto",
                "Kazusato Oko",
                "Zhuoran Yang",
                "Taiji Suzuki"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01127v1",
                "http://arxiv.org/pdf/2312.01127v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01121v1",
            "title": "Virtual reservoir acceleration for CPU and GPU: Case study for coupled\n  spin-torque oscillator reservoir",
            "updated": "2023-12-02T12:28:56Z",
            "published": "2023-12-02T12:28:56Z",
            "summary": "We provide high-speed implementations for simulating reservoirs described by\n$N$-coupled spin-torque oscillators. Here $N$ also corresponds to the number of\nreservoir nodes. We benchmark a variety of implementations based on CPU and\nGPU. Our new methods are at least 2.6 times quicker than the baseline for $N$\nin range $1$ to $10^4$. More specifically, over all implementations the best\nfactor is 78.9 for $N=1$ which decreases to 2.6 for $N=10^3$ and finally\nincreases to 23.8 for $N=10^4$. GPU outperforms CPU significantly at $N=2500$.\nOur results show that GPU implementations should be tested for reservoir\nsimulations. The implementations considered here can be used for any reservoir\nwith evolution that can be approximated using an explicit method.",
            "author": [
                "Thomas Geert de Jong",
                "Nozomi Akashi",
                "Tomohiro Taniguchi",
                "Hirofumi Notsu",
                "Kohei Nakajima"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01121v1",
                "http://arxiv.org/pdf/2312.01121v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01118v1",
            "title": "Beyond Accuracy: Statistical Measures and Benchmark for Evaluation of\n  Representation from Self-Supervised Learning",
            "updated": "2023-12-02T12:23:46Z",
            "published": "2023-12-02T12:23:46Z",
            "summary": "Recently, self-supervised metric learning has raised attention for the\npotential to learn a generic distance function. It overcomes the limitations of\nconventional supervised one, e.g., scalability and label biases. Despite\nprogress in this domain, current benchmarks, incorporating a narrow scope of\nclasses, stop the nuanced evaluation of semantic representations. To bridge\nthis gap, we introduce a large-scale benchmark with diversity and granularity\nof classes, Statistical Metric Learning Benchmark (SMLB) built upon\nImageNet-21K and WordNet. SMLB is designed to rigorously evaluate the\ndiscriminative discernment and generalizability across more than 14M images,\n20K classes, and 16K taxonomic nodes. Alongside, we propose novel evaluation\nmetrics -- `overlap' for separability and `aSTD' for consistency -- to measure\ndistance statistical information, which are efficient and robust to the change\nof class number. Our benchmark offers a novel perspective of evaluating the\nquality of representations beyond accuracy. Our findings reveal the limitations\nof supervised learning and the class bias inherent in SSL models, offering\ninsights into potential areas for future model enhancement.",
            "author": [
                "Jiantao Wu",
                "Shentong Mo",
                "Sara Atito",
                "Josef Kittler",
                "Zhenhua Feng",
                "Muhammad Awais"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01118v1",
                "http://arxiv.org/pdf/2312.01118v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02195v1",
            "title": "Cancer Subtype Identification through Integrating Inter and Intra\n  Dataset Relationships in Multi-Omics Data",
            "updated": "2023-12-02T12:11:47Z",
            "published": "2023-12-02T12:11:47Z",
            "summary": "The integration of multi-omics data has emerged as a promising approach for\ngaining comprehensive insights into complex diseases such as cancer. This paper\nproposes a novel approach to identify cancer subtypes through the integration\nof multi-omics data for clustering. The proposed method, named LIDAF utilises\naffinity matrices based on linear relationships between and within different\nomics datasets (Linear Inter and Intra Dataset Affinity Fusion (LIDAF)).\nCanonical Correlation Analysis is in this paper employed to create distance\nmatrices based on Euclidean distances between canonical variates. The distance\nmatrices are converted to affinity matrices and those are fused in a three-step\nprocess. The proposed LIDAF addresses the limitations of the existing method\nresulting in improvement of clustering performance as measured by the Adjusted\nRand Index and the Normalized Mutual Information score. Moreover, our proposed\nLIDAF approach demonstrates a notable enhancement in 50% of the log10 rank\np-values obtained from Cox survival analysis, surpassing the performance of the\nbest reported method, highlighting its potential of identifying distinct cancer\nsubtypes.",
            "author": [
                "Mark Peelen",
                "Leila Bagheriye",
                "Johan Kwisthout"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02195v1",
                "http://arxiv.org/pdf/2312.02195v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "q-bio.GN",
                "stat.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01113v1",
            "title": "Malicious code detection in android: the role of sequence\n  characteristics and disassembling methods",
            "updated": "2023-12-02T11:55:05Z",
            "published": "2023-12-02T11:55:05Z",
            "summary": "The acceptance and widespread use of the Android operating system drew the\nattention of both legitimate developers and malware authors, which resulted in\na significant number of benign and malicious applications available on various\nonline markets. Since the signature-based methods fall short for detecting\nmalicious software effectively considering the vast number of applications,\nmachine learning techniques in this field have also become widespread. In this\ncontext, stating the acquired accuracy values in the contingency tables in\nmalware detection studies has become a popular and efficient method and enabled\nresearchers to evaluate their methodologies comparatively. In this study, we\nwanted to investigate and emphasize the factors that may affect the accuracy\nvalues of the models managed by researchers, particularly the disassembly\nmethod and the input data characteristics. Firstly, we developed a model that\ntackles the malware detection problem from a Natural Language Processing (NLP)\nperspective using Long Short-Term Memory (LSTM). Then, we experimented with\ndifferent base units (instruction, basic block, method, and class) and\nrepresentations of source code obtained from three commonly used disassembling\ntools (JEB, IDA, and Apktool) and examined the results. Our findings exhibit\nthat the disassembly method and different input representations affect the\nmodel results. More specifically, the datasets collected by the Apktool\nachieved better results compared to the other two disassemblers.",
            "author": [
                "Pinar G. Balikcioglu",
                "Melih Sirlanci",
                "Ozge A. Kucuk",
                "Bulut Ulukapi",
                "Ramazan K. Turkmen",
                "Cengiz Acarturk"
            ],
            "link": [
                "http://dx.doi.org/10.1007/s10207-022-00626-2",
                "http://arxiv.org/abs/2312.01113v1",
                "http://arxiv.org/pdf/2312.01113v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01110v1",
            "title": "Strong Duality Relations in Nonconvex Risk-Constrained Learning",
            "updated": "2023-12-02T11:21:00Z",
            "published": "2023-12-02T11:21:00Z",
            "summary": "We establish strong duality relations for functional two-step compositional\nrisk-constrained learning problems with multiple nonconvex loss functions\nand/or learning constraints, regardless of nonconvexity and under a minimal set\nof technical assumptions. Our results in particular imply zero duality gaps\nwithin the class of problems under study, both extending and improving on the\nstate of the art in (risk-neutral) constrained learning. More specifically, we\nconsider risk objectives/constraints which involve real-valued convex and\npositively homogeneous risk measures admitting dual representations with\nbounded risk envelopes, generalizing expectations and including popular\nexamples, such as the conditional value-at-risk (CVaR), the mean-absolute\ndeviation (MAD), and more generally all real-valued coherent risk measures on\nintegrable losses as special cases. Our results are based on recent advances in\nrisk-constrained nonconvex programming in infinite dimensions, which rely on a\nremarkable new application of J. J. Uhl's convexity theorem, which is an\nextension of A. A. Lyapunov's convexity theorem for general, infinite\ndimensional Banach spaces. By specializing to the risk-neutral setting, we\ndemonstrate, for the first time, that constrained classification and regression\ncan be treated under a unifying lens, while dispensing certain restrictive\nassumptions enforced in the current literature, yielding a new state-of-the-art\nstrong duality framework for nonconvex constrained learning.",
            "author": [
                "Dionysis Kalogerias",
                "Spyridon Pougkakiotis"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01110v1",
                "http://arxiv.org/pdf/2312.01110v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02194v1",
            "title": "Local Masking Meets Progressive Freezing: Crafting Efficient Vision\n  Transformers for Self-Supervised Learning",
            "updated": "2023-12-02T11:10:09Z",
            "published": "2023-12-02T11:10:09Z",
            "summary": "In this paper, we present an innovative approach to self-supervised learning\nfor Vision Transformers (ViTs), integrating local masked image modeling with\nprogressive layer freezing. This method focuses on enhancing the efficiency and\nspeed of initial layer training in ViTs. By systematically freezing specific\nlayers at strategic points during training, we reduce computational demands\nwhile maintaining or improving learning capabilities. Our approach employs a\nnovel multi-scale reconstruction process that fosters efficient learning in\ninitial layers and enhances semantic comprehension across scales. The results\ndemonstrate a substantial reduction in training time (~12.5\\%) with a minimal\nimpact on model accuracy (decrease in top-1 accuracy by 0.6\\%). Our method\nachieves top-1 and top-5 accuracies of 82.6\\% and 96.2\\%, respectively,\nunderscoring its potential in scenarios where computational resources and time\nare critical. This work marks an advancement in the field of self-supervised\nlearning for computer vision. The implementation of our approach is available\nat our project's GitHub repository: github.com/utkutpcgl/ViTFreeze.",
            "author": [
                "Utku Mert Topcuoglu",
                "Erdem Akag\u00fcnd\u00fcz"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02194v1",
                "http://arxiv.org/pdf/2312.02194v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01107v1",
            "title": "Rapid Speaker Adaptation in Low Resource Text to Speech Systems using\n  Synthetic Data and Transfer learning",
            "updated": "2023-12-02T10:52:00Z",
            "published": "2023-12-02T10:52:00Z",
            "summary": "Text-to-speech (TTS) systems are being built using end-to-end deep learning\napproaches. However, these systems require huge amounts of training data. We\npresent our approach to built production quality TTS and perform speaker\nadaptation in extremely low resource settings. We propose a transfer learning\napproach using high-resource language data and synthetically generated data. We\ntransfer the learnings from the out-domain high-resource English language.\nFurther, we make use of out-of-the-box single-speaker TTS in the target\nlanguage to generate in-domain synthetic data. We employ a three-step approach\nto train a high-quality single-speaker TTS system in a low-resource Indian\nlanguage Hindi. We use a Tacotron2 like setup with a spectrogram prediction\nnetwork and a waveglow vocoder. The Tacotron2 acoustic model is trained on\nEnglish data, followed by synthetic Hindi data from the existing TTS system.\nFinally, the decoder of this model is fine-tuned on only 3 hours of target\nHindi speaker data to enable rapid speaker adaptation. We show the importance\nof this dual pre-training and decoder-only fine-tuning using subjective MOS\nevaluation. Using transfer learning from high-resource language and synthetic\ncorpus we present a low-cost solution to train a custom TTS model.",
            "author": [
                "Raviraj Joshi",
                "Nikesh Garera"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01107v1",
                "http://arxiv.org/pdf/2312.01107v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01105v1",
            "title": "S2P3: Self-Supervised Polarimetric Pose Prediction",
            "updated": "2023-12-02T10:46:40Z",
            "published": "2023-12-02T10:46:40Z",
            "summary": "This paper proposes the first self-supervised 6D object pose prediction from\nmultimodal RGB+polarimetric images. The novel training paradigm comprises 1) a\nphysical model to extract geometric information of polarized light, 2) a\nteacher-student knowledge distillation scheme and 3) a self-supervised loss\nformulation through differentiable rendering and an invertible physical\nconstraint. Both networks leverage the physical properties of polarized light\nto learn robust geometric representations by encoding shape priors and\npolarization characteristics derived from our physical model. Geometric\npseudo-labels from the teacher support the student network without the need for\nannotated real data. Dense appearance and geometric information of objects are\nobtained through a differentiable renderer with the predicted pose for\nself-supervised direct coupling. The student network additionally features our\nproposed invertible formulation of the physical shape priors that enables\nend-to-end self-supervised training through physical constraints of derived\npolarization characteristics compared against polarimetric input images. We\nspecifically focus on photometrically challenging objects with texture-less or\nreflective surfaces and transparent materials for which the most prominent\nperformance gain is reported.",
            "author": [
                "Patrick Ruhkamp",
                "Daoyi Gao",
                "Nassir Navab",
                "Benjamin Busam"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01105v1",
                "http://arxiv.org/pdf/2312.01105v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01103v1",
            "title": "Code-Mixed Text to Speech Synthesis under Low-Resource Constraints",
            "updated": "2023-12-02T10:40:38Z",
            "published": "2023-12-02T10:40:38Z",
            "summary": "Text-to-speech (TTS) systems are an important component in voice-based\ne-commerce applications. These applications include end-to-end voice assistant\nand customer experience (CX) voice bot. Code-mixed TTS is also relevant in\nthese applications since the product names are commonly described in English\nwhile the surrounding text is in a regional language. In this work, we describe\nour approaches for production quality code-mixed Hindi-English TTS systems\nbuilt for e-commerce applications. We propose a data-oriented approach by\nutilizing monolingual data sets in individual languages. We leverage a\ntransliteration model to convert the Roman text into a common Devanagari script\nand then combine both datasets for training. We show that such single script\nbi-lingual training without any code-mixing works well for pure code-mixed test\nsets. We further present an exhaustive evaluation of single-speaker adaptation\nand multi-speaker training with Tacotron2 + Waveglow setup to show that the\nformer approach works better. These approaches are also coupled with transfer\nlearning and decoder-only fine-tuning to improve performance. We compare these\napproaches with the Google TTS and report a positive CMOS score of 0.02 with\nthe proposed transfer learning approach. We also perform low-resource voice\nadaptation experiments to show that a new voice can be onboarded with just 3\nhrs of data. This highlights the importance of our pre-trained models in\nresource-constrained settings. This subjective evaluation is performed on a\nlarge number of out-of-domain pure code-mixed sentences to demonstrate the high\nquality of the systems.",
            "author": [
                "Raviraj Joshi",
                "Nikesh Garera"
            ],
            "link": [
                "http://dx.doi.org/10.1007/978-3-031-48312-7_12",
                "http://arxiv.org/abs/2312.01103v1",
                "http://arxiv.org/pdf/2312.01103v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01100v1",
            "title": "Prior-Aware Robust Beam Alignment for Low-SNR Millimeter-Wave\n  Communications",
            "updated": "2023-12-02T10:29:04Z",
            "published": "2023-12-02T10:29:04Z",
            "summary": "This paper presents a robust beam alignment technique for millimeter-wave\ncommunications in low signal-to-noise ratio (SNR) environments. The core\nstrategy of our technique is to repeatedly transmit the most probable beam\ncandidates to reduce beam misalignment probability induced by noise.\nSpecifically, for a given beam training overhead, both the selection of\ncandidates and the number of repetitions for each beam candidate are optimized\nbased on channel prior information. To achieve this, a deep neural network is\nemployed to learn the prior probability of the optimal beam at each location.\nThe beam misalignment probability is then analyzed based on the channel prior,\nforming the basis for an optimization problem aimed at minimizing the analyzed\nbeam misalignment probability. A closed-form solution is derived for a special\ncase with two beam candidates, and an efficient algorithm is developed for\ngeneral cases with multiple beam candidates. Simulation results using the\nDeepMIMO dataset demonstrate the superior performance of our technique in\ndynamic low-SNR communication environments when compared to existing beam\nalignment techniques.",
            "author": [
                "Jihun Park",
                "Yongjeong Oh",
                "Jaewon Yun",
                "Seonjung Kim",
                "Yo-Seb Jeon"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01100v1",
                "http://arxiv.org/pdf/2312.01100v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "eess.SP",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01099v1",
            "title": "Rethinking Multiple Instance Learning for Whole Slide Image\n  Classification: A Bag-Level Classifier is a Good Instance-Level Teacher",
            "updated": "2023-12-02T10:16:03Z",
            "published": "2023-12-02T10:16:03Z",
            "summary": "Multiple Instance Learning (MIL) has demonstrated promise in Whole Slide\nImage (WSI) classification. However, a major challenge persists due to the high\ncomputational cost associated with processing these gigapixel images. Existing\nmethods generally adopt a two-stage approach, comprising a non-learnable\nfeature embedding stage and a classifier training stage. Though it can greatly\nreduce the memory consumption by using a fixed feature embedder pre-trained on\nother domains, such scheme also results in a disparity between the two stages,\nleading to suboptimal classification accuracy. To address this issue, we\npropose that a bag-level classifier can be a good instance-level teacher. Based\non this idea, we design Iteratively Coupled Multiple Instance Learning (ICMIL)\nto couple the embedder and the bag classifier at a low cost. ICMIL initially\nfix the patch embedder to train the bag classifier, followed by fixing the bag\nclassifier to fine-tune the patch embedder. The refined embedder can then\ngenerate better representations in return, leading to a more accurate\nclassifier for the next iteration. To realize more flexible and more effective\nembedder fine-tuning, we also introduce a teacher-student framework to\nefficiently distill the category knowledge in the bag classifier to help the\ninstance-level embedder fine-tuning. Thorough experiments were conducted on\nfour distinct datasets to validate the effectiveness of ICMIL. The experimental\nresults consistently demonstrate that our method significantly improves the\nperformance of existing MIL backbones, achieving state-of-the-art results. The\ncode is available at: https://github.com/Dootmaan/ICMIL/tree/confidence_based",
            "author": [
                "Hongyi Wang",
                "Luyang Luo",
                "Fang Wang",
                "Ruofeng Tong",
                "Yen-Wei Chen",
                "Hongjie Hu",
                "Lanfen Lin",
                "Hao Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01099v1",
                "http://arxiv.org/pdf/2312.01099v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01097v1",
            "title": "Planning as In-Painting: A Diffusion-Based Embodied Task Planning\n  Framework for Environments under Uncertainty",
            "updated": "2023-12-02T10:07:17Z",
            "published": "2023-12-02T10:07:17Z",
            "summary": "Task planning for embodied AI has been one of the most challenging problems\nwhere the community does not meet a consensus in terms of formulation. In this\npaper, we aim to tackle this problem with a unified framework consisting of an\nend-to-end trainable method and a planning algorithm. Particularly, we propose\na task-agnostic method named 'planning as in-painting'. In this method, we use\na Denoising Diffusion Model (DDM) for plan generation, conditioned on both\nlanguage instructions and perceptual inputs under partially observable\nenvironments. Partial observation often leads to the model hallucinating the\nplanning. Therefore, our diffusion-based method jointly models both state\ntrajectory and goal estimation to improve the reliability of the generated\nplan, given the limited available information at each step. To better leverage\nnewly discovered information along the plan execution for a higher success\nrate, we propose an on-the-fly planning algorithm to collaborate with the\ndiffusion-based planner. The proposed framework achieves promising performances\nin various embodied AI tasks, including vision-language navigation, object\nmanipulation, and task planning in a photorealistic virtual environment. The\ncode is available at: https://github.com/joeyy5588/planning-as-inpainting.",
            "author": [
                "Cheng-Fu Yang",
                "Haoyang Xu",
                "Te-Lin Wu",
                "Xiaofeng Gao",
                "Kai-Wei Chang",
                "Feng Gao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01097v1",
                "http://arxiv.org/pdf/2312.01097v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01093v1",
            "title": "Predicting Postoperative Nausea And Vomiting Using Machine Learning: A\n  Model Development and Validation Study",
            "updated": "2023-12-02T09:51:49Z",
            "published": "2023-12-02T09:51:49Z",
            "summary": "Background: Postoperative nausea and vomiting (PONV) is a frequently observed\ncomplication in patients undergoing surgery under general anesthesia. Moreover,\nit is a frequent cause of distress and dissatisfaction during the early\npostoperative period. The tools used for predicting PONV at present have not\nyielded satisfactory results. Therefore, prognostic tools for the prediction of\nearly and delayed PONV were developed in this study with the aim of achieving\nsatisfactory predictive performance.\n  Methods: The retrospective data of adult patients admitted to the\npost-anesthesia care unit after undergoing surgical procedures under general\nanesthesia at the Sheba Medical Center, Israel, between September 1, 2018, and\nSeptember 1, 2023, were used in this study. An ensemble model of machine\nlearning algorithms trained on the data of 54848 patients was developed. The\nk-fold cross-validation method was used followed by splitting the data to train\nand test sets that optimally preserve the sociodemographic features of the\npatients, such as age, sex, and smoking habits, using the Bee Colony algorithm.\n  Findings: Among the 54848 patients, early and delayed PONV were observed in\n2706 (4.93%) and 8218 (14.98%) patients, respectively. The proposed PONV\nprediction tools could correctly predict early and delayed PONV in 84.0% and\n77.3% of cases, respectively, outperforming the second-best PONV prediction\ntool (Koivuranta score) by 13.4% and 12.9%, respectively. Feature importance\nanalysis revealed that the performance of the proposed prediction tools aligned\nwith previous clinical knowledge, indicating their utility.\n  Interpretation: The machine learning-based tools developed in this study\nenabled improved PONV prediction, thereby facilitating personalized care and\nimproved patient outcomes.",
            "author": [
                "Maxim Glebov",
                "Teddy Lazebnik",
                "Boris Orkin",
                "Haim Berkenstadt",
                "Svetlana Bunimovich-Mendrazitsky"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01093v1",
                "http://arxiv.org/pdf/2312.01093v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01092v1",
            "title": "A Semi-Supervised Deep Learning Approach to Dataset Collection for\n  Query-By-Humming Task",
            "updated": "2023-12-02T09:50:00Z",
            "published": "2023-12-02T09:50:00Z",
            "summary": "Query-by-Humming (QbH) is a task that involves finding the most relevant song\nbased on a hummed or sung fragment. Despite recent successful commercial\nsolutions, implementing QbH systems remains challenging due to the lack of\nhigh-quality datasets for training machine learning models. In this paper, we\npropose a deep learning data collection technique and introduce Covers and\nHummings Aligned Dataset (CHAD), a novel dataset that contains 18 hours of\nshort music fragments, paired with time-aligned hummed versions. To expand our\ndataset, we employ a semi-supervised model training pipeline that leverages the\nQbH task as a specialized case of cover song identification (CSI) task.\nStarting with a model trained on the initial dataset, we iteratively collect\ngroups of fragments of cover versions of the same song and retrain the model on\nthe extended data. Using this pipeline, we collect over 308 hours of additional\nmusic fragments, paired with time-aligned cover versions. The final model is\nsuccessfully applied to the QbH task and achieves competitive results on\nbenchmark datasets. Our study shows that the proposed dataset and training\npipeline can effectively facilitate the implementation of QbH systems.",
            "author": [
                "Amantur Amatov",
                "Dmitry Lamanov",
                "Maksim Titov",
                "Ivan Vovk",
                "Ilya Makarov",
                "Mikhail Kudinov"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01092v1",
                "http://arxiv.org/pdf/2312.01092v1"
            ],
            "primary_category": "cs.SD",
            "category": [
                "cs.SD",
                "cs.LG",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01090v1",
            "title": "Self Generated Wargame AI: Double Layer Agent Task Planning Based on\n  Large Language Model",
            "updated": "2023-12-02T09:45:45Z",
            "published": "2023-12-02T09:45:45Z",
            "summary": "The big language model represented by ChatGPT has had a disruptive impact on\nthe field of artificial intelligence. But it mainly focuses on Natural language\nprocessing, speech recognition, machine learning and natural-language\nunderstanding. This paper innovatively applies the big language model to the\nfield of intelligent decision-making, places the big language model in the\ndecision-making center, and constructs an agent architecture with the big\nlanguage model as the core. Based on this, it further proposes a two-layer\nagent task planning, issues and executes decision commands through the\ninteraction of natural language, and carries out simulation verification\nthrough the wargame simulation environment. Through the game confrontation\nsimulation experiment, it is found that the intelligent decision-making ability\nof the big language model is significantly stronger than the commonly used\nreinforcement learning AI and rule AI, and the intelligence, understandability\nand generalization are all better. And through experiments, it was found that\nthe intelligence of the large language model is closely related to prompt. This\nwork also extends the large language model from previous human-computer\ninteraction to the field of intelligent decision-making, which has important\nreference value and significance for the development of intelligent\ndecision-making.",
            "author": [
                "Y. Sun",
                "C. Yu",
                "J. Zhao",
                "W. Wang",
                "X. Zhou"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01090v1",
                "http://arxiv.org/pdf/2312.01090v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01085v1",
            "title": "RobustCalib: Robust Lidar-Camera Extrinsic Calibration with Consistency\n  Learning",
            "updated": "2023-12-02T09:29:50Z",
            "published": "2023-12-02T09:29:50Z",
            "summary": "Current traditional methods for LiDAR-camera extrinsics estimation depend on\noffline targets and human efforts, while learning-based approaches resort to\niterative refinement for calibration results, posing constraints on their\ngeneralization and application in on-board systems. In this paper, we propose a\nnovel approach to address the extrinsic calibration problem in a robust,\nautomatic, and single-shot manner. Instead of directly optimizing extrinsics,\nwe leverage the consistency learning between LiDAR and camera to implement\nimplicit re-calibartion. Specially, we introduce an appearance-consistency loss\nand a geometric-consistency loss to minimizing the inconsitency between the\nattrbutes (e.g., intensity and depth) of projected LiDAR points and the\npredicted ones. This design not only enhances adaptability to various scenarios\nbut also enables a simple and efficient formulation during inference. We\nconduct comprehensive experiments on different datasets, and the results\ndemonstrate that our method achieves accurate and robust performance. To\npromote further research and development in this area, we will release our\nmodel and code.",
            "author": [
                "Shuang Xu",
                "Sifan Zhou",
                "Zhi Tian",
                "Jizhou Ma",
                "Qiong Nie",
                "Xiangxiang Chu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01085v1",
                "http://arxiv.org/pdf/2312.01085v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01084v1",
            "title": "General noise-resilient quantum amplitude estimation",
            "updated": "2023-12-02T09:27:40Z",
            "published": "2023-12-02T09:27:40Z",
            "summary": "Quantum advantage requires overcoming noise-induced degradation of quantum\nsystems. Conventional methods for reducing noise such as error mitigation face\nscalability issues in deep circuits. Specifically, noise hampers the extraction\nof amplitude and observable information from quantum systems. In this work, we\npresent a novel algorithm that enhances the estimation of amplitude and\nobservable under noise. Remarkably, our algorithm exhibits robustness against\nnoise that varies across different depths of the quantum circuits. We assess\nthe accuracy of amplitude and observable using numerical analysis and\ntheoretically analyze the impact of gate-dependent noise on the results. This\nalgorithm is a potential candidate for noise-resilient approaches that have\nhigh computational accuracy.",
            "author": [
                "Yonglong Ding",
                "Ruyu Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01084v1",
                "http://arxiv.org/pdf/2312.01084v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01083v1",
            "title": "Consistency Prototype Module and Motion Compensation for Few-Shot Action\n  Recognition (CLIP-CP$\\mathbf{M^2}$C)",
            "updated": "2023-12-02T09:20:31Z",
            "published": "2023-12-02T09:20:31Z",
            "summary": "Recently, few-shot action recognition has significantly progressed by\nlearning the feature discriminability and designing suitable comparison\nmethods. Still, there are the following restrictions. (a) Previous works are\nmainly based on visual mono-modal. Although some multi-modal works use labels\nas supplementary to construct prototypes of support videos, they can not use\nthis information for query videos. The labels are not used efficiently. (b)\nMost of the works ignore the motion feature of video, although the motion\nfeatures are essential for distinguishing. We proposed a Consistency Prototype\nand Motion Compensation Network(CLIP-CP$M^2$C) to address these issues.\nFirstly, we use the CLIP for multi-modal few-shot action recognition with the\ntext-image comparison for domain adaption. Secondly, in order to make the\namount of information between the prototype and the query more similar, we\npropose a novel method to compensate for the text(prompt) information of query\nvideos when text(prompt) does not exist, which depends on a Consistency Loss.\nThirdly, we use the differential features of the adjacent frames in two\ndirections as the motion features, which explicitly embeds the network with\nmotion dynamics. We also apply the Consistency Loss to the motion features.\nExtensive experiments on standard benchmark datasets demonstrate that the\nproposed method can compete with state-of-the-art results. Our code is\navailable at the URL: https://github.com/xxx/xxx.git.",
            "author": [
                "Fei Guo",
                "Li Zhu",
                "YiKang Wang",
                "Han Qi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01083v1",
                "http://arxiv.org/pdf/2312.01083v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01082v1",
            "title": "On the Effects of Randomness on Stability of Learning with Limited\n  Labelled Data: A Systematic Literature Review",
            "updated": "2023-12-02T09:20:10Z",
            "published": "2023-12-02T09:20:10Z",
            "summary": "Learning with limited labelled data, such as few-shot learning, meta-learning\nor transfer learning, aims to effectively train a model using only small amount\nof labelled samples. However, these approaches were observed to be excessively\nsensitive to the effects of uncontrolled randomness caused by non-determinism\nin the training process. The randomness negatively affects the stability of the\nmodels, leading to large variance in results across training runs. When such\ninstability is disregarded, it can unintentionally, but unfortunately also\nintentionally, create an imaginary perception of research progress. Recently,\nthis area started to attract a research attention and the number of relevant\nstudies is continuously growing. In this survey, we provide a comprehensive\noverview of 134 papers addressing the effects of randomness on the stability of\nlearning with limited labelled data. We distinguish between four main tasks\naddressed in the papers (investigate/evaluate; determine; mitigate;\nbenchmark/compare/report randomness effects), providing findings for each one.\nFurthermore, we identify and discuss seven challenges and open problems\ntogether with possible directions to facilitate further research. The ultimate\ngoal of this survey is to emphasise the importance of this growing research\narea, which so far has not received appropriate level of attention.",
            "author": [
                "Branislav Pecher",
                "Ivan Srba",
                "Maria Bielikova"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01082v1",
                "http://arxiv.org/pdf/2312.01082v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01081v1",
            "title": "Adaptive Resource Allocation for Semantic Communication Networks",
            "updated": "2023-12-02T09:12:12Z",
            "published": "2023-12-02T09:12:12Z",
            "summary": "Semantic communication, recognized as a promising technology for future\nintelligent applications, has received widespread research attention. Despite\nthe potential of semantic communication to enhance transmission reliability,\nespecially in low signal-to-noise (SNR) environments, the critical issue of\nresource allocation and compatibility in the dynamic wireless environment\nremains largely unexplored. In this paper, we propose an adaptive semantic\nresource allocation paradigm with semantic-bit quantization (SBQ) compatibly\nfor existing wireless communications, where the inaccurate environment\nperception introduced by the additional mapping relationship between semantic\nmetrics and transmission metrics is solved. In order to investigate the\nperformance of semantic communication networks, the quality of service for\nsemantic communication (SC-QoS), including the semantic quantization efficiency\n(SQE) and transmission latency, is proposed for the first time. A problem of\nmaximizing the overall effective SC-QoS is formulated by jointly optimizing the\ntransmit beamforming of the base station, the bits for semantic representation,\nthe subchannel assignment, and the bandwidth resource allocation. To address\nthe non-convex formulated problem, an intelligent resource allocation scheme is\nproposed based on a hybrid deep reinforcement learning (DRL) algorithm, where\nthe intelligent agent can perceive both semantic tasks and dynamic wireless\nenvironments. Simulation results demonstrate that our design can effectively\ncombat semantic noise and achieve superior performance in wireless\ncommunications compared to several benchmark schemes. Furthermore, compared to\nmapping-guided paradigm based resource allocation schemes, our proposed\nadaptive scheme can achieve up to 13% performance improvement in terms of\nSC-QoS.",
            "author": [
                "Lingyi Wang",
                "Wei Wu",
                "Fuhui Zhou",
                "Zhaohui Yang",
                "Zhijin Qin"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01081v1",
                "http://arxiv.org/pdf/2312.01081v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "cs.AI",
                "cs.LG",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01080v1",
            "title": "A Novel Residual-guided Learning Method for Image Steganography",
            "updated": "2023-12-02T09:10:49Z",
            "published": "2023-12-02T09:10:49Z",
            "summary": "Traditional steganographic techniques have often relied on manually crafted\nattributes related to image residuals. These methods demand a significant level\nof expertise and face challenges in integrating diverse image residual\ncharacteristics. In this paper, we introduce an innovative deep learning-based\nmethodology that seamlessly integrates image residuals, residual distances, and\nimage local variance to autonomously learn embedding probabilities. Our\nframework includes an embedding probability generator and three pivotal guiding\ncomponents: Residual guidance strives to facilitate embedding in\ncomplex-textured areas. Residual distance guidance aims to minimize the\nresidual differences between cover and stego images. Local variance guidance\neffectively safeguards against modifications in regions characterized by\nuncomplicated or uniform textures. The three components collectively guide the\nlearning process, enhancing the security performance. Comprehensive\nexperimental findings underscore the superiority of our approach when compared\nto traditional steganographic methods and randomly initialized ReLOAD in the\nspatial domain.",
            "author": [
                "Miaoxin Ye",
                "Dongxia Huang",
                "Kangkang Wei",
                "Weiqi Luo"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01080v1",
                "http://arxiv.org/pdf/2312.01080v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01072v1",
            "title": "A Survey of Temporal Credit Assignment in Deep Reinforcement Learning",
            "updated": "2023-12-02T08:49:51Z",
            "published": "2023-12-02T08:49:51Z",
            "summary": "The Credit Assignment Problem (CAP) refers to the longstanding challenge of\nReinforcement Learning (RL) agents to associate actions with their long-term\nconsequences. Solving the CAP is a crucial step towards the successful\ndeployment of RL in the real world since most decision problems provide\nfeedback that is noisy, delayed, and with little or no information about the\ncauses. These conditions make it hard to distinguish serendipitous outcomes\nfrom those caused by informed decision-making. However, the mathematical nature\nof credit and the CAP remains poorly understood and defined. In this survey, we\nreview the state of the art of Temporal Credit Assignment (CA) in deep RL. We\npropose a unifying formalism for credit that enables equitable comparisons of\nstate of the art algorithms and improves our understanding of the trade-offs\nbetween the various methods. We cast the CAP as the problem of learning the\ninfluence of an action over an outcome from a finite amount of experience. We\ndiscuss the challenges posed by delayed effects, transpositions, and a lack of\naction influence, and analyse how existing methods aim to address them.\nFinally, we survey the protocols to evaluate a credit assignment method, and\nsuggest ways to diagnoses the sources of struggle for different credit\nassignment methods. Overall, this survey provides an overview of the field for\nnew-entry practitioners and researchers, it offers a coherent perspective for\nscholars looking to expedite the starting stages of a new study on the CAP, and\nit suggests potential directions for future research",
            "author": [
                "Eduardo Pignatelli",
                "Johan Ferret",
                "Matthieu Geist",
                "Thomas Mesnard",
                "Hado van Hasselt",
                "Laura Toni"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01072v1",
                "http://arxiv.org/pdf/2312.01072v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01071v1",
            "title": "Hybrid Hierarchical DRL Enabled Resource Allocation for Secure\n  Transmission in Multi-IRS-Assisted Sensing-Enhanced Spectrum Sharing Networks",
            "updated": "2023-12-02T08:49:31Z",
            "published": "2023-12-02T08:49:31Z",
            "summary": "Secure communications are of paramount importance in spectrum sharing\nnetworks due to the allocation and sharing characteristics of spectrum\nresources. To further explore the potential of intelligent reflective surfaces\n(IRSs) in enhancing spectrum sharing and secure transmission performance, a\nmultiple intelligent reflection surface (multi-IRS)-assisted sensing-enhanced\nwideband spectrum sharing network is investigated by considering physical layer\nsecurity techniques. An intelligent resource allocation scheme based on double\ndeep Q networks (D3QN) algorithm and soft Actor-Critic (SAC) algorithm is\nproposed to maximize the secure transmission rate of the secondary network by\njointly optimizing IRS pairings, subchannel assignment, transmit beamforming of\nthe secondary base station, reflection coefficients of IRSs and the sensing\ntime. To tackle the sparse reward problem caused by a significant amount of\nreflection elements of multiple IRSs, the method of hierarchical reinforcement\nlearning is exploited. An alternative optimization (AO)-based conventional\nmathematical scheme is introduced to verify the computational complexity\nadvantage of our proposed intelligent scheme. Simulation results demonstrate\nthe efficiency of our proposed intelligent scheme as well as the superiority of\nmulti-IRS design in enhancing secrecy rate and spectrum utilization. It is\nshown that inappropriate deployment of IRSs can reduce the security performance\nwith the presence of multiple eavesdroppers (Eves), and the arrangement of IRSs\ndeserves further consideration.",
            "author": [
                "Lingyi Wang",
                "Wei Wu",
                "Fuhui Zhou",
                "Qihui Wu",
                "Octavia A. Dobre",
                "Tony Q. S. Quek"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01071v1",
                "http://arxiv.org/pdf/2312.01071v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "eess.SP",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01062v1",
            "title": "Acoustic Signal Analysis with Deep Neural Network for Detecting Fault\n  Diagnosis in Industrial Machines",
            "updated": "2023-12-02T08:09:27Z",
            "published": "2023-12-02T08:09:27Z",
            "summary": "Detecting machine malfunctions at an early stage is crucial for reducing\ninterruptions in operational processes within industrial settings. Recently,\nthe deep learning approach has started to be preferred for the detection of\nfailures in machines. Deep learning provides an effective solution in fault\ndetection processes thanks to automatic feature extraction. In this study, a\ndeep learning-based system was designed to analyze the sound signals produced\nby industrial machines. Acoustic sound signals were converted into Mel\nspectrograms. For the purpose of classifying spectrogram images, the\nDenseNet-169 model, a deep learning architecture recognized for its\neffectiveness in image classification tasks, was used. The model was trained\nusing the transfer learning method on the MIMII dataset including sounds from\nfour types of industrial machines. The results showed that the proposed method\nreached an accuracy rate varying between 97.17% and 99.87% at different Sound\nNoise Rate levels.",
            "author": [
                "Mustafa Yurdakul",
                "Sakir Tasdemir"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01062v1",
                "http://arxiv.org/pdf/2312.01062v1"
            ],
            "primary_category": "cs.SD",
            "category": [
                "cs.SD",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01061v1",
            "title": "Spectral-wise Implicit Neural Representation for Hyperspectral Image\n  Reconstruction",
            "updated": "2023-12-02T08:06:07Z",
            "published": "2023-12-02T08:06:07Z",
            "summary": "Coded Aperture Snapshot Spectral Imaging (CASSI) reconstruction aims to\nrecover the 3D spatial-spectral signal from 2D measurement. Existing methods\nfor reconstructing Hyperspectral Image (HSI) typically involve learning\nmappings from a 2D compressed image to a predetermined set of discrete spectral\nbands. However, this approach overlooks the inherent continuity of the spectral\ninformation. In this study, we propose an innovative method called\nSpectral-wise Implicit Neural Representation (SINR) as a pioneering step toward\naddressing this limitation. SINR introduces a continuous spectral amplification\nprocess for HSI reconstruction, enabling spectral super-resolution with\ncustomizable magnification factors. To achieve this, we leverage the concept of\nimplicit neural representation. Specifically, our approach introduces a\nspectral-wise attention mechanism that treats individual channels as distinct\ntokens, thereby capturing global spectral dependencies. Additionally, our\napproach incorporates two components, namely a Fourier coordinate encoder and a\nspectral scale factor module. The Fourier coordinate encoder enhances the\nSINR's ability to emphasize high-frequency components, while the spectral scale\nfactor module guides the SINR to adapt to the variable number of spectral\nchannels. Notably, the SINR framework enhances the flexibility of CASSI\nreconstruction by accommodating an unlimited number of spectral bands in the\ndesired output. Extensive experiments demonstrate that our SINR outperforms\nbaseline methods. By enabling continuous reconstruction within the CASSI\nframework, we take the initial stride toward integrating implicit neural\nrepresentation into the field.",
            "author": [
                "Huan Chen",
                "Wangcai Zhao",
                "Tingfa Xu",
                "Shiyun Zhou",
                "Peifu Liu",
                "Jianan Li"
            ],
            "link": [
                "http://dx.doi.org/10.1109/TCSVT.2023.3318366",
                "http://arxiv.org/abs/2312.01061v1",
                "http://arxiv.org/pdf/2312.01061v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01058v1",
            "title": "A Survey of Progress on Cooperative Multi-agent Reinforcement Learning\n  in Open Environment",
            "updated": "2023-12-02T08:04:31Z",
            "published": "2023-12-02T08:04:31Z",
            "summary": "Multi-agent Reinforcement Learning (MARL) has gained wide attention in recent\nyears and has made progress in various fields. Specifically, cooperative MARL\nfocuses on training a team of agents to cooperatively achieve tasks that are\ndifficult for a single agent to handle. It has shown great potential in\napplications such as path planning, autonomous driving, active voltage control,\nand dynamic algorithm configuration. One of the research focuses in the field\nof cooperative MARL is how to improve the coordination efficiency of the\nsystem, while research work has mainly been conducted in simple, static, and\nclosed environment settings. To promote the application of artificial\nintelligence in real-world, some research has begun to explore multi-agent\ncoordination in open environments. These works have made progress in exploring\nand researching the environments where important factors might change. However,\nthe mainstream work still lacks a comprehensive review of the research\ndirection. In this paper, starting from the concept of reinforcement learning,\nwe subsequently introduce multi-agent systems (MAS), cooperative MARL, typical\nmethods, and test environments. Then, we summarize the research work of\ncooperative MARL from closed to open environments, extract multiple research\ndirections, and introduce typical works. Finally, we summarize the strengths\nand weaknesses of the current research, and look forward to the future\ndevelopment direction and research problems in cooperative MARL in open\nenvironments.",
            "author": [
                "Lei Yuan",
                "Ziqian Zhang",
                "Lihe Li",
                "Cong Guan",
                "Yang Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01058v1",
                "http://arxiv.org/pdf/2312.01058v1"
            ],
            "primary_category": "cs.MA",
            "category": [
                "cs.MA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01057v1",
            "title": "RLHF and IIA: Perverse Incentives",
            "updated": "2023-12-02T08:04:29Z",
            "published": "2023-12-02T08:04:29Z",
            "summary": "Existing algorithms for reinforcement learning from human feedback (RLHF) can\nincentivize responses at odds with preferences because they are based on models\nthat assume independence of irrelevant alternatives (IIA). The perverse\nincentives induced by IIA give rise to egregious behavior when innovating on\nquery formats or learning algorithms.",
            "author": [
                "Wanqiao Xu",
                "Shi Dong",
                "Xiuyuan Lu",
                "Grace Lam",
                "Zheng Wen",
                "Benjamin Van Roy"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01057v1",
                "http://arxiv.org/pdf/2312.01057v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01050v1",
            "title": "Detection and Analysis of Stress-Related Posts in Reddit Acamedic\n  Communities",
            "updated": "2023-12-02T07:34:03Z",
            "published": "2023-12-02T07:34:03Z",
            "summary": "Nowadays, the significance of monitoring stress levels and recognizing early\nsigns of mental illness cannot be overstated. Automatic stress detection in\ntext can proactively help manage stress and protect mental well-being. In\ntoday's digital era, social media platforms reflect the psychological\nwell-being and stress levels within various communities. This study focuses on\ndetecting and analyzing stress-related posts in Reddit academic communities.\nDue to online education and remote work, these communities have become central\nfor academic discussions and support. We classify text as stressed or not using\nnatural language processing and machine learning classifiers, with Dreaddit as\nour training dataset, which contains labeled data from Reddit. Next, we collect\nand analyze posts from various academic subreddits. We identified that the most\neffective individual feature for stress detection is the Bag of Words, paired\nwith the Logistic Regression classifier, achieving a 77.78% accuracy rate and\nan F1 score of 0.79 on the DReaddit dataset. This combination also performs\nbest in stress detection on human-annotated datasets, with a 72% accuracy rate.\nOur key findings reveal that posts and comments in professors Reddit\ncommunities are the most stressful, compared to other academic levels,\nincluding bachelor, graduate, and Ph.D. students. This research contributes to\nour understanding of the stress levels within academic communities. It can help\nacademic institutions and online communities develop measures and interventions\nto address this issue effectively.",
            "author": [
                "Nazzere Oryngozha",
                "Pakizar Shamoi",
                "Ayan Igali"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01050v1",
                "http://arxiv.org/pdf/2312.01050v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02191v1",
            "title": "Prompt Tuning for Zero-shot Compositional Learning",
            "updated": "2023-12-02T07:32:24Z",
            "published": "2023-12-02T07:32:24Z",
            "summary": "Open World Compositional Zero-Shot Learning (OW-CZSL) is known to be an\nextremely challenging task, which aims to recognize unseen compositions formed\nfrom seen attributes and objects without any prior assumption of the output\nspace. In order to achieve this goal, a model has to be \"smart\" and\n\"knowledgeable\". To be smart, a model should be good at reasoning the\ninteractions between attributes and objects from the seen compositions. While\n\"knowledgeable\" means the model owns \"common sense\" to the open world that can\n\"foresee\" some features of the unseen compositions. Most previous work focuses\non the \"smart\" part, while few of them provided an effective solution to\nachieve the \"knowledgeable\" goal. In this paper, we proposed a framework named\nMulti-Modal Prompt Tuning (MMPT) to inherit the \"knowledgeable\" property from\nthe large pre-trained vision-language model. Extensive experiments show that\nour proposed MMPT obtains new state-of-the-art results in OW-CZSL task. On the\nUT-Zappos dataset, MMPT pushes the AUC score to $29.8$, while the previous best\nscore is $26.5$. On the more challenging MIT-States dataset, the AUC score of\nMMPT is 1.5 times better than the current state-of-the-art.",
            "author": [
                "Lingyu Zhang",
                "Ting Hua",
                "Yilin Shen",
                "Hongxia Jin"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02191v1",
                "http://arxiv.org/pdf/2312.02191v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01049v1",
            "title": "Joint User Association and Resource Allocation for Multi-Cell Networks\n  with Adaptive Semantic Communication",
            "updated": "2023-12-02T07:17:42Z",
            "published": "2023-12-02T07:17:42Z",
            "summary": "Semantic communication is a promising communication paradigm that utilizes\nDeep Neural Networks (DNNs) to extract the information relevant to downstream\ntasks, hence significantly reducing the amount of transmitted data. In current\npractice, the semantic communication transmitter for a specific task is\ntypically pre-trained and shared by all users. However, due to user\nheterogeneity, it is desirable to use different transmitters according to the\navailable computational and communication resources of users. In this paper, we\nfirst show that it is possible to dynamically adjust the computational and\ncommunication overhead of DNN-based transmitters, thereby achieving adaptive\nsemantic communication. After that, we investigate the user association and\nresource allocation problem in a multi-cell network where users are equipped\nwith adaptive semantic communication transmitters. To solve this problem, we\ndecompose it into three subproblems involving the scheduling of each user, the\nresource allocation of each base station (BS), and the user association between\nusers and BSs. Then we solve each problem progressively based on the solution\nof the previous subproblem. The final algorithm can obtain near-optimal\nsolutions in polynomial time. Numerical results show that our algorithm\noutperforms benchmarks under various situations.",
            "author": [
                "Xingqiu He",
                "Chaoqun You",
                "Tony Q. S. Quek"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01049v1",
                "http://arxiv.org/pdf/2312.01049v1"
            ],
            "primary_category": "cs.NI",
            "category": [
                "cs.NI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01047v1",
            "title": "A New Random Reshuffling Method for Nonsmooth Nonconvex Finite-sum\n  Optimization",
            "updated": "2023-12-02T07:12:00Z",
            "published": "2023-12-02T07:12:00Z",
            "summary": "In this work, we propose and study a novel stochastic optimization algorithm,\ntermed the normal map-based proximal random reshuffling (norm-PRR) method, for\nnonsmooth nonconvex finite-sum problems. Random reshuffling techniques are\nprevalent and widely utilized in large-scale applications, e.g., in the\ntraining of neural networks. While the convergence behavior and advantageous\nacceleration effects of random reshuffling methods are fairly well understood\nin the smooth setting, much less seems to be known in the nonsmooth case and\nonly few proximal-type random reshuffling approaches with provable guarantees\nexist.\n  We establish the iteration complexity ${\\cal O}(n^{-1/3}T^{-2/3})$ for\nnorm-PRR, where $n$ is the number of component functions and $T$ counts the\ntotal number of iteration. We also provide novel asymptotic convergence results\nfor norm-PRR. Specifically, under the Kurdyka-{\\L}ojasiewicz (KL) inequality,\nwe establish strong limit-point convergence, i.e., the iterates generated by\nnorm-PRR converge to a single stationary point. Moreover, we derive last\niterate convergence rates of the form ${\\cal O}(k^{-p})$; here, $p \\in [0, 1]$\ndepends on the KL exponent $\\theta \\in [0,1)$ and step size dynamics. Finally,\nwe present preliminary numerical results on machine learning problems that\ndemonstrate the efficiency of the proposed method.",
            "author": [
                "Xiao Li",
                "Andre Milzarek",
                "Junwen Qiu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01047v1",
                "http://arxiv.org/pdf/2312.01047v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "cs.LG",
                "90C26, 90C15"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01046v1",
            "title": "Bagged Regularized $k$-Distances for Anomaly Detection",
            "updated": "2023-12-02T07:00:46Z",
            "published": "2023-12-02T07:00:46Z",
            "summary": "We consider the paradigm of unsupervised anomaly detection, which involves\nthe identification of anomalies within a dataset in the absence of labeled\nexamples. Though distance-based methods are top-performing for unsupervised\nanomaly detection, they suffer heavily from the sensitivity to the choice of\nthe number of the nearest neighbors. In this paper, we propose a new\ndistance-based algorithm called bagged regularized $k$-distances for anomaly\ndetection (BRDAD) converting the unsupervised anomaly detection problem into a\nconvex optimization problem. Our BRDAD algorithm selects the weights by\nminimizing the surrogate risk, i.e., the finite sample bound of the empirical\nrisk of the bagged weighted $k$-distances for density estimation (BWDDE). This\napproach enables us to successfully address the sensitivity challenge of the\nhyperparameter choice in distance-based algorithms. Moreover, when dealing with\nlarge-scale datasets, the efficiency issues can be addressed by the\nincorporated bagging technique in our BRDAD algorithm. On the theoretical side,\nwe establish fast convergence rates of the AUC regret of our algorithm and\ndemonstrate that the bagging technique significantly reduces the computational\ncomplexity. On the practical side, we conduct numerical experiments on anomaly\ndetection benchmarks to illustrate the insensitivity of parameter selection of\nour algorithm compared with other state-of-the-art distance-based methods.\nMoreover, promising improvements are brought by applying the bagging technique\nin our algorithm on real-world datasets.",
            "author": [
                "Yuchao Cai",
                "Yuheng Ma",
                "Hanfang Yang",
                "Hanyuan Hang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01046v1",
                "http://arxiv.org/pdf/2312.01046v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG",
                "math.ST",
                "stat.TH"
            ]
        }
    }
]