[
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c242438dac5aa4d9b13766c14240bb8426690d58",
            "@type": "ScholarlyArticle",
            "paperId": "c242438dac5aa4d9b13766c14240bb8426690d58",
            "corpusId": 54040953,
            "url": "https://www.semanticscholar.org/paper/c242438dac5aa4d9b13766c14240bb8426690d58",
            "title": "e-SNLI: Natural Language Inference with Natural Language Explanations",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2951936329",
                "DBLP": "conf/nips/CamburuRLB18",
                "ArXiv": "1812.01193",
                "CorpusId": 54040953
            },
            "abstract": "In order for machine learning to garner widespread public adoption, models must be able to provide interpretable and robust explanations for their decisions, as well as learn from human-provided explanations at train time. In this work, we extend the Stanford Natural Language Inference dataset with an additional layer of human-annotated natural language explanations of the entailment relations. We further implement models that incorporate these explanations into their training process and output them at test time. We show how our corpus of explanations, which we call e-SNLI, can be used for various goals, such as obtaining full sentence justifications of a model\u2019s decisions, improving universal sentence representations and transferring to out-of-domain NLI datasets. Our dataset thus opens up a range of research directions for using natural language explanations, both for improving models and for asserting their trust",
            "referenceCount": 27,
            "citationCount": 445,
            "influentialCitationCount": 111,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-12-04",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Camburu2018eSNLINL,\n author = {Oana-Maria Camburu and Tim Rockt\u00e4schel and Thomas Lukasiewicz and Phil Blunsom},\n booktitle = {Neural Information Processing Systems},\n pages = {9560-9572},\n title = {e-SNLI: Natural Language Inference with Natural Language Explanations},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4dde390436fe0f526a0dcfbc3b12a662d5f58b9f",
            "@type": "ScholarlyArticle",
            "paperId": "4dde390436fe0f526a0dcfbc3b12a662d5f58b9f",
            "corpusId": 20757419,
            "url": "https://www.semanticscholar.org/paper/4dde390436fe0f526a0dcfbc3b12a662d5f58b9f",
            "title": "Natural Language Processing to identify pneumonia from radiology reports",
            "venue": "Pharmacoepidemiology and Drug Safety",
            "publicationVenue": {
                "id": "urn:research:0f9e8bfc-fdfb-402f-bb26-c23177ec102a",
                "name": "Pharmacoepidemiology and Drug Safety",
                "alternate_names": [
                    "Pharmacoepidemiol Drug Saf"
                ],
                "issn": "1053-8569",
                "url": "http://onlinelibrary.wiley.com/journal/10.1002/(issn)1099-1557"
            },
            "year": 2013,
            "externalIds": {
                "MAG": "1560862568",
                "DOI": "10.1002/pds.3418",
                "CorpusId": 20757419,
                "PubMed": "23554109"
            },
            "abstract": "This study aimed to develop Natural Language Processing (NLP) approaches to supplement manual outcome validation, specifically to validate pneumonia cases from chest radiograph reports.",
            "referenceCount": 26,
            "citationCount": 61,
            "influentialCitationCount": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://europepmc.org/articles/pmc3811072?pdf=render",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Study",
                "JournalArticle"
            ],
            "publicationDate": "2013-08-01",
            "journal": {
                "name": "Pharmacoepidemiology and Drug Safety",
                "volume": "22"
            },
            "citationStyles": {
                "bibtex": "@Article{Dublin2013NaturalLP,\n author = {S. Dublin and E. Baldwin and Rod L. Walker and Lee M. Christensen and P. Haug and Michael L. Jackson and J. Nelson and Jeffrey P. Ferraro and D. Carrell and W. Chapman},\n booktitle = {Pharmacoepidemiology and Drug Safety},\n journal = {Pharmacoepidemiology and Drug Safety},\n title = {Natural Language Processing to identify pneumonia from radiology reports},\n volume = {22},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b1d2acf0702837ef20d9112847e2dffd46a25016",
            "@type": "ScholarlyArticle",
            "paperId": "b1d2acf0702837ef20d9112847e2dffd46a25016",
            "corpusId": 38805447,
            "url": "https://www.semanticscholar.org/paper/b1d2acf0702837ef20d9112847e2dffd46a25016",
            "title": "Natural language processing future",
            "venue": "2013 International Conference on Optical Imaging Sensor and Security (ICOSS)",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2013,
            "externalIds": {
                "MAG": "2126716737",
                "DOI": "10.1109/ICOISS.2013.6678407",
                "CorpusId": 38805447
            },
            "abstract": "Natural Language Processing is a technique where machine can become more human and thereby reducing the distance between human being and the machine can be reduced. Therefore in simple sense NLP makes human to communicate with the machine easily. There are many applications developed in past few decades in NLP. Most of these are very useful in everyday life for example a machine that takes instructions by voice. There are lots of research groups working on this topic to develop more practical are useful systems. Natural Language Processing holds great promise for making computer interfaces that are easier to use for people, since people will hopefully be able to talk to the computer in their own language, rather than learn a specialized language of computer commands. For programming, however, the necessity of a formal programming language for communicating with a computer has always been taken for granted. We would like to challenge this assumption. We believe that modern Natural Language Processing techniques can make possible the use of natural language to express programming ideas, thus drastically increasing the accessibility of programming to non-expert users. To demonstrate the feasibility of Natural Language Programming, this paper tackles what are perceived to be some of the hardest cases: steps and loops.",
            "referenceCount": 2,
            "citationCount": 37,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Conference"
            ],
            "publicationDate": "2013-07-02",
            "journal": {
                "name": "2013 International Conference on Optical Imaging Sensor and Security (ICOSS)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Conference{Surabhi2013NaturalLP,\n author = {M. Surabhi},\n booktitle = {2013 International Conference on Optical Imaging Sensor and Security (ICOSS)},\n journal = {2013 International Conference on Optical Imaging Sensor and Security (ICOSS)},\n pages = {1-3},\n title = {Natural language processing future},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3450225bde5cb3b3a23920258af38be0429ea347",
            "@type": "ScholarlyArticle",
            "paperId": "3450225bde5cb3b3a23920258af38be0429ea347",
            "corpusId": 7489171,
            "url": "https://www.semanticscholar.org/paper/3450225bde5cb3b3a23920258af38be0429ea347",
            "title": "Perspectives on crowdsourcing annotations for natural language processing",
            "venue": "Language Resources and Evaluation",
            "publicationVenue": {
                "id": "urn:research:7dda5bd1-752f-45e5-bc7b-09633096916e",
                "name": "Language Resources and Evaluation",
                "alternate_names": [
                    "Lang Resour Evaluation"
                ],
                "issn": "1574-020X",
                "url": "https://link.springer.com/journal/10579"
            },
            "year": 2013,
            "externalIds": {
                "MAG": "2056584528",
                "DBLP": "journals/lre/WangHK13",
                "DOI": "10.1007/s10579-012-9176-1",
                "CorpusId": 7489171
            },
            "abstract": null,
            "referenceCount": 71,
            "citationCount": 146,
            "influentialCitationCount": 9,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.comp.nus.edu.sg/%7Ekanmy/papers/Crowdsourcing.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2013-03-01",
            "journal": {
                "name": "Language Resources and Evaluation",
                "volume": "47"
            },
            "citationStyles": {
                "bibtex": "@Article{Wang2013PerspectivesOC,\n author = {Aobo Wang and Cong Duy Vu Hoang and Min-Yen Kan},\n booktitle = {Language Resources and Evaluation},\n journal = {Language Resources and Evaluation},\n pages = {9-31},\n title = {Perspectives on crowdsourcing annotations for natural language processing},\n volume = {47},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e7569d3899707930abcd4efff56f8a301ef7fed5",
            "@type": "ScholarlyArticle",
            "paperId": "e7569d3899707930abcd4efff56f8a301ef7fed5",
            "corpusId": 62451301,
            "url": "https://www.semanticscholar.org/paper/e7569d3899707930abcd4efff56f8a301ef7fed5",
            "title": "NATURAL LANGUAGE PROCESSING (NLP)",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2013,
            "externalIds": {
                "MAG": "2215005193",
                "CorpusId": 62451301
            },
            "abstract": "Natural Language Processing (NLP) is an area of research and application that explores how computers can be used to understand and manipulate natural language text or speech to do useful things. NLP researchers aim to gather knowledge on how human beings understand and use language so that appropriate tools \u00a0and techniques can be developed to make computer systems\u00a0 understand and manipulate natural languages to perform the desired tasks. Applications of NLP include a number of fields of studies, such as machine translation, natural language text processing and summarization, user interfaces, multilingual and cross language information retrieval (CUR), speech \u00a0recognition, \u00a0artificial intelligence and expert systems. Keyword:\u00a0 natural language processing",
            "referenceCount": 15,
            "citationCount": 26,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2013-04-16",
            "journal": {
                "name": "",
                "volume": "32"
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Fatkhurrozi2013NATURALLP,\n author = {B. Fatkhurrozi},\n title = {NATURAL LANGUAGE PROCESSING (NLP)},\n volume = {32},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4b2d04556f7d3377998f4f236b57086a1be74f23",
            "@type": "ScholarlyArticle",
            "paperId": "4b2d04556f7d3377998f4f236b57086a1be74f23",
            "corpusId": 227912089,
            "url": "https://www.semanticscholar.org/paper/4b2d04556f7d3377998f4f236b57086a1be74f23",
            "title": "A hierarchy of linguistic predictions during natural language comprehension",
            "venue": "bioRxiv",
            "publicationVenue": {
                "id": "urn:research:027ffd21-ebb0-4af8-baf5-911124292fd0",
                "name": "bioRxiv",
                "alternate_names": null,
                "issn": null,
                "url": "http://biorxiv.org/"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3108978252",
                "PubMedCentral": "9371745",
                "DOI": "10.1073/pnas.2201968119",
                "CorpusId": 227912089,
                "PubMed": "35921434"
            },
            "abstract": "Understanding spoken language requires transforming ambiguous acoustic streams into a hierarchy of representations, from phonemes to meaning. It has been suggested that the brain uses prediction to guide the interpretation of incoming input. However, the role of prediction in language processing remains disputed, with disagreement about both the ubiquity and representational nature of predictions. Here, we address both issues by analysing brain recordings of participants listening to audiobooks, and using a deep neural network (GPT-2) to precisely quantify contextual predictions. First, we establish that brain responses to words are modulated by ubiquitous, probabilistic predictions. Next, we disentangle model-based predictions into distinct dimensions, revealing dissociable signatures of syntactic, phonemic and semantic predictions. Finally, we show that high-level (word) predictions inform low-level (phoneme) predictions, supporting hierarchical predictive processing. Together, these results underscore the ubiquity of prediction in language processing, showing that the brain spontaneously predicts upcoming language at multiple levels of abstraction.",
            "referenceCount": 175,
            "citationCount": 115,
            "influentialCitationCount": 9,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Biology",
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Biology",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-12-03",
            "journal": {
                "name": "Proceedings of the National Academy of Sciences of the United States of America",
                "volume": "119"
            },
            "citationStyles": {
                "bibtex": "@Article{Heilbron2020AHO,\n author = {Micha Heilbron and K. Armeni and J. Schoffelen and P. Hagoort and F. D. de Lange},\n booktitle = {bioRxiv},\n journal = {Proceedings of the National Academy of Sciences of the United States of America},\n title = {A hierarchy of linguistic predictions during natural language comprehension},\n volume = {119},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1c6ae3f0e6f35d72f36832fe54d26383ddaf84c0",
            "@type": "ScholarlyArticle",
            "paperId": "1c6ae3f0e6f35d72f36832fe54d26383ddaf84c0",
            "corpusId": 219305634,
            "url": "https://www.semanticscholar.org/paper/1c6ae3f0e6f35d72f36832fe54d26383ddaf84c0",
            "title": "Stopwords in Technical Language Processing",
            "venue": "PLoS ONE",
            "publicationVenue": {
                "id": "urn:research:0aed7a40-85f3-4c66-9e1b-c1556c57001b",
                "name": "PLoS ONE",
                "alternate_names": [
                    "Plo ONE",
                    "PLOS ONE",
                    "PLO ONE"
                ],
                "issn": "1932-6203",
                "url": "https://journals.plos.org/plosone/"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "journals/corr/abs-2006-02633",
                "ArXiv": "2006.02633",
                "MAG": "3033185959",
                "DOI": "10.1371/journal.pone.0254937",
                "CorpusId": 219305634,
                "PubMed": "34351911"
            },
            "abstract": "There are increasing applications of natural language processing techniques for information retrieval, indexing, topic modelling and text classification in engineering contexts. A standard component of such tasks is the removal of stopwords, which are uninformative components of the data. While researchers use readily available stopwords lists that are derived from non-technical resources, the technical jargon of engineering fields contains their own highly frequent and uninformative words and there exists no standard stopwords list for technical language processing applications. Here we address this gap by rigorously identifying generic, insignificant, uninformative stopwords in engineering texts beyond the stopwords in general texts, based on the synthesis of alternative statistical measures such as term frequency, inverse document frequency, and entropy, and curating a stopwords dataset ready for technical language processing applications.",
            "referenceCount": 45,
            "citationCount": 61,
            "influentialCitationCount": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0254937&type=printable",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-06-04",
            "journal": {
                "name": "PloS one",
                "volume": "16 8"
            },
            "citationStyles": {
                "bibtex": "@Article{Sarica2020StopwordsIT,\n author = {Serhad Sarica and Jianxi Luo},\n booktitle = {PLoS ONE},\n journal = {PloS one},\n pages = {\n          e0254937\n        },\n title = {Stopwords in Technical Language Processing},\n volume = {16 8},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c75f3f2d199f115730a0ce2addcd38f7691e5b9b",
            "@type": "ScholarlyArticle",
            "paperId": "c75f3f2d199f115730a0ce2addcd38f7691e5b9b",
            "corpusId": 58270562,
            "url": "https://www.semanticscholar.org/paper/c75f3f2d199f115730a0ce2addcd38f7691e5b9b",
            "title": "Book Review: Natural Language Processing for Historical Texts by Michael Piotrowski",
            "venue": "International Conference on Computational Logic",
            "publicationVenue": {
                "id": "urn:research:30a8645d-22d4-42e2-b3f6-304bf4ce3a02",
                "name": "International Conference on Computational Logic",
                "alternate_names": [
                    "CL",
                    "Int Conf Comput Log"
                ],
                "issn": null,
                "url": null
            },
            "year": 2012,
            "externalIds": {
                "MAG": "2138238723",
                "ACL": "J14-1008",
                "DBLP": "series/synthesis/2012Piotrowski",
                "DOI": "10.1162/COLI_r_00180",
                "CorpusId": 58270562
            },
            "abstract": "More and more historical texts are becoming available in digital form. Digitization of paper documents is motivated by the aim of preserving cultural heritage and making it more accessible, both to laypeople and scholars. As digital images cannot be searched for text, digitization projects increasingly strive to create digital text, which can be searched and otherwise automatically processed, in addition to facsimiles. Indeed, the emerging field of digital humanities heavily relies on the availability of digital text for its studies. Together with the increasing availability of historical texts in digital form, there is a growing interest in applying natural language processing (NLP) methods and tools to historical texts. However, the specific linguistic properties of historical texts -- the lack of standardized orthography, in particular -- pose special challenges for NLP. This book aims to give an introduction to NLP for historical texts and an overview of the state of the art in this field. The book starts with an overview of methods for the acquisition of historical texts (scanning and OCR), discusses text encoding and annotation schemes, and presents examples of corpora of historical texts in a variety of languages. The book then discusses specific methods, such as creating part-of-speech taggers for historical languages or handling spelling variation. A final chapter analyzes the relationship between NLP and the digital humanities. Certain recently emerging textual genres, such as SMS, social media, and chat messages, or newsgroup and forum postings share a number of properties with historical texts, for example, nonstandard orthography and grammar, and profuse use of abbreviations. The methods and techniques required for the effective processing of historical texts are thus also of interest for research in other domains.",
            "referenceCount": 140,
            "citationCount": 160,
            "influentialCitationCount": 6,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/bfm:978-3-031-02146-6/1?pdf=chapter%20toc",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "History",
                "Art",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "History",
                    "source": "external"
                },
                {
                    "category": "Art",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "History",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2012-09-28",
            "journal": {
                "name": "Computational Linguistics",
                "volume": "40"
            },
            "citationStyles": {
                "bibtex": "@Article{Romary2012BookRN,\n author = {Laurent Romary},\n booktitle = {International Conference on Computational Logic},\n journal = {Computational Linguistics},\n pages = {231-233},\n title = {Book Review: Natural Language Processing for Historical Texts by Michael Piotrowski},\n volume = {40},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:bda86449d83359daa2dbbbcbd10a3a4ead538534",
            "@type": "ScholarlyArticle",
            "paperId": "bda86449d83359daa2dbbbcbd10a3a4ead538534",
            "corpusId": 17008709,
            "url": "https://www.semanticscholar.org/paper/bda86449d83359daa2dbbbcbd10a3a4ead538534",
            "title": "Natural language processing in an intelligent writing strategy tutoring system",
            "venue": "Behavior Research Methods",
            "publicationVenue": {
                "id": "urn:research:3418dc73-cff9-4be2-882e-5014f80264c1",
                "name": "Behavior Research Methods",
                "alternate_names": [
                    "Behav Res Method"
                ],
                "issn": "1554-351X",
                "url": null
            },
            "year": 2012,
            "externalIds": {
                "MAG": "1993306479",
                "DOI": "10.3758/s13428-012-0258-1",
                "CorpusId": 17008709,
                "PubMed": "23055164"
            },
            "abstract": null,
            "referenceCount": 53,
            "citationCount": 149,
            "influentialCitationCount": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.3758%2Fs13428-012-0258-1.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Education",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2012-10-06",
            "journal": {
                "name": "Behavior Research Methods",
                "volume": "45"
            },
            "citationStyles": {
                "bibtex": "@Article{McNamara2012NaturalLP,\n author = {D. McNamara and S. Crossley and Rod D. Roscoe},\n booktitle = {Behavior Research Methods},\n journal = {Behavior Research Methods},\n pages = {499 - 515},\n title = {Natural language processing in an intelligent writing strategy tutoring system},\n volume = {45},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6ed9417eaa7ee16f0563599829a061421a3e0563",
            "@type": "ScholarlyArticle",
            "paperId": "6ed9417eaa7ee16f0563599829a061421a3e0563",
            "corpusId": 1438450,
            "url": "https://www.semanticscholar.org/paper/6ed9417eaa7ee16f0563599829a061421a3e0563",
            "title": "NLTK: The Natural Language Toolkit",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                "name": "Annual Meeting of the Association for Computational Linguistics",
                "alternate_names": [
                    "Annu Meet Assoc Comput Linguistics",
                    "Meeting of the Association for Computational Linguistics",
                    "ACL",
                    "Meet Assoc Comput Linguistics"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/anthology/venues/acl/"
            },
            "year": 2006,
            "externalIds": {
                "DBLP": "conf/acl/Bird06",
                "ACL": "P06-4018",
                "DOI": "10.3115/1225403.1225421",
                "CorpusId": 1438450
            },
            "abstract": "The Natural Language Toolkit is a suite of program modules, data sets and tutorials supporting research and teaching in computational linguistics and natural language processing. NLTK is written in Python and distributed under the GPL open source license. Over the past year the toolkit has been rewritten, simplifying many linguistic data structures and taking advantage of recent enhancements in the Python language. This paper reports on the simplified toolkit and explains how it is used in teaching NLP.",
            "referenceCount": 7,
            "citationCount": 4175,
            "influentialCitationCount": 315,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://dl.acm.org/ft_gateway.cfm?id=1225421&type=pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference",
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Bird2006NLTKTN,\n author = {Steven Bird},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n title = {NLTK: The Natural Language Toolkit},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6fb14c13d635f26ce15b3eb5f78d5d535cfebeee",
            "@type": "ScholarlyArticle",
            "paperId": "6fb14c13d635f26ce15b3eb5f78d5d535cfebeee",
            "corpusId": 6494633,
            "url": "https://www.semanticscholar.org/paper/6fb14c13d635f26ce15b3eb5f78d5d535cfebeee",
            "title": "A Tutorial on Dual Decomposition and Lagrangian Relaxation for Inference in Natural Language Processing",
            "venue": "Journal of Artificial Intelligence Research",
            "publicationVenue": {
                "id": "urn:research:aef12dca-60a0-4ca3-819b-cad26d309d4e",
                "name": "Journal of Artificial Intelligence Research",
                "alternate_names": [
                    "JAIR",
                    "J Artif Intell Res",
                    "The Journal of Artificial Intelligence Research"
                ],
                "issn": "1076-9757",
                "url": "http://www.jair.org/"
            },
            "year": 2012,
            "externalIds": {
                "MAG": "2132678463",
                "DBLP": "journals/jair/RushC12",
                "ArXiv": "1405.5208",
                "DOI": "10.1613/jair.3680",
                "CorpusId": 6494633
            },
            "abstract": "Dual decomposition, and more generally Lagrangian relaxation, is a classical method for combinatorial optimization; it has recently been applied to several inference problems in natural language processing (NLP). This tutorial gives an overview of the technique. We describe example algorithms, describe formal guarantees for the method, and describe practical issues in implementing the algorithms. While our examples are predominantly drawn from the NLP literature, the material should be of general relevance to inference problems in machine learning. A central theme of this tutorial is that Lagrangian relaxation is naturally applied in conjunction with a broad class of combinatorial algorithms, allowing inference in models that go significantly beyond previous work on Lagrangian relaxation for inference in graphical models.\n",
            "referenceCount": 62,
            "citationCount": 117,
            "influentialCitationCount": 10,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://jair.org/index.php/jair/article/download/10785/25750",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2012-09-01",
            "journal": {
                "name": "J. Artif. Intell. Res.",
                "volume": "45"
            },
            "citationStyles": {
                "bibtex": "@Article{Rush2012ATO,\n author = {Alexander M. Rush and Michael Collins},\n booktitle = {Journal of Artificial Intelligence Research},\n journal = {J. Artif. Intell. Res.},\n pages = {305-362},\n title = {A Tutorial on Dual Decomposition and Lagrangian Relaxation for Inference in Natural Language Processing},\n volume = {45},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c051069850f6825178bb0c4ca6b23c44ecbc94bf",
            "@type": "ScholarlyArticle",
            "paperId": "c051069850f6825178bb0c4ca6b23c44ecbc94bf",
            "corpusId": 11965490,
            "url": "https://www.semanticscholar.org/paper/c051069850f6825178bb0c4ca6b23c44ecbc94bf",
            "title": "Using natural language processing technology for qualitative data analysis",
            "venue": "International Journal of Social Research Methodology",
            "publicationVenue": {
                "id": "urn:research:5aee8275-5126-4591-8dec-558219db9cce",
                "name": "International Journal of Social Research Methodology",
                "alternate_names": [
                    "Int J Soc Res Methodol"
                ],
                "issn": "1364-5579",
                "url": "http://www.tandfonline.com/loi/tsrm20"
            },
            "year": 2012,
            "externalIds": {
                "MAG": "1981740417",
                "DOI": "10.1080/13645579.2011.625764",
                "CorpusId": 11965490
            },
            "abstract": "Social researchers often apply qualitative research methods to study groups and their communications artifacts. The use of computer-mediated communications has dramatically increased the volume of text available, but coding such text requires considerable manual effort. We discuss how systems that process text in human languages (i.e. natural language processing [NLP]) might partially automate content analysis by extracting theoretical evidence. We present a case study of the use of NLP for qualitative analysis in which the NLP rules showed good performance on a number of codes. With the current level of performance, use of an NLP system could reduce the amount of text to be examined by a human coder by an order of magnitude or more, potentially increasing the speed of coding by a comparable degree. The paper is significant as it is one of the first to demonstrate the use of high-level NLP techniques for qualitative data analysis.",
            "referenceCount": 39,
            "citationCount": 97,
            "influentialCitationCount": 7,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Sociology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2012-10-17",
            "journal": {
                "name": "International Journal of Social Research Methodology",
                "volume": "15"
            },
            "citationStyles": {
                "bibtex": "@Article{Crowston2012UsingNL,\n author = {Kevin Crowston and Eileen Allen and Robert Heckman},\n booktitle = {International Journal of Social Research Methodology},\n journal = {International Journal of Social Research Methodology},\n pages = {523 - 543},\n title = {Using natural language processing technology for qualitative data analysis},\n volume = {15},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5a52857892e15cf4bb0ae2ee452db0c697206af0",
            "@type": "ScholarlyArticle",
            "paperId": "5a52857892e15cf4bb0ae2ee452db0c697206af0",
            "corpusId": 2271082,
            "url": "https://www.semanticscholar.org/paper/5a52857892e15cf4bb0ae2ee452db0c697206af0",
            "title": "A corpus of full-text journal articles is a robust evaluation tool for revealing differences in performance of biomedical natural language processing tools",
            "venue": "BMC Bioinformatics",
            "publicationVenue": {
                "id": "urn:research:be3f884c-b44a-496a-a593-1cad3f89d254",
                "name": "BMC Bioinformatics",
                "alternate_names": [
                    "BMC Bioinform"
                ],
                "issn": "1471-2105",
                "url": "http://www.biomedcentral.com/bmcbioinformatics"
            },
            "year": 2012,
            "externalIds": {
                "PubMedCentral": "3483229",
                "DBLP": "journals/bmcbi/VerspoorCLWJRCFMEXBBPH12",
                "MAG": "2055032581",
                "DOI": "10.1186/1471-2105-13-207",
                "CorpusId": 2271082,
                "PubMed": "22901054"
            },
            "abstract": null,
            "referenceCount": 77,
            "citationCount": 123,
            "influentialCitationCount": 8,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://bmcbioinformatics.biomedcentral.com/counter/pdf/10.1186/1471-2105-13-207",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2012-08-17",
            "journal": {
                "name": "BMC Bioinformatics",
                "volume": "13"
            },
            "citationStyles": {
                "bibtex": "@Article{Verspoor2012ACO,\n author = {Karin M. Verspoor and K. Cohen and Arrick Lanfranchi and Colin Warner and Helen L. Johnson and Christophe Roeder and Jinho D. Choi and Christopher S. Funk and Yuriy Malenkiy and Miriam Eckert and Nianwen Xue and W. Baumgartner and M. Bada and Martha Palmer and L. Hunter},\n booktitle = {BMC Bioinformatics},\n journal = {BMC Bioinformatics},\n pages = {207 - 207},\n title = {A corpus of full-text journal articles is a robust evaluation tool for revealing differences in performance of biomedical natural language processing tools},\n volume = {13},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2a733deaa7c55e427337860c7643e6b5f8b750fc",
            "@type": "ScholarlyArticle",
            "paperId": "2a733deaa7c55e427337860c7643e6b5f8b750fc",
            "corpusId": 62391688,
            "url": "https://www.semanticscholar.org/paper/2a733deaa7c55e427337860c7643e6b5f8b750fc",
            "title": "Book Reviews: Introduction to Arabic Natural Language Processing by Nizar Y. Habash",
            "venue": "International Conference on Computational Logic",
            "publicationVenue": {
                "id": "urn:research:30a8645d-22d4-42e2-b3f6-304bf4ce3a02",
                "name": "International Conference on Computational Logic",
                "alternate_names": [
                    "CL",
                    "Int Conf Comput Log"
                ],
                "issn": null,
                "url": null
            },
            "year": 2010,
            "externalIds": {
                "MAG": "2147272182",
                "DBLP": "journals/coling/Zitouni11",
                "ACL": "J11-3008",
                "DOI": "10.1162/COLI_r_00066",
                "CorpusId": 62391688
            },
            "abstract": "Abstract This book provides system developers and researchers in natural language processing and computational linguistics with the necessary background information for working with the Arabic language. The goal is to introduce Arabic linguistic phenomena and review the state-of-the-art in Arabic processing. The book discusses Arabic script, phonology, orthography, morphology, syntax and semantics, with a final chapter on machine translation issues. The chapter sizes correspond more or less to what is linguistically distinctive about Arabic, with morphology getting the lion's share, followed by Arabic script. No previous knowledge of Arabic is needed. This book is designed for computer scientists and linguists alike. The focus of the book is on Modern Standard Arabic; however, notes on practical issues related to Arabic dialects and languages written in the Arabic script are presented in different chapters. Table of Contents: What is \"Arabic\"? / Arabic Script / Arabic Phonology and Orthography / Arabic Mo...",
            "referenceCount": 227,
            "citationCount": 707,
            "influentialCitationCount": 65,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/bfm:978-3-031-02139-8/1?pdf=chapter%20toc",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "History",
                "Computer Science",
                "Art"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "History",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Art",
                    "source": "external"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2010-08-30",
            "journal": {
                "name": "Computational Linguistics",
                "volume": "37"
            },
            "citationStyles": {
                "bibtex": "@Article{Habash2010BookRI,\n author = {Nizar Habash},\n booktitle = {International Conference on Computational Logic},\n journal = {Computational Linguistics},\n pages = {623-625},\n title = {Book Reviews: Introduction to Arabic Natural Language Processing by Nizar Y. Habash},\n volume = {37},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:45048eee994bffe8915e905ab5c24e34512fb4a1",
            "@type": "ScholarlyArticle",
            "paperId": "45048eee994bffe8915e905ab5c24e34512fb4a1",
            "corpusId": 7447565,
            "url": "https://www.semanticscholar.org/paper/45048eee994bffe8915e905ab5c24e34512fb4a1",
            "title": "The Application of Natural Language Processing to Augmentative and Alternative Communication",
            "venue": "Assistive technology",
            "publicationVenue": {
                "id": "urn:research:b3f91fe8-46bd-490a-88e1-42d35ac2afa1",
                "name": "Assistive technology",
                "alternate_names": [
                    "Assist technol",
                    "Assistive Technology",
                    "Assist Technol"
                ],
                "issn": "1040-0435",
                "url": "http://www.informaworld.com/uaty"
            },
            "year": 2012,
            "externalIds": {
                "MAG": "2135620230",
                "DOI": "10.1080/10400435.2011.648714",
                "CorpusId": 7447565,
                "PubMed": "22590796"
            },
            "abstract": "ABSTRACT Significant progress has been made in the application of natural language processing (NLP) to augmentative and alternative communication (AAC), particularly in the areas of interface design and word prediction. This article will survey the current state-of-the-science of NLP in AAC and discuss its future applications for the development of next generation of AAC technology.",
            "referenceCount": 54,
            "citationCount": 78,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2012-03-01",
            "journal": {
                "name": "Assistive Technology",
                "volume": "24"
            },
            "citationStyles": {
                "bibtex": "@Article{Higginbotham2012TheAO,\n author = {D. Higginbotham and G. Lesher and Bryan J. Moulton and Brian Roark},\n booktitle = {Assistive technology},\n journal = {Assistive Technology},\n pages = {14 - 24},\n title = {The Application of Natural Language Processing to Augmentative and Alternative Communication},\n volume = {24},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c78244c1e0606ad606c644c7f872e8a4f905a43a",
            "@type": "ScholarlyArticle",
            "paperId": "c78244c1e0606ad606c644c7f872e8a4f905a43a",
            "corpusId": 16535832,
            "url": "https://www.semanticscholar.org/paper/c78244c1e0606ad606c644c7f872e8a4f905a43a",
            "title": "Assisted Behavior Driven Development Using Natural Language Processing",
            "venue": "International Conference on Software Technology: Methods and Tools",
            "publicationVenue": {
                "id": "urn:research:4a63d235-b8de-41aa-b5f7-b28efb66f736",
                "name": "International Conference on Software Technology: Methods and Tools",
                "alternate_names": [
                    "Technology of Object-Oriented Languages and Systems",
                    "TOOLS",
                    "Technol Object-oriented Lang Syst",
                    "Int Conf Softw Technol Method Tool"
                ],
                "issn": null,
                "url": "https://web.archive.org/web/*/http://tools.ethz.ch/"
            },
            "year": 2012,
            "externalIds": {
                "MAG": "1511212867",
                "DBLP": "conf/tools/SoekenWD12",
                "DOI": "10.1007/978-3-642-30561-0_19",
                "CorpusId": 16535832
            },
            "abstract": null,
            "referenceCount": 20,
            "citationCount": 92,
            "influentialCitationCount": 5,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2012-05-29",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Soeken2012AssistedBD,\n author = {Mathias Soeken and R. Wille and R. Drechsler},\n booktitle = {International Conference on Software Technology: Methods and Tools},\n pages = {269-287},\n title = {Assisted Behavior Driven Development Using Natural Language Processing},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1f2beca46dd33792af6d6ce4e8824f2d989a5488",
            "@type": "ScholarlyArticle",
            "paperId": "1f2beca46dd33792af6d6ce4e8824f2d989a5488",
            "corpusId": 3370682,
            "url": "https://www.semanticscholar.org/paper/1f2beca46dd33792af6d6ce4e8824f2d989a5488",
            "title": "The feasibility of using natural language processing to extract clinical information from breast pathology reports",
            "venue": "Journal of Pathology Informatics",
            "publicationVenue": {
                "id": "urn:research:fbb118d8-a37f-443c-98c6-06b1bd25f9f6",
                "name": "Journal of Pathology Informatics",
                "alternate_names": [
                    "J Pathol Informatics"
                ],
                "issn": "2229-5089",
                "url": "http://www.jpathinformatics.org/"
            },
            "year": 2012,
            "externalIds": {
                "MAG": "1982938880",
                "PubMedCentral": "3424662",
                "DOI": "10.4103/2153-3539.97788",
                "CorpusId": 3370682,
                "PubMed": "22934236"
            },
            "abstract": "Objective: The opportunity to integrate clinical decision support systems into clinical practice is limited due to the lack of structured, machine readable data in the current format of the electronic health record. Natural language processing has been designed to convert free text into machine readable data. The aim of the current study was to ascertain the feasibility of using natural language processing to extract clinical information from >76,000 breast pathology reports. Approach and Procedure: Breast pathology reports from three institutions were analyzed using natural language processing software (Clearforest, Waltham, MA) to extract information on a variety of pathologic diagnoses of interest. Data tables were created from the extracted information according to date of surgery, side of surgery, and medical record number. The variety of ways in which each diagnosis could be represented was recorded, as a means of demonstrating the complexity of machine interpretation of free text. Results: There was widespread variation in how pathologists reported common pathologic diagnoses. We report, for example, 124 ways of saying invasive ductal carcinoma and 95 ways of saying invasive lobular carcinoma. There were >4000 ways of saying invasive ductal carcinoma was not present. Natural language processor sensitivity and specificity were 99.1% and 96.5% when compared to expert human coders. Conclusion: We have demonstrated how a large body of free text medical information such as seen in breast pathology reports, can be converted to a machine readable format using natural language processing, and described the inherent complexities of the task.",
            "referenceCount": 3,
            "citationCount": 91,
            "influentialCitationCount": 1,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2012-01-01",
            "journal": {
                "name": "Journal of Pathology Informatics",
                "volume": "3"
            },
            "citationStyles": {
                "bibtex": "@Article{Buckley2012TheFO,\n author = {J. Buckley and S. Coopey and J. Sharko and Fernanda C. G. Polubriaginof and B. Drohan and A. Belli and Elizabeth M. H. Kim and J. Garber and Barbara L. Smith and M. Gadd and M. Specht and C. Roche and T. Gudewicz and K. Hughes},\n booktitle = {Journal of Pathology Informatics},\n journal = {Journal of Pathology Informatics},\n title = {The feasibility of using natural language processing to extract clinical information from breast pathology reports},\n volume = {3},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e9a53649f7d210ec28f980f148dbb42cd4deacb4",
            "@type": "ScholarlyArticle",
            "paperId": "e9a53649f7d210ec28f980f148dbb42cd4deacb4",
            "corpusId": 2192224,
            "url": "https://www.semanticscholar.org/paper/e9a53649f7d210ec28f980f148dbb42cd4deacb4",
            "title": "Applications of Natural Language Processing in Biodiversity Science",
            "venue": "Adv. Bioinformatics",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2012,
            "externalIds": {
                "PubMedCentral": "3364545",
                "DBLP": "journals/abi/ThessenCM12",
                "MAG": "1987248861",
                "DOI": "10.1155/2012/391574",
                "CorpusId": 2192224,
                "PubMed": "22685456"
            },
            "abstract": "Centuries of biological knowledge are contained in the massive body of scientific literature, written for human-readability but too big for any one person to consume. Large-scale mining of information from the literature is necessary if biology is to transform into a data-driven science. A computer can handle the volume but cannot make sense of the language. This paper reviews and discusses the use of natural language processing (NLP) and machine-learning algorithms to extract information from systematic literature. NLP algorithms have been used for decades, but require special development for application in the biological realm due to the special nature of the language. Many tools exist for biological information extraction (cellular processes, taxonomic names, and morphological characters), but none have been applied life wide and most still require testing and development. Progress has been made in developing algorithms for automated annotation of taxonomic text, identification of taxonomic names in text, and extraction of morphological character information from taxonomic descriptions. This manuscript will briefly discuss the key steps in applying information extraction tools to enhance biodiversity science.",
            "referenceCount": 82,
            "citationCount": 78,
            "influentialCitationCount": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://downloads.hindawi.com/archive/2012/391574.pdf",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Environmental Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2012-05-22",
            "journal": {
                "name": "Advances in Bioinformatics",
                "volume": "2012"
            },
            "citationStyles": {
                "bibtex": "@Article{Thessen2012ApplicationsON,\n author = {A. Thessen and Hong Cui and D. Mozzherin},\n booktitle = {Adv. Bioinformatics},\n journal = {Advances in Bioinformatics},\n title = {Applications of Natural Language Processing in Biodiversity Science},\n volume = {2012},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:595b1c086f9a1cfffeef2870445364b2cbcb399d",
            "@type": "ScholarlyArticle",
            "paperId": "595b1c086f9a1cfffeef2870445364b2cbcb399d",
            "corpusId": 9295759,
            "url": "https://www.semanticscholar.org/paper/595b1c086f9a1cfffeef2870445364b2cbcb399d",
            "title": "Crowdsourcing research opportunities: lessons from natural language processing",
            "venue": "International Conference on Knowledge Management and Knowledge Technologies",
            "publicationVenue": {
                "id": "urn:research:19373198-f7c0-43a3-a956-0c59a7582f03",
                "name": "International Conference on Knowledge Management and Knowledge Technologies",
                "alternate_names": [
                    "I-KNOW",
                    "Int Conf Knowl Manag Knowl Technol"
                ],
                "issn": null,
                "url": "http://www.wikicfp.com/cfp/program?id=1575"
            },
            "year": 2012,
            "externalIds": {
                "DBLP": "conf/iknow/SabouBS12",
                "MAG": "1988584482",
                "DOI": "10.1145/2362456.2362479",
                "CorpusId": 9295759
            },
            "abstract": "Although the field has led to promising early results, the use of crowdsourcing as an integral part of science projects is still regarded with skepticism by some, largely due to a lack of awareness of the opportunities and implications of utilizing these new techniques. We address this lack of awareness, firstly by highlighting the positive impacts that crowdsourcing has had on Natural Language Processing research. Secondly, we discuss the challenges of more complex methodologies, quality control, and the necessity to deal with ethical issues. We conclude with future trends and opportunities of crowdsourcing for science, including its potential for disseminating results, making science more accessible, and enriching educational programs.",
            "referenceCount": 52,
            "citationCount": 90,
            "influentialCitationCount": 2,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2012-09-05",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Sabou2012CrowdsourcingRO,\n author = {M. Sabou and Kalina Bontcheva and A. Scharl},\n booktitle = {International Conference on Knowledge Management and Knowledge Technologies},\n pages = {17},\n title = {Crowdsourcing research opportunities: lessons from natural language processing},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e45fa1aa940333d566ae230604a80b7ce90172dc",
            "@type": "ScholarlyArticle",
            "paperId": "e45fa1aa940333d566ae230604a80b7ce90172dc",
            "corpusId": 624982,
            "url": "https://www.semanticscholar.org/paper/e45fa1aa940333d566ae230604a80b7ce90172dc",
            "title": "Natural Language Processing and Language Learning",
            "venue": "The Encyclopedia of Applied Linguistics",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2012,
            "externalIds": {
                "MAG": "1497861021",
                "DOI": "10.1002/9781405198431.WBEAL0858",
                "CorpusId": 624982
            },
            "abstract": "As a relatively young field of research and development that began with work on crypt-analysis and machine translation around 50 years ago, natural language processing (NLP) is concerned with the automated processing of human language. \n \n \nKeywords: \n \nCALL; \ncomputational linguistics; \ncorpora; \nICALL; \nnatural language processing",
            "referenceCount": 130,
            "citationCount": 44,
            "influentialCitationCount": 5,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2012-11-05",
            "journal": {
                "name": "The Encyclopedia of Applied Linguistics",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Meurers2012NaturalLP,\n author = {Walt Detmar Meurers},\n booktitle = {The Encyclopedia of Applied Linguistics},\n journal = {The Encyclopedia of Applied Linguistics},\n title = {Natural Language Processing and Language Learning},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:355e815f5d7225d90f7a702f68539a6fca4b1f80",
            "@type": "ScholarlyArticle",
            "paperId": "355e815f5d7225d90f7a702f68539a6fca4b1f80",
            "corpusId": 63425539,
            "url": "https://www.semanticscholar.org/paper/355e815f5d7225d90f7a702f68539a6fca4b1f80",
            "title": "Emerging Applications of Natural Language Processing: Concepts and New Research",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2012,
            "externalIds": {
                "MAG": "2506027861",
                "DOI": "10.4018/978-1-4666-2169-5",
                "CorpusId": 63425539
            },
            "abstract": "Over the last few years, the area of Natural Language Processing has drastically grown in recognition, not only within the research and development community, but also with industry professionals. As NLP continues to be discussed and researched, certain areas continue to grow and mature. As a result, the need for advanced research and information is in high demand.Emerging Applications of Natural Language Processing: Concepts and New Research provides pertinent and vital information that researchers, postgraduate, doctoral students, and practitioners are seeking. This book compiles the latest discoveries and advances in NLP methodologies and applications while expanding upon various topics regarding the future of NLP.",
            "referenceCount": 0,
            "citationCount": 59,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2012-10-31",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Bandyopadhyay2012EmergingAO,\n author = {Sivaji Bandyopadhyay and S. Naskar and Asif Ekbal},\n title = {Emerging Applications of Natural Language Processing: Concepts and New Research},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:bf818b434edff9299374b079228c518e34fcd61a",
            "@type": "ScholarlyArticle",
            "paperId": "bf818b434edff9299374b079228c518e34fcd61a",
            "corpusId": 22864696,
            "url": "https://www.semanticscholar.org/paper/bf818b434edff9299374b079228c518e34fcd61a",
            "title": "Roget's Thesaurus as a Lexical Resource for Natural Language Processing",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2012,
            "externalIds": {
                "MAG": "1782572861",
                "DBLP": "journals/corr/abs-1204-0140",
                "ArXiv": "1204.0140",
                "DOI": "10.20381/RUOR-18213",
                "CorpusId": 22864696
            },
            "abstract": "WordNet proved that it is possible to construct a large-scale electronic lexical database on the principles of lexical semantics. It has been accepted and used extensively by computational linguists ever since it was released. Inspired by WordNet's success, we propose as an alternative a similar resource, based on the 1987 Penguin edition of Roget's Thesaurus of English Words and Phrases. \nPeter Mark Roget published his first Thesaurus over 150 years ago. Countless writers, orators and students of the English language have used it. Computational linguists have employed Roget's for almost 50 years in Natural Language Processing, however hesitated in accepting Roget's Thesaurus because a proper machine tractable version was not available. \nThis dissertation presents an implementation of a machine-tractable version of the 1987 Penguin edition of Roget's Thesaurus - the first implementation of its kind to use an entire current edition. It explains the steps necessary for taking a machine-readable file and transforming it into a tractable system. This involves converting the lexical material into a format that can be more easily exploited, identifying data structures and designing classes to computerize the Thesaurus. Roget's organization is studied in detail and contrasted with WordNet's. \nWe show two applications of the computerized Thesaurus: computing semantic similarity between words and phrases, and building lexical chains in a text. The experiments are performed using well-known benchmarks and the results are compared to those of other systems that use Roget's, WordNet and statistical techniques. Roget's has turned out to be an excellent resource for measuring semantic similarity; lexical chains are easily built but more difficult to evaluate. We also explain ways in which Roget's Thesaurus and WordNet can be combined.",
            "referenceCount": 82,
            "citationCount": 57,
            "influentialCitationCount": 4,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2012-03-31",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1204.0140"
            },
            "citationStyles": {
                "bibtex": "@Article{Jarmasz2012RogetsTA,\n author = {M. Jarmasz},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Roget's Thesaurus as a Lexical Resource for Natural Language Processing},\n volume = {abs/1204.0140},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:668f42a4d4094f0a66d402a16087e14269b31a1f",
            "@type": "ScholarlyArticle",
            "paperId": "668f42a4d4094f0a66d402a16087e14269b31a1f",
            "corpusId": 56657817,
            "url": "https://www.semanticscholar.org/paper/668f42a4d4094f0a66d402a16087e14269b31a1f",
            "title": "Analysis Methods in Neural Language Processing: A Survey",
            "venue": "Transactions of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:e0dbf116-86aa-418d-859f-a49952d7e44a",
                "name": "Transactions of the Association for Computational Linguistics",
                "alternate_names": [
                    "Trans Assoc Comput Linguistics",
                    "TACL"
                ],
                "issn": "2307-387X",
                "url": "https://www.mitpressjournals.org/loi/tacl"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/corr/abs-1812-08951",
                "ACL": "Q19-1004",
                "MAG": "2954730351",
                "ArXiv": "1812.08951",
                "DOI": "10.1162/tacl_a_00254",
                "CorpusId": 56657817
            },
            "abstract": "The field of natural language processing has seen impressive progress in recent years, with neural network models replacing many of the traditional systems. A plethora of new models have been proposed, many of which are thought to be opaque compared to their feature-rich counterparts. This has led researchers to analyze, interpret, and evaluate neural networks in novel and more fine-grained ways. In this survey paper, we review analysis methods in neural language processing, categorize them according to prominent research trends, highlight existing limitations, and point to potential directions for future work.",
            "referenceCount": 203,
            "citationCount": 441,
            "influentialCitationCount": 23,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00254",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2018-12-21",
            "journal": {
                "name": "Transactions of the Association for Computational Linguistics",
                "volume": "7"
            },
            "citationStyles": {
                "bibtex": "@Article{Belinkov2018AnalysisMI,\n author = {Yonatan Belinkov and James R. Glass},\n booktitle = {Transactions of the Association for Computational Linguistics},\n journal = {Transactions of the Association for Computational Linguistics},\n pages = {49-72},\n title = {Analysis Methods in Neural Language Processing: A Survey},\n volume = {7},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:11eaa4f1cba9281ecbc1ac44a6b3ba5817bf1a25",
            "@type": "ScholarlyArticle",
            "paperId": "11eaa4f1cba9281ecbc1ac44a6b3ba5817bf1a25",
            "corpusId": 4612975,
            "url": "https://www.semanticscholar.org/paper/11eaa4f1cba9281ecbc1ac44a6b3ba5817bf1a25",
            "title": "T-REx: A Large Scale Alignment of Natural Language with Knowledge Base Triples",
            "venue": "International Conference on Language Resources and Evaluation",
            "publicationVenue": {
                "id": "urn:research:7474c4a0-75d9-4105-9809-8e7d5201c5e1",
                "name": "International Conference on Language Resources and Evaluation",
                "alternate_names": [
                    "LREC",
                    "Int Conf Lang Resour Evaluation"
                ],
                "issn": null,
                "url": "http://www.lrec-conf.org/"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "conf/lrec/ElSaharVRGHLS18",
                "ACL": "L18-1544",
                "MAG": "2785611959",
                "CorpusId": 4612975
            },
            "abstract": "Alignments between natural language and Knowledge Base (KB) triples are an essential prerequisite for training machine learning approaches employed in a variety of Natural Language Processing problems. These include Relation Extraction, KB Population, Question Answering and Natural Language Generation from KB triples. Available datasets that provide those alignments are plagued by significant shortcomings \u2013 they are of limited size, they exhibit a restricted predicate coverage, and/or they are of unreported quality. To alleviate these shortcomings, we present T-REx, a dataset of large scale alignments between Wikipedia abstracts and Wikidata triples. T-REx consists of 11 million triples aligned with 3.09 million Wikipedia abstracts (6.2 million sentences). T-REx is two orders of magnitude larger than the largest available alignments dataset and covers 2.5 times more predicates. Additionally, we stress the quality of this language resource thanks to an extensive crowdsourcing evaluation. T-REx is publicly available at: https://w3id.org/t-rex.",
            "referenceCount": 18,
            "citationCount": 218,
            "influentialCitationCount": 54,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-05-07",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Article{ElSahar2018TRExAL,\n author = {Hady ElSahar and P. Vougiouklis and Arslen Remaci and C. Gravier and Jonathon S. Hare and F. Laforest and E. Simperl},\n booktitle = {International Conference on Language Resources and Evaluation},\n title = {T-REx: A Large Scale Alignment of Natural Language with Knowledge Base Triples},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2cd8e8f510c89c7c18268e8ad51c061e459ad321",
            "@type": "ScholarlyArticle",
            "paperId": "2cd8e8f510c89c7c18268e8ad51c061e459ad321",
            "corpusId": 8495258,
            "url": "https://www.semanticscholar.org/paper/2cd8e8f510c89c7c18268e8ad51c061e459ad321",
            "title": "A Decomposable Attention Model for Natural Language Inference",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "publicationVenue": {
                "id": "urn:research:41bf9ed3-85b3-4c90-b015-150e31690253",
                "name": "Conference on Empirical Methods in Natural Language Processing",
                "alternate_names": [
                    "Empir Method Nat Lang Process",
                    "Empirical Methods in Natural Language Processing",
                    "Conf Empir Method Nat Lang Process",
                    "EMNLP"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/emnlp"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "journals/corr/ParikhT0U16",
                "ACL": "D16-1244",
                "MAG": "2413794162",
                "ArXiv": "1606.01933",
                "DOI": "10.18653/v1/D16-1244",
                "CorpusId": 8495258
            },
            "abstract": "We propose a simple neural architecture for natural language inference. Our approach uses attention to decompose the problem into subproblems that can be solved separately, thus making it trivially parallelizable. On the Stanford Natural Language Inference (SNLI) dataset, we obtain state-of-the-art results with almost an order of magnitude fewer parameters than previous work and without relying on any word-order information. Adding intra-sentence attention that takes a minimum amount of order into account yields further improvements.",
            "referenceCount": 31,
            "citationCount": 1259,
            "influentialCitationCount": 143,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/D16-1244.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-06-06",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1606.01933"
            },
            "citationStyles": {
                "bibtex": "@Article{Parikh2016ADA,\n author = {Ankur P. Parikh and Oscar T\u00e4ckstr\u00f6m and Dipanjan Das and Jakob Uszkoreit},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n journal = {ArXiv},\n title = {A Decomposable Attention Model for Natural Language Inference},\n volume = {abs/1606.01933},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7dc156eb9d84ae8fd521ecac5ccc5b5426a42b50",
            "@type": "ScholarlyArticle",
            "paperId": "7dc156eb9d84ae8fd521ecac5ccc5b5426a42b50",
            "corpusId": 182952502,
            "url": "https://www.semanticscholar.org/paper/7dc156eb9d84ae8fd521ecac5ccc5b5426a42b50",
            "title": "A Survey of Reinforcement Learning Informed by Natural Language",
            "venue": "International Joint Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:67f7f831-711a-43c8-8785-1e09005359b5",
                "name": "International Joint Conference on Artificial Intelligence",
                "alternate_names": [
                    "Int Jt Conf Artif Intell",
                    "IJCAI"
                ],
                "issn": null,
                "url": "http://www.ijcai.org/"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "conf/ijcai/LuketinaNFFAGWR19",
                "MAG": "2948380112",
                "ArXiv": "1906.03926",
                "DOI": "10.24963/ijcai.2019/880",
                "CorpusId": 182952502
            },
            "abstract": "To be successful in real-world tasks, Reinforcement Learning (RL) needs to exploit the compositional, relational, and hierarchical structure of the world, and learn to transfer it to the task at hand. Recent advances in representation learning for language make it possible to build models that acquire world knowledge from text corpora and integrate this knowledge into downstream decision making problems. We thus argue that the time is right to investigate a tight integration of natural language understanding into RL in particular. We survey the state of the field, including work on instruction following, text games, and learning from textual domain knowledge. Finally, we call for the development of new environments as well as further investigation into the potential uses of recent Natural Language Processing (NLP) techniques for such tasks.",
            "referenceCount": 102,
            "citationCount": 238,
            "influentialCitationCount": 16,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.ijcai.org/proceedings/2019/0880.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference",
                "Review"
            ],
            "publicationDate": "2019-06-10",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1906.03926"
            },
            "citationStyles": {
                "bibtex": "@Article{Luketina2019ASO,\n author = {Jelena Luketina and Nantas Nardelli and Gregory Farquhar and Jakob N. Foerster and Jacob Andreas and Edward Grefenstette and Shimon Whiteson and Tim Rockt\u00e4schel},\n booktitle = {International Joint Conference on Artificial Intelligence},\n journal = {ArXiv},\n title = {A Survey of Reinforcement Learning Informed by Natural Language},\n volume = {abs/1906.03926},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2c25be41e920b50ffe25ed0d80d7684113fac282",
            "@type": "ScholarlyArticle",
            "paperId": "2c25be41e920b50ffe25ed0d80d7684113fac282",
            "corpusId": 18154014,
            "url": "https://www.semanticscholar.org/paper/2c25be41e920b50ffe25ed0d80d7684113fac282",
            "title": "SURFACE SURFACE Natural Language Processing Natural Language Processing",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2015,
            "externalIds": {
                "DOI": "10.51202/9783186848109-167",
                "CorpusId": 18154014
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Liddy2015SURFACESN,\n author = {E. Liddy},\n title = {SURFACE SURFACE Natural Language Processing Natural Language Processing},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4f7f1ced3338a37448dbf508c905914284d136db",
            "@type": "ScholarlyArticle",
            "paperId": "4f7f1ced3338a37448dbf508c905914284d136db",
            "corpusId": 5589833,
            "url": "https://www.semanticscholar.org/paper/4f7f1ced3338a37448dbf508c905914284d136db",
            "title": "Natural Language Processing",
            "venue": "Computing Handbook, 3rd ed.",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2014,
            "externalIds": {
                "MAG": "2395018191",
                "DBLP": "books/crc/chb/Indurkhya14",
                "DOI": "10.1201/B16812-46",
                "CorpusId": 5589833
            },
            "abstract": null,
            "referenceCount": 1,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://calhoun.nps.edu/bitstream/10945/36804/1/NPS_Cebrowski_Institute%20-%20Natural%20Language%20Processing.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2014-05-07",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Indurkhya2014NaturalLP,\n author = {N. Indurkhya},\n booktitle = {Computing Handbook, 3rd ed.},\n pages = {40: 1-17},\n title = {Natural Language Processing},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b7dcfe0b2eba3bd1e362d3d04bfc26947069297e",
            "@type": "ScholarlyArticle",
            "paperId": "b7dcfe0b2eba3bd1e362d3d04bfc26947069297e",
            "corpusId": 53142912,
            "url": "https://www.semanticscholar.org/paper/b7dcfe0b2eba3bd1e362d3d04bfc26947069297e",
            "title": "Applied Natural Language Processing : Identification , Investigation , and Resolution",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2012,
            "externalIds": {
                "CorpusId": 53142912
            },
            "abstract": "AutoTutor is an intelligent tutoring system that helps students learn science, technology, and other technical subject matters by holding conversations with the student in natural language. AutoTutor\u2019s dialogues are organized around difficult questions and problems that require reasoning and explanations in the answers. The major components of AutoTutor include an animated conversational agent, dialogue management, speech act classification, a curriculum script, semantic evaluation of student contributions, and electronic documents (e.g., textbook and glossary). This chapter describes the computational components of AutoTutor, the similarity of these components to human tutors, and some challenges in handling smooth dialogue. We describe some ways that AutoTutor has been evaluated with respect to learning gains, conversation quality, and learner impressions. AutoTutor is sufficiently modular that the content and dialogue mechanisms can be modified with authoring tools. AutoTutor has spawned a number of other agent-based learning environments, such as AutoTutor-lite, Operation Aries!, and Guru.",
            "referenceCount": 79,
            "citationCount": 101,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Education",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Graesser2012AppliedNL,\n author = {A. Graesser},\n title = {Applied Natural Language Processing : Identification , Investigation , and Resolution},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:25e65b78b9af8361cf2ad7bba5dc734b21c3b1ea",
            "@type": "ScholarlyArticle",
            "paperId": "25e65b78b9af8361cf2ad7bba5dc734b21c3b1ea",
            "corpusId": 90260792,
            "url": "https://www.semanticscholar.org/paper/25e65b78b9af8361cf2ad7bba5dc734b21c3b1ea",
            "title": "A Survey of Code-switched Speech and Language Processing",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2019,
            "externalIds": {
                "ArXiv": "1904.00784",
                "DBLP": "journals/corr/abs-1904-00784",
                "MAG": "2928484296",
                "CorpusId": 90260792
            },
            "abstract": "Code-switching, the alternation of languages within a conversation or utterance, is a common communicative phenomenon that occurs in multilingual communities across the world. This survey reviews computational approaches for code-switched Speech and Natural Language Processing. We motivate why processing code-switched text and speech is essential for building intelligent agents and systems that interact with users in multilingual communities. As code-switching data and resources are scarce, we list what is available in various code-switched language pairs with the language processing tasks they can be used for. We review code-switching research in various Speech and NLP applications, including language processing tools and end-to-end systems. We conclude with future directions and open problems in the field.",
            "referenceCount": 135,
            "citationCount": 95,
            "influentialCitationCount": 8,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2019-03-25",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1904.00784"
            },
            "citationStyles": {
                "bibtex": "@Article{Sitaram2019ASO,\n author = {Sunayana Sitaram and Khyathi Raghavi Chandu and Sai Krishna Rallabandi and A. Black},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {A Survey of Code-switched Speech and Language Processing},\n volume = {abs/1904.00784},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8bcc314b6f53db304565cd39598e368015b9654e",
            "@type": "ScholarlyArticle",
            "paperId": "8bcc314b6f53db304565cd39598e368015b9654e",
            "corpusId": 64512972,
            "url": "https://www.semanticscholar.org/paper/8bcc314b6f53db304565cd39598e368015b9654e",
            "title": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2012,
            "externalIds": {
                "MAG": "2603289712",
                "CorpusId": 64512972
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 190,
            "influentialCitationCount": 9,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "The Association for Computational Linguistics",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Article{Eshky2012ProceedingsOT,\n author = {Aciel Eshky and B. Allison and M. Steedman},\n journal = {The Association for Computational Linguistics},\n title = {Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b88c23712c9fbf42094f32ef8e2b3810c2720d3f",
            "@type": "ScholarlyArticle",
            "paperId": "b88c23712c9fbf42094f32ef8e2b3810c2720d3f",
            "corpusId": 31450820,
            "url": "https://www.semanticscholar.org/paper/b88c23712c9fbf42094f32ef8e2b3810c2720d3f",
            "title": "Automated identification of postoperative complications within an electronic medical record using natural language processing.",
            "venue": "Journal of the American Medical Association (JAMA)",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2011,
            "externalIds": {
                "MAG": "2009790391",
                "DOI": "10.1001/jama.2011.1204",
                "CorpusId": 31450820,
                "PubMed": "21862746"
            },
            "abstract": "CONTEXT\nCurrently most automated methods to identify patient safety occurrences rely on administrative data codes; however, free-text searches of electronic medical records could represent an additional surveillance approach.\n\n\nOBJECTIVE\nTo evaluate a natural language processing search-approach to identify postoperative surgical complications within a comprehensive electronic medical record.\n\n\nDESIGN, SETTING, AND PATIENTS\nCross-sectional study involving 2974 patients undergoing inpatient surgical procedures at 6 Veterans Health Administration (VHA) medical centers from 1999 to 2006.\n\n\nMAIN OUTCOME MEASURES\nPostoperative occurrences of acute renal failure requiring dialysis, deep vein thrombosis, pulmonary embolism, sepsis, pneumonia, or myocardial infarction identified through medical record review as part of the VA Surgical Quality Improvement Program. We determined the sensitivity and specificity of the natural language processing approach to identify these complications and compared its performance with patient safety indicators that use discharge coding information.\n\n\nRESULTS\nThe proportion of postoperative events for each sample was 2% (39 of 1924) for acute renal failure requiring dialysis, 0.7% (18 of 2327) for pulmonary embolism, 1% (29 of 2327) for deep vein thrombosis, 7% (61 of 866) for sepsis, 16% (222 of 1405) for pneumonia, and 2% (35 of 1822) for myocardial infarction. Natural language processing correctly identified 82% (95% confidence interval [CI], 67%-91%) of acute renal failure cases compared with 38% (95% CI, 25%-54%) for patient safety indicators. Similar results were obtained for venous thromboembolism (59%, 95% CI, 44%-72% vs 46%, 95% CI, 32%-60%), pneumonia (64%, 95% CI, 58%-70% vs 5%, 95% CI, 3%-9%), sepsis (89%, 95% CI, 78%-94% vs 34%, 95% CI, 24%-47%), and postoperative myocardial infarction (91%, 95% CI, 78%-97%) vs 89%, 95% CI, 74%-96%). Both natural language processing and patient safety indicators were highly specific for these diagnoses.\n\n\nCONCLUSION\nAmong patients undergoing inpatient surgical procedures at VA medical centers, natural language processing analysis of electronic medical records to identify postoperative complications had higher sensitivity and lower specificity compared with patient safety indicators based on discharge coding.",
            "referenceCount": 26,
            "citationCount": 422,
            "influentialCitationCount": 10,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://jamanetwork.com/journals/jama/articlepdf/1108490/joc15101_848_855.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2011-08-24",
            "journal": {
                "name": "JAMA",
                "volume": "306 8"
            },
            "citationStyles": {
                "bibtex": "@Article{Murff2011AutomatedIO,\n author = {H. Murff and F. FitzHenry and M. Matheny and Nancy Gentry and Kristen Kotter and K. Crimin and R. Dittus and A. Rosen and P. Elkin and S. Brown and T. Speroff},\n booktitle = {Journal of the American Medical Association (JAMA)},\n journal = {JAMA},\n pages = {\n          848-55\n        },\n title = {Automated identification of postoperative complications within an electronic medical record using natural language processing.},\n volume = {306 8},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6c16b823ba4b98d8ce63eedb5efcbf0a29bf336e",
            "@type": "ScholarlyArticle",
            "paperId": "6c16b823ba4b98d8ce63eedb5efcbf0a29bf336e",
            "corpusId": 913889,
            "url": "https://www.semanticscholar.org/paper/6c16b823ba4b98d8ce63eedb5efcbf0a29bf336e",
            "title": "Learning to Rank for Information Retrieval and Natural Language Processing",
            "venue": "Synthesis Lectures on Human Language Technologies",
            "publicationVenue": {
                "id": "urn:research:400d73aa-0d51-4582-9144-2069958881cd",
                "name": "Synthesis Lectures on Human Language Technologies",
                "alternate_names": [
                    "Synth Lect Hum Lang Technol"
                ],
                "issn": "1947-4040",
                "url": "https://www.morganclaypool.com/toc/hlt/1/1"
            },
            "year": 2011,
            "externalIds": {
                "DBLP": "series/synthesis/2011Li",
                "MAG": "2009077327",
                "DOI": "10.1007/978-3-031-02141-1",
                "CorpusId": 913889
            },
            "abstract": null,
            "referenceCount": 118,
            "citationCount": 366,
            "influentialCitationCount": 38,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/bfm:978-3-031-02155-8/1?pdf=chapter%20toc",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2011-04-22",
            "journal": {
                "name": "Synthesis Lectures on Human Language Technologies",
                "volume": "4"
            },
            "citationStyles": {
                "bibtex": "@Article{Li2011LearningTR,\n author = {Hang Li},\n booktitle = {Synthesis Lectures on Human Language Technologies},\n journal = {Synthesis Lectures on Human Language Technologies},\n pages = {1-113},\n title = {Learning to Rank for Information Retrieval and Natural Language Processing},\n volume = {4},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a9ae26db3291045b562589df4647c3cca4ac5009",
            "@type": "ScholarlyArticle",
            "paperId": "a9ae26db3291045b562589df4647c3cca4ac5009",
            "corpusId": 60871580,
            "url": "https://www.semanticscholar.org/paper/a9ae26db3291045b562589df4647c3cca4ac5009",
            "title": "Natural Language Processing for Historical Texts",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2012,
            "externalIds": {
                "MAG": "1583824497",
                "CorpusId": 60871580
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 64,
            "influentialCitationCount": 2,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "History"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "History",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "History",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2012-07-29",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Site2012NaturalLP,\n author = {Esslli Site and Asds sp. z o.o.},\n title = {Natural Language Processing for Historical Texts},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:bcbdf4b03ba5c3f1dc3238bb9ffe77903046fd1d",
            "@type": "ScholarlyArticle",
            "paperId": "bcbdf4b03ba5c3f1dc3238bb9ffe77903046fd1d",
            "corpusId": 169034882,
            "url": "https://www.semanticscholar.org/paper/bcbdf4b03ba5c3f1dc3238bb9ffe77903046fd1d",
            "title": "Extracting Declarative Process Models from Natural Language",
            "venue": "International Conference on Advanced Information Systems Engineering",
            "publicationVenue": {
                "id": "urn:research:91785da4-7c6a-4e13-936c-0a801a1ef895",
                "name": "International Conference on Advanced Information Systems Engineering",
                "alternate_names": [
                    "Int Conf Adv Inf Syst Eng",
                    "Conference on Advanced Information Systems Engineering",
                    "CAiSE",
                    "Conf Adv Inf Syst Eng"
                ],
                "issn": null,
                "url": "https://link.springer.com/conference/caise"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "conf/caise/AaCLR19",
                "MAG": "2947164866",
                "DOI": "10.1007/978-3-030-21290-2_23",
                "CorpusId": 169034882
            },
            "abstract": null,
            "referenceCount": 35,
            "citationCount": 52,
            "influentialCitationCount": 8,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-06-03",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Aa2019ExtractingDP,\n author = {Han van der Aa and Claudio Di Ciccio and H. Leopold and H. Reijers},\n booktitle = {International Conference on Advanced Information Systems Engineering},\n pages = {365-382},\n title = {Extracting Declarative Process Models from Natural Language},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ababc1999b5f31409c78c39d1842219821e37a6d",
            "@type": "ScholarlyArticle",
            "paperId": "ababc1999b5f31409c78c39d1842219821e37a6d",
            "corpusId": 3333256,
            "url": "https://www.semanticscholar.org/paper/ababc1999b5f31409c78c39d1842219821e37a6d",
            "title": "Sentiment analysis: capturing favorability using natural language processing",
            "venue": "International Conference on Knowledge Capture",
            "publicationVenue": {
                "id": "urn:research:8d2784da-c796-42e8-a25a-cbd60f1b6c92",
                "name": "International Conference on Knowledge Capture",
                "alternate_names": [
                    "Int Conf Knowl Capture",
                    "K-CAP"
                ],
                "issn": null,
                "url": "http://www.wikicfp.com/cfp/program?id=1900"
            },
            "year": 2003,
            "externalIds": {
                "MAG": "2148506018",
                "DBLP": "conf/kcap/NasukawaY03",
                "DOI": "10.1145/945645.945658",
                "CorpusId": 3333256
            },
            "abstract": "This paper illustrates a sentiment analysis approach to extract sentiments associated with polarities of positive or negative for specific subjects from a document, instead of classifying the whole document into positive or negative.The essential issues in sentiment analysis are to identify how sentiments are expressed in texts and whether the expressions indicate positive (favorable) or negative (unfavorable) opinions toward the subject. In order to improve the accuracy of the sentiment analysis, it is important to properly identify the semantic relationships between the sentiment expressions and the subject. By applying semantic analysis with a syntactic parser and sentiment lexicon, our prototype system achieved high precision (75-95%, depending on the data) in finding sentiments within Web pages and news articles.",
            "referenceCount": 16,
            "citationCount": 1233,
            "influentialCitationCount": 45,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2003-10-23",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Nasukawa2003SentimentAC,\n author = {Tetsuya Nasukawa and Jeonghee Yi},\n booktitle = {International Conference on Knowledge Capture},\n pages = {70-77},\n title = {Sentiment analysis: capturing favorability using natural language processing},\n year = {2003}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7d6ac0755a5627ed6ab2a497f6eb3926e914d1d2",
            "@type": "ScholarlyArticle",
            "paperId": "7d6ac0755a5627ed6ab2a497f6eb3926e914d1d2",
            "corpusId": 65057131,
            "url": "https://www.semanticscholar.org/paper/7d6ac0755a5627ed6ab2a497f6eb3926e914d1d2",
            "title": "Word Embedding for Understanding Natural Language: A Survey",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2617241599",
                "DOI": "10.1007/978-3-319-53817-4_4",
                "CorpusId": 65057131
            },
            "abstract": null,
            "referenceCount": 62,
            "citationCount": 158,
            "influentialCitationCount": 2,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Li2018WordEF,\n author = {Yang Li and Tao Yang},\n pages = {83-104},\n title = {Word Embedding for Understanding Natural Language: A Survey},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2ffd89c78cf7806a7d9aebec69dad829185deb08",
            "@type": "ScholarlyArticle",
            "paperId": "2ffd89c78cf7806a7d9aebec69dad829185deb08",
            "corpusId": 62991645,
            "url": "https://www.semanticscholar.org/paper/2ffd89c78cf7806a7d9aebec69dad829185deb08",
            "title": "Coh-Metrix: An automated tool for theoretical and applied natural language processing",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2011,
            "externalIds": {
                "MAG": "2484187659",
                "DOI": "10.4018/978-1-60960-741-8.CH011",
                "CorpusId": 62991645
            },
            "abstract": "Coh-Metrix provides indices for the characteristics of texts on multiple levels of analysis, including word characteristics, sentence characteristics, and the discourse relationships between ideas in text. Coh-Metrix was developed to provide a wide range of indices within one tool. This chapter describes Coh-Metrix and studies that have been conducted validating the Coh-Metrix indices. Coh-Metrix can be used to better understand differences between texts and to explore the extent to which linguistic and discourse features successfully distinguish between text types. Coh-Metrix can also be used to develop and improve natural language processing approaches. We also describe the Coh-Metrix Text Easability Component Scores, which provide a picture of text ease (and hence potential challenges). The Text Easability components provided by Coh-Metrix go beyond traditional readability measures by providing metrics of text characteristics on multiple levels of language and discourse. DOI: 10.4018/978-1-60960-741-8.ch011",
            "referenceCount": 42,
            "citationCount": 118,
            "influentialCitationCount": 8,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2011-12-01",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{McNamara2011CohMetrixAA,\n author = {D. McNamara and A. Graesser},\n pages = {188-205},\n title = {Coh-Metrix: An automated tool for theoretical and applied natural language processing},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:cbc8488784ee77d5a9921580788cb53f995aaaeb",
            "@type": "ScholarlyArticle",
            "paperId": "cbc8488784ee77d5a9921580788cb53f995aaaeb",
            "corpusId": 7111881,
            "url": "https://www.semanticscholar.org/paper/cbc8488784ee77d5a9921580788cb53f995aaaeb",
            "title": "Coarse-to-Fine Natural Language Processing",
            "venue": "Theory and Applications of Natural Language Processing",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2011,
            "externalIds": {
                "DBLP": "books/daglib/0032688",
                "MAG": "2118584553",
                "DOI": "10.1007/978-3-642-22743-1",
                "CorpusId": 7111881
            },
            "abstract": null,
            "referenceCount": 128,
            "citationCount": 87,
            "influentialCitationCount": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://escholarship.org/content/qt8kp924f2/qt8kp924f2.pdf?t=mtfqsy",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2011-11-03",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Petrov2011CoarsetoFineNL,\n author = {Slav Petrov},\n booktitle = {Theory and Applications of Natural Language Processing},\n pages = {I-XX, 1-105},\n title = {Coarse-to-Fine Natural Language Processing},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:348a9a9e5b4a7a5bab93000c6b3795eb530d3a7c",
            "@type": "ScholarlyArticle",
            "paperId": "348a9a9e5b4a7a5bab93000c6b3795eb530d3a7c",
            "corpusId": 14728304,
            "url": "https://www.semanticscholar.org/paper/348a9a9e5b4a7a5bab93000c6b3795eb530d3a7c",
            "title": "Analysis and evaluation of unstructured data: text mining versus natural language processing",
            "venue": "Advanced Industrial Conference on Telecommunications",
            "publicationVenue": {
                "id": "urn:research:3454cd32-76b1-4baa-88f8-7faa8c04abd1",
                "name": "Advanced Industrial Conference on Telecommunications",
                "alternate_names": [
                    "AICT",
                    "Adv Ind Conf Telecommun",
                    "Int Conf Appl Inf Commun Technol",
                    "International Conference on Application of Information and Communication Technologies"
                ],
                "issn": null,
                "url": "http://www.wikicfp.com/cfp/program?id=101"
            },
            "year": 2011,
            "externalIds": {
                "MAG": "2030910246",
                "DOI": "10.1109/ICAICT.2011.6111017",
                "CorpusId": 14728304
            },
            "abstract": "Nowadays, most of information saved in companies are as unstructured models. Retrieval and extraction of the information is essential works and importance in semantic web areas. Many of these requirements will be depend on the storage efficiency and unstructured data analysis. Merrill Lynch recently estimated that more than 80% of all potentially useful business information is unstructured data. The large number and complexity of unstructured data opens up many new possibilities for the analyst. We analyze both structured and unstructured data individually and collectively. Text mining and natural language processing are two techniques with their methods for knowledge discovery form textual context in documents. In this study, text mining and natural language techniques will be illustrated. The aim of this work comparison and evaluation the similarities and differences between text mining and natural language processing for extraction useful information via suitable themselves methods.",
            "referenceCount": 29,
            "citationCount": 72,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Business",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Conference"
            ],
            "publicationDate": "2011-12-22",
            "journal": {
                "name": "2011 5th International Conference on Application of Information and Communication Technologies (AICT)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Conference{Gharehchopogh2011AnalysisAE,\n author = {F. S. Gharehchopogh and Zeynab Abbasi Khalifelu},\n booktitle = {Advanced Industrial Conference on Telecommunications},\n journal = {2011 5th International Conference on Application of Information and Communication Technologies (AICT)},\n pages = {1-4},\n title = {Analysis and evaluation of unstructured data: text mining versus natural language processing},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3a3d508407cf04be784c76b482c448a4b60c15a4",
            "@type": "ScholarlyArticle",
            "paperId": "3a3d508407cf04be784c76b482c448a4b60c15a4",
            "corpusId": 40456540,
            "url": "https://www.semanticscholar.org/paper/3a3d508407cf04be784c76b482c448a4b60c15a4",
            "title": "Graph-based Natural Language Processing and Information Retrieval",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2011,
            "externalIds": {
                "MAG": "2246281709",
                "DOI": "10.1017/CBO9780511976247",
                "CorpusId": 40456540
            },
            "abstract": "Graph theory and the fields of natural language processing and information retrieval are well-studied disciplines. Traditionally, these areas have been perceived as distinct, with different algorithms, different applications, and different potential end-users. However, recent research has shown that these disciplines are intimately connected, with a large variety of natural language processing and information retrieval applications finding efficient solutions within graph-theoretical frameworks. This book extensively covers the use of graph-based algorithms for natural language processing and information retrieval. It brings together topics as diverse as lexical semantics, text summarization, text mining, ontology construction, text classification, and information retrieval, which are connected by the common underlying theme of the use of graph-theoretical methods for text and information processing tasks. Readers will come away with a firm understanding of the major methods and applications in natural language processing and information retrieval that rely on graph-based representations and algorithms.",
            "referenceCount": 195,
            "citationCount": 45,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2011-04-11",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Biemann2011GraphbasedNL,\n author = {Chris Biemann},\n title = {Graph-based Natural Language Processing and Information Retrieval},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9405cc0d6169988371b2755e573cc28650d14dfe",
            "@type": "ScholarlyArticle",
            "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
            "corpusId": 160025533,
            "url": "https://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe",
            "title": "Language Models are Unsupervised Multitask Learners",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2955855238",
                "CorpusId": 160025533
            },
            "abstract": "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.",
            "referenceCount": 75,
            "citationCount": 13277,
            "influentialCitationCount": 2714,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Radford2019LanguageMA,\n author = {Alec Radford and Jeff Wu and Rewon Child and D. Luan and Dario Amodei and Ilya Sutskever},\n title = {Language Models are Unsupervised Multitask Learners},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:df2b0e26d0599ce3e70df8a9da02e51594e0e992",
            "@type": "ScholarlyArticle",
            "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
            "corpusId": 52967399,
            "url": "https://www.semanticscholar.org/paper/df2b0e26d0599ce3e70df8a9da02e51594e0e992",
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:01103732-3808-4930-b8e4-7e9e68d5c68d",
                "name": "North American Chapter of the Association for Computational Linguistics",
                "alternate_names": [
                    "North Am Chapter Assoc Comput Linguistics",
                    "NAACL"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/naacl"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2951055169",
                "ACL": "N19-1423",
                "DBLP": "journals/corr/abs-1810-04805",
                "ArXiv": "1810.04805",
                "DOI": "10.18653/v1/N19-1423",
                "CorpusId": 52967399
            },
            "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
            "referenceCount": 63,
            "citationCount": 65045,
            "influentialCitationCount": 18206,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Devlin2019BERTPO,\n author = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},\n booktitle = {North American Chapter of the Association for Computational Linguistics},\n pages = {4171-4186},\n title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e7ad08848d5d7c5c47673ffe0da06af443643bda",
            "@type": "ScholarlyArticle",
            "paperId": "e7ad08848d5d7c5c47673ffe0da06af443643bda",
            "corpusId": 249017743,
            "url": "https://www.semanticscholar.org/paper/e7ad08848d5d7c5c47673ffe0da06af443643bda",
            "title": "Large Language Models are Zero-Shot Reasoners",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2022,
            "externalIds": {
                "DBLP": "journals/corr/abs-2205-11916",
                "ArXiv": "2205.11916",
                "CorpusId": 249017743
            },
            "abstract": "Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding\"Let's think step by step\"before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with large InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.",
            "referenceCount": 60,
            "citationCount": 1106,
            "influentialCitationCount": 130,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2022-05-24",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2205.11916"
            },
            "citationStyles": {
                "bibtex": "@Article{Kojima2022LargeLM,\n author = {Takeshi Kojima and S. Gu and Machel Reid and Yutaka Matsuo and Yusuke Iwasawa},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Large Language Models are Zero-Shot Reasoners},\n volume = {abs/2205.11916},\n year = {2022}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c62de088af618ab1a5057f0d4a52161acab0d8e0",
            "@type": "ScholarlyArticle",
            "paperId": "c62de088af618ab1a5057f0d4a52161acab0d8e0",
            "corpusId": 17485080,
            "url": "https://www.semanticscholar.org/paper/c62de088af618ab1a5057f0d4a52161acab0d8e0",
            "title": "Natural Language Processing to the Rescue? Extracting \"Situational Awareness\" Tweets During Mass Emergency",
            "venue": "International Conference on Web and Social Media",
            "publicationVenue": {
                "id": "urn:research:7dc964d5-49e6-4c37-b1c4-a7f0de1fa425",
                "name": "International Conference on Web and Social Media",
                "alternate_names": [
                    "Int Conf Weblogs Soc Media",
                    "International Conference on Weblogs and Social Media",
                    "Int Conf Web Soc Media",
                    "ICWSM"
                ],
                "issn": null,
                "url": "http://www.aaai.org/Library/ICWSM/icwsm-library.php"
            },
            "year": 2011,
            "externalIds": {
                "MAG": "50740790",
                "DBLP": "conf/icwsm/VermaVCPMPSA11",
                "DOI": "10.1609/icwsm.v5i1.14119",
                "CorpusId": 17485080
            },
            "abstract": "\n \n In times of mass emergency, vast amounts of data are generated via computer-mediated communication (CMC) that are difficult to manually cull and organize into a coherent picture. Yet valuable information is broadcast, and can provide useful insight into time- and safety-critical situations if captured and analyzed properly and rapidly. We describe an approach for automatically identifying messages communicated via Twitter that contribute to situational awareness, and explain why it is beneficial for those seeking information during mass emergencies.We collected Twitter messages from four different crisis events of varying nature and magnitude and built a classifier to automatically detect messages that may contribute to situational awareness, utilizing a combination of hand-annotated and automatically-extracted linguistic features. Our system was able to achieve over 80% accuracy on categorizing tweets that contribute to situational awareness. Additionally, we show that a classifier developed for a specific emergency event performs well on similar events. The results are promising, and have the potential to aid the general public in culling and analyzing information communicated during times of mass emergency.\n \n",
            "referenceCount": 24,
            "citationCount": 348,
            "influentialCitationCount": 21,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/ICWSM/article/download/14119/13968",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2011-07-05",
            "journal": {
                "name": "Proceedings of the International AAAI Conference on Web and Social Media",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Verma2011NaturalLP,\n author = {Sudha Verma and Sarah Vieweg and William J. Corvey and L. Palen and James H. Martin and Martha Palmer and Aaron Schram and K. Anderson},\n booktitle = {International Conference on Web and Social Media},\n journal = {Proceedings of the International AAAI Conference on Web and Social Media},\n title = {Natural Language Processing to the Rescue? Extracting \"Situational Awareness\" Tweets During Mass Emergency},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:89c239b70a8956b259a00a4e4491491d93d06a78",
            "@type": "ScholarlyArticle",
            "paperId": "89c239b70a8956b259a00a4e4491491d93d06a78",
            "corpusId": 124583496,
            "url": "https://www.semanticscholar.org/paper/89c239b70a8956b259a00a4e4491491d93d06a78",
            "title": "Graph-Based Natural Language Processing and Information Retrieval: Notations, Properties, and Representations",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2011,
            "externalIds": {
                "MAG": "2278584509",
                "DOI": "10.1017/CBO9780511976247.002",
                "CorpusId": 124583496
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 164,
            "influentialCitationCount": 10,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Mihalcea2011GraphBasedNL,\n author = {Rada Mihalcea and Dragomir R. Radev},\n title = {Graph-Based Natural Language Processing and Information Retrieval: Notations, Properties, and Representations},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3f020157c741f869da2a5daa2971b90d37fa9581",
            "@type": "ScholarlyArticle",
            "paperId": "3f020157c741f869da2a5daa2971b90d37fa9581",
            "corpusId": 141282,
            "url": "https://www.semanticscholar.org/paper/3f020157c741f869da2a5daa2971b90d37fa9581",
            "title": "Computational Argumentation Quality Assessment in Natural Language",
            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:8de18c35-6785-4e54-99f2-21ee961302c6",
                "name": "Conference of the European Chapter of the Association for Computational Linguistics",
                "alternate_names": [
                    "Conf Eur Chapter Assoc Comput Linguistics",
                    "EACL"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/anthology/venues/eacl/"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "conf/eacl/WachsmuthSHPBHN17",
                "ACL": "E17-1017",
                "MAG": "2741686183",
                "DOI": "10.18653/V1/E17-1017",
                "CorpusId": 141282
            },
            "abstract": "Research on computational argumentation faces the problem of how to automatically assess the quality of an argument or argumentation. While different quality dimensions have been approached in natural language processing, a common understanding of argumentation quality is still missing. This paper presents the first holistic work on computational argumentation quality in natural language. We comprehensively survey the diverse existing theories and approaches to assess logical, rhetorical, and dialectical quality dimensions, and we derive a systematic taxonomy from these. In addition, we provide a corpus with 320 arguments, annotated for all 15 dimensions in the taxonomy. Our results establish a common ground for research on computational argumentation quality assessment.",
            "referenceCount": 70,
            "citationCount": 169,
            "influentialCitationCount": 42,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/E17-1017.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference",
                "Review"
            ],
            "publicationDate": "2017-04-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Wachsmuth2017ComputationalAQ,\n author = {Henning Wachsmuth and Nona Naderi and Yufang Hou and Yonatan Bilu and Vinodkumar Prabhakaran and Tim Alberdingk Thijm and Graeme Hirst and Benno Stein},\n booktitle = {Conference of the European Chapter of the Association for Computational Linguistics},\n pages = {176-187},\n title = {Computational Argumentation Quality Assessment in Natural Language},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:35321a8880d1ff77875ae39531f0ec92e9f888d4",
            "@type": "ScholarlyArticle",
            "paperId": "35321a8880d1ff77875ae39531f0ec92e9f888d4",
            "corpusId": 16540824,
            "url": "https://www.semanticscholar.org/paper/35321a8880d1ff77875ae39531f0ec92e9f888d4",
            "title": "Natural Language Processing methods and systems for biomedical ontology learning",
            "venue": "Journal of Biomedical Informatics",
            "publicationVenue": {
                "id": "urn:research:f9827422-a381-440c-a8a4-e5e50415934e",
                "name": "Journal of Biomedical Informatics",
                "alternate_names": [
                    "J Biomed Informatics"
                ],
                "issn": "1532-0464",
                "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/622857/description#description"
            },
            "year": 2011,
            "externalIds": {
                "MAG": "2037625543",
                "DBLP": "journals/jbi/LiuHC11",
                "DOI": "10.1016/j.jbi.2010.07.006",
                "CorpusId": 16540824,
                "PubMed": "20647054"
            },
            "abstract": null,
            "referenceCount": 143,
            "citationCount": 136,
            "influentialCitationCount": 5,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2011-02-01",
            "journal": {
                "name": "Journal of biomedical informatics",
                "volume": "44 1"
            },
            "citationStyles": {
                "bibtex": "@Article{Liu2011NaturalLP,\n author = {Kaihong Liu and W. Hogan and R. Crowley},\n booktitle = {Journal of Biomedical Informatics},\n journal = {Journal of biomedical informatics},\n pages = {\n          163-79\n        },\n title = {Natural Language Processing methods and systems for biomedical ontology learning},\n volume = {44 1},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0b0e2ad61d79a52d19c3b467ed59afb1319d23d5",
            "@type": "ScholarlyArticle",
            "paperId": "0b0e2ad61d79a52d19c3b467ed59afb1319d23d5",
            "corpusId": 56816977,
            "url": "https://www.semanticscholar.org/paper/0b0e2ad61d79a52d19c3b467ed59afb1319d23d5",
            "title": "Handbook of Natural Language Processing and Machine Translation: DARPA Global Autonomous Language Exploitation",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2011,
            "externalIds": {
                "MAG": "1594833364",
                "DOI": "10.1007/978-1-4419-7713-7",
                "CorpusId": 56816977
            },
            "abstract": null,
            "referenceCount": 481,
            "citationCount": 131,
            "influentialCitationCount": 9,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Book"
            ],
            "publicationDate": null,
            "journal": {
                "name": "Handbook of Natural Language Processing and Machine Translation",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Book{Olive2011HandbookON,\n author = {J. Olive and Caitlin Christianson and John McCary},\n journal = {Handbook of Natural Language Processing and Machine Translation},\n title = {Handbook of Natural Language Processing and Machine Translation: DARPA Global Autonomous Language Exploitation},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:25704777b72f13cf6a5ccfd6c092520b36bf2ed5",
            "@type": "ScholarlyArticle",
            "paperId": "25704777b72f13cf6a5ccfd6c092520b36bf2ed5",
            "corpusId": 54142979,
            "url": "https://www.semanticscholar.org/paper/25704777b72f13cf6a5ccfd6c092520b36bf2ed5",
            "title": "The Handbook of Computational Linguistics and Natural Language Processing",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2010,
            "externalIds": {
                "MAG": "2497265084",
                "DOI": "10.1002/9781444324044",
                "CorpusId": 54142979
            },
            "abstract": "The Handbook of Computational Linguistics and Natural Language Processing provides a comprehensive overview of the concepts, methodologies, and applications being undertaken today in computational linguistics and natural language processing. \n \nThe work begins with an introduction to the major theoretical issues in these fields, as well as the central engineering applications that the work has produced. Also included is a detailed synopsis of the most cutting edge research. The major developments in this dynamic field are presented in an accessible way that explains the close connection between scientific understanding of the computational properties of natural language and the creation of effective language technologies. \n \nThe Handbook serves as an invaluable state-of-the-art reference source for computational linguists and software engineers developing natural language applications in industrial research and development labs of software companies, as well as for graduate students and researchers in computer science, linguistics, psychology, philosophy, and mathematics working within computational linguistics.",
            "referenceCount": 0,
            "citationCount": 243,
            "influentialCitationCount": 9,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://media.wiley.com/product_data/excerpt/17/14051558/1405155817.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "2010-07-16",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Clark2010TheHO,\n author = {A. Clark and C. Fox and Shalom Lappin},\n title = {The Handbook of Computational Linguistics and Natural Language Processing},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d15f9480b40956281ff330769fb179abf5a6303d",
            "@type": "ScholarlyArticle",
            "paperId": "d15f9480b40956281ff330769fb179abf5a6303d",
            "corpusId": 7779743,
            "url": "https://www.semanticscholar.org/paper/d15f9480b40956281ff330769fb179abf5a6303d",
            "title": "Suicide Note Classification Using Natural Language Processing: A Content Analysis",
            "venue": "Biomedical Informatics Insights",
            "publicationVenue": {
                "id": "urn:research:7ab85f1f-de6f-4932-a062-28c7d2f01204",
                "name": "Biomedical Informatics Insights",
                "alternate_names": [
                    "Biomed Informatics Insight"
                ],
                "issn": "1178-2226",
                "url": "http://www.la-press.com/biomedical-informatics-insights-journal-j82"
            },
            "year": 2010,
            "externalIds": {
                "PubMedCentral": "3107011",
                "MAG": "2053479646",
                "DOI": "10.4137/BII.S4706",
                "CorpusId": 7779743,
                "PubMed": "21643548"
            },
            "abstract": "Suicide is the second leading cause of death among 25\u201334 year olds and the third leading cause of death among 15\u201325 year olds in the United States. In the Emergency Department, where suicidal patients often present, estimating the risk of repeated attempts is generally left to clinical judgment. This paper presents our second attempt to determine the role of computational algorithms in understanding a suicidal patient's thoughts, as represented by suicide notes. We focus on developing methods of natural language processing that distinguish between genuine and elicited suicide notes. We hypothesize that machine learning algorithms can categorize suicide notes as well as mental health professionals and psychiatric physician trainees do. The data used are comprised of suicide notes from 33 suicide completers and matched to 33 elicited notes from healthy control group members. Eleven mental health professionals and 31 psychiatric trainees were asked to decide if a note was genuine or elicited. Their decisions were compared to nine different machine-learning algorithms. The results indicate that trainees accurately classified notes 49% of the time, mental health professionals accurately classified notes 63% of the time, and the best machine learning algorithm accurately classified the notes 78% of the time. This is an important step in developing an evidence-based predictor of repeated suicide attempts because it shows that natural language processing can aid in distinguishing between classes of suicidal notes.",
            "referenceCount": 38,
            "citationCount": 210,
            "influentialCitationCount": 8,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://journals.sagepub.com/doi/pdf/10.4137/BII.S4706",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2010-01-01",
            "journal": {
                "name": "Biomedical Informatics Insights",
                "volume": "3"
            },
            "citationStyles": {
                "bibtex": "@Article{Pestian2010SuicideNC,\n author = {J. Pestian and H. Nasrallah and P. Matykiewicz and Aurora J Bennett and A. Leenaars},\n booktitle = {Biomedical Informatics Insights},\n journal = {Biomedical Informatics Insights},\n title = {Suicide Note Classification Using Natural Language Processing: A Content Analysis},\n volume = {3},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f4dd755a7f6238bf7f1b8671e57e098f248402e4",
            "@type": "ScholarlyArticle",
            "paperId": "f4dd755a7f6238bf7f1b8671e57e098f248402e4",
            "corpusId": 207079655,
            "url": "https://www.semanticscholar.org/paper/f4dd755a7f6238bf7f1b8671e57e098f248402e4",
            "title": "Natural language based financial forecasting: a survey",
            "venue": "Artificial Intelligence Review",
            "publicationVenue": {
                "id": "urn:research:ea8553fe-2467-4367-afee-c4deb3754820",
                "name": "Artificial Intelligence Review",
                "alternate_names": [
                    "Artif Intell Rev"
                ],
                "issn": "0269-2821",
                "url": "https://link.springer.com/journal/10462"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2766718178",
                "DBLP": "journals/air/XingCW18",
                "DOI": "10.1007/s10462-017-9588-9",
                "CorpusId": 207079655
            },
            "abstract": null,
            "referenceCount": 141,
            "citationCount": 276,
            "influentialCitationCount": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dspace.mit.edu/bitstream/1721.1/116314/2/10462_2017_9588_ReferencePDF.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Business"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Business",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Business",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2017-10-01",
            "journal": {
                "name": "Artificial Intelligence Review",
                "volume": "50"
            },
            "citationStyles": {
                "bibtex": "@Article{Xing2017NaturalLB,\n author = {Frank Xing and E. Cambria and R. Welsch},\n booktitle = {Artificial Intelligence Review},\n journal = {Artificial Intelligence Review},\n pages = {49-73},\n title = {Natural language based financial forecasting: a survey},\n volume = {50},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:79c80fc767726dae9ebd4fb8ce9c90a0b1c6f52e",
            "@type": "ScholarlyArticle",
            "paperId": "79c80fc767726dae9ebd4fb8ce9c90a0b1c6f52e",
            "corpusId": 263697073,
            "url": "https://www.semanticscholar.org/paper/79c80fc767726dae9ebd4fb8ce9c90a0b1c6f52e",
            "title": "Handbook of Natural Language Processing",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2010,
            "externalIds": {
                "MAG": "1410460",
                "DOI": "10.1201/9781420085938",
                "CorpusId": 263697073
            },
            "abstract": "The Handbook of Natural Language Processing, Second Edition presents practical tools and techniques for implementing natural language processing in computer systems. Along with removing outdated material, this edition updates every chapter and expands the content to include emerging areas, such as sentiment analysis. New to the Second Edition Greater prominence of statistical approaches New applications section Broader multilingual scope to include Asian and European languages, along with English An actively maintained wiki (http://handbookofnlp.cse.unsw.edu.au) that provides online resources, supplementary information, and up-to-date developments Divided into three sections, the book first surveys classical techniques, including both symbolic and empirical approaches. The second section focuses on statistical approaches in natural language processing. In the final section of the book, each chapter describes a particular class of application, from Chinese machine translation to information visualization to ontology construction to biomedical text mining. Fully updated with the latest developments in the field, this comprehensive, modern handbook emphasizes how to implement practical language processing tools in computational systems.",
            "referenceCount": 0,
            "citationCount": 303,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "2010-01-28",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Indurkhya2010HandbookON,\n author = {N. Indurkhya and Fred J. Damerau},\n title = {Handbook of Natural Language Processing},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8385d94ef79a41cfbf549fc1011ac5d4788ec2cd",
            "@type": "ScholarlyArticle",
            "paperId": "8385d94ef79a41cfbf549fc1011ac5d4788ec2cd",
            "corpusId": 17582547,
            "url": "https://www.semanticscholar.org/paper/8385d94ef79a41cfbf549fc1011ac5d4788ec2cd",
            "title": "Processing natural language arguments with theplatform",
            "venue": "Argument Comput.",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2012,
            "externalIds": {
                "DBLP": "journals/argcom/Saint-Dizier12",
                "MAG": "2025401082",
                "DOI": "10.1080/19462166.2012.663539",
                "CorpusId": 17582547
            },
            "abstract": "In this article, we first present the platform and the Dislog language, designed for discourse analysis with a logic and linguistic perspective. The platform has now reached a certain level of maturity which allows the recognition of a large diversity of discourse structures including general-purpose rhetorical structures as well as domain-specific discourse structures. The Dislog language is based on linguistic considerations and includes knowledge access and inference capabilities. Functionalities of the language are presented together with a method for writing discourse analysis rules. Efficiency and portability of the system over domains and languages are investigated to conclude this first part. In a second part, we analyse the different types of arguments found in several document genres, most notably: procedures, didactic texts and requirements. Arguments form a large class of discourse relations. A generic and frequently encountered form emerges from our analysis: \u2018reasons for conclusion\u2019...",
            "referenceCount": 66,
            "citationCount": 50,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://content.iospress.com:443/download/argument-and-computation/663539?id=argument-and-computation%2F663539",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2012-03-08",
            "journal": {
                "name": "Argument Comput.",
                "volume": "3"
            },
            "citationStyles": {
                "bibtex": "@Article{Saint-Dizier2012ProcessingNL,\n author = {P. Saint-Dizier},\n booktitle = {Argument Comput.},\n journal = {Argument Comput.},\n pages = {49-82},\n title = {Processing natural language arguments with theplatform},\n volume = {3},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2d80a4841b61feca8324ded6cc247ef68250b454",
            "@type": "ScholarlyArticle",
            "paperId": "2d80a4841b61feca8324ded6cc247ef68250b454",
            "corpusId": 1144073,
            "url": "https://www.semanticscholar.org/paper/2d80a4841b61feca8324ded6cc247ef68250b454",
            "title": "Tsnlp | Test Suites for Natural Language Processing",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": null,
            "externalIds": {
                "CorpusId": 1144073
            },
            "abstract": "Given the growing number of natural language applications, there is an increasing demand for reference data other than the type of data found in large collections of text corpora. Test suites have long been accepted in the NLP context, because they provide for controlled data that is systematically organized and documented. However, with increasing and realistic applications of NLP technology a generation of general-purpose test suites will be required that (i) have broad coverage, (ii) are multilingual , and (iii) incorporate consistent and highly structured linguistic annotations. Linguistic databases of this kind will not only facilitate the deployment of test suites as diagnostic tools, but as well support diierent kinds of evaluation and serve as repositories for developers. The objective of the tsnlp project 1 is to construct test suites in 1 The project was started in December 1993 and completed in March 1996; the consortium is comprised of three academic partners | viz. the University of Essex (UK), the Istituto Dalle Molle per gli Studii Semantici e Cognitivi ISSCO (Geneva, Switzerland), and DFKI Saarbr ucken (Germany) who have strong backgrounds in machine translation, evaluation, and natural language processing respectively | and the Common Research Center of Aerospatiale (Suresnes, France) as an industrial partner. Most of the project results (documents, bibliography, test data, and software) as well as on-line access to the test suite database (see section 1.2.3) can be obtained through the worldwide web from the tsnlp home page",
            "referenceCount": 13,
            "citationCount": 190,
            "influentialCitationCount": 15,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Misc{None,\n author = {Sabine Lehmann and S. Oepen and Sylvie Regnier-Prost and K. Netter and Veronika Lux and Judith Klein and Kirsten Falkedal and F. Fouvry and Dominique Estival and Eva Dauphin and Herve Compagnion and Judith Baur and Lorna Balkan and D. Arnold},\n title = {Tsnlp | Test Suites for Natural Language Processing}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:15b7f9d6fe249217c8567bf51dc7c10b5481a530",
            "@type": "ScholarlyArticle",
            "paperId": "15b7f9d6fe249217c8567bf51dc7c10b5481a530",
            "corpusId": 36295283,
            "url": "https://www.semanticscholar.org/paper/15b7f9d6fe249217c8567bf51dc7c10b5481a530",
            "title": "Statistical Natural Language Processing",
            "venue": "International Encyclopedia of Statistical Science",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2011,
            "externalIds": {
                "MAG": "300553845",
                "DBLP": "reference/stat/Hristea11",
                "DOI": "10.1007/978-3-642-04898-2_82",
                "CorpusId": 36295283
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 68,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Hristea2011StatisticalNL,\n author = {Florentina Hristea},\n booktitle = {International Encyclopedia of Statistical Science},\n pages = {1452-1453},\n title = {Statistical Natural Language Processing},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f98d7c2e0f881130350416c5420a98d72fd48f53",
            "@type": "ScholarlyArticle",
            "paperId": "f98d7c2e0f881130350416c5420a98d72fd48f53",
            "corpusId": 1541076,
            "url": "https://www.semanticscholar.org/paper/f98d7c2e0f881130350416c5420a98d72fd48f53",
            "title": "Portuguese Word Embeddings: Evaluating on Word Analogies and Natural Language Tasks",
            "venue": "Brazilian Symposium in Information and Human Language Technology",
            "publicationVenue": {
                "id": "urn:research:54b9f1b4-a27a-4b27-935b-9f529036b5c2",
                "name": "Brazilian Symposium in Information and Human Language Technology",
                "alternate_names": [
                    "STIL",
                    "Braz Symp Inf Hum Lang Technol"
                ],
                "issn": null,
                "url": null
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2963718212",
                "ACL": "W17-6615",
                "ArXiv": "1708.06025",
                "DBLP": "journals/corr/abs-1708-06025",
                "CorpusId": 1541076
            },
            "abstract": "Word embeddings have been found to provide meaningful representations for words in an efficient way; therefore, they have become common in Natural Language Processing sys- tems. In this paper, we evaluated different word embedding models trained on a large Portuguese corpus, including both Brazilian and European variants. We trained 31 word embedding models using FastText, GloVe, Wang2Vec and Word2Vec. We evaluated them intrinsically on syntactic and semantic analogies and extrinsically on POS tagging and sentence semantic similarity tasks. The obtained results suggest that word analogies are not appropriate for word embedding evaluation; task-specific evaluations appear to be a better option.",
            "referenceCount": 22,
            "citationCount": 176,
            "influentialCitationCount": 21,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-08-20",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1708.06025"
            },
            "citationStyles": {
                "bibtex": "@Article{Hartmann2017PortugueseWE,\n author = {N. Hartmann and Erick Rocha Fonseca and C. Shulby and Marcos Vin\u00edcius Treviso and J\u00e9ssica S. Rodrigues and S. Alu\u00edsio},\n booktitle = {Brazilian Symposium in Information and Human Language Technology},\n journal = {ArXiv},\n title = {Portuguese Word Embeddings: Evaluating on Word Analogies and Natural Language Tasks},\n volume = {abs/1708.06025},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:073c284f9a720f704bde806ec9cd55b30df5ea08",
            "@type": "ScholarlyArticle",
            "paperId": "073c284f9a720f704bde806ec9cd55b30df5ea08",
            "corpusId": 5956926,
            "url": "https://www.semanticscholar.org/paper/073c284f9a720f704bde806ec9cd55b30df5ea08",
            "title": "Urdu language processing: a survey",
            "venue": "Artificial Intelligence Review",
            "publicationVenue": {
                "id": "urn:research:ea8553fe-2467-4367-afee-c4deb3754820",
                "name": "Artificial Intelligence Review",
                "alternate_names": [
                    "Artif Intell Rev"
                ],
                "issn": "0269-2821",
                "url": "https://link.springer.com/journal/10462"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2409439155",
                "DBLP": "journals/air/DaudKC17",
                "DOI": "10.1007/s10462-016-9482-x",
                "CorpusId": 5956926
            },
            "abstract": null,
            "referenceCount": 77,
            "citationCount": 128,
            "influentialCitationCount": 11,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2017-03-01",
            "journal": {
                "name": "Artificial Intelligence Review",
                "volume": "47"
            },
            "citationStyles": {
                "bibtex": "@Article{Daud2017UrduLP,\n author = {Ali Daud and Wahab Khan and D. Che},\n booktitle = {Artificial Intelligence Review},\n journal = {Artificial Intelligence Review},\n pages = {279-311},\n title = {Urdu language processing: a survey},\n volume = {47},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:670c6f72d29e7ecc17b7d8ee06920c9237df3569",
            "@type": "ScholarlyArticle",
            "paperId": "670c6f72d29e7ecc17b7d8ee06920c9237df3569",
            "corpusId": 60811638,
            "url": "https://www.semanticscholar.org/paper/670c6f72d29e7ecc17b7d8ee06920c9237df3569",
            "title": "Speech and language processing: an introduction to natural language processing",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2000,
            "externalIds": {
                "MAG": "1606449776",
                "CorpusId": 60811638
            },
            "abstract": "For undergraduate or advanced undergraduate courses in Classical Natural Language Processing, Statistical Natural Language Processing, Speech Recognition, Computational Linguistics, and Human Language Processing. An explosion of Web-based language techniques, merging of distinct fields, availability of phone-based dialogue systems, and much more make this an exciting time in speech and language processing. The first of its kind to thoroughly cover language technology at all levels and with all modern technologies this text takes an empirical approach to the subject, based on applying statistical and other machine-learning algorithms to large corporations. The authors cover areas that traditionally are taught in different courses, to describe a unified vision of speech and language processing. Emphasis is on practical applications and scientific evaluation. An accompanying Website contains teaching materials for instructors, with pointers to language processing resources on the Web. The Second Edition offers a significant amount of new and extended material. Supplements: Click on the \"Resources\" tab to View Downloadable Files:Solutions Power Point Lecture Slides Chapters 1-5, 8-10, 12-13 and 24 Now Available! For additional resourcse visit the author website: http://www.cs.colorado.edu/~martin/slp.html",
            "referenceCount": 0,
            "citationCount": 917,
            "influentialCitationCount": 74,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Jurafsky2000SpeechAL,\n author = {Dan Jurafsky and James H. Martin},\n title = {Speech and language processing: an introduction to natural language processing},\n year = {2000}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0f7c3f51adaa6809d4e293b26463e5dec5159fa0",
            "@type": "ScholarlyArticle",
            "paperId": "0f7c3f51adaa6809d4e293b26463e5dec5159fa0",
            "corpusId": 63540462,
            "url": "https://www.semanticscholar.org/paper/0f7c3f51adaa6809d4e293b26463e5dec5159fa0",
            "title": "Rule-based Approach in Arabic Natural Language Processing",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2010,
            "externalIds": {
                "MAG": "2411190207",
                "CorpusId": 63540462
            },
            "abstract": "The rule-based approach has successfully been used in developing many natural language processing systems. Systems that use rule-based transformations are based on a core of solid linguistic knowledge. The linguistic knowledge acquired for one natural language processing system may be reused to build knowledge required for a similar task in another system. The advantage of the rule-based approach over the corpus-based approach is clear for: 1) less-resourced languages, for which large corpora, possibly parallel or bilingual, with representative structures and entities are neither available nor easily affordable, and 2) for morphologically rich languages, which even with the availability of corpora suffer from data sparseness. These have motivated many researchers to fully or partially follow the rule- based approach in developing their Arabic natural processing tools and systems. In this paper we address our successful efforts that involved rule-based approach for different Arabic natural language processing tasks.",
            "referenceCount": 31,
            "citationCount": 119,
            "influentialCitationCount": 8,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": "3"
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Shaalan2010RulebasedAI,\n author = {K. Shaalan},\n title = {Rule-based Approach in Arabic Natural Language Processing},\n volume = {3},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:636d4c0b0fe6919abe6eb546907d28ed39bf56e6",
            "@type": "ScholarlyArticle",
            "paperId": "636d4c0b0fe6919abe6eb546907d28ed39bf56e6",
            "corpusId": 11199202,
            "url": "https://www.semanticscholar.org/paper/636d4c0b0fe6919abe6eb546907d28ed39bf56e6",
            "title": "Using Natural Language Processing for Automatic Detection of Plagiarism",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2010,
            "externalIds": {
                "CorpusId": 11199202
            },
            "abstract": "Current plagiarism detection tools are mostly limited to comparisons of suspicious plagiarised texts and potential original texts at string level. In this study the aim is to improve the accuracy of plagiarism detection by incorporating Natural Language Processing (NLP) techniques into existing approaches. We propose a framework for external plagiarism detection in which a number of NLP techniques are applied to process a set of suspicious and original documents, not only to analyse strings but also the structure of the text, using resources to account for text relations. Initial results obtained with a corpus of plagiarised short paragraphs have showed that NLP techniques improve the accuracy of existing approaches.",
            "referenceCount": 36,
            "citationCount": 47,
            "influentialCitationCount": 3,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Chong2010UsingNL,\n author = {Miranda Chong and Lucia Specia and R. Mitkov},\n title = {Using Natural Language Processing for Automatic Detection of Plagiarism},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d1d9dab27dc05c4077f03f0b1d72602b88429168",
            "@type": "ScholarlyArticle",
            "paperId": "d1d9dab27dc05c4077f03f0b1d72602b88429168",
            "corpusId": 145708836,
            "url": "https://www.semanticscholar.org/paper/d1d9dab27dc05c4077f03f0b1d72602b88429168",
            "title": "The linguistic correlates of conversational deception: Comparing natural language processing technologies",
            "venue": "Applied Psycholinguistics",
            "publicationVenue": {
                "id": "urn:research:0328f97f-15b2-4177-b5cf-77c614b4b026",
                "name": "Applied Psycholinguistics",
                "alternate_names": [
                    "Appl Psycholinguist"
                ],
                "issn": "0142-7164",
                "url": "https://www.cambridge.org/core/journals/applied-psycholinguistics"
            },
            "year": 2010,
            "externalIds": {
                "MAG": "2119415273",
                "DOI": "10.1017/S0142716410000068",
                "CorpusId": 145708836
            },
            "abstract": "ABSTRACT The words people use and the way they use them can reveal a great deal about their mental states when they attempt to deceive. The challenge for researchers is how to reliably distinguish the linguistic features that characterize these hidden states. In this study, we use a natural language processing tool called Coh-Metrix to evaluate deceptive and truthful conversations that occur within a context of computer-mediated communication. Coh-Metrix is unique in that it tracks linguistic features based on cognitive and social factors that are hypothesized to influence deception. The results from Coh-Metrix are compared to linguistic features reported in previous independent research, which used a natural language processing tool called Linguistic Inquiry and Word Count. The comparison reveals converging and contrasting alignment for several linguistic features and establishes new insights on deceptive language and its use in conversation.",
            "referenceCount": 60,
            "citationCount": 72,
            "influentialCitationCount": 10,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2010-06-04",
            "journal": {
                "name": "Applied Psycholinguistics",
                "volume": "31"
            },
            "citationStyles": {
                "bibtex": "@Article{Duran2010TheLC,\n author = {Nicholas D. Duran and Charles E. Hall and Philip M. McCarthy and D. McNamara},\n booktitle = {Applied Psycholinguistics},\n journal = {Applied Psycholinguistics},\n pages = {439 - 462},\n title = {The linguistic correlates of conversational deception: Comparing natural language processing technologies},\n volume = {31},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:49512270b39636375880d611d7b2192d324f4ba6",
            "@type": "ScholarlyArticle",
            "paperId": "49512270b39636375880d611d7b2192d324f4ba6",
            "corpusId": 1914494,
            "url": "https://www.semanticscholar.org/paper/49512270b39636375880d611d7b2192d324f4ba6",
            "title": "Convolutional Neural Networks over Tree Structures for Programming Language Processing",
            "venue": "AAAI Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:bdc2e585-4e48-4e36-8af1-6d859763d405",
                "name": "AAAI Conference on Artificial Intelligence",
                "alternate_names": [
                    "National Conference on Artificial Intelligence",
                    "National Conf Artif Intell",
                    "AAAI Conf Artif Intell",
                    "AAAI"
                ],
                "issn": null,
                "url": "http://www.aaai.org/"
            },
            "year": 2014,
            "externalIds": {
                "MAG": "2963371736",
                "DBLP": "conf/aaai/MouLZWJ16",
                "DOI": "10.1609/aaai.v30i1.10139",
                "CorpusId": 1914494
            },
            "abstract": "\n \n Programming language processing (similar to natural language processing) is a hot research topic in the field of software engineering; it has also aroused growing interest in the artificial intelligence community. However, different from a natural language sentence, a program contains rich, explicit, and complicated structural information. Hence, traditional NLP models may be inappropriate for programs. In this paper, we propose a novel tree-based convolutional neural network (TBCNN) for programming language processing, in which a convolution kernel is designed over programs' abstract syntax trees to capture structural information. TBCNN is a generic architecture for programming language processing; our experiments show its effectiveness in two different program analysis tasks: classifying programs according to functionality, and detecting code snippets of certain patterns. TBCNN outperforms baseline methods, including several neural models for NLP.\n \n",
            "referenceCount": 36,
            "citationCount": 594,
            "influentialCitationCount": 90,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/10139/9998",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2014-09-18",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Mou2014ConvolutionalNN,\n author = {Lili Mou and Ge Li and Lu Zhang and Tao Wang and Zhi Jin},\n booktitle = {AAAI Conference on Artificial Intelligence},\n pages = {1287-1293},\n title = {Convolutional Neural Networks over Tree Structures for Programming Language Processing},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0eda1b40e7866ea5368b1d5e42c3a5c3b768347b",
            "@type": "ScholarlyArticle",
            "paperId": "0eda1b40e7866ea5368b1d5e42c3a5c3b768347b",
            "corpusId": 207176381,
            "url": "https://www.semanticscholar.org/paper/0eda1b40e7866ea5368b1d5e42c3a5c3b768347b",
            "title": "Arabic Natural Language Processing: Challenges and Solutions",
            "venue": "TALIP",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2009,
            "externalIds": {
                "DBLP": "journals/talip/FarghalyS09",
                "MAG": "1984708705",
                "DOI": "10.1145/1644879.1644881",
                "CorpusId": 207176381
            },
            "abstract": "The Arabic language presents researchers and developers of natural language processing (NLP) applications for Arabic text and speech with serious challenges. The purpose of this article is to describe some of these challenges and to present some solutions that would guide current and future practitioners in the field of Arabic natural language processing (ANLP). We begin with general features of the Arabic language in Sections 1, 2, and 3 and then we move to more specific properties of the language in the rest of the article. In Section 1 of this article we highlight the significance of the Arabic language today and describe its general properties. Section 2 presents the feature of Arabic Diglossia showing how the sociolinguistic aspects of the Arabic language differ from other languages. The stability of Arabic Diglossia and its implications for ANLP applications are discussed and ways to deal with this problematic property are proposed. Section 3 deals with the properties of the Arabic script and the explosion of ambiguity that results from the absence of short vowel representations and overt case markers in contemporary Arabic texts. We present in Section 4 specific features of the Arabic language such as the nonconcatenative property of Arabic morphology, Arabic as an agglutinative language, Arabic as a pro-drop language, and the challenge these properties pose to ANLP. We also present solutions that have already been adopted by some pioneering researchers in the field. In Section 5 we point out to the lack of formal and explicit grammars of Modern Standard Arabic which impedes the progress of more advanced ANLP systems. In Section 6 we draw our conclusion.",
            "referenceCount": 58,
            "citationCount": 486,
            "influentialCitationCount": 44,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2009-12-01",
            "journal": {
                "name": "ACM Trans. Asian Lang. Inf. Process.",
                "volume": "8"
            },
            "citationStyles": {
                "bibtex": "@Article{Farghaly2009ArabicNL,\n author = {A. Farghaly and K. Shaalan},\n booktitle = {TALIP},\n journal = {ACM Trans. Asian Lang. Inf. Process.},\n pages = {14:1-14:22},\n title = {Arabic Natural Language Processing: Challenges and Solutions},\n volume = {8},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f05950b895fbf826657f363d179c67eb556bbdd8",
            "@type": "ScholarlyArticle",
            "paperId": "f05950b895fbf826657f363d179c67eb556bbdd8",
            "corpusId": 1814112,
            "url": "https://www.semanticscholar.org/paper/f05950b895fbf826657f363d179c67eb556bbdd8",
            "title": "Natural Language Processing for Cultural Heritage Domains",
            "venue": "Language and Linguistics Compass",
            "publicationVenue": {
                "id": "urn:research:984325f8-27b7-4088-9a6d-c3de9e23beca",
                "name": "Language and Linguistics Compass",
                "alternate_names": [
                    "Lang Linguistics Compass"
                ],
                "issn": "1749-818X",
                "url": "http://www.blackwell-compass.com/subject/linguistics/"
            },
            "year": 2010,
            "externalIds": {
                "MAG": "2919599378",
                "DBLP": "journals/llc/Sporleder10",
                "DOI": "10.1111/J.1749-818X.2010.00230.X",
                "CorpusId": 1814112
            },
            "abstract": "Museums, archives, libraries and other cultural heritage institutes maintain large collections of artefacts which are valuable knowledge sources for both experts and interested lay persons. Recently, more and more cultural heritage institutes have started to digitise their collections, for instance to make them accessible via web portals. However, while digitisation is a necessary first step towards improved information access, to fully unlock the knowledge contained in these collections, users have to be able to easily browse, search and query these collections. This requires cleaning, linking and enriching the data, a process that is often too time-consuming to be performed manually. Information technology can help with (partially) automating this task. Since data processing and enrichment typically involve the textual metadata level, natural language processing has a key role to play in this endeavour. At the same time cultural heritage domains pose significant challenges for language technology and call for the development of very robust and flexible solutions. Consequently, cultural heritage data can also serve as a good test-bed for the development of robust natural language processing tools.",
            "referenceCount": 92,
            "citationCount": 29,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Art",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2010-09-01",
            "journal": {
                "name": "Lang. Linguistics Compass",
                "volume": "4"
            },
            "citationStyles": {
                "bibtex": "@Article{Sporleder2010NaturalLP,\n author = {C. Sporleder},\n booktitle = {Language and Linguistics Compass},\n journal = {Lang. Linguistics Compass},\n pages = {750-768},\n title = {Natural Language Processing for Cultural Heritage Domains},\n volume = {4},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1cb3df6e22212ec16ca8b709ac28e6ed443ebbef",
            "@type": "ScholarlyArticle",
            "paperId": "1cb3df6e22212ec16ca8b709ac28e6ed443ebbef",
            "corpusId": 60832077,
            "url": "https://www.semanticscholar.org/paper/1cb3df6e22212ec16ca8b709ac28e6ed443ebbef",
            "title": "Natural language processing",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2010,
            "externalIds": {
                "MAG": "1967646568",
                "DOI": "10.1002/wics.76",
                "CorpusId": 60832077
            },
            "abstract": "Approximately 40 years ago, the goal of endowing computers with the capacity to understand natural language began. These efforts were originally called natural language understanding, which is now more frequently called natural language processing (NLP). NLP is considered a branch of artificial intelligence (AI), but over the years it has become an interesting area of study in computational statistics and text data mining. NLP encompasses approaches that use computers to analyze, determine semantic similarity, and translate between languages. The area usually deals with written languages, but it could also be applied to speech. In this article, we cover definitions and concepts necessary for the understanding of NLP, methods at the word and sentence level (word sense disambiguation, part\u2010of\u2010speech tagging, and parsing), and the vector space model for NLP at the document level. Copyright \u00a9 2010 John Wiley & Sons, Inc.",
            "referenceCount": 24,
            "citationCount": 12,
            "influentialCitationCount": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://calhoun.nps.edu/bitstream/10945/36804/1/NPS_Cebrowski_Institute%20-%20Natural%20Language%20Processing.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2010-05-01",
            "journal": {
                "name": "Wiley Interdisciplinary Reviews: Computational Statistics",
                "volume": "2"
            },
            "citationStyles": {
                "bibtex": "@Article{Martinez2010NaturalLP,\n author = {Angel R. Martinez},\n journal = {Wiley Interdisciplinary Reviews: Computational Statistics},\n title = {Natural language processing},\n volume = {2},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:49843828acbbf9fbccd4d15fb99c5a4a7e56fcf4",
            "@type": "ScholarlyArticle",
            "paperId": "49843828acbbf9fbccd4d15fb99c5a4a7e56fcf4",
            "corpusId": 60029043,
            "url": "https://www.semanticscholar.org/paper/49843828acbbf9fbccd4d15fb99c5a4a7e56fcf4",
            "title": "Ontology and the lexicon : a natural language processing perspective",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2010,
            "externalIds": {
                "MAG": "2281581597",
                "DOI": "10.1017/CBO9780511676536",
                "CorpusId": 60029043
            },
            "abstract": "Part I. Fundamental Aspects: 1. Ontology and the lexicon: a multi-disciplinary perspective Laurent Prevot, Chu-Ren Huang, Nicoletta Calzolari, Aldo Gangemi, Alessandro Lenci and Alessandro Oltramari 2. Formal ontology as interlingua: the SUMO and WordNet linking project and GlobalWordNet Adam Pease and Christiane Fellbaum 3. Interfacing WordNet with DOLCE: towards OntoWordNet Aldo Gangemi, Nicola Guarino, Claudio Masolo and Alessandro Oltramari 4. Reasoning over natural language text by means of FrameNet and ontologies Jan Scheffczyk, Collin F. Baker and Srini Narayanan 5. Synergizing ontologies and the lexicon: a roadmap Alessandro Oltramari, Aldo Gangemi, Chu-Ren Huang, Nicoletta Calzolari, Alessandro Lenci and Laurent Prevot Part II. Discovery and Representation of Conceptual Systems: 6. Experiments of ontology construction with formal concept analysis SuJian Li, Qin Lu and Wenjie Li 7. Ontology, lexicon, and fact repository as leveraged to interpret events of change Marjorie McShane, Sergei Nirenburg and Stephen Beale 8. Hantology: conceptual system discovery based on orthographic convention Ya-Min Chou and Chu-Ren Huang 9. What's in a schema? A formal metamodel for ECG and FrameNet Aldo Gangemi Part III. Interfacing Ontologies and Lexical Resources: 10. Interfacing ontologies and lexical resources Laurent Prevot, Stefano Borgo and Alessandro Oltramari 11. Sinica BOW (Bilingual Ontological WordNet): integration of BilingualWord-Net and SUMO Chu-Ren Huang, Ru-Yng Chang and Hsiang-bin Lee 12. Ontology-based semantic lexicons: mapping between terms and object descriptions Paul Buitelaar 13. Merging global and specialized linguistic ontologies Manuela Speranza and Bernardo Magnini Part IV. Learning and Using Ontological Knowledge: 14. The life cycle of knowledge Alessandro Lenci 15. The omega ontology Andrew Philpot, Eduard Hovy and Patrick Pantel 16. Automatic acquisition of lexico-semantic knowledge for question answering Lonneke van der Plas, Gosse Bouma and Jori Mur 17. Agricultural ontology construction and maintenance in Thai Asanee Kawtrakul and Aurawan Imsombut.",
            "referenceCount": 0,
            "citationCount": 100,
            "influentialCitationCount": 2,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Huang2010OntologyAT,\n author = {Chu-Ren Huang and N. Calzolari and Aldo Gangemi and Alessandro Lenci and A. Oltramari and Laurent Pr\u00e9vot},\n title = {Ontology and the lexicon : a natural language processing perspective},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:171581afd71133e09aa19a4fe0d6c04c6bff5890",
            "@type": "ScholarlyArticle",
            "paperId": "171581afd71133e09aa19a4fe0d6c04c6bff5890",
            "corpusId": 1383474,
            "url": "https://www.semanticscholar.org/paper/171581afd71133e09aa19a4fe0d6c04c6bff5890",
            "title": "Wikipedia-based Semantic Interpretation for Natural Language Processing",
            "venue": "Journal of Artificial Intelligence Research",
            "publicationVenue": {
                "id": "urn:research:aef12dca-60a0-4ca3-819b-cad26d309d4e",
                "name": "Journal of Artificial Intelligence Research",
                "alternate_names": [
                    "JAIR",
                    "J Artif Intell Res",
                    "The Journal of Artificial Intelligence Research"
                ],
                "issn": "1076-9757",
                "url": "http://www.jair.org/"
            },
            "year": 2009,
            "externalIds": {
                "MAG": "3001753394",
                "ArXiv": "1401.5697",
                "DBLP": "journals/corr/GabrilovichM14",
                "DOI": "10.1613/jair.2669",
                "CorpusId": 1383474
            },
            "abstract": "Adequate representation of natural language semantics requires access to vast amounts of common sense and domain-specific world knowledge. Prior work in the field was based on purely statistical techniques that did not make use of background knowledge, on limited lexicographic knowledge bases such as WordNet, or on huge manual efforts such as the CYC project. Here we propose a novel method, called Explicit Semantic Analysis (ESA), for fine-grained semantic interpretation of unrestricted natural language texts. Our method represents meaning in a high-dimensional space of concepts derived from Wikipedia, the largest encyclopedia in existence. We explicitly represent the meaning of any text in terms of Wikipedia-based concepts. We evaluate the effectiveness of our method on text categorization and on computing the degree of semantic relatedness between fragments of natural language text. Using ESA results in significant improvements over the previous state of the art in both tasks. Importantly, due to the use of natural concepts, the ESA model is easy to explain to human users.",
            "referenceCount": 154,
            "citationCount": 420,
            "influentialCitationCount": 64,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://jair.org/index.php/jair/article/download/10595/25347",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2009-03-01",
            "journal": {
                "name": "J. Artif. Intell. Res.",
                "volume": "34"
            },
            "citationStyles": {
                "bibtex": "@Article{Gabrilovich2009WikipediabasedSI,\n author = {E. Gabrilovich and Shaul Markovitch},\n booktitle = {Journal of Artificial Intelligence Research},\n journal = {J. Artif. Intell. Res.},\n pages = {443-498},\n title = {Wikipedia-based Semantic Interpretation for Natural Language Processing},\n volume = {34},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:66f9a00f1a935b99516f2932e5d12233beaddf77",
            "@type": "ScholarlyArticle",
            "paperId": "66f9a00f1a935b99516f2932e5d12233beaddf77",
            "corpusId": 26343297,
            "url": "https://www.semanticscholar.org/paper/66f9a00f1a935b99516f2932e5d12233beaddf77",
            "title": "Handbook of Natural Language Processing, Second Edition",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2010,
            "externalIds": {
                "DBLP": "reference/nlp/2010",
                "CorpusId": 26343297
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 77,
            "influentialCitationCount": 7,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{None,\n title = {Handbook of Natural Language Processing, Second Edition},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a8ca46b171467ceb2d7652fbfb67fe701ad86092",
            "@type": "ScholarlyArticle",
            "paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092",
            "corpusId": 235458009,
            "url": "https://www.semanticscholar.org/paper/a8ca46b171467ceb2d7652fbfb67fe701ad86092",
            "title": "LoRA: Low-Rank Adaptation of Large Language Models",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2021,
            "externalIds": {
                "DBLP": "conf/iclr/HuSWALWWC22",
                "ArXiv": "2106.09685",
                "CorpusId": 235458009
            },
            "abstract": "An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.",
            "referenceCount": 65,
            "citationCount": 1752,
            "influentialCitationCount": 393,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2021-06-17",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2106.09685"
            },
            "citationStyles": {
                "bibtex": "@Article{Hu2021LoRALA,\n author = {J. E. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Weizhu Chen},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {LoRA: Low-Rank Adaptation of Large Language Models},\n volume = {abs/2106.09685},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0409ab66f7604b07109019f93256756d630d3bc2",
            "@type": "ScholarlyArticle",
            "paperId": "0409ab66f7604b07109019f93256756d630d3bc2",
            "corpusId": 1994530,
            "url": "https://www.semanticscholar.org/paper/0409ab66f7604b07109019f93256756d630d3bc2",
            "title": "On Dual Decomposition and Linear Programming Relaxations for Natural Language Processing",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "publicationVenue": {
                "id": "urn:research:41bf9ed3-85b3-4c90-b015-150e31690253",
                "name": "Conference on Empirical Methods in Natural Language Processing",
                "alternate_names": [
                    "Empir Method Nat Lang Process",
                    "Empirical Methods in Natural Language Processing",
                    "Conf Empir Method Nat Lang Process",
                    "EMNLP"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/emnlp"
            },
            "year": 2010,
            "externalIds": {
                "MAG": "1812474519",
                "ACL": "D10-1001",
                "DBLP": "conf/emnlp/RushSCJ10",
                "CorpusId": 1994530
            },
            "abstract": "This paper introduces dual decomposition as a framework for deriving inference algorithms for NLP problems. The approach relies on standard dynamic-programming algorithms as oracle solvers for sub-problems, together with a simple method for forcing agreement between the different oracles. The approach provably solves a linear programming (LP) relaxation of the global inference problem. It leads to algorithms that are simple, in that they use existing decoding algorithms; efficient, that they avoid exact algorithms for the full model; and often exact, in that empirically they often recover the correct solution in spite of using an LP relaxation. We give experimental results on two problems: 1) the combination of two lexicalized parsing models; and 2) the combination of a lexicalized parsing model and a trigram part-of-speech tagger.",
            "referenceCount": 30,
            "citationCount": 188,
            "influentialCitationCount": 17,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2010-10-09",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Rush2010OnDD,\n author = {Alexander M. Rush and D. Sontag and M. Collins and T. Jaakkola},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {1-11},\n title = {On Dual Decomposition and Linear Programming Relaxations for Natural Language Processing},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:946dabbc13f06070f7618cd4ca6733a95b4b03c3",
            "@type": "ScholarlyArticle",
            "paperId": "946dabbc13f06070f7618cd4ca6733a95b4b03c3",
            "corpusId": 5705211,
            "url": "https://www.semanticscholar.org/paper/946dabbc13f06070f7618cd4ca6733a95b4b03c3",
            "title": "A linguistic ontology of space for natural language processing",
            "venue": "Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:96018464-22dc-4b5c-a172-c2f4a30ce131",
                "name": "Artificial Intelligence",
                "alternate_names": [
                    "Artif Intell"
                ],
                "issn": "0004-3702",
                "url": "http://www.elsevier.com/locate/artint"
            },
            "year": 2010,
            "externalIds": {
                "MAG": "2089967673",
                "DBLP": "journals/ai/BatemanHRT10",
                "DOI": "10.1016/j.artint.2010.05.008",
                "CorpusId": 5705211
            },
            "abstract": null,
            "referenceCount": 193,
            "citationCount": 240,
            "influentialCitationCount": 20,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2010-09-01",
            "journal": {
                "name": "Artif. Intell.",
                "volume": "174"
            },
            "citationStyles": {
                "bibtex": "@Article{Bateman2010ALO,\n author = {J. Bateman and J. Hois and R. Ross and T. Tenbrink},\n booktitle = {Artificial Intelligence},\n journal = {Artif. Intell.},\n pages = {1027-1071},\n title = {A linguistic ontology of space for natural language processing},\n volume = {174},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7e386158f474a395618c5e065ac55844b507007c",
            "@type": "ScholarlyArticle",
            "paperId": "7e386158f474a395618c5e065ac55844b507007c",
            "corpusId": 233481577,
            "url": "https://www.semanticscholar.org/paper/7e386158f474a395618c5e065ac55844b507007c",
            "title": "SUPERB: Speech processing Universal PERformance Benchmark",
            "venue": "Interspeech",
            "publicationVenue": {
                "id": "urn:research:af90489e-312f-4514-bea2-bcb399cb8ece",
                "name": "Interspeech",
                "alternate_names": [
                    "Conf Int Speech Commun Assoc",
                    "INTERSPEECH",
                    "Conference of the International Speech Communication Association"
                ],
                "issn": "2308-457X",
                "url": "https://www.isca-speech.org/iscaweb/index.php/conferences/interspeech"
            },
            "year": 2021,
            "externalIds": {
                "DBLP": "journals/corr/abs-2105-01051",
                "ArXiv": "2105.01051",
                "DOI": "10.21437/interspeech.2021-1775",
                "CorpusId": 233481577
            },
            "abstract": "Self-supervised learning (SSL) has proven vital for advancing research in natural language processing (NLP) and computer vision (CV). The paradigm pretrains a shared model on large volumes of unlabeled data and achieves state-of-the-art (SOTA) for various tasks with minimal adaptation. However, the speech processing community lacks a similar setup to systematically explore the paradigm. To bridge this gap, we introduce Speech processing Universal PERformance Benchmark (SUPERB). SUPERB is a leaderboard to benchmark the performance of a shared model across a wide range of speech processing tasks with minimal architecture changes and labeled data. Among multiple usages of the shared model, we especially focus on extracting the representation learned from SSL due to its preferable re-usability. We present a simple framework to solve SUPERB tasks by learning task-specialized lightweight prediction heads on top of the frozen shared model. Our results demonstrate that the framework is promising as SSL representations show competitive generalizability and accessibility across SUPERB tasks. We release SUPERB as a challenge with a leaderboard and a benchmark toolkit to fuel the research in representation learning and general speech processing.",
            "referenceCount": 45,
            "citationCount": 509,
            "influentialCitationCount": 79,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2105.01051",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2021-05-03",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Yang2021SUPERBSP,\n author = {Shu-wen Yang and Po-Han Chi and Yung-Sung Chuang and Cheng-I Lai and Kushal Lakhotia and Yist Y. Lin and Andy T. Liu and Jiatong Shi and Xuankai Chang and Guan-Ting Lin and Tzu-hsien Huang and W. Tseng and Ko-tik Lee and Da-Rong Liu and Zili Huang and Shuyan Dong and Shang-Wen Li and Shinji Watanabe and Abdel-rahman Mohamed and Hung-yi Lee},\n booktitle = {Interspeech},\n pages = {1194-1198},\n title = {SUPERB: Speech processing Universal PERformance Benchmark},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1326de90dd9a7db7a0c03941c696fe0c98373399",
            "@type": "ScholarlyArticle",
            "paperId": "1326de90dd9a7db7a0c03941c696fe0c98373399",
            "corpusId": 14886018,
            "url": "https://www.semanticscholar.org/paper/1326de90dd9a7db7a0c03941c696fe0c98373399",
            "title": "Active computerized pharmacovigilance using natural language processing, statistics, and electronic health records: a feasibility study.",
            "venue": "JAMIA Journal of the American Medical Informatics Association",
            "publicationVenue": {
                "id": "urn:research:5eb9af16-8aea-4524-be6c-bd3418bf7570",
                "name": "JAMIA Journal of the American Medical Informatics Association",
                "alternate_names": [
                    "AMIA Annual Symposium Proceedings",
                    "AMIA Annu Symp Proc",
                    "Journal of the American Medical Informatics Association",
                    "J Am Med Informatics Assoc",
                    "JAMIA J Am Med Informatics Assoc"
                ],
                "issn": "1067-5027",
                "url": "http://jamia.bmj.com/"
            },
            "year": 2009,
            "externalIds": {
                "MAG": "2160194473",
                "DOI": "10.1197/jamia.M3028",
                "CorpusId": 14886018,
                "PubMed": "19261932"
            },
            "abstract": "OBJECTIVE It is vital to detect the full safety profile of a drug throughout its market life. Current pharmacovigilance systems still have substantial limitations, however. The objective of our work is to demonstrate the feasibility of using natural language processing (NLP), the comprehensive Electronic Health Record (EHR), and association statistics for pharmacovigilance purposes. DESIGN Narrative discharge summaries were collected from the Clinical Information System at New York Presbyterian Hospital (NYPH). MedLEE, an NLP system, was applied to the collection to identify medication events and entities which could be potential adverse drug events (ADEs). Co-occurrence statistics with adjusted volume tests were used to detect associations between the two types of entities, to calculate the strengths of the associations, and to determine their cutoff thresholds. Seven drugs/drug classes (ibuprofen, morphine, warfarin, bupropion, paroxetine, rosiglitazone, ACE inhibitors) with known ADEs were selected to evaluate the system. RESULTS One hundred thirty-two potential ADEs were found to be associated with the 7 drugs. Overall recall and precision were 0.75 and 0.31 for known ADEs respectively. Importantly, qualitative evaluation using historic roll back design suggested that novel ADEs could be detected using our system. CONCLUSIONS This study provides a framework for the development of active, high-throughput and prospective systems which could potentially unveil drug safety profiles throughout their entire market life. Our results demonstrate that the framework is feasible although there are some challenging issues. To the best of our knowledge, this is the first study using comprehensive unstructured data from the EHR for pharmacovigilance.",
            "referenceCount": 56,
            "citationCount": 288,
            "influentialCitationCount": 10,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://academic.oup.com/jamia/article-pdf/16/3/328/2546772/16-3-328.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2009-05-01",
            "journal": {
                "name": "Journal of the American Medical Informatics Association : JAMIA",
                "volume": "16 3"
            },
            "citationStyles": {
                "bibtex": "@Article{Wang2009ActiveCP,\n author = {Xiaoyan Wang and G. Hripcsak and M. Markatou and C. Friedman},\n booktitle = {JAMIA Journal of the American Medical Informatics Association},\n journal = {Journal of the American Medical Informatics Association : JAMIA},\n pages = {\n          328-37\n        },\n title = {Active computerized pharmacovigilance using natural language processing, statistics, and electronic health records: a feasibility study.},\n volume = {16 3},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5b73be2f2373f9f6bf3d3137bcdc7b8065901746",
            "@type": "ScholarlyArticle",
            "paperId": "5b73be2f2373f9f6bf3d3137bcdc7b8065901746",
            "corpusId": 13780041,
            "url": "https://www.semanticscholar.org/paper/5b73be2f2373f9f6bf3d3137bcdc7b8065901746",
            "title": "What can natural language processing do for clinical decision support?",
            "venue": "Journal of Biomedical Informatics",
            "publicationVenue": {
                "id": "urn:research:f9827422-a381-440c-a8a4-e5e50415934e",
                "name": "Journal of Biomedical Informatics",
                "alternate_names": [
                    "J Biomed Informatics"
                ],
                "issn": "1532-0464",
                "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/622857/description#description"
            },
            "year": 2009,
            "externalIds": {
                "DBLP": "journals/jbi/Demner-FushmanCM09",
                "MAG": "2002514548",
                "DOI": "10.1016/j.jbi.2009.08.007",
                "CorpusId": 13780041,
                "PubMed": "19683066"
            },
            "abstract": null,
            "referenceCount": 127,
            "citationCount": 567,
            "influentialCitationCount": 12,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://europepmc.org/articles/pmc2757540?pdf=render",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2009-10-01",
            "journal": {
                "name": "Journal of biomedical informatics",
                "volume": "42 5"
            },
            "citationStyles": {
                "bibtex": "@Article{Demner-Fushman2009WhatCN,\n author = {Dina Demner-Fushman and W. Chapman and C. McDonald},\n booktitle = {Journal of Biomedical Informatics},\n journal = {Journal of biomedical informatics},\n pages = {\n          760-72\n        },\n title = {What can natural language processing do for clinical decision support?},\n volume = {42 5},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:abebd207b1cf56ced502b0bb203d1f231b58d699",
            "@type": "ScholarlyArticle",
            "paperId": "abebd207b1cf56ced502b0bb203d1f231b58d699",
            "corpusId": 61674660,
            "url": "https://www.semanticscholar.org/paper/abebd207b1cf56ced502b0bb203d1f231b58d699",
            "title": "A literature survey of active machine learning in the context of natural language processing",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2009,
            "externalIds": {
                "MAG": "2138517257",
                "CorpusId": 61674660
            },
            "abstract": "Active learning is a supervised machine learning technique in which the learner is in control of the data used for learning. That control is utilized by the learner to ask an oracle, typically a human with extensive knowledge of the domain at hand, about the classes of the instances for which the model learned so far makes unreliable predictions. The active learning process takes as input a set of labeled examples, as well as a larger set of unlabeled examples, and produces a classifier and a relatively small set of newly labeled data. The overall goal is to create as good a classifier as possible, without having to mark-up and supply the learner with more data than necessary. The learning process aims at keeping the human annotation effort to a minimum, only asking for advice where the training utility of the result of such a query is high. Active learning has been successfully applied to a number of natural language processing tasks, such as, information extraction, named entity recognition, text categorization, part-of-speech tagging, parsing, and word sense disambiguation. This report is a literature survey of active learning from the perspective of natural language processing.",
            "referenceCount": 87,
            "citationCount": 261,
            "influentialCitationCount": 14,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "2009-04-01",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Olsson2009ALS,\n author = {Fredrik Olsson},\n title = {A literature survey of active machine learning in the context of natural language processing},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7cbc2a7843411a1768ab762930707af0a3c33a19",
            "@type": "ScholarlyArticle",
            "paperId": "7cbc2a7843411a1768ab762930707af0a3c33a19",
            "corpusId": 246411325,
            "url": "https://www.semanticscholar.org/paper/7cbc2a7843411a1768ab762930707af0a3c33a19",
            "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2022,
            "externalIds": {
                "ArXiv": "2201.11990",
                "DBLP": "journals/corr/abs-2201-11990",
                "CorpusId": 246411325
            },
            "abstract": "Pretrained general-purpose language models can achieve state-of-the-art accuracies in various natural language processing domains by adapting to downstream tasks via zero-shot, few-shot and fine-tuning techniques. Because of their success, the size of these models has increased rapidly, requiring high-performance hardware, software, and algorithmic techniques to enable training such large models. As the result of a joint effort between Microsoft and NVIDIA, we present details on the training of the largest monolithic transformer based language model, Megatron-Turing NLG 530B (MT-NLG), with 530 billion parameters. In this paper, we first focus on the infrastructure as well as the 3D parallelism methodology used to train this model using DeepSpeed and Megatron. Next, we detail the training process, the design of our training corpus, and our data curation techniques, which we believe is a key ingredient to the success of the model. Finally, we discuss various evaluation results, as well as other interesting observations and new properties exhibited by MT-NLG. We demonstrate that MT-NLG achieves superior zero-, one-, and few-shot learning accuracies on several NLP benchmarks and establishes new state-of-the-art results. We believe that our contributions will help further the development of large-scale training infrastructures, large-scale language models, and natural language generations.",
            "referenceCount": 78,
            "citationCount": 465,
            "influentialCitationCount": 33,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2022-01-28",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2201.11990"
            },
            "citationStyles": {
                "bibtex": "@Article{Smith2022UsingDA,\n author = {Shaden Smith and M. Patwary and Brandon Norick and P. LeGresley and Samyam Rajbhandari and J. Casper and Zhun Liu and Shrimai Prabhumoye and George Zerveas and V. Korthikanti and Elton Zhang and Rewon Child and Reza Yazdani Aminabadi and J. Bernauer and Xia Song and M. Shoeybi and Yuxiong He and Michael Houston and Saurabh Tiwary and Bryan Catanzaro},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model},\n volume = {abs/2201.11990},\n year = {2022}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:75fa915984f1903cd8d0e1ea54b9d008d5a87fe5",
            "@type": "ScholarlyArticle",
            "paperId": "75fa915984f1903cd8d0e1ea54b9d008d5a87fe5",
            "corpusId": 711424,
            "url": "https://www.semanticscholar.org/paper/75fa915984f1903cd8d0e1ea54b9d008d5a87fe5",
            "title": "Natural Language Comprehension with the EpiReader",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "publicationVenue": {
                "id": "urn:research:41bf9ed3-85b3-4c90-b015-150e31690253",
                "name": "Conference on Empirical Methods in Natural Language Processing",
                "alternate_names": [
                    "Empir Method Nat Lang Process",
                    "Empirical Methods in Natural Language Processing",
                    "Conf Empir Method Nat Lang Process",
                    "EMNLP"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/emnlp"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2952719696",
                "DBLP": "journals/corr/TrischlerYYS16",
                "ACL": "D16-1013",
                "ArXiv": "1606.02270",
                "DOI": "10.18653/v1/D16-1013",
                "CorpusId": 711424
            },
            "abstract": "We present the EpiReader, a novel model for machine comprehension of text. Machine comprehension of unstructured, real-world text is a major research goal for natural language processing. Current tests of machine comprehension pose questions whose answers can be inferred from some supporting text, and evaluate a model's response to the questions. The EpiReader is an end-to-end neural model comprising two components: the first component proposes a small set of candidate answers after comparing a question to its supporting text, and the second component formulates hypotheses using the proposed candidates and the question, then reranks the hypotheses based on their estimated concordance with the supporting text. We present experiments demonstrating that the EpiReader sets a new state-of-the-art on the CNN and Children's Book Test machine comprehension benchmarks, outperforming previous neural models by a significant margin.",
            "referenceCount": 38,
            "citationCount": 95,
            "influentialCitationCount": 6,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/D16-1013.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-06-07",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Trischler2016NaturalLC,\n author = {A. Trischler and Zheng Ye and Xingdi Yuan and Philip Bachman and Alessandro Sordoni and Kaheer Suleman},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {128-137},\n title = {Natural Language Comprehension with the EpiReader},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:596c882de006e4bb4a93f1fa08a5dd467bee060a",
            "@type": "ScholarlyArticle",
            "paperId": "596c882de006e4bb4a93f1fa08a5dd467bee060a",
            "corpusId": 11004224,
            "url": "https://www.semanticscholar.org/paper/596c882de006e4bb4a93f1fa08a5dd467bee060a",
            "title": "Learning Natural Language Inference with LSTM",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:01103732-3808-4930-b8e4-7e9e68d5c68d",
                "name": "North American Chapter of the Association for Computational Linguistics",
                "alternate_names": [
                    "North Am Chapter Assoc Comput Linguistics",
                    "NAACL"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/naacl"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "2221711388",
                "ArXiv": "1512.08849",
                "DBLP": "journals/corr/WangJ15b",
                "ACL": "N16-1170",
                "DOI": "10.18653/v1/N16-1170",
                "CorpusId": 11004224
            },
            "abstract": "Natural language inference (NLI) is a fundamentally important task in natural language processing that has many applications. The recently released Stanford Natural Language Inference (SNLI) corpus has made it possible to develop and evaluate learning-centered methods such as deep neural networks for natural language inference (NLI). In this paper, we propose a special long short-term memory (LSTM) architecture for NLI. Our model builds on top of a recently proposed neural attention model for NLI but is based on a significantly different idea. Instead of deriving sentence embeddings for the premise and the hypothesis to be used for classification, our solution uses a match-LSTM to perform word-by-word matching of the hypothesis with the premise. This LSTM is able to place more emphasis on important word-level matching results. In particular, we observe that this LSTM remembers important mismatches that are critical for predicting the contradiction or the neutral relationship label. On the SNLI corpus, our model achieves an accuracy of 86.1%, outperforming the state of the art.",
            "referenceCount": 27,
            "citationCount": 406,
            "influentialCitationCount": 50,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/N16-1170.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-12-30",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1512.08849"
            },
            "citationStyles": {
                "bibtex": "@Article{Wang2015LearningNL,\n author = {Shuohang Wang and Jing Jiang},\n booktitle = {North American Chapter of the Association for Computational Linguistics},\n journal = {ArXiv},\n title = {Learning Natural Language Inference with LSTM},\n volume = {abs/1512.08849},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9f9d16e6cf929c66be59fd56d9870196c19410c1",
            "@type": "ScholarlyArticle",
            "paperId": "9f9d16e6cf929c66be59fd56d9870196c19410c1",
            "corpusId": 54132,
            "url": "https://www.semanticscholar.org/paper/9f9d16e6cf929c66be59fd56d9870196c19410c1",
            "title": "Distributed Asynchronous Online Learning for Natural Language Processing",
            "venue": "Conference on Computational Natural Language Learning",
            "publicationVenue": {
                "id": "urn:research:3779a5a7-9119-4f69-84fe-f7eef193eb49",
                "name": "Conference on Computational Natural Language Learning",
                "alternate_names": [
                    "CoNLL",
                    "Conf Comput Nat Lang Learn"
                ],
                "issn": null,
                "url": null
            },
            "year": 2010,
            "externalIds": {
                "MAG": "1554447113",
                "ACL": "W10-2925",
                "DBLP": "conf/conll/GimpelDS10",
                "CorpusId": 54132
            },
            "abstract": "Recent speed-ups for training large-scale models like those found in statistical NLP exploit distributed computing (either on multicore or \"cloud\" architectures) and rapidly converging online learning algorithms. Here we aim to combine the two. We focus on distributed, \"mini-batch\" learners that make frequent updates asynchronously (Nedic et al., 2001; Langford et al., 2009). We generalize existing asynchronous algorithms and experiment extensively with structured prediction problems from NLP, including discriminative, unsupervised, and non-convex learning scenarios. Our results show asynchronous learning can provide substantial speedups compared to distributed and single-processor mini-batch algorithms with no signs of error arising from the approximate nature of the technique.",
            "referenceCount": 31,
            "citationCount": 43,
            "influentialCitationCount": 4,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2010-07-15",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Gimpel2010DistributedAO,\n author = {Kevin Gimpel and Dipanjan Das and Noah A. Smith},\n booktitle = {Conference on Computational Natural Language Learning},\n pages = {213-222},\n title = {Distributed Asynchronous Online Learning for Natural Language Processing},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f3cf71c51b882fe3111d71c4bf104297d38197f8",
            "@type": "ScholarlyArticle",
            "paperId": "f3cf71c51b882fe3111d71c4bf104297d38197f8",
            "corpusId": 250451569,
            "url": "https://www.semanticscholar.org/paper/f3cf71c51b882fe3111d71c4bf104297d38197f8",
            "title": "Inner Monologue: Embodied Reasoning through Planning with Language Models",
            "venue": "Conference on Robot Learning",
            "publicationVenue": {
                "id": "urn:research:fbfbf10a-faa4-4d2a-85be-3ac660454ce3",
                "name": "Conference on Robot Learning",
                "alternate_names": [
                    "CoRL",
                    "Conf Robot Learn"
                ],
                "issn": null,
                "url": null
            },
            "year": 2022,
            "externalIds": {
                "DBLP": "conf/corl/HuangXXCLFZTMCS22",
                "ArXiv": "2207.05608",
                "DOI": "10.48550/arXiv.2207.05608",
                "CorpusId": 250451569
            },
            "abstract": "Recent works have shown how the reasoning capabilities of Large Language Models (LLMs) can be applied to domains beyond natural language processing, such as planning and interaction for robots. These embodied problems require an agent to understand many semantic aspects of the world: the repertoire of skills available, how these skills influence the world, and how changes to the world map back to the language. LLMs planning in embodied environments need to consider not just what skills to do, but also how and when to do them - answers that change over time in response to the agent's own choices. In this work, we investigate to what extent LLMs used in such embodied contexts can reason over sources of feedback provided through natural language, without any additional training. We propose that by leveraging environment feedback, LLMs are able to form an inner monologue that allows them to more richly process and plan in robotic control scenarios. We investigate a variety of sources of feedback, such as success detection, scene description, and human interaction. We find that closed-loop language feedback significantly improves high-level instruction completion on three domains, including simulated and real table top rearrangement tasks and long-horizon mobile manipulation tasks in a kitchen environment in the real world.",
            "referenceCount": 112,
            "citationCount": 323,
            "influentialCitationCount": 31,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2207.05608",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2022-07-12",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Huang2022InnerME,\n author = {Wenlong Huang and F. Xia and Ted Xiao and Harris Chan and Jacky Liang and Peter R. Florence and Andy Zeng and Jonathan Tompson and Igor Mordatch and Yevgen Chebotar and P. Sermanet and Noah Brown and Tomas Jackson and Linda Luu and S. Levine and Karol Hausman and Brian Ichter},\n booktitle = {Conference on Robot Learning},\n pages = {1769-1782},\n title = {Inner Monologue: Embodied Reasoning through Planning with Language Models},\n year = {2022}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:30fd719d5c68233d29f30709dd9de5e0756e20bd",
            "@type": "ScholarlyArticle",
            "paperId": "30fd719d5c68233d29f30709dd9de5e0756e20bd",
            "corpusId": 45036122,
            "url": "https://www.semanticscholar.org/paper/30fd719d5c68233d29f30709dd9de5e0756e20bd",
            "title": "Opportunities for Natural Language Processing Research in Education",
            "venue": "Conference on Intelligent Text Processing and Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:a1c1b2d2-5629-4b8f-b764-20ea10fa588c",
                "name": "Conference on Intelligent Text Processing and Computational Linguistics",
                "alternate_names": [
                    "Conf Intell Text Process Comput Linguistics",
                    "CICLing"
                ],
                "issn": null,
                "url": "http://www.cicling.org/"
            },
            "year": 2009,
            "externalIds": {
                "DBLP": "conf/cicling/Burstein09",
                "MAG": "1835617890",
                "DOI": "10.1007/978-3-642-00382-0_2",
                "CorpusId": 45036122
            },
            "abstract": null,
            "referenceCount": 63,
            "citationCount": 60,
            "influentialCitationCount": 6,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Education",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2009-02-17",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Burstein2009OpportunitiesFN,\n author = {J. Burstein},\n booktitle = {Conference on Intelligent Text Processing and Computational Linguistics},\n pages = {6-27},\n title = {Opportunities for Natural Language Processing Research in Education},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:30ab03a7e1e53dd209bca35e913c1094b37daef1",
            "@type": "ScholarlyArticle",
            "paperId": "30ab03a7e1e53dd209bca35e913c1094b37daef1",
            "corpusId": 16896190,
            "url": "https://www.semanticscholar.org/paper/30ab03a7e1e53dd209bca35e913c1094b37daef1",
            "title": "Applications of Weighted Automata in Natural Language Processing",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2009,
            "externalIds": {
                "MAG": "1559990457",
                "DOI": "10.1007/978-3-642-01492-5_14",
                "CorpusId": 16896190
            },
            "abstract": null,
            "referenceCount": 71,
            "citationCount": 55,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Knight2009ApplicationsOW,\n author = {Kevin Knight and Jonathan May},\n pages = {571-596},\n title = {Applications of Weighted Automata in Natural Language Processing},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7affb61f07e69097a8685658bd67763711c02980",
            "@type": "ScholarlyArticle",
            "paperId": "7affb61f07e69097a8685658bd67763711c02980",
            "corpusId": 296856,
            "url": "https://www.semanticscholar.org/paper/7affb61f07e69097a8685658bd67763711c02980",
            "title": "Classical Approaches to Natural Language Processing",
            "venue": "Handbook of Natural Language Processing",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2010,
            "externalIds": {
                "MAG": "2404978784",
                "DBLP": "reference/nlp/Dale10",
                "DOI": "10.1201/9781420085938-c1",
                "CorpusId": 296856
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 24,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Dale2010ClassicalAT,\n author = {R. Dale},\n booktitle = {Handbook of Natural Language Processing},\n pages = {3-7},\n title = {Classical Approaches to Natural Language Processing},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:537d749988760e645fcc961afdeb5b900ee15810",
            "@type": "ScholarlyArticle",
            "paperId": "537d749988760e645fcc961afdeb5b900ee15810",
            "corpusId": 261727508,
            "url": "https://www.semanticscholar.org/paper/537d749988760e645fcc961afdeb5b900ee15810",
            "title": "Information Retrieval in Biomedicine: Natural Language Processing for Knowledge Integration",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2009,
            "externalIds": {
                "MAG": "2165748107",
                "DOI": "10.4018/978-1-60566-274-9",
                "CorpusId": 261727508
            },
            "abstract": "Today, there is an intense interest for bio natural language processing (NLP) creating a need among researchers, academicians, and practitioners for a comprehensive publication of articles in this area. Information Retrieval in Biomedicine: Natural Language Processing for Knowledge Integration provides relevant theoretical frameworks and the latest empirical research findings in this area according to a linguistic granularity. As a critical mass of advanced knowledge, this book presents original applications, going beyond existing publications while opening up the road for a broader use of NLP in biomedicine.",
            "referenceCount": 0,
            "citationCount": 59,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2009-02-27",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Prince2009InformationRI,\n author = {V. Prince and Mathieu Roche},\n title = {Information Retrieval in Biomedicine: Natural Language Processing for Knowledge Integration},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:bb0c9a6f8b7506bb60c097e26bedb2845609484c",
            "@type": "ScholarlyArticle",
            "paperId": "bb0c9a6f8b7506bb60c097e26bedb2845609484c",
            "corpusId": 27787263,
            "url": "https://www.semanticscholar.org/paper/bb0c9a6f8b7506bb60c097e26bedb2845609484c",
            "title": "Joint Inference for Natural Language Processing",
            "venue": "Conference on Computational Natural Language Learning",
            "publicationVenue": {
                "id": "urn:research:3779a5a7-9119-4f69-84fe-f7eef193eb49",
                "name": "Conference on Computational Natural Language Learning",
                "alternate_names": [
                    "CoNLL",
                    "Conf Comput Nat Lang Learn"
                ],
                "issn": null,
                "url": null
            },
            "year": 2009,
            "externalIds": {
                "MAG": "1993924277",
                "DBLP": "conf/conll/McCallum09",
                "ACL": "W09-1101",
                "DOI": "10.3115/1596374.1596376",
                "CorpusId": 27787263
            },
            "abstract": "In recent decades, researchers in natural language processing have made great progress on well-defined subproblems such as part-of-speech tagging, phrase chunking, syntactic parsing, named-entity recognition, coreference and semantic-role labeling. Better models, features, and learning algorithms have allowed systems to perform many of these tasks with 90% accuracy or better. However, success in integrated, end-to-end natural language understanding remains elusive. \n \nI contend that the chief reason for this failure is that errors cascade and accumulate through a pipeline of naively chained components. For example, if we naively use the single most likely output of a part-of-speech tagger as the input to a syntactic parser, and those parse trees as the input to a coreference system, and so on, errors in each step will propagate to later ones: each components 90% accuracy multiplied through six components becomes only 53%. \n \nConsider, for instance, the sentence \"I know you like your mother.\" If a part-of-speech tagger deterministically labels \"like\" as a verb, then certain later syntactic and semantic analysis will be blocked from alternative interpretations, such as \"I know you like your mother (does).\" The part-of-speech tagger needs more syntactic and semantic information to make this choice. Consider also the classic example \"The boy saw the man with the telescope.\" No single correct syntactic parse of this sentence is possible in isolation. Correct interpretation requires the integration of these syntactic decisions with semantics and context. \n \nHumans manage and resolve ambiguity by unified, simultaneous consideration of morphology, syntax, semantics, pragmatics and other contextual information. In statistical modeling such unified consideration is known as joint inference. The need for joint inference appears not only in natural language processing, but also in information integration, computer vision, robotics and elsewhere. All of these applications require integrating evidence from multiple sources, at multiple levels of abstraction. I believe that joint inference is one of the most fundamentally central issues in all of artificial intelligence. \n \nIn this talk I will describe work in probabilistic models that perform joint inference across multiple components of an information processing pipeline in order to avoid the brittle accumulation of errors. I will survey work in exact inference, variational inference and Markov-chain Monte Carlo methods. We will discuss various approaches that have been applied to natural language processing, and hypothesize about why joint inference has helped in some cases, and not in others. \n \nI will then focus on our recent work at University of Massachusetts in large-scale conditional random fields with complex relational structure. In a single factor graph we seamlessly integrate multiple subproblems, using our new probabilistic programming language to compactly express complex, mutable variable-factor structure both in first-order logic as well as in more expressive Turing-complete imperative procedures. We avoid unrolling this graphical model by using Markov-chain Monte Carlo for inference, and make inference more efficient with learned proposal distributions. Parameter estimation is performed by SampleRank, which avoids complete inference as a subroutine by learning simply to correctly rank successive states of the Markov-chain. \n \nJoint work with Aron Culotta, Michael Wick, Rob Hall, Khashayar Rohanimanesh, Karl Schultz, Sameer Singh, Charles Sutton and David Smith.",
            "referenceCount": 0,
            "citationCount": 22,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2009-06-04",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{McCallum2009JointIF,\n author = {A. McCallum},\n booktitle = {Conference on Computational Natural Language Learning},\n pages = {1},\n title = {Joint Inference for Natural Language Processing},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:75669c9df94f992985ac3318a744c4f3feb132ef",
            "@type": "ScholarlyArticle",
            "paperId": "75669c9df94f992985ac3318a744c4f3feb132ef",
            "corpusId": 6222768,
            "url": "https://www.semanticscholar.org/paper/75669c9df94f992985ac3318a744c4f3feb132ef",
            "title": "Do Multi-Sense Embeddings Improve Natural Language Understanding?",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "publicationVenue": {
                "id": "urn:research:41bf9ed3-85b3-4c90-b015-150e31690253",
                "name": "Conference on Empirical Methods in Natural Language Processing",
                "alternate_names": [
                    "Empir Method Nat Lang Process",
                    "Empirical Methods in Natural Language Processing",
                    "Conf Empir Method Nat Lang Process",
                    "EMNLP"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/emnlp"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "2952802852",
                "ACL": "D15-1200",
                "DBLP": "journals/corr/LiJ15a",
                "ArXiv": "1506.01070",
                "DOI": "10.18653/v1/D15-1200",
                "CorpusId": 6222768
            },
            "abstract": "Learning a distinct representation for each sense of an ambiguous word could lead to more powerful and fine-grained models of vector-space representations. Yet while \u2018multi-sense\u2019 methods have been proposed and tested on artificial wordsimilarity tasks, we don\u2019t know if they improve real natural language understanding tasks. In this paper we introduce a multisense embedding model based on Chinese Restaurant Processes that achieves state of the art performance on matching human word similarity judgments, and propose a pipelined architecture for incorporating multi-sense embeddings into language understanding. We then test the performance of our model on part-of-speech tagging, named entity recognition, sentiment analysis, semantic relation identification and semantic relatedness, controlling for embedding dimensionality. We find that multi-sense embeddings do improve performance on some tasks (part-of-speech tagging, semantic relation identification, semantic relatedness) but not on others (named entity recognition, various forms of sentiment analysis). We discuss how these differences may be caused by the different role of word sense information in each of the tasks. The results highlight the importance of testing embedding models in real applications.",
            "referenceCount": 38,
            "citationCount": 230,
            "influentialCitationCount": 25,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.18653/v1/d15-1200",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-06-02",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Li2015DoME,\n author = {Jiwei Li and Dan Jurafsky},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {1722-1732},\n title = {Do Multi-Sense Embeddings Improve Natural Language Understanding?},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:310b72fbc3d384ca88ca994b33476b8a2be2e27f",
            "@type": "ScholarlyArticle",
            "paperId": "310b72fbc3d384ca88ca994b33476b8a2be2e27f",
            "corpusId": 908212,
            "url": "https://www.semanticscholar.org/paper/310b72fbc3d384ca88ca994b33476b8a2be2e27f",
            "title": "Sentiment analyzer: extracting sentiments about a given topic using natural language processing techniques",
            "venue": "Third IEEE International Conference on Data Mining",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2003,
            "externalIds": {
                "DBLP": "conf/icdm/YiNBN03",
                "MAG": "2126854223",
                "DOI": "10.1109/ICDM.2003.1250949",
                "CorpusId": 908212
            },
            "abstract": "We present sentiment analyzer (SA) that extracts sentiment (or opinion) about a subject from online text documents. Instead of classifying the sentiment of an entire document about a subject, SA detects all references to the given subject, and determines sentiment in each of the references using natural language processing (NLP) techniques. Our sentiment analysis consists of 1) a topic specific feature term extraction, 2) sentiment extraction, and 3) (subject, sentiment) association by relationship analysis. SA utilizes two linguistic resources for the analysis: the sentiment lexicon and the sentiment pattern database. The performance of the algorithms was verified on online product review articles (\"digital camera\" and \"music\" reviews), and more general documents including general Webpages and news articles.",
            "referenceCount": 27,
            "citationCount": 745,
            "influentialCitationCount": 31,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference",
                "Review"
            ],
            "publicationDate": "2003-11-19",
            "journal": {
                "name": "Third IEEE International Conference on Data Mining",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Yi2003SentimentAE,\n author = {Jeonghee Yi and Tetsuya Nasukawa and Razvan C. Bunescu and W. Niblack},\n booktitle = {Third IEEE International Conference on Data Mining},\n journal = {Third IEEE International Conference on Data Mining},\n pages = {427-434},\n title = {Sentiment analyzer: extracting sentiments about a given topic using natural language processing techniques},\n year = {2003}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4dabd6182ce2681c758f654561d351739e8df7bf",
            "@type": "ScholarlyArticle",
            "paperId": "4dabd6182ce2681c758f654561d351739e8df7bf",
            "corpusId": 384520,
            "url": "https://www.semanticscholar.org/paper/4dabd6182ce2681c758f654561d351739e8df7bf",
            "title": "Multilingual Language Processing From Bytes",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:01103732-3808-4930-b8e4-7e9e68d5c68d",
                "name": "North American Chapter of the Association for Computational Linguistics",
                "alternate_names": [
                    "North Am Chapter Assoc Comput Linguistics",
                    "NAACL"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/naacl"
            },
            "year": 2015,
            "externalIds": {
                "ACL": "N16-1155",
                "ArXiv": "1512.00103",
                "DBLP": "journals/corr/GillickBVS15",
                "MAG": "2949777753",
                "DOI": "10.18653/v1/N16-1155",
                "CorpusId": 384520
            },
            "abstract": "We describe an LSTM-based model which we call Byte-to-Span (BTS) that reads text as bytes and outputs span annotations of the form [start, length, label] where start positions, lengths, and labels are separate entries in our vocabulary. Because we operate directly on unicode bytes rather than language-specific words or characters, we can analyze text in many languages with a single model. Due to the small vocabulary size, these multilingual models are very compact, but produce results similar to or better than the state-of- the-art in Part-of-Speech tagging and Named Entity Recognition that use only the provided training datasets (no external data sources). Our models are learning \"from scratch\" in that they do not rely on any elements of the standard pipeline in Natural Language Processing (including tokenization), and thus can run in standalone fashion on raw text.",
            "referenceCount": 38,
            "citationCount": 216,
            "influentialCitationCount": 21,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/N16-1155.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-12-01",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1512.00103"
            },
            "citationStyles": {
                "bibtex": "@Article{Gillick2015MultilingualLP,\n author = {D. Gillick and Clifford Brunk and Oriol Vinyals and A. Subramanya},\n booktitle = {North American Chapter of the Association for Computational Linguistics},\n journal = {ArXiv},\n title = {Multilingual Language Processing From Bytes},\n volume = {abs/1512.00103},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0165568bcc1a819c18564567f2ec15d859be2519",
            "@type": "ScholarlyArticle",
            "paperId": "0165568bcc1a819c18564567f2ec15d859be2519",
            "corpusId": 7008675,
            "url": "https://www.semanticscholar.org/paper/0165568bcc1a819c18564567f2ec15d859be2519",
            "title": "Cheap and Fast \u2013 But is it Good? Evaluating Non-Expert Annotations for Natural Language Tasks",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "publicationVenue": {
                "id": "urn:research:41bf9ed3-85b3-4c90-b015-150e31690253",
                "name": "Conference on Empirical Methods in Natural Language Processing",
                "alternate_names": [
                    "Empir Method Nat Lang Process",
                    "Empirical Methods in Natural Language Processing",
                    "Conf Empir Method Nat Lang Process",
                    "EMNLP"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/emnlp"
            },
            "year": 2008,
            "externalIds": {
                "ACL": "D08-1027",
                "DBLP": "conf/emnlp/SnowOJN08",
                "MAG": "1970381522",
                "DOI": "10.3115/1613715.1613751",
                "CorpusId": 7008675
            },
            "abstract": "Human linguistic annotation is crucial for many natural language processing tasks but can be expensive and time-consuming. We explore the use of Amazon's Mechanical Turk system, a significantly cheaper and faster method for collecting annotations from a broad base of paid non-expert contributors over the Web. We investigate five tasks: affect recognition, word similarity, recognizing textual entailment, event temporal ordering, and word sense disambiguation. For all five, we show high agreement between Mechanical Turk non-expert annotations and existing gold standard labels provided by expert labelers. For the task of affect recognition, we also show that using non-expert labels for training machine learning algorithms can be as effective as using gold standard annotations from experts. We propose a technique for bias correction that significantly improves annotation quality on two tasks. We conclude that many large labeling tasks can be effectively designed and carried out in this method at a fraction of the usual expense.",
            "referenceCount": 35,
            "citationCount": 2290,
            "influentialCitationCount": 175,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://dl.acm.org/ft_gateway.cfm?id=1613751&type=pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2008-10-25",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Snow2008CheapAF,\n author = {R. Snow and Brendan T. O'Connor and Dan Jurafsky and A. Ng},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {254-263},\n title = {Cheap and Fast \u2013 But is it Good? Evaluating Non-Expert Annotations for Natural Language Tasks},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5c771f98283c8fab333a91bc02901a8c08b7059e",
            "@type": "ScholarlyArticle",
            "paperId": "5c771f98283c8fab333a91bc02901a8c08b7059e",
            "corpusId": 17757564,
            "url": "https://www.semanticscholar.org/paper/5c771f98283c8fab333a91bc02901a8c08b7059e",
            "title": "Natural Language Processing for Conceptual Modeling",
            "venue": "J. Digit. Content Technol. its Appl.",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2009,
            "externalIds": {
                "DBLP": "journals/jdcta/Al-Safadi09",
                "MAG": "2024070962",
                "DOI": "10.4156/JDCTA.VOL3.ISSUE3.6",
                "CorpusId": 17757564
            },
            "abstract": "A semi-automated approach for the design of databases in enhanced-ERD notation is presented. It focuses on the very early stage of the database development which is the stage of user requirement analysis. It is supposed to be used between the requirements determination stage and analysis. The approach provides the opportunity of using natural language text documents as a source of knowledge for semi-automated generation of a conceptual data model. The system performs information extraction by parsing the syntax of the sentences and semantically analyzing their content.",
            "referenceCount": 23,
            "citationCount": 39,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": "J. Digit. Content Technol. its Appl.",
                "volume": "3"
            },
            "citationStyles": {
                "bibtex": "@Article{Al-Safadi2009NaturalLP,\n author = {L. Al-Safadi},\n booktitle = {J. Digit. Content Technol. its Appl.},\n journal = {J. Digit. Content Technol. its Appl.},\n pages = {47-59},\n title = {Natural Language Processing for Conceptual Modeling},\n volume = {3},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7c074685d50bcf727c26d98241200e38df2b1731",
            "@type": "ScholarlyArticle",
            "paperId": "7c074685d50bcf727c26d98241200e38df2b1731",
            "corpusId": 38824611,
            "url": "https://www.semanticscholar.org/paper/7c074685d50bcf727c26d98241200e38df2b1731",
            "title": "Speech and language processing: an introduction to natural language processing, computational linguistics, and speech recognition, 2nd Edition",
            "venue": "Prentice Hall series in artificial intelligence",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2009,
            "externalIds": {
                "DBLP": "books/lib/JurafskyM09",
                "CorpusId": 38824611
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 115,
            "influentialCitationCount": 6,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Jurafsky2009SpeechAL,\n author = {Dan Jurafsky and James H. Martin},\n booktitle = {Prentice Hall series in artificial intelligence},\n pages = {1-1024},\n title = {Speech and language processing: an introduction to natural language processing, computational linguistics, and speech recognition, 2nd Edition},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0d459e3be20f7f529bc0d92d42fa63e60fc1e1ba",
            "@type": "ScholarlyArticle",
            "paperId": "0d459e3be20f7f529bc0d92d42fa63e60fc1e1ba",
            "corpusId": 6914823,
            "url": "https://www.semanticscholar.org/paper/0d459e3be20f7f529bc0d92d42fa63e60fc1e1ba",
            "title": "Detection of Duplicate Defect Reports Using Natural Language Processing",
            "venue": "International Conference on Software Engineering",
            "publicationVenue": {
                "id": "urn:research:a36dc29e-4ea1-4567-b0fe-1c06daf8bee8",
                "name": "International Conference on Software Engineering",
                "alternate_names": [
                    "Int Conf Softw Eng",
                    "ICSE"
                ],
                "issn": null,
                "url": "http://www.icse-conferences.org/"
            },
            "year": 2007,
            "externalIds": {
                "DBLP": "conf/icse/RunesonAN07",
                "MAG": "2165022036",
                "DOI": "10.1109/ICSE.2007.32",
                "CorpusId": 6914823
            },
            "abstract": "Defect reports are generated from various testing and development activities in software engineering. Sometimes two reports are submitted that describe the same problem, leading to duplicate reports. These reports are mostly written in structured natural language, and as such, it is hard to compare two reports for similarity with formal methods. In order to identify duplicates, we investigate using natural language processing (NLP) techniques to support the identification. A prototype tool is developed and evaluated in a case study analyzing defect reports at Sony Ericsson mobile communications. The evaluation shows that about 2/3 of the duplicates can possibly be found using the NLP techniques. Different variants of the techniques provide only minor result differences, indicating a robust technology. User testing shows that the overall attitude towards the technique is positive and that it has a growth potential.",
            "referenceCount": 20,
            "citationCount": 545,
            "influentialCitationCount": 51,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2007-05-24",
            "journal": {
                "name": "29th International Conference on Software Engineering (ICSE'07)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Runeson2007DetectionOD,\n author = {P. Runeson and Magnus Alexandersson and Oskar Nyholm},\n booktitle = {International Conference on Software Engineering},\n journal = {29th International Conference on Software Engineering (ICSE'07)},\n pages = {499-510},\n title = {Detection of Duplicate Defect Reports Using Natural Language Processing},\n year = {2007}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:187c25de08261760cf48b4e9dabf308d2f7f15d9",
            "@type": "ScholarlyArticle",
            "paperId": "187c25de08261760cf48b4e9dabf308d2f7f15d9",
            "corpusId": 17364624,
            "url": "https://www.semanticscholar.org/paper/187c25de08261760cf48b4e9dabf308d2f7f15d9",
            "title": "Reasoning about Quantities in Natural Language",
            "venue": "Transactions of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:e0dbf116-86aa-418d-859f-a49952d7e44a",
                "name": "Transactions of the Association for Computational Linguistics",
                "alternate_names": [
                    "Trans Assoc Comput Linguistics",
                    "TACL"
                ],
                "issn": "2307-387X",
                "url": "https://www.mitpressjournals.org/loi/tacl"
            },
            "year": 2015,
            "externalIds": {
                "DBLP": "journals/tacl/RoyVR15",
                "MAG": "1539746312",
                "ACL": "Q15-1001",
                "DOI": "10.1162/tacl_a_00118",
                "CorpusId": 17364624
            },
            "abstract": "Little work from the Natural Language Processing community has targeted the role of quantities in Natural Language Understanding. This paper takes some key steps towards facilitating reasoning about quantities expressed in natural language. We investigate two different tasks of numerical reasoning. First, we consider Quantity Entailment, a new task formulated to understand the role of quantities in general textual inference tasks. Second, we consider the problem of automatically understanding and solving elementary school math word problems. In order to address these quantitative reasoning problems we first develop a computational approach which we show to successfully recognize and normalize textual expressions of quantities. We then use these capabilities to further develop algorithms to assist reasoning in the context of the aforementioned tasks.",
            "referenceCount": 53,
            "citationCount": 129,
            "influentialCitationCount": 19,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00118",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-01-09",
            "journal": {
                "name": "Transactions of the Association for Computational Linguistics",
                "volume": "3"
            },
            "citationStyles": {
                "bibtex": "@Article{Roy2015ReasoningAQ,\n author = {Subhro Roy and Tim Vieira and D. Roth},\n booktitle = {Transactions of the Association for Computational Linguistics},\n journal = {Transactions of the Association for Computational Linguistics},\n pages = {1-13},\n title = {Reasoning about Quantities in Natural Language},\n volume = {3},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8fc90855d400f55387d775bd780d51041cf83253",
            "@type": "ScholarlyArticle",
            "paperId": "8fc90855d400f55387d775bd780d51041cf83253",
            "corpusId": 62282940,
            "url": "https://www.semanticscholar.org/paper/8fc90855d400f55387d775bd780d51041cf83253",
            "title": "NATURAL LANGUAGE QUERY PROCESSING USING SEMANTIC GRAMMAR",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2010,
            "externalIds": {
                "MAG": "2337398535",
                "CorpusId": 62282940
            },
            "abstract": "The field of natural language processing (NLP) has seen a dramatic shift in both research direction and methodology in the past several years. In the past, most work in computational linguistics tended to focus on purely symbolic methods. Recently, more and more work is shifting toward hybrid methods that combine new empirical corpus-based methods, including the use of probabilistic and information theoretic techniques, with traditional symbolic methods. The main purpose of Natural Language Query Processing is for an English sentence to be interpreted by the computer and appropriate action taken. Asking questions to databases in natural language is a very convenient and easy method of data access, especially for casual users who do not understand complicated database query languages such as SQL. This paper proposes the architecture for translating English Query into SQL using Semantic Grammar",
            "referenceCount": 20,
            "citationCount": 45,
            "influentialCitationCount": 2,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Rao2010NATURALLQ,\n author = {G. Rao and Chanchal Agarwal and S. Chaudhry and Nikita J. Kulkarni},\n title = {NATURAL LANGUAGE QUERY PROCESSING USING SEMANTIC GRAMMAR},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:02b3e1a74d6f09a467823ade1460fb950b7f609b",
            "@type": "ScholarlyArticle",
            "paperId": "02b3e1a74d6f09a467823ade1460fb950b7f609b",
            "corpusId": 64734770,
            "url": "https://www.semanticscholar.org/paper/02b3e1a74d6f09a467823ade1460fb950b7f609b",
            "title": "Proceedings of the Conference on Empirical Methods in Natural Language Processing",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2008,
            "externalIds": {
                "MAG": "2611067198",
                "CorpusId": 64734770
            },
            "abstract": "Welcome to the 2008 Conference on Empirical Methods in Natural Language Processing! The conference is organized under the auspices of SIGDAT, the ACL Special Interest Group for linguistic data and corpus-based approaches to natural language processing. It is co-located this year with AMTA 2008 and the International Workshop on Spoken Language Translation, in Honolulu, Hawaii. \n \nEMNLP received 385 submissions. We were able to accept 116 papers in total (an acceptance rate of 30%). 81 of the papers (21%) were accepted for oral presentation, and 35 (9%) for poster presentation. Two poster papers were subsequently withdrawn after acceptance. The papers were selected by a program committee of 15 area chairs, from Asia, Europe, and North America, assisted by a panel of 339 reviewers. This year EMNLP introduced an author response period. Authors were able to read and respond to the reviews of their paper before the program committee made a final decision. They were asked to correct factual errors in the reviews and answer questions raised in the reviewer comments. The intention was to help produce more accurate reviews. In some cases, reviewers changed their scores in view of the authors' response and the area chairs read all responses carefully prior to making recommendations for acceptance.",
            "referenceCount": 0,
            "citationCount": 50,
            "influentialCitationCount": 2,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "2008-10-25",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Lapata2008ProceedingsOT,\n author = {Mirella Lapata and H. Ng},\n title = {Proceedings of the Conference on Empirical Methods in Natural Language Processing},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c4fcd220df00b881c1dcce32191fb4631e409045",
            "@type": "ScholarlyArticle",
            "paperId": "c4fcd220df00b881c1dcce32191fb4631e409045",
            "corpusId": 38930970,
            "url": "https://www.semanticscholar.org/paper/c4fcd220df00b881c1dcce32191fb4631e409045",
            "title": "Conceptual Model Generation from Requirements Model: A Natural Language Processing Approach",
            "venue": "International Conference on Applications of Natural Language to Data Bases",
            "publicationVenue": {
                "id": "urn:research:7c0b75bf-65e3-4094-9d72-e8f59ebb154d",
                "name": "International Conference on Applications of Natural Language to Data Bases",
                "alternate_names": [
                    "Appl Nat Lang Data Base",
                    "Int Conf Appl Nat Lang Data Base",
                    "Applications of Natural Language to Data Bases",
                    "NLDB"
                ],
                "issn": null,
                "url": "http://www.nldb.org/"
            },
            "year": 2008,
            "externalIds": {
                "DBLP": "conf/nldb/MontesPEP08",
                "MAG": "1494487217",
                "DOI": "10.1007/978-3-540-69858-6_32",
                "CorpusId": 38930970
            },
            "abstract": null,
            "referenceCount": 4,
            "citationCount": 40,
            "influentialCitationCount": 2,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2008-06-24",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Rend\u00f3n2008ConceptualMG,\n author = {Azucena Montes Rend\u00f3n and Hasdai Pacheco and Hugo Estrada and \u00d3. Pastor},\n booktitle = {International Conference on Applications of Natural Language to Data Bases},\n pages = {325-326},\n title = {Conceptual Model Generation from Requirements Model: A Natural Language Processing Approach},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:503043efb213996245c818b12ba8b1edebd3ada0",
            "@type": "ScholarlyArticle",
            "paperId": "503043efb213996245c818b12ba8b1edebd3ada0",
            "corpusId": 18656685,
            "url": "https://www.semanticscholar.org/paper/503043efb213996245c818b12ba8b1edebd3ada0",
            "title": "Speech and Language Processing An Introduction to Natural Language Processing , Computational Linguistics , and Speech Recognition Second Edition",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2008,
            "externalIds": {
                "CorpusId": 18656685
            },
            "abstract": "Dave Bowman: Open the pod bay doors, HAL. HAL: I'm sorry Dave, I'm afraid I can't do that. The idea of giving computers the ability to process human language is as old as the idea of computers themselves. This book is about the implementation and implications of that exciting idea. We introduce a vibrant interdisciplinary field with many names corresponding to its many facets, names like speech and language processing, human language technology, natural language processing, computational linguistics, and speech recognition and synthesis. The goal of this new field is to get computers to perform useful tasks involving human language, tasks like enabling human-machine communication, improving human-human communication, or simply doing useful processing of text or speech. One example of a useful such task is a conversational agent. The HAL 9000 com-Conversational agent puter in Stanley Kubrick's film 2001: A Space Odyssey is one of the most recognizable characters in 20th century cinema. HAL is an artificial agent capable of such advanced language behavior as speaking and understanding English, and at a crucial moment in the plot, even reading lips. It is now clear that HAL's creator, Arthur C. Clarke, was a little optimistic in predicting when an artificial agent such as HAL would be available. But just how far off was he? What would it take to create at least the language-related parts of HAL? We call programs like HAL that converse with humans in natural language conversational agents or dialogue systems. In this text we study the vari-Dialogue system ous components that make up modern conversational agents, including language input (automatic speech recognition and natural language understanding) and language output (dialogue and response planning and speech synthesis). Let's turn to another useful language-related task, that of making available to non-English-speaking readers the vast amount of scientific information on the Web in En-glish. Or translating for English speakers the hundreds of millions of Web pages written in other languages like Chinese. The goal of machine translation is to automatically Machine translation translate a document from one language to another. We introduce the algorithms and mathematical tools needed to understand how modern machine translation works. Machine translation is far from a solved problem; we cover the algorithms currently used in the field, as well as important component tasks. Many other language processing tasks are also related to the Web. Another such task is Web-based question answering. This is a generalization \u2026",
            "referenceCount": 0,
            "citationCount": 55,
            "influentialCitationCount": 3,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Jurafsky2008SpeechAL,\n author = {Dan Jurafsky and James H. Martin},\n title = {Speech and Language Processing An Introduction to Natural Language Processing , Computational Linguistics , and Speech Recognition Second Edition},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e26d4bcc087aa670700384126b755422b2fe828e",
            "@type": "ScholarlyArticle",
            "paperId": "e26d4bcc087aa670700384126b755422b2fe828e",
            "corpusId": 261614370,
            "url": "https://www.semanticscholar.org/paper/e26d4bcc087aa670700384126b755422b2fe828e",
            "title": "Domain Adaptation in Natural Language Processing",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2008,
            "externalIds": {
                "MAG": "1552919843",
                "DBLP": "phd/us/Jiang08",
                "CorpusId": 261614370
            },
            "abstract": "With the fast growth of the amount of digitalized texts in recent years, text information management becomes increasingly important in people's daily life. Natural language processing provides the foundation of many modern text information management technologies. For many natural language processing tasks, the state-of-the-art solutions are based on supervised statistical machine learning methods, which require large manually annotated corpora. However, the variations of text in vocabulary, format, style, etc. in different domains and the large amount of human efforts needed to create labeled training data make it practically infeasible to directly apply supervised machine learning methods to natural language processing tasks in new domains. There is therefore a great need to develop special learning algorithms and techniques to adapt classifiers trained on some old domains to a different but related new domain. \nThis thesis aims at understanding the domain adaptation problem and developing general learning techniques for solving the problem. To understand domain adaptation, a formal analysis is conducted from different perspectives. First, we look at the intrinsic distributional difference between two domains, which leads to an instance weighting solution to domain adaptation. Second, we look at the the extrinsic functional difference between the optimal classifiers for two domains, which leads to a feature selection solution to domain adaptation. Third, we distinguish the domain difference that comes from the old training domain from the difference that comes from the new test domain, and accordingly propose that domain adaptation should consist of two stages. \nThe instance weighting and feature selection solutions are formally developed into two general and principled frameworks for domain adaptation. Both frameworks modify the objective function of the standard risk minimization framework for supervised learning, and include standard supervised learning and semi-supervised learning as special cases. Evaluation of the two frameworks on a number of natural language processing tasks using real data sets demonstrates the effectiveness of the domain adaptation techniques incorporated in the frameworks compared with standard supervised and semi-supervised learning. \nObserving that the effectiveness of different domain adaptation techniques varies from data set to data set, we also study different types of domain adaptation and their associations with different domain adaptation techniques. Using perturbed real data sets, we are able to show that different types of domain difference indeed require different domain adaptation techniques. This analysis deepens our understanding of domain adaptation, and potentially helps us select the appropriate techniques for particular domain adaptation problems. \nAlthough we focus on domain adaptation in natural language processing in this thesis, most of the analysis of the problem and the proposed domain adaptation techniques are not restricted to natural language processing problems but can be generally applied to most classification tasks when the training and the test domains differ.",
            "referenceCount": 52,
            "citationCount": 44,
            "influentialCitationCount": 3,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Jiang2008DomainAI,\n author = {Jing Jiang},\n title = {Domain Adaptation in Natural Language Processing},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6ed7d96188fc9c56d93db7eb05f868a8e3469642",
            "@type": "ScholarlyArticle",
            "paperId": "6ed7d96188fc9c56d93db7eb05f868a8e3469642",
            "corpusId": 60721833,
            "url": "https://www.semanticscholar.org/paper/6ed7d96188fc9c56d93db7eb05f868a8e3469642",
            "title": "Review of Speech and language processing: an introduction to natural language processing, computational linguistics, and speech recognition by Daniel Jurafsky and James H. Martin. Prentice Hall 2000.",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2000,
            "externalIds": {
                "MAG": "1572130671",
                "CorpusId": 60721833
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 775,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "2000-12-01",
            "journal": {
                "name": "Computational Linguistics",
                "volume": "26"
            },
            "citationStyles": {
                "bibtex": "@Article{Reviewer-Teller2000ReviewOS,\n author = {Virginia Reviewer-Teller},\n journal = {Computational Linguistics},\n pages = {638-641},\n title = {Review of Speech and language processing: an introduction to natural language processing, computational linguistics, and speech recognition by Daniel Jurafsky and James H. Martin. Prentice Hall 2000.},\n volume = {26},\n year = {2000}\n}\n"
            }
        }
    }
]