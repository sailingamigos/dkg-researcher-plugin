[
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2",
            "@type": "ScholarlyArticle",
            "paperId": "af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2",
            "corpusId": 208117506,
            "url": "https://www.semanticscholar.org/paper/af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2",
            "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2980282514",
                "ArXiv": "1910.03771",
                "DBLP": "journals/corr/abs-1910-03771",
                "CorpusId": 208117506
            },
            "abstract": "Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. \\textit{Transformers} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. \\textit{Transformers} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \\url{this https URL}.",
            "referenceCount": 81,
            "citationCount": 6499,
            "influentialCitationCount": 280,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-10-09",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1910.03771"
            },
            "citationStyles": {
                "bibtex": "@Article{Wolf2019HuggingFacesTS,\n author = {Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R\u00e9mi Louf and Morgan Funtowicz and Jamie Brew},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {HuggingFace's Transformers: State-of-the-art Natural Language Processing},\n volume = {abs/1910.03771},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2f5102ec3f70d0dea98c957cc2cab4d15d83a2da",
            "@type": "ScholarlyArticle",
            "paperId": "2f5102ec3f70d0dea98c957cc2cab4d15d83a2da",
            "corpusId": 14068874,
            "url": "https://www.semanticscholar.org/paper/2f5102ec3f70d0dea98c957cc2cab4d15d83a2da",
            "title": "The Stanford CoreNLP Natural Language Processing Toolkit",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                "name": "Annual Meeting of the Association for Computational Linguistics",
                "alternate_names": [
                    "Annu Meet Assoc Comput Linguistics",
                    "Meeting of the Association for Computational Linguistics",
                    "ACL",
                    "Meet Assoc Comput Linguistics"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/anthology/venues/acl/"
            },
            "year": 2014,
            "externalIds": {
                "ACL": "P14-5010",
                "MAG": "2123442489",
                "DBLP": "conf/acl/ManningSBFBM14",
                "DOI": "10.3115/v1/P14-5010",
                "CorpusId": 14068874
            },
            "abstract": "We describe the design and use of the Stanford CoreNLP toolkit, an extensible pipeline that provides core natural language analysis. This toolkit is quite widely used, both in the research NLP community and also among commercial and government users of open source NLP technology. We suggest that this follows from a simple, approachable design, straightforward interfaces, the inclusion of robust and good quality analysis components, and not requiring use of a large amount of associated baggage.",
            "referenceCount": 27,
            "citationCount": 6839,
            "influentialCitationCount": 796,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2014-06-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Manning2014TheSC,\n author = {Christopher D. Manning and M. Surdeanu and John Bauer and J. Finkel and Steven Bethard and David McClosky},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {55-60},\n title = {The Stanford CoreNLP Natural Language Processing Toolkit},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:bc1022b031dc6c7019696492e8116598097a8c12",
            "@type": "ScholarlyArticle",
            "paperId": "bc1022b031dc6c7019696492e8116598097a8c12",
            "corpusId": 351666,
            "url": "https://www.semanticscholar.org/paper/bc1022b031dc6c7019696492e8116598097a8c12",
            "title": "Natural Language Processing (Almost) from Scratch",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2011,
            "externalIds": {
                "MAG": "2158899491",
                "DBLP": "journals/jmlr/CollobertWBKKK11",
                "ArXiv": "1103.0398",
                "DOI": "10.5555/1953048.2078186",
                "CorpusId": 351666
            },
            "abstract": "We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.",
            "referenceCount": 102,
            "citationCount": 7368,
            "influentialCitationCount": 733,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2011-02-01",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1103.0398"
            },
            "citationStyles": {
                "bibtex": "@Article{Collobert2011NaturalLP,\n author = {Ronan Collobert and J. Weston and L. Bottou and Michael Karlen and K. Kavukcuoglu and P. Kuksa},\n booktitle = {Journal of machine learning research},\n journal = {ArXiv},\n title = {Natural Language Processing (Almost) from Scratch},\n volume = {abs/1103.0398},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:28692beece311a90f5fa1ca2ec9d0c2ce293d069",
            "@type": "ScholarlyArticle",
            "paperId": "28692beece311a90f5fa1ca2ec9d0c2ce293d069",
            "corpusId": 236493269,
            "url": "https://www.semanticscholar.org/paper/28692beece311a90f5fa1ca2ec9d0c2ce293d069",
            "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing",
            "venue": "ACM Computing Surveys",
            "publicationVenue": {
                "id": "urn:research:7b2adce0-d53f-49d6-8784-b0645604fe62",
                "name": "ACM Computing Surveys",
                "alternate_names": [
                    "ACM Comput Surv"
                ],
                "issn": "0360-0300",
                "url": "http://www.acm.org/pubs/surveys/"
            },
            "year": 2021,
            "externalIds": {
                "DBLP": "journals/csur/LiuYFJHN23",
                "ArXiv": "2107.13586",
                "DOI": "10.1145/3560815",
                "CorpusId": 236493269
            },
            "abstract": "This article surveys and organizes research works in a new paradigm in natural language processing, which we dub \u201cprompt-based learning.\u201d Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y|x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x\u2032 that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x\u0302, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: It allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this article, we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g., the choice of pre-trained language models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts but also release other resources, e.g., a website NLPedia\u2013Pretrain including constantly updated survey and paperlist.",
            "referenceCount": 223,
            "citationCount": 1707,
            "influentialCitationCount": 154,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3560815",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2021-07-28",
            "journal": {
                "name": "ACM Computing Surveys",
                "volume": "55"
            },
            "citationStyles": {
                "bibtex": "@Article{Liu2021PretrainPA,\n author = {Pengfei Liu and Weizhe Yuan and Jinlan Fu and Zhengbao Jiang and Hiroaki Hayashi and Graham Neubig},\n booktitle = {ACM Computing Surveys},\n journal = {ACM Computing Surveys},\n pages = {1 - 35},\n title = {Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing},\n volume = {55},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ce2d5b5856bb6c9ab5c2390eb8b180c75a162055",
            "@type": "ScholarlyArticle",
            "paperId": "ce2d5b5856bb6c9ab5c2390eb8b180c75a162055",
            "corpusId": 3397190,
            "url": "https://www.semanticscholar.org/paper/ce2d5b5856bb6c9ab5c2390eb8b180c75a162055",
            "title": "Recent Trends in Deep Learning Based Natural Language Processing",
            "venue": "IEEE Computational Intelligence Magazine",
            "publicationVenue": {
                "id": "urn:research:ee372de7-efda-4907-a03f-359292ea27f6",
                "name": "IEEE Computational Intelligence Magazine",
                "alternate_names": [
                    "IEEE Comput Intell Mag"
                ],
                "issn": "1556-603X",
                "url": "https://ieeexplore.ieee.org/servlet/opac?punumber=10207"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2742947407",
                "DBLP": "journals/corr/abs-1708-02709",
                "ArXiv": "1708.02709",
                "DOI": "10.1109/MCI.2018.2840738",
                "CorpusId": 3397190
            },
            "abstract": "Deep learning methods employ multiple processing layers to learn hierarchical representations of data and have produced state-of-the-art results in many domains. Recently, a variety of model designs and methods have blossomed in the context of natural language processing (NLP). In this paper, we review significant deep learning related models and methods that have been employed for numerous NLP tasks and provide a walk-through of their evolution. We also summarize, compare and contrast the various models and put forward a detailed understanding of the past, present and future of deep learning in NLP.",
            "referenceCount": 204,
            "citationCount": 2533,
            "influentialCitationCount": 84,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2017-08-09",
            "journal": {
                "name": "IEEE Comput. Intell. Mag.",
                "volume": "13"
            },
            "citationStyles": {
                "bibtex": "@Article{Young2017RecentTI,\n author = {Tom Young and Devamanyu Hazarika and Soujanya Poria and E. Cambria},\n booktitle = {IEEE Computational Intelligence Magazine},\n journal = {IEEE Comput. Intell. Mag.},\n pages = {55-75},\n title = {Recent Trends in Deep Learning Based Natural Language Processing},\n volume = {13},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:084c55d6432265785e3ff86a2e900a49d501c00a",
            "@type": "ScholarlyArticle",
            "paperId": "084c55d6432265785e3ff86a2e900a49d501c00a",
            "corpusId": 52800448,
            "url": "https://www.semanticscholar.org/paper/084c55d6432265785e3ff86a2e900a49d501c00a",
            "title": "Book Reviews: Foundations of Statistical Natural Language Processing",
            "venue": "International Conference on Computational Logic",
            "publicationVenue": {
                "id": "urn:research:30a8645d-22d4-42e2-b3f6-304bf4ce3a02",
                "name": "International Conference on Computational Logic",
                "alternate_names": [
                    "CL",
                    "Int Conf Comput Log"
                ],
                "issn": null,
                "url": null
            },
            "year": 1999,
            "externalIds": {
                "MAG": "1574901103",
                "DBLP": "books/daglib/0001548",
                "ACL": "J00-2011",
                "CorpusId": 52800448
            },
            "abstract": "Statistical approaches to processing natural language text have become dominant in recent years. This foundational text is the first comprehensive introduction to statistical natural language processing (NLP) to appear. The book contains all the theory and algorithms needed for building NLP tools. It provides broad but rigorous coverage of mathematical and linguistic foundations, as well as detailed discussion of statistical methods, allowing students and researchers to construct their own implementations. The book covers collocation finding, word sense disambiguation, probabilistic parsing, information retrieval, and other applications.",
            "referenceCount": 10,
            "citationCount": 7570,
            "influentialCitationCount": 566,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "1999-06-18",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Manning1999BookRF,\n author = {Christopher D. Manning and Hinrich Sch\u00fctze},\n booktitle = {International Conference on Computational Logic},\n pages = {I-XXXVII, 1-680},\n title = {Book Reviews: Foundations of Statistical Natural Language Processing},\n year = {1999}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:57458bc1cffe5caa45a885af986d70f723f406b4",
            "@type": "ScholarlyArticle",
            "paperId": "57458bc1cffe5caa45a885af986d70f723f406b4",
            "corpusId": 2617020,
            "url": "https://www.semanticscholar.org/paper/57458bc1cffe5caa45a885af986d70f723f406b4",
            "title": "A unified architecture for natural language processing: deep neural networks with multitask learning",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2008,
            "externalIds": {
                "MAG": "2117130368",
                "DBLP": "conf/icml/CollobertW08",
                "DOI": "10.1145/1390156.1390177",
                "CorpusId": 2617020
            },
            "abstract": "We describe a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the generalization of the shared tasks, resulting in state-of-the-art-performance.",
            "referenceCount": 24,
            "citationCount": 5608,
            "influentialCitationCount": 301,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2008-07-05",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Collobert2008AUA,\n author = {Ronan Collobert and J. Weston},\n booktitle = {International Conference on Machine Learning},\n pages = {160-167},\n title = {A unified architecture for natural language processing: deep neural networks with multitask learning},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b54bcfca3fddc26b8889739a247a25e445818149",
            "@type": "ScholarlyArticle",
            "paperId": "b54bcfca3fddc26b8889739a247a25e445818149",
            "corpusId": 60691216,
            "url": "https://www.semanticscholar.org/paper/b54bcfca3fddc26b8889739a247a25e445818149",
            "title": "Speech and language processing - an introduction to natural language processing, computational linguistics, and speech recognition",
            "venue": "Prentice Hall series in artificial intelligence",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2000,
            "externalIds": {
                "MAG": "1579838312",
                "DBLP": "books/daglib/0004298",
                "CorpusId": 60691216
            },
            "abstract": "From the Publisher: \nThis book takes an empirical approach to language processing, based on applying statistical and other machine-learning algorithms to large corpora.Methodology boxes are included in each chapter. Each chapter is built around one or more worked examples to demonstrate the main idea of the chapter. Covers the fundamental algorithms of various fields, whether originally proposed for spoken or written language to demonstrate how the same algorithm can be used for speech recognition and word-sense disambiguation. Emphasis on web and other practical applications. Emphasis on scientific evaluation. Useful as a reference for professionals in any of the areas of speech and language processing.",
            "referenceCount": 258,
            "citationCount": 4066,
            "influentialCitationCount": 287,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Jurafsky2000SpeechAL,\n author = {Dan Jurafsky and James H. Martin},\n booktitle = {Prentice Hall series in artificial intelligence},\n pages = {I-XXVI, 1-934},\n title = {Speech and language processing - an introduction to natural language processing, computational linguistics, and speech recognition},\n year = {2000}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:641a9749fe546a02bbab9a86bfc91492db1c3bc5",
            "@type": "ScholarlyArticle",
            "paperId": "641a9749fe546a02bbab9a86bfc91492db1c3bc5",
            "corpusId": 212725611,
            "url": "https://www.semanticscholar.org/paper/641a9749fe546a02bbab9a86bfc91492db1c3bc5",
            "title": "Stanza: A Python Natural Language Processing Toolkit for Many Human Languages",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                "name": "Annual Meeting of the Association for Computational Linguistics",
                "alternate_names": [
                    "Annu Meet Assoc Comput Linguistics",
                    "Meeting of the Association for Computational Linguistics",
                    "ACL",
                    "Meet Assoc Comput Linguistics"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/anthology/venues/acl/"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3011573503",
                "ACL": "2020.acl-demos.14",
                "DBLP": "conf/acl/QiZZBM20",
                "ArXiv": "2003.07082",
                "DOI": "10.18653/v1/2020.acl-demos.14",
                "CorpusId": 212725611
            },
            "abstract": "We introduce Stanza, an open-source Python natural language processing toolkit supporting 66 human languages. Compared to existing widely used toolkits, Stanza features a language-agnostic fully neural pipeline for text analysis, including tokenization, multi-word token expansion, lemmatization, part-of-speech and morphological feature tagging, dependency parsing, and named entity recognition. We have trained Stanza on a total of 112 datasets, including the Universal Dependencies treebanks and other multilingual corpora, and show that the same neural architecture generalizes well and achieves competitive performance on all languages tested. Additionally, Stanza includes a native Python interface to the widely used Java Stanford CoreNLP software, which further extends its functionality to cover other tasks such as coreference resolution and relation extraction. Source code, documentation, and pretrained models for 66 languages are available at https://stanfordnlp.github.io/stanza/.",
            "referenceCount": 22,
            "citationCount": 1143,
            "influentialCitationCount": 161,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/2020.acl-demos.14.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-03-16",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2003.07082"
            },
            "citationStyles": {
                "bibtex": "@Article{Qi2020StanzaAP,\n author = {Peng Qi and Yuhao Zhang and Yuhui Zhang and Jason Bolton and Christopher D. Manning},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n journal = {ArXiv},\n title = {Stanza: A Python Natural Language Processing Toolkit for Many Human Languages},\n volume = {abs/2003.07082},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:54523ff961a1ac57a86696ef9a53b3a630b482c0",
            "@type": "ScholarlyArticle",
            "paperId": "54523ff961a1ac57a86696ef9a53b3a630b482c0",
            "corpusId": 220919723,
            "url": "https://www.semanticscholar.org/paper/54523ff961a1ac57a86696ef9a53b3a630b482c0",
            "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
            "venue": "ACM Trans. Comput. Heal.",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3046375318",
                "DBLP": "journals/corr/abs-2007-15779",
                "ArXiv": "2007.15779",
                "DOI": "10.1145/3458754",
                "CorpusId": 220919723
            },
            "abstract": "Pretraining large neural language models, such as BERT, has led to impressive gains on many natural language processing (NLP) tasks. However, most pretraining efforts focus on general domain corpora, such as newswire and Web. A prevailing assumption is that even domain-specific pretraining can benefit by starting from general-domain language models. In this article, we challenge this assumption by showing that for domains with abundant unlabeled text, such as biomedicine, pretraining language models from scratch results in substantial gains over continual pretraining of general-domain language models. To facilitate this investigation, we compile a comprehensive biomedical NLP benchmark from publicly available datasets. Our experiments show that domain-specific pretraining serves as a solid foundation for a wide range of biomedical NLP tasks, leading to new state-of-the-art results across the board. Further, in conducting a thorough evaluation of modeling choices, both for pretraining and task-specific fine-tuning, we discover that some common practices are unnecessary with BERT models, such as using complex tagging schemes in named entity recognition. To help accelerate research in biomedical NLP, we have released our state-of-the-art pretrained and task-specific models for the community, and created a leaderboard featuring our BLURB benchmark (short for Biomedical Language Understanding & Reasoning Benchmark) at https://aka.ms/BLURB.",
            "referenceCount": 64,
            "citationCount": 883,
            "influentialCitationCount": 164,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2007.15779",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-07-31",
            "journal": {
                "name": "ACM Transactions on Computing for Healthcare (HEALTH)",
                "volume": "3"
            },
            "citationStyles": {
                "bibtex": "@Article{Gu2020DomainSpecificLM,\n author = {Yu Gu and Robert Tinn and Hao Cheng and Michael R. Lucas and Naoto Usuyama and Xiaodong Liu and Tristan Naumann and Jianfeng Gao and Hoifung Poon},\n booktitle = {ACM Trans. Comput. Heal.},\n journal = {ACM Transactions on Computing for Healthcare (HEALTH)},\n pages = {1 - 23},\n title = {Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing},\n volume = {3},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:099f8bf42b95a4c239bc338b05821b592c60f44b",
            "@type": "ScholarlyArticle",
            "paperId": "099f8bf42b95a4c239bc338b05821b592c60f44b",
            "corpusId": 248004235,
            "url": "https://www.semanticscholar.org/paper/099f8bf42b95a4c239bc338b05821b592c60f44b",
            "title": "Natural language processing applied to mental illness detection: a narrative review",
            "venue": "npj Digital Medicine",
            "publicationVenue": {
                "id": "urn:research:ef485645-f75f-4344-8b9d-3c260e69503b",
                "name": "npj Digital Medicine",
                "alternate_names": [
                    "npj Digit Med"
                ],
                "issn": "2398-6352",
                "url": "http://www.nature.com/npjdigitalmed/"
            },
            "year": 2022,
            "externalIds": {
                "DBLP": "journals/npjdm/ZhangSJA22",
                "PubMedCentral": "8993841",
                "DOI": "10.1038/s41746-022-00589-7",
                "CorpusId": 248004235,
                "PubMed": "35396451"
            },
            "abstract": null,
            "referenceCount": 242,
            "citationCount": 82,
            "influentialCitationCount": 6,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.nature.com/articles/s41746-022-00589-7.pdf",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2022-04-08",
            "journal": {
                "name": "NPJ Digital Medicine",
                "volume": "5"
            },
            "citationStyles": {
                "bibtex": "@Article{Zhang2022NaturalLP,\n author = {Tianlin Zhang and Annika Marie Schoene and Shaoxiong Ji and S. Ananiadou},\n booktitle = {npj Digital Medicine},\n journal = {NPJ Digital Medicine},\n title = {Natural language processing applied to mental illness detection: a narrative review},\n volume = {5},\n year = {2022}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:83a491b6dfab0a9f30ce66d7dad1d7409e4d6e4d",
            "@type": "ScholarlyArticle",
            "paperId": "83a491b6dfab0a9f30ce66d7dad1d7409e4d6e4d",
            "corpusId": 246902471,
            "url": "https://www.semanticscholar.org/paper/83a491b6dfab0a9f30ce66d7dad1d7409e4d6e4d",
            "title": "Brains and algorithms partially converge in natural language processing",
            "venue": "Communications Biology",
            "publicationVenue": {
                "id": "urn:research:069e05e7-ca35-41d0-a8c7-bdc9ec6a82af",
                "name": "Communications Biology",
                "alternate_names": [
                    "Commun Biology"
                ],
                "issn": "2399-3642",
                "url": "http://www.nature.com/commsbio/"
            },
            "year": 2022,
            "externalIds": {
                "PubMedCentral": "8850612",
                "DOI": "10.1038/s42003-022-03036-1",
                "CorpusId": 246902471,
                "PubMed": "35173264"
            },
            "abstract": null,
            "referenceCount": 95,
            "citationCount": 104,
            "influentialCitationCount": 9,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.nature.com/articles/s42003-022-03036-1.pdf",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2022-02-16",
            "journal": {
                "name": "Communications Biology",
                "volume": "5"
            },
            "citationStyles": {
                "bibtex": "@Article{Caucheteux2022BrainsAA,\n author = {C. Caucheteux and J. King},\n booktitle = {Communications Biology},\n journal = {Communications Biology},\n title = {Brains and algorithms partially converge in natural language processing},\n volume = {5},\n year = {2022}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3bcb17559ce96eb20fa79af8194f4af0380d194a",
            "@type": "ScholarlyArticle",
            "paperId": "3bcb17559ce96eb20fa79af8194f4af0380d194a",
            "corpusId": 212747830,
            "url": "https://www.semanticscholar.org/paper/3bcb17559ce96eb20fa79af8194f4af0380d194a",
            "title": "Pre-trained models for natural language processing: A survey",
            "venue": "Science China Technological Sciences",
            "publicationVenue": {
                "id": "urn:research:e66be951-1e6a-417b-b768-fa43c83c31f6",
                "name": "Science China Technological Sciences",
                "alternate_names": [
                    "Sci China Technol Sci",
                    "Sci China-technological Sci",
                    "Science China-technological Sciences"
                ],
                "issn": "1869-1900",
                "url": "https://link.springer.com/journal/volumesAndIssues/11431"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "journals/corr/abs-2003-08271",
                "MAG": "3088409176",
                "ArXiv": "2003.08271",
                "DOI": "10.1007/s11431-020-1647-3",
                "CorpusId": 212747830
            },
            "abstract": null,
            "referenceCount": 265,
            "citationCount": 1000,
            "influentialCitationCount": 55,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2003.08271",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2020-03-18",
            "journal": {
                "name": "Science China Technological Sciences",
                "volume": "63"
            },
            "citationStyles": {
                "bibtex": "@Article{Qiu2020PretrainedMF,\n author = {Xipeng Qiu and Tianxiang Sun and Yige Xu and Yunfan Shao and Ning Dai and Xuanjing Huang},\n booktitle = {Science China Technological Sciences},\n journal = {Science China Technological Sciences},\n pages = {1872 - 1897},\n title = {Pre-trained models for natural language processing: A survey},\n volume = {63},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:912a39c2e0e4a35747531669cfa952d2c5627729",
            "@type": "ScholarlyArticle",
            "paperId": "912a39c2e0e4a35747531669cfa952d2c5627729",
            "corpusId": 252693405,
            "url": "https://www.semanticscholar.org/paper/912a39c2e0e4a35747531669cfa952d2c5627729",
            "title": "Is Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2022,
            "externalIds": {
                "ArXiv": "2210.01241",
                "DBLP": "journals/corr/abs-2210-01241",
                "DOI": "10.48550/arXiv.2210.01241",
                "CorpusId": 252693405
            },
            "abstract": "We tackle the problem of aligning pre-trained large language models (LMs) with human preferences. If we view text generation as a sequential decision-making problem, reinforcement learning (RL) appears to be a natural conceptual framework. However, using RL for LM-based generation faces empirical challenges, including training instability due to the combinatorial action space, as well as a lack of open-source libraries and benchmarks customized for LM alignment. Thus, a question rises in the research community: is RL a practical paradigm for NLP? To help answer this, we first introduce an open-source modular library, RL4LMs (Reinforcement Learning for Language Models), for optimizing language generators with RL. The library consists of on-policy RL algorithms that can be used to train any encoder or encoder-decoder LM in the HuggingFace library (Wolf et al. 2020) with an arbitrary reward function. Next, we present the GRUE (General Reinforced-language Understanding Evaluation) benchmark, a set of 6 language generation tasks which are supervised not by target strings, but by reward functions which capture automated measures of human preference. GRUE is the first leaderboard-style evaluation of RL algorithms for NLP tasks. Finally, we introduce an easy-to-use, performant RL algorithm, NLPO (Natural Language Policy Optimization) that learns to effectively reduce the combinatorial action space in language generation. We show 1) that RL techniques are generally better than supervised methods at aligning LMs to human preferences; and 2) that NLPO exhibits greater stability and performance than previous policy gradient methods (e.g., PPO (Schulman et al. 2017)), based on both automatic and human evaluations.",
            "referenceCount": 86,
            "citationCount": 89,
            "influentialCitationCount": 15,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2210.01241",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2022-10-03",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2210.01241"
            },
            "citationStyles": {
                "bibtex": "@Article{Ramamurthy2022IsRL,\n author = {Rajkumar Ramamurthy and Prithviraj Ammanabrolu and Kiant\u00e9 Brantley and Jack Hessel and R. Sifa and C. Bauckhage and Hannaneh Hajishirzi and Yejin Choi},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Is Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization},\n volume = {abs/2210.01241},\n year = {2022}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:96b0a95b67aa1ee556db21310b36555a3a373f95",
            "@type": "ScholarlyArticle",
            "paperId": "96b0a95b67aa1ee556db21310b36555a3a373f95",
            "corpusId": 115386587,
            "url": "https://www.semanticscholar.org/paper/96b0a95b67aa1ee556db21310b36555a3a373f95",
            "title": "Foundations of Statistical Natural Language Processing",
            "venue": "Information retrieval (Boston)",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2001,
            "externalIds": {
                "MAG": "1493708795",
                "DBLP": "journals/ir/Kantor01",
                "DOI": "10.1023/A:1011424425034",
                "CorpusId": 115386587
            },
            "abstract": null,
            "referenceCount": 4,
            "citationCount": 3689,
            "influentialCitationCount": 336,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2001-04-01",
            "journal": {
                "name": "Information Retrieval",
                "volume": "4"
            },
            "citationStyles": {
                "bibtex": "@Article{Kantor2001FoundationsOS,\n author = {P. Kantor},\n booktitle = {Information retrieval (Boston)},\n journal = {Information Retrieval},\n pages = {80-81},\n title = {Foundations of Statistical Natural Language Processing},\n volume = {4},\n year = {2001}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0aef6cc4c0082b6e3b3d104b8a074afa5739a28d",
            "@type": "ScholarlyArticle",
            "paperId": "0aef6cc4c0082b6e3b3d104b8a074afa5739a28d",
            "corpusId": 252782469,
            "url": "https://www.semanticscholar.org/paper/0aef6cc4c0082b6e3b3d104b8a074afa5739a28d",
            "title": "NLP4SM: Natural Language Processing for social media",
            "venue": "Annual Conference of the Spanish Society for Natural Language Processing",
            "publicationVenue": {
                "id": "urn:research:3f4140f3-7f72-4fff-b422-17e959d4beef",
                "name": "Annual Conference of the Spanish Society for Natural Language Processing",
                "alternate_names": [
                    "SEPLN",
                    "Annu Conf Span Soc Nat Lang Process"
                ],
                "issn": null,
                "url": null
            },
            "year": 2022,
            "externalIds": {
                "DBLP": "conf/sepln/MedinaCM22",
                "CorpusId": 252782469
            },
            "abstract": "NLP4SM is a website for the execution, analysis and comparison of tweet classification methods based on language models. Currently, NLP4SM supports the text classification tasks considered in TweetEval, but it aims at integrating additional text clasification tasks and to wider the number of language models available with the goal of becoming to a benchmark platform for assessing text classification methods with real data from social media.",
            "referenceCount": 15,
            "citationCount": 54,
            "influentialCitationCount": 5,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Medina2022NLP4SMNL,\n author = {Gonzalo Medina and Jos\u00e9 Camacho-Collados and Eugenio Mart\u00ednez-C\u00e1mara},\n booktitle = {Annual Conference of the Spanish Society for Natural Language Processing},\n pages = {66-69},\n title = {NLP4SM: Natural Language Processing for social media},\n year = {2022}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:fb486e03369a64de2d5b0df86ec0a7b55d3907db",
            "@type": "ScholarlyArticle",
            "paperId": "fb486e03369a64de2d5b0df86ec0a7b55d3907db",
            "corpusId": 1085832,
            "url": "https://www.semanticscholar.org/paper/fb486e03369a64de2d5b0df86ec0a7b55d3907db",
            "title": "A Maximum Entropy Approach to Natural Language Processing",
            "venue": "International Conference on Computational Logic",
            "publicationVenue": {
                "id": "urn:research:30a8645d-22d4-42e2-b3f6-304bf4ce3a02",
                "name": "International Conference on Computational Logic",
                "alternate_names": [
                    "CL",
                    "Int Conf Comput Log"
                ],
                "issn": null,
                "url": null
            },
            "year": 1996,
            "externalIds": {
                "DBLP": "journals/coling/BergerPP96",
                "MAG": "2096175520",
                "ACL": "J96-1002",
                "CorpusId": 1085832
            },
            "abstract": "The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing.",
            "referenceCount": 31,
            "citationCount": 3210,
            "influentialCitationCount": 260,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1996-03-01",
            "journal": {
                "name": "Comput. Linguistics",
                "volume": "22"
            },
            "citationStyles": {
                "bibtex": "@Article{Berger1996AME,\n author = {Adam L. Berger and S. D. Pietra and V. D. Pietra},\n booktitle = {International Conference on Computational Logic},\n journal = {Comput. Linguistics},\n pages = {39-71},\n title = {A Maximum Entropy Approach to Natural Language Processing},\n volume = {22},\n year = {1996}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:93b4cc549a1bc4bc112189da36c318193d05d806",
            "@type": "ScholarlyArticle",
            "paperId": "93b4cc549a1bc4bc112189da36c318193d05d806",
            "corpusId": 3994096,
            "url": "https://www.semanticscholar.org/paper/93b4cc549a1bc4bc112189da36c318193d05d806",
            "title": "AllenNLP: A Deep Semantic Natural Language Processing Platform",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2018,
            "externalIds": {
                "ArXiv": "1803.07640",
                "MAG": "2793978524",
                "ACL": "W18-2501",
                "DBLP": "journals/corr/abs-1803-07640",
                "DOI": "10.18653/v1/W18-2501",
                "CorpusId": 3994096
            },
            "abstract": "Modern natural language processing (NLP) research requires writing code. Ideally this code would provide a precise definition of the approach, easy repeatability of results, and a basis for extending the research. However, many research codebases bury high-level parameters under implementation details, are challenging to run and debug, and are difficult enough to extend that they are more likely to be rewritten. This paper describes AllenNLP, a library for applying deep learning methods to NLP research that addresses these issues with easy-to-use command-line tools, declarative configuration-driven experiments, and modular NLP abstractions. AllenNLP has already increased the rate of research experimentation and the sharing of NLP components at the Allen Institute for Artificial Intelligence, and we are working to have the same impact across the field.",
            "referenceCount": 33,
            "citationCount": 1149,
            "influentialCitationCount": 157,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/W18-2501.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-03-20",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1803.07640"
            },
            "citationStyles": {
                "bibtex": "@Article{Gardner2018AllenNLPAD,\n author = {Matt Gardner and Joel Grus and Mark Neumann and Oyvind Tafjord and Pradeep Dasigi and Nelson F. Liu and Matthew E. Peters and Michael Schmitz and Luke Zettlemoyer},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {AllenNLP: A Deep Semantic Natural Language Processing Platform},\n volume = {abs/1803.07640},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:126fb7df6bcab2b70000dfe5b940ada63ae1ba6a",
            "@type": "ScholarlyArticle",
            "paperId": "126fb7df6bcab2b70000dfe5b940ada63ae1ba6a",
            "corpusId": 218665481,
            "url": "https://www.semanticscholar.org/paper/126fb7df6bcab2b70000dfe5b940ada63ae1ba6a",
            "title": "COVID-Twitter-BERT: A natural language processing model to analyse COVID-19 content on Twitter",
            "venue": "Frontiers in Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:6a8c0041-d0b7-4e32-b52c-33adef005c7e",
                "name": "Frontiers in Artificial Intelligence",
                "alternate_names": [
                    "Front Artif Intell"
                ],
                "issn": "2624-8212",
                "url": "https://www.frontiersin.org/journals/artificial-intelligence#"
            },
            "year": 2020,
            "externalIds": {
                "ArXiv": "2005.07503",
                "MAG": "3024622987",
                "DBLP": "journals/corr/abs-2005-07503",
                "PubMedCentral": "10043293",
                "DOI": "10.3389/frai.2023.1023281",
                "CorpusId": 218665481,
                "PubMed": "36998290"
            },
            "abstract": "Introduction This study presents COVID-Twitter-BERT (CT-BERT), a transformer-based model that is pre-trained on a large corpus of COVID-19 related Twitter messages. CT-BERT is specifically designed to be used on COVID-19 content, particularly from social media, and can be utilized for various natural language processing tasks such as classification, question-answering, and chatbots. This paper aims to evaluate the performance of CT-BERT on different classification datasets and compare it with BERT-LARGE, its base model. Methods The study utilizes CT-BERT, which is pre-trained on a large corpus of COVID-19 related Twitter messages. The authors evaluated the performance of CT-BERT on five different classification datasets, including one in the target domain. The model's performance is compared to its base model, BERT-LARGE, to measure the marginal improvement. The authors also provide detailed information on the training process and the technical specifications of the model. Results The results indicate that CT-BERT outperforms BERT-LARGE with a marginal improvement of 10-30% on all five classification datasets. The largest improvements are observed in the target domain. The authors provide detailed performance metrics and discuss the significance of these results. Discussion The study demonstrates the potential of pre-trained transformer models, such as CT-BERT, for COVID-19 related natural language processing tasks. The results indicate that CT-BERT can improve the classification performance on COVID-19 related content, especially on social media. These findings have important implications for various applications, such as monitoring public sentiment and developing chatbots to provide COVID-19 related information. The study also highlights the importance of using domain-specific pre-trained models for specific natural language processing tasks. Overall, this work provides a valuable contribution to the development of COVID-19 related NLP models.",
            "referenceCount": 15,
            "citationCount": 248,
            "influentialCitationCount": 58,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.frontiersin.org/articles/10.3389/frai.2023.1023281/pdf",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-05-15",
            "journal": {
                "name": "Frontiers in Artificial Intelligence",
                "volume": "6"
            },
            "citationStyles": {
                "bibtex": "@Article{M\u00fcller2020COVIDTwitterBERTAN,\n author = {Martin M\u00fcller and M. Salath\u00e9 and P. Kummervold},\n booktitle = {Frontiers in Artificial Intelligence},\n journal = {Frontiers in Artificial Intelligence},\n title = {COVID-Twitter-BERT: A natural language processing model to analyse COVID-19 content on Twitter},\n volume = {6},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3673bccde93025e05431a2bcac4e8ff18c9c273a",
            "@type": "ScholarlyArticle",
            "paperId": "3673bccde93025e05431a2bcac4e8ff18c9c273a",
            "corpusId": 264266192,
            "url": "https://www.semanticscholar.org/paper/3673bccde93025e05431a2bcac4e8ff18c9c273a",
            "title": "Book Review: Natural Language Processing with Python by Steven Bird, Ewan Klein, and Edward Loper",
            "venue": "International Conference on Computational Logic",
            "publicationVenue": {
                "id": "urn:research:30a8645d-22d4-42e2-b3f6-304bf4ce3a02",
                "name": "International Conference on Computational Logic",
                "alternate_names": [
                    "CL",
                    "Int Conf Comput Log"
                ],
                "issn": null,
                "url": null
            },
            "year": 2009,
            "externalIds": {
                "MAG": "1521626219",
                "DBLP": "books/daglib/0022921",
                "ACL": "J10-4009",
                "DOI": "10.1162/coli_r_00022",
                "CorpusId": 264266192
            },
            "abstract": "This book offers a highly accessible introduction to natural language processing, the field that supports a variety of language technologies, from predictive text and email filtering to automatic summarization and translation. With it, you'll learn how to write Python programs that work with large collections of unstructured text. You'll access richly annotated datasets using a comprehensive range of linguistic data structures, and you'll understand the main algorithms for analyzing the content and structure of written communication. Packed with examples and exercises, Natural Language Processing with Python will help you: Extract information from unstructured text, either to guess the topic or identify \"named entities\" Analyze linguistic structure in text, including parsing and semantic analysis Access popular linguistic databases, including WordNet and treebanks Integrate techniques drawn from fields as diverse as linguistics and artificial intelligence This book will help you gain practical skills in natural language processing using the Python programming language and the Natural Language Toolkit (NLTK) open source library. If you're interested in developing web applications, analyzing multilingual news sources, or documenting endangered languages -- or if you're simply curious to have a programmer's perspective on how human language works -- you'll find Natural Language Processing with Python both fascinating and immensely useful.",
            "referenceCount": 108,
            "citationCount": 2776,
            "influentialCitationCount": 204,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Art",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Art",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2009-06-12",
            "journal": {
                "name": "Computational Linguistics",
                "volume": "36"
            },
            "citationStyles": {
                "bibtex": "@Article{Elhadad2009BookRN,\n author = {Michael Elhadad},\n booktitle = {International Conference on Computational Logic},\n journal = {Computational Linguistics},\n pages = {767-771},\n title = {Book Review: Natural Language Processing with Python by Steven Bird, Ewan Klein, and Edward Loper},\n volume = {36},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7b9b756ab509cb9f52dbac95e3e901d571f0784f",
            "@type": "ScholarlyArticle",
            "paperId": "7b9b756ab509cb9f52dbac95e3e901d571f0784f",
            "corpusId": 51872504,
            "url": "https://www.semanticscholar.org/paper/7b9b756ab509cb9f52dbac95e3e901d571f0784f",
            "title": "A Survey of the Usages of Deep Learning for Natural Language Processing",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems",
            "publicationVenue": {
                "id": "urn:research:79c5a18d-0295-432c-aaa5-961d73de6d88",
                "name": "IEEE Transactions on Neural Networks and Learning Systems",
                "alternate_names": [
                    "IEEE Trans Neural Netw Learn Syst"
                ],
                "issn": "2162-237X",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5962385"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "journals/tnn/OtterMK21",
                "MAG": "3019166713",
                "DOI": "10.1109/TNNLS.2020.2979670",
                "CorpusId": 51872504,
                "PubMed": "32324570"
            },
            "abstract": "Over the last several years, the field of natural language processing has been propelled forward by an explosion in the use of deep learning models. This article provides a brief introduction to the field and a quick overview of deep learning architectures and methods. It then sifts through the plethora of recent studies and summarizes a large assortment of relevant contributions. Analyzed research areas include several core linguistic processing issues in addition to many applications of computational linguistics. A discussion of the current state of the art is then provided along with recommendations for future research in the field.",
            "referenceCount": 278,
            "citationCount": 838,
            "influentialCitationCount": 30,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2020-04-21",
            "journal": {
                "name": "IEEE Transactions on Neural Networks and Learning Systems",
                "volume": "32"
            },
            "citationStyles": {
                "bibtex": "@Article{Otter2020ASO,\n author = {Dan Otter and Julian R. Medina and J. Kalita},\n booktitle = {IEEE Transactions on Neural Networks and Learning Systems},\n journal = {IEEE Transactions on Neural Networks and Learning Systems},\n pages = {604-624},\n title = {A Survey of the Usages of Deep Learning for Natural Language Processing},\n volume = {32},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c23d9d44e8bc68408cea9f305d1f24d915bc0d0d",
            "@type": "ScholarlyArticle",
            "paperId": "c23d9d44e8bc68408cea9f305d1f24d915bc0d0d",
            "corpusId": 240420063,
            "url": "https://www.semanticscholar.org/paper/c23d9d44e8bc68408cea9f305d1f24d915bc0d0d",
            "title": "Recent Advances in Natural Language Processing via Large Pre-trained Language Models: A Survey",
            "venue": "ACM Computing Surveys",
            "publicationVenue": {
                "id": "urn:research:7b2adce0-d53f-49d6-8784-b0645604fe62",
                "name": "ACM Computing Surveys",
                "alternate_names": [
                    "ACM Comput Surv"
                ],
                "issn": "0360-0300",
                "url": "http://www.acm.org/pubs/surveys/"
            },
            "year": 2021,
            "externalIds": {
                "ArXiv": "2111.01243",
                "DBLP": "journals/csur/MinRSVNSAHR24",
                "DOI": "10.1145/3605943",
                "CorpusId": 240420063
            },
            "abstract": "Large, pre-trained language models (PLMs) such as BERT and GPT have drastically changed the Natural Language Processing (NLP) field. For numerous NLP tasks, approaches leveraging PLMs have achieved state-of-the-art performance. The key idea is to learn a generic, latent representation of language from a generic task once, then share it across disparate NLP tasks. Language modeling serves as the generic task, one with abundant self-supervised text available for extensive training. This article presents the key fundamental concepts of PLM architectures and a comprehensive view of the shift to PLM-driven NLP techniques. It surveys work applying the pre-training then fine-tuning, prompting, and text generation approaches. In addition, it discusses PLM limitations and suggested directions for future research.",
            "referenceCount": 363,
            "citationCount": 168,
            "influentialCitationCount": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2111.01243",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2021-11-01",
            "journal": {
                "name": "ACM Computing Surveys",
                "volume": "56"
            },
            "citationStyles": {
                "bibtex": "@Article{Min2021RecentAI,\n author = {Bonan Min and Hayley Ross and Elior Sulem and Amir Pouran Ben Veyseh and Thien Huu Nguyen and Oscar Sainz and Eneko Agirre and Ilana Heinz and D. Roth},\n booktitle = {ACM Computing Surveys},\n journal = {ACM Computing Surveys},\n pages = {1 - 40},\n title = {Recent Advances in Natural Language Processing via Large Pre-trained Language Models: A Survey},\n volume = {56},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:de28ec1d7bd38c8fc4e8ac59b6133800818b4e29",
            "@type": "ScholarlyArticle",
            "paperId": "de28ec1d7bd38c8fc4e8ac59b6133800818b4e29",
            "corpusId": 67788603,
            "url": "https://www.semanticscholar.org/paper/de28ec1d7bd38c8fc4e8ac59b6133800818b4e29",
            "title": "ScispaCy: Fast and Robust Models for Biomedical Natural Language Processing",
            "venue": "BioNLP@ACL",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "conf/bionlp/NeumannKBA19",
                "ArXiv": "1902.07669",
                "MAG": "2970600483",
                "ACL": "W19-5034",
                "DOI": "10.18653/v1/W19-5034",
                "CorpusId": 67788603
            },
            "abstract": "Despite recent advances in natural language processing, many statistical models for processing text perform extremely poorly under domain shift. Processing biomedical and clinical text is a critically important application area of natural language processing, for which there are few robust, practical, publicly available models. This paper describes scispaCy, a new Python library and models for practical biomedical/scientific text processing, which heavily leverages the spaCy library. We detail the performance of two packages of models released in scispaCy and demonstrate their robustness on several tasks and datasets. Models and code are available at https://allenai.github.io/scispacy/.",
            "referenceCount": 45,
            "citationCount": 491,
            "influentialCitationCount": 67,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1902.07669",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-02-20",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1902.07669"
            },
            "citationStyles": {
                "bibtex": "@Article{Neumann2019ScispaCyFA,\n author = {Mark Neumann and Daniel King and Iz Beltagy and Waleed Ammar},\n booktitle = {BioNLP@ACL},\n journal = {ArXiv},\n title = {ScispaCy: Fast and Robust Models for Biomedical Natural Language Processing},\n volume = {abs/1902.07669},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:347bac45298f37cd83c3e79d99b826dc65a70c46",
            "@type": "ScholarlyArticle",
            "paperId": "347bac45298f37cd83c3e79d99b826dc65a70c46",
            "corpusId": 189762009,
            "url": "https://www.semanticscholar.org/paper/347bac45298f37cd83c3e79d99b826dc65a70c46",
            "title": "Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets",
            "venue": "BioNLP@ACL",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2951675429",
                "ArXiv": "1906.05474",
                "DBLP": "conf/bionlp/PengYL19",
                "ACL": "W19-5006",
                "DOI": "10.18653/v1/W19-5006",
                "CorpusId": 189762009
            },
            "abstract": "Inspired by the success of the General Language Understanding Evaluation benchmark, we introduce the Biomedical Language Understanding Evaluation (BLUE) benchmark to facilitate research in the development of pre-training language representations in the biomedicine domain. The benchmark consists of five tasks with ten datasets that cover both biomedical and clinical texts with different dataset sizes and difficulties. We also evaluate several baselines based on BERT and ELMo and find that the BERT model pre-trained on PubMed abstracts and MIMIC-III clinical notes achieves the best results. We make the datasets, pre-trained models, and codes publicly available at https://github.com/ ncbi-nlp/BLUE_Benchmark.",
            "referenceCount": 44,
            "citationCount": 616,
            "influentialCitationCount": 97,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/W19-5006.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-06-13",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Peng2019TransferLI,\n author = {Yifan Peng and Shankai Yan and Zhiyong Lu},\n booktitle = {BioNLP@ACL},\n pages = {58-65},\n title = {Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6afe0fb12ceacadbbfed7202d430770a3f344731",
            "@type": "ScholarlyArticle",
            "paperId": "6afe0fb12ceacadbbfed7202d430770a3f344731",
            "corpusId": 216641856,
            "url": "https://www.semanticscholar.org/paper/6afe0fb12ceacadbbfed7202d430770a3f344731",
            "title": "Revisiting Pre-Trained Models for Chinese Natural Language Processing",
            "venue": "Findings",
            "publicationVenue": {
                "id": "urn:research:479d5605-51be-4346-b1d6-4334084504df",
                "name": "Findings",
                "alternate_names": null,
                "issn": "2652-8800",
                "url": "https://findingspress.org/"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "journals/corr/abs-2004-13922",
                "ArXiv": "2004.13922",
                "MAG": "3021636956",
                "ACL": "2020.findings-emnlp.58",
                "DOI": "10.18653/v1/2020.findings-emnlp.58",
                "CorpusId": 216641856
            },
            "abstract": "Bidirectional Encoder Representations from Transformers (BERT) has shown marvelous improvements across various NLP tasks, and consecutive variants have been proposed to further improve the performance of the pre-trained language models. In this paper, we target on revisiting Chinese pre-trained language models to examine their effectiveness in a non-English language and release the Chinese pre-trained language model series to the community. We also propose a simple but effective model called MacBERT, which improves upon RoBERTa in several ways, especially the masking strategy that adopts MLM as correction (Mac). We carried out extensive experiments on eight Chinese NLP tasks to revisit the existing pre-trained language models as well as the proposed MacBERT. Experimental results show that MacBERT could achieve state-of-the-art performances on many NLP tasks, and we also ablate details with several findings that may help future research. https://github.com/ymcui/MacBERT",
            "referenceCount": 43,
            "citationCount": 439,
            "influentialCitationCount": 50,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/2020.findings-emnlp.58.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-04-01",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2004.13922"
            },
            "citationStyles": {
                "bibtex": "@Article{Cui2020RevisitingPM,\n author = {Yiming Cui and Wanxiang Che and Ting Liu and Bing Qin and Shijin Wang and Guoping Hu},\n booktitle = {Findings},\n journal = {ArXiv},\n title = {Revisiting Pre-Trained Models for Chinese Natural Language Processing},\n volume = {abs/2004.13922},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ef8d788a904ed66bd8e30ffa69bc3ea1fe57dda7",
            "@type": "ScholarlyArticle",
            "paperId": "ef8d788a904ed66bd8e30ffa69bc3ea1fe57dda7",
            "corpusId": 218971877,
            "url": "https://www.semanticscholar.org/paper/ef8d788a904ed66bd8e30ffa69bc3ea1fe57dda7",
            "title": "HAT: Hardware-Aware Transformers for Efficient Natural Language Processing",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                "name": "Annual Meeting of the Association for Computational Linguistics",
                "alternate_names": [
                    "Annu Meet Assoc Comput Linguistics",
                    "Meeting of the Association for Computational Linguistics",
                    "ACL",
                    "Meet Assoc Comput Linguistics"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/anthology/venues/acl/"
            },
            "year": 2020,
            "externalIds": {
                "ACL": "2020.acl-main.686",
                "DBLP": "journals/corr/abs-2005-14187",
                "MAG": "3029385331",
                "ArXiv": "2005.14187",
                "DOI": "10.18653/v1/2020.acl-main.686",
                "CorpusId": 218971877
            },
            "abstract": "Transformers are ubiquitous in Natural Language Processing (NLP) tasks, but they are difficult to be deployed on hardware due to the intensive computation. To enable low-latency inference on resource-constrained hardware platforms, we propose to design Hardware-Aware Transformers (HAT) with neural architecture search. We first construct a large design space with arbitrary encoder-decoder attention and heterogeneous layers. Then we train a SuperTransformer that covers all candidates in the design space, and efficiently produces many SubTransformers with weight sharing. Finally, we perform an evolutionary search with a hardware latency constraint to find a specialized SubTransformer dedicated to run fast on the target hardware. Extensive experiments on four machine translation tasks demonstrate that HAT can discover efficient models for different hardware (CPU, GPU, IoT device). When running WMT\u201914 translation task on Raspberry Pi-4, HAT can achieve 3\u00d7 speedup, 3.7\u00d7 smaller size over baseline Transformer; 2.7\u00d7 speedup, 3.6\u00d7 smaller size over Evolved Transformer with 12,041\u00d7 less search cost and no performance loss. HAT is open-sourced at https://github.com/mit-han-lab/hardware-aware-transformers.",
            "referenceCount": 52,
            "citationCount": 181,
            "influentialCitationCount": 39,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/2020.acl-main.686.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-05-28",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2005.14187"
            },
            "citationStyles": {
                "bibtex": "@Article{Wang2020HATHT,\n author = {Hanrui Wang and Zhanghao Wu and Zhijian Liu and Han Cai and Ligeng Zhu and Chuang Gan and Song Han},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n journal = {ArXiv},\n title = {HAT: Hardware-Aware Transformers for Efficient Natural Language Processing},\n volume = {abs/2005.14187},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:130d432ccbc836380a212bea618f84ff094a6a52",
            "@type": "ScholarlyArticle",
            "paperId": "130d432ccbc836380a212bea618f84ff094a6a52",
            "corpusId": 237386009,
            "url": "https://www.semanticscholar.org/paper/130d432ccbc836380a212bea618f84ff094a6a52",
            "title": "Causal Inference in Natural Language Processing: Estimation, Prediction, Interpretation and Beyond",
            "venue": "Transactions of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:e0dbf116-86aa-418d-859f-a49952d7e44a",
                "name": "Transactions of the Association for Computational Linguistics",
                "alternate_names": [
                    "Trans Assoc Comput Linguistics",
                    "TACL"
                ],
                "issn": "2307-387X",
                "url": "https://www.mitpressjournals.org/loi/tacl"
            },
            "year": 2021,
            "externalIds": {
                "ArXiv": "2109.00725",
                "DBLP": "journals/tacl/FederKMPSWEGRRS22",
                "DOI": "10.1162/tacl_a_00511",
                "CorpusId": 237386009
            },
            "abstract": "Abstract A fundamental goal of scientific research is to learn about causal relationships. However, despite its critical role in the life and social sciences, causality has not had the same importance in Natural Language Processing (NLP), which has traditionally placed more emphasis on predictive tasks. This distinction is beginning to fade, with an emerging area of interdisciplinary research at the convergence of causal inference and language processing. Still, research on causality in NLP remains scattered across domains without unified definitions, benchmark datasets and clear articulations of the challenges and opportunities in the application of causal inference to the textual domain, with its unique properties. In this survey, we consolidate research across academic areas and situate it in the broader NLP landscape. We introduce the statistical challenge of estimating causal effects with text, encompassing settings where text is used as an outcome, treatment, or to address confounding. In addition, we explore potential uses of causal inference to improve the robustness, fairness, and interpretability of NLP models. We thus provide a unified overview of causal inference for the NLP community.1",
            "referenceCount": 128,
            "citationCount": 110,
            "influentialCitationCount": 6,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00511/2054690/tacl_a_00511.pdf",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2021-09-02",
            "journal": {
                "name": "Transactions of the Association for Computational Linguistics",
                "volume": "10"
            },
            "citationStyles": {
                "bibtex": "@Article{Feder2021CausalII,\n author = {Amir Feder and Katherine A. Keith and Emaad A. Manzoor and Reid Pryzant and Dhanya Sridhar and Zach Wood-Doughty and Jacob Eisenstein and Justin Grimmer and Roi Reichart and Margaret E. Roberts and Brandon M Stewart and Victor Veitch and Diyi Yang},\n booktitle = {Transactions of the Association for Computational Linguistics},\n journal = {Transactions of the Association for Computational Linguistics},\n pages = {1138-1158},\n title = {Causal Inference in Natural Language Processing: Estimation, Prediction, Interpretation and Beyond},\n volume = {10},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8b430ae5af9d7991cb3e698b2b30296fdf43dd15",
            "@type": "ScholarlyArticle",
            "paperId": "8b430ae5af9d7991cb3e698b2b30296fdf43dd15",
            "corpusId": 237298625,
            "url": "https://www.semanticscholar.org/paper/8b430ae5af9d7991cb3e698b2b30296fdf43dd15",
            "title": "Five sources of bias in natural language processing",
            "venue": "Language and Linguistics Compass",
            "publicationVenue": {
                "id": "urn:research:984325f8-27b7-4088-9a6d-c3de9e23beca",
                "name": "Language and Linguistics Compass",
                "alternate_names": [
                    "Lang Linguistics Compass"
                ],
                "issn": "1749-818X",
                "url": "http://www.blackwell-compass.com/subject/linguistics/"
            },
            "year": 2021,
            "externalIds": {
                "DBLP": "journals/llc/HovyP21",
                "PubMedCentral": "9285808",
                "DOI": "10.1111/lnc3.12432",
                "CorpusId": 237298625,
                "PubMed": "35864931"
            },
            "abstract": "Abstract Recently, there has been an increased interest in demographically grounded bias in natural language processing (NLP) applications. Much of the recent work has focused on describing bias and providing an overview of bias in a larger context. Here, we provide a simple, actionable summary of this recent work. We outline five sources where bias can occur in NLP systems: (1) the data, (2) the annotation process, (3) the input representations, (4) the models, and finally (5) the research design (or how we conceptualize our research). We explore each of the bias sources in detail in this article, including examples and links to related work, as well as potential counter\u2010measures.",
            "referenceCount": 110,
            "citationCount": 111,
            "influentialCitationCount": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/lnc3.12432",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2021-08-01",
            "journal": {
                "name": "Language and Linguistics Compass",
                "volume": "15"
            },
            "citationStyles": {
                "bibtex": "@Article{Hovy2021FiveSO,\n author = {E. Hovy and Shrimai Prabhumoye},\n booktitle = {Language and Linguistics Compass},\n journal = {Language and Linguistics Compass},\n title = {Five sources of bias in natural language processing},\n volume = {15},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b53c386b7c65af80905dc05a9b27e98e03324739",
            "@type": "ScholarlyArticle",
            "paperId": "b53c386b7c65af80905dc05a9b27e98e03324739",
            "corpusId": 231572779,
            "url": "https://www.semanticscholar.org/paper/b53c386b7c65af80905dc05a9b27e98e03324739",
            "title": "Trankit: A Light-Weight Transformer-based Toolkit for Multilingual Natural Language Processing",
            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:8de18c35-6785-4e54-99f2-21ee961302c6",
                "name": "Conference of the European Chapter of the Association for Computational Linguistics",
                "alternate_names": [
                    "Conf Eur Chapter Assoc Comput Linguistics",
                    "EACL"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/anthology/venues/eacl/"
            },
            "year": 2021,
            "externalIds": {
                "DBLP": "conf/eacl/NguyenLVN21",
                "ArXiv": "2101.03289",
                "ACL": "2021.eacl-demos.10",
                "DOI": "10.18653/v1/2021.eacl-demos.10",
                "CorpusId": 231572779
            },
            "abstract": "We introduce Trankit, a light-weight Transformer-based Toolkit for multilingual Natural Language Processing (NLP). It provides a trainable pipeline for fundamental NLP tasks over 100 languages, and 90 pretrained pipelines for 56 languages. Built on a state-of-the-art pretrained language model, Trankit significantly outperforms prior multilingual NLP pipelines over sentence segmentation, part-of-speech tagging, morphological feature tagging, and dependency parsing while maintaining competitive performance for tokenization, multi-word token expansion, and lemmatization over 90 Universal Dependencies treebanks. Despite the use of a large pretrained transformer, our toolkit is still efficient in memory usage and speed. This is achieved by our novel plug-and-play mechanism with Adapters where a multilingual pretrained transformer is shared across pipelines for different languages. Our toolkit along with pretrained models and code are publicly available at: https://github.com/nlp-uoregon/trankit. A demo website for our toolkit is also available at: http://nlp.uoregon.edu/trankit. Finally, we create a demo video for Trankit at: https://youtu.be/q0KGP3zGjGc.",
            "referenceCount": 44,
            "citationCount": 81,
            "influentialCitationCount": 14,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2021.eacl-demos.10.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2021-01-09",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Nguyen2021TrankitAL,\n author = {Minh Nguyen and Viet Dac Lai and Amir Pouran Ben Veyseh and Thien Huu Nguyen},\n booktitle = {Conference of the European Chapter of the Association for Computational Linguistics},\n pages = {80-90},\n title = {Trankit: A Light-Weight Transformer-based Toolkit for Multilingual Natural Language Processing},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:cddf40e579a596d0110b260313adf43470617c4c",
            "@type": "ScholarlyArticle",
            "paperId": "cddf40e579a596d0110b260313adf43470617c4c",
            "corpusId": 237431340,
            "url": "https://www.semanticscholar.org/paper/cddf40e579a596d0110b260313adf43470617c4c",
            "title": "Datasets: A Community Library for Natural Language Processing",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "publicationVenue": {
                "id": "urn:research:41bf9ed3-85b3-4c90-b015-150e31690253",
                "name": "Conference on Empirical Methods in Natural Language Processing",
                "alternate_names": [
                    "Empir Method Nat Lang Process",
                    "Empirical Methods in Natural Language Processing",
                    "Conf Empir Method Nat Lang Process",
                    "EMNLP"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/emnlp"
            },
            "year": 2021,
            "externalIds": {
                "DBLP": "journals/corr/abs-2109-02846",
                "ArXiv": "2109.02846",
                "ACL": "2021.emnlp-demo.21",
                "DOI": "10.18653/v1/2021.emnlp-demo.21",
                "CorpusId": 237431340
            },
            "abstract": "The scale, variety, and quantity of publicly-available NLP datasets has grown rapidly as researchers propose new tasks, larger models, and novel benchmarks. Datasets is a community library for contemporary NLP designed to support this ecosystem. Datasets aims to standardize end-user interfaces, versioning, and documentation, while providing a lightweight front-end that behaves similarly for small datasets as for internet-scale corpora. The design of the library incorporates a distributed, community-driven approach to adding datasets and documenting usage. After a year of development, the library now includes more than 650 unique datasets, has more than 250 contributors, and has helped support a variety of novel cross-dataset research projects and shared tasks. The library is available at https://github.com/huggingface/datasets.",
            "referenceCount": 37,
            "citationCount": 366,
            "influentialCitationCount": 15,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2021.emnlp-demo.21.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2021-09-07",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2109.02846"
            },
            "citationStyles": {
                "bibtex": "@Article{Lhoest2021DatasetsAC,\n author = {Quentin Lhoest and Albert Villanova del Moral and Yacine Jernite and A. Thakur and Patrick von Platen and Suraj Patil and Julien Chaumond and Mariama Drame and Julien Plu and Lewis Tunstall and Joe Davison and Mario vSavsko and Gunjan Chhablani and Bhavitvya Malik and Simon Brandeis and Teven Le Scao and Victor Sanh and Canwen Xu and N. Patry and Angelina McMillan-Major and Philipp Schmid and Sylvain Gugger and Clement Delangue and Th'eo Matussiere and Lysandre Debut and Stas Bekman and Pierric Cistac and Thibault Goehringer and Victor Mustar and Fran\u00e7ois Lagunas and Alexander M. Rush and Thomas Wolf},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n journal = {ArXiv},\n title = {Datasets: A Community Library for Natural Language Processing},\n volume = {abs/2109.02846},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:04ec406caebff60e226695c921f0af1b29162c5f",
            "@type": "ScholarlyArticle",
            "paperId": "04ec406caebff60e226695c921f0af1b29162c5f",
            "corpusId": 245537923,
            "url": "https://www.semanticscholar.org/paper/04ec406caebff60e226695c921f0af1b29162c5f",
            "title": "A Survey on Gender Bias in Natural Language Processing",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2021,
            "externalIds": {
                "ArXiv": "2112.14168",
                "DBLP": "journals/corr/abs-2112-14168",
                "CorpusId": 245537923
            },
            "abstract": "Language can be used as a means of reproducing and enforcing harmful stereotypes and biases and has been analysed as such in numerous research. In this paper, we present a survey of 304 papers on gender bias in natural language processing. We analyse definitions of gender and its categories within social sciences and connect them to formal definitions of gender bias in NLP research. We survey lexica and datasets applied in research on gender bias and then compare and contrast approaches to detecting and mitigating gender bias. We find that research on gender bias suffers from four core limitations. 1) Most research treats gender as a binary variable neglecting its fluidity and continuity. 2) Most of the work has been conducted in monolingual setups for English or other high-resource languages. 3) Despite a myriad of papers on gender bias in NLP methods, we find that most of the newly developed algorithms do not test their models for bias and disregard possible ethical considerations of their work. 4) Finally, methodologies developed in this line of research are fundamentally flawed covering very limited definitions of gender bias and lacking evaluation baselines and pipelines. We suggest recommendations towards overcoming these limitations as a guide for future research.",
            "referenceCount": 168,
            "citationCount": 65,
            "influentialCitationCount": 3,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2021-12-28",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2112.14168"
            },
            "citationStyles": {
                "bibtex": "@Article{Sta\u0144czak2021ASO,\n author = {Karolina Sta\u0144czak and Isabelle Augenstein},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {A Survey on Gender Bias in Natural Language Processing},\n volume = {abs/2112.14168},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c4358134954d8e62939c3a8b9ba8e953d951f73b",
            "@type": "ScholarlyArticle",
            "paperId": "c4358134954d8e62939c3a8b9ba8e953d951f73b",
            "corpusId": 234358053,
            "url": "https://www.semanticscholar.org/paper/c4358134954d8e62939c3a8b9ba8e953d951f73b",
            "title": "Including Signed Languages in Natural Language Processing",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                "name": "Annual Meeting of the Association for Computational Linguistics",
                "alternate_names": [
                    "Annu Meet Assoc Comput Linguistics",
                    "Meeting of the Association for Computational Linguistics",
                    "ACL",
                    "Meet Assoc Comput Linguistics"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/anthology/venues/acl/"
            },
            "year": 2021,
            "externalIds": {
                "ACL": "2021.acl-long.570",
                "DBLP": "conf/acl/YinMHGA20",
                "ArXiv": "2105.05222",
                "DOI": "10.18653/v1/2021.acl-long.570",
                "CorpusId": 234358053
            },
            "abstract": "Signed languages are the primary means of communication for many deaf and hard of hearing individuals. Since signed languages exhibit all the fundamental linguistic properties of natural language, we believe that tools and theories of Natural Language Processing (NLP) are crucial towards its modeling. However, existing research in Sign Language Processing (SLP) seldom attempt to explore and leverage the linguistic organization of signed languages. This position paper calls on the NLP community to include signed languages as a research area with high social and scientific impact. We first discuss the linguistic properties of signed languages to consider during their modeling. Then, we review the limitations of current SLP models and identify the open challenges to extend NLP to signed languages. Finally, we urge (1) the adoption of an efficient tokenization method; (2) the development of linguistically-informed models; (3) the collection of real-world signed language data; (4) the inclusion of local signed language communities as an active and leading voice in the direction of research.",
            "referenceCount": 122,
            "citationCount": 62,
            "influentialCitationCount": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2021.acl-long.570.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference",
                "Review"
            ],
            "publicationDate": "2021-05-11",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2105.05222"
            },
            "citationStyles": {
                "bibtex": "@Article{Yin2021IncludingSL,\n author = {Kayo Yin and Amit Moryossef and J. Hochgesang and Yoav Goldberg and Malihe Alikhani},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n journal = {ArXiv},\n title = {Including Signed Languages in Natural Language Processing},\n volume = {abs/2105.05222},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:106176d796eda5efcd8a4d84ce98267350d679b9",
            "@type": "ScholarlyArticle",
            "paperId": "106176d796eda5efcd8a4d84ce98267350d679b9",
            "corpusId": 238835461,
            "url": "https://www.semanticscholar.org/paper/106176d796eda5efcd8a4d84ce98267350d679b9",
            "title": "An introduction to Deep Learning in Natural Language Processing: Models, techniques, and tools",
            "venue": "Neurocomputing",
            "publicationVenue": {
                "id": "urn:research:df12d289-f447-47d3-8846-75e39de3ab57",
                "name": "Neurocomputing",
                "alternate_names": null,
                "issn": "0925-2312",
                "url": "http://www.elsevier.com/locate/neucom"
            },
            "year": 2021,
            "externalIds": {
                "DBLP": "journals/ijon/LauriolaLA22",
                "MAG": "3190730109",
                "DOI": "10.1016/j.neucom.2021.05.103",
                "CorpusId": 238835461
            },
            "abstract": null,
            "referenceCount": 97,
            "citationCount": 161,
            "influentialCitationCount": 3,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2021-07-01",
            "journal": {
                "name": "Neurocomputing",
                "volume": "470"
            },
            "citationStyles": {
                "bibtex": "@Article{Lauriola2021AnIT,\n author = {Ivano Lauriola and A. Lavelli and F. Aiolli},\n booktitle = {Neurocomputing},\n journal = {Neurocomputing},\n pages = {443-456},\n title = {An introduction to Deep Learning in Natural Language Processing: Models, techniques, and tools},\n volume = {470},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:88338c58701f34503c7af77e34f19d9a5cd66313",
            "@type": "ScholarlyArticle",
            "paperId": "88338c58701f34503c7af77e34f19d9a5cd66313",
            "corpusId": 118674722,
            "url": "https://www.semanticscholar.org/paper/88338c58701f34503c7af77e34f19d9a5cd66313",
            "title": "Adversarial Attacks on Deep-learning Models in Natural Language Processing",
            "venue": "ACM Transactions on Intelligent Systems and Technology",
            "publicationVenue": {
                "id": "urn:research:0d993d4a-09ba-4df8-90a4-7dfe25f0cb9e",
                "name": "ACM Transactions on Intelligent Systems and Technology",
                "alternate_names": [
                    "ACM Trans Intell Syst Technol"
                ],
                "issn": "2157-6904",
                "url": "http://portal.acm.org/tist"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3015001695",
                "DBLP": "journals/tist/ZhangSAL20",
                "DOI": "10.1145/3374217",
                "CorpusId": 118674722
            },
            "abstract": "With the development of high computational devices, deep neural networks (DNNs), in recent years, have gained significant popularity in many Artificial Intelligence (AI) applications. However, previous efforts have shown that DNNs are vulnerable to strategically modified samples, named adversarial examples. These samples are generated with some imperceptible perturbations, but can fool the DNNs to give false predictions. Inspired by the popularity of generating adversarial examples against DNNs in Computer Vision (CV), research efforts on attacking DNNs for Natural Language Processing (NLP) applications have emerged in recent years. However, the intrinsic difference between image (CV) and text (NLP) renders challenges to directly apply attacking methods in CV to NLP. Various methods are proposed addressing this difference and attack a wide range of NLP applications. In this article, we present a systematic survey on these works. We collect all related academic works since the first appearance in 2017. We then select, summarize, discuss, and analyze 40 representative works in a comprehensive way. To make the article self-contained, we cover preliminary knowledge of NLP and discuss related seminal works in computer vision. We conclude our survey with a discussion on open issues to bridge the gap between the existing progress and more robust adversarial attacks on NLP DNNs.",
            "referenceCount": 157,
            "citationCount": 317,
            "influentialCitationCount": 32,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2020-04-01",
            "journal": {
                "name": "ACM Transactions on Intelligent Systems and Technology (TIST)",
                "volume": "11"
            },
            "citationStyles": {
                "bibtex": "@Article{Zhang2020AdversarialAO,\n author = {W. Zhang and Quan Z. Sheng and A. Alhazmi and Chenliang Li},\n booktitle = {ACM Transactions on Intelligent Systems and Technology},\n journal = {ACM Transactions on Intelligent Systems and Technology (TIST)},\n pages = {1 - 41},\n title = {Adversarial Attacks on Deep-learning Models in Natural Language Processing},\n volume = {11},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:cf54af6e6a1d71aa30fdd804afea116438681412",
            "@type": "ScholarlyArticle",
            "paperId": "cf54af6e6a1d71aa30fdd804afea116438681412",
            "corpusId": 235080553,
            "url": "https://www.semanticscholar.org/paper/cf54af6e6a1d71aa30fdd804afea116438681412",
            "title": "State of Art for Semantic Analysis of Natural Language Processing",
            "venue": "Qubahan Academic Journal",
            "publicationVenue": {
                "id": "urn:research:4ab6e851-5718-4087-8d3b-39fce3285721",
                "name": "Qubahan Academic Journal",
                "alternate_names": [
                    "Qubahan Acad J"
                ],
                "issn": "2709-8206",
                "url": null
            },
            "year": 2021,
            "externalIds": {
                "MAG": "3146248749",
                "DOI": "10.48161/QAJ.V1N2A44",
                "CorpusId": 235080553
            },
            "abstract": "Semantic analysis is an essential feature of the NLP approach. It indicates, in the appropriate format, the context of a sentence or paragraph. Semantics is about language significance study. The vocabulary used conveys the importance of the subject because of the interrelationship between linguistic classes. In this article, semantic interpretation is carried out in the area of Natural Language Processing. The findings suggest that the best-achieved accuracy of checked papers and those who relied on the Sentiment Analysis approach and the prediction error is minimal.",
            "referenceCount": 41,
            "citationCount": 84,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2021-03-30",
            "journal": {
                "name": "Qubahan Academic Journal",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Maulud2021StateOA,\n author = {Dastan Hussen Maulud and Subhi R. M. Zeebaree and Karwan Jacksi and M. A. Sadeeq and K. Sharif},\n booktitle = {Qubahan Academic Journal},\n journal = {Qubahan Academic Journal},\n title = {State of Art for Semantic Analysis of Natural Language Processing},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6c761cfdb031701072582e434d8f64d436255da6",
            "@type": "ScholarlyArticle",
            "paperId": "6c761cfdb031701072582e434d8f64d436255da6",
            "corpusId": 236987275,
            "url": "https://www.semanticscholar.org/paper/6c761cfdb031701072582e434d8f64d436255da6",
            "title": "AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2021,
            "externalIds": {
                "ArXiv": "2108.05542",
                "DBLP": "journals/corr/abs-2108-05542",
                "CorpusId": 236987275
            },
            "abstract": "Transformer-based pretrained language models (T-PTLMs) have achieved great success in almost every NLP task. The evolution of these models started with GPT and BERT. These models are built on the top of transformers, self-supervised learning and transfer learning. Transformed-based PTLMs learn universal language representations from large volumes of text data using self-supervised learning and transfer this knowledge to downstream tasks. These models provide good background knowledge to downstream tasks which avoids training of downstream models from scratch. In this comprehensive survey paper, we initially give a brief overview of self-supervised learning. Next, we explain various core concepts like pretraining, pretraining methods, pretraining tasks, embeddings and downstream adaptation methods. Next, we present a new taxonomy of T-PTLMs and then give brief overview of various benchmarks including both intrinsic and extrinsic. We present a summary of various useful libraries to work with T-PTLMs. Finally, we highlight some of the future research directions which will further improve these models. We strongly believe that this comprehensive survey paper will serve as a good reference to learn the core concepts as well as to stay updated with the recent happenings in T-PTLMs.",
            "referenceCount": 304,
            "citationCount": 137,
            "influentialCitationCount": 14,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2021-08-12",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2108.05542"
            },
            "citationStyles": {
                "bibtex": "@Article{Kalyan2021AMMUSA,\n author = {Katikapalli Subramanyam Kalyan and A. Rajasekharan and S. Sangeetha},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing},\n volume = {abs/2108.05542},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:37ee23634a9f7101ed3cd479222b3b16b40fedf0",
            "@type": "ScholarlyArticle",
            "paperId": "37ee23634a9f7101ed3cd479222b3b16b40fedf0",
            "corpusId": 237940861,
            "url": "https://www.semanticscholar.org/paper/37ee23634a9f7101ed3cd479222b3b16b40fedf0",
            "title": "Paradigm Shift in Natural Language Processing",
            "venue": "Machine Intelligence Research",
            "publicationVenue": {
                "id": "urn:research:1caabc5e-b06a-4ba8-bccd-8d3b71322232",
                "name": "Machine Intelligence Research",
                "alternate_names": [
                    "Mach Intell Res"
                ],
                "issn": "2731-538X",
                "url": null
            },
            "year": 2021,
            "externalIds": {
                "ArXiv": "2109.12575",
                "DBLP": "journals/ijautcomp/SunLQH22",
                "DOI": "10.1007/s11633-022-1331-6",
                "CorpusId": 237940861
            },
            "abstract": null,
            "referenceCount": 106,
            "citationCount": 61,
            "influentialCitationCount": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/s11633-022-1331-6.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2021-09-26",
            "journal": {
                "name": "Machine Intelligence Research",
                "volume": "19"
            },
            "citationStyles": {
                "bibtex": "@Article{Sun2021ParadigmSI,\n author = {Tianxiang Sun and Xiangyang Liu and Xipeng Qiu and Xuanjing Huang},\n booktitle = {Machine Intelligence Research},\n journal = {Machine Intelligence Research},\n pages = {169 - 183},\n title = {Paradigm Shift in Natural Language Processing},\n volume = {19},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8bd130a5531622d58f819db4cab743a83eae90e6",
            "@type": "ScholarlyArticle",
            "paperId": "8bd130a5531622d58f819db4cab743a83eae90e6",
            "corpusId": 234817664,
            "url": "https://www.semanticscholar.org/paper/8bd130a5531622d58f819db4cab743a83eae90e6",
            "title": "Natural Language Processing for Requirements Engineering",
            "venue": "ACM Computing Surveys",
            "publicationVenue": {
                "id": "urn:research:7b2adce0-d53f-49d6-8784-b0645604fe62",
                "name": "ACM Computing Surveys",
                "alternate_names": [
                    "ACM Comput Surv"
                ],
                "issn": "0360-0300",
                "url": "http://www.acm.org/pubs/surveys/"
            },
            "year": 2021,
            "externalIds": {
                "MAG": "3153389197",
                "DBLP": "journals/csur/ZhaoAFLACB21",
                "DOI": "10.1145/3444689",
                "CorpusId": 234817664
            },
            "abstract": "Natural Language Processing for Requirements Engineering (NLP4RE) is an area of research and development that seeks to apply natural language processing (NLP) techniques, tools, and resources to the requirements engineering (RE) process, to support human analysts to carry out various linguistic analysis tasks on textual requirements documents, such as detecting language issues, identifying key domain concepts, and establishing requirements traceability links. This article reports on a mapping study that surveys the landscape of NLP4RE research to provide a holistic understanding of the field. Following the guidance of systematic review, the mapping study is directed by five research questions, cutting across five aspects of NLP4RE research, concerning the state of the literature, the state of empirical research, the research focus, the state of tool development, and the usage of NLP technologies. Our main results are as follows: (i) we identify a total of 404 primary studies relevant to NLP4RE, which were published over the past 36 years and from 170 different venues; (ii) most of these studies (67.08%) are solution proposals, assessed by a laboratory experiment or an example application, while only a small percentage (7%) are assessed in industrial settings; (iii) a large proportion of the studies (42.70%) focus on the requirements analysis phase, with quality defect detection as their central task and requirements specification as their commonly processed document type; (iv) 130 NLP4RE tools (i.e., RE specific NLP tools) are extracted from these studies, but only 17 of them (13.08%) are available for download; (v) 231 different NLP technologies are also identified, comprising 140 NLP techniques, 66 NLP tools, and 25 NLP resources, but most of them\u2014particularly those novel NLP techniques and specialized tools\u2014are used infrequently; by contrast, commonly used NLP technologies are traditional analysis techniques (e.g., POS tagging and tokenization), general-purpose tools (e.g., Stanford CoreNLP and GATE) and generic language lexicons (WordNet and British National Corpus). The mapping study not only provides a collection of the literature in NLP4RE but also, more importantly, establishes a structure to frame the existing literature through categorization, synthesis and conceptualization of the main theoretical concepts and relationships that encompass both RE and NLP aspects. Our work thus produces a conceptual framework of NLP4RE. The framework is used to identify research gaps and directions, highlight technology transfer needs, and encourage more synergies between the RE community, the NLP one, and the software and systems practitioners. Our results can be used as a starting point to frame future studies according to a well-defined terminology and can be expanded as new technologies and novel solutions emerge.",
            "referenceCount": 121,
            "citationCount": 50,
            "influentialCitationCount": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2004.01099",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2021-04-17",
            "journal": {
                "name": "ACM Computing Surveys (CSUR)",
                "volume": "54"
            },
            "citationStyles": {
                "bibtex": "@Article{Zhao2021NaturalLP,\n author = {Liping Zhao and Waad Alhoshan and Alessio Ferrari and Keletso J. Letsholo and Muideen A. Ajagbe and Erol-Valeriu Chioasca and R. Batista-Navarro},\n booktitle = {ACM Computing Surveys},\n journal = {ACM Computing Surveys (CSUR)},\n pages = {1 - 41},\n title = {Natural Language Processing for Requirements Engineering},\n volume = {54},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:97bfa89addc6e5d76361e4c1e296949cad887b86",
            "@type": "ScholarlyArticle",
            "paperId": "97bfa89addc6e5d76361e4c1e296949cad887b86",
            "corpusId": 52255687,
            "url": "https://www.semanticscholar.org/paper/97bfa89addc6e5d76361e4c1e296949cad887b86",
            "title": "Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science",
            "venue": "Transactions of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:e0dbf116-86aa-418d-859f-a49952d7e44a",
                "name": "Transactions of the Association for Computational Linguistics",
                "alternate_names": [
                    "Trans Assoc Comput Linguistics",
                    "TACL"
                ],
                "issn": "2307-387X",
                "url": "https://www.mitpressjournals.org/loi/tacl"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/tacl/BenderF18",
                "ACL": "Q18-1041",
                "MAG": "2911227954",
                "DOI": "10.1162/tacl_a_00041",
                "CorpusId": 52255687
            },
            "abstract": "In this paper, we propose data statements as a design solution and professional practice for natural language processing technologists, in both research and development. Through the adoption and widespread use of data statements, the field can begin to address critical scientific and ethical issues that result from the use of data from certain populations in the development of technology for other populations. We present a form that data statements can take and explore the implications of adopting them as part of regular practice. We argue that data statements will help alleviate issues related to exclusion and bias in language technology, lead to better precision in claims about how natural language processing research can generalize and thus better engineering results, protect companies from public embarrassment, and ultimately lead to language technology that meets its users in their own preferred linguistic style and furthermore does not misrepresent them to others.",
            "referenceCount": 64,
            "citationCount": 609,
            "influentialCitationCount": 44,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00041/1567666/tacl_a_00041.pdf",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-12-01",
            "journal": {
                "name": "Transactions of the Association for Computational Linguistics",
                "volume": "6"
            },
            "citationStyles": {
                "bibtex": "@Article{Bender2018DataSF,\n author = {Emily M. Bender and Batya Friedman},\n booktitle = {Transactions of the Association for Computational Linguistics},\n journal = {Transactions of the Association for Computational Linguistics},\n pages = {587-604},\n title = {Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science},\n volume = {6},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2792f3a8058c7a019b2a18fb3b7e43c828086b81",
            "@type": "ScholarlyArticle",
            "paperId": "2792f3a8058c7a019b2a18fb3b7e43c828086b81",
            "corpusId": 43922517,
            "url": "https://www.semanticscholar.org/paper/2792f3a8058c7a019b2a18fb3b7e43c828086b81",
            "title": "Deep Learning for Natural Language Processing",
            "venue": "International Journal for Research in Applied Science and Engineering Technology",
            "publicationVenue": {
                "id": "urn:research:100ffd12-b334-4f02-826b-8ac408e7df49",
                "name": "International Journal for Research in Applied Science and Engineering Technology",
                "alternate_names": [
                    "Int J Res Appl Sci Eng Technol"
                ],
                "issn": "2321-9653",
                "url": "https://www.ijraset.com/"
            },
            "year": 2021,
            "externalIds": {
                "MAG": "3157984936",
                "DOI": "10.22214/IJRASET.2021.33702",
                "CorpusId": 43922517
            },
            "abstract": ",",
            "referenceCount": 14,
            "citationCount": 136,
            "influentialCitationCount": 9,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2021-04-30",
            "journal": {
                "name": "International Journal for Research in Applied Science and Engineering Technology",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{He2021DeepLF,\n author = {Xiaodong He and Jianfeng Gao and L. Deng and S. Yih and Yu and J. Markoff and Richard F. Rashid and Richard F. Rashid and Geoffrey E. Hinton and Dong Yu},\n booktitle = {International Journal for Research in Applied Science and Engineering Technology},\n journal = {International Journal for Research in Applied Science and Engineering Technology},\n title = {Deep Learning for Natural Language Processing},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1a3a0047d74639784e7a7450854c28c9e7bdaf0a",
            "@type": "ScholarlyArticle",
            "paperId": "1a3a0047d74639784e7a7450854c28c9e7bdaf0a",
            "corpusId": 232035689,
            "url": "https://www.semanticscholar.org/paper/1a3a0047d74639784e7a7450854c28c9e7bdaf0a",
            "title": "Teach Me to Explain: A Review of Datasets for Explainable Natural Language Processing",
            "venue": "NeurIPS Datasets and Benchmarks",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2021,
            "externalIds": {
                "DBLP": "conf/nips/WiegreffeM21",
                "ArXiv": "2102.12060",
                "CorpusId": 232035689
            },
            "abstract": "Explainable NLP (ExNLP) has increasingly focused on collecting human-annotated textual explanations. These explanations are used downstream in three ways: as data augmentation to improve performance on a predictive task, as supervision to train models to produce explanations for their predictions, and as a ground-truth to evaluate model-generated explanations. In this review, we identify 65 datasets with three predominant classes of textual explanations (highlights, free-text, and structured), organize the literature on annotating each type, identify strengths and shortcomings of existing collection methodologies, and give recommendations for collecting ExNLP datasets in the future.",
            "referenceCount": 171,
            "citationCount": 120,
            "influentialCitationCount": 7,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2021-02-24",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Wiegreffe2021TeachMT,\n author = {Sarah Wiegreffe and Ana Marasovi\u0107},\n booktitle = {NeurIPS Datasets and Benchmarks},\n title = {Teach Me to Explain: A Review of Datasets for Explainable Natural Language Processing},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:eeec016e6cb725efce680d8172c1fcaf58727a7e",
            "@type": "ScholarlyArticle",
            "paperId": "eeec016e6cb725efce680d8172c1fcaf58727a7e",
            "corpusId": 238354115,
            "url": "https://www.semanticscholar.org/paper/eeec016e6cb725efce680d8172c1fcaf58727a7e",
            "title": "Data Augmentation Approaches in Natural Language Processing: A Survey",
            "venue": "AI Open",
            "publicationVenue": {
                "id": "urn:research:6c35576a-a87d-4dc1-a576-780572d8d0e6",
                "name": "AI Open",
                "alternate_names": null,
                "issn": "2666-6510",
                "url": "https://www.keaipublishing.com/en/journals/ai-open/"
            },
            "year": 2021,
            "externalIds": {
                "DBLP": "journals/aiopen/LiHC22",
                "ArXiv": "2110.01852",
                "DOI": "10.1016/j.aiopen.2022.03.001",
                "CorpusId": 238354115
            },
            "abstract": null,
            "referenceCount": 154,
            "citationCount": 118,
            "influentialCitationCount": 7,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2021-10-05",
            "journal": {
                "name": "AI Open",
                "volume": "3"
            },
            "citationStyles": {
                "bibtex": "@Article{Li2021DataAA,\n author = {Bohan Li and Yutai Hou and Wanxiang Che},\n booktitle = {AI Open},\n journal = {AI Open},\n pages = {71-90},\n title = {Data Augmentation Approaches in Natural Language Processing: A Survey},\n volume = {3},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:00e1d284260e2f8be8cd47c0bb22a1892d91deba",
            "@type": "ScholarlyArticle",
            "paperId": "00e1d284260e2f8be8cd47c0bb22a1892d91deba",
            "corpusId": 232076239,
            "url": "https://www.semanticscholar.org/paper/00e1d284260e2f8be8cd47c0bb22a1892d91deba",
            "title": "Combat COVID-19 infodemic using explainable natural language processing models",
            "venue": "Information Processing & Management",
            "publicationVenue": {
                "id": "urn:research:37f5b9b7-f828-4ae1-a174-45b538cbd4e4",
                "name": "Information Processing & Management",
                "alternate_names": [
                    "Inf Process Manag",
                    "Inf Process  Manag",
                    "Information Processing and Management"
                ],
                "issn": "0306-4573",
                "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/244/description#description"
            },
            "year": 2021,
            "externalIds": {
                "DBLP": "journals/ipm/AyoubYZ21",
                "ArXiv": "2103.00747",
                "PubMedCentral": "7980090",
                "MAG": "3135835076",
                "DOI": "10.1016/j.ipm.2021.102569",
                "CorpusId": 232076239,
                "PubMed": "33776192"
            },
            "abstract": null,
            "referenceCount": 51,
            "citationCount": 82,
            "influentialCitationCount": 5,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2021-03-01",
            "journal": {
                "name": "Information Processing & Management",
                "volume": "58"
            },
            "citationStyles": {
                "bibtex": "@Article{Ayoub2021CombatCI,\n author = {Jackie Ayoub and X. J. Yang and Feng Zhou},\n booktitle = {Information Processing & Management},\n journal = {Information Processing & Management},\n pages = {102569 - 102569},\n title = {Combat COVID-19 infodemic using explainable natural language processing models},\n volume = {58},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6661802f5e1ee004c20a28dcce9b582d4b5fe6d7",
            "@type": "ScholarlyArticle",
            "paperId": "6661802f5e1ee004c20a28dcce9b582d4b5fe6d7",
            "corpusId": 9626793,
            "url": "https://www.semanticscholar.org/paper/6661802f5e1ee004c20a28dcce9b582d4b5fe6d7",
            "title": "A Survey on Hate Speech Detection using Natural Language Processing",
            "venue": "SocialNLP@EACL",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2017,
            "externalIds": {
                "ACL": "W17-1101",
                "DBLP": "conf/acl-socialnlp/SchmidtW17",
                "MAG": "2740168486",
                "DOI": "10.18653/v1/W17-1101",
                "CorpusId": 9626793
            },
            "abstract": "This paper presents a survey on hate speech detection. Given the steadily growing body of social media content, the amount of online hate speech is also increasing. Due to the massive scale of the web, methods that automatically detect hate speech are required. Our survey describes key areas that have been explored to automatically recognize these types of utterances using natural language processing. We also discuss limits of those approaches.",
            "referenceCount": 41,
            "citationCount": 999,
            "influentialCitationCount": 76,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/W17-1101.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2017-04-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Schmidt2017ASO,\n author = {Anna Schmidt and Michael Wiegand},\n booktitle = {SocialNLP@EACL},\n pages = {1-10},\n title = {A Survey on Hate Speech Detection using Natural Language Processing},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:452059171226626718eb677358836328f884298e",
            "@type": "ScholarlyArticle",
            "paperId": "452059171226626718eb677358836328f884298e",
            "corpusId": 2319779,
            "url": "https://www.semanticscholar.org/paper/452059171226626718eb677358836328f884298e",
            "title": "Ask Me Anything: Dynamic Memory Networks for Natural Language Processing",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "2949215902",
                "ArXiv": "1506.07285",
                "DBLP": "conf/icml/KumarIOIBGZPS16",
                "CorpusId": 2319779
            },
            "abstract": "Most tasks in natural language processing can be cast into question answering (QA) problems over language input. We introduce the dynamic memory network (DMN), a neural network architecture which processes input sequences and questions, forms episodic memories, and generates relevant answers. Questions trigger an iterative attention process which allows the model to condition its attention on the inputs and the result of previous iterations. These results are then reasoned over in a hierarchical recurrent sequence model to generate answers. The DMN can be trained end-to-end and obtains state-of-the-art results on several types of tasks and datasets: question answering (Facebook's bAbI dataset), text classification for sentiment analysis (Stanford Sentiment Treebank) and sequence modeling for part-of-speech tagging (WSJ-PTB). The training for these different tasks relies exclusively on trained word vector representations and input-question-answer triplets.",
            "referenceCount": 52,
            "citationCount": 1139,
            "influentialCitationCount": 103,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-06-24",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Kumar2015AskMA,\n author = {A. Kumar and Ozan Irsoy and Peter Ondruska and Mohit Iyyer and James Bradbury and Ishaan Gulrajani and Victor Zhong and Romain Paulus and R. Socher},\n booktitle = {International Conference on Machine Learning},\n pages = {1378-1387},\n title = {Ask Me Anything: Dynamic Memory Networks for Natural Language Processing},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:493fac37cea49afb98c52c2f5dd75c303a325b25",
            "@type": "ScholarlyArticle",
            "paperId": "493fac37cea49afb98c52c2f5dd75c303a325b25",
            "corpusId": 195316733,
            "url": "https://www.semanticscholar.org/paper/493fac37cea49afb98c52c2f5dd75c303a325b25",
            "title": "Mitigating Gender Bias in Natural Language Processing: Literature Review",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                "name": "Annual Meeting of the Association for Computational Linguistics",
                "alternate_names": [
                    "Annu Meet Assoc Comput Linguistics",
                    "Meeting of the Association for Computational Linguistics",
                    "ACL",
                    "Meet Assoc Comput Linguistics"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/anthology/venues/acl/"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2951864292",
                "DBLP": "journals/corr/abs-1906-08976",
                "ACL": "P19-1159",
                "ArXiv": "1906.08976",
                "DOI": "10.18653/v1/P19-1159",
                "CorpusId": 195316733
            },
            "abstract": "As Natural Language Processing (NLP) and Machine Learning (ML) tools rise in popularity, it becomes increasingly vital to recognize the role they play in shaping societal biases and stereotypes. Although NLP models have shown success in modeling various applications, they propagate and may even amplify gender bias found in text corpora. While the study of bias in artificial intelligence is not new, methods to mitigate gender bias in NLP are relatively nascent. In this paper, we review contemporary studies on recognizing and mitigating gender bias in NLP. We discuss gender bias based on four forms of representation bias and analyze methods recognizing gender bias. Furthermore, we discuss the advantages and drawbacks of existing gender debiasing methods. Finally, we discuss future studies for recognizing and mitigating gender bias in NLP.",
            "referenceCount": 63,
            "citationCount": 390,
            "influentialCitationCount": 29,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1906.08976",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Sociology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference",
                "Review"
            ],
            "publicationDate": "2019-06-21",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Sun2019MitigatingGB,\n author = {Tony Sun and Andrew Gaut and Shirlyn Tang and Yuxin Huang and Mai Elsherief and Jieyu Zhao and Diba Mirza and E. Belding-Royer and Kai-Wei Chang and William Yang Wang},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {1630-1640},\n title = {Mitigating Gender Bias in Natural Language Processing: Literature Review},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5e8d3c2dc0fc53949794fc00600e25558c4a2441",
            "@type": "ScholarlyArticle",
            "paperId": "5e8d3c2dc0fc53949794fc00600e25558c4a2441",
            "corpusId": 246016339,
            "url": "https://www.semanticscholar.org/paper/5e8d3c2dc0fc53949794fc00600e25558c4a2441",
            "title": "WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "publicationVenue": {
                "id": "urn:research:41bf9ed3-85b3-4c90-b015-150e31690253",
                "name": "Conference on Empirical Methods in Natural Language Processing",
                "alternate_names": [
                    "Empir Method Nat Lang Process",
                    "Empirical Methods in Natural Language Processing",
                    "Conf Empir Method Nat Lang Process",
                    "EMNLP"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/emnlp"
            },
            "year": 2022,
            "externalIds": {
                "DBLP": "journals/corr/abs-2201-05955",
                "ArXiv": "2201.05955",
                "DOI": "10.18653/v1/2022.findings-emnlp.508",
                "CorpusId": 246016339
            },
            "abstract": "A recurring challenge of crowdsourcing NLP datasets at scale is that human writers often rely on repetitive patterns when crafting examples, leading to a lack of linguistic diversity. We introduce a novel approach for dataset creation based on worker and AI collaboration, which brings together the generative strength of language models and the evaluative strength of humans. Starting with an existing dataset, MultiNLI for natural language inference (NLI), our approach uses dataset cartography to automatically identify examples that demonstrate challenging reasoning patterns, and instructs GPT-3 to compose new examples with similar patterns. Machine generated examples are then automatically filtered, and finally revised and labeled by human crowdworkers. The resulting dataset, WANLI, consists of 107,885 NLI examples and presents unique empirical strengths over existing NLI datasets. Remarkably, training a model on WANLI improves performance on eight out-of-domain test sets we consider, including by 11% on HANS and 9% on Adversarial NLI, compared to training on the 4x larger MultiNLI. Moreover, it continues to be more effective than MultiNLI augmented with other NLI datasets. Our results demonstrate the promise of leveraging natural language generation techniques and re-imagining the role of humans in the dataset creation process.",
            "referenceCount": 81,
            "citationCount": 128,
            "influentialCitationCount": 13,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2022.findings-emnlp.508.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2022-01-16",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Liu2022WANLIWA,\n author = {Alisa Liu and Swabha Swayamdipta and Noah A. Smith and Yejin Choi},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {6826-6847},\n title = {WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation},\n year = {2022}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f52b48a988489ed4d1c53f98e77aa34669547a94",
            "@type": "ScholarlyArticle",
            "paperId": "f52b48a988489ed4d1c53f98e77aa34669547a94",
            "corpusId": 17400045,
            "url": "https://www.semanticscholar.org/paper/f52b48a988489ed4d1c53f98e77aa34669547a94",
            "title": "Comparative Study of CNN and RNN for Natural Language Processing",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2587019100",
                "DBLP": "journals/corr/0001KYS17",
                "ArXiv": "1702.01923",
                "CorpusId": 17400045
            },
            "abstract": "Deep neural networks (DNN) have revolutionized the field of natural language processing (NLP). Convolutional neural network (CNN) and recurrent neural network (RNN), the two main types of DNN architectures, are widely explored to handle various NLP tasks. CNN is supposed to be good at extracting position-invariant features and RNN at modeling units in sequence. The state of the art on many NLP tasks often switches due to the battle between CNNs and RNNs. This work is the first systematic comparison of CNN and RNN on a wide range of representative NLP tasks, aiming to give basic guidance for DNN selection.",
            "referenceCount": 25,
            "citationCount": 819,
            "influentialCitationCount": 51,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-02-07",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1702.01923"
            },
            "citationStyles": {
                "bibtex": "@Article{Yin2017ComparativeSO,\n author = {Wenpeng Yin and Katharina Kann and Mo Yu and Hinrich Sch\u00fctze},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Comparative Study of CNN and RNN for Natural Language Processing},\n volume = {abs/1702.01923},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:cfdd423c8672a7b178ea85d56079328df4eea647",
            "@type": "ScholarlyArticle",
            "paperId": "cfdd423c8672a7b178ea85d56079328df4eea647",
            "corpusId": 207104425,
            "url": "https://www.semanticscholar.org/paper/cfdd423c8672a7b178ea85d56079328df4eea647",
            "title": "Steven Bird, Ewan Klein and Edward Loper: Natural Language Processing with Python, Analyzing Text with the Natural Language Toolkit",
            "venue": "Language Resources and Evaluation",
            "publicationVenue": {
                "id": "urn:research:7dda5bd1-752f-45e5-bc7b-09633096916e",
                "name": "Language Resources and Evaluation",
                "alternate_names": [
                    "Lang Resour Evaluation"
                ],
                "issn": "1574-020X",
                "url": "https://link.springer.com/journal/10579"
            },
            "year": 2010,
            "externalIds": {
                "DBLP": "journals/lre/Wagner10",
                "MAG": "1530622307",
                "DOI": "10.1007/S10579-010-9124-X",
                "CorpusId": 207104425
            },
            "abstract": null,
            "referenceCount": 2,
            "citationCount": 2358,
            "influentialCitationCount": 206,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2010-12-01",
            "journal": {
                "name": "Language Resources and Evaluation",
                "volume": "44"
            },
            "citationStyles": {
                "bibtex": "@Article{Wagner2010StevenBE,\n author = {Wiebke Wagner},\n booktitle = {Language Resources and Evaluation},\n journal = {Language Resources and Evaluation},\n pages = {421-424},\n title = {Steven Bird, Ewan Klein and Edward Loper: Natural Language Processing with Python, Analyzing Text with the Natural Language Toolkit},\n volume = {44},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f04df4e20a18358ea2f689b4c129781628ef7fc1",
            "@type": "ScholarlyArticle",
            "paperId": "f04df4e20a18358ea2f689b4c129781628ef7fc1",
            "corpusId": 14604520,
            "url": "https://www.semanticscholar.org/paper/f04df4e20a18358ea2f689b4c129781628ef7fc1",
            "title": "A large annotated corpus for learning natural language inference",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "publicationVenue": {
                "id": "urn:research:41bf9ed3-85b3-4c90-b015-150e31690253",
                "name": "Conference on Empirical Methods in Natural Language Processing",
                "alternate_names": [
                    "Empir Method Nat Lang Process",
                    "Empirical Methods in Natural Language Processing",
                    "Conf Empir Method Nat Lang Process",
                    "EMNLP"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/emnlp"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "2953084091",
                "DBLP": "conf/emnlp/BowmanAPM15",
                "ACL": "D15-1075",
                "ArXiv": "1508.05326",
                "DOI": "10.18653/v1/D15-1075",
                "CorpusId": 14604520
            },
            "abstract": "Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by the lack of large-scale resources. To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning. At 570K pairs, it is two orders of magnitude larger than all other resources of its type. This increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time.",
            "referenceCount": 40,
            "citationCount": 3531,
            "influentialCitationCount": 912,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.18653/v1/d15-1075",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-08-21",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Bowman2015ALA,\n author = {Samuel R. Bowman and Gabor Angeli and Christopher Potts and Christopher D. Manning},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {632-642},\n title = {A large annotated corpus for learning natural language inference},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4b322cf280f459deb6d9e2eb2430d1a28141934c",
            "@type": "ScholarlyArticle",
            "paperId": "4b322cf280f459deb6d9e2eb2430d1a28141934c",
            "corpusId": 222125099,
            "url": "https://www.semanticscholar.org/paper/4b322cf280f459deb6d9e2eb2430d1a28141934c",
            "title": "A Survey of the State of Explainable AI for Natural Language Processing",
            "venue": "AACL",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3090395639",
                "ArXiv": "2010.00711",
                "DBLP": "journals/corr/abs-2010-00711",
                "ACL": "2020.aacl-main.46",
                "CorpusId": 222125099
            },
            "abstract": "Recent years have seen important advances in the quality of state-of-the-art models, but this has come at the expense of models becoming less interpretable. This survey presents an overview of the current state of Explainable AI (XAI), considered within the domain of Natural Language Processing (NLP). We discuss the main categorization of explanations, as well as the various ways explanations can be arrived at and visualized. We detail the operations and explainability techniques currently available for generating explanations for NLP model predictions, to serve as a resource for model developers in the community. Finally, we point out the current gaps and encourage directions for future work in this important research area.",
            "referenceCount": 82,
            "citationCount": 230,
            "influentialCitationCount": 18,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2020-10-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Danilevsky2020ASO,\n author = {Marina Danilevsky and Kun Qian and R. Aharonov and Yannis Katsis and B. Kawas and P. Sen},\n booktitle = {AACL},\n pages = {447-459},\n title = {A Survey of the State of Explainable AI for Natural Language Processing},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8a191a131b2d81783ca9baac21f499548821c032",
            "@type": "ScholarlyArticle",
            "paperId": "8a191a131b2d81783ca9baac21f499548821c032",
            "corpusId": 238744248,
            "url": "https://www.semanticscholar.org/paper/8a191a131b2d81783ca9baac21f499548821c032",
            "title": "The Dawn of Quantum Natural Language Processing",
            "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
            "publicationVenue": {
                "id": "urn:research:0d6f7fba-7092-46b3-8039-93458dba736b",
                "name": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
                "alternate_names": [
                    "Int Conf Acoust Speech Signal Process",
                    "IEEE Int Conf Acoust Speech Signal Process",
                    "ICASSP",
                    "International Conference on Acoustics, Speech, and Signal Processing"
                ],
                "issn": null,
                "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000002"
            },
            "year": 2021,
            "externalIds": {
                "ArXiv": "2110.06510",
                "DBLP": "conf/icassp/SipioHCMW22",
                "DOI": "10.1109/icassp43922.2022.9747675",
                "CorpusId": 238744248
            },
            "abstract": "In this paper, we discuss the initial attempts at boosting understanding human language based on deep-learning models with quantum computing. We successfully train a quantum-enhanced Long Short-Term Memory network to perform the parts-of-speech tagging task via numerical simulations. Moreover, a quantum-enhanced Transformer is proposed to perform the sentiment analysis based on the existing dataset.",
            "referenceCount": 76,
            "citationCount": 32,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2110.06510",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Physics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2021-10-13",
            "journal": {
                "name": "ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Sipio2021TheDO,\n author = {R. Sipio and Jia-Hong Huang and Samuel Yen-Chi Chen and Stefano Mangini and M. Worring},\n booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing},\n journal = {ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\n pages = {8612-8616},\n title = {The Dawn of Quantum Natural Language Processing},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:56edaa1368ff4dfa45388e4be24fdfbded7d88a7",
            "@type": "ScholarlyArticle",
            "paperId": "56edaa1368ff4dfa45388e4be24fdfbded7d88a7",
            "corpusId": 8273530,
            "url": "https://www.semanticscholar.org/paper/56edaa1368ff4dfa45388e4be24fdfbded7d88a7",
            "title": "A Primer on Neural Network Models for Natural Language Processing",
            "venue": "Journal of Artificial Intelligence Research",
            "publicationVenue": {
                "id": "urn:research:aef12dca-60a0-4ca3-819b-cad26d309d4e",
                "name": "Journal of Artificial Intelligence Research",
                "alternate_names": [
                    "JAIR",
                    "J Artif Intell Res",
                    "The Journal of Artificial Intelligence Research"
                ],
                "issn": "1076-9757",
                "url": "http://www.jair.org/"
            },
            "year": 2015,
            "externalIds": {
                "ArXiv": "1510.00726",
                "MAG": "2963042536",
                "DBLP": "journals/corr/Goldberg15c",
                "DOI": "10.1613/jair.4992",
                "CorpusId": 8273530
            },
            "abstract": "Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing. More recently, neural network models started to be applied also to textual natural language signals, again with very promising results. This tutorial surveys neural network models from the perspective of natural language processing research, in an attempt to bring natural-language researchers up to speed with the neural techniques. The tutorial covers input encoding for natural language tasks, feed-forward networks, convolutional networks, recurrent networks and recursive networks, as well as the computation graph abstraction for automatic gradient computation.",
            "referenceCount": 185,
            "citationCount": 1006,
            "influentialCitationCount": 58,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://jair.org/index.php/jair/article/download/11030/26198",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2015-10-02",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1510.00726"
            },
            "citationStyles": {
                "bibtex": "@Article{Goldberg2015APO,\n author = {Yoav Goldberg},\n booktitle = {Journal of Artificial Intelligence Research},\n journal = {ArXiv},\n title = {A Primer on Neural Network Models for Natural Language Processing},\n volume = {abs/1510.00726},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:12d237dde277d578a72d726c21f6748c262df9ed",
            "@type": "ScholarlyArticle",
            "paperId": "12d237dde277d578a72d726c21f6748c262df9ed",
            "corpusId": 233681998,
            "url": "https://www.semanticscholar.org/paper/12d237dde277d578a72d726c21f6748c262df9ed",
            "title": "Natural language processing in medicine: A review",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2021,
            "externalIds": {
                "MAG": "3141847762",
                "DOI": "10.1016/J.TACC.2021.02.007",
                "CorpusId": 233681998
            },
            "abstract": null,
            "referenceCount": 53,
            "citationCount": 40,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "2021-03-25",
            "journal": {
                "name": "Trends in Anaesthesia and Critical Care",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Article{Locke2021NaturalLP,\n author = {Saskia Locke and A. Bashall and Sarah Al-Adely and John Moore and A. Wilson and G. Kitchen},\n journal = {Trends in Anaesthesia and Critical Care},\n title = {Natural language processing in medicine: A review},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d10df96b3fb0ab5c6b1d0cc22c7400d0acccc3cc",
            "@type": "ScholarlyArticle",
            "paperId": "d10df96b3fb0ab5c6b1d0cc22c7400d0acccc3cc",
            "corpusId": 21735129,
            "url": "https://www.semanticscholar.org/paper/d10df96b3fb0ab5c6b1d0cc22c7400d0acccc3cc",
            "title": "The Hitchhiker\u2019s Guide to Testing Statistical Significance in Natural Language Processing",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                "name": "Annual Meeting of the Association for Computational Linguistics",
                "alternate_names": [
                    "Annu Meet Assoc Comput Linguistics",
                    "Meeting of the Association for Computational Linguistics",
                    "ACL",
                    "Meet Assoc Comput Linguistics"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/anthology/venues/acl/"
            },
            "year": 2018,
            "externalIds": {
                "ACL": "P18-1128",
                "DBLP": "conf/acl/ReichartDBS18",
                "MAG": "2798935874",
                "DOI": "10.18653/v1/P18-1128",
                "CorpusId": 21735129
            },
            "abstract": "Statistical significance testing is a standard statistical tool designed to ensure that experimental results are not coincidental. In this opinion/ theoretical paper we discuss the role of statistical significance testing in Natural Language Processing (NLP) research. We establish the fundamental concepts of significance testing and discuss the specific aspects of NLP tasks, experimental setups and evaluation measures that affect the choice of significance tests in NLP research. Based on this discussion we propose a simple practical protocol for statistical significance test selection in NLP setups and accompany this protocol with a brief survey of the most relevant tests. We then survey recent empirical papers published in ACL and TACL during 2017 and show that while our community assigns great value to experimental results, statistical significance testing is often ignored or misused. We conclude with a brief discussion of open issues that should be properly addressed so that this important tool can be applied. in NLP research in a statistically sound manner.",
            "referenceCount": 38,
            "citationCount": 290,
            "influentialCitationCount": 41,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/P18-1128.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference",
                "Review"
            ],
            "publicationDate": "2018-07-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Dror2018TheHG,\n author = {Rotem Dror and G. Baumer and Segev Shlomov and Roi Reichart},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {1383-1392},\n title = {The Hitchhiker\u2019s Guide to Testing Statistical Significance in Natural Language Processing},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:157a7ae44613a1fcf34e2be8c1e19a4f6e3c50e3",
            "@type": "ScholarlyArticle",
            "paperId": "157a7ae44613a1fcf34e2be8c1e19a4f6e3c50e3",
            "corpusId": 186206211,
            "url": "https://www.semanticscholar.org/paper/157a7ae44613a1fcf34e2be8c1e19a4f6e3c50e3",
            "title": "Transfer Learning in Natural Language Processing",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:01103732-3808-4930-b8e4-7e9e68d5c68d",
                "name": "North American Chapter of the Association for Computational Linguistics",
                "alternate_names": [
                    "North Am Chapter Assoc Comput Linguistics",
                    "NAACL"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/naacl"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "conf/naacl/RuderPSW19",
                "MAG": "2963020969",
                "ACL": "N19-5004",
                "DOI": "10.18653/v1/N19-5004",
                "CorpusId": 186206211
            },
            "abstract": "The classic supervised machine learning paradigm is based on learning in isolation, a single predictive model for a task using a single dataset. This approach requires a large number of training examples and performs best for well-defined and narrow tasks. Transfer learning refers to a set of methods that extend this approach by leveraging data from additional domains or tasks to train a model with better generalization properties. Over the last two years, the field of Natural Language Processing (NLP) has witnessed the emergence of several transfer learning methods and architectures which significantly improved upon the state-of-the-art on a wide range of NLP tasks. These improvements together with the wide availability and ease of integration of these methods are reminiscent of the factors that led to the success of pretrained word embeddings and ImageNet pretraining in computer vision, and indicate that these methods will likely become a common tool in the NLP landscape as well as an important research direction. We will present an overview of modern transfer learning methods in NLP, how models are pre-trained, what information the representations they learn capture, and review examples and case studies on how these models can be integrated and adapted in downstream NLP tasks.",
            "referenceCount": 666,
            "citationCount": 370,
            "influentialCitationCount": 17,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference",
                "Review"
            ],
            "publicationDate": "2019-06-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Ruder2019TransferLI,\n author = {Sebastian Ruder and Matthew E. Peters and Swabha Swayamdipta and Thomas Wolf},\n booktitle = {North American Chapter of the Association for Computational Linguistics},\n pages = {15-18},\n title = {Transfer Learning in Natural Language Processing},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:455cdafd55a5b5ddefa029bf97801327e142646d",
            "@type": "ScholarlyArticle",
            "paperId": "455cdafd55a5b5ddefa029bf97801327e142646d",
            "corpusId": 225062337,
            "url": "https://www.semanticscholar.org/paper/455cdafd55a5b5ddefa029bf97801327e142646d",
            "title": "A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:01103732-3808-4930-b8e4-7e9e68d5c68d",
                "name": "North American Chapter of the Association for Computational Linguistics",
                "alternate_names": [
                    "North Am Chapter Assoc Comput Linguistics",
                    "NAACL"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/naacl"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "conf/naacl/HedderichLASK21",
                "MAG": "3094140582",
                "ACL": "2021.naacl-main.201",
                "ArXiv": "2010.12309",
                "DOI": "10.18653/V1/2021.NAACL-MAIN.201",
                "CorpusId": 225062337
            },
            "abstract": "Deep neural networks and huge language models are becoming omnipresent in natural language applications. As they are known for requiring large amounts of training data, there is a growing body of work to improve the performance in low-resource settings. Motivated by the recent fundamental changes towards neural models and the popular pre-train and fine-tune paradigm, we survey promising approaches for low-resource natural language processing. After a discussion about the different dimensions of data availability, we give a structured overview of methods that enable learning when training data is sparse. This includes mechanisms to create additional labeled data like data augmentation and distant supervision as well as transfer learning settings that reduce the need for target supervision. A goal of our survey is to explain how these methods differ in their requirements as understanding them is essential for choosing a technique suited for a specific low-resource setting. Further key aspects of this work are to highlight open issues and to outline promising directions for future research.",
            "referenceCount": 254,
            "citationCount": 159,
            "influentialCitationCount": 13,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2021.naacl-main.201.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference",
                "Review"
            ],
            "publicationDate": "2020-10-23",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Hedderich2020ASO,\n author = {Michael A. Hedderich and Lukas Lange and Heike Adel and Jannik Strotgen and D. Klakow},\n booktitle = {North American Chapter of the Association for Computational Linguistics},\n pages = {2545-2568},\n title = {A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:995ec006ac98a697ea38bd4eea8c1f3170a8adb4",
            "@type": "ScholarlyArticle",
            "paperId": "995ec006ac98a697ea38bd4eea8c1f3170a8adb4",
            "corpusId": 218973981,
            "url": "https://www.semanticscholar.org/paper/995ec006ac98a697ea38bd4eea8c1f3170a8adb4",
            "title": "CAMeL Tools: An Open Source Python Toolkit for Arabic Natural Language Processing",
            "venue": "International Conference on Language Resources and Evaluation",
            "publicationVenue": {
                "id": "urn:research:7474c4a0-75d9-4105-9809-8e7d5201c5e1",
                "name": "International Conference on Language Resources and Evaluation",
                "alternate_names": [
                    "LREC",
                    "Int Conf Lang Resour Evaluation"
                ],
                "issn": null,
                "url": "http://www.lrec-conf.org/"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3032746405",
                "ACL": "2020.lrec-1.868",
                "DBLP": "conf/lrec/ObeidZKTOAIEEH20",
                "CorpusId": 218973981
            },
            "abstract": "We present CAMeL Tools, a collection of open-source tools for Arabic natural language processing in Python. CAMeL Tools currently provides utilities for pre-processing, morphological modeling, Dialect Identification, Named Entity Recognition and Sentiment Analysis. In this paper, we describe the design of CAMeL Tools and the functionalities it provides.",
            "referenceCount": 73,
            "citationCount": 124,
            "influentialCitationCount": 12,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-05-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Obeid2020CAMeLTA,\n author = {Ossama Obeid and Nasser Zalmout and Salam Khalifa and Dima Taji and Mai Oudah and Bashar Alhafni and Go Inoue and Fadhl Eryani and Alexander Erdmann and Nizar Habash},\n booktitle = {International Conference on Language Resources and Evaluation},\n pages = {7022-7032},\n title = {CAMeL Tools: An Open Source Python Toolkit for Arabic Natural Language Processing},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:77b91d7607518994d04f75119db4138b23e2eb87",
            "@type": "ScholarlyArticle",
            "paperId": "77b91d7607518994d04f75119db4138b23e2eb87",
            "corpusId": 211818180,
            "url": "https://www.semanticscholar.org/paper/77b91d7607518994d04f75119db4138b23e2eb87",
            "title": "Natural Language Processing Advancements By Deep Learning: A Survey",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "journals/corr/abs-2003-01200",
                "ArXiv": "2003.01200",
                "MAG": "3009431628",
                "CorpusId": 211818180
            },
            "abstract": "Natural Language Processing (NLP) helps empower intelligent machines by enhancing a better understanding of the human language for linguistic-based human-computer communication. Recent developments in computational power and the advent of large amounts of linguistic data have heightened the need and demand for automating semantic analysis using data-driven approaches. The utilization of data-driven strategies is pervasive now due to the significant improvements demonstrated through the usage of deep learning methods in areas such as Computer Vision, Automatic Speech Recognition, and in particular, NLP. This survey categorizes and addresses the different aspects and applications of NLP that have benefited from deep learning. It covers core NLP tasks and applications and describes how deep learning methods and models advance these areas. We further analyze and compare different approaches and state-of-the-art models.",
            "referenceCount": 226,
            "citationCount": 131,
            "influentialCitationCount": 9,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2020-03-02",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2003.01200"
            },
            "citationStyles": {
                "bibtex": "@Article{Torfi2020NaturalLP,\n author = {A. Torfi and Rouzbeh A. Shirvani and Yaser Keneshloo and Nader Tavvaf and E. Fox},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Natural Language Processing Advancements By Deep Learning: A Survey},\n volume = {abs/2003.01200},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:fef9d9eb2d527174ac5b329b0a044e98a1808971",
            "@type": "ScholarlyArticle",
            "paperId": "fef9d9eb2d527174ac5b329b0a044e98a1808971",
            "corpusId": 51888520,
            "url": "https://www.semanticscholar.org/paper/fef9d9eb2d527174ac5b329b0a044e98a1808971",
            "title": "Gender Bias in Neural Natural Language Processing",
            "venue": "Logic, Language, and Security",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/corr/abs-1807-11714",
                "ArXiv": "1807.11714",
                "MAG": "3128232076",
                "DOI": "10.1007/978-3-030-62077-6_14",
                "CorpusId": 51888520
            },
            "abstract": null,
            "referenceCount": 31,
            "citationCount": 249,
            "influentialCitationCount": 39,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/1807.11714",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-07-31",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1807.11714"
            },
            "citationStyles": {
                "bibtex": "@Article{Lu2018GenderBI,\n author = {Kaiji Lu and Piotr (Peter) Mardziel and Fangjing Wu and Preetam Amancharla and Anupam Datta},\n booktitle = {Logic, Language, and Security},\n journal = {ArXiv},\n title = {Gender Bias in Neural Natural Language Processing},\n volume = {abs/1807.11714},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7e6b922d6bedb8f865c41aaa6820178fa9b62b89",
            "@type": "ScholarlyArticle",
            "paperId": "7e6b922d6bedb8f865c41aaa6820178fa9b62b89",
            "corpusId": 250273205,
            "url": "https://www.semanticscholar.org/paper/7e6b922d6bedb8f865c41aaa6820178fa9b62b89",
            "title": "NATURAL LANGUAGE PROCESSING",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2022,
            "externalIds": {
                "CorpusId": 250273205
            },
            "abstract": "Through the increase in the popularity of online reading, many people rely on online dictionaries to further understand the text [1]. However, looking up a word manually is a great inconvenience as well as a form of distraction [2]. This paper develops a chrome extension to automatically detect the difficult words for each user, and provide the words\u2019 associated definition with a mouse hover. The chrome extension can be customized by adding and removing personal difficult words and personal easy words [3]. Also, the chrome extension offers a deeper level of analytic, including the system analyzing part of speech of the world, to further understand the definition of a selected word or sentence. The chrome extension is applied to a school/work setting in order to improve the working efficiency by providing a simple model to analyze the word definition; it is also useful for casual reading, especially to those that aren\u2019t fluent in English. Following the strict SDLC model, the end of the testing stage reflects that most of the users gave positive feedback to the chrome extension with most of the comments centered around convenience and accuracy [4]. Through alpha testing and a small sample of beta testing, the Chrome extension presents productivity improvement on difficult texts.",
            "referenceCount": 16,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Cao2022NATURALLP,\n author = {Zhanhao Cao and Yu Sun},\n title = {NATURAL LANGUAGE PROCESSING},\n year = {2022}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:db528269ef800727245c0fcb35b692d29c1ccdc9",
            "@type": "ScholarlyArticle",
            "paperId": "db528269ef800727245c0fcb35b692d29c1ccdc9",
            "corpusId": 218917913,
            "url": "https://www.semanticscholar.org/paper/db528269ef800727245c0fcb35b692d29c1ccdc9",
            "title": "Natural language processing (NLP) in management research: A literature review",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3021395787",
                "DOI": "10.1080/23270012.2020.1756939",
                "CorpusId": 218917913
            },
            "abstract": "Natural language processing (NLP) is gaining momentum in management research for its ability to automatically analyze and comprehend human language. Yet, despite its extensive application in manage...",
            "referenceCount": 88,
            "citationCount": 132,
            "influentialCitationCount": 6,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Business",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "2020-05-04",
            "journal": {
                "name": "",
                "volume": "7"
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Kang2020NaturalLP,\n author = {Yue Kang and Zhao Cai and Chee\u2010Wee Tan and Qian Huang and Hefu Liu},\n pages = {139-172},\n title = {Natural language processing (NLP) in management research: A literature review},\n volume = {7},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4e561318668f0ae190217ffe82bf44c9c33b9c0d",
            "@type": "ScholarlyArticle",
            "paperId": "4e561318668f0ae190217ffe82bf44c9c33b9c0d",
            "corpusId": 265995886,
            "url": "https://www.semanticscholar.org/paper/4e561318668f0ae190217ffe82bf44c9c33b9c0d",
            "title": "Transformers: State-of-the-Art Natural Language Processing",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "publicationVenue": {
                "id": "urn:research:41bf9ed3-85b3-4c90-b015-150e31690253",
                "name": "Conference on Empirical Methods in Natural Language Processing",
                "alternate_names": [
                    "Empir Method Nat Lang Process",
                    "Empirical Methods in Natural Language Processing",
                    "Conf Empir Method Nat Lang Process",
                    "EMNLP"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/emnlp"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3098824823",
                "DBLP": "conf/emnlp/WolfDSCDMCRLFDS20",
                "ACL": "2020.emnlp-demos.6",
                "DOI": "10.18653/v1/2020.emnlp-demos.6",
                "CorpusId": 265995886
            },
            "abstract": "Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.",
            "referenceCount": 43,
            "citationCount": 130,
            "influentialCitationCount": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/2020.emnlp-demos.6.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-10-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Wolf2020TransformersSN,\n author = {Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R\u00e9mi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {38-45},\n title = {Transformers: State-of-the-Art Natural Language Processing},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4199089b239c6d38e5051208210cb66454bdbba0",
            "@type": "ScholarlyArticle",
            "paperId": "4199089b239c6d38e5051208210cb66454bdbba0",
            "corpusId": 5884485,
            "url": "https://www.semanticscholar.org/paper/4199089b239c6d38e5051208210cb66454bdbba0",
            "title": "Deep Learning for Natural Language Processing",
            "venue": "Apress",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2810576820",
                "DOI": "10.1007/978-1-4842-3685-7",
                "CorpusId": 5884485
            },
            "abstract": null,
            "referenceCount": 41,
            "citationCount": 329,
            "influentialCitationCount": 11,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/bfm:978-1-4842-3685-7/1?pdf=chapter%20toc",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "2019-06-11",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Goyal2019DeepLF,\n author = {Palash Goyal and Sumit Pandey and Karan Jain},\n booktitle = {Apress},\n title = {Deep Learning for Natural Language Processing},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b4206dd288958affeb314aee0ec1397de5c74c23",
            "@type": "ScholarlyArticle",
            "paperId": "b4206dd288958affeb314aee0ec1397de5c74c23",
            "corpusId": 247316687,
            "url": "https://www.semanticscholar.org/paper/b4206dd288958affeb314aee0ec1397de5c74c23",
            "title": "Shared computational principles for language processing in humans and deep language models",
            "venue": "Nature Neuroscience",
            "publicationVenue": {
                "id": "urn:research:7892f01e-f701-4d07-8c36-e108b84ec6ab",
                "name": "Nature Neuroscience",
                "alternate_names": [
                    "Nat Neurosci"
                ],
                "issn": "1097-6256",
                "url": "http://www.nature.com/neuro/"
            },
            "year": 2022,
            "externalIds": {
                "PubMedCentral": "8904253",
                "DOI": "10.1038/s41593-022-01026-4",
                "CorpusId": 247316687,
                "PubMed": "35260860"
            },
            "abstract": null,
            "referenceCount": 75,
            "citationCount": 138,
            "influentialCitationCount": 12,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.nature.com/articles/s41593-022-01026-4.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2022-03-01",
            "journal": {
                "name": "Nature Neuroscience",
                "volume": "25"
            },
            "citationStyles": {
                "bibtex": "@Article{Goldstein2022SharedCP,\n author = {Ariel Goldstein and Zaid Zada and Eliav Buchnik and Mariano Schain and A. Price and Bobbi Aubrey and Samuel A. Nastase and Amir Feder and Dotan Emanuel and Alon Cohen and A. Jansen and H. Gazula and Gina Choe and Aditi Rao and Catherine Kim and Colton Casto and Lora Fanda and W. Doyle and D. Friedman and Patricia Dugan and L. Melloni and Roi Reichart and S. Devore and A. Flinker and Liat Hasenfratz and Omer Levy and A. Hassidim and Michael Brenner and Yossi Matias and K. Norman and O. Devinsky and U. Hasson},\n booktitle = {Nature Neuroscience},\n journal = {Nature Neuroscience},\n pages = {369 - 380},\n title = {Shared computational principles for language processing in humans and deep language models},\n volume = {25},\n year = {2022}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:373bc164d7b552f8782988e7da6b0d00092a20b0",
            "@type": "ScholarlyArticle",
            "paperId": "373bc164d7b552f8782988e7da6b0d00092a20b0",
            "corpusId": 227231454,
            "url": "https://www.semanticscholar.org/paper/373bc164d7b552f8782988e7da6b0d00092a20b0",
            "title": "Continual Lifelong Learning in Natural Language Processing: A Survey",
            "venue": "International Conference on Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:f51ff783-cdff-4e22-94fb-28e6336d17b3",
                "name": "International Conference on Computational Linguistics",
                "alternate_names": [
                    "Int Conf Comput Linguistics",
                    "COLING"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/anthology/venues/coling/"
            },
            "year": 2020,
            "externalIds": {
                "ACL": "2020.coling-main.574",
                "DBLP": "conf/coling/BiesialskaBC20",
                "ArXiv": "2012.09823",
                "MAG": "3112170794",
                "DOI": "10.18653/v1/2020.coling-main.574",
                "CorpusId": 227231454
            },
            "abstract": "Continual learning (CL) aims to enable information systems to learn from a continuous data stream across time. However, it is difficult for existing deep learning architectures to learn a new task without largely forgetting previously acquired knowledge. Furthermore, CL is particularly challenging for language learning, as natural language is ambiguous: it is discrete, compositional, and its meaning is context-dependent. In this work, we look at the problem of CL through the lens of various NLP tasks. Our survey discusses major challenges in CL and current methods applied in neural network models. We also provide a critical review of the existing CL evaluation methods and datasets in NLP. Finally, we present our outlook on future research directions.",
            "referenceCount": 146,
            "citationCount": 130,
            "influentialCitationCount": 14,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/2020.coling-main.574.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference",
                "Review"
            ],
            "publicationDate": "2020-12-17",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2012.09823"
            },
            "citationStyles": {
                "bibtex": "@Article{Biesialska2020ContinualLL,\n author = {Magdalena Biesialska and Katarzyna Biesialska and M. Costa-juss\u00e0},\n booktitle = {International Conference on Computational Linguistics},\n journal = {ArXiv},\n title = {Continual Lifelong Learning in Natural Language Processing: A Survey},\n volume = {abs/2012.09823},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ab456c1ed181c5c48a34adb61395d4806a0ba949",
            "@type": "ScholarlyArticle",
            "paperId": "ab456c1ed181c5c48a34adb61395d4806a0ba949",
            "corpusId": 218972088,
            "url": "https://www.semanticscholar.org/paper/ab456c1ed181c5c48a34adb61395d4806a0ba949",
            "title": "Attention in Natural Language Processing",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems",
            "publicationVenue": {
                "id": "urn:research:79c5a18d-0295-432c-aaa5-961d73de6d88",
                "name": "IEEE Transactions on Neural Networks and Learning Systems",
                "alternate_names": [
                    "IEEE Trans Neural Netw Learn Syst"
                ],
                "issn": "2162-237X",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5962385"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "3031696893",
                "DBLP": "journals/tnn/GalassiLT21",
                "ArXiv": "1902.02181",
                "DOI": "10.1109/TNNLS.2020.3019893",
                "CorpusId": 218972088,
                "PubMed": "32915750"
            },
            "abstract": "Attention is an increasingly popular mechanism used in a wide range of neural architectures. The mechanism itself has been realized in a variety of formats. However, because of the fast-paced advances in this domain, a systematic overview of attention is still missing. In this article, we define a unified model for attention architectures in natural language processing, with a focus on those designed to work with vector representations of the textual data. We propose a taxonomy of attention models according to four dimensions: the representation of the input, the compatibility function, the distribution function, and the multiplicity of the input and/or output. We present the examples of how prior information can be exploited in attention models and discuss ongoing research efforts and open challenges in the area, providing the first extensive categorization of the vast body of literature in this exciting domain.",
            "referenceCount": 174,
            "citationCount": 269,
            "influentialCitationCount": 7,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/5962385/9559436/09194070.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2019-02-04",
            "journal": {
                "name": "IEEE Transactions on Neural Networks and Learning Systems",
                "volume": "32"
            },
            "citationStyles": {
                "bibtex": "@Article{Galassi2019AttentionIN,\n author = {Andrea Galassi and Marco Lippi and Paolo Torroni},\n booktitle = {IEEE Transactions on Neural Networks and Learning Systems},\n journal = {IEEE Transactions on Neural Networks and Learning Systems},\n pages = {4291-4308},\n title = {Attention in Natural Language Processing},\n volume = {32},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0578dfb2a28b77abde19b32de777e0365df3020e",
            "@type": "ScholarlyArticle",
            "paperId": "0578dfb2a28b77abde19b32de777e0365df3020e",
            "corpusId": 234441596,
            "url": "https://www.semanticscholar.org/paper/0578dfb2a28b77abde19b32de777e0365df3020e",
            "title": "Data-driven materials research enabled by natural language processing and information extraction",
            "venue": "Applied Physics Reviews",
            "publicationVenue": {
                "id": "urn:research:3687b41d-efd9-4e36-9cc1-0a6d6596b266",
                "name": "Applied Physics Reviews",
                "alternate_names": [
                    "Appl phys rev",
                    "Appl Phys Rev",
                    "Applied physics reviews"
                ],
                "issn": "1931-9401",
                "url": "http://apr.aip.org/"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3115677442",
                "DOI": "10.1063/5.0021106",
                "CorpusId": 234441596
            },
            "abstract": "Given the emergence of data science and machine learning throughout all aspects of society, but particularly in the scientific domain, there is increased importance placed on obtaining data. Data in materials science are particularly heterogeneous, based on the significant range in materials classes that are explored and the variety of materials properties that are of interest. This leads to data that range many orders of magnitude, and these data may manifest as numerical text or image-based information, which requires quantitative interpretation. The ability to automatically consume and codify the scientific literature across domains\u2014enabled by techniques adapted from the field of natural language processing\u2014therefore has immense potential to unlock and generate the rich datasets necessary for data science and machine learning. This review focuses on the progress and practices of natural language processing and text mining of materials science literature and highlights opportunities for extracting additional information beyond text contained in figures and tables in articles. We discuss and provide examples for several reasons for the pursuit of natural language processing for materials, including data compilation, hypothesis development, and understanding the trends within and across fields. Current and emerging natural language processing methods along with their applications to materials science are detailed. We, then, discuss natural language processing and data challenges within the materials science domain where future directions may prove valuable.",
            "referenceCount": 138,
            "citationCount": 121,
            "influentialCitationCount": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aip.scitation.org/doi/10.1063/5.0021106",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Materials Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "2020-12-01",
            "journal": {
                "name": "Applied Physics Reviews",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Olivetti2020DatadrivenMR,\n author = {E. Olivetti and J. Cole and Edward Kim and O. Kononova and G. Ceder and T. Y. Han and A. Hiszpanski},\n booktitle = {Applied Physics Reviews},\n journal = {Applied Physics Reviews},\n title = {Data-driven materials research enabled by natural language processing and information extraction},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a2d2a482ccd62a705c8fafeb5f93bca33a4d796d",
            "@type": "ScholarlyArticle",
            "paperId": "a2d2a482ccd62a705c8fafeb5f93bca33a4d796d",
            "corpusId": 208610705,
            "url": "https://www.semanticscholar.org/paper/a2d2a482ccd62a705c8fafeb5f93bca33a4d796d",
            "title": "Deep learning in clinical natural language processing: a methodical review",
            "venue": "J. Am. Medical Informatics Assoc.",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/jamia/WuRDDJSSWWXZX20",
                "MAG": "2993873509",
                "DOI": "10.1093/jamia/ocz200",
                "CorpusId": 208610705,
                "PubMed": "31794016"
            },
            "abstract": "OBJECTIVE\nThis article methodically reviews the literature on deep learning (DL) for natural language processing (NLP) in the clinical domain, providing quantitative analysis to answer 3 research questions concerning methods, scope, and context of current research.\n\n\nMATERIALS AND METHODS\nWe searched MEDLINE, EMBASE, Scopus, the Association for Computing Machinery Digital Library, and the Association for Computational Linguistics Anthology for articles using DL-based approaches to NLP problems in electronic health records. After screening 1,737 articles, we collected data on 25 variables across 212 papers.\n\n\nRESULTS\nDL in clinical NLP publications more than doubled each year, through 2018. Recurrent neural networks (60.8%) and word2vec embeddings (74.1%) were the most popular methods; the information extraction tasks of text classification, named entity recognition, and relation extraction were dominant (89.2%). However, there was a \"long tail\" of other methods and specific tasks. Most contributions were methodological variants or applications, but 20.8% were new methods of some kind. The earliest adopters were in the NLP community, but the medical informatics community was the most prolific.\n\n\nDISCUSSION\nOur analysis shows growing acceptance of deep learning as a baseline for NLP research, and of DL-based NLP in the medical community. A number of common associations were substantiated (eg, the preference of recurrent neural networks for sequence-labeling named entity recognition), while others were surprisingly nuanced (eg, the scarcity of French language clinical NLP with deep learning).\n\n\nCONCLUSION\nDeep learning has not yet fully penetrated clinical NLP and is growing rapidly. This review highlighted both the popular and unique trends in this active field.",
            "referenceCount": 121,
            "citationCount": 247,
            "influentialCitationCount": 6,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2019-12-03",
            "journal": {
                "name": "Journal of the American Medical Informatics Association : JAMIA",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Wu2019DeepLI,\n author = {Stephen T Wu and Kirk Roberts and Surabhi Datta and Jingcheng Du and Zongcheng Ji and Yuqi Si and Sarvesh Soni and Qiong Wang and Qiang Wei and Yang Xiang and Bo Zhao and Hua Xu},\n booktitle = {J. Am. Medical Informatics Assoc.},\n journal = {Journal of the American Medical Informatics Association : JAMIA},\n title = {Deep learning in clinical natural language processing: a methodical review},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:48167fbde4ef8d5caa69e1c0c9d6e2ac7fd48c3b",
            "@type": "ScholarlyArticle",
            "paperId": "48167fbde4ef8d5caa69e1c0c9d6e2ac7fd48c3b",
            "corpusId": 219636357,
            "url": "https://www.semanticscholar.org/paper/48167fbde4ef8d5caa69e1c0c9d6e2ac7fd48c3b",
            "title": "NAS-Bench-NLP: Neural Architecture Search Benchmark for Natural Language Processing",
            "venue": "IEEE Access",
            "publicationVenue": {
                "id": "urn:research:2633f5b2-c15c-49fe-80f5-07523e770c26",
                "name": "IEEE Access",
                "alternate_names": null,
                "issn": "2169-3536",
                "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3035620341",
                "DBLP": "journals/access/KlyuchnikovTASF22",
                "ArXiv": "2006.07116",
                "DOI": "10.1109/access.2022.3169897",
                "CorpusId": 219636357
            },
            "abstract": "Neural Architecture Search (NAS) is a promising and rapidly evolving research area. Training a large number of neural networks requires an exceptional amount of computational power, which makes NAS unreachable for those researchers who have limited or no access to high-performance clusters and supercomputers. A few benchmarks with precomputed neural architectures performances have been recently introduced to overcome this problem and ensure reproducible experiments. However, these benchmarks are only for the computer vision domain and, thus, are built from the image datasets and convolution-derived architectures. In this work, we step outside the computer vision domain by leveraging the language modeling task, which is the core of natural language processing (NLP). Our main contribution is as follows: we have provided search space of recurrent neural networks on the text datasets and trained 14k architectures within it; we have conducted both intrinsic and extrinsic evaluation of the trained models using datasets for semantic relatedness and language understanding evaluation; finally, we have tested several NAS algorithms to demonstrate how the precomputed results can be utilized. We consider that the benchmark will provide more reliable empirical findings in the community and stimulate progress in developing new NAS methods well suited for recurrent architectures.",
            "referenceCount": 49,
            "citationCount": 75,
            "influentialCitationCount": 10,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/6287639/9668973/09762315.pdf",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-06-12",
            "journal": {
                "name": "IEEE Access",
                "volume": "PP"
            },
            "citationStyles": {
                "bibtex": "@Article{Klyuchnikov2020NASBenchNLPNA,\n author = {Nikita Klyuchnikov and I. Trofimov and E. Artemova and Mikhail Salnikov and M. Fedorov and Evgeny Burnaev},\n booktitle = {IEEE Access},\n journal = {IEEE Access},\n pages = {1-1},\n title = {NAS-Bench-NLP: Neural Architecture Search Benchmark for Natural Language Processing},\n volume = {PP},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:bf9a8fb50aca26774ebf4815db2d8712e2c5830c",
            "@type": "ScholarlyArticle",
            "paperId": "bf9a8fb50aca26774ebf4815db2d8712e2c5830c",
            "corpusId": 218596224,
            "url": "https://www.semanticscholar.org/paper/bf9a8fb50aca26774ebf4815db2d8712e2c5830c",
            "title": "TextAttack: A Framework for Adversarial Attacks in Natural Language Processing",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3025408396",
                "DBLP": "journals/corr/abs-2005-05909",
                "ArXiv": "2005.05909",
                "CorpusId": 218596224
            },
            "abstract": "TextAttack is a library for running adversarial attacks against natural language processing (NLP) models. TextAttack builds attacks from four components: a search method, goal function, transformation, and a set of constraints. Researchers can use these components to easily assemble new attacks. Individual components can be isolated and compared for easier ablation studies. TextAttack currently supports attacks on models trained for text classification and entailment across a variety of datasets. Additionally, TextAttack's modular design makes it easily extensible to new NLP tasks, models, and attack strategies. TextAttack code and tutorials are available at https://github.com/QData/TextAttack.",
            "referenceCount": 26,
            "citationCount": 58,
            "influentialCitationCount": 10,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2020-04-29",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2005.05909"
            },
            "citationStyles": {
                "bibtex": "@Article{Morris2020TextAttackAF,\n author = {John X. Morris and Eli Lifland and Jin Yong Yoo and Yanjun Qi},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {TextAttack: A Framework for Adversarial Attacks in Natural Language Processing},\n volume = {abs/2005.05909},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:45f952c21130d655090058e864c2772358a1de72",
            "@type": "ScholarlyArticle",
            "paperId": "45f952c21130d655090058e864c2772358a1de72",
            "corpusId": 220060314,
            "url": "https://www.semanticscholar.org/paper/45f952c21130d655090058e864c2772358a1de72",
            "title": "Commonsense Reasoning for Natural Language Processing",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                "name": "Annual Meeting of the Association for Computational Linguistics",
                "alternate_names": [
                    "Annu Meet Assoc Comput Linguistics",
                    "Meeting of the Association for Computational Linguistics",
                    "ACL",
                    "Meet Assoc Comput Linguistics"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/anthology/venues/acl/"
            },
            "year": 2020,
            "externalIds": {
                "ACL": "2020.acl-tutorials.7",
                "DBLP": "conf/acl/SapSBCR20",
                "MAG": "3037763555",
                "DOI": "10.18653/v1/2020.acl-tutorials.7",
                "CorpusId": 220060314
            },
            "abstract": "Commonsense knowledge, such as knowing that \u201cbumping into people annoys them\u201d or \u201crain makes the road slippery\u201d, helps humans navigate everyday situations seamlessly. Yet, endowing machines with such human-like commonsense reasoning capabilities has remained an elusive goal of artificial intelligence research for decades. In recent years, commonsense knowledge and reasoning have received renewed attention from the natural language processing (NLP) community, yielding exploratory studies in automated commonsense understanding. We organize this tutorial to provide researchers with the critical foundations and recent advances in commonsense representation and reasoning, in the hopes of casting a brighter light on this promising area of future research. In our tutorial, we will (1) outline the various types of commonsense (e.g., physical, social), and (2) discuss techniques to gather and represent commonsense knowledge, while highlighting the challenges specific to this type of knowledge (e.g., reporting bias). We will then (3) discuss the types of commonsense knowledge captured by modern NLP systems (e.g., large pretrained language models), and (4) present ways to measure systems\u2019 commonsense reasoning abilities. We will finish with (5) a discussion of various ways in which commonsense reasoning can be used to improve performance on NLP tasks, exemplified by an (6) interactive session on integrating commonsense into a downstream task.",
            "referenceCount": 73,
            "citationCount": 71,
            "influentialCitationCount": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/2020.acl-tutorials.7.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference",
                "Review"
            ],
            "publicationDate": "2020-07-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Sap2020CommonsenseRF,\n author = {Maarten Sap and Vered Shwartz and Antoine Bosselut and Yejin Choi and Dan Roth},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {27-33},\n title = {Commonsense Reasoning for Natural Language Processing},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2d3fd17662078b16d9a4140a37c5584d14f9e778",
            "@type": "ScholarlyArticle",
            "paperId": "2d3fd17662078b16d9a4140a37c5584d14f9e778",
            "corpusId": 227334565,
            "url": "https://www.semanticscholar.org/paper/2d3fd17662078b16d9a4140a37c5584d14f9e778",
            "title": "Foundations for Near-Term Quantum Natural Language Processing",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3111333709",
                "DBLP": "journals/corr/abs-2012-03755",
                "ArXiv": "2012.03755",
                "CorpusId": 227334565
            },
            "abstract": "We provide conceptual and mathematical foundations for near-term quantum natural language processing (QNLP), and do so in quantum computer scientist friendly terms. We opted for an expository presentation style, and provide references for supporting empirical evidence and formal statements concerning mathematical generality. \nWe recall how the quantum model for natural language that we employ canonically combines linguistic meanings with rich linguistic structure, most notably grammar. In particular, the fact that it takes a quantum-like model to combine meaning and structure, establishes QNLP as quantum-native, on par with simulation of quantum systems. Moreover, the now leading Noisy Intermediate-Scale Quantum (NISQ) paradigm for encoding classical data on quantum hardware, variational quantum circuits, makes NISQ exceptionally QNLP-friendly: linguistic structure can be encoded as a free lunch, in contrast to the apparently exponentially expensive classical encoding of grammar. \nQuantum speed-up for QNLP tasks has already been established in previous work with Will Zeng. Here we provide a broader range of tasks which all enjoy the same advantage. \nDiagrammatic reasoning is at the heart of QNLP. Firstly, the quantum model interprets language as quantum processes via the diagrammatic formalism of categorical quantum mechanics. Secondly, these diagrams are via ZX-calculus translated into quantum circuits. Parameterisations of meanings then become the circuit variables to be learned. \nOur encoding of linguistic structure within quantum circuits also embodies a novel approach for establishing word-meanings that goes beyond the current standards in mainstream AI, by placing linguistic structure at the heart of Wittgenstein's meaning-is-context.",
            "referenceCount": 119,
            "citationCount": 61,
            "influentialCitationCount": 6,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Physics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Physics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-12-07",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2012.03755"
            },
            "citationStyles": {
                "bibtex": "@Article{Coecke2020FoundationsFN,\n author = {B. Coecke and G. Felice and K. Meichanetzidis and Alexis Toumi},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Foundations for Near-Term Quantum Natural Language Processing},\n volume = {abs/2012.03755},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c8ff66d1b15e2349c53a7c63ec740dc424787d74",
            "@type": "ScholarlyArticle",
            "paperId": "c8ff66d1b15e2349c53a7c63ec740dc424787d74",
            "corpusId": 52118869,
            "url": "https://www.semanticscholar.org/paper/c8ff66d1b15e2349c53a7c63ec740dc424787d74",
            "title": "Natural Language Processing of Social Media as Screening for Suicide Risk",
            "venue": "Biomedical Informatics Insights",
            "publicationVenue": {
                "id": "urn:research:7ab85f1f-de6f-4932-a062-28c7d2f01204",
                "name": "Biomedical Informatics Insights",
                "alternate_names": [
                    "Biomed Informatics Insight"
                ],
                "issn": "1178-2226",
                "url": "http://www.la-press.com/biomedical-informatics-insights-journal-j82"
            },
            "year": 2018,
            "externalIds": {
                "PubMedCentral": "6111391",
                "MAG": "2889391310",
                "DOI": "10.1177/1178222618792860",
                "CorpusId": 52118869,
                "PubMed": "30158822"
            },
            "abstract": "Suicide is among the 10 most common causes of death, as assessed by the World Health Organization. For every death by suicide, an estimated 138 people\u2019s lives are meaningfully affected, and almost any other statistic around suicide deaths is equally alarming. The pervasiveness of social media\u2014and the near-ubiquity of mobile devices used to access social media networks\u2014offers new types of data for understanding the behavior of those who (attempt to) take their own lives and suggests new possibilities for preventive intervention. We demonstrate the feasibility of using social media data to detect those at risk for suicide. Specifically, we use natural language processing and machine learning (specifically deep learning) techniques to detect quantifiable signals around suicide attempts, and describe designs for an automated system for estimating suicide risk, usable by those without specialized mental health training (eg, a primary care doctor). We also discuss the ethical use of such technology and examine privacy implications. Currently, this technology is only used for intervention for individuals who have \u201copted in\u201d for the analysis and intervention, but the technology enables scalable screening for suicide risk, potentially identifying many people who are at risk preventively and prior to any engagement with a health care system. This raises a significant cultural question about the trade-off between privacy and prevention\u2014we have potentially life-saving technology that is currently reaching only a fraction of the possible people at risk because of respect for their privacy. Is the current trade-off between privacy and prevention the right one?",
            "referenceCount": 60,
            "citationCount": 309,
            "influentialCitationCount": 15,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://journals.sagepub.com/doi/pdf/10.1177/1178222618792860",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2018-08-27",
            "journal": {
                "name": "Biomedical Informatics Insights",
                "volume": "10"
            },
            "citationStyles": {
                "bibtex": "@Article{Coppersmith2018NaturalLP,\n author = {Glen A. Coppersmith and Ryan Leary and P. Crutchley and Alex B. Fine},\n booktitle = {Biomedical Informatics Insights},\n journal = {Biomedical Informatics Insights},\n title = {Natural Language Processing of Social Media as Screening for Suicide Risk},\n volume = {10},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ec43d37aad744150af144d27a08b0b097607e712",
            "@type": "ScholarlyArticle",
            "paperId": "ec43d37aad744150af144d27a08b0b097607e712",
            "corpusId": 206451986,
            "url": "https://www.semanticscholar.org/paper/ec43d37aad744150af144d27a08b0b097607e712",
            "title": "Jumping NLP Curves: A Review of Natural Language Processing Research [Review Article]",
            "venue": "IEEE Computational Intelligence Magazine",
            "publicationVenue": {
                "id": "urn:research:ee372de7-efda-4907-a03f-359292ea27f6",
                "name": "IEEE Computational Intelligence Magazine",
                "alternate_names": [
                    "IEEE Comput Intell Mag"
                ],
                "issn": "1556-603X",
                "url": "https://ieeexplore.ieee.org/servlet/opac?punumber=10207"
            },
            "year": 2014,
            "externalIds": {
                "DBLP": "journals/cim/CambriaW14",
                "MAG": "2028140375",
                "DOI": "10.1109/MCI.2014.2307227",
                "CorpusId": 206451986
            },
            "abstract": "Natural language processing (NLP) is a theory-motivated range of computational techniques for the automatic analysis and representation of human language. NLP research has evolved from the era of punch cards and batch processing (in which the analysis of a sentence could take up to 7 minutes) to the era of Google and the likes of it (in which millions of webpages can be processed in less than a second). This review paper draws on recent developments in NLP research to look at the past, present, and future of NLP technology in a new light. Borrowing the paradigm of `jumping curves' from the field of business management and marketing prediction, this survey article reinterprets the evolution of NLP research as the intersection of three overlapping curves-namely Syntactics, Semantics, and Pragmatics Curveswhich will eventually lead NLP research to evolve into natural language understanding.",
            "referenceCount": 119,
            "citationCount": 852,
            "influentialCitationCount": 31,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2014-05-01",
            "journal": {
                "name": "IEEE Computational Intelligence Magazine",
                "volume": "9"
            },
            "citationStyles": {
                "bibtex": "@Article{Cambria2014JumpingNC,\n author = {E. Cambria and B. White},\n booktitle = {IEEE Computational Intelligence Magazine},\n journal = {IEEE Computational Intelligence Magazine},\n pages = {48-57},\n title = {Jumping NLP Curves: A Review of Natural Language Processing Research [Review Article]},\n volume = {9},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:59cebaf9ff644e91f3a813cb0413a7a9b8ddc690",
            "@type": "ScholarlyArticle",
            "paperId": "59cebaf9ff644e91f3a813cb0413a7a9b8ddc690",
            "corpusId": 213183574,
            "url": "https://www.semanticscholar.org/paper/59cebaf9ff644e91f3a813cb0413a7a9b8ddc690",
            "title": "Feature Extraction and Analysis of Natural Language Processing for Deep Learning English Language",
            "venue": "IEEE Access",
            "publicationVenue": {
                "id": "urn:research:2633f5b2-c15c-49fe-80f5-07523e770c26",
                "name": "IEEE Access",
                "alternate_names": null,
                "issn": "2169-3536",
                "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "journals/access/WangSY20",
                "MAG": "3006156918",
                "DOI": "10.1109/ACCESS.2020.2974101",
                "CorpusId": 213183574
            },
            "abstract": "NLP (Natural Language Processing) is a technology that enables computers to understand human languages. Deep-level grammatical and semantic analysis usually uses words as the basic unit, and word segmentation is usually the primary task of NLP. In order to solve the practical problem of huge structural differences between different data modalities in a multi-modal environment and traditional machine learning methods cannot be directly applied, this paper introduces the feature extraction method of deep learning and applies the ideas of deep learning to multi-modal feature extraction. This paper proposes a multi-modal neural network. For each mode, there is a multilayer sub-neural network with an independent structure corresponding to it. It is used to convert the features in different modes to the same-modal features. In terms of word segmentation processing, in view of the problems that existing word segmentation methods can hardly guarantee long-term dependency of text semantics and long training prediction time, a hybrid network English word segmentation processing method is proposed. This method applies BI-GRU (Bidirectional Gated Recurrent Unit) to English word segmentation, and uses the CRF (Conditional Random Field) model to annotate sentences in sequence, effectively solving the long-distance dependency of text semantics, shortening network training and predicted time. Experiments show that the processing effect of this method on word segmentation is similar to that of BI-LSTM-CRF (Bidirectional- Long Short Term Memory-Conditional Random Field) model, but the average predicted processing speed is 1.94 times that of BI-LSTM-CRF, effectively improving the efficiency of word segmentation processing.",
            "referenceCount": 45,
            "citationCount": 51,
            "influentialCitationCount": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/6287639/8948470/08999624.pdf",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-02-14",
            "journal": {
                "name": "IEEE Access",
                "volume": "8"
            },
            "citationStyles": {
                "bibtex": "@Article{Wang2020FeatureEA,\n author = {Dongyang Wang and Junli Su and Hongbin Yu},\n booktitle = {IEEE Access},\n journal = {IEEE Access},\n pages = {46335-46345},\n title = {Feature Extraction and Analysis of Natural Language Processing for Deep Learning English Language},\n volume = {8},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:618a84ea5d9ac23c6a93961ba798154026754ddd",
            "@type": "ScholarlyArticle",
            "paperId": "618a84ea5d9ac23c6a93961ba798154026754ddd",
            "corpusId": 227162992,
            "url": "https://www.semanticscholar.org/paper/618a84ea5d9ac23c6a93961ba798154026754ddd",
            "title": "A panoramic survey of natural language processing in the Arab world",
            "venue": "Communications of the ACM",
            "publicationVenue": {
                "id": "urn:research:4d9ce1c4-dc84-46b9-903e-e3751c00c7dd",
                "name": "Communications of the ACM",
                "alternate_names": [
                    "Commun ACM",
                    "Communications of The ACM"
                ],
                "issn": "0001-0782",
                "url": "http://www.acm.org/pubs/cacm/"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3108716566",
                "ArXiv": "2011.12631",
                "DBLP": "journals/corr/abs-2011-12631",
                "DOI": "10.1145/3447735",
                "CorpusId": 227162992
            },
            "abstract": "The term natural language refers to any system of symbolic communication (spoken, signed or written) without intentional human planning and design. This distinguishes natural languages such as Arabic and Japanese from artificially constructed languages such as Esperanto or Python. Natural language processing (NLP) is the sub-field of artificial intelligence (AI) focused on modeling natural languages to build applications such as speech recognition and synthesis, machine translation, optical character recognition (OCR), sentiment analysis (SA), question answering, dialogue systems, etc. NLP is a highly interdisciplinary field with connections to computer science, linguistics, cognitive science, psychology, mathematics and others. Some of the earliest AI applications were in NLP (e.g., machine translation); and the last decade (2010-2020) in particular has witnessed an incredible increase in quality, matched with a rise in public awareness, use, and expectations of what may have seemed like science fiction in the past. NLP researchers pride themselves on developing language independent models and tools that can be applied to all human languages, e.g. machine translation systems can be built for a variety of languages using the same basic mechanisms and models. However, the reality is that some languages do get more attention (e.g., English and Chinese) than others (e.g., Hindi and Swahili). Arabic, the primary language of the Arab world and the religious language of millions of non-Arab Muslims is somewhere in the middle of this continuum. Though Arabic NLP has many challenges, it has seen many successes and developments. Next we discuss Arabic's main challenges as a necessary background, and we present a brief history of Arabic NLP. We then survey a number of its research areas, and close with a critical discussion of the future of Arabic NLP.",
            "referenceCount": 206,
            "citationCount": 47,
            "influentialCitationCount": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2011.12631",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2020-11-25",
            "journal": {
                "name": "Communications of the ACM",
                "volume": "64"
            },
            "citationStyles": {
                "bibtex": "@Article{Darwish2020APS,\n author = {Kareem Darwish and Nizar Habash and Mourad Abbas and H. Al-Khalifa and Huseein T. Al-Natsheh and S. El-Beltagy and Houda Bouamor and Karim Bouzoubaa and V. Cavalli-Sforza and W. El-Hajj and Mustafa Jarrar and Hamdy Mubarak},\n booktitle = {Communications of the ACM},\n journal = {Communications of the ACM},\n pages = {72 - 81},\n title = {A panoramic survey of natural language processing in the Arab world},\n volume = {64},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:cad38cf6083c4d385f30e8b9ad7200391dc309bb",
            "@type": "ScholarlyArticle",
            "paperId": "cad38cf6083c4d385f30e8b9ad7200391dc309bb",
            "corpusId": 211525595,
            "url": "https://www.semanticscholar.org/paper/cad38cf6083c4d385f30e8b9ad7200391dc309bb",
            "title": "A Deep Learning Architecture for Psychometric Natural Language Processing",
            "venue": "ACM Trans. Inf. Syst.",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3004561114",
                "DBLP": "journals/tois/AhmadALDNCC20",
                "DOI": "10.1145/3365211",
                "CorpusId": 211525595
            },
            "abstract": "Psychometric measures reflecting people\u2019s knowledge, ability, attitudes, and personality traits are critical for many real-world applications, such as e-commerce, health care, and cybersecurity. However, traditional methods cannot collect and measure rich psychometric dimensions in a timely and unobtrusive manner. Consequently, despite their importance, psychometric dimensions have received limited attention from the natural language processing and information retrieval communities. In this article, we propose a deep learning architecture, PyNDA, to extract psychometric dimensions from user-generated texts. PyNDA contains a novel representation embedding, a demographic embedding, a structural equation model (SEM) encoder, and a multitask learning mechanism designed to work in unison to address the unique challenges associated with extracting rich, sophisticated, and user-centric psychometric dimensions. Our experiments on three real-world datasets encompassing 11 psychometric dimensions, including trust, anxiety, and literacy, show that PyNDA markedly outperforms traditional feature-based classifiers as well as the state-of-the-art deep learning architectures. Ablation analysis reveals that each component of PyNDA significantly contributes to its overall performance. Collectively, the results demonstrate the efficacy of the proposed architecture for facilitating rich psychometric analysis. Our results have important implications for user-centric information extraction and retrieval systems looking to measure and incorporate psychometric dimensions.",
            "referenceCount": 70,
            "citationCount": 47,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3365211",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-02-05",
            "journal": {
                "name": "ACM Transactions on Information Systems (TOIS)",
                "volume": "38"
            },
            "citationStyles": {
                "bibtex": "@Article{Ahmad2020ADL,\n author = {Faizan Ahmad and A. Abbasi and Jingjing Li and David G. Dobolyi and R. Netemeyer and G. Clifford and Hsinchun Chen},\n booktitle = {ACM Trans. Inf. Syst.},\n journal = {ACM Transactions on Information Systems (TOIS)},\n pages = {1 - 29},\n title = {A Deep Learning Architecture for Psychometric Natural Language Processing},\n volume = {38},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:eef4df3a5232c7ce70123aaebb326ff9169a3c8c",
            "@type": "ScholarlyArticle",
            "paperId": "eef4df3a5232c7ce70123aaebb326ff9169a3c8c",
            "corpusId": 209461005,
            "url": "https://www.semanticscholar.org/paper/eef4df3a5232c7ce70123aaebb326ff9169a3c8c",
            "title": "Predictive Biases in Natural Language Processing Models: A Conceptual Framework and Overview",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                "name": "Annual Meeting of the Association for Computational Linguistics",
                "alternate_names": [
                    "Annu Meet Assoc Comput Linguistics",
                    "Meeting of the Association for Computational Linguistics",
                    "ACL",
                    "Meet Assoc Comput Linguistics"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/anthology/venues/acl/"
            },
            "year": 2019,
            "externalIds": {
                "ACL": "2020.acl-main.468",
                "MAG": "2998334385",
                "DBLP": "journals/corr/abs-1912-11078",
                "ArXiv": "1912.11078",
                "DOI": "10.18653/v1/2020.acl-main.468",
                "CorpusId": 209461005
            },
            "abstract": "An increasing number of natural language processing papers address the effect of bias on predictions, introducing mitigation techniques at different parts of the standard NLP pipeline (data and models). However, these works have been conducted individually, without a unifying framework to organize efforts within the field. This situation leads to repetitive approaches, and focuses overly on bias symptoms/effects, rather than on their origins, which could limit the development of effective countermeasures. In this paper, we propose a unifying predictive bias framework for NLP. We summarize the NLP literature and suggest general mathematical definitions of predictive bias. We differentiate two consequences of bias: outcome disparities and error disparities, as well as four potential origins of biases: label bias, selection bias, model overamplification, and semantic bias. Our framework serves as an overview of predictive bias in NLP, integrating existing work into a single structure, and providing a conceptual baseline for improved frameworks.",
            "referenceCount": 97,
            "citationCount": 189,
            "influentialCitationCount": 8,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/2020.acl-main.468.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference",
                "Review"
            ],
            "publicationDate": "2019-11-09",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Shah2019PredictiveBI,\n author = {Deven Santosh Shah and H. A. Schwartz and Dirk Hovy},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {5248-5264},\n title = {Predictive Biases in Natural Language Processing Models: A Conceptual Framework and Overview},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:da3671fd5efe5ed0e96ce832bcc35291686f2aca",
            "@type": "ScholarlyArticle",
            "paperId": "da3671fd5efe5ed0e96ce832bcc35291686f2aca",
            "corpusId": 73453434,
            "url": "https://www.semanticscholar.org/paper/da3671fd5efe5ed0e96ce832bcc35291686f2aca",
            "title": "Natural language processing of symptoms documented in free-text narratives of electronic health records: a systematic review",
            "venue": "J. Am. Medical Informatics Assoc.",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/jamia/KoleckDBB19",
                "MAG": "2912971066",
                "DOI": "10.1093/jamia/ocy173",
                "CorpusId": 73453434,
                "PubMed": "30726935"
            },
            "abstract": "OBJECTIVE\nNatural language processing (NLP) of symptoms from electronic health records (EHRs) could contribute to the advancement of symptom science. We aim to synthesize the literature on the use of NLP to process or analyze symptom information documented in EHR free-text narratives.\n\n\nMATERIALS AND METHODS\nOur search of 1964 records from PubMed and EMBASE was narrowed to 27 eligible articles. Data related to the purpose, free-text corpus, patients, symptoms, NLP methodology, evaluation metrics, and quality indicators were extracted for each study.\n\n\nRESULTS\nSymptom-related information was presented as a primary outcome in 14 studies. EHR narratives represented various inpatient and outpatient clinical specialties, with general, cardiology, and mental health occurring most frequently. Studies encompassed a wide variety of symptoms, including shortness of breath, pain, nausea, dizziness, disturbed sleep, constipation, and depressed mood. NLP approaches included previously developed NLP tools, classification methods, and manually curated rule-based processing. Only one-third (n\u2009=\u20099) of studies reported patient demographic characteristics.\n\n\nDISCUSSION\nNLP is used to extract information from EHR free-text narratives written by a variety of healthcare providers on an expansive range of symptoms across diverse clinical specialties. The current focus of this field is on the development of methods to extract symptom information and the use of symptom information for disease classification tasks rather than the examination of symptoms themselves.\n\n\nCONCLUSION\nFuture NLP studies should concentrate on the investigation of symptoms and symptom documentation in EHR free-text narratives. Efforts should be undertaken to examine patient characteristics and make symptom-related NLP algorithms or pipelines and vocabularies openly available.",
            "referenceCount": 53,
            "citationCount": 213,
            "influentialCitationCount": 6,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://europepmc.org/articles/pmc6657282?pdf=render",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2019-02-06",
            "journal": {
                "name": "Journal of the American Medical Informatics Association",
                "volume": "26"
            },
            "citationStyles": {
                "bibtex": "@Article{Koleck2019NaturalLP,\n author = {Theresa A. Koleck and C. Dreisbach and P. Bourne and S. Bakken},\n booktitle = {J. Am. Medical Informatics Assoc.},\n journal = {Journal of the American Medical Informatics Association},\n pages = {364\u2013379},\n title = {Natural language processing of symptoms documented in free-text narratives of electronic health records: a systematic review},\n volume = {26},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2733574d40ce8e861d7d658bfc33ab36f529864d",
            "@type": "ScholarlyArticle",
            "paperId": "2733574d40ce8e861d7d658bfc33ab36f529864d",
            "corpusId": 195874273,
            "url": "https://www.semanticscholar.org/paper/2733574d40ce8e861d7d658bfc33ab36f529864d",
            "title": "GluonCV and GluonNLP: Deep Learning in Computer Vision and Natural Language Processing",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "3008618223",
                "DBLP": "journals/corr/abs-1907-04433",
                "ArXiv": "1907.04433",
                "CorpusId": 195874273
            },
            "abstract": "We present GluonCV and GluonNLP, the deep learning toolkits for computer vision and natural language processing based on Apache MXNet (incubating). These toolkits provide state-of-the-art pre-trained models, training scripts, and training logs, to facilitate rapid prototyping and promote reproducible research. We also provide modular APIs with flexible building blocks to enable efficient customization. Leveraging the MXNet ecosystem, the deep learning models in GluonCV and GluonNLP can be deployed onto a variety of platforms with different programming languages. The Apache 2.0 license has been adopted by GluonCV and GluonNLP to allow for software distribution, modification, and usage.",
            "referenceCount": 20,
            "citationCount": 161,
            "influentialCitationCount": 8,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-07-09",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1907.04433"
            },
            "citationStyles": {
                "bibtex": "@Article{Guo2019GluonCVAG,\n author = {Jian Guo and He He and Tong He and Leonard Lausen and Mu Li and Haibin Lin and Xingjian Shi and Chenguang Wang and Junyuan Xie and Sheng Zha and Aston Zhang and Hang Zhang and Zhi Zhang and Zhongyue Zhang and Shuai Zheng and Yi Zhu},\n booktitle = {Journal of machine learning research},\n journal = {ArXiv},\n title = {GluonCV and GluonNLP: Deep Learning in Computer Vision and Natural Language Processing},\n volume = {abs/1907.04433},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e050b06b2e4937b516d630043cb68b8bc77078df",
            "@type": "ScholarlyArticle",
            "paperId": "e050b06b2e4937b516d630043cb68b8bc77078df",
            "corpusId": 8420436,
            "url": "https://www.semanticscholar.org/paper/e050b06b2e4937b516d630043cb68b8bc77078df",
            "title": "Advances in natural language processing",
            "venue": "Science",
            "publicationVenue": {
                "id": "urn:research:f59506a8-d8bb-4101-b3d4-c4ac3ed03dad",
                "name": "Science",
                "alternate_names": null,
                "issn": "0193-4511",
                "url": "https://www.jstor.org/journal/science"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "1663984431",
                "DOI": "10.1126/science.aaa8685",
                "CorpusId": 8420436,
                "PubMed": "26185244"
            },
            "abstract": "Natural language processing employs computational techniques for the purpose of learning, understanding, and producing human language content. Early computational approaches to language research focused on automating the analysis of the linguistic structure of language and developing basic technologies such as machine translation, speech recognition, and speech synthesis. Today\u2019s researchers refine and make use of such tools in real-world applications, creating spoken dialogue systems and speech-to-speech translation engines, mining social media for information about health or finance, and identifying sentiment and emotion toward products and services. We describe successes and challenges in this rapidly advancing area.",
            "referenceCount": 86,
            "citationCount": 648,
            "influentialCitationCount": 21,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2015-07-17",
            "journal": {
                "name": "Science",
                "volume": "349"
            },
            "citationStyles": {
                "bibtex": "@Article{Hirschberg2015AdvancesIN,\n author = {Julia Hirschberg and Christopher D. Manning},\n booktitle = {Science},\n journal = {Science},\n pages = {261 - 266},\n title = {Advances in natural language processing},\n volume = {349},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:76e69e67d50092a01cd7e8320b82ee861d01de66",
            "@type": "ScholarlyArticle",
            "paperId": "76e69e67d50092a01cd7e8320b82ee861d01de66",
            "corpusId": 229465951,
            "url": "https://www.semanticscholar.org/paper/76e69e67d50092a01cd7e8320b82ee861d01de66",
            "title": "Natural language processing for similar languages, varieties, and dialects: A survey",
            "venue": "Natural Language Engineering",
            "publicationVenue": {
                "id": "urn:research:b0dc264e-1ef6-4c58-be54-d2e6137ac35f",
                "name": "Natural Language Engineering",
                "alternate_names": [
                    "Nat Lang Eng"
                ],
                "issn": "1351-3249",
                "url": "https://www.cambridge.org/core/journals/natural-language-engineering"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "journals/nle/ZampieriNS20",
                "MAG": "3109540321",
                "DOI": "10.1017/S1351324920000492",
                "CorpusId": 229465951
            },
            "abstract": "Abstract There has been a lot of recent interest in the natural language processing (NLP) community in the computational processing of language varieties and dialects, with the aim to improve the performance of applications such as machine translation, speech recognition, and dialogue systems. Here, we attempt to survey this growing field of research, with focus on computational methods for processing similar languages, varieties, and dialects. In particular, we discuss the most important challenges when dealing with diatopic language variation, and we present some of the available datasets, the process of data collection, and the most common data collection strategies used to compile datasets for similar languages, varieties, and dialects. We further present a number of studies on computational methods developed and/or adapted for preprocessing, normalization, part-of-speech tagging, and parsing similar languages, language varieties, and dialects. Finally, we discuss relevant applications such as language and dialect identification and machine translation for closely related languages, language varieties, and dialects.",
            "referenceCount": 152,
            "citationCount": 27,
            "influentialCitationCount": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://helda.helsinki.fi/bitstream/10138/330117/1/natural_language_processing_for_similar_languages_varieties_and_dialects_a_survey.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2020-11-01",
            "journal": {
                "name": "Natural Language Engineering",
                "volume": "26"
            },
            "citationStyles": {
                "bibtex": "@Article{Zampieri2020NaturalLP,\n author = {Marcos Zampieri and Preslav Nakov and Yves Scherrer},\n booktitle = {Natural Language Engineering},\n journal = {Natural Language Engineering},\n pages = {595 - 612},\n title = {Natural language processing for similar languages, varieties, and dialects: A survey},\n volume = {26},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a929a3d92e305f4ef22ad9e52d264e7782394bd4",
            "@type": "ScholarlyArticle",
            "paperId": "a929a3d92e305f4ef22ad9e52d264e7782394bd4",
            "corpusId": 7630289,
            "url": "https://www.semanticscholar.org/paper/a929a3d92e305f4ef22ad9e52d264e7782394bd4",
            "title": "Predicting judicial decisions of the European Court of Human Rights: a Natural Language Processing perspective",
            "venue": "PeerJ Computer Science",
            "publicationVenue": {
                "id": "urn:research:138e58fc-8643-41c2-a92c-068ea4f4c607",
                "name": "PeerJ Computer Science",
                "alternate_names": [
                    "Peerj Comput Sci"
                ],
                "issn": "2376-5992",
                "url": "https://peerj.com/archives/?journal=cs"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2536769020",
                "DBLP": "journals/peerj-cs/AletrasTPL16",
                "DOI": "10.7717/PEERJ-CS.93",
                "CorpusId": 7630289
            },
            "abstract": "Recent advances in Natural Language Processing and Machine Learning provide us with the tools to build predictive models that can be used to unveil patterns driving judicial decisions. This can be useful, for both lawyers and judges, as an assisting tool to rapidly identify cases and extract patterns which lead to certain decisions. This paper presents the first systematic study on predicting the outcome of cases tried by the European Court of Human Rights based solely on textual content. We formulate a binary classification task where the input of our classifiers is the textual content extracted from a case and the target output is the actual judgment as to whether there has been a violation of an article of the convention of human rights. Textual information is represented using contiguous word sequences, i.e., N-grams, and topics. Our models can predict the court\u2019s decisions with a strong accuracy (79% on average). Our empirical analysis indicates that the formal facts of a case are the most important predictive factor. This is consistent with the theory of legal realism suggesting that judicial decision-making is significantly affected by the stimulus of the facts. We also observe that the topical content of a case is another important feature in this classification task and explore this relationship further by conducting a qualitative analysis.",
            "referenceCount": 46,
            "citationCount": 440,
            "influentialCitationCount": 39,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://peerj.com/articles/cs-93.pdf",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Law",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-10-24",
            "journal": {
                "name": "PeerJ Comput. Sci.",
                "volume": "2"
            },
            "citationStyles": {
                "bibtex": "@Article{Aletras2016PredictingJD,\n author = {Nikolaos Aletras and D. Tsarapatsanis and Daniel Preotiuc-Pietro and Vasileios Lampos},\n booktitle = {PeerJ Computer Science},\n journal = {PeerJ Comput. Sci.},\n pages = {e93},\n title = {Predicting judicial decisions of the European Court of Human Rights: a Natural Language Processing perspective},\n volume = {2},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:54c1607bd79b4e7869ea4f284eb46212296989d2",
            "@type": "ScholarlyArticle",
            "paperId": "54c1607bd79b4e7869ea4f284eb46212296989d2",
            "corpusId": 234982088,
            "url": "https://www.semanticscholar.org/paper/54c1607bd79b4e7869ea4f284eb46212296989d2",
            "title": "Natural Language Processing",
            "venue": "Fundamentals of Artificial Intelligence",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3144293453",
                "DOI": "10.1007/978-81-322-3972-7_19",
                "CorpusId": 234982088
            },
            "abstract": null,
            "referenceCount": 22,
            "citationCount": 32,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "Fundamentals of Artificial Intelligence",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Chowdhary2020NaturalLP,\n author = {Prof. K. R. Chowdhary},\n booktitle = {Fundamentals of Artificial Intelligence},\n journal = {Fundamentals of Artificial Intelligence},\n title = {Natural Language Processing},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d4bbd9c7d95f00a5acaef2f2a48f8543f52df18a",
            "@type": "ScholarlyArticle",
            "paperId": "d4bbd9c7d95f00a5acaef2f2a48f8543f52df18a",
            "corpusId": 53296658,
            "url": "https://www.semanticscholar.org/paper/d4bbd9c7d95f00a5acaef2f2a48f8543f52df18a",
            "title": "A Survey on Natural Language Processing for Fake News Detection",
            "venue": "International Conference on Language Resources and Evaluation",
            "publicationVenue": {
                "id": "urn:research:7474c4a0-75d9-4105-9809-8e7d5201c5e1",
                "name": "International Conference on Language Resources and Evaluation",
                "alternate_names": [
                    "LREC",
                    "Int Conf Lang Resour Evaluation"
                ],
                "issn": null,
                "url": "http://www.lrec-conf.org/"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/corr/abs-1811-00770",
                "MAG": "2899204128",
                "ACL": "2020.lrec-1.747",
                "ArXiv": "1811.00770",
                "CorpusId": 53296658
            },
            "abstract": "Fake news detection is a critical yet challenging problem in Natural Language Processing (NLP). The rapid rise of social networking platforms has not only yielded a vast increase in information accessibility but has also accelerated the spread of fake news. Thus, the effect of fake news has been growing, sometimes extending to the offline world and threatening public safety. Given the massive amount of Web content, automatic fake news detection is a practical NLP problem useful to all online content providers, in order to reduce the human time and effort to detect and prevent the spread of fake news. In this paper, we describe the challenges involved in fake news detection and also describe related tasks. We systematically review and compare the task formulations, datasets and NLP solutions that have been developed for this task, and also discuss the potentials and limitations of them. Based on our insights, we outline promising research directions, including more fine-grained, detailed, fair, and practical detection models. We also highlight the difference between fake news detection and other related tasks, and the importance of NLP solutions for fake news detection.",
            "referenceCount": 46,
            "citationCount": 236,
            "influentialCitationCount": 13,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2018-11-02",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1811.00770"
            },
            "citationStyles": {
                "bibtex": "@Article{Oshikawa2018ASO,\n author = {Ray Oshikawa and Jing Qian and William Yang Wang},\n booktitle = {International Conference on Language Resources and Evaluation},\n journal = {ArXiv},\n title = {A Survey on Natural Language Processing for Fake News Detection},\n volume = {abs/1811.00770},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:47dba53fe8ec69f90b4abfa263be70eef3fdee7f",
            "@type": "ScholarlyArticle",
            "paperId": "47dba53fe8ec69f90b4abfa263be70eef3fdee7f",
            "corpusId": 4039842,
            "url": "https://www.semanticscholar.org/paper/47dba53fe8ec69f90b4abfa263be70eef3fdee7f",
            "title": "Clinical Natural Language Processing in languages other than English: opportunities and challenges",
            "venue": "American Medical Informatics Association Annual Symposium",
            "publicationVenue": {
                "id": "urn:research:c9e16b3c-2fdf-4b4c-82bf-5cdf3d3435b7",
                "name": "American Medical Informatics Association Annual Symposium",
                "alternate_names": [
                    "Conference of American Medical Informatics Association",
                    "Am Med Informatics Assoc Annu Symp",
                    "Conf Am Med Informatics Assoc",
                    "AMIA"
                ],
                "issn": null,
                "url": "https://knowledge.amia.org/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2576840767",
                "DBLP": "conf/amia/NeveolDSZ14",
                "PubMedCentral": "5877394",
                "DOI": "10.1186/s13326-018-0179-8",
                "CorpusId": 4039842,
                "PubMed": "29602312"
            },
            "abstract": null,
            "referenceCount": 174,
            "citationCount": 196,
            "influentialCitationCount": 13,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2018-03-30",
            "journal": {
                "name": "Journal of Biomedical Semantics",
                "volume": "9"
            },
            "citationStyles": {
                "bibtex": "@Article{N\u00e9v\u00e9ol2018ClinicalNL,\n author = {Aur\u00e9lie N\u00e9v\u00e9ol and H. Dalianis and G. Savova and Pierre Zweigenbaum},\n booktitle = {American Medical Informatics Association Annual Symposium},\n journal = {Journal of Biomedical Semantics},\n title = {Clinical Natural Language Processing in languages other than English: opportunities and challenges},\n volume = {9},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:08f648c1eb523f54122c85952ed6ba9626997b29",
            "@type": "ScholarlyArticle",
            "paperId": "08f648c1eb523f54122c85952ed6ba9626997b29",
            "corpusId": 233719081,
            "url": "https://www.semanticscholar.org/paper/08f648c1eb523f54122c85952ed6ba9626997b29",
            "title": "Machine Learning and Natural Language Processing in Mental Health: Systematic Review",
            "venue": "Journal of Medical Internet Research",
            "publicationVenue": {
                "id": "urn:research:2baad992-2268-4c38-9120-e453622f2eeb",
                "name": "Journal of Medical Internet Research",
                "alternate_names": [
                    "J Med Internet Res"
                ],
                "issn": "1438-8871",
                "url": "http://www.symposion.com/jmir/index.htm"
            },
            "year": 2019,
            "externalIds": {
                "PubMedCentral": "8132982",
                "DOI": "10.2196/15708",
                "CorpusId": 233719081,
                "PubMed": "33944788"
            },
            "abstract": "Background Machine learning systems are part of the field of artificial intelligence that automatically learn models from data to make better decisions. Natural language processing (NLP), by using corpora and learning approaches, provides good performance in statistical tasks, such as text classification or sentiment mining. Objective The primary aim of this systematic review was to summarize and characterize, in methodological and technical terms, studies that used machine learning and NLP techniques for mental health. The secondary aim was to consider the potential use of these methods in mental health clinical practice Methods This systematic review follows the PRISMA (Preferred Reporting Items for Systematic Review and Meta-analysis) guidelines and is registered with PROSPERO (Prospective Register of Systematic Reviews; number CRD42019107376). The search was conducted using 4 medical databases (PubMed, Scopus, ScienceDirect, and PsycINFO) with the following keywords: machine learning, data mining, psychiatry, mental health, and mental disorder. The exclusion criteria were as follows: languages other than English, anonymization process, case studies, conference papers, and reviews. No limitations on publication dates were imposed. Results A total of 327 articles were identified, of which 269 (82.3%) were excluded and 58 (17.7%) were included in the review. The results were organized through a qualitative perspective. Although studies had heterogeneous topics and methods, some themes emerged. Population studies could be grouped into 3 categories: patients included in medical databases, patients who came to the emergency room, and social media users. The main objectives were to extract symptoms, classify severity of illness, compare therapy effectiveness, provide psychopathological clues, and challenge the current nosography. Medical records and social media were the 2 major data sources. With regard to the methods used, preprocessing used the standard methods of NLP and unique identifier extraction dedicated to medical texts. Efficient classifiers were preferred rather than transparent functioning classifiers. Python was the most frequently used platform. Conclusions Machine learning and NLP models have been highly topical issues in medicine in recent years and may be considered a new paradigm in medical research. However, these processes tend to confirm clinical hypotheses rather than developing entirely new information, and only one major category of the population (ie, social media users) is an imprecise cohort. Moreover, some language-specific features can improve the performance of NLP methods, and their extension to other languages should be more closely investigated. However, machine learning and NLP techniques provide useful information from unexplored data (ie, patients\u2019 daily habits that are usually inaccessible to care providers). Before considering It as an additional tool of mental health care, ethical issues remain and should be discussed in a timely manner. Machine learning and NLP methods may offer multiple perspectives in mental health research but should also be considered as tools to support clinical practice.",
            "referenceCount": 85,
            "citationCount": 134,
            "influentialCitationCount": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.jmir.org/2021/5/e15708/PDF",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2019-07-31",
            "journal": {
                "name": "Journal of Medical Internet Research",
                "volume": "23"
            },
            "citationStyles": {
                "bibtex": "@Article{Glaz2019MachineLA,\n author = {A. Le Glaz and Y. Haralambous and Deok-Hee Kim-Dufor and P. Lenca and R. Billot and Taylor C. Ryan and Jonathan J. Marsh and J. Devylder and M. Walter and S. Berrouiguet and C. Lemey},\n booktitle = {Journal of Medical Internet Research},\n journal = {Journal of Medical Internet Research},\n title = {Machine Learning and Natural Language Processing in Mental Health: Systematic Review},\n volume = {23},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:85e3010ff82c07961bc21f63b91a4981fa5123fe",
            "@type": "ScholarlyArticle",
            "paperId": "85e3010ff82c07961bc21f63b91a4981fa5123fe",
            "corpusId": 208121296,
            "url": "https://www.semanticscholar.org/paper/85e3010ff82c07961bc21f63b91a4981fa5123fe",
            "title": "Neural transfer learning for natural language processing",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2979736514",
                "CorpusId": 208121296
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 197,
            "influentialCitationCount": 26,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2019-06-07",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Ruder2019NeuralTL,\n author = {Sebastian Ruder},\n title = {Neural transfer learning for natural language processing},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:838fbfd9066dbbac6c10059c5b183046fb1cd9d1",
            "@type": "ScholarlyArticle",
            "paperId": "838fbfd9066dbbac6c10059c5b183046fb1cd9d1",
            "corpusId": 52039644,
            "url": "https://www.semanticscholar.org/paper/838fbfd9066dbbac6c10059c5b183046fb1cd9d1",
            "title": "Deep Bayesian Active Learning for Natural Language Processing: Results of a Large-Scale Empirical Study",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "publicationVenue": {
                "id": "urn:research:41bf9ed3-85b3-4c90-b015-150e31690253",
                "name": "Conference on Empirical Methods in Natural Language Processing",
                "alternate_names": [
                    "Empir Method Nat Lang Process",
                    "Empirical Methods in Natural Language Processing",
                    "Conf Empir Method Nat Lang Process",
                    "EMNLP"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/emnlp"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2951443114",
                "ACL": "D18-1318",
                "ArXiv": "1808.05697",
                "DBLP": "conf/emnlp/SiddhantL18",
                "DOI": "10.18653/v1/D18-1318",
                "CorpusId": 52039644
            },
            "abstract": "Several recent papers investigate Active Learning (AL) for mitigating the data dependence of deep learning for natural language processing. However, the applicability of AL to real-world problems remains an open question. While in supervised learning, practitioners can try many different methods, evaluating each against a validation set before selecting a model, AL affords no such luxury. Over the course of one AL run, an agent annotates its dataset exhausting its labeling budget. Thus, given a new task, we have no opportunity to compare models and acquisition functions. This paper provides a large-scale empirical study of deep active learning, addressing multiple tasks and, for each, multiple datasets, multiple models, and a full suite of acquisition functions. We find that across all settings, Bayesian active learning by disagreement, using uncertainty estimates provided either by Dropout or Bayes-by-Backprop significantly improves over i.i.d. baselines and usually outperforms classic uncertainty sampling.",
            "referenceCount": 26,
            "citationCount": 169,
            "influentialCitationCount": 18,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/D18-1318.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-08-16",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1808.05697"
            },
            "citationStyles": {
                "bibtex": "@Article{Siddhant2018DeepBA,\n author = {Aditya Siddhant and Zachary Chase Lipton},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n journal = {ArXiv},\n title = {Deep Bayesian Active Learning for Natural Language Processing: Results of a Large-Scale Empirical Study},\n volume = {abs/1808.05697},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:56a87c5f533be4cbcf76c65328a54ffb227c0395",
            "@type": "ScholarlyArticle",
            "paperId": "56a87c5f533be4cbcf76c65328a54ffb227c0395",
            "corpusId": 219167035,
            "url": "https://www.semanticscholar.org/paper/56a87c5f533be4cbcf76c65328a54ffb227c0395",
            "title": "Language as a biomarker for psychosis: A natural language processing approach",
            "venue": "Schizophrenia Research",
            "publicationVenue": {
                "id": "urn:research:6db1ea70-a63f-4b17-94ed-eaea27ecee25",
                "name": "Schizophrenia Research",
                "alternate_names": [
                    "Schizophr Res"
                ],
                "issn": "0920-9964",
                "url": "http://www.elsevier.com/locate/schres"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3032934243",
                "DOI": "10.1016/j.schres.2020.04.032",
                "CorpusId": 219167035,
                "PubMed": "32499162"
            },
            "abstract": null,
            "referenceCount": 110,
            "citationCount": 75,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Medicine",
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2020-06-01",
            "journal": {
                "name": "Schizophrenia Research",
                "volume": "226"
            },
            "citationStyles": {
                "bibtex": "@Article{Corcoran2020LanguageAA,\n author = {C. Corcoran and V. Mittal and C. Bearden and R. Gur and Kasia Hitczenko and Z. Bilgrami and A. Savic and G. Cecchi and P. Wolff},\n booktitle = {Schizophrenia Research},\n journal = {Schizophrenia Research},\n pages = {158-166},\n title = {Language as a biomarker for psychosis: A natural language processing approach},\n volume = {226},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e75b3c12da067552fda910a5bbed8b4d0e82dbcb",
            "@type": "ScholarlyArticle",
            "paperId": "e75b3c12da067552fda910a5bbed8b4d0e82dbcb",
            "corpusId": 115844916,
            "url": "https://www.semanticscholar.org/paper/e75b3c12da067552fda910a5bbed8b4d0e82dbcb",
            "title": " Neural Network Methods for Natural Language Processing",
            "venue": "Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:ee37a78c-f3d8-407a-bd24-bb97fe6dbab9",
                "name": "Computational Linguistics",
                "alternate_names": [
                    "Comput Linguistics"
                ],
                "issn": "0891-2017",
                "url": "http://aclanthology.info/venues/cl"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "journals/coling/GoldbergHLZ18",
                "MAG": "2793652419",
                "DOI": "10.2200/S00762ED1V01Y201703HLT037",
                "CorpusId": 115844916
            },
            "abstract": "Neural networks are a family of powerful machine learning models. This book focuses on the application of neural network models to natural language data. The first half of the book (Parts I and II) covers the basics of supervised machine learning and feed-forward neural networks, the basics of working with machine learning over language data, and the use of vector-based rather than symbolic representations for words. It also covers the computation-graph abstraction, which allows to easily define and train arbitrary neural networks, and is the basis behind the design of contemporary neural network software libraries.\r\n\r\nThe second part of the book (Parts III and IV) introduces more specialized neural network architectures, including 1D convolutional neural networks, recurrent neural networks, conditioned-generation models, and attention-based models. These architectures and techniques are the driving force behind state-of-the-art algorithms for machine translation, syntactic parsing, and many other applications. Finally, we also discuss tree-shaped networks, structured prediction, and the prospects of multi-task learning.",
            "referenceCount": 0,
            "citationCount": 537,
            "influentialCitationCount": 40,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/bfm%3A978-3-031-02165-7%2F1",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": "Synthesis Lectures on Human Language Technologies",
                "volume": "-"
            },
            "citationStyles": {
                "bibtex": "@Article{Goldberg2017NeuralNM,\n author = {Yoav Goldberg},\n booktitle = {Computational Linguistics},\n journal = {Synthesis Lectures on Human Language Technologies},\n pages = {1-309},\n title = { Neural Network Methods for Natural Language Processing},\n volume = {-},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8271311ceeabe333d4555deedcd3926b2145314a",
            "@type": "ScholarlyArticle",
            "paperId": "8271311ceeabe333d4555deedcd3926b2145314a",
            "corpusId": 199453123,
            "url": "https://www.semanticscholar.org/paper/8271311ceeabe333d4555deedcd3926b2145314a",
            "title": "Self-Knowledge Distillation in Natural Language Processing",
            "venue": "Recent Advances in Natural Language Processing",
            "publicationVenue": {
                "id": "urn:research:3413b6f7-e718-4940-a26a-e208f732ada0",
                "name": "Recent Advances in Natural Language Processing",
                "alternate_names": [
                    "RANLP",
                    "Recent Adv Nat Lang Process"
                ],
                "issn": null,
                "url": null
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/corr/abs-1908-01851",
                "ACL": "R19-1050",
                "ArXiv": "1908.01851",
                "MAG": "2983128379",
                "DOI": "10.26615/978-954-452-056-4_050",
                "CorpusId": 199453123
            },
            "abstract": "Since deep learning became a key player in natural language processing (NLP), many deep learning models have been showing remarkable performances in a variety of NLP tasks. Such high performance can be explained by efficient knowledge representation of deep learning models. Knowledge distillation from pretrained deep networks suggests that we can use more information from the soft target probability to train other neural networks. In this paper, we propose a self-knowledge distillation method, based on the soft target probabilities of the training model itself, where multimode information is distilled from the word embedding space right below the softmax layer. Due to the time complexity, our method approximates the soft target probabilities. In experiments, we applied the proposed method to two different and fundamental NLP tasks: language model and neural machine translation. The experiment results show that our proposed method improves performance on the tasks.",
            "referenceCount": 21,
            "citationCount": 75,
            "influentialCitationCount": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.26615/978-954-452-056-4_050",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-08-02",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1908.01851"
            },
            "citationStyles": {
                "bibtex": "@Article{Hahn2019SelfKnowledgeDI,\n author = {Sangchul Hahn and Heeyoul Choi},\n booktitle = {Recent Advances in Natural Language Processing},\n journal = {ArXiv},\n title = {Self-Knowledge Distillation in Natural Language Processing},\n volume = {abs/1908.01851},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:cf901e5b02f2ed7489d302240cd8f5ee936c2e19",
            "@type": "ScholarlyArticle",
            "paperId": "cf901e5b02f2ed7489d302240cd8f5ee936c2e19",
            "corpusId": 4547415,
            "url": "https://www.semanticscholar.org/paper/cf901e5b02f2ed7489d302240cd8f5ee936c2e19",
            "title": "VnCoreNLP: A Vietnamese Natural Language Processing Toolkit",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:01103732-3808-4930-b8e4-7e9e68d5c68d",
                "name": "North American Chapter of the Association for Computational Linguistics",
                "alternate_names": [
                    "North Am Chapter Assoc Comput Linguistics",
                    "NAACL"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/naacl"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2782238183",
                "ACL": "N18-5012",
                "DBLP": "journals/corr/abs-1801-01331",
                "ArXiv": "1801.01331",
                "DOI": "10.18653/v1/N18-5012",
                "CorpusId": 4547415
            },
            "abstract": "We present an easy-to-use and fast toolkit, namely VnCoreNLP\u2014a Java NLP annotation pipeline for Vietnamese. Our VnCoreNLP supports key natural language processing (NLP) tasks including word segmentation, part-of-speech (POS) tagging, named entity recognition (NER) and dependency parsing, and obtains state-of-the-art (SOTA) results for these tasks. We release VnCoreNLP to provide rich linguistic annotations to facilitate research work on Vietnamese NLP. Our VnCoreNLP is open-source and available at: https://github.com/vncorenlp/VnCoreNLP",
            "referenceCount": 31,
            "citationCount": 121,
            "influentialCitationCount": 15,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/N18-5012.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-01-04",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Vu2018VnCoreNLPAV,\n author = {Thanh Vu and Dat Quoc Nguyen and D. Q. Nguyen and M. Dras and Mark Johnson},\n booktitle = {North American Chapter of the Association for Computational Linguistics},\n pages = {56-60},\n title = {VnCoreNLP: A Vietnamese Natural Language Processing Toolkit},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:fdbb252f29ee0b72fc5467c0ae11f7cb30149f46",
            "@type": "ScholarlyArticle",
            "paperId": "fdbb252f29ee0b72fc5467c0ae11f7cb30149f46",
            "corpusId": 131988231,
            "url": "https://www.semanticscholar.org/paper/fdbb252f29ee0b72fc5467c0ae11f7cb30149f46",
            "title": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "publicationVenue": {
                "id": "urn:research:41bf9ed3-85b3-4c90-b015-150e31690253",
                "name": "Conference on Empirical Methods in Natural Language Processing",
                "alternate_names": [
                    "Empir Method Nat Lang Process",
                    "Empirical Methods in Natural Language Processing",
                    "Conf Empir Method Nat Lang Process",
                    "EMNLP"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/emnlp"
            },
            "year": 2014,
            "externalIds": {
                "MAG": "2930957955",
                "DOI": "10.3115/v1/d14-1",
                "CorpusId": 131988231
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 756,
            "influentialCitationCount": 85,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.3115/v1/d14-1",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Moschitti2014ProceedingsOT,\n author = {Alessandro Moschitti and B. Pang and Walter Daelemans},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n title = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a1f3f113b452971543ca310426ff0e7d0f982e4e",
            "@type": "ScholarlyArticle",
            "paperId": "a1f3f113b452971543ca310426ff0e7d0f982e4e",
            "corpusId": 245106673,
            "url": "https://www.semanticscholar.org/paper/a1f3f113b452971543ca310426ff0e7d0f982e4e",
            "title": "Natural Language Processing",
            "venue": "Building an Effective Data Science Practice",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2021,
            "externalIds": {
                "DOI": "10.1007/978-1-4842-7419-4_6",
                "CorpusId": 245106673
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2021-12-09",
            "journal": {
                "name": "Building an Effective Data Science Practice",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Raina2021NaturalLP,\n author = {Vineet Raina and S. Krishnamurthy},\n booktitle = {Building an Effective Data Science Practice},\n journal = {Building an Effective Data Science Practice},\n title = {Natural Language Processing},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c52044dcf1b985c1373de672d89c354e85463575",
            "@type": "ScholarlyArticle",
            "paperId": "c52044dcf1b985c1373de672d89c354e85463575",
            "corpusId": 7678100,
            "url": "https://www.semanticscholar.org/paper/c52044dcf1b985c1373de672d89c354e85463575",
            "title": "Natural language processing: state of the art, current trends and challenges",
            "venue": "Multimedia tools and applications",
            "publicationVenue": {
                "id": "urn:research:477368e9-7a8e-475a-8c93-6d623797fd06",
                "name": "Multimedia tools and applications",
                "alternate_names": [
                    "Multimedia Tools and Applications",
                    "Multimedia Tool Appl",
                    "Multimedia tool appl"
                ],
                "issn": "1380-7501",
                "url": "https://www.springer.com/computer/information+systems+and+applications/journal/11042"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "journals/mta/KhuranaKKS23",
                "ArXiv": "1708.05148",
                "PubMedCentral": "9281254",
                "MAG": "2747680751",
                "DOI": "10.1007/s11042-022-13428-4",
                "CorpusId": 7678100,
                "PubMed": "35855771"
            },
            "abstract": null,
            "referenceCount": 166,
            "citationCount": 324,
            "influentialCitationCount": 23,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/s11042-022-13428-4.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-08-17",
            "journal": {
                "name": "Multimedia Tools and Applications",
                "volume": "82"
            },
            "citationStyles": {
                "bibtex": "@Article{Khurana2017NaturalLP,\n author = {Diksha Khurana and Aditya Koli and Kiran Khatter and Sukhdev Singh},\n booktitle = {Multimedia tools and applications},\n journal = {Multimedia Tools and Applications},\n pages = {3713 - 3744},\n title = {Natural language processing: state of the art, current trends and challenges},\n volume = {82},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e3567830f32444917af2d06c213435b7f1a92cd2",
            "@type": "ScholarlyArticle",
            "paperId": "e3567830f32444917af2d06c213435b7f1a92cd2",
            "corpusId": 167217728,
            "url": "https://www.semanticscholar.org/paper/e3567830f32444917af2d06c213435b7f1a92cd2",
            "title": "Interpreting and improving natural-language processing (in machines) with natural language-processing (in the brain)",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2947012833",
                "ArXiv": "1905.11833",
                "DBLP": "conf/nips/TonevaW19",
                "CorpusId": 167217728
            },
            "abstract": "Neural networks models for NLP are typically implemented without the explicit encoding of language rules and yet they are able to break one performance record after another. This has generated a lot of research interest in interpreting the representations learned by these networks. We propose here a novel interpretation approach that relies on the only processing system we have that does understand language: the human brain. We use brain imaging recordings of subjects reading complex natural text to interpret word and sequence embeddings from 4 recent NLP models - ELMo, USE, BERT and Transformer-XL. We study how their representations differ across layer depth, context length, and attention type. Our results reveal differences in the context-related representations across these models. Further, in the transformer models, we find an interaction between layer depth and context length, and between layer depth and attention type. We finally hypothesize that altering BERT to better align with brain recordings would enable it to also better understand language. Probing the altered BERT using syntactic NLP tasks reveals that the model with increased brain-alignment outperforms the original model. Cognitive neuroscientists have already begun using NLP networks to study the brain, and this work closes the loop to allow the interaction between NLP and cognitive neuroscience to be a true cross-pollination.",
            "referenceCount": 48,
            "citationCount": 150,
            "influentialCitationCount": 10,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-05-28",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1905.11833"
            },
            "citationStyles": {
                "bibtex": "@Article{Toneva2019InterpretingAI,\n author = {Mariya Toneva and Leila Wehbe},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Interpreting and improving natural-language processing (in machines) with natural language-processing (in the brain)},\n volume = {abs/1905.11833},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3cd8d09e47b0acbd52e07d14eeb3faf6260760da",
            "@type": "ScholarlyArticle",
            "paperId": "3cd8d09e47b0acbd52e07d14eeb3faf6260760da",
            "corpusId": 49564714,
            "url": "https://www.semanticscholar.org/paper/3cd8d09e47b0acbd52e07d14eeb3faf6260760da",
            "title": "Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing",
            "venue": "Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:ee37a78c-f3d8-407a-bd24-bb97fe6dbab9",
                "name": "Computational Linguistics",
                "alternate_names": [
                    "Comput Linguistics"
                ],
                "issn": "0891-2017",
                "url": "http://aclanthology.info/venues/cl"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2810095012",
                "DBLP": "journals/corr/abs-1807-00914",
                "ArXiv": "1807.00914",
                "DOI": "10.1162/coli_a_00357",
                "CorpusId": 49564714
            },
            "abstract": "Linguistic typology aims to capture structural and semantic variation across the world\u2019s languages. A large-scale typology could provide excellent guidance for multilingual Natural Language Processing (NLP), particularly for languages that suffer from the lack of human labeled resources. We present an extensive literature survey on the use of typological information in the development of NLP techniques. Our survey demonstrates that to date, the use of information in existing typological databases has resulted in consistent but modest improvements in system performance. We show that this is due to both intrinsic limitations of databases (in terms of coverage and feature granularity) and under-utilization of the typological features included in them. We advocate for a new approach that adapts the broad and discrete nature of typological categories to the contextual and continuous nature of machine learning algorithms used in contemporary NLP. In particular, we suggest that such an approach could be facilitated by recent developments in data-driven induction of typological knowledge.",
            "referenceCount": 235,
            "citationCount": 122,
            "influentialCitationCount": 8,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://direct.mit.edu/coli/article-pdf/45/3/559/1847397/coli_a_00357.pdf",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2018-07-02",
            "journal": {
                "name": "Computational Linguistics",
                "volume": "45"
            },
            "citationStyles": {
                "bibtex": "@Article{Ponti2018ModelingLV,\n author = {E. Ponti and Helen O'Horan and Yevgeni Berzak and Ivan Vulic and Roi Reichart and T. Poibeau and Ekaterina Shutova and A. Korhonen},\n booktitle = {Computational Linguistics},\n journal = {Computational Linguistics},\n pages = {559-601},\n title = {Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing},\n volume = {45},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:80d8f048ec3d5ab8595ae58cc9105eaafbc57f14",
            "@type": "ScholarlyArticle",
            "paperId": "80d8f048ec3d5ab8595ae58cc9105eaafbc57f14",
            "corpusId": 22432448,
            "url": "https://www.semanticscholar.org/paper/80d8f048ec3d5ab8595ae58cc9105eaafbc57f14",
            "title": "Natural Language Processing in Radiology: A Systematic Review.",
            "venue": "Radiology",
            "publicationVenue": {
                "id": "urn:research:357207a3-a4af-4091-822c-75ef52d02fb5",
                "name": "Radiology",
                "alternate_names": null,
                "issn": "0033-8419",
                "url": "http://radiology.rsna.org/"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2338526423",
                "DOI": "10.1148/radiol.16142770",
                "CorpusId": 22432448,
                "PubMed": "27089187"
            },
            "abstract": "Radiological reporting has generated large quantities of digital content within the electronic health record, which is potentially a valuable source of information for improving clinical care and supporting research. Although radiology reports are stored for communication and documentation of diagnostic imaging, harnessing their potential requires efficient and automated information extraction: they exist mainly as free-text clinical narrative, from which it is a major challenge to obtain structured data. Natural language processing (NLP) provides techniques that aid the conversion of text into a structured representation, and thus enables computers to derive meaning from human (ie, natural language) input. Used on radiology reports, NLP techniques enable automatic identification and extraction of information. By exploring the various purposes for their use, this review examines how radiology benefits from NLP. A systematic literature search identified 67 relevant publications describing NLP methods that support practical applications in radiology. This review takes a close look at the individual studies in terms of tasks (ie, the extracted information), the NLP methodology and tools used, and their application purpose and performance results. Additionally, limitations, future challenges, and requirements for advancing NLP in radiology will be discussed.",
            "referenceCount": 84,
            "citationCount": 395,
            "influentialCitationCount": 8,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2016-04-18",
            "journal": {
                "name": "Radiology",
                "volume": "279 2"
            },
            "citationStyles": {
                "bibtex": "@Article{Pons2016NaturalLP,\n author = {E. Pons and Loes M. M. Braun and M. Hunink and J. Kors},\n booktitle = {Radiology},\n journal = {Radiology},\n pages = {\n          329-43\n        },\n title = {Natural Language Processing in Radiology: A Systematic Review.},\n volume = {279 2},\n year = {2016}\n}\n"
            }
        }
    }
]