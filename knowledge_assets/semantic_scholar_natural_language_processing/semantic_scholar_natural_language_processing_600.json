[
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:81a4fd3004df0eb05d6c1cef96ad33d5407820df",
            "@type": "ScholarlyArticle",
            "paperId": "81a4fd3004df0eb05d6c1cef96ad33d5407820df",
            "corpusId": 57375753,
            "url": "https://www.semanticscholar.org/paper/81a4fd3004df0eb05d6c1cef96ad33d5407820df",
            "title": "A Comprehensive Survey on Graph Neural Networks",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems",
            "publicationVenue": {
                "id": "urn:research:79c5a18d-0295-432c-aaa5-961d73de6d88",
                "name": "IEEE Transactions on Neural Networks and Learning Systems",
                "alternate_names": [
                    "IEEE Trans Neural Netw Learn Syst"
                ],
                "issn": "2162-237X",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5962385"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/corr/abs-1901-00596",
                "ArXiv": "1901.00596",
                "MAG": "2907492528",
                "DOI": "10.1109/TNNLS.2020.2978386",
                "CorpusId": 57375753,
                "PubMed": "32217482"
            },
            "abstract": "Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications, where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on the existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this article, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art GNNs into four categories, namely, recurrent GNNs, convolutional GNNs, graph autoencoders, and spatial\u2013temporal GNNs. We further discuss the applications of GNNs across various domains and summarize the open-source codes, benchmark data sets, and model evaluation of GNNs. Finally, we propose potential research directions in this rapidly growing field.",
            "referenceCount": 193,
            "citationCount": 5537,
            "influentialCitationCount": 377,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": "IEEE Transactions on Neural Networks and Learning Systems",
                "volume": "32"
            },
            "citationStyles": {
                "bibtex": "@Article{Wu2019ACS,\n author = {Zonghan Wu and Shirui Pan and Fengwen Chen and Guodong Long and Chengqi Zhang and Philip S. Yu},\n booktitle = {IEEE Transactions on Neural Networks and Learning Systems},\n journal = {IEEE Transactions on Neural Networks and Learning Systems},\n pages = {4-24},\n title = {A Comprehensive Survey on Graph Neural Networks},\n volume = {32},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:33c95f4d13958e65e676a9cd64bd4d8df0fdb44d",
            "@type": "ScholarlyArticle",
            "paperId": "33c95f4d13958e65e676a9cd64bd4d8df0fdb44d",
            "corpusId": 38022159,
            "url": "https://www.semanticscholar.org/paper/33c95f4d13958e65e676a9cd64bd4d8df0fdb44d",
            "title": "Sentiment Analysis and Opinion Mining",
            "venue": "Synthesis Lectures on Human Language Technologies",
            "publicationVenue": {
                "id": "urn:research:400d73aa-0d51-4582-9144-2069958881cd",
                "name": "Synthesis Lectures on Human Language Technologies",
                "alternate_names": [
                    "Synth Lect Hum Lang Technol"
                ],
                "issn": "1947-4040",
                "url": "https://www.morganclaypool.com/toc/hlt/1/1"
            },
            "year": 2012,
            "externalIds": {
                "MAG": "2108646579",
                "DBLP": "series/synthesis/2012Liu",
                "DOI": "10.2200/s00416ed1v01y201204hlt016",
                "CorpusId": 38022159
            },
            "abstract": "Sentiment analysis and opinion mining is the field of study that analyzes people's opinions, sentiments, evaluations, attitudes, and emotions from written language. It is one of the most active research areas in natural language processing and is also widely studied in data mining, Web mining, and text mining. In fact, this research has spread outside of computer science to the management sciences and social sciences due to its importance to business and society as a whole. The growing importance of sentiment analysis coincides with the growth of social media such as reviews, forum discussions, blogs, micro-blogs, Twitter, and social networks. For the first time in human history, we now have a huge volume of opinionated data recorded in digital form for analysis. Sentiment analysis systems are being applied in almost every business and social domain because opinions are central to almost all human activities and are key influencers of our behaviors. Our beliefs and perceptions of reality, and the choices we make, are largely conditioned on how others see and evaluate the world. For this reason, when we need to make a decision we often seek out the opinions of others. This is true not only for individuals but also for organizations. This book is a comprehensive introductory and survey text. It covers all important topics and the latest developments in the field with over 400 references. It is suitable for students, researchers and practitioners who are interested in social media analysis in general and sentiment analysis in particular. Lecturers can readily use it in class for courses on natural language processing, social media analysis, text mining, and data mining. Lecture slides are also available online.",
            "referenceCount": 433,
            "citationCount": 5681,
            "influentialCitationCount": 455,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/bfm:978-3-031-02145-9/1?pdf=chapter%20toc",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "2012-05-01",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Zhang2012SentimentAA,\n author = {Lei Zhang and B. Liu},\n booktitle = {Synthesis Lectures on Human Language Technologies},\n title = {Sentiment Analysis and Opinion Mining},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:05f5f8b2065a520846d89771ebaea2bb1534e9c6",
            "@type": "ScholarlyArticle",
            "paperId": "05f5f8b2065a520846d89771ebaea2bb1534e9c6",
            "corpusId": 219531210,
            "url": "https://www.semanticscholar.org/paper/05f5f8b2065a520846d89771ebaea2bb1534e9c6",
            "title": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "journals/corr/abs-2006-03654",
                "MAG": "3033187248",
                "ArXiv": "2006.03654",
                "CorpusId": 219531210
            },
            "abstract": "Recent progress in pre-trained neural language models has significantly improved the performance of many natural language processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with disentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism, where each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions. Second, an enhanced mask decoder is used to replace the output softmax layer to predict the masked tokens for model pretraining. We show that these two techniques significantly improve the efficiency of model pre-training and performance of downstream tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%). The DeBERTa code and pre-trained models will be made publicly available at this https URL.",
            "referenceCount": 71,
            "citationCount": 1373,
            "influentialCitationCount": 292,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-06-05",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2006.03654"
            },
            "citationStyles": {
                "bibtex": "@Article{He2020DeBERTaDB,\n author = {Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {DeBERTa: Decoding-enhanced BERT with Disentangled Attention},\n volume = {abs/2006.03654},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ff79d5d1282aea51195afcb7de898dca2c879a97",
            "@type": "ScholarlyArticle",
            "paperId": "ff79d5d1282aea51195afcb7de898dca2c879a97",
            "corpusId": 57370597,
            "url": "https://www.semanticscholar.org/paper/ff79d5d1282aea51195afcb7de898dca2c879a97",
            "title": "The Process of Question Answering",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2022,
            "externalIds": {
                "MAG": "1495683013",
                "DOI": "10.4324/9781003316817",
                "CorpusId": 57370597
            },
            "abstract": "Abstract : Problems in computational question answering assume a new perspective when question answering is viewed as a problem in natural language processing. A theory of question answering has been proposed which relies on ideas in conceptual information processing and theories of human memory organization. This theory of question answering has been implemented in a computer program, QUALM, currently being used by two story understanding systems to complete a natural language processing system which reads stories and answers questions about what was read. The processes in QUALM are divided into 4 phases: (1) Conceptual categorization which guides subsequent processing by dictating which specific inference mechanisms and memory retrieval strategies should be invoked in the course of answering a question; (2) Inferential analysis which is responsible for understanding what the questioner really meant when a question should not be taken literally; (3) Content specification which determines how much of an answer should be returned in terms of detail and elaborations, and (4) Retrieval heuristics which do the actual digging to extract an answer from memory.",
            "referenceCount": 0,
            "citationCount": 402,
            "influentialCitationCount": 9,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2022-08-02",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Lehnert2022ThePO,\n author = {W. Lehnert},\n title = {The Process of Question Answering},\n year = {2022}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:44279244407a64431810f982be6d0c7da4429dd7",
            "@type": "ScholarlyArticle",
            "paperId": "44279244407a64431810f982be6d0c7da4429dd7",
            "corpusId": 252542956,
            "url": "https://www.semanticscholar.org/paper/44279244407a64431810f982be6d0c7da4429dd7",
            "title": "BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining",
            "venue": "Briefings Bioinform.",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2022,
            "externalIds": {
                "ArXiv": "2210.10341",
                "DBLP": "journals/bib/LuoSXQZPL22",
                "DOI": "10.1093/bib/bbac409",
                "CorpusId": 252542956,
                "PubMed": "36156661"
            },
            "abstract": "Pre-trained language models have attracted increasing attention in the biomedical domain, inspired by their great success in the general natural language domain. Among the two main branches of pre-trained language models in the general language domain, i.e. BERT (and its variants) and GPT (and its variants), the first one has been extensively studied in the biomedical domain, such as BioBERT and PubMedBERT. While they have achieved great success on a variety of discriminative downstream biomedical tasks, the lack of generation ability constrains their application scope. In this paper, we propose BioGPT, a domain-specific generative Transformer language model pre-trained on large-scale biomedical literature. We evaluate BioGPT on six biomedical natural language processing tasks and demonstrate that our model outperforms previous models on most tasks. Especially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI end-to-end relation extraction tasks, respectively, and 78.2% accuracy on PubMedQA, creating a new record. Our case study on text generation further demonstrates the advantage of BioGPT on biomedical literature to generate fluent descriptions for biomedical terms.",
            "referenceCount": 58,
            "citationCount": 243,
            "influentialCitationCount": 28,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2210.10341",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2022-09-24",
            "journal": {
                "name": "Briefings in bioinformatics",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Luo2022BioGPTGP,\n author = {Renqian Luo and Liai Sun and Yingce Xia and Tao Qin and Sheng Zhang and Hoifung Poon and Tie-Yan Liu},\n booktitle = {Briefings Bioinform.},\n journal = {Briefings in bioinformatics},\n title = {BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining},\n year = {2022}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6ff2a434578ff2746b9283e45abf296887f48a2d",
            "@type": "ScholarlyArticle",
            "paperId": "6ff2a434578ff2746b9283e45abf296887f48a2d",
            "corpusId": 13046179,
            "url": "https://www.semanticscholar.org/paper/6ff2a434578ff2746b9283e45abf296887f48a2d",
            "title": "A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "journals/corr/HendrycksG16c",
                "ArXiv": "1610.02136",
                "MAG": "2952053192",
                "CorpusId": 13046179
            },
            "abstract": "We consider the two related problems of detecting if an example is misclassified or out-of-distribution. We present a simple baseline that utilizes probabilities from softmax distributions. Correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples, allowing for their detection. We assess performance by defining several tasks in computer vision, natural language processing, and automatic speech recognition, showing the effectiveness of this baseline across all. We then show the baseline can sometimes be surpassed, demonstrating the room for future research on these underexplored detection tasks.",
            "referenceCount": 47,
            "citationCount": 2363,
            "influentialCitationCount": 548,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-10-07",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1610.02136"
            },
            "citationStyles": {
                "bibtex": "@Article{Hendrycks2016ABF,\n author = {Dan Hendrycks and Kevin Gimpel},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks},\n volume = {abs/1610.02136},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e75638f8dee71ea0439e139f53411a98a9c9f825",
            "@type": "ScholarlyArticle",
            "paperId": "e75638f8dee71ea0439e139f53411a98a9c9f825",
            "corpusId": 229297794,
            "url": "https://www.semanticscholar.org/paper/e75638f8dee71ea0439e139f53411a98a9c9f825",
            "title": "PCT: Point cloud transformer",
            "venue": "Computational Visual Media",
            "publicationVenue": {
                "id": "urn:research:d2dfc02a-9028-4345-b6cf-556b76ac435b",
                "name": "Computational Visual Media",
                "alternate_names": [
                    "Comput Vis Media"
                ],
                "issn": "2096-0433",
                "url": "http://www.springer.com/41095"
            },
            "year": 2020,
            "externalIds": {
                "ArXiv": "2012.09688",
                "DBLP": "journals/cvm/GuoCLMMH21",
                "MAG": "3111535274",
                "DOI": "10.1007/s41095-021-0229-5",
                "CorpusId": 229297794
            },
            "abstract": null,
            "referenceCount": 49,
            "citationCount": 830,
            "influentialCitationCount": 112,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/s41095-021-0229-5.pdf",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-12-17",
            "journal": {
                "name": "Computational Visual Media",
                "volume": "7"
            },
            "citationStyles": {
                "bibtex": "@Article{Guo2020PCTPC,\n author = {Meng-Hao Guo and Junxiong Cai and Zheng-Ning Liu and Tai-Jiang Mu and Ralph Robert Martin and Shimin Hu},\n booktitle = {Computational Visual Media},\n journal = {Computational Visual Media},\n pages = {187 - 199},\n title = {PCT: Point cloud transformer},\n volume = {7},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:de4b9bc12ddb6b9f6090c032ef5c6290bd64ef36",
            "@type": "ScholarlyArticle",
            "paperId": "de4b9bc12ddb6b9f6090c032ef5c6290bd64ef36",
            "corpusId": 3538256,
            "url": "https://www.semanticscholar.org/paper/de4b9bc12ddb6b9f6090c032ef5c6290bd64ef36",
            "title": "Artificial Intelligence",
            "venue": "Communications in Computer and Information Science",
            "publicationVenue": {
                "id": "urn:research:82cd90f0-386f-4446-b237-6404c27f6a15",
                "name": "Communications in Computer and Information Science",
                "alternate_names": [
                    "Communications in computer and information science",
                    "Commun Comput Inf Sci",
                    "Commun comput inf sci"
                ],
                "issn": "1865-0929",
                "url": "http://www.springer.com/east/home/business/business+information+systems?SGWID=5-170-69-173753703-0"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "conf/bnaic/2017",
                "MAG": "2895834617",
                "DOI": "10.1007/978-3-319-76892-2",
                "CorpusId": 3538256
            },
            "abstract": null,
            "referenceCount": 27,
            "citationCount": 3314,
            "influentialCitationCount": 249,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Education",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": "823"
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Verheij2017ArtificialI,\n author = {Bart Verheij and M. Wiering},\n booktitle = {Communications in Computer and Information Science},\n title = {Artificial Intelligence},\n volume = {823},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:de5e7320729f5d3cbb6709eb6329ec41ace8c95d",
            "@type": "ScholarlyArticle",
            "paperId": "de5e7320729f5d3cbb6709eb6329ec41ace8c95d",
            "corpusId": 125617073,
            "url": "https://www.semanticscholar.org/paper/de5e7320729f5d3cbb6709eb6329ec41ace8c95d",
            "title": "Gaussian Error Linear Units (GELUs)",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2016,
            "externalIds": {
                "ArXiv": "1606.08415",
                "MAG": "2899663614",
                "CorpusId": 125617073
            },
            "abstract": "We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU activation function is $x\\Phi(x)$, where $\\Phi(x)$ the standard Gaussian cumulative distribution function. The GELU nonlinearity weights inputs by their value, rather than gates inputs by their sign as in ReLUs ($x\\mathbf{1}_{x>0}$). We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all considered computer vision, natural language processing, and speech tasks.",
            "referenceCount": 30,
            "citationCount": 2819,
            "influentialCitationCount": 345,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2016-06-27",
            "journal": {
                "name": "arXiv: Learning",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Article{Hendrycks2016GaussianEL,\n author = {Dan Hendrycks and Kevin Gimpel},\n journal = {arXiv: Learning},\n title = {Gaussian Error Linear Units (GELUs)},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ccf6a69a7f33bcf052aa7def176d3b9de495beb7",
            "@type": "ScholarlyArticle",
            "paperId": "ccf6a69a7f33bcf052aa7def176d3b9de495beb7",
            "corpusId": 1704893,
            "url": "https://www.semanticscholar.org/paper/ccf6a69a7f33bcf052aa7def176d3b9de495beb7",
            "title": "Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2483215953",
                "DBLP": "conf/nips/BolukbasiCZSK16",
                "ArXiv": "1607.06520",
                "CorpusId": 1704893
            },
            "abstract": "The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.",
            "referenceCount": 48,
            "citationCount": 2378,
            "influentialCitationCount": 329,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-07-21",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Bolukbasi2016ManIT,\n author = {Tolga Bolukbasi and Kai-Wei Chang and James Y. Zou and Venkatesh Saligrama and A. Kalai},\n booktitle = {Neural Information Processing Systems},\n pages = {4349-4357},\n title = {Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:83a6cacc126d85c45605797406262677c256a6af",
            "@type": "ScholarlyArticle",
            "paperId": "83a6cacc126d85c45605797406262677c256a6af",
            "corpusId": 18593743,
            "url": "https://www.semanticscholar.org/paper/83a6cacc126d85c45605797406262677c256a6af",
            "title": "Software Framework for Topic Modelling with Large Corpora",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2010,
            "externalIds": {
                "MAG": "168564468",
                "CorpusId": 18593743
            },
            "abstract": "Large corpora are ubiquitous in today's world and memory\nquickly becomes the limiting factor in practical applications\nof the Vector Space Model (VSM). We identify gap in existing\nVSM implementations, which is their scalability and ease of\nuse. We describe a Natural Language Processing software\nframework which is based on the idea of document streaming,\ni.e. processing corpora document after document, in a memory\nindependent fashion. In this framework, we implement several\npopular algorithms for topical inference, including Latent\nSemantic Analysis and Latent Dirichlet Allocation, in a way\nthat makes them completely independent of the training corpus\nsize. Particular emphasis is placed on straightforward and\nintuitive framework design, so that modifications and\nextensions of the methods and/or their application by\ninterested practitioners are effortless. We demonstrate the\nusefulness of our approach on a real-world scenario of\ncomputing document similarities within an existing digital\nlibrary DML-CZ.",
            "referenceCount": 29,
            "citationCount": 4354,
            "influentialCitationCount": 269,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2010-05-22",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Rehurek2010SoftwareFF,\n author = {Radim Rehurek and Petr Sojka},\n title = {Software Framework for Topic Modelling with Large Corpora},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7e5709d81558d3ef4265de29ea75931afeb1f2dd",
            "@type": "ScholarlyArticle",
            "paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
            "corpusId": 221702858,
            "url": "https://www.semanticscholar.org/paper/7e5709d81558d3ef4265de29ea75931afeb1f2dd",
            "title": "Efficient Transformers: A Survey",
            "venue": "ACM Computing Surveys",
            "publicationVenue": {
                "id": "urn:research:7b2adce0-d53f-49d6-8784-b0645604fe62",
                "name": "ACM Computing Surveys",
                "alternate_names": [
                    "ACM Comput Surv"
                ],
                "issn": "0360-0300",
                "url": "http://www.acm.org/pubs/surveys/"
            },
            "year": 2020,
            "externalIds": {
                "ArXiv": "2009.06732",
                "DBLP": "journals/corr/abs-2009-06732",
                "MAG": "3085139254",
                "DOI": "10.1145/3530811",
                "CorpusId": 221702858
            },
            "abstract": "Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision, and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of \u201cX-former\u201d models have been proposed\u2014Reformer, Linformer, Performer, Longformer, to name a few\u2014which improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency. With the aim of helping the avid researcher navigate this flurry, this article characterizes a large and thoughtful selection of recent efficiency-flavored \u201cX-former\u201d models, providing an organized and comprehensive overview of existing work and models across multiple domains.",
            "referenceCount": 106,
            "citationCount": 741,
            "influentialCitationCount": 67,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3530811",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2020-09-14",
            "journal": {
                "name": "ACM Computing Surveys",
                "volume": "55"
            },
            "citationStyles": {
                "bibtex": "@Article{Tay2020EfficientTA,\n author = {Yi Tay and Mostafa Dehghani and Dara Bahri and Donald Metzler},\n booktitle = {ACM Computing Surveys},\n journal = {ACM Computing Surveys},\n pages = {1 - 28},\n title = {Efficient Transformers: A Survey},\n volume = {55},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f58ea68448a584b8c8540c86bb9965d746b767a9",
            "@type": "ScholarlyArticle",
            "paperId": "f58ea68448a584b8c8540c86bb9965d746b767a9",
            "corpusId": 53304118,
            "url": "https://www.semanticscholar.org/paper/f58ea68448a584b8c8540c86bb9965d746b767a9",
            "title": "Deep Learning: Methods and Applications",
            "venue": "Foundations and Trends\u00ae in Signal Processing",
            "publicationVenue": {
                "id": "urn:research:a30697a9-2a04-4a33-ae2f-03ea5fbb71f8",
                "name": "Foundations and Trends\u00ae in Signal Processing",
                "alternate_names": [
                    "Found Trends Signal Process",
                    "Found Trends\u00ae Signal Process",
                    "Foundations and Trends in Signal Processing"
                ],
                "issn": "1932-8346",
                "url": "https://www.nowpublishers.com/sig"
            },
            "year": 2014,
            "externalIds": {
                "MAG": "2150341604",
                "DBLP": "journals/ftsig/DengY14",
                "DOI": "10.1561/2000000039",
                "CorpusId": 53304118
            },
            "abstract": "This monograph provides an overview of general deep learning methodology and its applications to a variety of signal and information processing tasks. The application areas are chosen with the following three criteria in mind: (1) expertise or knowledge of the authors; (2) the application areas that have already been transformed by the successful use of deep learning technology, such as speech recognition and computer vision; and (3) the application areas that have the potential to be impacted significantly by deep learning and that have been experiencing research growth, including natural language and text processing, information retrieval, and multimodal information processing empowered by multi-task deep learning.",
            "referenceCount": 454,
            "citationCount": 3183,
            "influentialCitationCount": 143,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.ped-perinatology.ru/jour/article/download/586/570",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2014-06-12",
            "journal": {
                "name": "Found. Trends Signal Process.",
                "volume": "7"
            },
            "citationStyles": {
                "bibtex": "@Article{Deng2014DeepLM,\n author = {L. Deng and Dong Yu},\n booktitle = {Foundations and Trends\u00ae in Signal Processing},\n journal = {Found. Trends Signal Process.},\n pages = {197-387},\n title = {Deep Learning: Methods and Applications},\n volume = {7},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0e779fd59353a7f1f5b559b9d65fa4bfe367890c",
            "@type": "ScholarlyArticle",
            "paperId": "0e779fd59353a7f1f5b559b9d65fa4bfe367890c",
            "corpusId": 15195762,
            "url": "https://www.semanticscholar.org/paper/0e779fd59353a7f1f5b559b9d65fa4bfe367890c",
            "title": "Geometric Deep Learning: Going beyond Euclidean data",
            "venue": "IEEE Signal Processing Magazine",
            "publicationVenue": {
                "id": "urn:research:f62e5eab-173a-4e0a-a963-ed8de9835d22",
                "name": "IEEE Signal Processing Magazine",
                "alternate_names": [
                    "IEEE Signal Process Mag"
                ],
                "issn": "1053-5888",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=79"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "journals/spm/BronsteinBLSV17",
                "MAG": "3102013575",
                "ArXiv": "1611.08097",
                "DOI": "10.1109/MSP.2017.2693418",
                "CorpusId": 15195762
            },
            "abstract": "Many scientific fields study data with an underlying structure that is non-Euclidean. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions) and are natural targets for machine-learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural-language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure and in cases where the invariances of these structures are built into networks used to model them.",
            "referenceCount": 125,
            "citationCount": 2754,
            "influentialCitationCount": 155,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1611.08097",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-11-24",
            "journal": {
                "name": "IEEE Signal Process. Mag.",
                "volume": "34"
            },
            "citationStyles": {
                "bibtex": "@Article{Bronstein2016GeometricDL,\n author = {M. Bronstein and Joan Bruna and Yann LeCun and Arthur Szlam and P. Vandergheynst},\n booktitle = {IEEE Signal Processing Magazine},\n journal = {IEEE Signal Process. Mag.},\n pages = {18-42},\n title = {Geometric Deep Learning: Going beyond Euclidean data},\n volume = {34},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4f410ab5c8b12b34b38421241366ee456bbebab9",
            "@type": "ScholarlyArticle",
            "paperId": "4f410ab5c8b12b34b38421241366ee456bbebab9",
            "corpusId": 10977241,
            "url": "https://www.semanticscholar.org/paper/4f410ab5c8b12b34b38421241366ee456bbebab9",
            "title": "Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                "name": "Annual Meeting of the Association for Computational Linguistics",
                "alternate_names": [
                    "Annu Meet Assoc Comput Linguistics",
                    "Meeting of the Association for Computational Linguistics",
                    "ACL",
                    "Meet Assoc Comput Linguistics"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/anthology/venues/acl/"
            },
            "year": 2005,
            "externalIds": {
                "MAG": "2096765155",
                "ACL": "P05-1045",
                "DBLP": "conf/acl/FinkelGM05",
                "DOI": "10.3115/1219840.1219885",
                "CorpusId": 10977241
            },
            "abstract": "Most current statistical natural language processing models use only local features so as to permit dynamic programming in inference, but this makes them unable to fully account for the long distance structure that is prevalent in language use. We show how to solve this dilemma with Gibbs sampling, a simple Monte Carlo method used to perform approximate inference in factored probabilistic models. By using simulated annealing in place of Viterbi decoding in sequence models such as HMMs, CMMs, and CRFs, it is possible to incorporate non-local structure while preserving tractable inference. We use this technique to augment an existing CRF-based information extraction system with long-distance dependency models, enforcing label consistency and extraction template consistency constraints. This technique results in an error reduction of up to 9% over state-of-the-art systems on two established information extraction tasks.",
            "referenceCount": 26,
            "citationCount": 3392,
            "influentialCitationCount": 435,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://dl.acm.org/ft_gateway.cfm?id=1219885&type=pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2005-06-25",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Finkel2005IncorporatingNI,\n author = {J. Finkel and Trond Grenager and Christopher D. Manning},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {363-370},\n title = {Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling},\n year = {2005}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6d431f835c06afdea45dff6b24486bf301ebdef0",
            "@type": "ScholarlyArticle",
            "paperId": "6d431f835c06afdea45dff6b24486bf301ebdef0",
            "corpusId": 10175374,
            "url": "https://www.semanticscholar.org/paper/6d431f835c06afdea45dff6b24486bf301ebdef0",
            "title": "An Overview of Multi-Task Learning in Deep Neural Networks",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "journals/corr/Ruder17a",
                "ArXiv": "1706.05098",
                "MAG": "2624871570",
                "CorpusId": 10175374
            },
            "abstract": "Multi-task learning (MTL) has led to successes in many applications of machine learning, from natural language processing and speech recognition to computer vision and drug discovery. This article aims to give a general overview of MTL, particularly in deep neural networks. It introduces the two most common methods for MTL in Deep Learning, gives an overview of the literature, and discusses recent advances. In particular, it seeks to help ML practitioners apply MTL by shedding light on how MTL works and providing guidelines for choosing appropriate auxiliary tasks.",
            "referenceCount": 58,
            "citationCount": 2312,
            "influentialCitationCount": 152,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2017-06-15",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1706.05098"
            },
            "citationStyles": {
                "bibtex": "@Article{Ruder2017AnOO,\n author = {Sebastian Ruder},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {An Overview of Multi-Task Learning in Deep Neural Networks},\n volume = {abs/1706.05098},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:91b1bc2026c46b71c3479138efd43ae35fa2ccca",
            "@type": "ScholarlyArticle",
            "paperId": "91b1bc2026c46b71c3479138efd43ae35fa2ccca",
            "corpusId": 22609608,
            "url": "https://www.semanticscholar.org/paper/91b1bc2026c46b71c3479138efd43ae35fa2ccca",
            "title": "Thinking in Words: Language as an Embodied Medium of Thought",
            "venue": "Topics in Cognitive Science",
            "publicationVenue": {
                "id": "urn:research:c2da8960-5aa5-430f-938c-2c2c811e5c96",
                "name": "Topics in Cognitive Science",
                "alternate_names": [
                    "Top Cogn Sci"
                ],
                "issn": "1756-8757",
                "url": "http://www3.interscience.wiley.com/journal/121673067/toc"
            },
            "year": 2014,
            "externalIds": {
                "DBLP": "journals/topics/Dove14",
                "MAG": "2030440611",
                "DOI": "10.1111/tops.12102",
                "CorpusId": 22609608,
                "PubMed": "24943737"
            },
            "abstract": "Recently, there has been a great deal of interest in the idea that natural language enhances and extends our cognitive capabilities. Supporters of embodied cognition have been particularly interested in the way in which language may provide a solution to the problem of abstract concepts. Toward this end, some have emphasized the way in which language may act as form of cognitive scaffolding and others have emphasized the potential importance of language-based distributional information. This essay defends a version of the cognitive enhancement thesis that integrates and builds on both of these proposals. I argue that the embodied representations associated with language processing serve as a supplementary medium for conceptual processing. The acquisition of a natural language provides a means of extending our cognitive reach by giving us access to an internalized combinatorial symbol system that augments and supports the context-sensitive embodied representational systems that exist independently of language.",
            "referenceCount": 124,
            "citationCount": 100,
            "influentialCitationCount": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/tops.12102",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Psychology",
                "Medicine",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Philosophy",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2014-07-01",
            "journal": {
                "name": "Topics in cognitive science",
                "volume": "6 3"
            },
            "citationStyles": {
                "bibtex": "@Article{Dove2014ThinkingIW,\n author = {Guy Dove},\n booktitle = {Topics in Cognitive Science},\n journal = {Topics in cognitive science},\n pages = {\n          371-89\n        },\n title = {Thinking in Words: Language as an Embodied Medium of Thought},\n volume = {6 3},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2573af4e13d9a5dddb257d22cd38a600528d9a8b",
            "@type": "ScholarlyArticle",
            "paperId": "2573af4e13d9a5dddb257d22cd38a600528d9a8b",
            "corpusId": 215238853,
            "url": "https://www.semanticscholar.org/paper/2573af4e13d9a5dddb257d22cd38a600528d9a8b",
            "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                "name": "Annual Meeting of the Association for Computational Linguistics",
                "alternate_names": [
                    "Annu Meet Assoc Comput Linguistics",
                    "Meeting of the Association for Computational Linguistics",
                    "ACL",
                    "Meet Assoc Comput Linguistics"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/anthology/venues/acl/"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3034457371",
                "DBLP": "journals/corr/abs-2004-02984",
                "ArXiv": "2004.02984",
                "ACL": "2020.acl-main.195",
                "DOI": "10.18653/v1/2020.acl-main.195",
                "CorpusId": 215238853
            },
            "abstract": "Natural Language Processing (NLP) has recently achieved great success by using huge pre-trained models with hundreds of millions of parameters. However, these models suffer from heavy model sizes and high latency such that they cannot be deployed to resource-limited mobile devices. In this paper, we propose MobileBERT for compressing and accelerating the popular BERT model. Like the original BERT, MobileBERT is task-agnostic, that is, it can be generically applied to various downstream NLP tasks via simple fine-tuning. Basically, MobileBERT is a thin version of BERT_LARGE, while equipped with bottleneck structures and a carefully designed balance between self-attentions and feed-forward networks. To train MobileBERT, we first train a specially designed teacher model, an inverted-bottleneck incorporated BERT_LARGE model. Then, we conduct knowledge transfer from this teacher to MobileBERT. Empirical studies show that MobileBERT is 4.3x smaller and 5.5x faster than BERT_BASE while achieving competitive results on well-known benchmarks. On the natural language inference tasks of GLUE, MobileBERT achieves a GLUE score of 77.7 (0.6 lower than BERT_BASE), and 62 ms latency on a Pixel 4 phone. On the SQuAD v1.1/v2.0 question answering task, MobileBERT achieves a dev F1 score of 90.0/79.2 (1.5/2.1 higher than BERT_BASE).",
            "referenceCount": 68,
            "citationCount": 565,
            "influentialCitationCount": 97,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/2020.acl-main.195.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-04-06",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Sun2020MobileBERTAC,\n author = {Zhiqing Sun and Hongkun Yu and Xiaodan Song and Renjie Liu and Yiming Yang and Denny Zhou},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {2158-2170},\n title = {MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7b95d389bc6affe6a127d53b04bcfd68138f1a1a",
            "@type": "ScholarlyArticle",
            "paperId": "7b95d389bc6affe6a127d53b04bcfd68138f1a1a",
            "corpusId": 577937,
            "url": "https://www.semanticscholar.org/paper/7b95d389bc6affe6a127d53b04bcfd68138f1a1a",
            "title": "TextRank: Bringing Order into Text",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "publicationVenue": {
                "id": "urn:research:41bf9ed3-85b3-4c90-b015-150e31690253",
                "name": "Conference on Empirical Methods in Natural Language Processing",
                "alternate_names": [
                    "Empir Method Nat Lang Process",
                    "Empirical Methods in Natural Language Processing",
                    "Conf Empir Method Nat Lang Process",
                    "EMNLP"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/emnlp"
            },
            "year": 2004,
            "externalIds": {
                "ACL": "W04-3252",
                "DBLP": "conf/emnlp/MihalceaT04",
                "MAG": "1525595230",
                "CorpusId": 577937
            },
            "abstract": "In this paper, the authors introduce TextRank, a graph-based ranking model for text processing, and show how this model can be successfully used in natural language applications.",
            "referenceCount": 17,
            "citationCount": 3771,
            "influentialCitationCount": 595,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2004-07-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Mihalcea2004TextRankBO,\n author = {Rada Mihalcea and Paul Tarau},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {404-411},\n title = {TextRank: Bringing Order into Text},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:affcf19551b01c4c8009d061750700d91c2f79e9",
            "@type": "ScholarlyArticle",
            "paperId": "affcf19551b01c4c8009d061750700d91c2f79e9",
            "corpusId": 20370792,
            "url": "https://www.semanticscholar.org/paper/affcf19551b01c4c8009d061750700d91c2f79e9",
            "title": "Principles of Artificial Intelligence",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "publicationVenue": {
                "id": "urn:research:25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "alternate_names": [
                    "IEEE Trans Pattern Anal Mach Intell"
                ],
                "issn": "0162-8828",
                "url": "http://www.computer.org/tpami/"
            },
            "year": 1980,
            "externalIds": {
                "MAG": "3022783439",
                "DBLP": "books/sp/Nilsson82",
                "DOI": "10.1007/978-3-662-09438-9",
                "CorpusId": 20370792
            },
            "abstract": null,
            "referenceCount": 30,
            "citationCount": 3910,
            "influentialCitationCount": 176,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www3.ub.tu-berlin.de/ihv/000413539.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1980-01-23",
            "journal": {
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "volume": "PAMI-3"
            },
            "citationStyles": {
                "bibtex": "@Article{Nilsson1980PrinciplesOA,\n author = {N. Nilsson},\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n pages = {112-112},\n title = {Principles of Artificial Intelligence},\n volume = {PAMI-3},\n year = {1980}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2a567ebd78939d0861d788f0fedff8d40ae62bf2",
            "@type": "ScholarlyArticle",
            "paperId": "2a567ebd78939d0861d788f0fedff8d40ae62bf2",
            "corpusId": 102352093,
            "url": "https://www.semanticscholar.org/paper/2a567ebd78939d0861d788f0fedff8d40ae62bf2",
            "title": "Publicly Available Clinical BERT Embeddings",
            "venue": "Proceedings of the 2nd Clinical Natural Language Processing Workshop",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2925863688",
                "DBLP": "journals/corr/abs-1904-03323",
                "ArXiv": "1904.03323",
                "ACL": "W19-1909",
                "DOI": "10.18653/v1/W19-1909",
                "CorpusId": 102352093
            },
            "abstract": "Contextual word embedding models such as ELMo and BERT have dramatically improved performance for many natural language processing (NLP) tasks in recent months. However, these models have been minimally explored on specialty corpora, such as clinical text; moreover, in the clinical domain, no publicly-available pre-trained BERT models yet exist. In this work, we address this need by exploring and releasing BERT models for clinical text: one for generic clinical text and another for discharge summaries specifically. We demonstrate that using a domain-specific model yields performance improvements on 3/5 clinical NLP tasks, establishing a new state-of-the-art on the MedNLI dataset. We find that these domain-specific models are not as performant on 2 clinical de-identification tasks, and argue that this is a natural consequence of the differences between de-identified source text and synthetically non de-identified task text.",
            "referenceCount": 27,
            "citationCount": 1296,
            "influentialCitationCount": 230,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-04-06",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1904.03323"
            },
            "citationStyles": {
                "bibtex": "@Article{Alsentzer2019PubliclyAC,\n author = {Emily Alsentzer and John R. Murphy and Willie Boag and W. Weng and Di Jin and Tristan Naumann and Matthew B. A. McDermott},\n booktitle = {Proceedings of the 2nd Clinical Natural Language Processing Workshop},\n journal = {ArXiv},\n title = {Publicly Available Clinical BERT Embeddings},\n volume = {abs/1904.03323},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5507d267bbf0b4cdb9f893c3c0960a45016f7010",
            "@type": "ScholarlyArticle",
            "paperId": "5507d267bbf0b4cdb9f893c3c0960a45016f7010",
            "corpusId": 195316471,
            "url": "https://www.semanticscholar.org/paper/5507d267bbf0b4cdb9f893c3c0960a45016f7010",
            "title": "Deep Leakage from Gradients",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/corr/abs-1906-08935",
                "MAG": "2953304078",
                "ArXiv": "1906.08935",
                "DOI": "10.1007/978-3-030-63076-8_2",
                "CorpusId": 195316471
            },
            "abstract": null,
            "referenceCount": 42,
            "citationCount": 1277,
            "influentialCitationCount": 207,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics",
                "Materials Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Materials Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-06-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zhu2019DeepLF,\n author = {Ligeng Zhu and Zhijian Liu and Song Han},\n booktitle = {Neural Information Processing Systems},\n pages = {17-31},\n title = {Deep Leakage from Gradients},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:63748e59f4e106cbda6b65939b77589f40e48fcb",
            "@type": "ScholarlyArticle",
            "paperId": "63748e59f4e106cbda6b65939b77589f40e48fcb",
            "corpusId": 201304248,
            "url": "https://www.semanticscholar.org/paper/63748e59f4e106cbda6b65939b77589f40e48fcb",
            "title": "Text Summarization with Pretrained Encoders",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "publicationVenue": {
                "id": "urn:research:41bf9ed3-85b3-4c90-b015-150e31690253",
                "name": "Conference on Empirical Methods in Natural Language Processing",
                "alternate_names": [
                    "Empir Method Nat Lang Process",
                    "Empirical Methods in Natural Language Processing",
                    "Conf Empir Method Nat Lang Process",
                    "EMNLP"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/emnlp"
            },
            "year": 2019,
            "externalIds": {
                "ACL": "D19-1387",
                "ArXiv": "1908.08345",
                "DBLP": "conf/emnlp/LiuL19",
                "MAG": "2970419734",
                "DOI": "10.18653/v1/D19-1387",
                "CorpusId": 201304248
            },
            "abstract": "Bidirectional Encoder Representations from Transformers (BERT) represents the latest incarnation of pretrained language models which have recently advanced a wide range of natural language processing tasks. In this paper, we showcase how BERT can be usefully applied in text summarization and propose a general framework for both extractive and abstractive models. We introduce a novel document-level encoder based on BERT which is able to express the semantics of a document and obtain representations for its sentences. Our extractive model is built on top of this encoder by stacking several inter-sentence Transformer layers. For abstractive summarization, we propose a new fine-tuning schedule which adopts different optimizers for the encoder and the decoder as a means of alleviating the mismatch between the two (the former is pretrained while the latter is not). We also demonstrate that a two-staged fine-tuning approach can further boost the quality of the generated summaries. Experiments on three datasets show that our model achieves state-of-the-art results across the board in both extractive and abstractive settings.",
            "referenceCount": 36,
            "citationCount": 1129,
            "influentialCitationCount": 214,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/D19-1387.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-08-01",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1908.08345"
            },
            "citationStyles": {
                "bibtex": "@Article{Liu2019TextSW,\n author = {Yang Liu and Mirella Lapata},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n journal = {ArXiv},\n title = {Text Summarization with Pretrained Encoders},\n volume = {abs/1908.08345},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ebd03c093df7ec30a2a360b377e85b6f00ecb96f",
            "@type": "ScholarlyArticle",
            "paperId": "ebd03c093df7ec30a2a360b377e85b6f00ecb96f",
            "corpusId": 18130231,
            "url": "https://www.semanticscholar.org/paper/ebd03c093df7ec30a2a360b377e85b6f00ecb96f",
            "title": "LTP: A Chinese Language Technology Platform",
            "venue": "International Conference on Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:f51ff783-cdff-4e22-94fb-28e6336d17b3",
                "name": "International Conference on Computational Linguistics",
                "alternate_names": [
                    "Int Conf Comput Linguistics",
                    "COLING"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/anthology/venues/coling/"
            },
            "year": 2010,
            "externalIds": {
                "MAG": "2116343275",
                "DBLP": "conf/coling/CheLL10",
                "ACL": "C10-3004",
                "CorpusId": 18130231
            },
            "abstract": "LTP (Language Technology Platform) is an integrated Chinese processing platform which includes a suite of high performance natural language processing (NLP) modules and relevant corpora. Especially for the syntactic and semantic parsing modules, we achieved good results in some relevant evaluations, such as CoNLL and SemEval. Based on XML internal data representation, users can easily use these modules and corpora by invoking DLL (Dynamic Link Library) or Web service APIs (Application Program Interface), and view the processing results directly by the visualization tool.",
            "referenceCount": 8,
            "citationCount": 462,
            "influentialCitationCount": 45,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2010-08-23",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Che2010LTPAC,\n author = {Wanxiang Che and Zhenghua Li and Ting Liu},\n booktitle = {International Conference on Computational Linguistics},\n pages = {13-16},\n title = {LTP: A Chinese Language Technology Platform},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ba10b8f9ee40b68053af9e6c2383aa2c6e39e9be",
            "@type": "ScholarlyArticle",
            "paperId": "ba10b8f9ee40b68053af9e6c2383aa2c6e39e9be",
            "corpusId": 119297355,
            "url": "https://www.semanticscholar.org/paper/ba10b8f9ee40b68053af9e6c2383aa2c6e39e9be",
            "title": "Text Classification Algorithms: A Survey",
            "venue": "Inf.",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/corr/abs-1904-08067",
                "ArXiv": "1904.08067",
                "MAG": "2937423263",
                "DOI": "10.3390/info10040150",
                "CorpusId": 119297355
            },
            "abstract": "In recent years, there has been an exponential growth in the number of complex documentsand texts that require a deeper understanding of machine learning methods to be able to accuratelyclassify texts in many applications. Many machine learning approaches have achieved surpassingresults in natural language processing. The success of these learning algorithms relies on their capacityto understand complex models and non-linear relationships within data. However, finding suitablestructures, architectures, and techniques for text classification is a challenge for researchers. In thispaper, a brief overview of text classification algorithms is discussed. This overview covers differenttext feature extractions, dimensionality reduction methods, existing algorithms and techniques, andevaluations methods. Finally, the limitations of each technique and their application in real-worldproblems are discussed.",
            "referenceCount": 262,
            "citationCount": 952,
            "influentialCitationCount": 76,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/2078-2489/10/4/150/pdf?version=1556181437",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2019-04-17",
            "journal": {
                "name": "Inf.",
                "volume": "10"
            },
            "citationStyles": {
                "bibtex": "@Article{Kowsari2019TextCA,\n author = {Kamran Kowsari and K. Meimandi and Mojtaba Heidarysafa and Sanjana Mendu and Laura E. Barnes and Donald E. Brown},\n booktitle = {Inf.},\n journal = {Inf.},\n pages = {150},\n title = {Text Classification Algorithms: A Survey},\n volume = {10},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a97c7876ebf9ae6c468db92c3c6dc1c0be832192",
            "@type": "ScholarlyArticle",
            "paperId": "a97c7876ebf9ae6c468db92c3c6dc1c0be832192",
            "corpusId": 2065400,
            "url": "https://www.semanticscholar.org/paper/a97c7876ebf9ae6c468db92c3c6dc1c0be832192",
            "title": "brat: a Web-based Tool for NLP-Assisted Text Annotation",
            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:8de18c35-6785-4e54-99f2-21ee961302c6",
                "name": "Conference of the European Chapter of the Association for Computational Linguistics",
                "alternate_names": [
                    "Conf Eur Chapter Assoc Comput Linguistics",
                    "EACL"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/anthology/venues/eacl/"
            },
            "year": 2012,
            "externalIds": {
                "DBLP": "conf/eacl/StenetorpPTOAT12",
                "MAG": "8550301",
                "ACL": "E12-2021",
                "CorpusId": 2065400
            },
            "abstract": "We introduce the brat rapid annotation tool (BRAT), an intuitive web-based tool for text annotation supported by Natural Language Processing (NLP) technology. BRAT has been developed for rich structured annotation for a variety of NLP tasks and aims to support manual curation efforts and increase annotator productivity using NLP techniques. We discuss several case studies of real-world annotation projects using pre-release versions of BRAT and present an evaluation of annotation assisted by semantic class disambiguation on a multicategory entity mention annotation task, showing a 15% decrease in total annotation time. BRAT is available under an open-source license from: http://brat.nlplab.org",
            "referenceCount": 24,
            "citationCount": 1226,
            "influentialCitationCount": 117,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2012-04-23",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Stenetorp2012bratAW,\n author = {Pontus Stenetorp and Sampo Pyysalo and Goran Topic and Tomoko Ohta and S. Ananiadou and Junichi Tsujii},\n booktitle = {Conference of the European Chapter of the Association for Computational Linguistics},\n pages = {102-107},\n title = {brat: a Web-based Tool for NLP-Assisted Text Annotation},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0fe73c19513dfd17372d8ef58da0d0149725832c",
            "@type": "ScholarlyArticle",
            "paperId": "0fe73c19513dfd17372d8ef58da0d0149725832c",
            "corpusId": 3411445,
            "url": "https://www.semanticscholar.org/paper/0fe73c19513dfd17372d8ef58da0d0149725832c",
            "title": "Learning Word Vectors for 157 Languages",
            "venue": "International Conference on Language Resources and Evaluation",
            "publicationVenue": {
                "id": "urn:research:7474c4a0-75d9-4105-9809-8e7d5201c5e1",
                "name": "International Conference on Language Resources and Evaluation",
                "alternate_names": [
                    "LREC",
                    "Int Conf Lang Resour Evaluation"
                ],
                "issn": null,
                "url": "http://www.lrec-conf.org/"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/corr/abs-1802-06893",
                "ArXiv": "1802.06893",
                "MAG": "2950062634",
                "ACL": "L18-1550",
                "CorpusId": 3411445
            },
            "abstract": "Distributed word representations, or word vectors, have recently been applied to many tasks in natural language processing, leading to state-of-the-art performance. A key ingredient to the successful application of these representations is to train them on very large corpora, and use these pre-trained models in downstream tasks. In this paper, we describe how we trained such high quality word representations for 157 languages. We used two sources of data to train these models: the free online encyclopedia Wikipedia and data from the common crawl project. We also introduce three new word analogy datasets to evaluate these word vectors, for French, Hindi and Polish. Finally, we evaluate our pre-trained word vectors on 10 languages for which evaluation datasets exists, showing very strong performance compared to previous models.",
            "referenceCount": 26,
            "citationCount": 1195,
            "influentialCitationCount": 157,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-02-19",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1802.06893"
            },
            "citationStyles": {
                "bibtex": "@Article{Grave2018LearningWV,\n author = {Edouard Grave and Piotr Bojanowski and Prakhar Gupta and Armand Joulin and Tomas Mikolov},\n booktitle = {International Conference on Language Resources and Evaluation},\n journal = {ArXiv},\n title = {Learning Word Vectors for 157 Languages},\n volume = {abs/1802.06893},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1c3112ef8a346b9817382ed34a8c146c53d5bcf5",
            "@type": "ScholarlyArticle",
            "paperId": "1c3112ef8a346b9817382ed34a8c146c53d5bcf5",
            "corpusId": 52271711,
            "url": "https://www.semanticscholar.org/paper/1c3112ef8a346b9817382ed34a8c146c53d5bcf5",
            "title": "XNLI: Evaluating Cross-lingual Sentence Representations",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "publicationVenue": {
                "id": "urn:research:41bf9ed3-85b3-4c90-b015-150e31690253",
                "name": "Conference on Empirical Methods in Natural Language Processing",
                "alternate_names": [
                    "Empir Method Nat Lang Process",
                    "Empirical Methods in Natural Language Processing",
                    "Conf Empir Method Nat Lang Process",
                    "EMNLP"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/emnlp"
            },
            "year": 2018,
            "externalIds": {
                "ArXiv": "1809.05053",
                "ACL": "D18-1269",
                "MAG": "2891555348",
                "DBLP": "conf/emnlp/ConneauRLWBSS18",
                "DOI": "10.18653/v1/D18-1269",
                "CorpusId": 52271711
            },
            "abstract": "State-of-the-art natural language processing systems rely on supervision in the form of annotated data to learn competent models. These models are generally trained on data in a single language (usually English), and cannot be directly used beyond that language. Since collecting data in every language is not realistic, there has been a growing interest in cross-lingual language understanding (XLU) and low-resource cross-language transfer. In this work, we construct an evaluation set for XLU by extending the development and test sets of the Multi-Genre Natural Language Inference Corpus (MultiNLI) to 14 languages, including low-resource languages such as Swahili and Urdu. We hope that our dataset, dubbed XNLI, will catalyze research in cross-lingual sentence understanding by providing an informative standard evaluation task. In addition, we provide several baselines for multilingual sentence understanding, including two based on machine translation systems, and two that use parallel data to train aligned multilingual bag-of-words and LSTM encoders. We find that XNLI represents a practical and challenging evaluation suite, and that directly translating the test data yields the best performance among available baselines.",
            "referenceCount": 65,
            "citationCount": 906,
            "influentialCitationCount": 245,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/D18-1269.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-09-13",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Conneau2018XNLIEC,\n author = {Alexis Conneau and Guillaume Lample and Ruty Rinott and Adina Williams and Samuel R. Bowman and Holger Schwenk and Veselin Stoyanov},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {2475-2485},\n title = {XNLI: Evaluating Cross-lingual Sentence Representations},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6b8b2075319accc23fef43e4cf76bc3682189d82",
            "@type": "ScholarlyArticle",
            "paperId": "6b8b2075319accc23fef43e4cf76bc3682189d82",
            "corpusId": 9870160,
            "url": "https://www.semanticscholar.org/paper/6b8b2075319accc23fef43e4cf76bc3682189d82",
            "title": "Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                "name": "Annual Meeting of the Association for Computational Linguistics",
                "alternate_names": [
                    "Annu Meet Assoc Comput Linguistics",
                    "Meeting of the Association for Computational Linguistics",
                    "ACL",
                    "Meet Assoc Comput Linguistics"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/anthology/venues/acl/"
            },
            "year": 2016,
            "externalIds": {
                "ACL": "P16-2034",
                "DBLP": "conf/acl/ZhouSTQLHX16",
                "MAG": "2517194566",
                "DOI": "10.18653/v1/P16-2034",
                "CorpusId": 9870160
            },
            "abstract": "Relation classification is an important semantic processing task in the field of natural language processing (NLP). State-ofthe-art systems still rely on lexical resources such as WordNet or NLP systems like dependency parser and named entity recognizers (NER) to get high-level features. Another challenge is that important information can appear at any position in the sentence. To tackle these problems, we propose Attention-Based Bidirectional Long Short-Term Memory Networks(AttBLSTM) to capture the most important semantic information in a sentence. The experimental results on the SemEval-2010 relation classification task show that our method outperforms most of the existing methods, with only word vectors.",
            "referenceCount": 25,
            "citationCount": 1537,
            "influentialCitationCount": 157,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-08-01",
            "journal": {
                "name": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zhou2016AttentionBasedBL,\n author = {P. Zhou and Wei Shi and Jun Tian and Zhenyu Qi and B. Li and Hongwei Hao and Bo Xu},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n journal = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},\n title = {Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b7aee9dfb027d6061c6a653684c0fa9a9bba750d",
            "@type": "ScholarlyArticle",
            "paperId": "b7aee9dfb027d6061c6a653684c0fa9a9bba750d",
            "corpusId": 7147309,
            "url": "https://www.semanticscholar.org/paper/b7aee9dfb027d6061c6a653684c0fa9a9bba750d",
            "title": "Sequence Level Training with Recurrent Neural Networks",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2015,
            "externalIds": {
                "ArXiv": "1511.06732",
                "DBLP": "journals/corr/RanzatoCAZ15",
                "MAG": "2963248296",
                "CorpusId": 7147309
            },
            "abstract": "Many natural language processing applications use language models to generate text. These models are typically trained to predict the next word in a sequence, given the previous words and some context such as an image. However, at test time the model is expected to generate the entire sequence from scratch. This discrepancy makes generation brittle, as errors may accumulate along the way. We address this issue by proposing a novel sequence level training algorithm that directly optimizes the metric used at test time, such as BLEU or ROUGE. On three different tasks, our approach outperforms several strong baselines for greedy generation. The method is also competitive when these baselines employ beam search, while being several times faster.",
            "referenceCount": 36,
            "citationCount": 1482,
            "influentialCitationCount": 211,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-11-20",
            "journal": {
                "name": "CoRR",
                "volume": "abs/1511.06732"
            },
            "citationStyles": {
                "bibtex": "@Article{Ranzato2015SequenceLT,\n author = {Marc'Aurelio Ranzato and S. Chopra and Michael Auli and Wojciech Zaremba},\n booktitle = {International Conference on Learning Representations},\n journal = {CoRR},\n title = {Sequence Level Training with Recurrent Neural Networks},\n volume = {abs/1511.06732},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:bf55591e09b58ea9ce8d66110d6d3000ee804bdd",
            "@type": "ScholarlyArticle",
            "paperId": "bf55591e09b58ea9ce8d66110d6d3000ee804bdd",
            "corpusId": 3120635,
            "url": "https://www.semanticscholar.org/paper/bf55591e09b58ea9ce8d66110d6d3000ee804bdd",
            "title": "Image Captioning with Semantic Attention",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2302086703",
                "ArXiv": "1603.03925",
                "DBLP": "journals/corr/YouJWFL16",
                "DOI": "10.1109/CVPR.2016.503",
                "CorpusId": 3120635
            },
            "abstract": "Automatically generating a natural language description of an image has attracted interests recently both because of its importance in practical applications and because it connects two major artificial intelligence fields: computer vision and natural language processing. Existing approaches are either top-down, which start from a gist of an image and convert it into words, or bottom-up, which come up with words describing various aspects of an image and then combine them. In this paper, we propose a new algorithm that combines both approaches through a model of semantic attention. Our algorithm learns to selectively attend to semantic concept proposals and fuse them into hidden states and outputs of recurrent neural networks. The selection and fusion form a feedback connecting the top-down and bottom-up computation. We evaluate our algorithm on two public benchmarks: Microsoft COCO and Flickr30K. Experimental results show that our algorithm significantly outperforms the state-of-the-art approaches consistently across different evaluation metrics.",
            "referenceCount": 39,
            "citationCount": 1499,
            "influentialCitationCount": 136,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1603.03925",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-03-12",
            "journal": {
                "name": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{You2016ImageCW,\n author = {Quanzeng You and Hailin Jin and Zhaowen Wang and Chen Fang and Jiebo Luo},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {4651-4659},\n title = {Image Captioning with Semantic Attention},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e28ab7c3b994dd4e30baac1eb67c7f87e40c2b7b",
            "@type": "ScholarlyArticle",
            "paperId": "e28ab7c3b994dd4e30baac1eb67c7f87e40c2b7b",
            "corpusId": 16017905,
            "url": "https://www.semanticscholar.org/paper/e28ab7c3b994dd4e30baac1eb67c7f87e40c2b7b",
            "title": "Recurrent Neural Network for Text Classification with Multi-Task Learning",
            "venue": "International Joint Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:67f7f831-711a-43c8-8785-1e09005359b5",
                "name": "International Joint Conference on Artificial Intelligence",
                "alternate_names": [
                    "Int Jt Conf Artif Intell",
                    "IJCAI"
                ],
                "issn": null,
                "url": "http://www.ijcai.org/"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2950246426",
                "DBLP": "conf/ijcai/LiuQH16",
                "ArXiv": "1605.05101",
                "CorpusId": 16017905
            },
            "abstract": "Neural network based methods have obtained great progress on a variety of natural language processing tasks. However, in most previous works, the models are learned based on single-task supervised objectives, which often suffer from insufficient training data. In this paper, we use the multi-task learning framework to jointly learn across multiple related tasks. Based on recurrent neural network, we propose three different mechanisms of sharing information to model text with task-specific and shared layers. The entire network is trained jointly on all these tasks. Experiments on four benchmark text classification tasks show that our proposed models can improve the performance of a task with the help of other related tasks.",
            "referenceCount": 39,
            "citationCount": 1118,
            "influentialCitationCount": 151,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-05-17",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1605.05101"
            },
            "citationStyles": {
                "bibtex": "@Article{Liu2016RecurrentNN,\n author = {Pengfei Liu and Xipeng Qiu and Xuanjing Huang},\n booktitle = {International Joint Conference on Artificial Intelligence},\n journal = {ArXiv},\n title = {Recurrent Neural Network for Text Classification with Multi-Task Learning},\n volume = {abs/1605.05101},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5e077918a979537f27b9a0820672f23a434a98ee",
            "@type": "ScholarlyArticle",
            "paperId": "5e077918a979537f27b9a0820672f23a434a98ee",
            "corpusId": 564263,
            "url": "https://www.semanticscholar.org/paper/5e077918a979537f27b9a0820672f23a434a98ee",
            "title": "Mayo clinical Text Analysis and Knowledge Extraction System (cTAKES): architecture, component evaluation and applications",
            "venue": "J. Am. Medical Informatics Assoc.",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2010,
            "externalIds": {
                "MAG": "2146089916",
                "DBLP": "journals/jamia/SavovaMOZSSC10",
                "DOI": "10.1136/jamia.2009.001560",
                "CorpusId": 564263,
                "PubMed": "20819853"
            },
            "abstract": "We aim to build and evaluate an open-source natural language processing system for information extraction from electronic medical record clinical free-text. We describe and evaluate our system, the clinical Text Analysis and Knowledge Extraction System (cTAKES), released open-source at http://www.ohnlp.org. The cTAKES builds on existing open-source technologies-the Unstructured Information Management Architecture framework and OpenNLP natural language processing toolkit. Its components, specifically trained for the clinical domain, create rich linguistic and semantic annotations. Performance of individual components: sentence boundary detector accuracy=0.949; tokenizer accuracy=0.949; part-of-speech tagger accuracy=0.936; shallow parser F-score=0.924; named entity recognizer and system-level evaluation F-score=0.715 for exact and 0.824 for overlapping spans, and accuracy for concept mapping, negation, and status attributes for exact and overlapping spans of 0.957, 0.943, 0.859, and 0.580, 0.939, and 0.839, respectively. Overall performance is discussed against five applications. The cTAKES annotations are the foundation for methods and modules for higher-level semantic processing of clinical free-text.",
            "referenceCount": 71,
            "citationCount": 1766,
            "influentialCitationCount": 75,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://academic.oup.com/jamia/article-pdf/17/5/507/5940551/17-5-507.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Study"
            ],
            "publicationDate": "2010-09-01",
            "journal": {
                "name": "Journal of the American Medical Informatics Association : JAMIA",
                "volume": "17 5"
            },
            "citationStyles": {
                "bibtex": "@Article{Savova2010MayoCT,\n author = {G. Savova and James J. Masanz and P. Ogren and Jiaping Zheng and S. Sohn and K. Schuler and C. Chute},\n booktitle = {J. Am. Medical Informatics Assoc.},\n journal = {Journal of the American Medical Informatics Association : JAMIA},\n pages = {\n          507-13\n        },\n title = {Mayo clinical Text Analysis and Knowledge Extraction System (cTAKES): architecture, component evaluation and applications},\n volume = {17 5},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:85e07116316e686bf787114ba10ca60f4ea7c5b2",
            "@type": "ScholarlyArticle",
            "paperId": "85e07116316e686bf787114ba10ca60f4ea7c5b2",
            "corpusId": 58004692,
            "url": "https://www.semanticscholar.org/paper/85e07116316e686bf787114ba10ca60f4ea7c5b2",
            "title": "Passage Re-ranking with BERT",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/corr/abs-1901-04085",
                "MAG": "2909544278",
                "ArXiv": "1901.04085",
                "CorpusId": 58004692
            },
            "abstract": "Recently, neural models pretrained on a language modeling task, such as ELMo (Peters et al., 2017), OpenAI GPT (Radford et al., 2018), and BERT (Devlin et al., 2018), have achieved impressive results on various natural language processing tasks such as question-answering and natural language inference. In this paper, we describe a simple re-implementation of BERT for query-based passage re-ranking. Our system is the state of the art on the TREC-CAR dataset and the top entry in the leaderboard of the MS MARCO passage retrieval task, outperforming the previous state of the art by 27% (relative) in MRR@10. The code to reproduce our results is available at this https URL",
            "referenceCount": 23,
            "citationCount": 797,
            "influentialCitationCount": 133,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-01-13",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1901.04085"
            },
            "citationStyles": {
                "bibtex": "@Article{Nogueira2019PassageRW,\n author = {Rodrigo Nogueira and Kyunghyun Cho},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Passage Re-ranking with BERT},\n volume = {abs/1901.04085},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1406f6b5ed4034b72ed2dccc3fcfa4c5c0810924",
            "@type": "ScholarlyArticle",
            "paperId": "1406f6b5ed4034b72ed2dccc3fcfa4c5c0810924",
            "corpusId": 14187105,
            "url": "https://www.semanticscholar.org/paper/1406f6b5ed4034b72ed2dccc3fcfa4c5c0810924",
            "title": "Effective mapping of biomedical text to the UMLS Metathesaurus: the MetaMap program",
            "venue": "American Medical Informatics Association Annual Symposium",
            "publicationVenue": {
                "id": "urn:research:c9e16b3c-2fdf-4b4c-82bf-5cdf3d3435b7",
                "name": "American Medical Informatics Association Annual Symposium",
                "alternate_names": [
                    "Conference of American Medical Informatics Association",
                    "Am Med Informatics Assoc Annu Symp",
                    "Conf Am Med Informatics Assoc",
                    "AMIA"
                ],
                "issn": null,
                "url": "https://knowledge.amia.org/"
            },
            "year": 2001,
            "externalIds": {
                "DBLP": "conf/amia/Aronson01",
                "MAG": "76971170",
                "CorpusId": 14187105,
                "PubMed": "11825149"
            },
            "abstract": "The UMLS Metathesaurus, the largest thesaurus in the biomedical domain, provides a representation of biomedical knowledge consisting of concepts classified by semantic type and both hierarchical and non-hierarchical relationships among the concepts. This knowledge has proved useful for many applications including decision support systems, management of patient records, information retrieval (IR) and data mining. Gaining effective access to the knowledge is critical to the success of these applications. This paper describes MetaMap, a program developed at the National Library of Medicine (NLM) to map biomedical text to the Metathesaurus or, equivalently, to discover Metathesaurus concepts referred to in text. MetaMap uses a knowledge intensive approach based on symbolic, natural language processing (NLP) and computational linguistic techniques. Besides being applied for both IR and data mining applications, MetaMap is one of the foundations of NLM's Indexing Initiative System which is being applied to both semi-automatic and fully automatic indexing of the biomedical literature at the library.",
            "referenceCount": 35,
            "citationCount": 2168,
            "influentialCitationCount": 311,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": "Proceedings. AMIA Symposium",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Aronson2001EffectiveMO,\n author = {A. Aronson},\n booktitle = {American Medical Informatics Association Annual Symposium},\n journal = {Proceedings. AMIA Symposium},\n pages = {\n          17-21\n        },\n title = {Effective mapping of biomedical text to the UMLS Metathesaurus: the MetaMap program},\n year = {2001}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a99e5bf7273da127be1fcdf4f3cb911f17550304",
            "@type": "ScholarlyArticle",
            "paperId": "a99e5bf7273da127be1fcdf4f3cb911f17550304",
            "corpusId": 653762,
            "url": "https://www.semanticscholar.org/paper/a99e5bf7273da127be1fcdf4f3cb911f17550304",
            "title": "Probabilistic Latent Semantic Analysis",
            "venue": "Conference on Uncertainty in Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:f9af8000-42f8-410d-a622-e8811e41660a",
                "name": "Conference on Uncertainty in Artificial Intelligence",
                "alternate_names": [
                    "Uncertainty in Artificial Intelligence",
                    "UAI",
                    "Conf Uncertain Artif Intell",
                    "Uncertain Artif Intell"
                ],
                "issn": null,
                "url": "http://www.auai.org/"
            },
            "year": 1999,
            "externalIds": {
                "DBLP": "conf/uai/Hofmann99",
                "MAG": "2953062473",
                "ArXiv": "1301.6705",
                "CorpusId": 653762
            },
            "abstract": "Probabilistic Latent Semantic Analysis is a novel statistical technique for the analysis of two-mode and co-occurrence data, which has applications in information retrieval and filtering, natural language processing, machine learning from text, and in related areas. Compared to standard Latent Semantic Analysis which stems from linear algebra and performs a Singular Value Decomposition of co-occurrence tables, the proposed method is based on a mixture decomposition derived from a latent class model. This results in a more principled approach which has a solid foundation in statistics. In order to avoid overfitting, we propose a widely applicable generalization of maximum likelihood model fitting by tempered EM. Our approach yields substantial and consistent improvements over Latent Semantic Analysis in a number of experiments.",
            "referenceCount": 15,
            "citationCount": 2744,
            "influentialCitationCount": 330,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1999-07-30",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Hofmann1999ProbabilisticLS,\n author = {Thomas Hofmann},\n booktitle = {Conference on Uncertainty in Artificial Intelligence},\n pages = {289-296},\n title = {Probabilistic Latent Semantic Analysis},\n year = {1999}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:44e1dd74f0446ec91221189ad3a65edb1a0208fe",
            "@type": "ScholarlyArticle",
            "paperId": "44e1dd74f0446ec91221189ad3a65edb1a0208fe",
            "corpusId": 205572964,
            "url": "https://www.semanticscholar.org/paper/44e1dd74f0446ec91221189ad3a65edb1a0208fe",
            "title": "A guide to deep learning in healthcare",
            "venue": "Nature Network Boston",
            "publicationVenue": {
                "id": "urn:research:9e995b6d-f30b-4ab4-a13b-3dc2cc992f47",
                "name": "Nature Network Boston",
                "alternate_names": [
                    "Nat Netw Boston",
                    "Nat Med",
                    "Nature Medicine"
                ],
                "issn": "1744-7933",
                "url": "https://www.nature.com/nature/articles?code=archive_news"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2905810301",
                "DOI": "10.1038/s41591-018-0316-z",
                "CorpusId": 205572964,
                "PubMed": "30617335"
            },
            "abstract": null,
            "referenceCount": 65,
            "citationCount": 1773,
            "influentialCitationCount": 41,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2019-01-01",
            "journal": {
                "name": "Nature Medicine",
                "volume": "25"
            },
            "citationStyles": {
                "bibtex": "@Article{Esteva2019AGT,\n author = {A. Esteva and Alexandre Robicquet and Bharath Ramsundar and Volodymyr Kuleshov and M. DePristo and Katherine Chou and Claire Cui and Greg S. Corrado and S. Thrun and J. Dean},\n booktitle = {Nature Network Boston},\n journal = {Nature Medicine},\n pages = {24 - 29},\n title = {A guide to deep learning in healthcare},\n volume = {25},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:80cf2a6af4200ecfca1c18fc89de16148f1cd4bf",
            "@type": "ScholarlyArticle",
            "paperId": "80cf2a6af4200ecfca1c18fc89de16148f1cd4bf",
            "corpusId": 201670719,
            "url": "https://www.semanticscholar.org/paper/80cf2a6af4200ecfca1c18fc89de16148f1cd4bf",
            "title": "Patient Knowledge Distillation for BERT Model Compression",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "publicationVenue": {
                "id": "urn:research:41bf9ed3-85b3-4c90-b015-150e31690253",
                "name": "Conference on Empirical Methods in Natural Language Processing",
                "alternate_names": [
                    "Empir Method Nat Lang Process",
                    "Empirical Methods in Natural Language Processing",
                    "Conf Empir Method Nat Lang Process",
                    "EMNLP"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/emnlp"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "conf/emnlp/SunCGL19",
                "MAG": "2969515962",
                "ACL": "D19-1441",
                "ArXiv": "1908.09355",
                "DOI": "10.18653/v1/D19-1441",
                "CorpusId": 201670719
            },
            "abstract": "Pre-trained language models such as BERT have proven to be highly effective for natural language processing (NLP) tasks. However, the high demand for computing resources in training such models hinders their application in practice. In order to alleviate this resource hunger in large-scale model training, we propose a Patient Knowledge Distillation approach to compress an original large model (teacher) into an equally-effective lightweight shallow network (student). Different from previous knowledge distillation methods, which only use the output from the last layer of the teacher network for distillation, our student model patiently learns from multiple intermediate layers of the teacher model for incremental knowledge extraction, following two strategies: (i) PKD-Last: learning from the last k layers; and (ii) PKD-Skip: learning from every k layers. These two patient distillation schemes enable the exploitation of rich information in the teacher\u2019s hidden layers, and encourage the student model to patiently learn from and imitate the teacher through a multi-layer distillation process. Empirically, this translates into improved results on multiple NLP tasks with a significant gain in training efficiency, without sacrificing model accuracy.",
            "referenceCount": 40,
            "citationCount": 623,
            "influentialCitationCount": 133,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/D19-1441.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-08-25",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Sun2019PatientKD,\n author = {S. Sun and Yu Cheng and Zhe Gan and Jingjing Liu},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {4322-4331},\n title = {Patient Knowledge Distillation for BERT Model Compression},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9fa8d73e572c3ca824a04a5f551b602a17831bc5",
            "@type": "ScholarlyArticle",
            "paperId": "9fa8d73e572c3ca824a04a5f551b602a17831bc5",
            "corpusId": 15978939,
            "url": "https://www.semanticscholar.org/paper/9fa8d73e572c3ca824a04a5f551b602a17831bc5",
            "title": "Domain Adaptation with Structural Correspondence Learning",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "publicationVenue": {
                "id": "urn:research:41bf9ed3-85b3-4c90-b015-150e31690253",
                "name": "Conference on Empirical Methods in Natural Language Processing",
                "alternate_names": [
                    "Empir Method Nat Lang Process",
                    "Empirical Methods in Natural Language Processing",
                    "Conf Empir Method Nat Lang Process",
                    "EMNLP"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/emnlp"
            },
            "year": 2006,
            "externalIds": {
                "ACL": "W06-1615",
                "DBLP": "conf/emnlp/BlitzerMP06",
                "MAG": "2158108973",
                "DOI": "10.3115/1610075.1610094",
                "CorpusId": 15978939
            },
            "abstract": "Discriminative learning methods are widely used in natural language processing. These methods work best when their training and test data are drawn from the same distribution. For many NLP tasks, however, we are confronted with new domains in which labeled data is scarce or non-existent. In such cases, we seek to adapt existing models from a resource-rich source domain to a resource-poor target domain. We introduce structural correspondence learning to automatically induce correspondences among features from different domains. We test our technique on part of speech tagging and show performance gains for varying amounts of source and target training data, as well as improvements in target domain parsing accuracy using our improved tagger.",
            "referenceCount": 26,
            "citationCount": 1642,
            "influentialCitationCount": 194,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.5555/1610075.1610094",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2006-07-22",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Blitzer2006DomainAW,\n author = {John Blitzer and Ryan T. McDonald and Fernando C Pereira},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {120-128},\n title = {Domain Adaptation with Structural Correspondence Learning},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:402c7b1da9b27db172e64c6eb0c557cdd7bfdf62",
            "@type": "ScholarlyArticle",
            "paperId": "402c7b1da9b27db172e64c6eb0c557cdd7bfdf62",
            "corpusId": 42596294,
            "url": "https://www.semanticscholar.org/paper/402c7b1da9b27db172e64c6eb0c557cdd7bfdf62",
            "title": "Toward a mechanistic psychology of dialogue",
            "venue": "Behavioral and Brain Sciences",
            "publicationVenue": {
                "id": "urn:research:f51399af-b5cb-4819-9d81-57ec1d17ebf0",
                "name": "Behavioral and Brain Sciences",
                "alternate_names": [
                    "Behav Brain Sci"
                ],
                "issn": "0140-525X",
                "url": "http://www.bbsonline.org/"
            },
            "year": 2004,
            "externalIds": {
                "MAG": "2159398820",
                "DOI": "10.1017/S0140525X04000056",
                "CorpusId": 42596294,
                "PubMed": "15595235"
            },
            "abstract": "Traditional mechanistic accounts of language processing derive almost entirely from the study of monologue. Yet, the most natural and basic form of language use is dialogue. As a result, these accounts may only offer limited theories of the mechanisms that underlie language processing in general. We propose a mechanistic account of dialogue, the interactive alignment account, and use it to derive a number of predictions about basic language processes. The account assumes that, in dialogue, the linguistic representations employed by the interlocutors become aligned at many levels, as a result of a largely automatic process. This process greatly simplifies production and comprehension in dialogue. After considering the evidence for the interactive alignment model, we concentrate on three aspects of processing that follow from it. It makes use of a simple interactive inference mechanism, enables the development of local dialogue routines that greatly simplify language processing, and explains the origins of self-monitoring in production. We consider the need for a grammatical framework that is designed to deal with language in dialogue rather than monologue, and discuss a range of implications of the account.",
            "referenceCount": 357,
            "citationCount": 2329,
            "influentialCitationCount": 196,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.pure.ed.ac.uk/ws/files/11823730/Toward_a_mechanistic_psychology_of_dialogue.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2004-04-01",
            "journal": {
                "name": "Behavioral and Brain Sciences",
                "volume": "27"
            },
            "citationStyles": {
                "bibtex": "@Article{Pickering2004TowardAM,\n author = {M. Pickering and S. Garrod},\n booktitle = {Behavioral and Brain Sciences},\n journal = {Behavioral and Brain Sciences},\n pages = {169 - 190},\n title = {Toward a mechanistic psychology of dialogue},\n volume = {27},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:031e4e43aaffd7a479738dcea69a2d5be7957aa3",
            "@type": "ScholarlyArticle",
            "paperId": "031e4e43aaffd7a479738dcea69a2d5be7957aa3",
            "corpusId": 125977708,
            "url": "https://www.semanticscholar.org/paper/031e4e43aaffd7a479738dcea69a2d5be7957aa3",
            "title": "ERNIE: Enhanced Representation through Knowledge Integration",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/corr/abs-1904-09223",
                "ArXiv": "1904.09223",
                "MAG": "2938830017",
                "CorpusId": 125977708
            },
            "abstract": "We present a novel language representation model enhanced by knowledge called ERNIE (Enhanced Representation through kNowledge IntEgration). Inspired by the masking strategy of BERT, ERNIE is designed to learn language representation enhanced by knowledge masking strategies, which includes entity-level masking and phrase-level masking. Entity-level strategy masks entities which are usually composed of multiple words.Phrase-level strategy masks the whole phrase which is composed of several words standing together as a conceptual unit.Experimental results show that ERNIE outperforms other baseline methods, achieving new state-of-the-art results on five Chinese natural language processing tasks including natural language inference, semantic similarity, named entity recognition, sentiment analysis and question answering. We also demonstrate that ERNIE has more powerful knowledge inference capacity on a cloze test.",
            "referenceCount": 23,
            "citationCount": 708,
            "influentialCitationCount": 113,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-04-19",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1904.09223"
            },
            "citationStyles": {
                "bibtex": "@Article{Sun2019ERNIEER,\n author = {Yu Sun and Shuohuan Wang and Yukun Li and Shikun Feng and Xuyi Chen and Han Zhang and Xin Tian and Danxiang Zhu and Hao Tian and Hua Wu},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {ERNIE: Enhanced Representation through Knowledge Integration},\n volume = {abs/1904.09223},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5194b668c67aa83c037e71599a087f63c98eb713",
            "@type": "ScholarlyArticle",
            "paperId": "5194b668c67aa83c037e71599a087f63c98eb713",
            "corpusId": 915058,
            "url": "https://www.semanticscholar.org/paper/5194b668c67aa83c037e71599a087f63c98eb713",
            "title": "A sequential algorithm for training text classifiers",
            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "publicationVenue": {
                "id": "urn:research:8dce23a9-44e0-4381-a39e-2acc1edff700",
                "name": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                "alternate_names": [
                    "International ACM SIGIR Conference on Research and Development in Information Retrieval",
                    "Int ACM SIGIR Conf Res Dev Inf Retr",
                    "SIGIR",
                    "Annu Int ACM SIGIR Conf Res Dev Inf Retr"
                ],
                "issn": null,
                "url": "http://www.acm.org/sigir/"
            },
            "year": 1994,
            "externalIds": {
                "MAG": "2085989833",
                "DBLP": "journals/corr/LewisG94",
                "DOI": "10.1007/978-1-4471-2099-5_1",
                "CorpusId": 915058
            },
            "abstract": null,
            "referenceCount": 32,
            "citationCount": 2645,
            "influentialCitationCount": 210,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/cmp-lg/9407020",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1994-07-24",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Lewis1994ASA,\n author = {D. Lewis and W. Gale},\n booktitle = {Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},\n pages = {3-12},\n title = {A sequential algorithm for training text classifiers},\n year = {1994}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2bc1daaba330f4ea8a4f951d5dcd40c39bef5a8a",
            "@type": "ScholarlyArticle",
            "paperId": "2bc1daaba330f4ea8a4f951d5dcd40c39bef5a8a",
            "corpusId": 5319073,
            "url": "https://www.semanticscholar.org/paper/2bc1daaba330f4ea8a4f951d5dcd40c39bef5a8a",
            "title": "Constraint Processing",
            "venue": "Lecture Notes in Computer Science",
            "publicationVenue": {
                "id": "urn:research:2f5d0e8a-faad-4f10-b323-2b2e3c439a78",
                "name": "Lecture Notes in Computer Science",
                "alternate_names": [
                    "LNCS",
                    "Transactions on Computational Systems Biology",
                    "Trans Comput Syst Biology",
                    "Lect Note Comput Sci"
                ],
                "issn": "0302-9743",
                "url": "http://www.springer.com/lncs"
            },
            "year": 1995,
            "externalIds": {
                "DBLP": "conf/cp/1995s",
                "MAG": "2337098149",
                "DOI": "10.1007/3-540-59479-5",
                "CorpusId": 5319073
            },
            "abstract": null,
            "referenceCount": 200,
            "citationCount": 956,
            "influentialCitationCount": 109,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Dechter1995ConstraintP,\n author = {R. Dechter},\n booktitle = {Lecture Notes in Computer Science},\n pages = {I-XX, 1-481},\n title = {Constraint Processing},\n year = {1995}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:95b3d6b9d50bfd4b0a77a096269762b28e218346",
            "@type": "ScholarlyArticle",
            "paperId": "95b3d6b9d50bfd4b0a77a096269762b28e218346",
            "corpusId": 19426100,
            "url": "https://www.semanticscholar.org/paper/95b3d6b9d50bfd4b0a77a096269762b28e218346",
            "title": "Advances in Pre-Training Distributed Word Representations",
            "venue": "International Conference on Language Resources and Evaluation",
            "publicationVenue": {
                "id": "urn:research:7474c4a0-75d9-4105-9809-8e7d5201c5e1",
                "name": "International Conference on Language Resources and Evaluation",
                "alternate_names": [
                    "LREC",
                    "Int Conf Lang Resour Evaluation"
                ],
                "issn": null,
                "url": "http://www.lrec-conf.org/"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "journals/corr/abs-1712-09405",
                "ACL": "L18-1008",
                "MAG": "2962772361",
                "ArXiv": "1712.09405",
                "CorpusId": 19426100
            },
            "abstract": "Many Natural Language Processing applications nowadays rely on pre-trained word representations estimated from large text corpora such as news collections, Wikipedia and Web Crawl. In this paper, we show how to train high-quality word vector representations by using a combination of known tricks that are however rarely used together. The main result of our work is the new set of publicly available pre-trained models that outperform the current state of the art by a large margin on a number of tasks.",
            "referenceCount": 18,
            "citationCount": 1089,
            "influentialCitationCount": 93,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-12-26",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1712.09405"
            },
            "citationStyles": {
                "bibtex": "@Article{Mikolov2017AdvancesIP,\n author = {Tomas Mikolov and Edouard Grave and Piotr Bojanowski and Christian Puhrsch and Armand Joulin},\n booktitle = {International Conference on Language Resources and Evaluation},\n journal = {ArXiv},\n title = {Advances in Pre-Training Distributed Word Representations},\n volume = {abs/1712.09405},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8a734b3964aafb8d50a96eee80c259e6f9b82e4d",
            "@type": "ScholarlyArticle",
            "paperId": "8a734b3964aafb8d50a96eee80c259e6f9b82e4d",
            "corpusId": 4397499,
            "url": "https://www.semanticscholar.org/paper/8a734b3964aafb8d50a96eee80c259e6f9b82e4d",
            "title": "The fifth 'CHiME' Speech Separation and Recognition Challenge: Dataset, task and baselines",
            "venue": "Interspeech",
            "publicationVenue": {
                "id": "urn:research:af90489e-312f-4514-bea2-bcb399cb8ece",
                "name": "Interspeech",
                "alternate_names": [
                    "Conf Int Speech Commun Assoc",
                    "INTERSPEECH",
                    "Conference of the International Speech Communication Association"
                ],
                "issn": "2308-457X",
                "url": "https://www.isca-speech.org/iscaweb/index.php/conferences/interspeech"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "conf/interspeech/BarkerWVT18",
                "ArXiv": "1803.10609",
                "MAG": "2884797218",
                "DOI": "10.21437/Interspeech.2018-1768",
                "CorpusId": 4397499
            },
            "abstract": "The CHiME challenge series aims to advance robust automatic speech recognition (ASR) technology by promoting research at the interface of speech and language processing, signal processing , and machine learning. This paper introduces the 5th CHiME Challenge, which considers the task of distant multi-microphone conversational ASR in real home environments. Speech material was elicited using a dinner party scenario with efforts taken to capture data that is representative of natural conversational speech and recorded by 6 Kinect microphone arrays and 4 binaural microphone pairs. The challenge features a single-array track and a multiple-array track and, for each track, distinct rankings will be produced for systems focusing on robustness with respect to distant-microphone capture vs. systems attempting to address all aspects of the task including conversational language modeling. We discuss the rationale for the challenge and provide a detailed description of the data collection procedure, the task, and the baseline systems for array synchronization, speech enhancement, and conventional and end-to-end ASR.",
            "referenceCount": 40,
            "citationCount": 767,
            "influentialCitationCount": 101,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1803.10609",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-03-28",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Barker2018TheF,\n author = {J. Barker and Shinji Watanabe and E. Vincent and J. Trmal},\n booktitle = {Interspeech},\n pages = {1561-1565},\n title = {The fifth 'CHiME' Speech Separation and Recognition Challenge: Dataset, task and baselines},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0767c2e1c4148fbb18103aa3ae15e5949aed5e46",
            "@type": "ScholarlyArticle",
            "paperId": "0767c2e1c4148fbb18103aa3ae15e5949aed5e46",
            "corpusId": 262269888,
            "url": "https://www.semanticscholar.org/paper/0767c2e1c4148fbb18103aa3ae15e5949aed5e46",
            "title": "Automated Planning: Theory and Practice",
            "venue": "K\u00fcnstliche Intell.",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2007,
            "externalIds": {
                "DBLP": "journals/ki/Edelkamp07a",
                "MAG": "279050352",
                "CorpusId": 262269888
            },
            "abstract": "A translation apparatus is provided which comprises: an inputting section for inputting a source document in a natural language; a layout analyzing section for analyzing layout information including cascade information, itemization information, numbered itemization information, labeled itemization information and separator line information in the source document inputted by the inputting section and specifying a translation range on the basis of the layout information; a translation processing section for translating a source document text in the specified translation range into a second language; and an outputting section for outputting a translated text provided by the translation processing section.",
            "referenceCount": 0,
            "citationCount": 1714,
            "influentialCitationCount": 97,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": "K\u00fcnstliche Intell.",
                "volume": "21"
            },
            "citationStyles": {
                "bibtex": "@Article{Edelkamp2007AutomatedPT,\n author = {Stefan Edelkamp},\n booktitle = {K\u00fcnstliche Intell.},\n journal = {K\u00fcnstliche Intell.},\n pages = {42-43},\n title = {Automated Planning: Theory and Practice},\n volume = {21},\n year = {2007}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c7870b78c57e0bd2e9fb6907c0702e28eb87e239",
            "@type": "ScholarlyArticle",
            "paperId": "c7870b78c57e0bd2e9fb6907c0702e28eb87e239",
            "corpusId": 342976,
            "url": "https://www.semanticscholar.org/paper/c7870b78c57e0bd2e9fb6907c0702e28eb87e239",
            "title": "An Introduction to Conditional Random Fields",
            "venue": "Found. Trends Mach. Learn.",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2010,
            "externalIds": {
                "MAG": "2129999749",
                "ArXiv": "1011.4088",
                "DBLP": "journals/ftml/SuttonM12",
                "DOI": "10.1561/2200000013",
                "CorpusId": 342976
            },
            "abstract": "Many tasks involve predicting a large number of variables that depend on each other as well as on other observed variables. Structured prediction methods are essentially a combination of classification and graphical modeling. They combine the ability of graphical models to compactly model multivariate data with the ability of classification methods to perform prediction using large sets of input features. This survey describes conditional random fields, a popular probabilistic method for structured prediction. CRFs have seen wide application in many areas, including natural language processing, computer vision, and bioinformatics. We describe methods for inference and parameter estimation for CRFs, including practical issues for implementing large-scale CRFs. We do not assume previous knowledge of graphical modeling, so this survey is intended to be useful to practitioners in a wide variety of fields.",
            "referenceCount": 169,
            "citationCount": 1082,
            "influentialCitationCount": 100,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1011.4088",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2010-11-17",
            "journal": {
                "name": "Found. Trends Mach. Learn.",
                "volume": "4"
            },
            "citationStyles": {
                "bibtex": "@Article{Sutton2010AnIT,\n author = {Charles Sutton and A. McCallum},\n booktitle = {Found. Trends Mach. Learn.},\n journal = {Found. Trends Mach. Learn.},\n pages = {267-373},\n title = {An Introduction to Conditional Random Fields},\n volume = {4},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d0c882bcae6531fa13e75bcc5c297b9985f207f7",
            "@type": "ScholarlyArticle",
            "paperId": "d0c882bcae6531fa13e75bcc5c297b9985f207f7",
            "corpusId": 60455,
            "url": "https://www.semanticscholar.org/paper/d0c882bcae6531fa13e75bcc5c297b9985f207f7",
            "title": "Web mining research: a survey",
            "venue": "SKDD",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2000,
            "externalIds": {
                "MAG": "2950166996",
                "DBLP": "journals/corr/cs-LG-0011033",
                "ArXiv": "cs/0011033",
                "DOI": "10.1145/360402.360406",
                "CorpusId": 60455
            },
            "abstract": "With the huge amount of information available online, the World Wide Web is a fertile area for data mining research. The Web mining research is at the cross road of research from several research communities, such as database, information retrieval, and within AI, especially the sub-areas of machine learning and natural language processing. However, there is a lot of confusions when comparing research efforts from different point of views. In this paper, we survey the research in the area of Web mining, point out some confusions regarded the usage of the term Web mining and suggest three Web mining categories. Then we situate some of the research with respect to these three categories. We also explore the connection between the Web mining categories and the related agent paradigm. For the survey, we focus on representation issues, on the process, on the learning algorithm, and on the application of the recent works as the criteria. We conclude the paper with some research issues.",
            "referenceCount": 132,
            "citationCount": 1879,
            "influentialCitationCount": 91,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://lirias.kuleuven.be/bitstream/123456789/132847/1/33042.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2000-06-01",
            "journal": {
                "name": "SIGKDD Explor.",
                "volume": "2"
            },
            "citationStyles": {
                "bibtex": "@Article{Kosala2000WebMR,\n author = {R. Kosala and H. Blockeel},\n booktitle = {SKDD},\n journal = {SIGKDD Explor.},\n pages = {1-15},\n title = {Web mining research: a survey},\n volume = {2},\n year = {2000}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:62f74d3aaf9e86633e4d88b04a6d04ca93e8b81e",
            "@type": "ScholarlyArticle",
            "paperId": "62f74d3aaf9e86633e4d88b04a6d04ca93e8b81e",
            "corpusId": 8289133,
            "url": "https://www.semanticscholar.org/paper/62f74d3aaf9e86633e4d88b04a6d04ca93e8b81e",
            "title": "Show and Tell: Lessons Learned from the 2015 MSCOCO Image Captioning Challenge",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "publicationVenue": {
                "id": "urn:research:25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "alternate_names": [
                    "IEEE Trans Pattern Anal Mach Intell"
                ],
                "issn": "0162-8828",
                "url": "http://www.computer.org/tpami/"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2463955103",
                "DBLP": "journals/corr/VinyalsTBE16",
                "ArXiv": "1609.06647",
                "DOI": "10.1109/TPAMI.2016.2587640",
                "CorpusId": 8289133,
                "PubMed": "28055847"
            },
            "abstract": "Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. Finally, given the recent surge of interest in this task, a competition was organized in 2015 using the newly released COCO dataset. We describe and analyze the various improvements we applied to our own baseline and show the resulting performance in the competition, which we won ex-aequo with a team from Microsoft Research.",
            "referenceCount": 55,
            "citationCount": 766,
            "influentialCitationCount": 104,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1609.06647",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-09-21",
            "journal": {
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "volume": "39"
            },
            "citationStyles": {
                "bibtex": "@Article{Vinyals2016ShowAT,\n author = {Oriol Vinyals and Alexander Toshev and Samy Bengio and D. Erhan},\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n pages = {652-663},\n title = {Show and Tell: Lessons Learned from the 2015 MSCOCO Image Captioning Challenge},\n volume = {39},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:13fe71da009484f240c46f14d9330e932f8de210",
            "@type": "ScholarlyArticle",
            "paperId": "13fe71da009484f240c46f14d9330e932f8de210",
            "corpusId": 6506243,
            "url": "https://www.semanticscholar.org/paper/13fe71da009484f240c46f14d9330e932f8de210",
            "title": "Long Short-Term Memory-Networks for Machine Reading",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "publicationVenue": {
                "id": "urn:research:41bf9ed3-85b3-4c90-b015-150e31690253",
                "name": "Conference on Empirical Methods in Natural Language Processing",
                "alternate_names": [
                    "Empir Method Nat Lang Process",
                    "Empirical Methods in Natural Language Processing",
                    "Conf Empir Method Nat Lang Process",
                    "EMNLP"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/emnlp"
            },
            "year": 2016,
            "externalIds": {
                "ACL": "D16-1053",
                "MAG": "2952191002",
                "DBLP": "conf/emnlp/0001DL16",
                "ArXiv": "1601.06733",
                "DOI": "10.18653/v1/D16-1053",
                "CorpusId": 6506243
            },
            "abstract": "In this paper we address the question of how to render sequence-level networks better at handling structured input. We propose a machine reading simulator which processes text incrementally from left to right and performs shallow reasoning with memory and attention. The reader extends the Long Short-Term Memory architecture with a memory network in place of a single memory cell. This enables adaptive memory usage during recurrence with neural attention, offering a way to weakly induce relations among tokens. The system is initially designed to process a single sequence but we also demonstrate how to integrate it with an encoder-decoder architecture. Experiments on language modeling, sentiment analysis, and natural language inference show that our model matches or outperforms the state of the art.",
            "referenceCount": 70,
            "citationCount": 972,
            "influentialCitationCount": 62,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/D16-1053.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-01-25",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1601.06733"
            },
            "citationStyles": {
                "bibtex": "@Article{Cheng2016LongSM,\n author = {Jianpeng Cheng and Li Dong and Mirella Lapata},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n journal = {ArXiv},\n title = {Long Short-Term Memory-Networks for Machine Reading},\n volume = {abs/1601.06733},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:bc8fa64625d9189f5801837e7b133e7fe3c581f7",
            "@type": "ScholarlyArticle",
            "paperId": "bc8fa64625d9189f5801837e7b133e7fe3c581f7",
            "corpusId": 9447219,
            "url": "https://www.semanticscholar.org/paper/bc8fa64625d9189f5801837e7b133e7fe3c581f7",
            "title": "Learned in Translation: Contextualized Word Vectors",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2017,
            "externalIds": {
                "ArXiv": "1708.00107",
                "DBLP": "conf/nips/McCannBXS17",
                "MAG": "2949553999",
                "CorpusId": 9447219
            },
            "abstract": "Computer vision has benefited from initializing multiple deep layers with weights pretrained on large supervised training sets like ImageNet. Natural language processing (NLP) typically sees initialization of only the lowest layer of deep models with pretrained word vectors. In this paper, we use a deep LSTM encoder from an attentional sequence-to-sequence model trained for machine translation (MT) to contextualize word vectors. We show that adding these context vectors (CoVe) improves performance over using only unsupervised word and character vectors on a wide variety of common NLP tasks: sentiment analysis (SST, IMDb), question classification (TREC), entailment (SNLI), and question answering (SQuAD). For fine-grained sentiment analysis and entailment, CoVe improves performance of our baseline models to the state of the art.",
            "referenceCount": 73,
            "citationCount": 843,
            "influentialCitationCount": 77,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-08-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{McCann2017LearnedIT,\n author = {Bryan McCann and James Bradbury and Caiming Xiong and R. Socher},\n booktitle = {Neural Information Processing Systems},\n pages = {6294-6305},\n title = {Learned in Translation: Contextualized Word Vectors},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:cd49acefc8d51e324aa562e5337e1c2aff067053",
            "@type": "ScholarlyArticle",
            "paperId": "cd49acefc8d51e324aa562e5337e1c2aff067053",
            "corpusId": 90063862,
            "url": "https://www.semanticscholar.org/paper/cd49acefc8d51e324aa562e5337e1c2aff067053",
            "title": "An Overview of Multi-task Learning",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2753709519",
                "DOI": "10.1093/NSR/NWX105",
                "CorpusId": 90063862
            },
            "abstract": "As a promising area in machine learning, multi-task learning (MTL) aims to improve the performance of multiple related learning tasks by leveraging useful information among them. In this paper, we give an overview of MTL by first giving a definition of MTL. Then several different settings of MTL are introduced, including multi-task supervised learning, multi-task unsupervised learning, multi-task semi-supervised learning, multi-task active learning, multi-task reinforcement learning, multi-task online learning and multi-task multi-view learning. For each setting, representative MTL models are presented. In order to speed up the learning process, parallel and distributed MTL models are introduced. Many areas, including computer vision, bioinformatics, health informatics, speech, natural language processing, web applications and ubiquitous computing, use MTL to improve the performance of the applications involved and some representative works are reviewed. Finally, recent theoretical analyses for MTL are presented.",
            "referenceCount": 136,
            "citationCount": 1311,
            "influentialCitationCount": 31,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://academic.oup.com/nsr/article-pdf/5/1/30/24164435/nwx105.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": "National Science Review",
                "volume": "5"
            },
            "citationStyles": {
                "bibtex": "@Article{Zhang2018AnOO,\n author = {Yu Zhang and Qiang Yang},\n journal = {National Science Review},\n pages = {30-43},\n title = {An Overview of Multi-task Learning},\n volume = {5},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:10f919b1a5161b560504c225cfb2d1b3a4768f80",
            "@type": "ScholarlyArticle",
            "paperId": "10f919b1a5161b560504c225cfb2d1b3a4768f80",
            "corpusId": 3779750,
            "url": "https://www.semanticscholar.org/paper/10f919b1a5161b560504c225cfb2d1b3a4768f80",
            "title": "Artificial intelligence in healthcare: past, present and future",
            "venue": "Stroke and vascular neurology",
            "publicationVenue": {
                "id": "urn:research:f46dbd50-8ba0-47aa-9649-1ba237101795",
                "name": "Stroke and vascular neurology",
                "alternate_names": [
                    "Stroke vasc neurol",
                    "Stroke and Vascular Neurology",
                    "Stroke Vasc Neurol"
                ],
                "issn": "2059-8696",
                "url": "https://svn.bmj.com/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2664267452",
                "PubMedCentral": "5829945",
                "DOI": "10.1136/svn-2017-000101",
                "CorpusId": 3779750,
                "PubMed": "29507784"
            },
            "abstract": "Artificial intelligence (AI) aims to mimic human cognitive functions. It is bringing a paradigm shift to healthcare, powered by increasing availability of healthcare data and rapid progress of analytics techniques. We survey the current status of AI applications in healthcare and discuss its future. AI can be applied to various types of healthcare data (structured and unstructured). Popular AI techniques include machine learning methods for structured data, such as the classical support vector machine and neural network, and the modern deep learning, as well as natural language processing for unstructured data. Major disease areas that use AI tools include cancer, neurology and cardiology. We then review in more details the AI applications in stroke, in the three major areas of early detection and diagnosis, treatment, as well as outcome prediction and prognosis evaluation. We conclude with discussion about pioneer AI systems, such as IBM Watson, and hurdles for real-life deployment of AI.",
            "referenceCount": 73,
            "citationCount": 1848,
            "influentialCitationCount": 43,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://svn.bmj.com/content/svnbmj/2/4/230.full.pdf",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2017-06-21",
            "journal": {
                "name": "Stroke and Vascular Neurology",
                "volume": "2"
            },
            "citationStyles": {
                "bibtex": "@Article{Jiang2017ArtificialII,\n author = {F. Jiang and Yong Jiang and Hui Zhi and Yi Dong and Hao Li and Sufeng Ma and Yilong Wang and Q. Dong and Haipeng Shen and Yongjun Wang},\n booktitle = {Stroke and vascular neurology},\n journal = {Stroke and Vascular Neurology},\n pages = {230 - 243},\n title = {Artificial intelligence in healthcare: past, present and future},\n volume = {2},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:da6c3fdf8ef9aae979a5dd156e074ba6691b2e2c",
            "@type": "ScholarlyArticle",
            "paperId": "da6c3fdf8ef9aae979a5dd156e074ba6691b2e2c",
            "corpusId": 11522524,
            "url": "https://www.semanticscholar.org/paper/da6c3fdf8ef9aae979a5dd156e074ba6691b2e2c",
            "title": "GENIA corpus - a semantically annotated corpus for bio-textmining",
            "venue": "Intelligent Systems in Molecular Biology",
            "publicationVenue": {
                "id": "urn:research:9d3979be-d38e-4ba6-8e93-ce5332aa70ce",
                "name": "Intelligent Systems in Molecular Biology",
                "alternate_names": [
                    "ISMB",
                    "Intell Syst Mol Biology"
                ],
                "issn": null,
                "url": "http://www.iscb.org/"
            },
            "year": 2003,
            "externalIds": {
                "MAG": "2163107094",
                "DBLP": "conf/ismb/KimOTT03",
                "DOI": "10.1093/BIOINFORMATICS/BTG1023",
                "CorpusId": 11522524,
                "PubMed": "12855455"
            },
            "abstract": "MOTIVATION\nNatural language processing (NLP) methods are regarded as being useful to raise the potential of text mining from biological literature. The lack of an extensively annotated corpus of this literature, however, causes a major bottleneck for applying NLP techniques. GENIA corpus is being developed to provide reference materials to let NLP techniques work for bio-textmining.\n\n\nRESULTS\nGENIA corpus version 3.0 consisting of 2000 MEDLINE abstracts has been released with more than 400,000 words and almost 100,000 annotations for biological terms.",
            "referenceCount": 2,
            "citationCount": 1227,
            "influentialCitationCount": 138,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://academic.oup.com/bioinformatics/article-pdf/19/suppl_1/i180/614820/btg1023.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2003-07-03",
            "journal": {
                "name": "Bioinformatics",
                "volume": "19 Suppl 1"
            },
            "citationStyles": {
                "bibtex": "@Article{Kim2003GENIAC,\n author = {Jin-Dong Kim and Tomoko Ohta and Yuka Tateisi and Junichi Tsujii},\n booktitle = {Intelligent Systems in Molecular Biology},\n journal = {Bioinformatics},\n pages = {\n          i180-2\n        },\n title = {GENIA corpus - a semantically annotated corpus for bio-textmining},\n volume = {19 Suppl 1},\n year = {2003}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:fe30dc915eefa40755b25a363813fcc575536661",
            "@type": "ScholarlyArticle",
            "paperId": "fe30dc915eefa40755b25a363813fcc575536661",
            "corpusId": 5216592,
            "url": "https://www.semanticscholar.org/paper/fe30dc915eefa40755b25a363813fcc575536661",
            "title": "A Simple Rule-Based Part of Speech Tagger",
            "venue": "Applied Natural Language Processing Conference",
            "publicationVenue": {
                "id": "urn:research:a8d0722b-8d14-4675-ae77-47b7d0e3fd64",
                "name": "Applied Natural Language Processing Conference",
                "alternate_names": [
                    "Conf Appl Nat Lang Process",
                    "Appl Nat Lang Process Conf",
                    "Conference on Applied Natural Language Processing",
                    "ANLP"
                ],
                "issn": null,
                "url": "https://aclweb.org/anthology/venues/anlp/"
            },
            "year": 1992,
            "externalIds": {
                "MAG": "2157693466",
                "DBLP": "conf/naacl/Brill92",
                "ACL": "A92-1021",
                "DOI": "10.3115/974499.974526",
                "CorpusId": 5216592
            },
            "abstract": "Automatic part of speech tagging is an area of natural language processing where statistical techniques have been more successful than rule-based methods. In this paper, we present a simple rule-based part of speech tagger which automatically acquires its rules and tags with accuracy comparable to stochastic taggers. The rule-based tagger has many advantages over these taggers, including: a vast reduction in stored information required, the perspicuity of a small set of meaningful rules, ease of finding and implementing improvements to the tagger, and better portability from one tag set, corpus genre or language to another. Perhaps the biggest contribution of this work is in demonstrating that the stochastic method is not the only viable method for part of speech tagging. The fact that a simple rule-based tagger that automatically learns its rules can perform so well should offer encouragement for researchers to further explore rule-based tagging, searching for a better and more expressive set of rule templates and other variations on the simple but effective theme described below.",
            "referenceCount": 15,
            "citationCount": 1918,
            "influentialCitationCount": 118,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1992-02-23",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Brill1992ASR,\n author = {Eric Brill},\n booktitle = {Applied Natural Language Processing Conference},\n pages = {152-155},\n title = {A Simple Rule-Based Part of Speech Tagger},\n year = {1992}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5504f4ec7b7b7b074e9dc557beeaf68c76b65540",
            "@type": "ScholarlyArticle",
            "paperId": "5504f4ec7b7b7b074e9dc557beeaf68c76b65540",
            "corpusId": 1826481,
            "url": "https://www.semanticscholar.org/paper/5504f4ec7b7b7b074e9dc557beeaf68c76b65540",
            "title": "Multiword Expressions: A Pain in the Neck for NLP",
            "venue": "Conference on Intelligent Text Processing and Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:a1c1b2d2-5629-4b8f-b764-20ea10fa588c",
                "name": "Conference on Intelligent Text Processing and Computational Linguistics",
                "alternate_names": [
                    "Conf Intell Text Process Comput Linguistics",
                    "CICLing"
                ],
                "issn": null,
                "url": "http://www.cicling.org/"
            },
            "year": 2002,
            "externalIds": {
                "MAG": "1498763386",
                "DBLP": "conf/cicling/SagBBCF02",
                "DOI": "10.1007/3-540-45715-1_1",
                "CorpusId": 1826481
            },
            "abstract": null,
            "referenceCount": 30,
            "citationCount": 1231,
            "influentialCitationCount": 119,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dr.ntu.edu.sg/bitstream/10356/79581/1/2002-cicling-mwe.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference",
                "Review"
            ],
            "publicationDate": "2002-02-17",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Sag2002MultiwordEA,\n author = {I. Sag and Timothy Baldwin and Francis Bond and Ann A. Copestake and D. Flickinger},\n booktitle = {Conference on Intelligent Text Processing and Computational Linguistics},\n pages = {1-15},\n title = {Multiword Expressions: A Pain in the Neck for NLP},\n year = {2002}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f1a210051a89f06fdb90f50a09f78ddb87bf308f",
            "@type": "ScholarlyArticle",
            "paperId": "f1a210051a89f06fdb90f50a09f78ddb87bf308f",
            "corpusId": 53242374,
            "url": "https://www.semanticscholar.org/paper/f1a210051a89f06fdb90f50a09f78ddb87bf308f",
            "title": "quanteda: An R package for the quantitative analysis of textual data",
            "venue": "Journal of Open Source Software",
            "publicationVenue": {
                "id": "urn:research:1236e136-01b7-42d5-8c4a-593153a3ab37",
                "name": "Journal of Open Source Software",
                "alternate_names": [
                    "The Journal of Open Source Software",
                    "J Open Source Softw"
                ],
                "issn": "2475-9066",
                "url": "https://joss.theoj.org/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2895553377",
                "DBLP": "journals/jossw/BenoitWWNOMM18",
                "DOI": "10.21105/JOSS.00774",
                "CorpusId": 53242374
            },
            "abstract": "quanteda is an R package providing a comprehensive workflow and toolkit for natural language processing tasks such as corpus management, tokenization, analysis, and visualization. It has extensive functions for applying dictionary analysis, exploring texts using keywords-in-context, computing document and feature similarities, and discovering multi-word expressions through collocation scoring. Based entirely on sparse operations,it provides highly efficient methods for compiling document-feature matrices and for manipulating these or using them in further quantitative analysis. Using C++ and multi-threading extensively, quanteda is also considerably faster and more efficient than other R and Python packages in processing large textual data. The package is designed for R users needing to apply natural language processing to texts,from documents to final analysis. Its capabilities match or exceed those provided in many end-user software applications, many of which are expensive and not open source. The package is therefore of great benefit to researchers, students, and other analysts with fewer financial resources. While using quanteda requires R programming knowledge, its API is designed to enable powerful, efficient analysis with a minimum of steps. By emphasizing consistent design, furthermore, quanteda lowers the barriers to learning and using NLP and quantitative text analysis even for proficient R programmers.",
            "referenceCount": 18,
            "citationCount": 682,
            "influentialCitationCount": 39,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://joss.theoj.org/papers/10.21105/joss.00774.pdf",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-10-06",
            "journal": {
                "name": "J. Open Source Softw.",
                "volume": "3"
            },
            "citationStyles": {
                "bibtex": "@Article{Benoit2018quantedaAR,\n author = {K. Benoit and Kohei Watanabe and Haiyan Wang and P. Nulty and Adam Obeng and Stefan M\u00fcller and Akitaka Matsuo},\n booktitle = {Journal of Open Source Software},\n journal = {J. Open Source Softw.},\n pages = {774},\n title = {quanteda: An R package for the quantitative analysis of textual data},\n volume = {3},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ac64fb7e6d2ddf236332ec9f371fe85d308c114d",
            "@type": "ScholarlyArticle",
            "paperId": "ac64fb7e6d2ddf236332ec9f371fe85d308c114d",
            "corpusId": 3158329,
            "url": "https://www.semanticscholar.org/paper/ac64fb7e6d2ddf236332ec9f371fe85d308c114d",
            "title": "A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2014,
            "externalIds": {
                "MAG": "2951619830",
                "DBLP": "journals/corr/MalinowskiF14",
                "ArXiv": "1410.0210",
                "CorpusId": 3158329
            },
            "abstract": "We propose a method for automatically answering questions about images by bringing together recent advances from natural language processing and computer vision. We combine discrete reasoning with uncertain predictions by a multi-world approach that represents uncertainty about the perceived world in a bayesian framework. Our approach can handle human questions of high complexity about realistic scenes and replies with range of answer like counts, object classes, instances and lists of them. The system is directly trained from question-answer pairs. We establish a first benchmark for this task that can be seen as a modern attempt at a visual turing test.",
            "referenceCount": 25,
            "citationCount": 635,
            "influentialCitationCount": 83,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2014-10-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Malinowski2014AMA,\n author = {Mateusz Malinowski and Mario Fritz},\n booktitle = {Neural Information Processing Systems},\n pages = {1682-1690},\n title = {A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:032fbcb58e2282f02426a0f09c6d5b42787936ec",
            "@type": "ScholarlyArticle",
            "paperId": "032fbcb58e2282f02426a0f09c6d5b42787936ec",
            "corpusId": 5403702,
            "url": "https://www.semanticscholar.org/paper/032fbcb58e2282f02426a0f09c6d5b42787936ec",
            "title": "Classifying Relations via Long Short Term Memory Networks along Shortest Dependency Paths",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "publicationVenue": {
                "id": "urn:research:41bf9ed3-85b3-4c90-b015-150e31690253",
                "name": "Conference on Empirical Methods in Natural Language Processing",
                "alternate_names": [
                    "Empir Method Nat Lang Process",
                    "Empirical Methods in Natural Language Processing",
                    "Conf Empir Method Nat Lang Process",
                    "EMNLP"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/emnlp"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "2964217331",
                "ACL": "D15-1206",
                "DBLP": "journals/corr/XuMLCPJ15",
                "ArXiv": "1508.03720",
                "DOI": "10.18653/v1/D15-1206",
                "CorpusId": 5403702
            },
            "abstract": "Relation classification is an important research arena in the field of natural language processing (NLP). In this paper, we present SDP-LSTM, a novel neural network to classify the relation of two entities in a sentence. Our neural architecture leverages the shortest dependency path (SDP) between two entities; multichannel recurrent neural networks, with long short term memory (LSTM) units, pick up heterogeneous information along the SDP. Our proposed model has several distinct features: (1) The shortest dependency paths retain most relevant information (to relation classification), while eliminating irrelevant words in the sentence. (2) The multichannel LSTM networks allow effective information integration from heterogeneous sources over the dependency paths. (3) A customized dropout strategy regularizes the neural network to alleviate overfitting. We test our model on the SemEval 2010 relation classification task, and achieve an $F_1$-score of 83.7\\%, higher than competing methods in the literature.",
            "referenceCount": 35,
            "citationCount": 611,
            "influentialCitationCount": 76,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/D15-1206.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-08-15",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1508.03720"
            },
            "citationStyles": {
                "bibtex": "@Article{Xu2015ClassifyingRV,\n author = {Yan Xu and Lili Mou and Ge Li and Yunchuan Chen and Hao Peng and Zhi Jin},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n journal = {ArXiv},\n title = {Classifying Relations via Long Short Term Memory Networks along Shortest Dependency Paths},\n volume = {abs/1508.03720},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a1eb97b787581ca37a44ad6b2331c5fbe89cf2ea",
            "@type": "ScholarlyArticle",
            "paperId": "a1eb97b787581ca37a44ad6b2331c5fbe89cf2ea",
            "corpusId": 61079112,
            "url": "https://www.semanticscholar.org/paper/a1eb97b787581ca37a44ad6b2331c5fbe89cf2ea",
            "title": "Text Processing with GATE",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2011,
            "externalIds": {
                "MAG": "1674586686",
                "CorpusId": 61079112
            },
            "abstract": "GATE is a free open-source infrastructure for developing and deploying software components that process human language. It is more than 15 years old and is in active use for all types of computational tasks involving language (frequently called natural language processing, text analytics, or text mining). GATE excels at text analysis of all shapes and sizes. From large corporations to small startups, from multi-million research consortia to undergraduate projects, our user community is the largest and most diverse of any system of this type, and is active world-wide. This book contains a highly accessible introduction to GATE Version 6 and is the first port of call for all GATE-related questions. It includes a guide to using GATE Developer and GATE Embedded, and chapters on all major areas of functionality, such as processing multiple languages and large collections of unstructured text. It also includes complete plugin documentation (e.g. named entity recognition, parsing, semantic analysis, , as well as details on other members of the GATE Family: GATECloud.net, Teamware, and Mimir. To join the GATE community visit http://gate.ac.uk/.",
            "referenceCount": 0,
            "citationCount": 253,
            "influentialCitationCount": 27,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2011-04-15",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Cunningham2011TextPW,\n author = {H. Cunningham and D. Maynard and Kalina Bontcheva},\n title = {Text Processing with GATE},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:02e46711fc86877bdd279c736abe5415a2415e48",
            "@type": "ScholarlyArticle",
            "paperId": "02e46711fc86877bdd279c736abe5415a2415e48",
            "corpusId": 237304047,
            "url": "https://www.semanticscholar.org/paper/02e46711fc86877bdd279c736abe5415a2415e48",
            "title": "A Survey on Automated Fact-Checking",
            "venue": "Transactions of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:e0dbf116-86aa-418d-859f-a49952d7e44a",
                "name": "Transactions of the Association for Computational Linguistics",
                "alternate_names": [
                    "Trans Assoc Comput Linguistics",
                    "TACL"
                ],
                "issn": "2307-387X",
                "url": "https://www.mitpressjournals.org/loi/tacl"
            },
            "year": 2021,
            "externalIds": {
                "ArXiv": "2108.11896",
                "DBLP": "journals/tacl/GuoSV22",
                "ACL": "2022.tacl-1.11",
                "DOI": "10.1162/tacl_a_00454",
                "CorpusId": 237304047
            },
            "abstract": "Fact-checking has become increasingly important due to the speed with which both information and misinformation can spread in the modern media ecosystem. Therefore, researchers have been exploring how fact-checking can be automated, using techniques based on natural language processing, machine learning, knowledge representation, and databases to automatically predict the veracity of claims. In this paper, we survey automated fact-checking stemming from natural language processing, and discuss its connections to related tasks and disciplines. In this process, we present an overview of existing datasets and models, aiming to unify the various definitions given and identify common concepts. Finally, we highlight challenges for future research.",
            "referenceCount": 218,
            "citationCount": 170,
            "influentialCitationCount": 8,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00454/1987018/tacl_a_00454.pdf",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2021-08-26",
            "journal": {
                "name": "Transactions of the Association for Computational Linguistics",
                "volume": "10"
            },
            "citationStyles": {
                "bibtex": "@Article{Guo2021ASO,\n author = {Zhijiang Guo and M. Schlichtkrull and Andreas Vlachos},\n booktitle = {Transactions of the Association for Computational Linguistics},\n journal = {Transactions of the Association for Computational Linguistics},\n pages = {178-206},\n title = {A Survey on Automated Fact-Checking},\n volume = {10},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9ef33af1b2ebda2f2edd6c1394f314d7ac2f00f2",
            "@type": "ScholarlyArticle",
            "paperId": "9ef33af1b2ebda2f2edd6c1394f314d7ac2f00f2",
            "corpusId": 225062026,
            "url": "https://www.semanticscholar.org/paper/9ef33af1b2ebda2f2edd6c1394f314d7ac2f00f2",
            "title": "TweetEval: Unified Benchmark and Comparative Evaluation for Tweet Classification",
            "venue": "Findings",
            "publicationVenue": {
                "id": "urn:research:479d5605-51be-4346-b1d6-4334084504df",
                "name": "Findings",
                "alternate_names": null,
                "issn": "2652-8800",
                "url": "https://findingspress.org/"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "journals/corr/abs-2010-12421",
                "ArXiv": "2010.12421",
                "ACL": "2020.findings-emnlp.148",
                "MAG": "3099215402",
                "DOI": "10.18653/v1/2020.findings-emnlp.148",
                "CorpusId": 225062026
            },
            "abstract": "The experimental landscape in natural language processing for social media is too fragmented. Each year, new shared tasks and datasets are proposed, ranging from classics like sentiment analysis to irony detection or emoji prediction. Therefore, it is unclear what the current state of the art is, as there is no standardized evaluation protocol, neither a strong set of baselines trained on such domain-specific data. In this paper, we propose a new evaluation framework (TweetEval) consisting of seven heterogeneous Twitter-specific classification tasks. We also provide a strong set of baselines as starting point, and compare different language modeling pre-training strategies. Our initial experiments show the effectiveness of starting off with existing pre-trained generic language models, and continue training them on Twitter corpora.",
            "referenceCount": 33,
            "citationCount": 405,
            "influentialCitationCount": 73,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/2020.findings-emnlp.148.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-10-23",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2010.12421"
            },
            "citationStyles": {
                "bibtex": "@Article{Barbieri2020TweetEvalUB,\n author = {Francesco Barbieri and Jos\u00e9 Camacho-Collados and Leonardo Neves and Luis Espinosa-Anke},\n booktitle = {Findings},\n journal = {ArXiv},\n title = {TweetEval: Unified Benchmark and Comparative Evaluation for Tweet Classification},\n volume = {abs/2010.12421},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2b9762e91305986ac8a2d624d0a69521304405f3",
            "@type": "ScholarlyArticle",
            "paperId": "2b9762e91305986ac8a2d624d0a69521304405f3",
            "corpusId": 233241004,
            "url": "https://www.semanticscholar.org/paper/2b9762e91305986ac8a2d624d0a69521304405f3",
            "title": "XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "publicationVenue": {
                "id": "urn:research:41bf9ed3-85b3-4c90-b015-150e31690253",
                "name": "Conference on Empirical Methods in Natural Language Processing",
                "alternate_names": [
                    "Empir Method Nat Lang Process",
                    "Empirical Methods in Natural Language Processing",
                    "Conf Empir Method Nat Lang Process",
                    "EMNLP"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/emnlp"
            },
            "year": 2021,
            "externalIds": {
                "ArXiv": "2104.07412",
                "ACL": "2021.emnlp-main.802",
                "DBLP": "journals/corr/abs-2104-07412",
                "DOI": "10.18653/v1/2021.emnlp-main.802",
                "CorpusId": 233241004
            },
            "abstract": "Machine learning has brought striking advances in multilingual natural language processing capabilities over the past year. For example, the latest techniques have improved the state-of-the-art performance on the XTREME multilingual benchmark by more than 13 points. While a sizeable gap to human-level performance remains, improvements have been easier to achieve in some tasks than in others. This paper analyzes the current state of cross-lingual transfer learning and summarizes some lessons learned. In order to catalyze meaningful progress, we extend XTREME to XTREME-R, which consists of an improved set of ten natural language understanding tasks, including challenging language-agnostic retrieval tasks, and covers 50 typologically diverse languages. In addition, we provide a massively multilingual diagnostic suite and fine-grained multi-dataset evaluation capabilities through an interactive public leaderboard to gain a better understanding of such models.",
            "referenceCount": 65,
            "citationCount": 129,
            "influentialCitationCount": 23,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2021.emnlp-main.802.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2021-04-15",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2104.07412"
            },
            "citationStyles": {
                "bibtex": "@Article{Ruder2021XTREMERTM,\n author = {Sebastian Ruder and Noah Constant and Jan A. Botha and Aditya Siddhant and Orhan Firat and Jinlan Fu and Pengfei Liu and Junjie Hu and Graham Neubig and Melvin Johnson},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n journal = {ArXiv},\n title = {XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation},\n volume = {abs/2104.07412},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d517b13f2b152c913b81ce534a149493517dbdad",
            "@type": "ScholarlyArticle",
            "paperId": "d517b13f2b152c913b81ce534a149493517dbdad",
            "corpusId": 10158224,
            "url": "https://www.semanticscholar.org/paper/d517b13f2b152c913b81ce534a149493517dbdad",
            "title": "Big Data Deep Learning: Challenges and Perspectives",
            "venue": "IEEE Access",
            "publicationVenue": {
                "id": "urn:research:2633f5b2-c15c-49fe-80f5-07523e770c26",
                "name": "IEEE Access",
                "alternate_names": null,
                "issn": "2169-3536",
                "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html"
            },
            "year": 2014,
            "externalIds": {
                "DBLP": "journals/access/ChenL14",
                "MAG": "1984020445",
                "DOI": "10.1109/ACCESS.2014.2325029",
                "CorpusId": 10158224
            },
            "abstract": "Deep learning is currently an extremely active research area in machine learning and pattern recognition society. It has gained huge successes in a broad area of applications such as speech recognition, computer vision, and natural language processing. With the sheer size of data available today, big data brings big opportunities and transformative potential for various sectors; on the other hand, it also presents unprecedented challenges to harnessing data and information. As the data keeps getting bigger, deep learning is coming to play a key role in providing big data predictive analytics solutions. In this paper, we provide a brief overview of deep learning, and highlight current research efforts and the challenges to big data, as well as the future trends.",
            "referenceCount": 115,
            "citationCount": 973,
            "influentialCitationCount": 32,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2014-05-16",
            "journal": {
                "name": "IEEE Access",
                "volume": "2"
            },
            "citationStyles": {
                "bibtex": "@Article{Chen2014BigDD,\n author = {Xue-wen Chen and Xiaotong Lin},\n booktitle = {IEEE Access},\n journal = {IEEE Access},\n pages = {514-525},\n title = {Big Data Deep Learning: Challenges and Perspectives},\n volume = {2},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1f4a4769e4d2fb846e59c2f185e0377190739f18",
            "@type": "ScholarlyArticle",
            "paperId": "1f4a4769e4d2fb846e59c2f185e0377190739f18",
            "corpusId": 715463,
            "url": "https://www.semanticscholar.org/paper/1f4a4769e4d2fb846e59c2f185e0377190739f18",
            "title": "Learning Structured Embeddings of Knowledge Bases",
            "venue": "AAAI Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:bdc2e585-4e48-4e36-8af1-6d859763d405",
                "name": "AAAI Conference on Artificial Intelligence",
                "alternate_names": [
                    "National Conference on Artificial Intelligence",
                    "National Conf Artif Intell",
                    "AAAI Conf Artif Intell",
                    "AAAI"
                ],
                "issn": null,
                "url": "http://www.aaai.org/"
            },
            "year": 2011,
            "externalIds": {
                "DBLP": "conf/aaai/BordesWCB11",
                "MAG": "2247119764",
                "DOI": "10.1609/aaai.v25i1.7917",
                "CorpusId": 715463
            },
            "abstract": "\n \n Many Knowledge Bases (KBs) are now readily available and encompass colossal quantities of information thanks to either a long-term funding effort (e.g. WordNet, OpenCyc) or a collaborative process (e.g. Freebase, DBpedia). However, each of them is based on a different rigorous symbolic framework which makes it hard to use their data in other systems. It is unfortunate because such rich structured knowledge might lead to a huge leap forward in many other areas of AI like nat- ural language processing (word-sense disambiguation, natural language understanding, ...), vision (scene classification, image semantic annotation, ...) or collaborative filtering. In this paper, we present a learning process based on an innovative neural network architecture designed to embed any of these symbolic representations into a more flexible continuous vector space in which the original knowledge is kept and enhanced. These learnt embeddings would allow data from any KB to be easily used in recent machine learning meth- ods for prediction and information retrieval. We illustrate our method on WordNet and Freebase and also present a way to adapt it to knowledge extraction from raw text.\n \n",
            "referenceCount": 26,
            "citationCount": 842,
            "influentialCitationCount": 85,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/7917/7776",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2011-08-04",
            "journal": {
                "name": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Bordes2011LearningSE,\n author = {Antoine Bordes and J. Weston and Ronan Collobert and Yoshua Bengio},\n booktitle = {AAAI Conference on Artificial Intelligence},\n journal = {Proceedings of the AAAI Conference on Artificial Intelligence},\n title = {Learning Structured Embeddings of Knowledge Bases},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:76a1dca3a9c2b0229c1b12c95752dcf40dc95a11",
            "@type": "ScholarlyArticle",
            "paperId": "76a1dca3a9c2b0229c1b12c95752dcf40dc95a11",
            "corpusId": 1539668,
            "url": "https://www.semanticscholar.org/paper/76a1dca3a9c2b0229c1b12c95752dcf40dc95a11",
            "title": "Corpus-Guided Sentence Generation of Natural Images",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "publicationVenue": {
                "id": "urn:research:41bf9ed3-85b3-4c90-b015-150e31690253",
                "name": "Conference on Empirical Methods in Natural Language Processing",
                "alternate_names": [
                    "Empir Method Nat Lang Process",
                    "Empirical Methods in Natural Language Processing",
                    "Conf Empir Method Nat Lang Process",
                    "EMNLP"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/emnlp"
            },
            "year": 2011,
            "externalIds": {
                "DBLP": "conf/emnlp/YangTDA11",
                "ACL": "D11-1041",
                "MAG": "1858383477",
                "CorpusId": 1539668
            },
            "abstract": "We propose a sentence generation strategy that describes images by predicting the most likely nouns, verbs, scenes and prepositions that make up the core sentence structure. The input are initial noisy estimates of the objects and scenes detected in the image using state of the art trained detectors. As predicting actions from still images directly is unreliable, we use a language model trained from the English Gigaword corpus to obtain their estimates; together with probabilities of co-located nouns, scenes and prepositions. We use these estimates as parameters on a HMM that models the sentence generation process, with hidden nodes as sentence components and image detections as the emissions. Experimental results show that our strategy of combining vision and language produces readable and descriptive sentences compared to naive strategies that use vision alone.",
            "referenceCount": 27,
            "citationCount": 400,
            "influentialCitationCount": 26,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2011-07-27",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Yang2011CorpusGuidedSG,\n author = {Yezhou Yang and C. L. Teo and Hal Daum\u00e9 and Y. Aloimonos},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {444-454},\n title = {Corpus-Guided Sentence Generation of Natural Images},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3f46b61a7216e5763ffab5d33e06b88c9d490c85",
            "@type": "ScholarlyArticle",
            "paperId": "3f46b61a7216e5763ffab5d33e06b88c9d490c85",
            "corpusId": 6627923,
            "url": "https://www.semanticscholar.org/paper/3f46b61a7216e5763ffab5d33e06b88c9d490c85",
            "title": "Towards Answering Opinion Questions: Separating Facts from Opinions and Identifying the Polarity of Opinion Sentences",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "publicationVenue": {
                "id": "urn:research:41bf9ed3-85b3-4c90-b015-150e31690253",
                "name": "Conference on Empirical Methods in Natural Language Processing",
                "alternate_names": [
                    "Empir Method Nat Lang Process",
                    "Empirical Methods in Natural Language Processing",
                    "Conf Empir Method Nat Lang Process",
                    "EMNLP"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/emnlp"
            },
            "year": 2003,
            "externalIds": {
                "ACL": "W03-1017",
                "MAG": "2080558111",
                "DBLP": "conf/emnlp/YuH03",
                "DOI": "10.3115/1119355.1119372",
                "CorpusId": 6627923
            },
            "abstract": "Opinion question answering is a challenging task for natural language processing. In this paper, we discuss a necessary component for an opinion question answering system: separating opinions from fact, at both the document and sentence level. We present a Bayesian classifier for discriminating between documents with a preponderance of opinions such as editorials from regular news stories, and describe three unsupervised, statistical techniques for the significantly harder task of detecting opinions at the sentence level. We also present a first model for classifying opinion sentences as positive or negative in terms of the main perspective being expressed in the opinion. Results from a large collection of news stories and a human evaluation of 400 sentences are reported, indicating that we achieve very high performance in document classification (upwards of 97% precision and recall), and respectable performance in detecting opinions and classifying them at the sentence level as positive, negative, or neutral (up to 91% accuracy).",
            "referenceCount": 12,
            "citationCount": 1233,
            "influentialCitationCount": 72,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.3115/1119355.1119372",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2003-07-11",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Article{Yu2003TowardsAO,\n author = {Hong Yu and V. Hatzivassiloglou},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {129-136},\n title = {Towards Answering Opinion Questions: Separating Facts from Opinions and Identifying the Polarity of Opinion Sentences},\n year = {2003}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ba91490d9775474e0e7e53f3dd43005ca547240e",
            "@type": "ScholarlyArticle",
            "paperId": "ba91490d9775474e0e7e53f3dd43005ca547240e",
            "corpusId": 219792180,
            "url": "https://www.semanticscholar.org/paper/ba91490d9775474e0e7e53f3dd43005ca547240e",
            "title": "Deep Learning Enabled Semantic Communication Systems",
            "venue": "IEEE Transactions on Signal Processing",
            "publicationVenue": {
                "id": "urn:research:1f6f3f05-6a23-42f0-8d31-98ab8089c1f2",
                "name": "IEEE Transactions on Signal Processing",
                "alternate_names": [
                    "IEEE Trans Signal Process"
                ],
                "issn": "1053-587X",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=78"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3036851434",
                "DBLP": "journals/tsp/XieQLJ21",
                "ArXiv": "2006.10685",
                "DOI": "10.1109/TSP.2021.3071210",
                "CorpusId": 219792180
            },
            "abstract": "Recently, deep learned enabled end-to-end communication systems have been developed to merge all physical layer blocks in the traditional communication systems, which make joint transceiver optimization possible. Powered by deep learning, natural language processing has achieved great success in analyzing and understanding a large amount of language texts. Inspired by research results in both areas, we aim to provide a new view on communication systems from the semantic level. Particularly, we propose a deep learning based semantic communication system, named DeepSC, for text transmission. Based on the Transformer, the DeepSC aims at maximizing the system capacity and minimizing the semantic errors by recovering the meaning of sentences, rather than bit- or symbol-errors in traditional communications. Moreover, transfer learning is used to ensure the DeepSC applicable to different communication environments and to accelerate the model training process. To justify the performance of semantic communications accurately, we also initialize a new metric, named sentence similarity. Compared with the traditional communication system without considering semantic information exchange, the proposed DeepSC is more robust to channel variation and is able to achieve better performance, especially in the low signal-to-noise (SNR) regime, as demonstrated by the extensive simulation results.",
            "referenceCount": 39,
            "citationCount": 380,
            "influentialCitationCount": 66,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/78/9307529/09398576.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-06-18",
            "journal": {
                "name": "IEEE Transactions on Signal Processing",
                "volume": "69"
            },
            "citationStyles": {
                "bibtex": "@Article{Xie2020DeepLE,\n author = {Huiqiang Xie and Zhijin Qin and Geoffrey Y. Li and B. Juang},\n booktitle = {IEEE Transactions on Signal Processing},\n journal = {IEEE Transactions on Signal Processing},\n pages = {2663-2675},\n title = {Deep Learning Enabled Semantic Communication Systems},\n volume = {69},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:22ae02d81c21cb90b0de071550cfb99e6a623e62",
            "@type": "ScholarlyArticle",
            "paperId": "22ae02d81c21cb90b0de071550cfb99e6a623e62",
            "corpusId": 3337266,
            "url": "https://www.semanticscholar.org/paper/22ae02d81c21cb90b0de071550cfb99e6a623e62",
            "title": "Deep Sentence Embedding Using Long Short-Term Memory Networks: Analysis and Application to Information Retrieval",
            "venue": "IEEE/ACM Transactions on Audio Speech and Language Processing",
            "publicationVenue": {
                "id": "urn:research:309e00f7-4bbd-461f-ab37-a90cd14ef21d",
                "name": "IEEE/ACM Transactions on Audio Speech and Language Processing",
                "alternate_names": [
                    "IEEE/ACM Trans Audio Speech Lang Process"
                ],
                "issn": "2329-9290",
                "url": "https://ieeexplore.ieee.org/servlet/opac?punumber=6570655"
            },
            "year": 2015,
            "externalIds": {
                "ArXiv": "1502.06922",
                "MAG": "2142920810",
                "DBLP": "journals/corr/PalangiDSGHCSW15",
                "DOI": "10.1109/TASLP.2016.2520371",
                "CorpusId": 3337266
            },
            "abstract": "This paper develops a model that addresses sentence embedding, a hot topic in current natural language processing research, using recurrent neural networks (RNN) with Long Short-Term Memory (LSTM) cells. The proposed LSTM-RNN model sequentially takes each word in a sentence, extracts its information, and embeds it into a semantic vector. Due to its ability to capture long term memory, the LSTM-RNN accumulates increasingly richer information as it goes through the sentence, and when it reaches the last word, the hidden layer of the network provides a semantic representation of the whole sentence. In this paper, the LSTM-RNN is trained in a weakly supervised manner on user click-through data logged by a commercial web search engine. Visualization and analysis are performed to understand how the embedding process works. The model is found to automatically attenuate the unimportant words and detect the salient keywords in the sentence. Furthermore, these detected keywords are found to automatically activate different cells of the LSTM-RNN, where words belonging to a similar topic activate the same cell. As a semantic representation of the sentence, the embedding vector can be used in many different applications. These automatic keyword detection and topic allocation abilities enabled by the LSTM-RNN allow the network to perform document retrieval, a difficult language processing task, where the similarity between the query and documents can be measured by the distance between their corresponding sentence embedding vectors computed by the LSTM-RNN. On a web search task, the LSTM-RNN embedding is shown to significantly outperform several existing state of the art methods. We emphasize that the proposed model generates sentence embedding vectors that are specially useful for web document retrieval tasks. A comparison with a well known general sentence embedding method, the Paragraph Vector, is performed. The results show that the proposed method in this paper significantly outperforms Paragraph Vector method for web document retrieval task.",
            "referenceCount": 54,
            "citationCount": 766,
            "influentialCitationCount": 36,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/1502.06922",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-02-01",
            "journal": {
                "name": "IEEE/ACM Transactions on Audio, Speech, and Language Processing",
                "volume": "24"
            },
            "citationStyles": {
                "bibtex": "@Article{Palangi2015DeepSE,\n author = {H. Palangi and L. Deng and Yelong Shen and Jianfeng Gao and Xiaodong He and Jianshu Chen and Xinying Song and R. Ward},\n booktitle = {IEEE/ACM Transactions on Audio Speech and Language Processing},\n journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},\n pages = {694-707},\n title = {Deep Sentence Embedding Using Long Short-Term Memory Networks: Analysis and Application to Information Retrieval},\n volume = {24},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:487ed99e00bf6803a53a6059ceccd1510a63e72d",
            "@type": "ScholarlyArticle",
            "paperId": "487ed99e00bf6803a53a6059ceccd1510a63e72d",
            "corpusId": 8197231,
            "url": "https://www.semanticscholar.org/paper/487ed99e00bf6803a53a6059ceccd1510a63e72d",
            "title": "An Analysis of Active Learning Strategies for Sequence Labeling Tasks",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "publicationVenue": {
                "id": "urn:research:41bf9ed3-85b3-4c90-b015-150e31690253",
                "name": "Conference on Empirical Methods in Natural Language Processing",
                "alternate_names": [
                    "Empir Method Nat Lang Process",
                    "Empirical Methods in Natural Language Processing",
                    "Conf Empir Method Nat Lang Process",
                    "EMNLP"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/emnlp"
            },
            "year": 2008,
            "externalIds": {
                "ACL": "D08-1112",
                "DBLP": "conf/emnlp/SettlesC08",
                "MAG": "2171671120",
                "DOI": "10.3115/1613715.1613855",
                "CorpusId": 8197231
            },
            "abstract": "Active learning is well-suited to many problems in natural language processing, where unlabeled data may be abundant but annotation is slow and expensive. This paper aims to shed light on the best active learning approaches for sequence labeling tasks such as information extraction and document segmentation. We survey previously used query selection strategies for sequence models, and propose several novel algorithms to address their shortcomings. We also conduct a large-scale empirical comparison using multiple corpora, which demonstrates that our proposed methods advance the state of the art.",
            "referenceCount": 31,
            "citationCount": 1005,
            "influentialCitationCount": 105,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.5555/1613715.1613855",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference",
                "Review"
            ],
            "publicationDate": "2008-10-25",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Settles2008AnAO,\n author = {Burr Settles and M. Craven},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {1070-1079},\n title = {An Analysis of Active Learning Strategies for Sequence Labeling Tasks},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8edf070ee55db69f06b43fb46b055182837598f7",
            "@type": "ScholarlyArticle",
            "paperId": "8edf070ee55db69f06b43fb46b055182837598f7",
            "corpusId": 226262227,
            "url": "https://www.semanticscholar.org/paper/8edf070ee55db69f06b43fb46b055182837598f7",
            "title": "On the Sentence Embeddings from BERT for Semantic Textual Similarity",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "publicationVenue": {
                "id": "urn:research:41bf9ed3-85b3-4c90-b015-150e31690253",
                "name": "Conference on Empirical Methods in Natural Language Processing",
                "alternate_names": [
                    "Empir Method Nat Lang Process",
                    "Empirical Methods in Natural Language Processing",
                    "Conf Empir Method Nat Lang Process",
                    "EMNLP"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/emnlp"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3105816068",
                "DBLP": "conf/emnlp/LiZHWYL20",
                "ACL": "2020.emnlp-main.733",
                "ArXiv": "2011.05864",
                "DOI": "10.18653/v1/2020.emnlp-main.733",
                "CorpusId": 226262227
            },
            "abstract": "Pre-trained contextual representations like BERT have achieved great success in natural language processing. However, the sentence embeddings from the pre-trained language models without fine-tuning have been found to poorly capture semantic meaning of sentences. In this paper, we argue that the semantic information in the BERT embeddings is not fully exploited. We first reveal the theoretical connection between the masked language model pre-training objective and the semantic similarity task theoretically, and then analyze the BERT sentence embeddings empirically. We find that BERT always induces a non-smooth anisotropic semantic space of sentences, which harms its performance of semantic similarity. To address this issue, we propose to transform the anisotropic sentence embedding distribution to a smooth and isotropic Gaussian distribution through normalizing flows that are learned with an unsupervised objective. Experimental results show that our proposed BERT-flow method obtains significant performance gains over the state-of-the-art sentence embeddings on a variety of semantic textual similarity tasks. The code is available at https://github.com/bohanli/BERT-flow.",
            "referenceCount": 36,
            "citationCount": 310,
            "influentialCitationCount": 60,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/2020.emnlp-main.733.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-11-01",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2011.05864"
            },
            "citationStyles": {
                "bibtex": "@Article{Li2020OnTS,\n author = {Bohan Li and Hao Zhou and Junxian He and Mingxuan Wang and Yiming Yang and Lei Li},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n journal = {ArXiv},\n title = {On the Sentence Embeddings from BERT for Semantic Textual Similarity},\n volume = {abs/2011.05864},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:66f0f35fc78bdf2af9de46093d49a428970cde2e",
            "@type": "ScholarlyArticle",
            "paperId": "66f0f35fc78bdf2af9de46093d49a428970cde2e",
            "corpusId": 218665313,
            "url": "https://www.semanticscholar.org/paper/66f0f35fc78bdf2af9de46093d49a428970cde2e",
            "title": "Movement Pruning: Adaptive Sparsity by Fine-Tuning",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3099715410",
                "DBLP": "conf/nips/Sanh0R20",
                "ArXiv": "2005.07683",
                "CorpusId": 218665313
            },
            "abstract": "Magnitude pruning is a widely used strategy for reducing model size in pure supervised learning; however, it is less effective in the transfer learning regime that has become standard for state-of-the-art natural language processing applications. We propose the use of movement pruning, a simple, deterministic first-order weight pruning method that is more adaptive to pretrained model fine-tuning. We give mathematical foundations to the method and compare it to existing zeroth- and first-order pruning methods. Experiments show that when pruning large pretrained language models, movement pruning shows significant improvements in high-sparsity regimes. When combined with distillation, the approach achieves minimal accuracy loss with down to only 3% of the model parameters.",
            "referenceCount": 58,
            "citationCount": 296,
            "influentialCitationCount": 60,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-05-15",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2005.07683"
            },
            "citationStyles": {
                "bibtex": "@Article{Sanh2020MovementPA,\n author = {Victor Sanh and Thomas Wolf and Alexander M. Rush},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Movement Pruning: Adaptive Sparsity by Fine-Tuning},\n volume = {abs/2005.07683},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5785466bc14529e94e54baa4ed051f7037f3b1d3",
            "@type": "ScholarlyArticle",
            "paperId": "5785466bc14529e94e54baa4ed051f7037f3b1d3",
            "corpusId": 1868294,
            "url": "https://www.semanticscholar.org/paper/5785466bc14529e94e54baa4ed051f7037f3b1d3",
            "title": "Boosting Image Captioning with Attributes",
            "venue": "IEEE International Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:7654260e-79f9-45c5-9663-d72027cf88f3",
                "name": "IEEE International Conference on Computer Vision",
                "alternate_names": [
                    "ICCV",
                    "IEEE Int Conf Comput Vis",
                    "ICCV Workshops",
                    "ICCV Work"
                ],
                "issn": null,
                "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2552161745",
                "ArXiv": "1611.01646",
                "DBLP": "journals/corr/YaoPLQM16",
                "DOI": "10.1109/ICCV.2017.524",
                "CorpusId": 1868294
            },
            "abstract": "Automatically describing an image with a natural language has been an emerging challenge in both fields of computer vision and natural language processing. In this paper, we present Long Short-Term Memory with Attributes (LSTM-A) - a novel architecture that integrates attributes into the successful Convolutional Neural Networks (CNNs) plus Recurrent Neural Networks (RNNs) image captioning framework, by training them in an end-to-end manner. Particularly, the learning of attributes is strengthened by integrating inter-attribute correlations into Multiple Instance Learning (MIL). To incorporate attributes into captioning, we construct variants of architectures by feeding image representations and attributes into RNNs in different ways to explore the mutual but also fuzzy relationship between them. Extensive experiments are conducted on COCO image captioning dataset and our framework shows clear improvements when compared to state-of-the-art deep models. More remarkably, we obtain METEOR/CIDEr-D of 25.5%/100.2% on testing data of widely used and publicly available splits in [10] when extracting image representations by GoogleNet and achieve superior performance on COCO captioning Leaderboard.",
            "referenceCount": 47,
            "citationCount": 583,
            "influentialCitationCount": 52,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1611.01646",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-11-05",
            "journal": {
                "name": "2017 IEEE International Conference on Computer Vision (ICCV)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Yao2016BoostingIC,\n author = {Ting Yao and Yingwei Pan and Yehao Li and Zhaofan Qiu and Tao Mei},\n booktitle = {IEEE International Conference on Computer Vision},\n journal = {2017 IEEE International Conference on Computer Vision (ICCV)},\n pages = {4904-4912},\n title = {Boosting Image Captioning with Attributes},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2c953a3c378b40dadf2e3fb486713c8608b8e282",
            "@type": "ScholarlyArticle",
            "paperId": "2c953a3c378b40dadf2e3fb486713c8608b8e282",
            "corpusId": 222310837,
            "url": "https://www.semanticscholar.org/paper/2c953a3c378b40dadf2e3fb486713c8608b8e282",
            "title": "Pretrained Transformers for Text Ranking: BERT and Beyond",
            "venue": "Web Search and Data Mining",
            "publicationVenue": {
                "id": "urn:research:ea38228f-6ed3-4222-a3ce-d963d8cc9516",
                "name": "Web Search and Data Mining",
                "alternate_names": [
                    "Web Search Data Min",
                    "WSDM"
                ],
                "issn": null,
                "url": "http://www.wikicfp.com/cfp/program?id=3158"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3092952717",
                "DBLP": "conf/wsdm/YatesNL21",
                "ArXiv": "2010.06467",
                "DOI": "10.1145/3437963.3441667",
                "CorpusId": 222310837
            },
            "abstract": "The goal of text ranking is to generate an ordered list of texts retrieved from a corpus in response to a query. Although the most common formulation of text ranking is search, instances of the task can also be found in many natural language processing applications. This tutorial, based on a forthcoming book, provides an overview of text ranking with neural network architectures known as transformers, of which BERT is the best-known example. The combination of transformers and self-supervised pretraining has, without exaggeration, revolutionized the fields of natural language processing (NLP), information retrieval (IR), and beyond. We provide a synthesis of existing work as a single point of entry for both researchers and practitioners. Our coverage is grouped into two categories: transformer models that perform reranking in multi-stage ranking architectures and learned dense representations that perform ranking directly. Two themes pervade our treatment: techniques for handling long documents and techniques for addressing the tradeoff between effectiveness (result quality) and efficiency (query latency). Although transformer architectures and pretraining techniques are recent innovations, many aspects of their application are well understood. Nevertheless, there remain many open research questions, and thus in addition to laying out the foundations of pretrained transformers for text ranking, we also attempt to prognosticate the future.",
            "referenceCount": 480,
            "citationCount": 414,
            "influentialCitationCount": 37,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3437963.3441667",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Book",
                "Review"
            ],
            "publicationDate": "2020-10-13",
            "journal": {
                "name": "Proceedings of the 14th ACM International Conference on Web Search and Data Mining",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Lin2020PretrainedTF,\n author = {Jimmy J. Lin and Rodrigo Nogueira and Andrew Yates},\n booktitle = {Web Search and Data Mining},\n journal = {Proceedings of the 14th ACM International Conference on Web Search and Data Mining},\n title = {Pretrained Transformers for Text Ranking: BERT and Beyond},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b951b9f78b98a186ba259027996a48e4189d37e5",
            "@type": "ScholarlyArticle",
            "paperId": "b951b9f78b98a186ba259027996a48e4189d37e5",
            "corpusId": 982,
            "url": "https://www.semanticscholar.org/paper/b951b9f78b98a186ba259027996a48e4189d37e5",
            "title": "Inducing Features of Random Fields",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "publicationVenue": {
                "id": "urn:research:25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "alternate_names": [
                    "IEEE Trans Pattern Anal Mach Intell"
                ],
                "issn": "0162-8828",
                "url": "http://www.computer.org/tpami/"
            },
            "year": 1995,
            "externalIds": {
                "ArXiv": "cmp-lg/9506014",
                "MAG": "2952123276",
                "DBLP": "journals/corr/abs-cmp-lg-9506014",
                "DOI": "10.1109/34.588021",
                "CorpusId": 982
            },
            "abstract": "We present a technique for constructing random fields from a set of training samples. The learning paradigm builds increasingly complex fields by allowing potential functions, or features, that are supported by increasingly large subgraphs. Each feature has a weight that is trained by minimizing the Kullback-Leibler divergence between the model and the empirical distribution of the training data. A greedy algorithm determines how features are incrementally added to the field and an iterative scaling algorithm is used to estimate the optimal values of the weights. The random field models and techniques introduced in this paper differ from those common to much of the computer vision literature in that the underlying random fields are non-Markovian and have a large number of parameters that must be estimated. Relations to other learning approaches, including decision trees, are given. As a demonstration of the method, we describe its application to the problem of automatic word classification in natural language processing.",
            "referenceCount": 37,
            "citationCount": 1328,
            "influentialCitationCount": 121,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/cmp-lg/9506014",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1995-05-01",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/cmp-lg/9506014"
            },
            "citationStyles": {
                "bibtex": "@Article{Pietra1995InducingFO,\n author = {S. D. Pietra and V. D. Pietra and J. Lafferty},\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n journal = {ArXiv},\n title = {Inducing Features of Random Fields},\n volume = {abs/cmp-lg/9506014},\n year = {1995}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:bd7bd1d2945a58cdcc1797ba9698b8810fe68f60",
            "@type": "ScholarlyArticle",
            "paperId": "bd7bd1d2945a58cdcc1797ba9698b8810fe68f60",
            "corpusId": 738850,
            "url": "https://www.semanticscholar.org/paper/bd7bd1d2945a58cdcc1797ba9698b8810fe68f60",
            "title": "Ask Your Neurons: A Neural-Based Approach to Answering Questions about Images",
            "venue": "IEEE International Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:7654260e-79f9-45c5-9663-d72027cf88f3",
                "name": "IEEE International Conference on Computer Vision",
                "alternate_names": [
                    "ICCV",
                    "IEEE Int Conf Comput Vis",
                    "ICCV Workshops",
                    "ICCV Work"
                ],
                "issn": null,
                "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
            },
            "year": 2015,
            "externalIds": {
                "DBLP": "conf/iccv/MalinowskiRF15",
                "ArXiv": "1505.01121",
                "MAG": "2142192571",
                "DOI": "10.1109/ICCV.2015.9",
                "CorpusId": 738850
            },
            "abstract": "We address a question answering task on real-world images that is set up as a Visual Turing Test. By combining latest advances in image representation and natural language processing, we propose Neural-Image-QA, an end-to-end formulation to this problem for which all parts are trained jointly. In contrast to previous efforts, we are facing a multi-modal problem where the language output (answer) is conditioned on visual and natural language input (image and question). Our approach Neural-Image-QA doubles the performance of the previous best approach on this problem. We provide additional insights into the problem by analyzing how much information is contained only in the language part for which we provide a new human baseline. To study human consensus, which is related to the ambiguities inherent in this challenging task, we propose two novel metrics and collect additional answers which extends the original DAQUAR dataset to DAQUAR-Consensus.",
            "referenceCount": 39,
            "citationCount": 566,
            "influentialCitationCount": 49,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/1505.01121",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-05-05",
            "journal": {
                "name": "2015 IEEE International Conference on Computer Vision (ICCV)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Malinowski2015AskYN,\n author = {Mateusz Malinowski and Marcus Rohrbach and Mario Fritz},\n booktitle = {IEEE International Conference on Computer Vision},\n journal = {2015 IEEE International Conference on Computer Vision (ICCV)},\n pages = {1-9},\n title = {Ask Your Neurons: A Neural-Based Approach to Answering Questions about Images},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a97fe719f2f1e62169320e1c45e3f96e094ccd19",
            "@type": "ScholarlyArticle",
            "paperId": "a97fe719f2f1e62169320e1c45e3f96e094ccd19",
            "corpusId": 207206865,
            "url": "https://www.semanticscholar.org/paper/a97fe719f2f1e62169320e1c45e3f96e094ccd19",
            "title": "Improving efficiency and accuracy in multilingual entity extraction",
            "venue": "International Conference on Semantic Systems",
            "publicationVenue": {
                "id": "urn:research:4360f990-6268-434d-ad80-56727c789ee0",
                "name": "International Conference on Semantic Systems",
                "alternate_names": [
                    "SEMANTICS",
                    "I-SEMANTICS",
                    "Int Conf Semantic Syst"
                ],
                "issn": null,
                "url": "http://i-semantics.tugraz.at/"
            },
            "year": 2013,
            "externalIds": {
                "DBLP": "conf/i-semantics/DaiberJHM13",
                "MAG": "2026810221",
                "DOI": "10.1145/2506182.2506198",
                "CorpusId": 207206865
            },
            "abstract": "There has recently been an increased interest in named entity recognition and disambiguation systems at major conferences such as WWW, SIGIR, ACL, KDD, etc. However, most work has focused on algorithms and evaluations, leaving little space for implementation details. In this paper, we discuss some implementation and data processing challenges we encountered while developing a new multilingual version of DBpedia Spotlight that is faster, more accurate and easier to configure. We compare our solution to the previous system, considering time performance, space requirements and accuracy in the context of the Dutch and English languages. Additionally, we report results for 9 additional languages among the largest Wikipedias. Finally, we present challenges and experiences to foment the discussion with other developers interested in recognition and disambiguation of entities in natural language text.",
            "referenceCount": 5,
            "citationCount": 551,
            "influentialCitationCount": 93,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2013-09-04",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Daiber2013ImprovingEA,\n author = {Joachim Daiber and Max Jakob and Chris Hokamp and Pablo N. Mendes},\n booktitle = {International Conference on Semantic Systems},\n pages = {121-124},\n title = {Improving efficiency and accuracy in multilingual entity extraction},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:dac91333674c45b4ab5f3b27c6d2aa6fae8ac229",
            "@type": "ScholarlyArticle",
            "paperId": "dac91333674c45b4ab5f3b27c6d2aa6fae8ac229",
            "corpusId": 6430811,
            "url": "https://www.semanticscholar.org/paper/dac91333674c45b4ab5f3b27c6d2aa6fae8ac229",
            "title": "Local and Global Algorithms for Disambiguation to Wikipedia",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                "name": "Annual Meeting of the Association for Computational Linguistics",
                "alternate_names": [
                    "Annu Meet Assoc Comput Linguistics",
                    "Meeting of the Association for Computational Linguistics",
                    "ACL",
                    "Meet Assoc Comput Linguistics"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/anthology/venues/acl/"
            },
            "year": 2011,
            "externalIds": {
                "DBLP": "conf/acl/RatinovRDA11",
                "ACL": "P11-1138",
                "MAG": "2151048449",
                "CorpusId": 6430811
            },
            "abstract": "Disambiguating concepts and entities in a context sensitive way is a fundamental problem in natural language processing. The comprehensiveness of Wikipedia has made the online encyclopedia an increasingly popular target for disambiguation. Disambiguation to Wikipedia is similar to a traditional Word Sense Disambiguation task, but distinct in that the Wikipedia link structure provides additional information about which disambiguations are compatible. In this work we analyze approaches that utilize this information to arrive at coherent sets of disambiguations for a given document (which we call \"global\" approaches), and compare them to more traditional (local) approaches. We show that previous approaches for global disambiguation can be improved, but even then the local disambiguation provides a baseline which is very hard to beat.",
            "referenceCount": 19,
            "citationCount": 721,
            "influentialCitationCount": 132,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2011-06-19",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Ratinov2011LocalAG,\n author = {Lev-Arie Ratinov and D. Roth and Doug Downey and Mike Anderson},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {1375-1384},\n title = {Local and Global Algorithms for Disambiguation to Wikipedia},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8648dbfff9662fa9c62a95622712dd2951b5b3a3",
            "@type": "ScholarlyArticle",
            "paperId": "8648dbfff9662fa9c62a95622712dd2951b5b3a3",
            "corpusId": 2618014,
            "url": "https://www.semanticscholar.org/paper/8648dbfff9662fa9c62a95622712dd2951b5b3a3",
            "title": "The Design for the Wall Street Journal-based CSR Corpus",
            "venue": "Human Language Technology - The Baltic Perspectiv",
            "publicationVenue": {
                "id": "urn:research:f8e3f8d0-0f40-48c0-b3c0-0c540237b859",
                "name": "Human Language Technology - The Baltic Perspectiv",
                "alternate_names": [
                    "Human Language Technology",
                    "HLT",
                    "Hum Lang Technol",
                    "Hum Lang Technol  Balt Perspect"
                ],
                "issn": null,
                "url": null
            },
            "year": 1992,
            "externalIds": {
                "MAG": "2024490156",
                "DBLP": "conf/naacl/PaulB92",
                "ACL": "H92-1073",
                "DOI": "10.21437/ICSLP.1992-277",
                "CorpusId": 2618014
            },
            "abstract": "The DARPA Spoken Language System (SLS) community has long taken a leadership position in designing, implementing, and globally distributing significant speech corpora widely used for advancing speech recognition research. The Wall Street Journal (WSJ) CSR Corpus described here is the newest addition to this valuable set of resources. In contrast to previous corpora, the WSJ corpus will provide DARPA its first general-purpose English, large vocabulary, natural language, high perplexity, corpus containing significant quantities of both speech data (400 hrs.) and text data (47M words), thereby providing a means to integrate speech recognition and natural language processing in application domains with high potential practical value. This paper presents the motivating goals, acoustic data design, text processing steps, lexicons, and testing paradigms incorporated into the multi-faceted WSJ CSR Corpus.",
            "referenceCount": 23,
            "citationCount": 1406,
            "influentialCitationCount": 175,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://dl.acm.org/ft_gateway.cfm?id=1075614&type=pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1992-02-23",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Paul1992TheDF,\n author = {D. Paul and J. Baker},\n booktitle = {Human Language Technology - The Baltic Perspectiv},\n pages = {899-902},\n title = {The Design for the Wall Street Journal-based CSR Corpus},\n year = {1992}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:248108c458314fdad417171efadaccbe3e83619d",
            "@type": "ScholarlyArticle",
            "paperId": "248108c458314fdad417171efadaccbe3e83619d",
            "corpusId": 7662734,
            "url": "https://www.semanticscholar.org/paper/248108c458314fdad417171efadaccbe3e83619d",
            "title": "Brain potentials indicate immediate use of prosodic cues in natural speech processing",
            "venue": "Nature Neuroscience",
            "publicationVenue": {
                "id": "urn:research:7892f01e-f701-4d07-8c36-e108b84ec6ab",
                "name": "Nature Neuroscience",
                "alternate_names": [
                    "Nat Neurosci"
                ],
                "issn": "1097-6256",
                "url": "http://www.nature.com/neuro/"
            },
            "year": 1999,
            "externalIds": {
                "MAG": "1550452347",
                "DOI": "10.1038/5757",
                "CorpusId": 7662734,
                "PubMed": "10195205"
            },
            "abstract": null,
            "referenceCount": 39,
            "citationCount": 484,
            "influentialCitationCount": 38,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1999-02-01",
            "journal": {
                "name": "Nature Neuroscience",
                "volume": "2"
            },
            "citationStyles": {
                "bibtex": "@Article{Steinhauer1999BrainPI,\n author = {Karsten Steinhauer and K. Alter and A. Friederici},\n booktitle = {Nature Neuroscience},\n journal = {Nature Neuroscience},\n pages = {191-196},\n title = {Brain potentials indicate immediate use of prosodic cues in natural speech processing},\n volume = {2},\n year = {1999}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:91b21ce88d97ce37a70c8c5d725c35b5ba466638",
            "@type": "ScholarlyArticle",
            "paperId": "91b21ce88d97ce37a70c8c5d725c35b5ba466638",
            "corpusId": 219401607,
            "url": "https://www.semanticscholar.org/paper/91b21ce88d97ce37a70c8c5d725c35b5ba466638",
            "title": "Unsupervised Translation of Programming Languages",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3033638351",
                "ArXiv": "2006.03511",
                "DBLP": "journals/corr/abs-2006-03511",
                "CorpusId": 219401607
            },
            "abstract": "A transcompiler, also known as source-to-source translator, is a system that converts source code from a high-level programming language (such as C++ or Python) to another. Transcompilers are primarily used for interoperability, and to port codebases written in an obsolete or deprecated language (e.g. COBOL, Python 2) to a modern one. They typically rely on handcrafted rewrite rules, applied to the source code abstract syntax tree. Unfortunately, the resulting translations often lack readability, fail to respect the target language conventions, and require manual modifications in order to work properly. The overall translation process is timeconsuming and requires expertise in both the source and target languages, making code-translation projects expensive. Although neural models significantly outperform their rule-based counterparts in the context of natural language translation, their applications to transcompilation have been limited due to the scarcity of parallel data in this domain. In this paper, we propose to leverage recent approaches in unsupervised machine translation to train a fully unsupervised neural transcompiler. We train our model on source code from open source GitHub projects, and show that it can translate functions between C++, Java, and Python with high accuracy. Our method relies exclusively on monolingual source code, requires no expertise in the source or target languages, and can easily be generalized to other programming languages. We also build and release a test set composed of 852 parallel functions, along with unit tests to check the correctness of translations. We show that our model outperforms rule-based commercial baselines by a significant margin.",
            "referenceCount": 53,
            "citationCount": 253,
            "influentialCitationCount": 38,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-06-05",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2006.03511"
            },
            "citationStyles": {
                "bibtex": "@Article{Lachaux2020UnsupervisedTO,\n author = {Marie-Anne Lachaux and Baptiste Rozi\u00e8re and L. Chanussot and Guillaume Lample},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Unsupervised Translation of Programming Languages},\n volume = {abs/2006.03511},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d2044ca37a948fc34ea1f3f87e9090ec8bda4a33",
            "@type": "ScholarlyArticle",
            "paperId": "d2044ca37a948fc34ea1f3f87e9090ec8bda4a33",
            "corpusId": 5431215,
            "url": "https://www.semanticscholar.org/paper/d2044ca37a948fc34ea1f3f87e9090ec8bda4a33",
            "title": "Techniques for automatically correcting words in text",
            "venue": "CSUR",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1992,
            "externalIds": {
                "DBLP": "journals/csur/Kukich92",
                "MAG": "2010595692",
                "DOI": "10.1145/146370.146380",
                "CorpusId": 5431215
            },
            "abstract": "Research aimed at correcting words in text has focused on three progressively more difficult problems:(1) nonword error detection; (2) isolated-word error correction; and (3) context-dependent work correction. In response to the first problem, efficient pattern-matching and n-gram analysis techniques have been developed for detecting strings that do not appear in a given word list. In response to the second problem, a variety of general and application-specific spelling correction techniques have been developed. Some of them were based on detailed studies of spelling error patterns. In response to the third problem, a few experiments using natural-language-processing tools or statistical-language models have been carried out. This article surveys documented findings on spelling error patterns, provides descriptions of various nonword detection and isolated-word error correction techniques, reviews the state of the art of context-dependent word correction techniques, and discusses research issues related to all three areas of automatic error correction in text.",
            "referenceCount": 190,
            "citationCount": 1380,
            "influentialCitationCount": 100,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/146370.146380",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "1992-12-01",
            "journal": {
                "name": "ACM Comput. Surv.",
                "volume": "24"
            },
            "citationStyles": {
                "bibtex": "@Article{Kukich1992TechniquesFA,\n author = {K. Kukich},\n booktitle = {CSUR},\n journal = {ACM Comput. Surv.},\n pages = {377-439},\n title = {Techniques for automatically correcting words in text},\n volume = {24},\n year = {1992}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a87ca203325c7c2a57e319e069a36afde4a7d46a",
            "@type": "ScholarlyArticle",
            "paperId": "a87ca203325c7c2a57e319e069a36afde4a7d46a",
            "corpusId": 8959407,
            "url": "https://www.semanticscholar.org/paper/a87ca203325c7c2a57e319e069a36afde4a7d46a",
            "title": "The Meaning of Intonational Contours in the Interpretation of Discourse",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1990,
            "externalIds": {
                "MAG": "2967426412",
                "DOI": "10.7916/D8KD24FP",
                "CorpusId": 8959407
            },
            "abstract": "Recent investigations of the contribution that intonation makes to overall utterance and discourse interpretation promise new sources of information for the investigation of long-time concerns in natural-language processing. In Hirschberg and Pierrehumbert 1986 we proposed that intonational fea tures such as phrasing, accmt pUuttTWIt, pilch rangt",
            "referenceCount": 45,
            "citationCount": 1466,
            "influentialCitationCount": 102,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://academiccommons.columbia.edu/doi/10.7916/D8FN1CRD/download",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1990-06-01",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Pierrehumbert1990TheMO,\n author = {J. Pierrehumbert and Julia Hirschberg},\n pages = {271-311},\n title = {The Meaning of Intonational Contours in the Interpretation of Discourse},\n year = {1990}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ea40fba2dc52bde32dedd45b05f83d27dc2ea6c7",
            "@type": "ScholarlyArticle",
            "paperId": "ea40fba2dc52bde32dedd45b05f83d27dc2ea6c7",
            "corpusId": 6205777,
            "url": "https://www.semanticscholar.org/paper/ea40fba2dc52bde32dedd45b05f83d27dc2ea6c7",
            "title": "Tokenizing, POS Tagging, Lemmatizing and Parsing UD 2.0 with UDPipe",
            "venue": "Conference on Computational Natural Language Learning",
            "publicationVenue": {
                "id": "urn:research:3779a5a7-9119-4f69-84fe-f7eef193eb49",
                "name": "Conference on Computational Natural Language Learning",
                "alternate_names": [
                    "CoNLL",
                    "Conf Comput Nat Lang Learn"
                ],
                "issn": null,
                "url": null
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "conf/conll/StrakaS17",
                "MAG": "2741029840",
                "ACL": "K17-3009",
                "DOI": "10.18653/v1/K17-3009",
                "CorpusId": 6205777
            },
            "abstract": "Many natural language processing tasks, including the most advanced ones, routinely start by several basic processing steps \u2013 tokenization and segmentation, most likely also POS tagging and lemmatization, and commonly parsing as well. A multilingual pipeline performing these steps can be trained using the Universal Dependencies project, which contains annotations of the described tasks for 50 languages in the latest release UD 2.0. We present an update to UDPipe, a simple-to-use pipeline processing CoNLL-U version 2.0 files, which performs these tasks for multiple languages without requiring additional external data. We provide models for all 50 languages of UD 2.0, and furthermore, the pipeline can be trained easily using data in CoNLL-U format. UDPipe is a standalone application in C++, with bindings available for Python, Java, C# and Perl. In the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, UDPipe was the eight best system, while achieving low running times and moderately sized models.",
            "referenceCount": 28,
            "citationCount": 518,
            "influentialCitationCount": 102,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/K17-3009.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Straka2017TokenizingPT,\n author = {Milan Straka and Jana Strakov\u00e1},\n booktitle = {Conference on Computational Natural Language Learning},\n pages = {88-99},\n title = {Tokenizing, POS Tagging, Lemmatizing and Parsing UD 2.0 with UDPipe},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:47f1eb0dc42189ba7cf21b76598c8217eb1b6e05",
            "@type": "ScholarlyArticle",
            "paperId": "47f1eb0dc42189ba7cf21b76598c8217eb1b6e05",
            "corpusId": 203591519,
            "url": "https://www.semanticscholar.org/paper/47f1eb0dc42189ba7cf21b76598c8217eb1b6e05",
            "title": "Learning the Difference that Makes a Difference with Counterfactually-Augmented Data",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/corr/abs-1909-12434",
                "ArXiv": "1909.12434",
                "MAG": "2977235550",
                "CorpusId": 203591519
            },
            "abstract": "Despite alarm over the reliance of machine learning systems on so-called spurious patterns, the term lacks coherent meaning in standard statistical frameworks. However, the language of causality offers clarity: spurious associations are due to confounding (e.g., a common cause), but not direct or indirect causal effects. In this paper, we focus on natural language processing, introducing methods and resources for training models less sensitive to spurious patterns. Given documents and their initial labels, we task humans with revising each document so that it (i) accords with a counterfactual target label; (ii) retains internal coherence; and (iii) avoids unnecessary changes. Interestingly, on sentiment analysis and natural language inference tasks, classifiers trained on original data fail on their counterfactually-revised counterparts and vice versa. Classifiers trained on combined datasets perform remarkably well, just shy of those specialized to either domain. While classifiers trained on either original or manipulated data alone are sensitive to spurious features (e.g., mentions of genre), models trained on the combined data are less sensitive to this signal. Both datasets are publicly available.",
            "referenceCount": 39,
            "citationCount": 449,
            "influentialCitationCount": 73,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-09-26",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1909.12434"
            },
            "citationStyles": {
                "bibtex": "@Article{Kaushik2019LearningTD,\n author = {Divyansh Kaushik and E. Hovy and Zachary Chase Lipton},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Learning the Difference that Makes a Difference with Counterfactually-Augmented Data},\n volume = {abs/1909.12434},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:af8dc425bbc7eff3e0f814a062ce6fbe7317e0bf",
            "@type": "ScholarlyArticle",
            "paperId": "af8dc425bbc7eff3e0f814a062ce6fbe7317e0bf",
            "corpusId": 60654417,
            "url": "https://www.semanticscholar.org/paper/af8dc425bbc7eff3e0f814a062ce6fbe7317e0bf",
            "title": "Algorithms on strings",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2007,
            "externalIds": {
                "MAG": "2020379242",
                "DBLP": "books/daglib/0020103",
                "DOI": "10.1017/CBO9780511546853",
                "CorpusId": 60654417
            },
            "abstract": "Describing algorithms in a C-like language, this text presents examples related to the automatic processing of natural language, to the analysis of molecular sequences and to the management of textual databases.",
            "referenceCount": 0,
            "citationCount": 927,
            "influentialCitationCount": 70,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Chemistry",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2007-04-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Crochemore2007AlgorithmsOS,\n author = {M. Crochemore and C. Hancart and T. Lecroq},\n pages = {I-VIII, 1-383},\n title = {Algorithms on strings},\n year = {2007}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d57212cbbc3cd44179b782256173149aaeb5b1b9",
            "@type": "ScholarlyArticle",
            "paperId": "d57212cbbc3cd44179b782256173149aaeb5b1b9",
            "corpusId": 14001233,
            "url": "https://www.semanticscholar.org/paper/d57212cbbc3cd44179b782256173149aaeb5b1b9",
            "title": "Synchronous data flow",
            "venue": "Proceedings of the IEEE",
            "publicationVenue": {
                "id": "urn:research:6faaccca-1cc4-45a9-aeb6-96a4901d2606",
                "name": "Proceedings of the IEEE",
                "alternate_names": [
                    "Proc IEEE"
                ],
                "issn": "0018-9219",
                "url": "http://www.ieee.org/portal/pages/pubs/proceedings/"
            },
            "year": 1987,
            "externalIds": {
                "MAG": "2091158003",
                "DBLP": "journals/pieee/LeeM87",
                "DOI": "10.1109/PROC.1987.13876",
                "CorpusId": 14001233
            },
            "abstract": "Data flow is a natural paradigm for describing DSP applications for concurrent implementation on parallel hardware. Data flow programs for signal processing are directed graphs where each node represents a function and each arc represents a signal path. Synchronous data flow (SDF) is a special case of data flow (either atomic or large grain) in which the number of data samples produced or consumed by each node on each invocation is specified a priori. Nodes can be scheduled statically (at compile time) onto single or parallel programmable processors so the run-time overhead usually associated with data flow evaporates. Multiple sample rates within the same system are easily and naturally handled. Conditions for correctness of SDF graph are explained and scheduling algorithms are described for homogeneous parallel processors sharing memory. A preliminary SDF software system for automatically generating assembly language code for DSP microcomputers is described. Two new efficiency techniques are introduced, static buffering and an extension to SDF to efficiently implement conditionals.",
            "referenceCount": 56,
            "citationCount": 2044,
            "influentialCitationCount": 297,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1987-09-01",
            "journal": {
                "name": "Proceedings of the IEEE",
                "volume": "75"
            },
            "citationStyles": {
                "bibtex": "@Article{Lee1987SynchronousDF,\n author = {Edward A. Lee and D. Messerschmitt},\n booktitle = {Proceedings of the IEEE},\n journal = {Proceedings of the IEEE},\n pages = {1235-1245},\n title = {Synchronous data flow},\n volume = {75},\n year = {1987}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4f0a8cad6d6a8d0397ad1bd35acce6458aa7164c",
            "@type": "ScholarlyArticle",
            "paperId": "4f0a8cad6d6a8d0397ad1bd35acce6458aa7164c",
            "corpusId": 222291214,
            "url": "https://www.semanticscholar.org/paper/4f0a8cad6d6a8d0397ad1bd35acce6458aa7164c",
            "title": "Contrastive Representation Learning: A Framework and Review",
            "venue": "IEEE Access",
            "publicationVenue": {
                "id": "urn:research:2633f5b2-c15c-49fe-80f5-07523e770c26",
                "name": "IEEE Access",
                "alternate_names": null,
                "issn": "2169-3536",
                "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3096655658",
                "ArXiv": "2010.05113",
                "DBLP": "journals/access/Le-KhacHS20",
                "DOI": "10.1109/ACCESS.2020.3031549",
                "CorpusId": 222291214
            },
            "abstract": "Contrastive Learning has recently received interest due to its success in self-supervised representation learning in the computer vision domain. However, the origins of Contrastive Learning date as far back as the 1990s and its development has spanned across many fields and domains including Metric Learning and natural language processing. In this paper, we provide a comprehensive literature review and we propose a general Contrastive Representation Learning framework that simplifies and unifies many different contrastive learning methods. We also provide a taxonomy for each of the components of contrastive learning in order to summarise it and distinguish it from other forms of machine learning. We then discuss the inductive biases which are present in any contrastive learning system and we analyse our framework under different views from various sub-fields of Machine Learning. Examples of how contrastive learning has been applied in computer vision, natural language processing, audio processing, and others, as well as in Reinforcement Learning are also presented. Finally, we discuss the challenges and some of the most promising future research directions ahead.",
            "referenceCount": 142,
            "citationCount": 368,
            "influentialCitationCount": 11,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/6287639/8948470/09226466.pdf",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2020-10-10",
            "journal": {
                "name": "IEEE Access",
                "volume": "8"
            },
            "citationStyles": {
                "bibtex": "@Article{Kh\u1eafc2020ContrastiveRL,\n author = {Ph\u00fac H. L\u00ea Kh\u1eafc and G. Healy and A. Smeaton},\n booktitle = {IEEE Access},\n journal = {IEEE Access},\n pages = {193907-193934},\n title = {Contrastive Representation Learning: A Framework and Review},\n volume = {8},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:476029ac9be26bf7f121a388f5c1e45d204efe52",
            "@type": "ScholarlyArticle",
            "paperId": "476029ac9be26bf7f121a388f5c1e45d204efe52",
            "corpusId": 67855472,
            "url": "https://www.semanticscholar.org/paper/476029ac9be26bf7f121a388f5c1e45d204efe52",
            "title": "BERT for Joint Intent Classification and Slot Filling",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2917128112",
                "DBLP": "journals/corr/abs-1902-10909",
                "ArXiv": "1902.10909",
                "CorpusId": 67855472
            },
            "abstract": "Intent classification and slot filling are two essential tasks for natural language understanding. They often suffer from small-scale human-labeled training data, resulting in poor generalization capability, especially for rare words. Recently a new language representation model, BERT (Bidirectional Encoder Representations from Transformers), facilitates pre-training deep bidirectional representations on large-scale unlabeled corpora, and has created state-of-the-art models for a wide variety of natural language processing tasks after simple fine-tuning. However, there has not been much effort on exploring BERT for natural language understanding. In this work, we propose a joint intent classification and slot filling model based on BERT. Experimental results demonstrate that our proposed model achieves significant improvement on intent classification accuracy, slot filling F1, and sentence-level semantic frame accuracy on several public benchmark datasets, compared to the attention-based recurrent neural network models and slot-gated models.",
            "referenceCount": 26,
            "citationCount": 430,
            "influentialCitationCount": 79,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-02-28",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1902.10909"
            },
            "citationStyles": {
                "bibtex": "@Article{Chen2019BERTFJ,\n author = {Qian Chen and Zhu Zhuo and Wen Wang},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {BERT for Joint Intent Classification and Slot Filling},\n volume = {abs/1902.10909},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f4a8480cffa491020bdbb8c4c4e7a7e923b1c2c1",
            "@type": "ScholarlyArticle",
            "paperId": "f4a8480cffa491020bdbb8c4c4e7a7e923b1c2c1",
            "corpusId": 202750230,
            "url": "https://www.semanticscholar.org/paper/f4a8480cffa491020bdbb8c4c4e7a7e923b1c2c1",
            "title": "Reducing Transformer Depth on Demand with Structured Dropout",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2996159613",
                "DBLP": "conf/iclr/FanGJ20",
                "ArXiv": "1909.11556",
                "CorpusId": 202750230
            },
            "abstract": "Overparameterized transformer networks have obtained state of the art results in various natural language processing tasks, such as machine translation, language modeling, and question answering. These models contain hundreds of millions of parameters, necessitating a large amount of computation and making them prone to overfitting. In this work, we explore LayerDrop, a form of structured dropout, which has a regularization effect during training and allows for efficient pruning at inference time. In particular, we show that it is possible to select sub-networks of any depth from one large network without having to finetune them and with limited impact on performance. We demonstrate the effectiveness of our approach by improving the state of the art on machine translation, language modeling, summarization, question answering, and language understanding benchmarks. Moreover, we show that our approach leads to small BERT-like models of higher quality compared to training from scratch or using distillation.",
            "referenceCount": 69,
            "citationCount": 435,
            "influentialCitationCount": 70,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-09-25",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1909.11556"
            },
            "citationStyles": {
                "bibtex": "@Article{Fan2019ReducingTD,\n author = {Angela Fan and Edouard Grave and Armand Joulin},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Reducing Transformer Depth on Demand with Structured Dropout},\n volume = {abs/1909.11556},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c41655809f991fa0553cbb8e9a66f184a6cdc154",
            "@type": "ScholarlyArticle",
            "paperId": "c41655809f991fa0553cbb8e9a66f184a6cdc154",
            "corpusId": 224846337,
            "url": "https://www.semanticscholar.org/paper/c41655809f991fa0553cbb8e9a66f184a6cdc154",
            "title": "Resources and benchmark corpora for hate speech detection: a systematic review",
            "venue": "Language Resources and Evaluation",
            "publicationVenue": {
                "id": "urn:research:7dda5bd1-752f-45e5-bc7b-09633096916e",
                "name": "Language Resources and Evaluation",
                "alternate_names": [
                    "Lang Resour Evaluation"
                ],
                "issn": "1574-020X",
                "url": "https://link.springer.com/journal/10579"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3091315987",
                "DBLP": "journals/lre/PolettoBSBP21",
                "DOI": "10.1007/s10579-020-09502-8",
                "CorpusId": 224846337
            },
            "abstract": null,
            "referenceCount": 111,
            "citationCount": 252,
            "influentialCitationCount": 21,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/s10579-020-09502-8.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2020-09-30",
            "journal": {
                "name": "Language Resources and Evaluation",
                "volume": "55"
            },
            "citationStyles": {
                "bibtex": "@Article{Poletto2020ResourcesAB,\n author = {Fabio Poletto and Valerio Basile and M. Sanguinetti and C. Bosco and V. Patti},\n booktitle = {Language Resources and Evaluation},\n journal = {Language Resources and Evaluation},\n pages = {477 - 523},\n title = {Resources and benchmark corpora for hate speech detection: a systematic review},\n volume = {55},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ce20678cfbffded477f43df156f6ab37f6edc6a0",
            "@type": "ScholarlyArticle",
            "paperId": "ce20678cfbffded477f43df156f6ab37f6edc6a0",
            "corpusId": 2896078,
            "url": "https://www.semanticscholar.org/paper/ce20678cfbffded477f43df156f6ab37f6edc6a0",
            "title": "Learning Subjective Language",
            "venue": "International Conference on Computational Logic",
            "publicationVenue": {
                "id": "urn:research:30a8645d-22d4-42e2-b3f6-304bf4ce3a02",
                "name": "International Conference on Computational Logic",
                "alternate_names": [
                    "CL",
                    "Int Conf Comput Log"
                ],
                "issn": null,
                "url": null
            },
            "year": 2004,
            "externalIds": {
                "ACL": "J04-3002",
                "DBLP": "journals/coling/WiebeWBBM04",
                "MAG": "2133341045",
                "DOI": "10.1162/0891201041850885",
                "CorpusId": 2896078
            },
            "abstract": "Subjectivity in natural language refers to aspects of language used to express opinions, evaluations, and speculations. There are numerous natural language processing applications for which subjectivity analysis is relevant, including information extraction and text categorization. The goal of this work is learning subjective language from corpora. Clues of subjectivity are generated and tested, including low-frequency words, collocations, and adjectives and verbs identified using distributional similarity. The features are also examined working together in concert. The features, generated from different data sets using different procedures, exhibit consistency in performance in that they all do better and worse on the same data sets. In addition, this article shows that the density of subjectivity clues in the surrounding context strongly affects how likely it is that a word is subjective, and it provides the results of an annotation study assessing the subjectivity of sentences with high-density features. Finally, the clues are used to perform opinion piece recognition (a type of text categorization and genre detection) to demonstrate the utility of the knowledge acquired in this article.",
            "referenceCount": 103,
            "citationCount": 791,
            "influentialCitationCount": 46,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://direct.mit.edu/coli/article-pdf/30/3/277/1798072/0891201041850885.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2004-09-01",
            "journal": {
                "name": "Computational Linguistics",
                "volume": "30"
            },
            "citationStyles": {
                "bibtex": "@Article{Wiebe2004LearningSL,\n author = {J. Wiebe and Theresa Wilson and Rebecca F. Bruce and Matthew Bell and Melanie J. Martin},\n booktitle = {International Conference on Computational Logic},\n journal = {Computational Linguistics},\n pages = {277-308},\n title = {Learning Subjective Language},\n volume = {30},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8b1b28d95e744202f9ae013a37a429b2855f877e",
            "@type": "ScholarlyArticle",
            "paperId": "8b1b28d95e744202f9ae013a37a429b2855f877e",
            "corpusId": 17760683,
            "url": "https://www.semanticscholar.org/paper/8b1b28d95e744202f9ae013a37a429b2855f877e",
            "title": "Lecture Notes in Artificial Intelligence",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1999,
            "externalIds": {
                "MAG": "26772505",
                "CorpusId": 17760683
            },
            "abstract": "LNAI was established in the mid-1980s as a topical subseries of LNCS focusing on artificial intelligence. This subseries is devoted to the publication of state-of-the-art research results in artificial intelligence, at a high level and in both printed and electronic versions making use of the well-established LNCS publication machinery. As with the LNCS mother series, proceedings and postproceedings are at the core of LNAI; however, all other sublines are available for LNAI as well. The topics in LNAI include automated reasoning, automated programming, algorithms, knowledge representation, agent-based systems, intelligent systems, expert systems, machine learning, natural-language processing, machine vision, robotics, search systems, knowledge discovery, data mining, and related programming languages.",
            "referenceCount": 59,
            "citationCount": 1803,
            "influentialCitationCount": 9,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Br\u00e9zillon1999LectureNI,\n author = {P. Br\u00e9zillon and Paolo Bouquet},\n title = {Lecture Notes in Artificial Intelligence},\n year = {1999}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5328e5dc801d1cf0f997621519f6b31d79021006",
            "@type": "ScholarlyArticle",
            "paperId": "5328e5dc801d1cf0f997621519f6b31d79021006",
            "corpusId": 16177937,
            "url": "https://www.semanticscholar.org/paper/5328e5dc801d1cf0f997621519f6b31d79021006",
            "title": "Sentiment analysis using product review data",
            "venue": "Journal of Big Data",
            "publicationVenue": {
                "id": "urn:research:d60da343-ab92-4310-b3d7-2c0860287a9d",
                "name": "Journal of Big Data",
                "alternate_names": [
                    "J Big Data",
                    "Journal on Big Data"
                ],
                "issn": "2196-1115",
                "url": "http://www.journalofbigdata.com/"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "1572786359",
                "DBLP": "journals/jbd/FangZ15",
                "DOI": "10.1186/s40537-015-0015-2",
                "CorpusId": 16177937
            },
            "abstract": null,
            "referenceCount": 34,
            "citationCount": 568,
            "influentialCitationCount": 23,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://journalofbigdata.springeropen.com/track/pdf/10.1186/s40537-015-0015-2",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2015-06-16",
            "journal": {
                "name": "Journal of Big Data",
                "volume": "2"
            },
            "citationStyles": {
                "bibtex": "@Article{Fang2015SentimentAU,\n author = {Xing Fang and J. Zhan},\n booktitle = {Journal of Big Data},\n journal = {Journal of Big Data},\n pages = {1-14},\n title = {Sentiment analysis using product review data},\n volume = {2},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:251dc3d8dd55281bc6b3281dc891b41b147b0807",
            "@type": "ScholarlyArticle",
            "paperId": "251dc3d8dd55281bc6b3281dc891b41b147b0807",
            "corpusId": 144024177,
            "url": "https://www.semanticscholar.org/paper/251dc3d8dd55281bc6b3281dc891b41b147b0807",
            "title": "TOWARD AN INTERACTIVE-COMPENSATORY MODEL OF INDIVIDUAL DIFFERENCES IN THE DEVELOPMENT OF READING FLUENCY",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1980,
            "externalIds": {
                "MAG": "1989306084",
                "DOI": "10.2307/747348",
                "CorpusId": 144024177
            },
            "abstract": "INTERACTIVE MODELS OF READING appear to provide a more accurate conceptualization of reading performance than do strictly top-down or bottom-up models. When combined with an assumption of compensatory processing (that a deficit in any particular process will result in a greater reliance on other knowledge sources, regardless of their level in the processing hierarchy), interactive models provide a better account of the existing data on the use of orthographic structure and sentence context by good and poor readers. A review of the research literature seems to indicate that, beyond the initial stages of reading acquisition, superior reading ability is not associated with a greater tendency to use the redundancy inherent in natural language to speed word recognition. Instead, general comprehension strategies and rapid context-free word recognition appear to be the processes that most clearly distinguish good from poor readers.",
            "referenceCount": 126,
            "citationCount": 1734,
            "influentialCitationCount": 101,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Education",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": "Reading Research Quarterly",
                "volume": "16"
            },
            "citationStyles": {
                "bibtex": "@Article{Stanovich1980TOWARDAI,\n author = {K. Stanovich},\n journal = {Reading Research Quarterly},\n pages = {32-71},\n title = {TOWARD AN INTERACTIVE-COMPENSATORY MODEL OF INDIVIDUAL DIFFERENCES IN THE DEVELOPMENT OF READING FLUENCY},\n volume = {16},\n year = {1980}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:56676aef356ebb13cba77fc9e4d70760fbc151f5",
            "@type": "ScholarlyArticle",
            "paperId": "56676aef356ebb13cba77fc9e4d70760fbc151f5",
            "corpusId": 221845203,
            "url": "https://www.semanticscholar.org/paper/56676aef356ebb13cba77fc9e4d70760fbc151f5",
            "title": "ETC: Encoding Long and Structured Inputs in Transformers",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "publicationVenue": {
                "id": "urn:research:41bf9ed3-85b3-4c90-b015-150e31690253",
                "name": "Conference on Empirical Methods in Natural Language Processing",
                "alternate_names": [
                    "Empir Method Nat Lang Process",
                    "Empirical Methods in Natural Language Processing",
                    "Conf Empir Method Nat Lang Process",
                    "EMNLP"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/emnlp"
            },
            "year": 2020,
            "externalIds": {
                "ACL": "2020.emnlp-main.19",
                "MAG": "3105238007",
                "DBLP": "conf/emnlp/AinslieOACFPRSW20",
                "DOI": "10.18653/v1/2020.emnlp-main.19",
                "CorpusId": 221845203
            },
            "abstract": "Transformer models have advanced the state of the art in many Natural Language Processing (NLP) tasks. In this paper, we present a new Transformer architecture, Extended Transformer Construction (ETC), that addresses two key challenges of standard Transformer architectures, namely scaling input length and encoding structured inputs. To scale attention to longer inputs, we introduce a novel global-local attention mechanism between global tokens and regular input tokens. We also show that combining global-local attention with relative position encodings and a Contrastive Predictive Coding (CPC) pre-training objective allows ETC to encode structured inputs. We achieve state-of-the-art results on four natural language datasets requiring long and/or structured inputs.",
            "referenceCount": 47,
            "citationCount": 245,
            "influentialCitationCount": 24,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/2020.emnlp-main.19.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-04-17",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Ainslie2020ETCEL,\n author = {J. Ainslie and Santiago Onta\u00f1\u00f3n and Chris Alberti and V. Cvicek and Zachary Kenneth Fisher and Philip Pham and Anirudh Ravula and Sumit K. Sanghai and Qifan Wang and Li Yang},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {268-284},\n title = {ETC: Encoding Long and Structured Inputs in Transformers},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c20c68c45127439139a08adb0b1f2b8354a94d6c",
            "@type": "ScholarlyArticle",
            "paperId": "c20c68c45127439139a08adb0b1f2b8354a94d6c",
            "corpusId": 207870323,
            "url": "https://www.semanticscholar.org/paper/c20c68c45127439139a08adb0b1f2b8354a94d6c",
            "title": "CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data",
            "venue": "International Conference on Language Resources and Evaluation",
            "publicationVenue": {
                "id": "urn:research:7474c4a0-75d9-4105-9809-8e7d5201c5e1",
                "name": "International Conference on Language Resources and Evaluation",
                "alternate_names": [
                    "LREC",
                    "Int Conf Lang Resour Evaluation"
                ],
                "issn": null,
                "url": "http://www.lrec-conf.org/"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/corr/abs-1911-00359",
                "MAG": "2989539713",
                "ACL": "2020.lrec-1.494",
                "ArXiv": "1911.00359",
                "CorpusId": 207870323
            },
            "abstract": "Pre-training text representations have led to significant improvements in many areas of natural language processing. The quality of these models benefits greatly from the size of the pretraining corpora as long as its quality is preserved. In this paper, we describe an automatic pipeline to extract massive high-quality monolingual datasets from Common Crawl for a variety of languages. Our pipeline follows the data processing introduced in fastText (Mikolov et al., 2017; Grave et al., 2018), that deduplicates documents and identifies their language. We augment this pipeline with a filtering step to select documents that are close to high quality corpora like Wikipedia.",
            "referenceCount": 21,
            "citationCount": 376,
            "influentialCitationCount": 43,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-11-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Wenzek2019CCNetEH,\n author = {Guillaume Wenzek and Marie-Anne Lachaux and Alexis Conneau and Vishrav Chaudhary and Francisco Guzm'an and Armand Joulin and Edouard Grave},\n booktitle = {International Conference on Language Resources and Evaluation},\n pages = {4003-4012},\n title = {CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1945711ed147087a65cf4ab163b8e21c38705273",
            "@type": "ScholarlyArticle",
            "paperId": "1945711ed147087a65cf4ab163b8e21c38705273",
            "corpusId": 53223972,
            "url": "https://www.semanticscholar.org/paper/1945711ed147087a65cf4ab163b8e21c38705273",
            "title": "tidytext: Text Mining and Analysis Using Tidy Data Principles in R",
            "venue": "Journal of Open Source Software",
            "publicationVenue": {
                "id": "urn:research:1236e136-01b7-42d5-8c4a-593153a3ab37",
                "name": "Journal of Open Source Software",
                "alternate_names": [
                    "The Journal of Open Source Software",
                    "J Open Source Softw"
                ],
                "issn": "2475-9066",
                "url": "https://joss.theoj.org/"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "journals/jossw/SilgeR16",
                "MAG": "2465750682",
                "DOI": "10.21105/JOSS.00037",
                "CorpusId": 53223972
            },
            "abstract": "Tidy data sets allow manipulation with a standard set of \u201ctidy\u201d tools, including popular packages such as dplyr (Wickham, Francois, and RStudio 2015), ggplot2 (Wickham, Chang, and RStudio 2016), and broom (Robinson et al. 2015). These tools do not yet, however, have the infrastructure to work fluently with text data and natural language processing tools. In developing this package, we provide functions and supporting data sets to allow conversion of text to and from tidy formats, and to switch seamlessly between tidy tools and existing text mining packages.",
            "referenceCount": 8,
            "citationCount": 549,
            "influentialCitationCount": 20,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.theoj.org/joss-papers/joss.00037/10.21105.joss.00037.pdf",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-07-11",
            "journal": {
                "name": "J. Open Source Softw.",
                "volume": "1"
            },
            "citationStyles": {
                "bibtex": "@Article{Silge2016tidytextTM,\n author = {Julia Silge and David Robinson},\n booktitle = {Journal of Open Source Software},\n journal = {J. Open Source Softw.},\n pages = {37},\n title = {tidytext: Text Mining and Analysis Using Tidy Data Principles in R},\n volume = {1},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b672ef69f60aea81220d658963445c41e60bb0e3",
            "@type": "ScholarlyArticle",
            "paperId": "b672ef69f60aea81220d658963445c41e60bb0e3",
            "corpusId": 15036406,
            "url": "https://www.semanticscholar.org/paper/b672ef69f60aea81220d658963445c41e60bb0e3",
            "title": "Instance Weighting for Domain Adaptation in NLP",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                "name": "Annual Meeting of the Association for Computational Linguistics",
                "alternate_names": [
                    "Annu Meet Assoc Comput Linguistics",
                    "Meeting of the Association for Computational Linguistics",
                    "ACL",
                    "Meet Assoc Comput Linguistics"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/anthology/venues/acl/"
            },
            "year": 2007,
            "externalIds": {
                "DBLP": "conf/acl/JiangZ07",
                "ACL": "P07-1034",
                "MAG": "2111362445",
                "CorpusId": 15036406
            },
            "abstract": "Domain adaptation is an important problem in natural language processing (NLP) due to the lack of labeled data in novel domains. In this paper, we study the domain adaptation problem from the instance weighting perspective. We formally analyze and characterize the domain adaptation problem from a distributional view, and show that there are two distinct needs for adaptation, corresponding to the different distributions of instances and classification functions in the source and the target domains. We then propose a general instance weighting framework for domain adaptation. Our empirical results on three NLP tasks show that incorporating and exploiting more information from the target domain through instance weighting is effective.",
            "referenceCount": 8,
            "citationCount": 863,
            "influentialCitationCount": 46,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2007-12-01",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Article{Jiang2007InstanceWF,\n author = {Jing Jiang and ChengXiang Zhai},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {264-271},\n title = {Instance Weighting for Domain Adaptation in NLP},\n year = {2007}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:eb606d9ce65139754232cee62f6ab77f3e0c665f",
            "@type": "ScholarlyArticle",
            "paperId": "eb606d9ce65139754232cee62f6ab77f3e0c665f",
            "corpusId": 198967997,
            "url": "https://www.semanticscholar.org/paper/eb606d9ce65139754232cee62f6ab77f3e0c665f",
            "title": "Leveraging Pre-trained Checkpoints for Sequence Generation Tasks",
            "venue": "Transactions of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:e0dbf116-86aa-418d-859f-a49952d7e44a",
                "name": "Transactions of the Association for Computational Linguistics",
                "alternate_names": [
                    "Trans Assoc Comput Linguistics",
                    "TACL"
                ],
                "issn": "2307-387X",
                "url": "https://www.mitpressjournals.org/loi/tacl"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2964910501",
                "DBLP": "journals/tacl/RotheNS20",
                "ArXiv": "1907.12461",
                "DOI": "10.1162/tacl_a_00313",
                "CorpusId": 198967997
            },
            "abstract": "Abstract Unsupervised pre-training of large neural models has recently revolutionized Natural Language Processing. By warm-starting from the publicly released checkpoints, NLP practitioners have pushed the state-of-the-art on multiple benchmarks while saving significant amounts of compute time. So far the focus has been mainly on the Natural Language Understanding tasks. In this paper, we demonstrate the efficacy of pre-trained checkpoints for Sequence Generation. We developed a Transformer-based sequence-to-sequence model that is compatible with publicly available pre-trained BERT, GPT-2, and RoBERTa checkpoints and conducted an extensive empirical study on the utility of initializing our model, both encoder and decoder, with these checkpoints. Our models result in new state-of-the-art results on Machine Translation, Text Summarization, Sentence Splitting, and Sentence Fusion.",
            "referenceCount": 67,
            "citationCount": 345,
            "influentialCitationCount": 43,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00313/1923422/tacl_a_00313.pdf",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-07-29",
            "journal": {
                "name": "Transactions of the Association for Computational Linguistics",
                "volume": "8"
            },
            "citationStyles": {
                "bibtex": "@Article{Rothe2019LeveragingPC,\n author = {S. Rothe and Shashi Narayan and Aliaksei Severyn},\n booktitle = {Transactions of the Association for Computational Linguistics},\n journal = {Transactions of the Association for Computational Linguistics},\n pages = {264-280},\n title = {Leveraging Pre-trained Checkpoints for Sequence Generation Tasks},\n volume = {8},\n year = {2019}\n}\n"
            }
        }
    }
]