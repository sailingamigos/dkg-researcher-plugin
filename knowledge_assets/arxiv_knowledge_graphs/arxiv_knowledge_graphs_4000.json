[
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.09507v1",
            "title": "Foundation Ark: Accruing and Reusing Knowledge for Superior and Robust\n  Performance",
            "updated": "2023-10-14T06:31:44Z",
            "published": "2023-10-14T06:31:44Z",
            "summary": "Deep learning nowadays offers expert-level and sometimes even\nsuper-expert-level performance, but achieving such performance demands massive\nannotated data for training (e.g., Google's proprietary CXR Foundation Model\n(CXR-FM) was trained on 821,544 labeled and mostly private chest X-rays\n(CXRs)). Numerous datasets are publicly available in medical imaging but\nindividually small and heterogeneous in expert labels. We envision a powerful\nand robust foundation model that can be trained by aggregating numerous small\npublic datasets. To realize this vision, we have developed Ark, a framework\nthat accrues and reuses knowledge from heterogeneous expert annotations in\nvarious datasets. As a proof of concept, we have trained two Ark models on\n335,484 and 704,363 CXRs, respectively, by merging several datasets including\nChestX-ray14, CheXpert, MIMIC-II, and VinDr-CXR, evaluated them on a wide range\nof imaging tasks covering both classification and segmentation via fine-tuning,\nlinear-probing, and gender-bias analysis, and demonstrated our Ark's superior\nand robust performance over the SOTA fully/self-supervised baselines and\nGoogle's proprietary CXR-FM. This enhanced performance is attributed to our\nsimple yet powerful observation that aggregating numerous public datasets\ndiversifies patient populations and accrues knowledge from diverse experts,\nyielding unprecedented performance yet saving annotation cost. With all codes\nand pretrained models released at GitHub.com/JLiangLab/Ark, we hope that Ark\nexerts an important impact on open science, as accruing and reusing knowledge\nfrom expert annotations in public datasets can potentially surpass the\nperformance of proprietary models trained on unusually large data, inspiring\nmany more researchers worldwide to share codes and datasets to build open\nfoundation models, accelerate open science, and democratize deep learning for\nmedical imaging.",
            "author": [
                "DongAo Ma",
                "Jiaxuan Pang",
                "Michael B. Gotway",
                "Jianming Liang"
            ],
            "link": [
                "http://dx.doi.org/10.1007/978-3-031-43907-0_62",
                "http://arxiv.org/abs/2310.09507v1",
                "http://arxiv.org/pdf/2310.09507v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.09501v1",
            "title": "DepNeCTI: Dependency-based Nested Compound Type Identification for\n  Sanskrit",
            "updated": "2023-10-14T06:11:53Z",
            "published": "2023-10-14T06:11:53Z",
            "summary": "Multi-component compounding is a prevalent phenomenon in Sanskrit, and\nunderstanding the implicit structure of a compound's components is crucial for\ndeciphering its meaning. Earlier approaches in Sanskrit have focused on binary\ncompounds and neglected the multi-component compound setting. This work\nintroduces the novel task of nested compound type identification (NeCTI), which\naims to identify nested spans of a multi-component compound and decode the\nimplicit semantic relations between them. To the best of our knowledge, this is\nthe first attempt in the field of lexical semantics to propose this task.\n  We present 2 newly annotated datasets including an out-of-domain dataset for\nthis task. We also benchmark these datasets by exploring the efficacy of the\nstandard problem formulations such as nested named entity recognition,\nconstituency parsing and seq2seq, etc. We present a novel framework named\nDepNeCTI: Dependency-based Nested Compound Type Identifier that surpasses the\nperformance of the best baseline with an average absolute improvement of 13.1\npoints F1-score in terms of Labeled Span Score (LSS) and a 5-fold enhancement\nin inference efficiency. In line with the previous findings in the binary\nSanskrit compound identification task, context provides benefits for the NeCTI\ntask. The codebase and datasets are publicly available at:\nhttps://github.com/yaswanth-iitkgp/DepNeCTI",
            "author": [
                "Jivnesh Sandhan",
                "Yaswanth Narsupalli",
                "Sreevatsa Muppirala",
                "Sriram Krishnan",
                "Pavankumar Satuluri",
                "Amba Kulkarni",
                "Pawan Goyal"
            ],
            "link": [
                "http://arxiv.org/abs/2310.09501v1",
                "http://arxiv.org/pdf/2310.09501v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.09500v1",
            "title": "Algebraic connectedness and bipartiteness of quantum graphs",
            "updated": "2023-10-14T05:55:35Z",
            "published": "2023-10-14T05:55:35Z",
            "summary": "Connectedness and bipartiteness are basic properties of classical graphs, and\nthe purpose of this paper is to investigate the case of quantum graphs. We\nintroduce the notion of connectedness and bipartiteness of quantum graphs in\nterms of graph homomorphisms. This paper shows that regular tracial quantum\ngraphs have the same algebraic characterization of connectedness and\nbipartiteness as classical graphs. We also prove the equivalence between\nbipartiteness and two-colorability of quantum graphs by comparing two notions\nof graph homomorphisms respecting adjacency matrices or edge spaces. In\nparticular, all kinds of quantum two-colorability are mutually equivalent for\nregular connected tracial quantum graphs.",
            "author": [
                "Junichiro Matsuda"
            ],
            "link": [
                "http://arxiv.org/abs/2310.09500v1",
                "http://arxiv.org/pdf/2310.09500v1"
            ],
            "primary_category": "math.OA",
            "category": [
                "math.OA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.09486v2",
            "title": "Mirage: Model-Agnostic Graph Distillation for Graph Classification",
            "updated": "2023-10-17T07:55:22Z",
            "published": "2023-10-14T04:21:52Z",
            "summary": "GNNs, like other deep learning models, are data and computation hungry. There\nis a pressing need to scale training of GNNs on large datasets to enable their\nusage on low-resource environments. Graph distillation is an effort in that\ndirection with the aim to construct a smaller synthetic training set from the\noriginal training data without significantly compromising model performance.\nWhile initial efforts are promising, this work is motivated by two key\nobservations: (1) Existing graph distillation algorithms themselves rely on\ntraining with the full dataset, which undermines the very premise of graph\ndistillation. (2) The distillation process is specific to the target GNN\narchitecture and hyper-parameters and thus not robust to changes in the\nmodeling pipeline. We circumvent these limitations by designing a distillation\nalgorithm called Mirage for graph classification. Mirage is built on the\ninsight that a message-passing GNN decomposes the input graph into a multiset\nof computation trees. Furthermore, the frequency distribution of computation\ntrees is often skewed in nature, enabling us to condense this data into a\nconcise distilled summary. By compressing the computation data itself, as\nopposed to emulating gradient flows on the original training set-a prevalent\napproach to date-Mirage transforms into an unsupervised and\narchitecture-agnostic distillation algorithm. Extensive benchmarking on\nreal-world datasets underscores Mirage's superiority, showcasing enhanced\ngeneralization accuracy, data compression, and distillation efficiency when\ncompared to state-of-the-art baselines.",
            "author": [
                "Mridul Gupta",
                "Sahil Manchanda",
                "Hariprasad Kodamana",
                "Sayan Ranu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.09486v2",
                "http://arxiv.org/pdf/2310.09486v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.09461v1",
            "title": "MAC: ModAlity Calibration for Object Detection",
            "updated": "2023-10-14T00:58:32Z",
            "published": "2023-10-14T00:58:32Z",
            "summary": "The flourishing success of Deep Neural Networks(DNNs) on RGB-input perception\ntasks has opened unbounded possibilities for non-RGB-input perception tasks,\nsuch as object detection from wireless signals, lidar scans, and infrared\nimages. Compared to the matured development pipeline of RGB-input (source\nmodality) models, developing non-RGB-input (target-modality) models from\nscratch poses excessive challenges in the modality-specific network\ndesign/training tricks and labor in the target-modality annotation. In this\npaper, we propose ModAlity Calibration (MAC), an efficient pipeline for\ncalibrating target-modality inputs to the DNN object detection models developed\non the RGB (source) modality. We compose a target-modality-input model by\nadding a small calibrator module ahead of a source-modality model and introduce\nMAC training techniques to impose dense supervision on the calibrator. By\nleveraging (1) prior knowledge synthesized from the source-modality model and\n(2) paired {target, source} data with zero manual annotations, our\ntarget-modality models reach comparable or better metrics than baseline models\nthat require 100% manual annotations. We demonstrate the effectiveness of MAC\nby composing the WiFi-input, Lidar-input, and Thermal-Infrared-input models\nupon the pre-trained RGB-input models respectively.",
            "author": [
                "Yutian Lei",
                "Jun Liu",
                "Dong Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.09461v1",
                "http://arxiv.org/pdf/2310.09461v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.MM",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.09460v1",
            "title": "On subgroups of finite classical groups with exactly two orbits on\n  singular or isotropic points",
            "updated": "2023-10-14T00:51:03Z",
            "published": "2023-10-14T00:51:03Z",
            "summary": "In this paper, we classify the groups of semisimilarities of finite classical\npolar spaces with exactly two orbits on the singular or isotropic points. As a\nbyproduct, we obtain many highly symmetric regular sets in the point graphs of\nfinite classical polar spaces.",
            "author": [
                "Tao Feng",
                "Qing Xiang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.09460v1",
                "http://arxiv.org/pdf/2310.09460v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "math.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.10683v1",
            "title": "Large Language Model Unlearning",
            "updated": "2023-10-14T00:32:55Z",
            "published": "2023-10-14T00:32:55Z",
            "summary": "We study how to perform unlearning, i.e. forgetting undesirable\n(mis)behaviors, on large language models (LLMs). We show at least three\nscenarios of aligning LLMs with human preferences can benefit from unlearning:\n(1) removing harmful responses, (2) erasing copyright-protected content as\nrequested, and (3) eliminating hallucinations. Unlearning, as an alignment\ntechnique, has three advantages. (1) It only requires negative (e.g. harmful)\nexamples, which are much easier and cheaper to collect (e.g. via red teaming or\nuser reporting) than positive (e.g. helpful and often human-written) examples\nrequired in RLHF (RL from human feedback). (2) It is computationally efficient.\n(3) It is especially effective when we know which training samples cause the\nmisbehavior. To the best of our knowledge, our work is among the first to\nexplore LLM unlearning. We are also among the first to formulate the settings,\ngoals, and evaluations in LLM unlearning. We show that if practitioners only\nhave limited resources, and therefore the priority is to stop generating\nundesirable outputs rather than to try to generate desirable outputs,\nunlearning is particularly appealing. Despite only having negative samples, our\nablation study shows that unlearning can still achieve better alignment\nperformance than RLHF with just 2% of its computational time.",
            "author": [
                "Yuanshun Yao",
                "Xiaojun Xu",
                "Yang Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.10683v1",
                "http://arxiv.org/pdf/2310.10683v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.09438v1",
            "title": "Relaxed data-consistency for limited bandwidth photoacoustic tomography",
            "updated": "2023-10-13T23:08:59Z",
            "published": "2023-10-13T23:08:59Z",
            "summary": "We study the effect of using weaker forms of data-fidelity terms in\ngeneralized Tikhonov regularization accounting for model uncertainties. We show\nthat relaxed data-consistency conditions can be beneficial for integrating\navailable prior knowledge.",
            "author": [
                "Daniel Obmann",
                "Markus Haltmeier"
            ],
            "link": [
                "http://arxiv.org/abs/2310.09438v1",
                "http://arxiv.org/pdf/2310.09438v1"
            ],
            "primary_category": "math.NA",
            "category": [
                "math.NA",
                "cs.NA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.09436v1",
            "title": "Sub-network Discovery and Soft-masking for Continual Learning of Mixed\n  Tasks",
            "updated": "2023-10-13T23:00:39Z",
            "published": "2023-10-13T23:00:39Z",
            "summary": "Continual learning (CL) has two main objectives: preventing catastrophic\nforgetting (CF) and encouraging knowledge transfer (KT). The existing\nliterature mainly focused on overcoming CF. Some work has also been done on KT\nwhen the tasks are similar. To our knowledge, only one method has been proposed\nto learn a sequence of mixed tasks. However, these techniques still suffer from\nCF and/or limited KT. This paper proposes a new CL method to achieve both. It\novercomes CF by isolating the knowledge of each task via discovering a\nsubnetwork for it. A soft-masking mechanism is also proposed to preserve the\nprevious knowledge and to enable the new task to leverage the past knowledge to\nachieve KT. Experiments using classification, generation, information\nextraction, and their mixture (i.e., heterogeneous tasks) show that the\nproposed method consistently outperforms strong baselines.",
            "author": [
                "Zixuan Ke",
                "Bing Liu",
                "Wenhan Xiong",
                "Asli Celikyilmaz",
                "Haoran Li"
            ],
            "link": [
                "http://arxiv.org/abs/2310.09436v1",
                "http://arxiv.org/pdf/2310.09436v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG",
                "cs.NE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.09416v1",
            "title": "The maximum size of an induced forest in the binomial random graph",
            "updated": "2023-10-13T21:44:24Z",
            "published": "2023-10-13T21:44:24Z",
            "summary": "The celebrated Frieze's result about the independence number of $G(n,p)$\nstates that it is concentrated in an interval of size $o(1/p)$ for all\n$C_{\\varepsilon}/n<p=o(1)$. We show concentration in an interval of size\n$o(1/p)$ for the maximum size (number of vertices) of an induced forest in\n$G(n,p)$ for all $C_{\\varepsilon}/n<p<1-\\varepsilon$. Presumably, it is the\nfirst generalization of Frieze's result to another class of induced subgraphs\nfor such a range of $p$.",
            "author": [
                "Margarita Akhmejanova",
                "Vladislav Kozhevnikov"
            ],
            "link": [
                "http://arxiv.org/abs/2310.09416v1",
                "http://arxiv.org/pdf/2310.09416v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.09379v1",
            "title": "Erd\u0151s-Ko-Rado in the $\\ell_2$-norm",
            "updated": "2023-10-13T19:57:51Z",
            "published": "2023-10-13T19:57:51Z",
            "summary": "For a $k$-uniform hypergraph $\\mathcal{H}$, the codegree squared sum\n$\\text{co}_2(\\mathcal{H})$ is the square of the $\\ell_2$-norm of the codegree\nvector of $\\mathcal{H}$, and for a family $\\mathcal{F}$ of $k$-uniform\nhypergraphs, the codegree squared extremal number $\\text{exco}_2(n,\n\\mathcal{F})$ is the maximum codegree squared sum of a hypergraph on $n$\nvertices which does not contain any hypergraph in $\\mathcal{F}$. Balogh, Clemen\nand Lidick\\'y recently introduced the codegree squared extremal number and\ndetermined it for a number of $3$-uniform hypergraphs, including the complete\ngraphs $K_4^3$ and $K_5^3$.\n  In this paper, we prove a version of the classical Erd\\H{o}s-Ko-Rado theorem\nfor the codegree squared extremal number: if $\\mathcal{F} \\subset\n\\binom{[n]}{k}$ is intersecting and $n\\ge 2k$, then \\[\\text{co}_2(\\mathcal{F})\n\\le \\binom{n-1}{k-1}(1+(n-k+1)(k-1)),\\] with equality only for the star for $n\n> 2k$. Our main tool is an inequality of Bey.\n  We also prove versions of the Erd\\H{o}s Matching Conjecture and the\n$t$-intersecting Erd\\H{o}s-Ko-Rado theorem for the codegree squared extremal\nnumber for large $n$, determine the exact codegree squared extremal number of\nminimal and linear $3$-paths and $3$-cycles, and determine asymptotically the\ncodegree squared extremal number of minimal and linear $s$-paths and $s$-cycles\nfor $s\\ge 4$.",
            "author": [
                "William Linz"
            ],
            "link": [
                "http://arxiv.org/abs/2310.09379v1",
                "http://arxiv.org/pdf/2310.09379v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.09366v1",
            "title": "A priori screening of data-enabled turbulence models",
            "updated": "2023-10-13T19:19:48Z",
            "published": "2023-10-13T19:19:48Z",
            "summary": "Assessing the compliance of a white-box turbulence model with known turbulent\nknowledge is straightforward. It enables users to screen conventional\nturbulence models and identify apparent inadequacies, thereby allowing for a\nmore focused and fruitful validation and verification. However, comparing a\nblack-box machine-learning model to known empirical scalings is not\nstraightforward. Unless one implements and tests the model, it would not be\nclear if a machine-learning model, trained at finite Reynolds numbers preserves\nthe known high Reynolds number limit. This is inconvenient, particularly\nbecause model implementation involves retraining and re-interfacing. This work\nattempts to address this issue, allowing fast a priori screening of\nmachine-learning models that are based on feed-forward neural networks (FNN).\nThe method leverages the mathematical theorems we present in the paper. These\ntheorems offer estimates of a network's limits even when the exact weights and\nbiases are unknown. For demonstration purposes, we screen existing\nmachine-learning wall models and RANS models for their compliance with the log\nlayer physics and the viscous layer physics in a priori manner. In addition,\nthe theorems serve as essential guidelines for future machine-learning models.",
            "author": [
                "Peng E S Chen",
                "Yuanwei Bin",
                "Xiang I A Yang",
                "Yipeng Shi",
                "Mahdi Abkar",
                "George I. Park"
            ],
            "link": [
                "http://arxiv.org/abs/2310.09366v1",
                "http://arxiv.org/pdf/2310.09366v1"
            ],
            "primary_category": "physics.flu-dyn",
            "category": [
                "physics.flu-dyn"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.09359v1",
            "title": "Laser-driven ultrafast impedance spectroscopy for measuring complex ion\n  hopping processes",
            "updated": "2023-10-13T18:54:02Z",
            "published": "2023-10-13T18:54:02Z",
            "summary": "Superionic conductors, or solid-state ion-conductors that surpass 0.01 S/cm\nin conductivity, can enable more energy dense batteries, robust artificial ion\npumps, and optimized fuel cells. However, tailoring superionic conductors\nrequire precise knowledge of ion migration mechanisms that are still not well\nunderstood, due to limitations set by available spectroscopic tools. Most\nspectroscopic techniques do not probe ion hopping on its inherent picosecond\ntimescale, nor the many-body correlations between the migrating ions, lattice\nvibrational modes, and charge screening clouds--all of which are posited to\ngreatly enhance ionic conduction. Here, we develop an ultrafast technique that\nmeasures the time-resolved change in impedance upon light excitation which\ntriggers selective ion-coupled correlations. We apply our proposed technique to\nstudy a solid-state Li+ conductor Li0.5La0.5TiO3 (LLTO). We compare the\nrelative change in impedance of LLTO before and after a UV to THz frequency\nexcitation to map the corresponding ion-many-body-interaction correlations. We\nalso develop a cost-effective, non-time-resolved laser-driven impedance method\nthat is more accessible for lab-scale adoption. From both our techniques, we\ndetermine that electronic screening and phonon-mode interactions dominate the\nion migration pathway of LLTO. Although we only present one case study, our\ntechnique can also probe O2-, H+, or other ion and charge carrier transport\nphenomena where ultrafast correlations control transport. Furthermore, the\ntemporal relaxation of the measured impedance can distinguish ion transport\neffects caused by many-body correlations, optical heating, correlation, and\nmemory behavior.",
            "author": [
                "Kim H. Pham",
                "Scott K. Cushing"
            ],
            "link": [
                "http://arxiv.org/abs/2310.09359v1",
                "http://arxiv.org/pdf/2310.09359v1"
            ],
            "primary_category": "physics.ins-det",
            "category": [
                "physics.ins-det",
                "physics.app-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.09343v2",
            "title": "Dialogue Chain-of-Thought Distillation for Commonsense-aware\n  Conversational Agents",
            "updated": "2023-10-22T09:16:52Z",
            "published": "2023-10-13T18:17:23Z",
            "summary": "Human-like chatbots necessitate the use of commonsense reasoning in order to\neffectively comprehend and respond to implicit information present within\nconversations. Achieving such coherence and informativeness in responses,\nhowever, is a non-trivial task. Even for large language models (LLMs), the task\nof identifying and aggregating key evidence within a single hop presents a\nsubstantial challenge. This complexity arises because such evidence is\nscattered across multiple turns in a conversation, thus necessitating\nintegration over multiple hops. Hence, our focus is to facilitate such\nmulti-hop reasoning over a dialogue context, namely dialogue chain-of-thought\n(CoT) reasoning. To this end, we propose a knowledge distillation framework\nthat leverages LLMs as unreliable teachers and selectively distills consistent\nand helpful rationales via alignment filters. We further present DOCTOR, a\nDialOgue Chain-of-ThOught Reasoner that provides reliable CoT rationales for\nresponse generation. We conduct extensive experiments to show that enhancing\ndialogue agents with high-quality rationales from DOCTOR significantly improves\nthe quality of their responses.",
            "author": [
                "Hyungjoo Chae",
                "Yongho Song",
                "Kai Tzu-iunn Ong",
                "Taeyoon Kwon",
                "Minjin Kim",
                "Youngjae Yu",
                "Dongha Lee",
                "Dongyeop Kang",
                "Jinyoung Yeo"
            ],
            "link": [
                "http://arxiv.org/abs/2310.09343v2",
                "http://arxiv.org/pdf/2310.09343v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.09341v1",
            "title": "Addressing the cold start problem in privacy preserving content-based\n  recommender systems using hypercube graphs",
            "updated": "2023-10-13T18:11:12Z",
            "published": "2023-10-13T18:11:12Z",
            "summary": "The initial interaction of a user with a recommender system is problematic\nbecause, in such a so-called cold start situation, the recommender system has\nvery little information about the user, if any. Moreover, in collaborative\nfiltering, users need to share their preferences with the service provider by\nrating items while in content-based filtering there is no need for such\ninformation sharing. We have recently shown that a content-based model that\nuses hypercube graphs can determine user preferences with a very limited number\nof ratings while better preserving user privacy. In this paper, we confirm\nthese findings on the basis of experiments with more than 1,000 users in the\nrestaurant and movie domains. We show that the proposed method outperforms\nstandard machine learning algorithms when the number of available ratings is at\nmost 10, which often happens, and is competitive with larger training sets. In\naddition, training is simple and does not require large computational efforts.",
            "author": [
                "Noa Tuval",
                "Alain Hertz",
                "Tsvi Kuflik"
            ],
            "link": [
                "http://arxiv.org/abs/2310.09341v1",
                "http://arxiv.org/pdf/2310.09341v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.09340v1",
            "title": "Geo-knowledge-guided GPT models improve the extraction of location\n  descriptions from disaster-related social media messages",
            "updated": "2023-10-13T18:09:21Z",
            "published": "2023-10-13T18:09:21Z",
            "summary": "Social media messages posted by people during natural disasters often contain\nimportant location descriptions, such as the locations of victims. Recent\nresearch has shown that many of these location descriptions go beyond simple\nplace names, such as city names and street names, and are difficult to extract\nusing typical named entity recognition (NER) tools. While advanced machine\nlearning models could be trained, they require large labeled training datasets\nthat can be time-consuming and labor-intensive to create. In this work, we\npropose a method that fuses geo-knowledge of location descriptions and a\nGenerative Pre-trained Transformer (GPT) model, such as ChatGPT and GPT-4. The\nresult is a geo-knowledge-guided GPT model that can accurately extract location\ndescriptions from disaster-related social media messages. Also, only 22\ntraining examples encoding geo-knowledge are used in our method. We conduct\nexperiments to compare this method with nine alternative approaches on a\ndataset of tweets from Hurricane Harvey. Our method demonstrates an over 40%\nimprovement over typically used NER approaches. The experiment results also\nshow that geo-knowledge is indispensable for guiding the behavior of GPT\nmodels. The extracted location descriptions can help disaster responders reach\nvictims more quickly and may even save lives.",
            "author": [
                "Yingjie Hu",
                "Gengchen Mai",
                "Chris Cundy",
                "Kristy Choi",
                "Ni Lao",
                "Wei Liu",
                "Gaurish Lakhanpal",
                "Ryan Zhenqi Zhou",
                "Kenneth Joseph"
            ],
            "link": [
                "http://dx.doi.org/10.1080/13658816.2023.2266495",
                "http://arxiv.org/abs/2310.09340v1",
                "http://arxiv.org/pdf/2310.09340v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.09286v1",
            "title": "Percolation with invariant Poisson processes of lines in the $3$-regular\n  tree",
            "updated": "2023-10-13T17:53:19Z",
            "published": "2023-10-13T17:53:19Z",
            "summary": "In this paper, we study invariant Poisson processes of lines (i.e,\nbi-infinite geodesics) in the $3$-regular tree. More precisely, there exists a\nunique (up to multiplicative constant) locally finite Borel measure on the\nspace of lines that is invariant under graph automorphisms, and we consider two\nPoissonian ways of playing with this invariant measure. First, following\nBenjamini, Jonasson, Schramm and Tykesson, we consider an invariant Poisson\nprocess of lines, and show that there is a critical value of the intensity\nbelow which a.s. the vacant set of the process percolates, and above which all\nits connected components are finite. Then, we consider an invariant Poisson\nprocess of roads (i.e, lines with speed limits), and show that there is a\ncritical value of the parameter governing the speed limits of the roads below\nwhich a.s. one can drive to infinity in finite time using the road network\ngenerated by the process, and above which this is impossible.",
            "author": [
                "Guillaume Blanc"
            ],
            "link": [
                "http://arxiv.org/abs/2310.09286v1",
                "http://arxiv.org/pdf/2310.09286v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.09281v1",
            "title": "Holographic imaging of antiferromagnetic domains with in-situ magnetic\n  field",
            "updated": "2023-10-13T17:44:40Z",
            "published": "2023-10-13T17:44:40Z",
            "summary": "Lensless coherent x-ray imaging techniques have great potential for\nhigh-resolution imaging of magnetic systems with a variety of in-situ\nperturbations. Despite many investigations of ferromagnets, extending these\ntechniques to the study of other magnetic materials, primarily\nantiferromagnets, is lacking. Here, we demonstrate the first (to our knowledge)\nstudy of an antiferromagnet using holographic imaging through the \"holography\nwith extended reference by autocorrelation linear differential operation\"\ntechnique. Energy-dependent contrast with both linearly and circularly\npolarised x-rays are demonstrated. Antiferromagnetic domains and topological\ntextures are studied in the presence of applied magnetic fields, demonstrating\nquasi-cyclic domain reconfiguration up to 500 mT.",
            "author": [
                "Jack Harrison",
                "Hariom Jani",
                "Junxiong Hu",
                "Manohar Lal",
                "Jheng-Cyuan Lin",
                "Horia Popescu",
                "Jason Brown",
                "Nicolas Jaouen",
                "A. Ariando",
                "Paolo G. Radaelli"
            ],
            "link": [
                "http://arxiv.org/abs/2310.09281v1",
                "http://arxiv.org/pdf/2310.09281v1"
            ],
            "primary_category": "cond-mat.mes-hall",
            "category": [
                "cond-mat.mes-hall"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.09274v1",
            "title": "Geometry of unimodular systems",
            "updated": "2023-10-13T17:37:58Z",
            "published": "2023-10-13T17:37:58Z",
            "summary": "A collection of vectors in a real vector space is called a unimodular system\nif any of its maximal linearly independent subsets generates the same free\nabelian group. This notion is closely connected with totally unimodular\nmatrices: rows or columns of a totally unimodular matrix form a unimodular\nsystem and the matrix of coefficients of expansions of all vectors of a\nunimodular system with respect to its maximal linearly independent subset is\ntotally unimodular.\n  In this paper we show that a unimodular system defines the following\ngeometric data: a Euclidean space, an integral lattice in it, and a reflexive\nlattice zonotope. The discriminant of the lattice is equal to the number of\nmaximal linearly independent subsystems, and we call this number the complexity\nof the unimodular system. For a unimodular system $\\Omega $ we also define the\nGale dual unimodular system $\\Omega ^{\\bot}$ which has the same complexity.\nThese notions may be illustrated by the well-known graphic and cographic\nunimodular systems of a graph. Both graphic and cographic unimodular systems\nhave the same complexity which is equal to the complexity of the graph. For\ngraphs without loops and bridges the graphic and the cographic unimodular\nsystems are Gale dual to each other.\n  We describe this geometric data for certain examples: for the graphic and the\ncographic unimodular systems of a generalized theta-graph, consisting of two\nvertices connected by $N$ edges, for the cographic system of the complete graph\n$K_N$, and for the famous Bixby-Seymour unimodular system, which is neither\ngraphic nor cographic.",
            "author": [
                "I. V. Artamkin"
            ],
            "link": [
                "http://arxiv.org/abs/2310.09274v1",
                "http://arxiv.org/pdf/2310.09274v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.09270v1",
            "title": "Retro-fallback: retrosynthetic planning in an uncertain world",
            "updated": "2023-10-13T17:35:04Z",
            "published": "2023-10-13T17:35:04Z",
            "summary": "Retrosynthesis is the task of proposing a series of chemical reactions to\ncreate a desired molecule from simpler, buyable molecules. While previous works\nhave proposed algorithms to find optimal solutions for a range of metrics (e.g.\nshortest, lowest-cost), these works generally overlook the fact that we have\nimperfect knowledge of the space of possible reactions, meaning plans created\nby the algorithm may not work in a laboratory. In this paper we propose a novel\nformulation of retrosynthesis in terms of stochastic processes to account for\nthis uncertainty. We then propose a novel greedy algorithm called\nretro-fallback which maximizes the probability that at least one synthesis plan\ncan be executed in the lab. Using in-silico benchmarks we demonstrate that\nretro-fallback generally produces better sets of synthesis plans than the\npopular MCTS and retro* algorithms.",
            "author": [
                "Austin Tripp",
                "Krzysztof Maziarz",
                "Sarah Lewis",
                "Marwin Segler",
                "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato"
            ],
            "link": [
                "http://arxiv.org/abs/2310.09270v1",
                "http://arxiv.org/pdf/2310.09270v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.09265v1",
            "title": "PromptRE: Weakly-Supervised Document-Level Relation Extraction via\n  Prompting-Based Data Programming",
            "updated": "2023-10-13T17:23:17Z",
            "published": "2023-10-13T17:23:17Z",
            "summary": "Relation extraction aims to classify the relationships between two entities\ninto pre-defined categories. While previous research has mainly focused on\nsentence-level relation extraction, recent studies have expanded the scope to\ndocument-level relation extraction. Traditional relation extraction methods\nheavily rely on human-annotated training data, which is time-consuming and\nlabor-intensive. To mitigate the need for manual annotation, recent\nweakly-supervised approaches have been developed for sentence-level relation\nextraction while limited work has been done on document-level relation\nextraction. Weakly-supervised document-level relation extraction faces\nsignificant challenges due to an imbalanced number \"no relation\" instances and\nthe failure of directly probing pretrained large language models for document\nrelation extraction. To address these challenges, we propose PromptRE, a novel\nweakly-supervised document-level relation extraction method that combines\nprompting-based techniques with data programming. Furthermore, PromptRE\nincorporates the label distribution and entity types as prior knowledge to\nimprove the performance. By leveraging the strengths of both prompting and data\nprogramming, PromptRE achieves improved performance in relation classification\nand effectively handles the \"no relation\" problem. Experimental results on\nReDocRED, a benchmark dataset for document-level relation extraction,\ndemonstrate the superiority of PromptRE over baseline approaches.",
            "author": [
                "Chufan Gao",
                "Xulin Fan",
                "Jimeng Sun",
                "Xuan Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.09265v1",
                "http://arxiv.org/pdf/2310.09265v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.09236v1",
            "title": "Time CNN and Graph Convolution Network for Epileptic Spike Detection in\n  MEG Data",
            "updated": "2023-10-13T16:40:29Z",
            "published": "2023-10-13T16:40:29Z",
            "summary": "Magnetoencephalography (MEG) recordings of patients with epilepsy exhibit\nspikes, a typical biomarker of the pathology. Detecting those spikes allows\naccurate localization of brain regions triggering seizures. Spike detection is\noften performed manually. However, it is a burdensome and error prone task due\nto the complexity of MEG data. To address this problem, we propose a 1D\ntemporal convolutional neural network (Time CNN) coupled with a graph\nconvolutional network (GCN) to classify short time frames of MEG recording as\ncontaining a spike or not. Compared to other recent approaches, our models have\nfewer parameters to train and we propose to use a GCN to account for MEG\nsensors spatial relationships. Our models produce clinically relevant results\nand outperform deep learning-based state-of-the-art methods reaching a\nclassification f1-score of 76.7% on a balanced dataset and of 25.5% on a\nrealistic, highly imbalanced dataset, for the spike class.",
            "author": [
                "Pauline Mouches",
                "Thibaut Dejean",
                "Julien Jung",
                "Romain Bouet",
                "Carole Lartizien",
                "Romain Quentin"
            ],
            "link": [
                "http://arxiv.org/abs/2310.09236v1",
                "http://arxiv.org/pdf/2310.09236v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.09234v2",
            "title": "ClickPrompt: CTR Models are Strong Prompt Generators for Adapting\n  Language Models to CTR Prediction",
            "updated": "2023-10-17T04:53:08Z",
            "published": "2023-10-13T16:37:53Z",
            "summary": "Click-through rate (CTR) prediction has become increasingly indispensable for\nvarious Internet applications. Traditional CTR models convert the multi-field\ncategorical data into ID features via one-hot encoding, and extract the\ncollaborative signals among features. Such a paradigm suffers from the problem\nof semantic information loss. Another line of research explores the potential\nof pretrained language models (PLMs) for CTR prediction by converting input\ndata into textual sentences through hard prompt templates. Although semantic\nsignals are preserved, they generally fail to capture the collaborative\ninformation (e.g., feature interactions, pure ID features), not to mention the\nunacceptable inference overhead brought by the huge model size. In this paper,\nwe aim to model both the semantic knowledge and collaborative knowledge for\naccurate CTR estimation, and meanwhile address the inference inefficiency\nissue. To benefit from both worlds and close their gaps, we propose a novel\nmodel-agnostic framework (i.e., ClickPrompt), where we incorporate CTR models\nto generate interaction-aware soft prompts for PLMs. We design a\nprompt-augmented masked language modeling (PA-MLM) pretraining task, where PLM\nhas to recover the masked tokens based on the language context, as well as the\nsoft prompts generated by CTR model. The collaborative and semantic knowledge\nfrom ID and textual features would be explicitly aligned and interacted via the\nprompt interface. Then, we can either tune the CTR model with PLM for superior\nperformance, or solely tune the CTR model without PLM for inference efficiency.\nExperiments on four real-world datasets validate the effectiveness of\nClickPrompt compared with existing baselines.",
            "author": [
                "Jianghao Lin",
                "Bo Chen",
                "Hangyu Wang",
                "Yunjia Xi",
                "Yanru Qu",
                "Xinyi Dai",
                "Kangning Zhang",
                "Ruiming Tang",
                "Yong Yu",
                "Weinan Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.09234v2",
                "http://arxiv.org/pdf/2310.09234v2"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.09232v2",
            "title": "Bounds on Guessing Numbers and Secret Sharing Combining Information\n  Theory Methods",
            "updated": "2023-10-18T16:41:14Z",
            "published": "2023-10-13T16:34:35Z",
            "summary": "This paper is on developing some computer-assisted proof methods involving\nnon-classical inequalities for Shannon entropy. Two areas of the applications\nof information inequalities are studied: Secret sharing schemes and hat\nguessing games. In the former a random secret value is transformed into shares\ndistributed among several participants in such a way that only the qualified\ngroups of participants can recover the secret value. In the latter each\nparticipant is assigned a hat colour and they try to guess theirs while seeing\nonly some of the others'. The aim is to maximize the probability that every\nplayer guesses correctly, the optimal probability depends on the underlying\nsight graph. We use for both problems the method of non-Shannon-type\ninformation inequalities going back to Z. Zhang and R. W. Yeung. We employ the\nlinear programming technique that allows to apply new information inequalities\nindirectly, without even writing them down explicitly. To reduce the complexity\nof the problems of linear programming involved in the bounds we extensively use\nsymmetry considerations. Using these tools, we improve lower bounds on the\nratio of key size to secret size for the former problem and an upper bound for\none of the ten vertex graphs related to an open question by Riis for the latter\nproblem.",
            "author": [
                "Emirhan G\u00fcrp\u0131nar"
            ],
            "link": [
                "http://arxiv.org/abs/2310.09232v2",
                "http://arxiv.org/pdf/2310.09232v2"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.09227v1",
            "title": "Integer diagonal forms for subset intersection relations",
            "updated": "2023-10-13T16:26:17Z",
            "published": "2023-10-13T16:26:17Z",
            "summary": "For integers $0 \\leq \\ell \\leq k_{r} \\leq k_{c} \\leq n$, we give a\ndescription for the Smith group of the incidence matrix with rows (columns)\nindexed by the size $k_r$ ($k_c$, respectively) subsets of an $n$-element set,\nwhere incidence means intersection in a set of size $\\ell$. This generalizes\nwork of Wilson and Bier from the 1990s which dealt only with the case where\nincidence meant inclusion. Our approach also describes the Smith group of any\nmatrix in the $\\mathbb{Z}$-linear span of these matrices so includes all\ninteger matrices in the Bose-Mesner algebra of the Johnson association scheme:\nfor example, the association matrices themselves as well as the Laplacian,\nsignless Laplacian, Seidel adjacency matrix, etc. of the associated graphs. In\nparticular, we describe the critical (also known as sandpile) groups of these\ngraphs. The complexity of our formula grows with the $k$ parameters, but is\nindependent of $n$ and $\\ell$, which often leads to an efficient algorithm for\ncomputing these groups. We illustrate our techniques to give diagonal forms of\nmatrices attached to the Kneser and Johnson graphs for subsets of size $3$,\nwhose invariants have never before been described, and recover results from a\nvariety of papers in the literature in a unified way.",
            "author": [
                "Joshua E. Ducey",
                "Lauren Engelthaler",
                "Jacob Gathje",
                "Brant Jones",
                "Izzy Pfaff",
                "Jenna Plute"
            ],
            "link": [
                "http://arxiv.org/abs/2310.09227v1",
                "http://arxiv.org/pdf/2310.09227v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C50, 05E30"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.09222v1",
            "title": "Fast & Efficient Learning of Bayesian Networks from Data: Knowledge\n  Discovery and Causality",
            "updated": "2023-10-13T16:20:20Z",
            "published": "2023-10-13T16:20:20Z",
            "summary": "Structure learning is essential for Bayesian networks (BNs) as it uncovers\ncausal relationships, and enables knowledge discovery, predictions, inferences,\nand decision-making under uncertainty. Two novel algorithms, FSBN and SSBN,\nbased on the PC algorithm, employ local search strategy and conditional\nindependence tests to learn the causal network structure from data. They\nincorporate d-separation to infer additional topology information, prioritize\nconditioning sets, and terminate the search immediately and efficiently. FSBN\nachieves up to 52% computation cost reduction, while SSBN surpasses it with a\nremarkable 72% reduction for a 200-node network. SSBN demonstrates further\nefficiency gains due to its intelligent strategy. Experimental studies show\nthat both algorithms match the induction quality of the PC algorithm while\nsignificantly reducing computation costs. This enables them to offer\ninterpretability and adaptability while reducing the computational burden,\nmaking them valuable for various applications in big data analytics.",
            "author": [
                "Minn Sein",
                "Fu Shunkai"
            ],
            "link": [
                "http://arxiv.org/abs/2310.09222v1",
                "http://arxiv.org/pdf/2310.09222v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.09202v1",
            "title": "Graph Condensation via Eigenbasis Matching",
            "updated": "2023-10-13T15:48:12Z",
            "published": "2023-10-13T15:48:12Z",
            "summary": "The increasing amount of graph data places requirements on the efficiency and\nscalability of graph neural networks (GNNs), despite their effectiveness in\nvarious graph-related applications. Recently, the emerging graph condensation\n(GC) sheds light on reducing the computational cost of GNNs from a data\nperspective. It aims to replace the real large graph with a significantly\nsmaller synthetic graph so that GNNs trained on both graphs exhibit comparable\nperformance. However, our empirical investigation reveals that existing GC\nmethods suffer from poor generalization, i.e., different GNNs trained on the\nsame synthetic graph have obvious performance gaps. What factors hinder the\ngeneralization of GC and how can we mitigate it? To answer this question, we\ncommence with a detailed analysis and observe that GNNs will inject spectrum\nbias into the synthetic graph, resulting in a distribution shift. To tackle\nthis issue, we propose eigenbasis matching for spectrum-free graph\ncondensation, named GCEM, which has two key steps: First, GCEM matches the\neigenbasis of the real and synthetic graphs, rather than the graph structure,\nwhich eliminates the spectrum bias of GNNs. Subsequently, GCEM leverages the\nspectrum of the real graph and the synthetic eigenbasis to construct the\nsynthetic graph, thereby preserving the essential structural information. We\ntheoretically demonstrate that the synthetic graph generated by GCEM maintains\nthe spectral similarity, i.e., total variation, of the real graph. Extensive\nexperiments conducted on five graph datasets verify that GCEM not only achieves\nstate-of-the-art performance over baselines but also significantly narrows the\nperformance gaps between different GNNs.",
            "author": [
                "Yang Liu",
                "Deyu Bo",
                "Chuan Shi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.09202v1",
                "http://arxiv.org/pdf/2310.09202v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.09196v2",
            "title": "A 4-approximation algorithm for min max correlation clustering",
            "updated": "2023-10-30T09:15:41Z",
            "published": "2023-10-13T15:42:55Z",
            "summary": "We introduce a lower bounding technique for the min max correlation\nclustering problem and, based on this technique, a combinatorial\n4-approximation algorithm for complete graphs. This improves upon the previous\nbest known approximation guarantees of 5, using a linear program formulation\n(Kalhan et al., 2019), and 40, for a combinatorial algorithm (Davies et al.,\n2023). We extend this algorithm by a greedy joining heuristic and show\nempirically that it improves the state of the art in solution quality and\nruntime on several benchmark datasets.",
            "author": [
                "Holger Heidrich",
                "Jannik Irmai",
                "Bjoern Andres"
            ],
            "link": [
                "http://arxiv.org/abs/2310.09196v2",
                "http://arxiv.org/pdf/2310.09196v2"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "cs.DM",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.09192v1",
            "title": "Does Graph Distillation See Like Vision Dataset Counterpart?",
            "updated": "2023-10-13T15:36:48Z",
            "published": "2023-10-13T15:36:48Z",
            "summary": "Training on large-scale graphs has achieved remarkable results in graph\nrepresentation learning, but its cost and storage have attracted increasing\nconcerns. Existing graph condensation methods primarily focus on optimizing the\nfeature matrices of condensed graphs while overlooking the impact of the\nstructure information from the original graphs. To investigate the impact of\nthe structure information, we conduct analysis from the spectral domain and\nempirically identify substantial Laplacian Energy Distribution (LED) shifts in\nprevious works. Such shifts lead to poor performance in cross-architecture\ngeneralization and specific tasks, including anomaly detection and link\nprediction. In this paper, we propose a novel Structure-broadcasting Graph\nDataset Distillation (SGDD) scheme for broadcasting the original structure\ninformation to the generation of the synthetic one, which explicitly prevents\noverlooking the original structure information. Theoretically, the synthetic\ngraphs by SGDD are expected to have smaller LED shifts than previous works,\nleading to superior performance in both cross-architecture settings and\nspecific tasks. We validate the proposed SGDD across 9 datasets and achieve\nstate-of-the-art results on all of them: for example, on the YelpChi dataset,\nour approach maintains 98.6% test accuracy of training on the original graph\ndataset with 1,000 times saving on the scale of the graph. Moreover, we\nempirically evaluate there exist 17.6% ~ 31.4% reductions in LED shift crossing\n9 datasets. Extensive experiments and analysis verify the effectiveness and\nnecessity of the proposed designs. The code is available in the GitHub\nrepository: https://github.com/RingBDStack/SGDD.",
            "author": [
                "Beining Yang",
                "Kai Wang",
                "Qingyun Sun",
                "Cheng Ji",
                "Xingcheng Fu",
                "Hao Tang",
                "Yang You",
                "Jianxin Li"
            ],
            "link": [
                "http://arxiv.org/abs/2310.09192v1",
                "http://arxiv.org/pdf/2310.09192v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.09183v2",
            "title": "PRIOR: Personalized Prior for Reactivating the Information Overlooked in\n  Federated Learning",
            "updated": "2023-11-10T09:53:32Z",
            "published": "2023-10-13T15:21:25Z",
            "summary": "Classical federated learning (FL) enables training machine learning models\nwithout sharing data for privacy preservation, but heterogeneous data\ncharacteristic degrades the performance of the localized model. Personalized FL\n(PFL) addresses this by synthesizing personalized models from a global model\nvia training on local data. Such a global model may overlook the specific\ninformation that the clients have been sampled. In this paper, we propose a\nnovel scheme to inject personalized prior knowledge into the global model in\neach client, which attempts to mitigate the introduced incomplete information\nproblem in PFL. At the heart of our proposed approach is a framework, the PFL\nwith Bregman Divergence (pFedBreD), decoupling the personalized prior from the\nlocal objective function regularized by Bregman divergence for greater\nadaptability in personalized scenarios. We also relax the mirror descent (RMD)\nto extract the prior explicitly to provide optional strategies. Additionally,\nour pFedBreD is backed up by a convergence analysis. Sufficient experiments\ndemonstrate that our method reaches the state-of-the-art performances on 5\ndatasets and outperforms other methods by up to 3.5% across 8 benchmarks.\nExtensive analyses verify the robustness and necessity of proposed designs.",
            "author": [
                "Mingjia Shi",
                "Yuhao Zhou",
                "Kai Wang",
                "Huaizheng Zhang",
                "Shudong Huang",
                "Qing Ye",
                "Jiangcheng Lv"
            ],
            "link": [
                "http://arxiv.org/abs/2310.09183v2",
                "http://arxiv.org/pdf/2310.09183v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.DC",
                "68T07",
                "I.2.11"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.09169v1",
            "title": "Ising model on a Galton-Watson tree with a sparse random external field",
            "updated": "2023-10-13T15:03:16Z",
            "published": "2023-10-13T15:03:16Z",
            "summary": "We consider the Ising model on a supercritical Galton-Watson tree\n$\\mathbf{T}_n$ of depth $n$ with a sparse random external field, given by a\ncollection of i.i.d. Bernouilli random variables with vanishing parameter\n$p_n$. This may me viewed as a toy model for the Ising model on a configuration\nmodel with a few interfering external vertices carrying a plus spin: the\nquestion is to know how many (or how few) interfering vertices are enough to\ninfluence the whole graph. Our main result consists in providing a necessary\nand sufficient condition on the parameters $(p_n)_{n\\geq 0}$ for the root of\n$\\mathbf{T}_n$ to remain magnetized in the large $n$ limit. Our model is\nclosely related to the Ising model on a (random) pruned sub-tree\n$\\mathbf{T}_n^*$ with plus boundary condition; one key result is that this\npruned tree turns out to be an inhomogeneous, $n$-dependent, Branching Process.\nWe then use standard tools such as tree recursions and non-linear capacities to\nstudy the Ising model on this sequence of Galton-Watson trees; one difficulty\nis that the offspring distributions of $\\mathbf{T}_n^*$, in addition to vary\nalong the generations $0\\leq k \\leq n-1$, also depend on~$n$.",
            "author": [
                "Irene Ayuso Ventura",
                "Quentin Berger"
            ],
            "link": [
                "http://arxiv.org/abs/2310.09169v1",
                "http://arxiv.org/pdf/2310.09169v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR",
                "math-ph",
                "math.MP",
                "82B20, 60K35, 82B44, 82B26"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.09147v1",
            "title": "Exploring Sparse Spatial Relation in Graph Inference for Text-Based VQA",
            "updated": "2023-10-13T14:39:34Z",
            "published": "2023-10-13T14:39:34Z",
            "summary": "Text-based visual question answering (TextVQA) faces the significant\nchallenge of avoiding redundant relational inference. To be specific, a large\nnumber of detected objects and optical character recognition (OCR) tokens\nresult in rich visual relationships. Existing works take all visual\nrelationships into account for answer prediction. However, there are three\nobservations: (1) a single subject in the images can be easily detected as\nmultiple objects with distinct bounding boxes (considered repetitive objects).\nThe associations between these repetitive objects are superfluous for answer\nreasoning; (2) two spatially distant OCR tokens detected in the image\nfrequently have weak semantic dependencies for answer reasoning; and (3) the\nco-existence of nearby objects and tokens may be indicative of important visual\ncues for predicting answers. Rather than utilizing all of them for answer\nprediction, we make an effort to identify the most important connections or\neliminate redundant ones. We propose a sparse spatial graph network (SSGN) that\nintroduces a spatially aware relation pruning technique to this task. As\nspatial factors for relation measurement, we employ spatial distance, geometric\ndimension, overlap area, and DIoU for spatially aware pruning. We consider\nthree visual relationships for graph learning: object-object, OCR-OCR tokens,\nand object-OCR token relationships. SSGN is a progressive graph learning\narchitecture that verifies the pivotal relations in the correlated object-token\nsparse graph, and then in the respective object-based sparse graph and\ntoken-based sparse graph. Experiment results on TextVQA and ST-VQA datasets\ndemonstrate that SSGN achieves promising performances. And some visualization\nresults further demonstrate the interpretability of our method.",
            "author": [
                "Sheng Zhou",
                "Dan Guo",
                "Jia Li",
                "Xun Yang",
                "Meng Wang"
            ],
            "link": [
                "http://dx.doi.org/10.1109/TIP.2023.3310332",
                "http://arxiv.org/abs/2310.09147v1",
                "http://arxiv.org/pdf/2310.09147v1"
            ],
            "primary_category": "cs.MM",
            "category": [
                "cs.MM",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.09145v1",
            "title": "Lincoln AI Computing Survey (LAICS) Update",
            "updated": "2023-10-13T14:36:26Z",
            "published": "2023-10-13T14:36:26Z",
            "summary": "This paper is an update of the survey of AI accelerators and processors from\npast four years, which is now called the Lincoln AI Computing Survey - LAICS\n(pronounced \"lace\"). As in past years, this paper collects and summarizes the\ncurrent commercial accelerators that have been publicly announced with peak\nperformance and peak power consumption numbers. The performance and power\nvalues are plotted on a scatter graph, and a number of dimensions and\nobservations from the trends on this plot are again discussed and analyzed.\nMarket segments are highlighted on the scatter plot, and zoomed plots of each\nsegment are also included. Finally, a brief description of each of the new\naccelerators that have been added in the survey this year is included.",
            "author": [
                "Albert Reuther",
                "Peter Michaleas",
                "Michael Jones",
                "Vijay Gadepally",
                "Siddharth Samsi",
                "Jeremy Kepner"
            ],
            "link": [
                "http://arxiv.org/abs/2310.09145v1",
                "http://arxiv.org/pdf/2310.09145v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.DC",
                "C.1.4; C.4"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.09143v1",
            "title": "An Intrinsic Integrity-Driven Rating Model for a Sustainable Reputation\n  System",
            "updated": "2023-10-13T14:34:10Z",
            "published": "2023-10-13T14:34:10Z",
            "summary": "In the era of digital markets, the challenge for consumers is discerning\nquality amidst information asymmetry. While traditional markets use brand\nmechanisms to address this issue, transferring such systems to internet-based\nP2P markets, where misleading practices like fake ratings are rampant, remains\nchallenging. Current internet platforms strive to counter this through\nverification algorithms, but these efforts find themselves in a continuous\ntug-of-war with counterfeit actions.\n  Exploiting the transparency, immutability, and traceability of blockchain\ntechnology, this paper introduces a robust reputation voting system grounded in\nit. Unlike existing blockchain-based reputation systems, our model harnesses an\nintrinsically economically incentivized approach to bolster agent integrity. We\noptimize this model to mirror real-world user behavior, preserving the\nreputation system's foundational sustainability. Through Monte-Carlo\nsimulations, using both uniform and power-law distributions enabled by an\ninnovative inverse transform method, we traverse a broad parameter landscape,\nreplicating real-world complexity. The findings underscore the promise of a\nsustainable, transparent, and formidable reputation mechanism. Given its\nstructure, our framework can potentially function as a universal, sustainable\noracle for offchain-onchain bridging, aiding entities in perpetually\ncultivating their reputation. Future integration with technologies like Ring\nSignature and Zero Knowledge Proof could amplify the system's privacy facets,\nrendering it particularly influential in the ever-evolving digital domain.",
            "author": [
                "H. Wen",
                "T. Huang",
                "D. Xiao"
            ],
            "link": [
                "http://arxiv.org/abs/2310.09143v1",
                "http://arxiv.org/pdf/2310.09143v1"
            ],
            "primary_category": "cs.CE",
            "category": [
                "cs.CE",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.09135v2",
            "title": "HierarchicalContrast: A Coarse-to-Fine Contrastive Learning Framework\n  for Cross-Domain Zero-Shot Slot Filling",
            "updated": "2023-10-20T11:29:53Z",
            "published": "2023-10-13T14:23:33Z",
            "summary": "In task-oriented dialogue scenarios, cross-domain zero-shot slot filling\nplays a vital role in leveraging source domain knowledge to learn a model with\nhigh generalization ability in unknown target domain where annotated data is\nunavailable. However, the existing state-of-the-art zero-shot slot filling\nmethods have limited generalization ability in target domain, they only show\neffective knowledge transfer on seen slots and perform poorly on unseen slots.\nTo alleviate this issue, we present a novel Hierarchical Contrastive Learning\nFramework (HiCL) for zero-shot slot filling. Specifically, we propose a coarse-\nto fine-grained contrastive learning based on Gaussian-distributed embedding to\nlearn the generalized deep semantic relations between utterance-tokens, by\noptimizing inter- and intra-token distribution distance. This encourages HiCL\nto generalize to the slot types unseen at training phase. Furthermore, we\npresent a new iterative label set semantics inference method to unbiasedly and\nseparately evaluate the performance of unseen slot types which entangled with\ntheir counterparts (i.e., seen slot types) in the previous zero-shot slot\nfilling evaluation methods. The extensive empirical experiments on four\ndatasets demonstrate that the proposed method achieves comparable or even\nbetter performance than the current state-of-the-art zero-shot slot filling\napproaches.",
            "author": [
                "Junwen Zhang",
                "Yin Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.09135v2",
                "http://arxiv.org/pdf/2310.09135v2"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL",
                "I.2; I.2.7"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.09128v1",
            "title": "Isolation of squares in graphs",
            "updated": "2023-10-13T14:16:58Z",
            "published": "2023-10-13T14:16:58Z",
            "summary": "Given a set $\\mathcal{F}$ of graphs, we call a copy of a graph in\n$\\mathcal{F}$ an $\\mathcal{F}$-graph. The $\\mathcal{F}$-isolation number of a\ngraph $G$, denoted by $\\iota(G,\\mathcal{F})$, is the size of a smallest subset\n$D$ of the vertex set $V(G)$ such that the closed neighbourhood of $D$\nintersects the vertex sets of the $\\mathcal{F}$-graphs contained by $G$\n(equivalently, $G - N[D]$ contains no $\\mathcal{F}$-graph). Thus,\n$\\iota(G,\\{K_1\\})$ is the domination number of $G$. The second author showed\nthat if $\\mathcal{F}$ is the set of cycles and $G$ is a connected $n$-vertex\ngraph that is not a triangle, then $\\iota(G,\\mathcal{F}) \\leq \\left \\lfloor\n\\frac{n}{4} \\right \\rfloor$. This bound is attainable for every $n$ and solved\na problem of Caro and Hansberg. A question that arises immediately is how\nsmaller an upper bound can be if $\\mathcal{F} = \\{C_k\\}$ for some $k \\geq 3$,\nwhere $C_k$ is a cycle of length $k$. The problem is to determine the smallest\nreal number $c_k$ (if it exists) such that for some finite set $\\mathcal{E}_k$\nof graphs, $\\iota(G, \\{C_k\\}) \\leq c_k |V(G)|$ for every connected graph $G$\nthat is not an $\\mathcal{E}_k$-graph. The above-mentioned result yields $c_3 =\n\\frac{1}{4}$ and $\\mathcal{E}_3 = \\{C_3\\}$. The second author also showed that\nif $k \\geq 5$ and $c_k$ exists, then $c_k \\geq \\frac{2}{2k + 1}$. We prove that\n$c_4 = \\frac{1}{5}$ and determine $\\mathcal{E}_4$, which consists of three\n$4$-vertex graphs and six $9$-vertex graphs. The $9$-vertex graphs in\n$\\mathcal{E}_4$ were fully determined by means of a computer program. A method\nthat has the potential of yielding similar results is introduced.",
            "author": [
                "Karl Bartolo",
                "Peter Borg",
                "Dayle Scicluna"
            ],
            "link": [
                "http://arxiv.org/abs/2310.09128v1",
                "http://arxiv.org/pdf/2310.09128v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "cs.DM",
                "05C35, 05C38, 05C69"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.09118v1",
            "title": "DSG: An End-to-End Document Structure Generator",
            "updated": "2023-10-13T14:03:01Z",
            "published": "2023-10-13T14:03:01Z",
            "summary": "Information in industry, research, and the public sector is widely stored as\nrendered documents (e.g., PDF files, scans). Hence, to enable downstream tasks,\nsystems are needed that map rendered documents onto a structured hierarchical\nformat. However, existing systems for this task are limited by heuristics and\nare not end-to-end trainable. In this work, we introduce the Document Structure\nGenerator (DSG), a novel system for document parsing that is fully end-to-end\ntrainable. DSG combines a deep neural network for parsing (i) entities in\ndocuments (e.g., figures, text blocks, headers, etc.) and (ii) relations that\ncapture the sequence and nested structure between entities. Unlike existing\nsystems that rely on heuristics, our DSG is trained end-to-end, making it\neffective and flexible for real-world applications. We further contribute a\nnew, large-scale dataset called E-Periodica comprising real-world magazines\nwith complex document structures for evaluation. Our results demonstrate that\nour DSG outperforms commercial OCR tools and, on top of that, achieves\nstate-of-the-art performance. To the best of our knowledge, our DSG system is\nthe first end-to-end trainable system for hierarchical document parsing.",
            "author": [
                "Johannes Rausch",
                "Gentiana Rashiti",
                "Maxim Gusev",
                "Ce Zhang",
                "Stefan Feuerriegel"
            ],
            "link": [
                "http://arxiv.org/abs/2310.09118v1",
                "http://arxiv.org/pdf/2310.09118v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.09119v1",
            "title": "A Frustratingly Easy Plug-and-Play Detection-and-Reasoning Module for\n  Chinese Spelling Check",
            "updated": "2023-10-13T14:03:01Z",
            "published": "2023-10-13T14:03:01Z",
            "summary": "In recent years, Chinese Spelling Check (CSC) has been greatly improved by\ndesigning task-specific pre-training methods or introducing auxiliary tasks,\nwhich mostly solve this task in an end-to-end fashion. In this paper, we\npropose to decompose the CSC workflow into detection, reasoning, and searching\nsubtasks so that the rich external knowledge about the Chinese language can be\nleveraged more directly and efficiently. Specifically, we design a\nplug-and-play detection-and-reasoning module that is compatible with existing\nSOTA non-autoregressive CSC models to further boost their performance. We find\nthat the detection-and-reasoning module trained for one model can also benefit\nother models. We also study the primary interpretability provided by the task\ndecomposition. Extensive experiments and detailed analyses demonstrate the\neffectiveness and competitiveness of the proposed module.",
            "author": [
                "Haojing Huang",
                "Jingheng Ye",
                "Qingyu Zhou",
                "Yinghui Li",
                "Yangning Li",
                "Feng Zhou",
                "Hai-Tao Zheng"
            ],
            "link": [
                "http://arxiv.org/abs/2310.09119v1",
                "http://arxiv.org/pdf/2310.09119v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.09091v1",
            "title": "Insightful analysis of historical sources at scales beyond human\n  capabilities using unsupervised Machine Learning and XAI",
            "updated": "2023-10-13T13:22:05Z",
            "published": "2023-10-13T13:22:05Z",
            "summary": "Historical materials are abundant. Yet, piecing together how human knowledge\nhas evolved and spread both diachronically and synchronically remains a\nchallenge that can so far only be very selectively addressed. The vast volume\nof materials precludes comprehensive studies, given the restricted number of\nhuman specialists. However, as large amounts of historical materials are now\navailable in digital form there is a promising opportunity for AI-assisted\nhistorical analysis. In this work, we take a pivotal step towards analyzing\nvast historical corpora by employing innovative machine learning (ML)\ntechniques, enabling in-depth historical insights on a grand scale. Our study\ncenters on the evolution of knowledge within the `Sacrobosco Collection' -- a\ndigitized collection of 359 early modern printed editions of textbooks on\nastronomy used at European universities between 1472 and 1650 -- roughly 76,000\npages, many of which contain astronomic, computational tables. An ML based\nanalysis of these tables helps to unveil important facets of the\nspatio-temporal evolution of knowledge and innovation in the field of\nmathematical astronomy in the period, as taught at European universities.",
            "author": [
                "Oliver Eberle",
                "Jochen B\u00fcttner",
                "Hassan El-Hajj",
                "Gr\u00e9goire Montavon",
                "Klaus-Robert M\u00fcller",
                "Matteo Valleriani"
            ],
            "link": [
                "http://arxiv.org/abs/2310.09091v1",
                "http://arxiv.org/pdf/2310.09091v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CY",
                "cs.DL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.09089v1",
            "title": "Qilin-Med: Multi-stage Knowledge Injection Advanced Medical Large\n  Language Model",
            "updated": "2023-10-13T13:17:03Z",
            "published": "2023-10-13T13:17:03Z",
            "summary": "Integrating large language models (LLMs) into healthcare presents potential\nbut faces challenges. Directly pre-training LLMs for domains like medicine is\nresource-heavy and sometimes unfeasible. Sole reliance on Supervised\nFine-tuning (SFT) can result in overconfident predictions and may not tap into\ndomain specific insights. Addressing these challenges, we present a multi-stage\ntraining method combining Domain-specific Continued Pre-training (DCPT), SFT,\nand Direct Preference Optimization (DPO). A notable contribution of our study\nis the introduction of a 3Gb Chinese Medicine (ChiMed) dataset, encompassing\nmedical question answering, plain texts, knowledge graphs, and dialogues,\nsegmented into three training stages. The medical LLM trained with our\npipeline, Qilin-Med, exhibits significant performance boosts. In the CPT and\nSFT phases, it achieves 38.4% and 40.0% accuracy on the CMExam, surpassing\nBaichuan-7B's 33.5%. In the DPO phase, on the Huatuo-26M test set, it scores\n16.66 in BLEU-1 and 27.44 in ROUGE1, outperforming the SFT's 12.69 and 24.21.\nThis highlights the strength of our training approach in refining LLMs for\nmedical applications.",
            "author": [
                "Qichen Ye",
                "Junling Liu",
                "Dading Chong",
                "Peilin Zhou",
                "Yining Hua",
                "Andrew Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.09089v1",
                "http://arxiv.org/pdf/2310.09089v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.09087v1",
            "title": "A physics-infused Immersed Boundary Method using online sequential Data\n  Assimilation",
            "updated": "2023-10-13T13:14:56Z",
            "published": "2023-10-13T13:14:56Z",
            "summary": "A physics-infused strategy relying on the Ensemble Kalman Filter (EnKF) is\nhere used to augment the accuracy of a continuous Immersed Boundary Method\n(IBM). The latter is a classical penalty method accounting for the presence of\nthe immersed body via a volume source term which is included in the\nNavier-Stokes equations. The model coefficients of the penalization method,\nwhich are usually selected by the user, are optimized here using an EnKF\ndata-driven strategy. The parametric inference is governed by the physical\nknowledge of local and global features of the flow, such as the no-slip\ncondition and the shear stress at the wall. The C++ library CONES (Coupling\nOpenFOAM with Numerical EnvironmentS) developed by the team is used to perform\nan online investigation, coupling on-the-fly data from synthetic sensors with\nresults from an ensemble of coarse-grained numerical simulations. The analysis\nis performed for a classical test case, namely the turbulent channel flow with\n$Re_\\tau = 550$. The comparison of the results with a high-fidelity Direct\nNumerical Simulation (DNS) shows that the data-driven procedure exhibits\nremarkable accuracy despite the relatively low grid resolution of the ensemble\nmembers.",
            "author": [
                "Miguel M. Valero",
                "Marcello Meldi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.09087v1",
                "http://arxiv.org/pdf/2310.09087v1"
            ],
            "primary_category": "physics.flu-dyn",
            "category": [
                "physics.flu-dyn"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.09086v2",
            "title": "Laplacian eigenvalue distribution for unicyclic graphs",
            "updated": "2023-11-07T03:18:25Z",
            "published": "2023-10-13T13:11:34Z",
            "summary": "Let $G$ be a unicyclic graph. In this paper, we provide an upper bound for\nthe number of Laplacian eigenvalues of $G$ within the interval $[0,1)$ in terms\nof the diameter and the girth of $G$.",
            "author": [
                "Sunyo Moon",
                "Seungkook Park"
            ],
            "link": [
                "http://arxiv.org/abs/2310.09086v2",
                "http://arxiv.org/pdf/2310.09086v2"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "15A18, 05C50"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.09077v1",
            "title": "Dust evolution in protoplanetary disks",
            "updated": "2023-10-13T12:57:36Z",
            "published": "2023-10-13T12:57:36Z",
            "summary": "Planet formation models rely on knowledge of the physical conditions and\nevolutionary processes in protoplanetary disks, in particular the grain size\ndistribution and dust growth timescales. In theoretical models, several\nbarriers exist that prevent grain growth to pebble sizes and beyond, such as\nthe radial drift and fragmentation. Pressure bumps have been proposed to\novercome such barriers. In the past decade ALMA has revealed observational\nevidence for the existence of such pressure bumps in the form of dust traps,\nsuch as dust rings, gaps, cavities and crescents through high-resolution\nmillimeter continuum data originating from thermal dust emission of\npebble-sized dust grains. These substructures may be related to young\nprotoplanets, either as the starting point or the consequence of early planet\nformation. Furthermore, disk dust masses have been measured for complete\nsamples of young stars in clusters, which provide initial conditions for the\nsolid mass budget available for planet formation. However, observational biases\nexist in the selection of high-resolution ALMA observations and uncertainties\nexist in the derivation of the disk dust mass, which both may affect the\nobserved trends. This chapter describes the latest insights in dust evolution\nand disk continuum observations. Specifically, disk populations and\nevolutionary trends are described, as well as the uncertainties therein, and\ncompared with exoplanet demographics.",
            "author": [
                "Nienke van der Marel",
                "Paola Pinilla"
            ],
            "link": [
                "http://arxiv.org/abs/2310.09077v1",
                "http://arxiv.org/pdf/2310.09077v1"
            ],
            "primary_category": "astro-ph.EP",
            "category": [
                "astro-ph.EP",
                "astro-ph.SR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.09058v1",
            "title": "A Note on Eigenvalues of Cayley Graphs",
            "updated": "2023-10-13T12:26:25Z",
            "published": "2023-10-13T12:26:25Z",
            "summary": "A graph is called integral if all its eigenvalues are integers. A Cayley\ngraph is called normal if its connection set is a union of conjugacy classes.\nWe show that a non-empty integral normal Cayley graph for a group of odd order\nhas an odd eigenvalue.",
            "author": [
                "Arnbj\u00f6rg Soff\u00eda \u00c1rnad\u00f3ttir",
                "Chris Godsil"
            ],
            "link": [
                "http://arxiv.org/abs/2310.09058v1",
                "http://arxiv.org/pdf/2310.09058v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "math.GR",
                "05C50 (Primary) 05C25 (Secondary)"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.09045v1",
            "title": "On the fixation probability of an advantageous allele in a population\n  with skewed offspring distribution",
            "updated": "2023-10-13T12:13:01Z",
            "published": "2023-10-13T12:13:01Z",
            "summary": "Consider an advantageous allele that arises in a haploid population of size\n$N$ evolving in continuous time according to a skewed reproduction mechanism,\nwhich generates under neutrality genealogies lying in the domain of attraction\nof a Beta$(2-\\alpha, \\alpha)$-coalescent for $\\alpha \\in (1,2)$. We prove in a\nsetting of moderate selection that the fixation probability $\\pi_N$ of the\nadvantageous allele is asymptotically equal to $\\alpha^{1/(\\alpha-1)}\ns_N^{1/(\\alpha-1)}$ , where $s_N$ is the selection strength of the advantageous\nallele. Our proof uses duality with a suitable $\\Lambda$-ancestral selection\ngraph.",
            "author": [
                "Matthias Birkner",
                "Florin Boenkost",
                "Iulia Dahmer",
                "Cornelia Pokalyuk"
            ],
            "link": [
                "http://arxiv.org/abs/2310.09045v1",
                "http://arxiv.org/pdf/2310.09045v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR",
                "Primary: 60J95, Secondary: 60J28, 92D15, 60J90"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.09044v1",
            "title": "KCTS: Knowledge-Constrained Tree Search Decoding with Token-Level\n  Hallucination Detection",
            "updated": "2023-10-13T12:12:34Z",
            "published": "2023-10-13T12:12:34Z",
            "summary": "Large Language Models (LLMs) have demonstrated remarkable human-level natural\nlanguage generation capabilities. However, their potential to generate\nmisinformation, often called the hallucination problem, poses a significant\nrisk to their deployment. A common approach to address this issue is to\nretrieve relevant knowledge and fine-tune the LLM with the knowledge in its\ninput. Unfortunately, this method incurs high training costs and may cause\ncatastrophic forgetting for multi-tasking models. To overcome these\nlimitations, we propose a knowledge-constrained decoding method called KCTS\n(Knowledge-Constrained Tree Search), which guides a frozen LM to generate text\naligned with the reference knowledge at each decoding step using a knowledge\nclassifier score and MCTS (Monte-Carlo Tree Search). To adapt the\nsequence-level knowledge classifier to token-level guidance, we also propose a\nnovel token-level hallucination detection method called RIPA (Reward Inflection\nPoint Approximation). Our empirical results on knowledge-grounded dialogue and\nabstractive summarization demonstrate the strength of KCTS as a plug-and-play,\nmodel-agnostic decoding method that can effectively reduce hallucinations in\nnatural language generation.",
            "author": [
                "Sehyun Choi",
                "Tianqing Fang",
                "Zhaowei Wang",
                "Yangqiu Song"
            ],
            "link": [
                "http://arxiv.org/abs/2310.09044v1",
                "http://arxiv.org/pdf/2310.09044v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.09042v2",
            "title": "Improving power-grid systems via topological changes, or how\n  self-organized criticality can help stability",
            "updated": "2023-11-03T15:26:31Z",
            "published": "2023-10-13T12:08:53Z",
            "summary": "Cascade failures in power grids occur when the failure of one component or\nsubsystem causes a chain reaction of failures in other components or\nsubsystems, ultimately leading to a widespread blackout or outage. Controlling\ncascade failures on power grids is important for many reasons like economic\nimpact, national security, public safety and even rippled effects like\ntroubling transportation systems. Monitoring the networks on node level has\nbeen suggested by many, either controlling all nodes of a network or by\nsubsets. This study identifies sensitive graph elements of the weighted\nEuropean power-grids (from 2016, 2022) by two different methods. Bridges are\ndetermined between communities and \"weak\" nodes are selected by the lowest\nlocal synchronization of the swing equation. In the latter case we add bypasses\nof the same number as the bridges at weak nodes, and we compare the\nsynchronization, cascade failure behavior by the dynamical improvement with the\npurely topological changes. The results are also compared if bridges are\nremoved from networks, which results in a case similar to islanding, and with\nthe addition of links at randomly selected places. Bypassing was found to\nimprove synchronization the best, while the average cascade sizes are the\nlowest with bridge additions. However, for very large or small global couplings\nthese network changes do not help, they seem to be useful near the\nsynchronization transition region, where self-organization drives the\npower-grid. Thus, we provide a demonstration for the Braess' Paradox on\ncontinent-sized power grid simulations and uncover the limitations of this\nphenomenon. We also determine the cascade size distributions and justify the\npower-law tails near the transition point on these grids.",
            "author": [
                "G\u00e9za \u00d3dor",
                "Istv\u00e1n Papp",
                "Krist\u00f3f Benedek",
                "B\u00e1lint Hartmann"
            ],
            "link": [
                "http://arxiv.org/abs/2310.09042v2",
                "http://arxiv.org/pdf/2310.09042v2"
            ],
            "primary_category": "physics.soc-ph",
            "category": [
                "physics.soc-ph",
                "cond-mat.dis-nn",
                "cond-mat.stat-mech",
                "nlin.AO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.09038v2",
            "title": "The exponential logic of sequentialization",
            "updated": "2023-11-17T20:32:52Z",
            "published": "2023-10-13T11:58:48Z",
            "summary": "Linear logic has provided new perspectives on proof-theory, denotational\nsemantics and the study of programming languages. One of its main successes are\nproof-nets, canonical representations of proofs that lie at the intersection\nbetween logic and graph theory. In the case of the minimalist proof-system of\nmultiplicative linear logic without units (MLL), these two aspects are\ncompletely fused: proof-nets for this system are graphs satisfying a\ncorrectness criterion that can be fully expressed in the language of graphs.\n  For more expressive logical systems (containing logical constants,\nquantifiers and exponential modalities), this is not completely the case. The\npurely graphical approach of proof-nets deprives them of any sequential\nstructure that is crucial to represent the order in which arguments are\npresented, which is necessary for these extensions. Rebuilding this order of\npresentation - sequentializing the graph - is thus a requirement for a graph to\nbe logical. Presentations and study of the artifacts ensuring that\nsequentialization can be done, such as boxes or jumps, are an integral part of\nresearches on linear logic.\n  Jumps, extensively studied by Faggian and di Giamberardino, can express\nintermediate degrees of sequentialization between a sequent calculus proof and\na fully desequentialized proof-net. We propose to analyze the logical strength\nof jumps by internalizing them in an extention of MLL where axioms on a\nspecific formula, the jumping formula, introduce constrains on the possible\nsequentializations. The jumping formula needs to be treated non-linearly, which\nwe do either axiomatically, or by embedding it in a very controlled fragment of\nmultiplicative-exponential linear logic, uncovering the exponential logic of\nsequentialization.",
            "author": [
                "Aurore Alcolei",
                "Luc Pellissier",
                "Alexis Saurin"
            ],
            "link": [
                "http://arxiv.org/abs/2310.09038v2",
                "http://arxiv.org/pdf/2310.09038v2"
            ],
            "primary_category": "cs.LO",
            "category": [
                "cs.LO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.09033v2",
            "title": "Tetravalent distance magic graphs of small order and an infinite family\n  of examples",
            "updated": "2023-12-07T17:16:03Z",
            "published": "2023-10-13T11:51:57Z",
            "summary": "A graph of order $n$ is distance magic if it admits a bijective labeling of\nits vertices with integers from $1$ to $n$ such that each vertex has the same\nsum of the labels of its neighbors.\n  This paper contributes to the long term project of characterizing all\ntetravalent distance magic graphs. With the help of a computer we find that out\nof almost nine million connected tetravalent graphs up to order $16$ only nine\nare distance magic. In fact, besides the six well known wreath graphs there are\nonly three other examples, one of each of the orders $12$, $14$ and $16$. We\nintroduce a generalization of wreath graphs, the so-called quasi wreath graphs,\nand classify all distance magic graphs among them. This way we obtain\ninfinitely many new tetravalent distance magic graphs. Moreover, the two\nnon-wreath graphs of orders $12$ and $14$ are quasi wreath graphs while the one\nof order $16$ can be obtained from a quasi wreath graph of order $14$ using a\nsimple construction due to Kov\\'a\\v{r}, Fron\\v{c}ek and Kov\\'a\\v{r}ov\\'a.",
            "author": [
                "Ksenija Rozman",
                "Primo\u017e \u0160parl"
            ],
            "link": [
                "http://arxiv.org/abs/2310.09033v2",
                "http://arxiv.org/pdf/2310.09033v2"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C78"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.09028v1",
            "title": "Subspace Adaptation Prior for Few-Shot Learning",
            "updated": "2023-10-13T11:40:18Z",
            "published": "2023-10-13T11:40:18Z",
            "summary": "Gradient-based meta-learning techniques aim to distill useful prior knowledge\nfrom a set of training tasks such that new tasks can be learned more\nefficiently with gradient descent. While these methods have achieved successes\nin various scenarios, they commonly adapt all parameters of trainable layers\nwhen learning new tasks. This neglects potentially more efficient learning\nstrategies for a given task distribution and may be susceptible to overfitting,\nespecially in few-shot learning where tasks must be learned from a limited\nnumber of examples. To address these issues, we propose Subspace Adaptation\nPrior (SAP), a novel gradient-based meta-learning algorithm that jointly learns\ngood initialization parameters (prior knowledge) and layer-wise parameter\nsubspaces in the form of operation subsets that should be adaptable. In this\nway, SAP can learn which operation subsets to adjust with gradient descent\nbased on the underlying task distribution, simultaneously decreasing the risk\nof overfitting when learning new tasks. We demonstrate that this ability is\nhelpful as SAP yields superior or competitive performance in few-shot image\nclassification settings (gains between 0.1% and 3.9% in accuracy). Analysis of\nthe learned subspaces demonstrates that low-dimensional operations often yield\nhigh activation strengths, indicating that they may be important for achieving\ngood few-shot learning performance. For reproducibility purposes, we publish\nall our research code publicly.",
            "author": [
                "Mike Huisman",
                "Aske Plaat",
                "Jan N. van Rijn"
            ],
            "link": [
                "http://arxiv.org/abs/2310.09028v1",
                "http://arxiv.org/pdf/2310.09028v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.09012v1",
            "title": "Weil pairing on twisted curves",
            "updated": "2023-10-13T11:20:22Z",
            "published": "2023-10-13T11:20:22Z",
            "summary": "We define and study the Weil pairing on the moduli of twisted curves. If $X$\nis a twisted curve, then we can combinatorially describe a certain subgroup and\na quotient group of $\\text{Pic}(X)[2]$ that are Weil dual. Moreover, the\npairing between them can be realized as combinatorial integration-homology\npairing on the dual graph of $X$. We also prove that the kernel of the\ntropicalization is isotropic for the Weil pairing.",
            "author": [
                "Ashwin Deopurkar"
            ],
            "link": [
                "http://arxiv.org/abs/2310.09012v1",
                "http://arxiv.org/pdf/2310.09012v1"
            ],
            "primary_category": "math.AG",
            "category": [
                "math.AG",
                "14H10, 14T15"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.09009v1",
            "title": "The Complexity of Homomorphism Reconstructibility",
            "updated": "2023-10-13T11:10:22Z",
            "published": "2023-10-13T11:10:22Z",
            "summary": "Representing graphs by their homomorphism counts has led to the beautiful\ntheory of homomorphism indistinguishability in recent years. Moreover,\nhomomorphism counts have promising applications in database theory and machine\nlearning, where one would like to answer queries or classify graphs solely\nbased on the representation of a graph $G$ as a finite vector of homomorphism\ncounts from some fixed finite set of graphs to $G$. We study the computational\ncomplexity of the arguably most fundamental computational problem associated to\nthese representations, the homomorphism reconstructability problem: given a\nfinite sequence of graphs and a corresponding vector of natural numbers, decide\nwhether there exists a graph $G$ that realises the given vector as the\nhomomorphism counts from the given graphs.\n  We show that this problem yields a natural example of an\n$\\mathsf{NP}^{#\\mathsf{P}}$-hard problem, which still can be $\\mathsf{NP}$-hard\nwhen restricted to a fixed number of input graphs of bounded treewidth and a\nfixed input vector of natural numbers, or alternatively, when restricted to a\nfinite input set of graphs. We further show that, when restricted to a finite\ninput set of graphs and given an upper bound on the order of the graph $G$ as\nadditional input, the problem cannot be $\\mathsf{NP}$-hard unless $\\mathsf{P} =\n\\mathsf{NP}$. For this regime, we obtain partial positive results. We also\ninvestigate the problem's parameterised complexity and provide fpt-algorithms\nfor the case that a single graph is given and that multiple graphs of the same\norder with subgraph instead of homomorphism counts are given.",
            "author": [
                "Jan B\u00f6ker",
                "Louis H\u00e4rtel",
                "Nina Runde",
                "Tim Seppelt",
                "Christoph Standke"
            ],
            "link": [
                "http://arxiv.org/abs/2310.09009v1",
                "http://arxiv.org/pdf/2310.09009v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "cs.DM",
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08996v3",
            "title": "Qualitative Analysis for Validating IEC 62443-4-2 Requirements in\n  DevSecOps",
            "updated": "2023-10-23T06:59:08Z",
            "published": "2023-10-13T10:24:58Z",
            "summary": "Validation of conformance to cybersecurity standards for industrial\nautomation and control systems is an expensive and time consuming process which\ncan delay the time to market. It is therefore crucial to introduce conformance\nvalidation stages into the continuous integration/continuous delivery pipeline\nof products. However, designing such conformance validation in an automated\nfashion is a highly non-trivial task that requires expert knowledge and depends\nupon the available security tools, ease of integration into the DevOps\npipeline, as well as support for IT and OT interfaces and protocols.\n  This paper addresses the aforementioned problem focusing on the automated\nvalidation of ISA/IEC 62443-4-2 standard component requirements. We present an\nextensive qualitative analysis of the standard requirements and the current\ntooling landscape to perform validation. Our analysis demonstrates the coverage\nestablished by the currently available tools and sheds light on current gaps to\nachieve full automation and coverage. Furthermore, we showcase for every\ncomponent requirement where in the CI/CD pipeline stage it is recommended to\ntest it and the tools to do so.",
            "author": [
                "Christian G\u00f6ttel",
                "Ma\u00eblle Kabir-Querrec",
                "David Kozhaya",
                "Thanikesavan Sivanthi",
                "Ognjen Vukovi\u0107"
            ],
            "link": [
                "http://dx.doi.org/10.1109/ETFA54631.2023.10275637",
                "http://arxiv.org/abs/2310.08996v3",
                "http://arxiv.org/pdf/2310.08996v3"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08991v1",
            "title": "The clustering of dark siren host galaxies",
            "updated": "2023-10-13T10:14:45Z",
            "published": "2023-10-13T10:14:45Z",
            "summary": "Dark sirens are a powerful way to infer cosmological and astrophysical\nparameters from the combination of gravitational wave sirens and galaxy\ncatalogues. Importantly, the method relies on the completeness of the galaxy\ncatalogues being well modelled. A magnitude-limited catalogue will always be\nincomplete to some extent, requiring a completion scheme to avoid biasing the\nparameter inference. Standard methods include homogeneous and multiplicative\ncompletion, which have the advantage of simplicity but underestimate or\noverestimate the amplitude of structure at low completeness, respectively. In\nthis work, we propose a new method to complete galaxy catalogues which uses\nclustering information to incorporate knowledge of the large scale structure\ninto the dark sirens method. We find that if the structure of the true number\nof galaxies is sufficiently well preserved in the catalogue, our estimator can\nperform drastically better than both homogeneous and multiplicative completion.\nWe lay the foundations for a maximally informative dark sirens analysis and\ndiscuss its limitations.",
            "author": [
                "Charles Dalang",
                "Tessa Baker"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08991v1",
                "http://arxiv.org/pdf/2310.08991v1"
            ],
            "primary_category": "astro-ph.CO",
            "category": [
                "astro-ph.CO",
                "gr-qc"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08990v1",
            "title": "Routing in Quantum Repeater Networks with Mixed Noise Figures",
            "updated": "2023-10-13T10:13:16Z",
            "published": "2023-10-13T10:13:16Z",
            "summary": "Quantum network holds the key to the next generation of secure communication,\nlong-distance communication, and quantum internet. Due to inherent quantum\neffects, routing in the quantum network is a major challenge. This study\nexplores a realistic approach to routing in quantum networks which aims to\nmirror real-world networks by segregating sources and destinations from the\nnetwork. By addressing practical constraints we examine the impact of\nheterogeneous nodes on network performance. In particular, we focused on\nperformance in terms of the ratio of high-quality to total nodes and path\nestablishment order. This work unveils relationships between them and\ncommunication path fidelity. It highlights the critical role of the fraction of\nhigh-quality nodes in end-to-end fidelity and explores the trade-offs between\nupgrading all nodes to high quality or retaining a subset of lower-quality\nnodes. Our simulations show that incorporating the knowledge of node quality\nnot only helps in strategically boosting the fidelities of some of the routing\npaths but also reduces the number of blocked paths in the quantum network.",
            "author": [
                "Vinay Kumar",
                "Claudio Cicconetti",
                "Marco Conti",
                "Andrea Passarella"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08990v1",
                "http://arxiv.org/pdf/2310.08990v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08975v1",
            "title": "ChatKBQA: A Generate-then-Retrieve Framework for Knowledge Base Question\n  Answering with Fine-tuned Large Language Models",
            "updated": "2023-10-13T09:45:14Z",
            "published": "2023-10-13T09:45:14Z",
            "summary": "Knowledge Base Question Answering (KBQA) aims to derive answers to natural\nlanguage questions over large-scale knowledge bases (KBs), which are generally\ndivided into two research components: knowledge retrieval and semantic parsing.\nHowever, three core challenges remain, including inefficient knowledge\nretrieval, retrieval errors adversely affecting semantic parsing, and the\ncomplexity of previous KBQA methods. In the era of large language models\n(LLMs), we introduce ChatKBQA, a novel generate-then-retrieve KBQA framework\nbuilt on fine-tuning open-source LLMs such as Llama-2, ChatGLM2 and Baichuan2.\nChatKBQA proposes generating the logical form with fine-tuned LLMs first, then\nretrieving and replacing entities and relations through an unsupervised\nretrieval method, which improves both generation and retrieval more\nstraightforwardly. Experimental results reveal that ChatKBQA achieves new\nstate-of-the-art performance on standard KBQA datasets, WebQSP, and\nComplexWebQuestions (CWQ). This work also provides a new paradigm for combining\nLLMs with knowledge graphs (KGs) for interpretable and knowledge-required\nquestion answering. Our code is publicly available.",
            "author": [
                "Haoran Luo",
                "Haihong E",
                "Zichen Tang",
                "Shiyao Peng",
                "Yikai Guo",
                "Wentai Zhang",
                "Chenghao Ma",
                "Guanting Dong",
                "Meina Song",
                "Wei Lin"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08975v1",
                "http://arxiv.org/pdf/2310.08975v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08948v1",
            "title": "Federated Class-Incremental Learning with Prompting",
            "updated": "2023-10-13T08:35:02Z",
            "published": "2023-10-13T08:35:02Z",
            "summary": "As Web technology continues to develop, it has become increasingly common to\nuse data stored on different clients. At the same time, federated learning has\nreceived widespread attention due to its ability to protect data privacy when\nlet models learn from data which is distributed across various clients.\nHowever, most existing works assume that the client's data are fixed. In\nreal-world scenarios, such an assumption is most likely not true as data may be\ncontinuously generated and new classes may also appear. To this end, we focus\non the practical and challenging federated class-incremental learning (FCIL)\nproblem. For FCIL, the local and global models may suffer from catastrophic\nforgetting on old classes caused by the arrival of new classes and the data\ndistributions of clients are non-independent and identically distributed\n(non-iid).\n  In this paper, we propose a novel method called Federated Class-Incremental\nLearning with PrompTing (FCILPT). Given the privacy and limited memory, FCILPT\ndoes not use a rehearsal-based buffer to keep exemplars of old data. We choose\nto use prompts to ease the catastrophic forgetting of the old classes.\nSpecifically, we encode the task-relevant and task-irrelevant knowledge into\nprompts, preserving the old and new knowledge of the local clients and solving\nthe problem of catastrophic forgetting. We first sort the task information in\nthe prompt pool in the local clients to align the task information on different\nclients before global aggregation. It ensures that the same task's knowledge\nare fully integrated, solving the problem of non-iid caused by the lack of\nclasses among different clients in the same incremental task. Experiments on\nCIFAR-100, Mini-ImageNet, and Tiny-ImageNet demonstrate that FCILPT achieves\nsignificant accuracy improvements over the state-of-the-art methods.",
            "author": [
                "Jiale Liu",
                "Yu-Wei Zhan",
                "Chong-Yu Zhang",
                "Xin Luo",
                "Zhen-Duo Chen",
                "Yinwei Wei",
                "Xin-Shun Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08948v1",
                "http://arxiv.org/pdf/2310.08948v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08943v2",
            "title": "Multi-level Adaptive Contrastive Learning for Knowledge Internalization\n  in Dialogue Generation",
            "updated": "2023-10-17T12:53:58Z",
            "published": "2023-10-13T08:16:27Z",
            "summary": "Knowledge-grounded dialogue generation aims to mitigate the issue of text\ndegeneration by incorporating external knowledge to supplement the context.\nHowever, the model often fails to internalize this information into responses\nin a human-like manner. Instead, it simply inserts segments of the provided\nknowledge into generic responses. As a result, the generated responses tend to\nbe tedious, incoherent, and in lack of interactivity which means the\ndegeneration problem is still unsolved. In this work, we first find that such\ncopying-style degeneration is primarily due to the weak likelihood objective,\nwhich allows the model to \"cheat\" the objective by merely duplicating knowledge\nsegments in a superficial pattern matching based on overlap. To overcome this\nchallenge, we then propose a Multi-level Adaptive Contrastive Learning (MACL)\nframework that dynamically samples negative examples and subsequently penalizes\ndegeneration behaviors at both the token-level and sequence-level. Extensive\nexperiments on the WoW dataset demonstrate the effectiveness of our approach\nacross various pre-trained models.",
            "author": [
                "Chenxu Yang",
                "Zheng Lin",
                "Lanrui Wang",
                "Chong Tian",
                "Liang Pang",
                "Jiangnan Li",
                "Qirong Ho",
                "Yanan Cao",
                "Weiping Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08943v2",
                "http://arxiv.org/pdf/2310.08943v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08928v1",
            "title": "SIDE: Self-supervised Intermediate Domain Exploration for Source-free\n  Domain Adaptation",
            "updated": "2023-10-13T07:50:37Z",
            "published": "2023-10-13T07:50:37Z",
            "summary": "Domain adaptation aims to alleviate the domain shift when transferring the\nknowledge learned from the source domain to the target domain. Due to privacy\nissues, source-free domain adaptation (SFDA), where source data is unavailable\nduring adaptation, has recently become very demanding yet challenging. Existing\nSFDA methods focus on either self-supervised learning of target samples or\nreconstruction of virtual source data. The former overlooks the transferable\nknowledge in the source model, whilst the latter introduces even more\nuncertainty. To address the above issues, this paper proposes self-supervised\nintermediate domain exploration (SIDE) that effectively bridges the domain gap\nwith an intermediate domain, where samples are cyclically filtered out in a\nself-supervised fashion. First, we propose cycle intermediate domain filtering\n(CIDF) to cyclically select intermediate samples with similar distributions\nover source and target domains. Second, with the aid of those intermediate\nsamples, an inter-domain gap transition (IDGT) module is developed to mitigate\npossible distribution mismatches between the source and target data. Finally,\nwe introduce cross-view consistency learning (CVCL) to maintain the intrinsic\nclass discriminability whilst adapting the model to the target domain.\nExtensive experiments on three popular benchmarks, i.e. Office-31, Office-Home\nand VisDA-C, show that our proposed SIDE achieves competitive performance\nagainst state-of-the-art methods.",
            "author": [
                "Jiamei Liu",
                "Han Sun",
                "Yizhen Jia",
                "Jie Qin",
                "Huiyu Zhou",
                "Ningzhong Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08928v1",
                "http://arxiv.org/pdf/2310.08928v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08922v1",
            "title": "LLaMA Rider: Spurring Large Language Models to Explore the Open World",
            "updated": "2023-10-13T07:47:44Z",
            "published": "2023-10-13T07:47:44Z",
            "summary": "Recently, various studies have leveraged Large Language Models (LLMs) to help\ndecision-making and planning in environments, and try to align the LLMs'\nknowledge with the world conditions. Nonetheless, the capacity of LLMs to\ncontinuously acquire environmental knowledge and adapt in an open world remains\nuncertain. In this paper, we propose an approach to spur LLMs to explore the\nopen world, gather experiences, and learn to improve their task-solving\ncapabilities. In this approach, a multi-round feedback-revision mechanism is\nutilized to encourage LLMs to actively select appropriate revision actions\nguided by feedback information from the environment. This facilitates\nexploration and enhances the model's performance. Besides, we integrate\nsub-task relabeling to assist LLMs in maintaining consistency in sub-task\nplanning and help the model learn the combinatorial nature between tasks,\nenabling it to complete a wider range of tasks through training based on the\nacquired exploration experiences. By evaluation in Minecraft, an open-ended\nsandbox world, we demonstrate that our approach LLaMA-Rider enhances the\nefficiency of the LLM in exploring the environment, and effectively improves\nthe LLM's ability to accomplish more tasks through fine-tuning with merely 1.3k\ninstances of collected data, showing minimal training costs compared to the\nbaseline using reinforcement learning.",
            "author": [
                "Yicheng Feng",
                "Yuxuan Wang",
                "Jiazheng Liu",
                "Sipeng Zheng",
                "Zongqing Lu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08922v1",
                "http://arxiv.org/pdf/2310.08922v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08919v1",
            "title": "Elementary Constructions of conic sections",
            "updated": "2023-10-13T07:41:46Z",
            "published": "2023-10-13T07:41:46Z",
            "summary": "In classical geometry, there is no such well-known and much-studied topic as\nthe construction of conic sections (or briefly conics) from its five points.\nIts importance in many applications of mechanical engineering, civil\nengineering and architectural engineering, as well as other applied sciences is\nclear. The beauty of the topic is that it raises difficult questions that can\nbe approached with basic tools. In this article, we provide constructions (and\ncorresponding theories) that can be taught to high school and university\nstudents without knowledge of projective geometry. For this, we recall some\nimportant facts about conic sections that can be found in the rich literature.\nWe use the concepts of power of a point on a circle, similarity, orthogonal\naffinity and inversion. We also mention famous constructions related to our\nquestions. We begin our article at this point, where the standard teaching ends\nthe discussion of conic sections. We therefore assume that the reader knows the\nbasic definitions and constructions of conics, the concepts of focus, axis,\ntangent, leading circle and leading line.",
            "author": [
                "\u00c1kos G. Horv\u00e1th"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08919v1",
                "http://arxiv.org/pdf/2310.08919v1"
            ],
            "primary_category": "math.HO",
            "category": [
                "math.HO",
                "math.MG",
                "00A35, 51M15, 51N05"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08917v1",
            "title": "Relation-aware Ensemble Learning for Knowledge Graph Embedding",
            "updated": "2023-10-13T07:40:12Z",
            "published": "2023-10-13T07:40:12Z",
            "summary": "Knowledge graph (KG) embedding is a fundamental task in natural language\nprocessing, and various methods have been proposed to explore semantic patterns\nin distinctive ways. In this paper, we propose to learn an ensemble by\nleveraging existing methods in a relation-aware manner. However, exploring\nthese semantics using relation-aware ensemble leads to a much larger search\nspace than general ensemble methods. To address this issue, we propose a\ndivide-search-combine algorithm RelEns-DSC that searches the relation-wise\nensemble weights independently. This algorithm has the same computation cost as\ngeneral ensemble methods but with much better performance. Experimental results\non benchmark datasets demonstrate the effectiveness of the proposed method in\nefficiently searching relation-aware ensemble weights and achieving\nstate-of-the-art embedding performance. The code is public at\nhttps://github.com/LARS-research/RelEns.",
            "author": [
                "Ling Yue",
                "Yongqi Zhang",
                "Quanming Yao",
                "Yong Li",
                "Xian Wu",
                "Ziheng Zhang",
                "Zhenxi Lin",
                "Yefeng Zheng"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08917v1",
                "http://arxiv.org/pdf/2310.08917v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08910v1",
            "title": "Scalarization for Multi-Task and Multi-Domain Learning at Scale",
            "updated": "2023-10-13T07:31:04Z",
            "published": "2023-10-13T07:31:04Z",
            "summary": "Training a single model on multiple input domains and/or output tasks allows\nfor compressing information from multiple sources into a unified backbone hence\nimproves model efficiency. It also enables potential positive knowledge\ntransfer across tasks/domains, leading to improved accuracy and data-efficient\ntraining. However, optimizing such networks is a challenge, in particular due\nto discrepancies between the different tasks or domains: Despite several\nhypotheses and solutions proposed over the years, recent work has shown that\nuniform scalarization training, i.e., simply minimizing the average of the task\nlosses, yields on-par performance with more costly SotA optimization methods.\nThis raises the issue of how well we understand the training dynamics of\nmulti-task and multi-domain networks. In this work, we first devise a\nlarge-scale unified analysis of multi-domain and multi-task learning to better\nunderstand the dynamics of scalarization across varied task/domain combinations\nand model sizes. Following these insights, we then propose to leverage\npopulation-based training to efficiently search for the optimal scalarization\nweights when dealing with a large number of tasks or domains.",
            "author": [
                "Amelie Royer",
                "Tijmen Blankevoort",
                "Babak Ehteshami Bejnordi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08910v1",
                "http://arxiv.org/pdf/2310.08910v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08909v1",
            "title": "Community Membership Hiding as Counterfactual Graph Search via Deep\n  Reinforcement Learning",
            "updated": "2023-10-13T07:30:50Z",
            "published": "2023-10-13T07:30:50Z",
            "summary": "Community detection techniques are useful tools for social media platforms to\ndiscover tightly connected groups of users who share common interests. However,\nthis functionality often comes at the expense of potentially exposing\nindividuals to privacy breaches by inadvertently revealing their tastes or\npreferences. Therefore, some users may wish to safeguard their anonymity and\nopt out of community detection for various reasons, such as affiliation with\npolitical or religious organizations.\n  In this study, we address the challenge of community membership hiding, which\ninvolves strategically altering the structural properties of a network graph to\nprevent one or more nodes from being identified by a given community detection\nalgorithm. We tackle this problem by formulating it as a constrained\ncounterfactual graph objective, and we solve it via deep reinforcement\nlearning. We validate the effectiveness of our method through two distinct\ntasks: node and community deception. Extensive experiments show that our\napproach overall outperforms existing baselines in both tasks.",
            "author": [
                "Andrea Bernini",
                "Fabrizio Silvestri",
                "Gabriele Tolomei"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08909v1",
                "http://arxiv.org/pdf/2310.08909v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08885v1",
            "title": "InstructTODS: Large Language Models for End-to-End Task-Oriented\n  Dialogue Systems",
            "updated": "2023-10-13T06:36:26Z",
            "published": "2023-10-13T06:36:26Z",
            "summary": "Large language models (LLMs) have been used for diverse tasks in natural\nlanguage processing (NLP), yet remain under-explored for task-oriented dialogue\nsystems (TODS), especially for end-to-end TODS. We present InstructTODS, a\nnovel off-the-shelf framework for zero-shot end-to-end task-oriented dialogue\nsystems that can adapt to diverse domains without fine-tuning. By leveraging\nLLMs, InstructTODS generates a proxy belief state that seamlessly translates\nuser intentions into dynamic queries for efficient interaction with any KB. Our\nextensive experiments demonstrate that InstructTODS achieves comparable\nperformance to fully fine-tuned TODS in guiding dialogues to successful\ncompletion without prior knowledge or task-specific data. Furthermore, a\nrigorous human evaluation of end-to-end TODS shows that InstructTODS produces\ndialogue responses that notably outperform both the gold responses and the\nstate-of-the-art TODS in terms of helpfulness, informativeness, and humanness.\nMoreover, the effectiveness of LLMs in TODS is further supported by our\ncomprehensive evaluations on TODS subtasks: dialogue state tracking, intent\nclassification, and response generation. Code and implementations could be\nfound here https://github.com/WillyHC22/InstructTODS/",
            "author": [
                "Willy Chung",
                "Samuel Cahyawijaya",
                "Bryan Wilie",
                "Holy Lovenia",
                "Pascale Fung"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08885v1",
                "http://arxiv.org/pdf/2310.08885v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08884v1",
            "title": "Extending Multi-modal Contrastive Representations",
            "updated": "2023-10-13T06:34:23Z",
            "published": "2023-10-13T06:34:23Z",
            "summary": "Multi-modal contrastive representation (MCR) of more than three modalities is\ncritical in multi-modal learning. Although recent methods showcase impressive\nachievements, the high dependence on large-scale, high-quality paired data and\nthe expensive training costs limit their further development. Inspired by\nrecent C-MCR, this paper proposes Extending Multimodal Contrastive\nRepresentation (Ex-MCR), a training-efficient and paired-data-free method to\nflexibly learn unified contrastive representation space for more than three\nmodalities by integrating the knowledge of existing MCR spaces. Specifically,\nEx-MCR aligns multiple existing MCRs into the same based MCR, which can\neffectively preserve the original semantic alignment of the based MCR. Besides,\nwe comprehensively enhance the entire learning pipeline for aligning MCR spaces\nfrom the perspectives of training data, architecture, and learning objectives.\nWith the preserved original modality alignment and the enhanced space\nalignment, Ex-MCR shows superior representation learning performance and\nexcellent modality extensibility. To demonstrate the effectiveness of Ex-MCR,\nwe align the MCR spaces of CLAP (audio-text) and ULIP (3D-vision) into the CLIP\n(vision-text), leveraging the overlapping text and image modality,\nrespectively. Remarkably, without using any paired data, Ex-MCR learns a\n3D-image-text-audio unified contrastive representation, and it achieves\nstate-of-the-art performance on audio-visual, 3D-image, audio-text, visual-text\nretrieval, and 3D object classification tasks. More importantly, extensive\nqualitative results further demonstrate the emergent semantic alignment between\nthe extended modalities (e.g., audio and 3D), which highlights the great\npotential of modality extensibility.",
            "author": [
                "Zehan Wang",
                "Ziang Zhang",
                "Luping Liu",
                "Yang Zhao",
                "Haifeng Huang",
                "Tao Jin",
                "Zhou Zhao"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08884v1",
                "http://arxiv.org/pdf/2310.08884v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08880v1",
            "title": "On the sum of the two largest signless Laplacian eigenvalues",
            "updated": "2023-10-13T06:19:32Z",
            "published": "2023-10-13T06:19:32Z",
            "summary": "Let $G$ be a simple connected graph and let $S_k(G)$ be the sum of the first\n$k$ largest signless Laplacian eigenvalues of $G$. It was conjectured by\nAshraf, Omidi and Tayfeh-Rezaibe in 2013 that $S_k(G)\\leq e(G)+\\binom{k+1}{2}$\nholds for $1\\leq k\\leq n-1$. They gave a proof for the conjecture when $k = 2$,\nbut applied an incorrect key lemma. Therefore, the conjecture is still open\nwhen $k = 2$. In this paper, we prove that $S_2(G)<e(G)+3$ is true for any\ngraphs which also confirm the conjecture when $k = 2$.",
            "author": [
                "Zi-Ming Zhou",
                "Chang-Xiang He"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08880v1",
                "http://arxiv.org/pdf/2310.08880v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08877v2",
            "title": "Retrieval-Generation Alignment for End-to-End Task-Oriented Dialogue\n  System",
            "updated": "2023-10-20T09:18:42Z",
            "published": "2023-10-13T06:03:47Z",
            "summary": "Developing an efficient retriever to retrieve knowledge from a large-scale\nknowledge base (KB) is critical for task-oriented dialogue systems to\neffectively handle localized and specialized tasks. However, widely used\ngenerative models such as T5 and ChatGPT often struggle to differentiate subtle\ndifferences among the retrieved KB records when generating responses, resulting\nin suboptimal quality of generated responses. In this paper, we propose the\napplication of maximal marginal likelihood to train a perceptive retriever by\nutilizing signals from response generation for supervision. In addition, our\napproach goes beyond considering solely retrieved entities and incorporates\nvarious meta knowledge to guide the generator, thus improving the utilization\nof knowledge. We evaluate our approach on three task-oriented dialogue datasets\nusing T5 and ChatGPT as the backbone models. The results demonstrate that when\ncombined with meta knowledge, the response generator can effectively leverage\nhigh-quality knowledge records from the retriever and enhance the quality of\ngenerated responses. The codes and models of this paper are available at\nhttps://github.com/shenwzh3/MK-TOD.",
            "author": [
                "Weizhou Shen",
                "Yingqi Gao",
                "Canbin Huang",
                "Fanqi Wan",
                "Xiaojun Quan",
                "Wei Bi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08877v2",
                "http://arxiv.org/pdf/2310.08877v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08869v1",
            "title": "Learning to Behave Like Clean Speech: Dual-Branch Knowledge Distillation\n  for Noise-Robust Fake Audio Detection",
            "updated": "2023-10-13T05:37:29Z",
            "published": "2023-10-13T05:37:29Z",
            "summary": "Most research in fake audio detection (FAD) focuses on improving performance\non standard noise-free datasets. However, in actual situations, there is\nusually noise interference, which will cause significant performance\ndegradation in FAD systems. To improve the noise robustness, we propose a\ndual-branch knowledge distillation fake audio detection (DKDFAD) method.\nSpecifically, a parallel data flow of the clean teacher branch and the noisy\nstudent branch is designed, and interactive fusion and response-based\nteacher-student paradigms are proposed to guide the training of noisy data from\nthe data distribution and decision-making perspectives. In the noise branch,\nspeech enhancement is first introduced for denoising, which reduces the\ninterference of strong noise. The proposed interactive fusion combines\ndenoising features and noise features to reduce the impact of speech distortion\nand seek consistency with the data distribution of clean branch. The\nteacher-student paradigm maps the student's decision space to the teacher's\ndecision space, making noisy speech behave as clean. In addition, a joint\ntraining method is used to optimize the two branches to achieve global\noptimality. Experimental results based on multiple datasets show that the\nproposed method performs well in noisy environments and maintains performance\nin cross-dataset experiments.",
            "author": [
                "Cunhang Fan",
                "Mingming Ding",
                "Jianhua Tao",
                "Ruibo Fu",
                "Jiangyan Yi",
                "Zhengqi Wen",
                "Zhao Lv"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08869v1",
                "http://arxiv.org/pdf/2310.08869v1"
            ],
            "primary_category": "cs.SD",
            "category": [
                "cs.SD",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08866v1",
            "title": "Adaptivity and Modularity for Efficient Generalization Over Task\n  Complexity",
            "updated": "2023-10-13T05:29:09Z",
            "published": "2023-10-13T05:29:09Z",
            "summary": "Can transformers generalize efficiently on problems that require dealing with\nexamples with different levels of difficulty? We introduce a new task tailored\nto assess generalization over different complexities and present results that\nindicate that standard transformers face challenges in solving these tasks.\nThese tasks are variations of pointer value retrieval previously introduced by\nZhang et al. (2021). We investigate how the use of a mechanism for adaptive and\nmodular computation in transformers facilitates the learning of tasks that\ndemand generalization over the number of sequential computation steps (i.e.,\nthe depth of the computation graph). Based on our observations, we propose a\ntransformer-based architecture called Hyper-UT, which combines dynamic function\ngeneration from hyper networks with adaptive depth from Universal Transformers.\nThis model demonstrates higher accuracy and a fairer allocation of\ncomputational resources when generalizing to higher numbers of computation\nsteps. We conclude that mechanisms for adaptive depth and modularity complement\neach other in improving efficient generalization concerning example complexity.\nAdditionally, to emphasize the broad applicability of our findings, we\nillustrate that in a standard image recognition task, Hyper- UT's performance\nmatches that of a ViT model but with considerably reduced computational demands\n(achieving over 70\\% average savings by effectively using fewer layers).",
            "author": [
                "Samira Abnar",
                "Omid Saremi",
                "Laurent Dinh",
                "Shantel Wilson",
                "Miguel Angel Bautista",
                "Chen Huang",
                "Vimal Thilak",
                "Etai Littwin",
                "Jiatao Gu",
                "Josh Susskind",
                "Samy Bengio"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08866v1",
                "http://arxiv.org/pdf/2310.08866v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08860v1",
            "title": "Guiding AMR Parsing with Reverse Graph Linearization",
            "updated": "2023-10-13T05:03:13Z",
            "published": "2023-10-13T05:03:13Z",
            "summary": "Abstract Meaning Representation (AMR) parsing aims to extract an abstract\nsemantic graph from a given sentence. The sequence-to-sequence approaches,\nwhich linearize the semantic graph into a sequence of nodes and edges and\ngenerate the linearized graph directly, have achieved good performance.\nHowever, we observed that these approaches suffer from structure loss\naccumulation during the decoding process, leading to a much lower F1-score for\nnodes and edges decoded later compared to those decoded earlier. To address\nthis issue, we propose a novel Reverse Graph Linearization (RGL) enhanced\nframework. RGL defines both default and reverse linearization orders of an AMR\ngraph, where most structures at the back part of the default order appear at\nthe front part of the reversed order and vice versa. RGL incorporates the\nreversed linearization to the original AMR parser through a two-pass\nself-distillation mechanism, which guides the model when generating the default\nlinearizations. Our analysis shows that our proposed method significantly\nmitigates the problem of structure loss accumulation, outperforming the\npreviously best AMR parsing model by 0.8 and 0.5 Smatch scores on the AMR 2.0\nand AMR 3.0 dataset, respectively. The code are available at\nhttps://github.com/pkunlp-icler/AMR_reverse_graph_linearization.",
            "author": [
                "Bofei Gao",
                "Liang Chen",
                "Peiyi Wang",
                "Zhifang Sui",
                "Baobao Chang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08860v1",
                "http://arxiv.org/pdf/2310.08860v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08855v1",
            "title": "Overcoming Recency Bias of Normalization Statistics in Continual\n  Learning: Balance and Adaptation",
            "updated": "2023-10-13T04:50:40Z",
            "published": "2023-10-13T04:50:40Z",
            "summary": "Continual learning entails learning a sequence of tasks and balancing their\nknowledge appropriately. With limited access to old training samples, much of\nthe current work in deep neural networks has focused on overcoming catastrophic\nforgetting of old tasks in gradient-based optimization. However, the\nnormalization layers provide an exception, as they are updated interdependently\nby the gradient and statistics of currently observed training samples, which\nrequire specialized strategies to mitigate recency bias. In this work, we focus\non the most popular Batch Normalization (BN) and provide an in-depth\ntheoretical analysis of its sub-optimality in continual learning. Our analysis\ndemonstrates the dilemma between balance and adaptation of BN statistics for\nincremental tasks, which potentially affects training stability and\ngeneralization. Targeting on these particular challenges, we propose Adaptive\nBalance of BN (AdaB$^2$N), which incorporates appropriately a Bayesian-based\nstrategy to adapt task-wise contributions and a modified momentum to balance BN\nstatistics, corresponding to the training and testing stages. By implementing\nBN in a continual learning fashion, our approach achieves significant\nperformance gains across a wide range of benchmarks, particularly for the\nchallenging yet realistic online scenarios (e.g., up to 7.68%, 6.86% and 4.26%\non Split CIFAR-10, Split CIFAR-100 and Split Mini-ImageNet, respectively). Our\ncode is available at https://github.com/lvyilin/AdaB2N.",
            "author": [
                "Yilin Lyu",
                "Liyuan Wang",
                "Xingxing Zhang",
                "Zicheng Sun",
                "Hang Su",
                "Jun Zhu",
                "Liping Jing"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08855v1",
                "http://arxiv.org/pdf/2310.08855v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08844v1",
            "title": "Impact of Stricter Content Moderation on Parler's Users' Discourse",
            "updated": "2023-10-13T04:09:39Z",
            "published": "2023-10-13T04:09:39Z",
            "summary": "Social media platforms employ various content moderation techniques to remove\nharmful, offensive, and hate speech content. The moderation level varies across\nplatforms; even over time, it can evolve in a platform. For example, Parler, a\nfringe social media platform popular among conservative users, was known to\nhave the least restrictive moderation policies, claiming to have open\ndiscussion spaces for their users. However, after linking the 2021 US Capitol\nRiots and the activity of some groups on Parler, such as QAnon and Proud Boys,\non January 12, 2021, Parler was removed from the Apple and Google App Store and\nsuspended from Amazon Cloud hosting service. Parler would have to modify their\nmoderation policies to return to these online stores. After a month of\ndowntime, Parler was back online with a new set of user guidelines, which\nreflected stricter content moderation, especially regarding the \\emph{hate\nspeech} policy.\n  In this paper, we studied the moderation changes performed by Parler and\ntheir effect on the toxicity of its content. We collected a large longitudinal\nParler dataset with 17M parleys from 432K active users from February 2021 to\nJanuary 2022, after its return to the Internet and App Store. To the best of\nour knowledge, this is the first study investigating the effectiveness of\ncontent moderation techniques using data-driven approaches and also the first\nParler dataset after its brief hiatus. Our quasi-experimental time series\nanalysis indicates that after the change in Parler's moderation, the severe\nforms of toxicity (above a threshold of 0.5) immediately decreased and\nsustained. In contrast, the trend did not change for less severe threats and\ninsults (a threshold between 0.5 - 0.7). Finally, we found an increase in the\nfactuality of the news sites being shared, as well as a decrease in the number\nof conspiracy or pseudoscience sources being shared.",
            "author": [
                "Nihal Kumarswamy",
                "Mohit Singhal",
                "Shirin Nilizadeh"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08844v1",
                "http://arxiv.org/pdf/2310.08844v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08840v1",
            "title": "Large Language Models as Source Planner for Personalized\n  Knowledge-grounded Dialogue",
            "updated": "2023-10-13T03:38:38Z",
            "published": "2023-10-13T03:38:38Z",
            "summary": "Open-domain dialogue system usually requires different sources of knowledge\nto generate more informative and evidential responses. However, existing\nknowledge-grounded dialogue systems either focus on a single knowledge source\nor overlook the dependency between multiple sources of knowledge, which may\nresult in generating inconsistent or even paradoxical responses. To incorporate\nmultiple knowledge sources and dependencies between them, we propose SAFARI, a\nnovel framework that leverages the exceptional capabilities of large language\nmodels (LLMs) in planning, understanding, and incorporating under both\nsupervised and unsupervised settings. Specifically, SAFARI decouples the\nknowledge grounding into multiple sources and response generation, which allows\neasy extension to various knowledge sources including the possibility of not\nusing any sources. To study the problem, we construct a personalized\nknowledge-grounded dialogue dataset \\textit{\\textbf{K}nowledge \\textbf{B}ehind\n\\textbf{P}ersona}~(\\textbf{KBP}), which is the first to consider the dependency\nbetween persona and implicit knowledge. Experimental results on the KBP dataset\ndemonstrate that the SAFARI framework can effectively produce\npersona-consistent and knowledge-enhanced responses.",
            "author": [
                "Hongru Wang",
                "Minda Hu",
                "Yang Deng",
                "Rui Wang",
                "Fei Mi",
                "Weichao Wang",
                "Yasheng Wang",
                "Wai-Chung Kwan",
                "Irwin King",
                "Kam-Fai Wong"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08840v1",
                "http://arxiv.org/pdf/2310.08840v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08836v1",
            "title": "A Framework for Few-Shot Policy Transfer through Observation Mapping and\n  Behavior Cloning",
            "updated": "2023-10-13T03:15:42Z",
            "published": "2023-10-13T03:15:42Z",
            "summary": "Despite recent progress in Reinforcement Learning for robotics applications,\nmany tasks remain prohibitively difficult to solve because of the expensive\ninteraction cost. Transfer learning helps reduce the training time in the\ntarget domain by transferring knowledge learned in a source domain. Sim2Real\ntransfer helps transfer knowledge from a simulated robotic domain to a physical\ntarget domain. Knowledge transfer reduces the time required to train a task in\nthe physical world, where the cost of interactions is high. However, most\nexisting approaches assume exact correspondence in the task structure and the\nphysical properties of the two domains. This work proposes a framework for\nFew-Shot Policy Transfer between two domains through Observation Mapping and\nBehavior Cloning. We use Generative Adversarial Networks (GANs) along with a\ncycle-consistency loss to map the observations between the source and target\ndomains and later use this learned mapping to clone the successful source task\nbehavior policy to the target domain. We observe successful behavior policy\ntransfer with limited target task interactions and in cases where the source\nand target task are semantically dissimilar.",
            "author": [
                "Yash Shukla",
                "Bharat Kesari",
                "Shivam Goel",
                "Robert Wright",
                "Jivko Sinapov"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08836v1",
                "http://arxiv.org/pdf/2310.08836v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08826v1",
            "title": "Revisiting Multi-modal 3D Semantic Segmentation in Real-world Autonomous\n  Driving",
            "updated": "2023-10-13T02:43:35Z",
            "published": "2023-10-13T02:43:35Z",
            "summary": "LiDAR and camera are two critical sensors for multi-modal 3D semantic\nsegmentation and are supposed to be fused efficiently and robustly to promise\nsafety in various real-world scenarios. However, existing multi-modal methods\nface two key challenges: 1) difficulty with efficient deployment and real-time\nexecution; and 2) drastic performance degradation under weak calibration\nbetween LiDAR and cameras. To address these challenges, we propose CPGNet-LCF,\na new multi-modal fusion framework extending the LiDAR-only CPGNet. CPGNet-LCF\nsolves the first challenge by inheriting the easy deployment and real-time\ncapabilities of CPGNet. For the second challenge, we introduce a novel weak\ncalibration knowledge distillation strategy during training to improve the\nrobustness against the weak calibration. CPGNet-LCF achieves state-of-the-art\nperformance on the nuScenes and SemanticKITTI benchmarks. Remarkably, it can be\neasily deployed to run in 20ms per frame on a single Tesla V100 GPU using\nTensorRT TF16 mode. Furthermore, we benchmark performance over four weak\ncalibration levels, demonstrating the robustness of our proposed approach.",
            "author": [
                "Feng Jiang",
                "Chaoping Tu",
                "Gang Zhang",
                "Jun Li",
                "Hanqing Huang",
                "Junyu Lin",
                "Di Feng",
                "Jian Pu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08826v1",
                "http://arxiv.org/pdf/2310.08826v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08820v2",
            "title": "SAM-guided Unsupervised Domain Adaptation for 3D Segmentation",
            "updated": "2023-10-16T01:55:18Z",
            "published": "2023-10-13T02:28:40Z",
            "summary": "Unsupervised domain adaptation (UDA) in 3D segmentation tasks presents a\nformidable challenge, primarily stemming from the sparse and unordered nature\nof point cloud data. Especially for LiDAR point clouds, the domain discrepancy\nbecomes obvious across varying capture scenes, fluctuating weather conditions,\nand the diverse array of LiDAR devices in use. While previous UDA methodologies\nhave often sought to mitigate this gap by aligning features between source and\ntarget domains, this approach falls short when applied to 3D segmentation due\nto the substantial domain variations. Inspired by the remarkable generalization\ncapabilities exhibited by the vision foundation model, SAM, in the realm of\nimage segmentation, our approach leverages the wealth of general knowledge\nembedded within SAM to unify feature representations across diverse 3D domains\nand further solves the 3D domain adaptation problem. Specifically, we harness\nthe corresponding images associated with point clouds to facilitate knowledge\ntransfer and propose an innovative hybrid feature augmentation methodology,\nwhich significantly enhances the alignment between the 3D feature space and\nSAM's feature space, operating at both the scene and instance levels. Our\nmethod is evaluated on many widely-recognized datasets and achieves\nstate-of-the-art performance.",
            "author": [
                "Xidong Peng",
                "Runnan Chen",
                "Feng Qiao",
                "Lingdong Kong",
                "Youquan Liu",
                "Tai Wang",
                "Xinge Zhu",
                "Yuexin Ma"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08820v2",
                "http://arxiv.org/pdf/2310.08820v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08806v2",
            "title": "Loops in surfaces, chord diagrams, interlace graphs: operad\n  factorisations and generating grammars",
            "updated": "2023-11-03T02:26:05Z",
            "published": "2023-10-13T01:30:10Z",
            "summary": "A filoop is a generic immersion of a circle in a closed oriented surface,\nwhose complement is a disjoint union of discs, considered up to orientation\npreserving diffeomorphisms. It gives rise to a chord diagram C which has an\ninterlace graph G, called a chordiagraph. For a graph G with even degrees, we\ncompute a quantity mg(G) which yields, for every chord diagram $C$ with\ninterlace graph G, the minimal genus of filoops with chord diagram C. If\nmg(G)=0 then C admits exactly two framings of genus 0, corresponding to\nspheriloops. After recalling the Cunningham factorisation of connected graphs,\nwe describe a canonical factorisation of filoops into spheric sums followed by\ntoric sums, for which the genus is additive. This is analogous to the\nfactorisation of compact connected 3-manifolds along spheres and tori. We\ndescribe unambiguous context-sensitive grammars generating the set of all\ngraphs and with mg(G)=0 and deduce stability properties with respect to spheric\nand toric factorisations. Similar results hold for chordiagraphs with mg(G) = 0\nand their corresponding spheriloops.",
            "author": [
                "Christopher-Lloyd Simon"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08806v2",
                "http://arxiv.org/pdf/2310.08806v2"
            ],
            "primary_category": "math.GT",
            "category": [
                "math.GT",
                "math.CO",
                "57M15, 05C10, 57K12, 81Q30"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08802v1",
            "title": "Multi-Robot Geometric Task-and-Motion Planning for Collaborative\n  Manipulation Tasks",
            "updated": "2023-10-13T01:20:17Z",
            "published": "2023-10-13T01:20:17Z",
            "summary": "We address multi-robot geometric task-and-motion planning (MR-GTAMP) problems\nin synchronous, monotone setups. The goal of the MR-GTAMP problem is to move\nobjects with multiple robots to goal regions in the presence of other movable\nobjects. We focus on collaborative manipulation tasks where the robots have to\nadopt intelligent collaboration strategies to be successful and effective,\ni.e., decide which robot should move which objects to which positions, and\nperform collaborative actions, such as handovers. To endow robots with these\ncollaboration capabilities, we propose to first collect occlusion and\nreachability information for each robot by calling motion-planning algorithms.\nWe then propose a method that uses the collected information to build a graph\nstructure which captures the precedence of the manipulations of different\nobjects and supports the implementation of a mixed-integer program to guide the\nsearch for highly effective collaborative task-and-motion plans. The search\nprocess for collaborative task-and-motion plans is based on a Monte-Carlo Tree\nSearch (MCTS) exploration strategy to achieve exploration-exploitation balance.\nWe evaluate our framework in two challenging MR-GTAMP domains and show that it\noutperforms two state-of-the-art baselines with respect to the planning time,\nthe resulting plan length and the number of objects moved. We also show that\nour framework can be applied to underground mining operations where a robotic\narm needs to coordinate with an autonomous roof bolter. We demonstrate plan\nexecution in two roof-bolting scenarios both in simulation and on robots.",
            "author": [
                "Hejia Zhang",
                "Shao-Hung Chan",
                "Jie Zhong",
                "Jiaoyang Li",
                "Peter Kolapo",
                "Sven Koenig",
                "Zach Agioutantis",
                "Steven Schafrik",
                "Stefanos Nikolaidis"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08802v1",
                "http://arxiv.org/pdf/2310.08802v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08800v2",
            "title": "DDMT: Denoising Diffusion Mask Transformer Models for Multivariate Time\n  Series Anomaly Detection",
            "updated": "2023-10-30T06:23:59Z",
            "published": "2023-10-13T01:18:41Z",
            "summary": "Anomaly detection in multivariate time series has emerged as a crucial\nchallenge in time series research, with significant research implications in\nvarious fields such as fraud detection, fault diagnosis, and system state\nestimation. Reconstruction-based models have shown promising potential in\nrecent years for detecting anomalies in time series data. However, due to the\nrapid increase in data scale and dimensionality, the issues of noise and Weak\nIdentity Mapping (WIM) during time series reconstruction have become\nincreasingly pronounced. To address this, we introduce a novel Adaptive Dynamic\nNeighbor Mask (ADNM) mechanism and integrate it with the Transformer and\nDenoising Diffusion Model, creating a new framework for multivariate time\nseries anomaly detection, named Denoising Diffusion Mask Transformer (DDMT).\nThe ADNM module is introduced to mitigate information leakage between input and\noutput features during data reconstruction, thereby alleviating the problem of\nWIM during reconstruction. The Denoising Diffusion Transformer (DDT) employs\nthe Transformer as an internal neural network structure for Denoising Diffusion\nModel. It learns the stepwise generation process of time series data to model\nthe probability distribution of the data, capturing normal data patterns and\nprogressively restoring time series data by removing noise, resulting in a\nclear recovery of anomalies. To the best of our knowledge, this is the first\nmodel that combines Denoising Diffusion Model and the Transformer for\nmultivariate time series anomaly detection. Experimental evaluations were\nconducted on five publicly available multivariate time series anomaly detection\ndatasets. The results demonstrate that the model effectively identifies\nanomalies in time series data, achieving state-of-the-art performance in\nanomaly detection.",
            "author": [
                "Chaocheng Yang",
                "Tingyin Wang",
                "Xuanhui Yan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08800v2",
                "http://arxiv.org/pdf/2310.08800v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08797v1",
            "title": "A Comparative Analysis of Task-Agnostic Distillation Methods for\n  Compressing Transformer Language Models",
            "updated": "2023-10-13T01:00:15Z",
            "published": "2023-10-13T01:00:15Z",
            "summary": "Large language models have become a vital component in modern NLP, achieving\nstate of the art performance in a variety of tasks. However, they are often\ninefficient for real-world deployment due to their expensive inference costs.\nKnowledge distillation is a promising technique to improve their efficiency\nwhile retaining most of their effectiveness. In this paper, we reproduce,\ncompare and analyze several representative methods for task-agnostic\n(general-purpose) distillation of Transformer language models. Our target of\nstudy includes Output Distribution (OD) transfer, Hidden State (HS) transfer\nwith various layer mapping strategies, and Multi-Head Attention (MHA) transfer\nbased on MiniLMv2. Through our extensive experiments, we study the\neffectiveness of each method for various student architectures in both\nmonolingual (English) and multilingual settings. Overall, we show that MHA\ntransfer based on MiniLMv2 is generally the best option for distillation and\nexplain the potential reasons behind its success. Moreover, we show that HS\ntransfer remains as a competitive baseline, especially under a sophisticated\nlayer mapping strategy, while OD transfer consistently lags behind other\napproaches. Findings from this study helped us deploy efficient yet effective\nstudent models for latency-critical applications.",
            "author": [
                "Takuma Udagawa",
                "Aashka Trivedi",
                "Michele Merler",
                "Bishwaranjan Bhattacharjee"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08797v1",
                "http://arxiv.org/pdf/2310.08797v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08788v1",
            "title": "Sensory Manipulation as a Countermeasure to Robot Teleoperation Delays:\n  System and Evidence",
            "updated": "2023-10-13T00:22:39Z",
            "published": "2023-10-13T00:22:39Z",
            "summary": "In the field of robotics, robot teleoperation for remote or hazardous\nenvironments has become increasingly vital. A major challenge is the lag\nbetween command and action, negatively affecting operator awareness,\nperformance, and mental strain. Even with advanced technology, mitigating these\ndelays, especially in long-distance operations, remains challenging. Current\nsolutions largely focus on machine-based adjustments. Yet, there's a gap in\nusing human perceptions to improve the teleoperation experience. This paper\npresents a unique method of sensory manipulation to help humans adapt to such\ndelays. Drawing from motor learning principles, it suggests that modifying\nsensory stimuli can lessen the perception of these delays. Instead of\nintroducing new skills, the approach uses existing motor coordination\nknowledge. The aim is to minimize the need for extensive training or complex\nautomation. A study with 41 participants explored the effects of altered haptic\ncues in delayed teleoperations. These cues were sourced from advanced physics\nengines and robot sensors. Results highlighted benefits like reduced task time\nand improved perceptions of visual delays. Real-time haptic feedback\nsignificantly contributed to reduced mental strain and increased confidence.\nThis research emphasizes human adaptation as a key element in robot\nteleoperation, advocating for improved teleoperation efficiency via swift human\nadaptation, rather than solely optimizing robots for delay adjustment.",
            "author": [
                "Jing Du",
                "William Vann",
                "Tianyu Zhou",
                "Yang Ye",
                "Qi Zhu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08788v1",
                "http://arxiv.org/pdf/2310.08788v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08782v3",
            "title": "Selectivity Drives Productivity: Efficient Dataset Pruning for Enhanced\n  Transfer Learning",
            "updated": "2023-11-18T16:08:35Z",
            "published": "2023-10-13T00:07:49Z",
            "summary": "Massive data is often considered essential for deep learning applications,\nbut it also incurs significant computational and infrastructural costs.\nTherefore, dataset pruning (DP) has emerged as an effective way to improve data\nefficiency by identifying and removing redundant training samples without\nsacrificing performance. In this work, we aim to address the problem of DP for\ntransfer learning, i.e., how to prune a source dataset for improved pretraining\nefficiency and lossless finetuning accuracy on downstream target tasks. To our\nbest knowledge, the problem of DP for transfer learning remains open, as\nprevious studies have primarily addressed DP and transfer learning as separate\nproblems. By contrast, we establish a unified viewpoint to integrate DP with\ntransfer learning and find that existing DP methods are not suitable for the\ntransfer learning paradigm. We then propose two new DP methods, label mapping\nand feature mapping, for supervised and self-supervised pretraining settings\nrespectively, by revisiting the DP problem through the lens of source-target\ndomain mapping. Furthermore, we demonstrate the effectiveness of our approach\non numerous transfer learning tasks. We show that source data classes can be\npruned by up to 40% ~ 80% without sacrificing downstream performance, resulting\nin a significant 2 ~ 5 times speed-up during the pretraining stage. Besides,\nour proposal exhibits broad applicability and can improve other computationally\nintensive transfer learning techniques, such as adversarial pretraining. Codes\nare available at https://github.com/OPTML-Group/DP4TL.",
            "author": [
                "Yihua Zhang",
                "Yimeng Zhang",
                "Aochuan Chen",
                "Jinghan Jia",
                "Jiancheng Liu",
                "Gaowen Liu",
                "Mingyi Hong",
                "Shiyu Chang",
                "Sijia Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08782v3",
                "http://arxiv.org/pdf/2310.08782v3"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08780v1",
            "title": "\"Im not Racist but...\": Discovering Bias in the Internal Knowledge of\n  Large Language Models",
            "updated": "2023-10-13T00:03:37Z",
            "published": "2023-10-13T00:03:37Z",
            "summary": "Large language models (LLMs) have garnered significant attention for their\nremarkable performance in a continuously expanding set of natural language\nprocessing tasks. However, these models have been shown to harbor inherent\nsocietal biases, or stereotypes, which can adversely affect their performance\nin their many downstream applications. In this paper, we introduce a novel,\npurely prompt-based approach to uncover hidden stereotypes within any arbitrary\nLLM. Our approach dynamically generates a knowledge representation of internal\nstereotypes, enabling the identification of biases encoded within the LLM's\ninternal knowledge. By illuminating the biases present in LLMs and offering a\nsystematic methodology for their analysis, our work contributes to advancing\ntransparency and promoting fairness in natural language processing systems.",
            "author": [
                "Abel Salinas",
                "Louis Penafiel",
                "Robert McCormack",
                "Fred Morstatter"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08780v1",
                "http://arxiv.org/pdf/2310.08780v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08769v1",
            "title": "Extragalactic electromagnetic cascades and cascade gamma-ray emission in\n  magnetic fields of various strength",
            "updated": "2023-10-12T23:27:19Z",
            "published": "2023-10-12T23:27:19Z",
            "summary": "We discuss the magnetic field influence on diffuse gamma-ray emission from\nextragalactic electromagnetic cascades initiated by ultra-high energy cosmic\nrays. Regions in space vary considerably in field strength: it is possibly of\n10^(-12) G and lower in voids, of ~10^(-6) G inside galaxies, galactic clusters\nand groups, of ~10^(-7) G around them, and of ~ 10^(-8)-10^(-9) G in filaments.\nStructures having fields higher than in voids occupy comparatively small\nfraction of the Universe, so they affect weakly on cascade emission. Still\nknowledge of this influence may be relevant studying large-scale component of\nthe extragalactic magnetic field and to the search for exotic particles, as in\nthe latter case contribution of all components to extragalactic gamma-ray\nbackground should be known, one of which is cascade emission. To study magnetic\nfield effect we simulate particle propagation in homogeneous magnetic field of\n~10^(-6), 10^(-9), and 10^(-12) G and lower. It is found that in fields of\n~10^(-9) G and lower the spectra of diffuse cascade gamma-rays at energies\nE<=10^17 eV coincide. Thus no specific models of EGMF are required to study\ncontribution of cascade emission in the extragalactic gamma-ray background at\nE<=10^17 eV. In the case of uniform field of 10^(-6) G (which seems to be\nunrealistic), this inference is valid in the energy range of ~10^7-10^9 eV.\nResults obtained can be also used studying large-scale component of the\nextragalactic magnetic field.",
            "author": [
                "A. Uryson"
            ],
            "link": [
                "http://dx.doi.org/10.1016/j.asr.2023.09.45",
                "http://arxiv.org/abs/2310.08769v1",
                "http://arxiv.org/pdf/2310.08769v1"
            ],
            "primary_category": "astro-ph.HE",
            "category": [
                "astro-ph.HE",
                "J.2"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08725v1",
            "title": "Heterophily-Based Graph Neural Network for Imbalanced Classification",
            "updated": "2023-10-12T21:19:47Z",
            "published": "2023-10-12T21:19:47Z",
            "summary": "Graph neural networks (GNNs) have shown promise in addressing graph-related\nproblems, including node classification. However, conventional GNNs assume an\neven distribution of data across classes, which is often not the case in\nreal-world scenarios, where certain classes are severely underrepresented. This\nleads to suboptimal performance of standard GNNs on imbalanced graphs. In this\npaper, we introduce a unique approach that tackles imbalanced classification on\ngraphs by considering graph heterophily. We investigate the intricate\nrelationship between class imbalance and graph heterophily, revealing that\nminority classes not only exhibit a scarcity of samples but also manifest lower\nlevels of homophily, facilitating the propagation of erroneous information\namong neighboring nodes. Drawing upon this insight, we propose an efficient\nmethod, called Fast Im-GBK, which integrates an imbalance classification\nstrategy with heterophily-aware GNNs to effectively address the class imbalance\nproblem while significantly reducing training time. Our experiments on\nreal-world graphs demonstrate our model's superiority in classification\nperformance and efficiency for node classification tasks compared to existing\nbaselines.",
            "author": [
                "Zirui Liang",
                "Yuntao Li",
                "Tianjin Huang",
                "Akrati Saxena",
                "Yulong Pei",
                "Mykola Pechenizkiy"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08725v1",
                "http://arxiv.org/pdf/2310.08725v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08718v1",
            "title": "A Framework for Developing and Evaluating Algorithms for Estimating\n  Multipath Propagation Parameters from Channel Sounder Measurements",
            "updated": "2023-10-12T20:59:25Z",
            "published": "2023-10-12T20:59:25Z",
            "summary": "A framework is proposed for developing and evaluating algorithms for\nextracting multipath propagation components (MPCs) from measurements collected\nby channel sounders at millimeter-wave frequencies. Sounders equipped with an\nomnidirectional transmitter and a receiver with a uniform planar array (UPA)\nare considered. An accurate mathematical model is developed for the spatial\nfrequency response of the sounder that incorporates the non-ideal cross-polar\nbeampatterns for the UPA elements. Due to the limited Field-of-View (FoV) of\neach element, the model is extended to accommodate multi-FoV measurements in\ndistinct azimuth directions. A beamspace representation of the spatial\nfrequency response is leveraged to develop three progressively complex\nalgorithms aimed at solving the singlesnapshot maximum likelihood estimation\nproblem: greedy matching pursuit (CLEAN), space-alternative generalized\nexpectationmaximization (SAGE), and RiMAX. The first two are based on purely\nspecular MPCs whereas RiMAX also accommodates diffuse MPCs. Two approaches for\nperformance evaluation are proposed, one with knowledge of ground truth\nparameters, and one based on reconstruction mean-squared error. The three\nalgorithms are compared through a demanding channel model with hundreds of MPCs\nand through real measurements. The results demonstrate that CLEAN gives quite\nreasonable estimates which are improved by SAGE and RiMAX. Lessons learned and\ndirections for future research are discussed.",
            "author": [
                "Akbar Sayeed",
                "Damla Guven",
                "Michael Doebereiner",
                "Sebastian Semper",
                "Camillo Gentile",
                "Anuraag Bodi",
                "Zihang Cheng"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08718v1",
                "http://arxiv.org/pdf/2310.08718v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08710v1",
            "title": "Waymax: An Accelerated, Data-Driven Simulator for Large-Scale Autonomous\n  Driving Research",
            "updated": "2023-10-12T20:49:15Z",
            "published": "2023-10-12T20:49:15Z",
            "summary": "Simulation is an essential tool to develop and benchmark autonomous vehicle\nplanning software in a safe and cost-effective manner. However, realistic\nsimulation requires accurate modeling of nuanced and complex multi-agent\ninteractive behaviors. To address these challenges, we introduce Waymax, a new\ndata-driven simulator for autonomous driving in multi-agent scenes, designed\nfor large-scale simulation and testing. Waymax uses publicly-released,\nreal-world driving data (e.g., the Waymo Open Motion Dataset) to initialize or\nplay back a diverse set of multi-agent simulated scenarios. It runs entirely on\nhardware accelerators such as TPUs/GPUs and supports in-graph simulation for\ntraining, making it suitable for modern large-scale, distributed machine\nlearning workflows. To support online training and evaluation, Waymax includes\nseveral learned and hard-coded behavior models that allow for realistic\ninteraction within simulation. To supplement Waymax, we benchmark a suite of\npopular imitation and reinforcement learning algorithms with ablation studies\non different design decisions, where we highlight the effectiveness of routes\nas guidance for planning agents and the ability of RL to overfit against\nsimulated agents.",
            "author": [
                "Cole Gulino",
                "Justin Fu",
                "Wenjie Luo",
                "George Tucker",
                "Eli Bronstein",
                "Yiren Lu",
                "Jean Harb",
                "Xinlei Pan",
                "Yan Wang",
                "Xiangyu Chen",
                "John D. Co-Reyes",
                "Rishabh Agarwal",
                "Rebecca Roelofs",
                "Yao Lu",
                "Nico Montali",
                "Paul Mougin",
                "Zoey Yang",
                "Brandyn White",
                "Aleksandra Faust",
                "Rowan McAllister",
                "Dragomir Anguelov",
                "Benjamin Sapp"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08710v1",
                "http://arxiv.org/pdf/2310.08710v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08705v1",
            "title": "A Benchmarking Protocol for SAR Colorization: From Regression to Deep\n  Learning Approaches",
            "updated": "2023-10-12T20:31:20Z",
            "published": "2023-10-12T20:31:20Z",
            "summary": "Synthetic aperture radar (SAR) images are widely used in remote sensing.\nInterpreting SAR images can be challenging due to their intrinsic speckle noise\nand grayscale nature. To address this issue, SAR colorization has emerged as a\nresearch direction to colorize gray scale SAR images while preserving the\noriginal spatial information and radiometric information. However, this\nresearch field is still in its early stages, and many limitations can be\nhighlighted. In this paper, we propose a full research line for supervised\nlearning-based approaches to SAR colorization. Our approach includes a protocol\nfor generating synthetic color SAR images, several baselines, and an effective\nmethod based on the conditional generative adversarial network (cGAN) for SAR\ncolorization. We also propose numerical assessment metrics for the problem at\nhand. To our knowledge, this is the first attempt to propose a research line\nfor SAR colorization that includes a protocol, a benchmark, and a complete\nperformance evaluation. Our extensive tests demonstrate the effectiveness of\nour proposed cGAN-based network for SAR colorization. The code will be made\npublicly available.",
            "author": [
                "Kangqing Shen",
                "Gemine Vivone",
                "Xiaoyuan Yang",
                "Simone Lolli",
                "Michael Schmitt"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08705v1",
                "http://arxiv.org/pdf/2310.08705v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08698v1",
            "title": "Magnetic microcalorimeter with paramagnetic temperature sensors and\n  integrated dc-SQUID readout for high-resolution X-ray emission spectroscopy",
            "updated": "2023-10-12T20:07:00Z",
            "published": "2023-10-12T20:07:00Z",
            "summary": "We present two variants of a magnetic microcalorimeter with paramagnetic\ntemperature sensors and integrated dc-SQUID readout for high-resolution X-ray\nemission spectroscopy. Each variant employs two overhanging gold absorbers with\na sensitive area of 150$\\mu$m x 150$\\mu$m and a thickness of 3$\\mu$m, thus\nproviding a quantum efficiency of 98% for photons up to 5keV and 50% for\nphotons up to 10keV. The first variant turned out to be fully operational, but,\nat the same time, to suffer from Joule power dissipation of the Josephson\njunction shunt resistors, athermal phonon loss, and slew rate limitations of\nthe overall setup. Overall, it only achieved an energy resolution $\\Delta\nE_\\mathrm{FWHM} = 8.9eV$. In the second variant, we introduced an innovative\n`tetrapod absorber geometry' as well as a membrane-technique for protecting the\ntemperature sensors against the power dissipation of the shunt resistors. By\nthis, the second variant achieves an outstanding energy resolution of $\\Delta\nE_\\mathrm{FWHM} =1.25(18)eV$ and hence provides, to our knowledge, the present\nbest energy resolving power $E/\\Delta E_\\mathrm{FWHM}$ among all existing\nenergy-dispersive detectors for soft and tender X-rays.",
            "author": [
                "Matth\u00e4us Krantz",
                "Francesco Toschi",
                "Benedikt Maier",
                "Greta Heine",
                "Christian Enss",
                "Sebastian Kempf"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08698v1",
                "http://arxiv.org/pdf/2310.08698v1"
            ],
            "primary_category": "physics.ins-det",
            "category": [
                "physics.ins-det",
                "cond-mat.supr-con"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08696v1",
            "title": "End-to-end Online Speaker Diarization with Target Speaker Tracking",
            "updated": "2023-10-12T20:02:07Z",
            "published": "2023-10-12T20:02:07Z",
            "summary": "This paper proposes an online target speaker voice activity detection system\nfor speaker diarization tasks, which does not require a priori knowledge from\nthe clustering-based diarization system to obtain the target speaker\nembeddings. By adapting the conventional target speaker voice activity\ndetection for real-time operation, this framework can identify speaker\nactivities using self-generated embeddings, resulting in consistent performance\nwithout permutation inconsistencies in the inference phase. During the\ninference process, we employ a front-end model to extract the frame-level\nspeaker embeddings for each coming block of a signal. Next, we predict the\ndetection state of each speaker based on these frame-level speaker embeddings\nand the previously estimated target speaker embedding. Then, the target speaker\nembeddings are updated by aggregating these frame-level speaker embeddings\naccording to the predictions in the current block. Our model predicts the\nresults for each block and updates the target speakers' embeddings until\nreaching the end of the signal. Experimental results show that the proposed\nmethod outperforms the offline clustering-based diarization system on the\nDIHARD III and AliMeeting datasets. The proposed method is further extended to\nmulti-channel data, which achieves similar performance with the\nstate-of-the-art offline diarization systems.",
            "author": [
                "Weiqing Wang",
                "Ming Li"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08696v1",
                "http://arxiv.org/pdf/2310.08696v1"
            ],
            "primary_category": "eess.AS",
            "category": [
                "eess.AS",
                "cs.SD"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08690v1",
            "title": "Quantifying State Transfer Strength on Graphs with Involution",
            "updated": "2023-10-12T19:52:13Z",
            "published": "2023-10-12T19:52:13Z",
            "summary": "This paper discusses continuous-time quantum walks and asymptotic state\ntransfer in graphs with an involution. By providing quantitative bounds on the\neigenvectors of the Hamiltonian, it provides an approach to achieving\nhigh-fidelity state transfer by strategically selecting energy potentials based\non the maximum degrees of the graphs. The study also involves an analysis of\nthe time necessary for quantum transfer to occur.",
            "author": [
                "Gabor Lippner",
                "Yujia Shi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08690v1",
                "http://arxiv.org/pdf/2310.08690v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "math.CO",
                "05C50, 81P45"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08677v1",
            "title": "GDL-DS: A Benchmark for Geometric Deep Learning under Distribution\n  Shifts",
            "updated": "2023-10-12T19:27:43Z",
            "published": "2023-10-12T19:27:43Z",
            "summary": "Geometric deep learning (GDL) has gained significant attention in various\nscientific fields, chiefly for its proficiency in modeling data with intricate\ngeometric structures. Yet, very few works have delved into its capability of\ntackling the distribution shift problem, a prevalent challenge in many relevant\napplications. To bridge this gap, we propose GDL-DS, a comprehensive benchmark\ndesigned for evaluating the performance of GDL models in scenarios with\ndistribution shifts. Our evaluation datasets cover diverse scientific domains\nfrom particle physics and materials science to biochemistry, and encapsulate a\nbroad spectrum of distribution shifts including conditional, covariate, and\nconcept shifts. Furthermore, we study three levels of information access from\nthe out-of-distribution (OOD) testing data, including no OOD information, only\nOOD features without labels, and OOD features with a few labels. Overall, our\nbenchmark results in 30 different experiment settings, and evaluates 3 GDL\nbackbones and 11 learning algorithms in each setting. A thorough analysis of\nthe evaluation results is provided, poised to illuminate insights for DGL\nresearchers and domain practitioners who are to use DGL in their applications.",
            "author": [
                "Deyu Zou",
                "Shikun Liu",
                "Siqi Miao",
                "Victor Fung",
                "Shiyu Chang",
                "Pan Li"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08677v1",
                "http://arxiv.org/pdf/2310.08677v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08663v1",
            "title": "One n Remains to Settle the Tree Conjecture",
            "updated": "2023-10-12T18:44:17Z",
            "published": "2023-10-12T18:44:17Z",
            "summary": "In the famous network creation game of Fabrikant et al. a set of agents play\na game to build a connected graph. The $n$ agents form the vertex set $V$ of\nthe graph and each vertex $v\\in V$ buys a set $E_v$ of edges inducing a graph\n$G=(V,\\bigcup\\limits_{v\\in V} E_v)$. The private objective of each vertex is to\nminimize the sum of its building cost (the cost of the edges it buys) plus its\nconnection cost (the total distance from itself to every other vertex). Given a\ncost of $\\alpha$ for each individual edge, a long-standing conjecture, called\nthe tree conjecture, states that if $\\alpha > n$ then every Nash equilibrium\ngraph in the game is a spanning tree. After a plethora of work, it is known\nthat the conjecture holds for any $\\alpha>3n-3$. In this paper we prove the\ntree conjecture holds for $\\alpha>2n$. This reduces by half the open range for\n$\\alpha$ with only $[n, 2n)$ remaining in order to settle the conjecture.",
            "author": [
                "Jack Dippel",
                "Adrian Vetta"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08663v1",
                "http://arxiv.org/pdf/2310.08663v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "cs.GT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08644v2",
            "title": "A Mass-Conserving-Perceptron for Machine Learning-Based Modeling of\n  Geoscientific Systems",
            "updated": "2023-10-24T02:12:22Z",
            "published": "2023-10-12T18:09:33Z",
            "summary": "Although decades of effort have been devoted to building Physical-Conceptual\n(PC) models for predicting the time-series evolution of geoscientific systems,\nrecent work shows that Machine Learning (ML) based Gated Recurrent Neural\nNetwork technology can be used to develop models that are much more accurate.\nHowever, the difficulty of extracting physical understanding from ML-based\nmodels complicates their utility for enhancing scientific knowledge regarding\nsystem structure and function. Here, we propose a physically-interpretable Mass\nConserving Perceptron (MCP) as a way to bridge the gap between PC-based and\nML-based modeling approaches. The MCP exploits the inherent isomorphism between\nthe directed graph structures underlying both PC models and GRNNs to explicitly\nrepresent the mass-conserving nature of physical processes while enabling the\nfunctional nature of such processes to be directly learned (in an interpretable\nmanner) from available data using off-the-shelf ML technology. As a proof of\nconcept, we investigate the functional expressivity (capacity) of the MCP,\nexplore its ability to parsimoniously represent the rainfall-runoff (RR)\ndynamics of the Leaf River Basin, and demonstrate its utility for scientific\nhypothesis testing. To conclude, we discuss extensions of the concept to enable\nML-based physical-conceptual representation of the coupled nature of\nmass-energy-information flows through geoscientific systems.",
            "author": [
                "Yuan-Heng Wang",
                "Hoshin V. Gupta"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08644v2",
                "http://arxiv.org/pdf/2310.08644v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08587v1",
            "title": "Is Generalized Dynamic Novel View Synthesis from Monocular Videos\n  Possible Today?",
            "updated": "2023-10-12T17:59:58Z",
            "published": "2023-10-12T17:59:58Z",
            "summary": "Rendering scenes observed in a monocular video from novel viewpoints is a\nchallenging problem. For static scenes the community has studied both\nscene-specific optimization techniques, which optimize on every test scene, and\ngeneralized techniques, which only run a deep net forward pass on a test scene.\nIn contrast, for dynamic scenes, scene-specific optimization techniques exist,\nbut, to our best knowledge, there is currently no generalized method for\ndynamic novel view synthesis from a given monocular video. To answer whether\ngeneralized dynamic novel view synthesis from monocular videos is possible\ntoday, we establish an analysis framework based on existing techniques and work\ntoward the generalized approach. We find a pseudo-generalized process without\nscene-specific appearance optimization is possible, but geometrically and\ntemporally consistent depth estimates are needed. Despite no scene-specific\nappearance optimization, the pseudo-generalized approach improves upon some\nscene-specific methods.",
            "author": [
                "Xiaoming Zhao",
                "Alex Colburn",
                "Fangchang Ma",
                "Miguel Angel Bautista",
                "Joshua M. Susskind",
                "Alexander G. Schwing"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08587v1",
                "http://arxiv.org/pdf/2310.08587v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08581v1",
            "title": "Universal Visual Decomposer: Long-Horizon Manipulation Made Easy",
            "updated": "2023-10-12T17:59:41Z",
            "published": "2023-10-12T17:59:41Z",
            "summary": "Real-world robotic tasks stretch over extended horizons and encompass\nmultiple stages. Learning long-horizon manipulation tasks, however, is a\nlong-standing challenge, and demands decomposing the overarching task into\nseveral manageable subtasks to facilitate policy learning and generalization to\nunseen tasks. Prior task decomposition methods require task-specific knowledge,\nare computationally intensive, and cannot readily be applied to new tasks. To\naddress these shortcomings, we propose Universal Visual Decomposer (UVD), an\noff-the-shelf task decomposition method for visual long horizon manipulation\nusing pre-trained visual representations designed for robotic control. At a\nhigh level, UVD discovers subgoals by detecting phase shifts in the embedding\nspace of the pre-trained representation. Operating purely on visual\ndemonstrations without auxiliary information, UVD can effectively extract\nvisual subgoals embedded in the videos, while incurring zero additional\ntraining cost on top of standard visuomotor policy training. Goal-conditioned\npolicies learned with UVD-discovered subgoals exhibit significantly improved\ncompositional generalization at test time to unseen tasks. Furthermore,\nUVD-discovered subgoals can be used to construct goal-based reward shaping that\njump-starts temporally extended exploration for reinforcement learning. We\nextensively evaluate UVD on both simulation and real-world tasks, and in all\ncases, UVD substantially outperforms baselines across imitation and\nreinforcement learning settings on in-domain and out-of-domain task sequences\nalike, validating the clear advantage of automated visual task decomposition\nwithin the simple, compact UVD framework.",
            "author": [
                "Zichen Zhang",
                "Yunshuang Li",
                "Osbert Bastani",
                "Abhishek Gupta",
                "Dinesh Jayaraman",
                "Yecheng Jason Ma",
                "Luca Weihs"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08581v1",
                "http://arxiv.org/pdf/2310.08581v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08563v1",
            "title": "Tverberg Partition Graphs",
            "updated": "2023-10-12T17:53:39Z",
            "published": "2023-10-12T17:53:39Z",
            "summary": "Given a finite set of points in $\\mathbb{R}^d$, Tverberg's theorem guarantees\nthe existence of partitions of this set into parts whose convex hulls\nintersect. We introduce a graph structured on the family of Tverberg partitions\nof a given set of points, whose edges describe closeness between different\nTverberg partitions. We prove bounds on the minimum and maximum degree of this\ngraph, the number of vertices of maximal degree, its clique number, and its\nconnectedness.",
            "author": [
                "Deborah Oliveros",
                "\u00c9rika Rold\u00e1n",
                "Pablo Sober\u00f3n",
                "Antonio J. Torres"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08563v1",
                "http://arxiv.org/pdf/2310.08563v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08552v1",
            "title": "Threshold graphs, Kemeny's constant, and related random walk parameters",
            "updated": "2023-10-12T17:47:22Z",
            "published": "2023-10-12T17:47:22Z",
            "summary": "Kemeny's constant measures how fast a random walker moves around in a graph.\nExpressions for Kemeny's constant can be quite involved, and for this reason,\nmany lines of research focus on graphs with structure that makes them amenable\nto more in-depth study (for example, regular graphs, acyclic graphs, and\n1-connected graphs). In this article, we study Kemeny's constant for random\nwalks on threshold graphs, which are an interesting family of graphs with\nproperties that make examining Kemeny's constant difficult; that is, they are\nusually not regular, not acyclic, and not 1-connected. This article is a\nshowcase of various techniques for calculating Kemeny's constant and related\nrandom walk parameters for graphs. We establish explicit formulae for\n$\\mathcal{K}(G)$ in terms of the construction code of a threshold graph, and\ncompletely determine the ordering of the accessibility indices of vertices in\nthreshold graphs.",
            "author": [
                "Jane Breen",
                "Sooyeong Kim",
                "Alexander Low Fung",
                "Amy Mann",
                "Andrei A. Parfeni",
                "Giovanni Tedesco"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08552v1",
                "http://arxiv.org/pdf/2310.08552v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "60J10, 05C81, 05C50, 05A19"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08551v1",
            "title": "How secure is the time-modulated array-enabled ofdm directional\n  modulation?",
            "updated": "2023-10-12T17:47:17Z",
            "published": "2023-10-12T17:47:17Z",
            "summary": "Time-modulated arrays (TMA) transmitting orthogonal frequency division\nmultiplexing (OFDM) waveforms achieve physical layer security by allowing the\nsignal to reach the legitimate destination undistorted, while making the signal\nappear scrambled in all other directions. In this paper, we examine how secure\nthe TMA OFDM system is, and show that it is possible for the eavesdropper to\ndefy the scrambling. In particular, we show that, based on the scrambled\nsignal, the eavesdropper can formulate a blind source separation problem and\nrecover data symbols and TMA parameters via independent component analysis\n(ICA) techniques. We show how the scaling and permutation ambiguities arising\nin ICA can be resolved by exploiting the Toeplitz structure of the\ncorresponding mixing matrix, and knowledge of data constellation, OFDM\nspecifics, and the rules for choosing TMA parameters. We also introduce a novel\nTMA implementation to defend the scrambling against the eavesdropper.",
            "author": [
                "Zhihao Tao",
                "Zhaoyi Xu",
                "Athina Petropulu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08551v1",
                "http://arxiv.org/pdf/2310.08551v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08538v1",
            "title": "Image2PCI -- A Multitask Learning Framework for Estimating Pavement\n  Condition Indices Directly from Images",
            "updated": "2023-10-12T17:28:06Z",
            "published": "2023-10-12T17:28:06Z",
            "summary": "The Pavement Condition Index (PCI) is a widely used metric for evaluating\npavement performance based on the type, extent and severity of distresses\ndetected on a pavement surface. In recent times, significant progress has been\nmade in utilizing deep-learning approaches to automate PCI estimation process.\nHowever, the current approaches rely on at least two separate models to\nestimate PCI values -- one model dedicated to determining the type and extent\nand another for estimating their severity. This approach presents several\nchallenges, including complexities, high computational resource demands, and\nmaintenance burdens that necessitate careful consideration and resolution. To\novercome these challenges, the current study develops a unified multi-tasking\nmodel that predicts the PCI directly from a top-down pavement image. The\nproposed architecture is a multi-task model composed of one encoder for feature\nextraction and four decoders to handle specific tasks: two detection heads, one\nsegmentation head and one PCI estimation head. By multitasking, we are able to\nextract features from the detection and segmentation heads for automatically\nestimating the PCI directly from the images. The model performs very well on\nour benchmarked and open pavement distress dataset that is annotated for\nmultitask learning (the first of its kind). To our best knowledge, this is the\nfirst work that can estimate PCI directly from an image at real time speeds\nwhile maintaining excellent accuracy on all related tasks for crack detection\nand segmentation.",
            "author": [
                "Neema Jakisa Owor",
                "Hang Du",
                "Abdulateef Daud",
                "Armstrong Aboah",
                "Yaw Adu-Gyamfi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08538v1",
                "http://arxiv.org/pdf/2310.08538v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08523v1",
            "title": "LLM-augmented Preference Learning from Natural Language",
            "updated": "2023-10-12T17:17:27Z",
            "published": "2023-10-12T17:17:27Z",
            "summary": "Finding preferences expressed in natural language is an important but\nchallenging task. State-of-the-art(SotA) methods leverage transformer-based\nmodels such as BERT, RoBERTa, etc. and graph neural architectures such as graph\nattention networks. Since Large Language Models (LLMs) are equipped to deal\nwith larger context lengths and have much larger model sizes than the\ntransformer-based model, we investigate their ability to classify comparative\ntext directly. This work aims to serve as a first step towards using LLMs for\nthe CPC task. We design and conduct a set of experiments that format the\nclassification task into an input prompt for the LLM and a methodology to get a\nfixed-format response that can be automatically evaluated. Comparing\nperformances with existing methods, we see that pre-trained LLMs are able to\noutperform the previous SotA models with no fine-tuning involved. Our results\nshow that the LLMs can consistently outperform the SotA when the target text is\nlarge -- i.e. composed of multiple sentences --, and are still comparable to\nthe SotA performance in shorter text. We also find that few-shot learning\nyields better performance than zero-shot learning.",
            "author": [
                "Inwon Kang",
                "Sikai Ruan",
                "Tyler Ho",
                "Jui-Chien Lin",
                "Farhad Mohsin",
                "Oshani Seneviratne",
                "Lirong Xia"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08523v1",
                "http://arxiv.org/pdf/2310.08523v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08496v1",
            "title": "The Uncertainty-based Retrieval Framework for Ancient Chinese CWS and\n  POS",
            "updated": "2023-10-12T16:55:44Z",
            "published": "2023-10-12T16:55:44Z",
            "summary": "Automatic analysis for modern Chinese has greatly improved the accuracy of\ntext mining in related fields, but the study of ancient Chinese is still\nrelatively rare. Ancient text division and lexical annotation are important\nparts of classical literature comprehension, and previous studies have tried to\nconstruct auxiliary dictionary and other fused knowledge to improve the\nperformance. In this paper, we propose a framework for ancient Chinese Word\nSegmentation and Part-of-Speech Tagging that makes a twofold effort: on the one\nhand, we try to capture the wordhood semantics; on the other hand, we\nre-predict the uncertain samples of baseline model by introducing external\nknowledge. The performance of our architecture outperforms pre-trained BERT\nwith CRF and existing tools such as Jiayan.",
            "author": [
                "Pengyu Wang",
                "Zhichen Ren"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08496v1",
                "http://arxiv.org/pdf/2310.08496v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08494v1",
            "title": "An Experience-based TAMP Framework for Foliated Manifolds",
            "updated": "2023-10-12T16:52:00Z",
            "published": "2023-10-12T16:52:00Z",
            "summary": "Due to their complexity, foliated structure problems often pose intricate\nchallenges to task and motion planning in robotics manipulation. To counter\nthis, our study presents the ``Foliated Repetition Roadmap.'' This roadmap\nassists task and motion planners by transforming the complex foliated structure\nproblem into a more accessible graph format. By leveraging query experiences\nfrom different foliated manifolds, our framework can dynamically and\nefficiently update this graph. The refined graph can generate distribution\nsets, optimizing motion planning performance in foliated structure problems. In\nour paper, we lay down the theoretical groundwork and illustrate its practical\napplications through real-world examples.",
            "author": [
                "Jiaming Hu",
                "Shrutheesh R. Iyer",
                "Henrik I. Christensen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08494v1",
                "http://arxiv.org/pdf/2310.08494v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08488v1",
            "title": "Community Consensus: Converging Locally despite Adversaries and\n  Heterogeneous Connectivity",
            "updated": "2023-10-12T16:47:15Z",
            "published": "2023-10-12T16:47:15Z",
            "summary": "We introduce the concept of community consensus in the presence of malicious\nagents using a well-known median-based consensus algorithm. We consider\nnetworks that have multiple well-connected regions that we term communities,\ncharacterized by specific robustness and minimum degree properties. Prior work\nderives conditions on properties that are necessary and sufficient for\nachieving global consensus in a network. This however, requires the minimum\ndegree of the network graph to be proportional to the number of malicious\nagents in the network, which is not very practical in large networks. In this\nwork we present a natural generalization of this previous result. We\ncharacterize cases when although global consensus is not reached, some subsets\nof agents $V_i$ will still converge to the same values $\\mathcal{M}_i$ among\nthemselves. We define more relaxed requirements for this new type of consensus\nto be reached in terms of the number $k$ of edges connecting an agent in a\ncommunity to agents external to the community, and the number of malicious\nagents in each community.",
            "author": [
                "Cristina Gava",
                "Aron Vekassy",
                "Matthew Cavorsi",
                "Stephanie Gil",
                "Frederik Mallmann-Trenn"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08488v1",
                "http://arxiv.org/pdf/2310.08488v1"
            ],
            "primary_category": "cs.MA",
            "category": [
                "cs.MA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08487v1",
            "title": "GraphextQA: A Benchmark for Evaluating Graph-Enhanced Large Language\n  Models",
            "updated": "2023-10-12T16:46:58Z",
            "published": "2023-10-12T16:46:58Z",
            "summary": "While multi-modal models have successfully integrated information from image,\nvideo, and audio modalities, integrating graph modality into large language\nmodels (LLMs) remains unexplored. This discrepancy largely stems from the\ninherent divergence between structured graph data and unstructured text data.\nIncorporating graph knowledge provides a reliable source of information,\nenabling potential solutions to address issues in text generation, e.g.,\nhallucination, and lack of domain knowledge. To evaluate the integration of\ngraph knowledge into language models, a dedicated dataset is needed. However,\nthere is currently no benchmark dataset specifically designed for multimodal\ngraph-language models. To address this gap, we propose GraphextQA, a question\nanswering dataset with paired subgraphs, retrieved from Wikidata, to facilitate\nthe evaluation and future development of graph-language models. Additionally,\nwe introduce a baseline model called CrossGNN, which conditions answer\ngeneration on the paired graphs by cross-attending question-aware graph\nfeatures at decoding. The proposed dataset is designed to evaluate\ngraph-language models' ability to understand graphs and make use of it for\nanswer generation. We perform experiments with language-only models and the\nproposed graph-language model to validate the usefulness of the paired graphs\nand to demonstrate the difficulty of the task.",
            "author": [
                "Yuanchun Shen",
                "Ruotong Liao",
                "Zhen Han",
                "Yunpu Ma",
                "Volker Tresp"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08487v1",
                "http://arxiv.org/pdf/2310.08487v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08486v1",
            "title": "Sparse critical graphs for defective $(1,3)$-coloring",
            "updated": "2023-10-12T16:45:27Z",
            "published": "2023-10-12T16:45:27Z",
            "summary": "A graph $G$ is $(1,3)$-colorable if its vertices can be partitioned into\nsubsets $V_1$ and $V_2$ so that every vertex in $G[V_1]$ has degree at most $1$\nand every vertex in $G[V_2]$ has degree at most $3$. We prove that every graph\nwith maximum average degree at most 28/9 is $(1, 3)$-colorable.",
            "author": [
                "Alexandr Kostochka",
                "Jingwei Xu",
                "Xuding Zhu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08486v1",
                "http://arxiv.org/pdf/2310.08486v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C15, 05C35"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08461v1",
            "title": "DistillSpec: Improving Speculative Decoding via Knowledge Distillation",
            "updated": "2023-10-12T16:21:04Z",
            "published": "2023-10-12T16:21:04Z",
            "summary": "Speculative decoding (SD) accelerates large language model inference by\nemploying a faster draft model for generating multiple tokens, which are then\nverified in parallel by the larger target model, resulting in the text\ngenerated according to the target model distribution. However, identifying a\ncompact draft model that is well-aligned with the target model is challenging.\nTo tackle this issue, we propose DistillSpec that uses knowledge distillation\nto better align the draft model with the target model, before applying SD.\nDistillSpec makes two key design choices, which we demonstrate via systematic\nstudy to be crucial to improving the draft and target alignment: utilizing\non-policy data generation from the draft model, and tailoring the divergence\nfunction to the task and decoding strategy. Notably, DistillSpec yields\nimpressive 10 - 45% speedups over standard SD on a range of standard\nbenchmarks, using both greedy and non-greedy sampling. Furthermore, we combine\nDistillSpec with lossy SD to achieve fine-grained control over the latency vs.\ntask performance trade-off. Finally, in practical scenarios with models of\nvarying sizes, first using distillation to boost the performance of the target\nmodel and then applying DistillSpec to train a well-aligned draft model can\nreduce decoding latency by 6-10x with minimal performance drop, compared to\nstandard decoding without distillation.",
            "author": [
                "Yongchao Zhou",
                "Kaifeng Lyu",
                "Ankit Singh Rawat",
                "Aditya Krishna Menon",
                "Afshin Rostamizadeh",
                "Sanjiv Kumar",
                "Jean-Fran\u00e7ois Kagy",
                "Rishabh Agarwal"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08461v1",
                "http://arxiv.org/pdf/2310.08461v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08459v2",
            "title": "A Survey of Heterogeneous Transfer Learning",
            "updated": "2023-10-15T19:18:42Z",
            "published": "2023-10-12T16:19:58Z",
            "summary": "The application of transfer learning, an approach utilizing knowledge from a\nsource domain to enhance model performance in a target domain, has seen a\ntremendous rise in recent years, underpinning many real-world scenarios. The\nkey to its success lies in the shared common knowledge between the domains, a\nprerequisite in most transfer learning methodologies. These methods typically\npresuppose identical feature spaces and label spaces in both domains, known as\nhomogeneous transfer learning, which, however, is not always a practical\nassumption. Oftentimes, the source and target domains vary in feature spaces,\ndata distributions, and label spaces, making it challenging or costly to secure\nsource domain data with identical feature and label spaces as the target\ndomain. Arbitrary elimination of these differences is not always feasible or\noptimal. Thus, heterogeneous transfer learning, acknowledging and dealing with\nsuch disparities, has emerged as a promising approach for a variety of tasks.\nDespite the existence of a survey in 2017 on this topic, the fast-paced\nadvances post-2017 necessitate an updated, in-depth review. We therefore\npresent a comprehensive survey of recent developments in heterogeneous transfer\nlearning methods, offering a systematic guide for future research. Our paper\nreviews methodologies for diverse learning scenarios, discusses the limitations\nof current studies, and covers various application contexts, including Natural\nLanguage Processing, Computer Vision, Multimodality, and Biomedicine, to foster\na deeper understanding and spur future research.",
            "author": [
                "Runxue Bao",
                "Yiming Sun",
                "Yuhe Gao",
                "Jindong Wang",
                "Qiang Yang",
                "Haifeng Chen",
                "Zhi-Hong Mao",
                "Ye Ye"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08459v2",
                "http://arxiv.org/pdf/2310.08459v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08405v2",
            "title": "Emergence of noise-induced barren plateaus in arbitrary layered noise\n  models",
            "updated": "2023-11-29T09:40:35Z",
            "published": "2023-10-12T15:18:27Z",
            "summary": "In variational quantum algorithms the parameters of a parameterized quantum\ncircuit are optimized in order to minimize a cost function that encodes the\nsolution of the problem. The barren plateau phenomenon manifests as an\nexponentially vanishing dependence of the cost function with respect to the\nvariational parameters, and thus hampers the optimization process. We discuss\nhow, and in which sense, the phenomenon of noise-induced barren plateaus\nemerges in parameterized quantum circuits with a layered noise model. Previous\nresults have shown the existence of noise-induced barren plateaus in the\npresence of local Pauli noise [arXiv:2007.14384]. We extend these results\nanalytically to arbitrary completely-positive trace preserving maps in two\ncases: 1) when a parameter-shift rule holds, 2) when the parameterized quantum\ncircuit at each layer forms a unitary $2$-design. The second example shows how\nhighly expressive unitaries give rise not only to standard barren plateaus\n[arXiv:1803.11173], but also to noise-induced ones. In the second part of the\npaper, we study numerically the emergence of noise-induced barren plateaus in\nQAOA circuits focusing on the case of MaxCut problems on $d$-regular graphs and\namplitude damping noise.",
            "author": [
                "Marco Schumann",
                "Frank K. Wilhelm",
                "Alessandro Ciani"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08405v2",
                "http://arxiv.org/pdf/2310.08405v2"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08396v1",
            "title": "Uncertainty-Aware Planning for Heterogeneous Robot Teams using Dynamic\n  Topological Graphs and Mixed-Integer Programming",
            "updated": "2023-10-12T15:08:36Z",
            "published": "2023-10-12T15:08:36Z",
            "summary": "Planning under uncertainty is a fundamental challenge in robotics. For\nmulti-robot teams, the challenge is further exacerbated, since the planning\nproblem can quickly become computationally intractable as the number of robots\nincrease. In this paper, we propose a novel approach for planning under\nuncertainty using heterogeneous multi-robot teams. In particular, we leverage\nthe notion of a dynamic topological graph and mixed-integer programming to\ngenerate multi-robot plans that deploy fast scout team members to reduce\nuncertainty about the environment. We test our approach in a number of\nrepresentative scenarios where the robot team must move through an environment\nwhile minimizing detection in the presence of uncertain observer positions. We\ndemonstrate that our approach is sufficiently computationally tractable for\nreal-time re-planning in changing environments, can improve performance in the\npresence of imperfect information, and can be adjusted to accommodate different\nrisk profiles.",
            "author": [
                "Cora A. Dimmig",
                "Kevin C. Wolfe",
                "Marin Kobilarov",
                "Joseph Moore"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08396v1",
                "http://arxiv.org/pdf/2310.08396v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08395v3",
            "title": "Prompting Large Language Models with Chain-of-Thought for Few-Shot\n  Knowledge Base Question Generation",
            "updated": "2023-10-23T07:34:28Z",
            "published": "2023-10-12T15:08:14Z",
            "summary": "The task of Question Generation over Knowledge Bases (KBQG) aims to convert a\nlogical form into a natural language question. For the sake of expensive cost\nof large-scale question annotation, the methods of KBQG under low-resource\nscenarios urgently need to be developed. However, current methods heavily rely\non annotated data for fine-tuning, which is not well-suited for few-shot\nquestion generation. The emergence of Large Language Models (LLMs) has shown\ntheir impressive generalization ability in few-shot tasks. Inspired by\nChain-of-Thought (CoT) prompting, which is an in-context learning strategy for\nreasoning, we formulate KBQG task as a reasoning problem, where the generation\nof a complete question is splitted into a series of sub-question generation.\nOur proposed prompting method KQG-CoT first retrieves supportive logical forms\nfrom the unlabeled data pool taking account of the characteristics of the\nlogical form. Then, we write a prompt to explicit the reasoning chain of\ngenerating complicated questions based on the selected demonstrations. To\nfurther ensure prompt quality, we extend KQG-CoT into KQG-CoT+ via sorting the\nlogical forms by their complexity. We conduct extensive experiments over three\npublic KBQG datasets. The results demonstrate that our prompting method\nconsistently outperforms other prompting baselines on the evaluated datasets.\nRemarkably, our KQG-CoT+ method could surpass existing few-shot SoTA results of\nthe PathQuestions dataset by 18.25, 10.72, and 10.18 absolute points on BLEU-4,\nMETEOR, and ROUGE-L, respectively.",
            "author": [
                "Yuanyuan Liang",
                "Jianing Wang",
                "Hanlun Zhu",
                "Lei Wang",
                "Weining Qian",
                "Yunshi Lan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08395v3",
                "http://arxiv.org/pdf/2310.08395v3"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08383v1",
            "title": "Reconstructing Materials Tetrahedron: Challenges in Materials\n  Information Extraction",
            "updated": "2023-10-12T14:57:24Z",
            "published": "2023-10-12T14:57:24Z",
            "summary": "Discovery of new materials has a documented history of propelling human\nprogress for centuries and more. The behaviour of a material is a function of\nits composition, structure, and properties, which further depend on its\nprocessing and testing conditions. Recent developments in deep learning and\nnatural language processing have enabled information extraction at scale from\npublished literature such as peer-reviewed publications, books, and patents.\nHowever, this information is spread in multiple formats, such as tables, text,\nand images, and with little or no uniformity in reporting style giving rise to\nseveral machine learning challenges. Here, we discuss, quantify, and document\nthese outstanding challenges in automated information extraction (IE) from\nmaterials science literature towards the creation of a large materials science\nknowledge base. Specifically, we focus on IE from text and tables and outline\nseveral challenges with examples. We hope the present work inspires researchers\nto address the challenges in a coherent fashion, providing to fillip to IE for\nthe materials knowledge base.",
            "author": [
                "Kausik Hira",
                "Mohd Zaki",
                "Dhruvil Sheth",
                "Mausam",
                "N M Anoop Krishnan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08383v1",
                "http://arxiv.org/pdf/2310.08383v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cond-mat.mtrl-sci"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08372v3",
            "title": "Improving Factual Consistency for Knowledge-Grounded Dialogue Systems\n  via Knowledge Enhancement and Alignment",
            "updated": "2023-11-03T07:26:42Z",
            "published": "2023-10-12T14:44:05Z",
            "summary": "Pretrained language models (PLMs) based knowledge-grounded dialogue systems\nare prone to generate responses that are factually inconsistent with the\nprovided knowledge source. In such inconsistent responses, the dialogue models\nfail to accurately express the external knowledge they rely upon. Inspired by\nprevious work which identified that feed-forward networks (FFNs) within\nTransformers are responsible for factual knowledge expressions, we investigate\ntwo methods to efficiently improve the factual expression capability {of FFNs}\nby knowledge enhancement and alignment respectively. We first propose\n\\textsc{K-Dial}, which {explicitly} introduces {extended FFNs in Transformers\nto enhance factual knowledge expressions} given the specific patterns of\nknowledge-grounded dialogue inputs. Additionally, we apply the reinforcement\nlearning for factual consistency (RLFC) method to implicitly adjust FFNs'\nexpressions in responses by aligning with gold knowledge for the factual\nconsistency preference. To comprehensively assess the factual consistency and\ndialogue quality of responses, we employ extensive automatic measures and human\nevaluations including sophisticated fine-grained NLI-based metrics.\nExperimental results on WoW and CMU\\_DoG datasets demonstrate that our methods\nefficiently enhance the ability of the FFN module to convey factual knowledge,\nvalidating the efficacy of improving factual consistency for knowledge-grounded\ndialogue systems.",
            "author": [
                "Boyang Xue",
                "Weichao Wang",
                "Hongru Wang",
                "Fei Mi",
                "Rui Wang",
                "Yasheng Wang",
                "Lifeng Shang",
                "Xin Jiang",
                "Qun Liu",
                "Kam-Fai Wong"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08372v3",
                "http://arxiv.org/pdf/2310.08372v3"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08365v2",
            "title": "From Large Language Models to Knowledge Graphs for Biomarker Discovery\n  in Cancer",
            "updated": "2023-11-19T17:27:48Z",
            "published": "2023-10-12T14:36:13Z",
            "summary": "Domain experts often rely on most recent knowledge for apprehending and\ndisseminating specific biological processes that help them design strategies\nfor developing prevention and therapeutic decision-making in various disease\nscenarios. A challenging scenarios for artificial intelligence (AI) is using\nbiomedical data (e.g., texts, imaging, omics, and clinical) to provide\ndiagnosis and treatment recommendations for cancerous conditions.~Data and\nknowledge about biomedical entities like cancer, drugs, genes, proteins, and\ntheir mechanism is spread across structured (knowledge bases (KBs)) and\nunstructured (e.g., scientific articles) sources. A large-scale knowledge graph\n(KG) can be constructed by integrating and extracting facts about semantically\ninterrelated entities and relations. Such a KG not only allows exploration and\nquestion answering (QA) but also enables domain experts to deduce new\nknowledge. However, exploring and querying large-scale KGs is tedious for\nnon-domain users due to their lack of understanding of the data assets and\nsemantic technologies. In this paper, we develop a domain KG to leverage\ncancer-specific biomarker discovery and interactive QA. For this, we\nconstructed a domain ontology called OncoNet Ontology (ONO), which enables\nsemantic reasoning for validating gene-disease (different types of cancer)\nrelations. The KG is further enriched by harmonizing the ONO, metadata,\ncontrolled vocabularies, and biomedical concepts from scientific articles by\nemploying BioBERT- and SciBERT-based information extractors. Further, since the\nbiomedical domain is evolving, where new findings often replace old ones,\nwithout having access to up-to-date scientific findings, there is a high chance\nan AI system exhibits concept drift while providing diagnosis and treatment.\nTherefore, we fine-tune the KG using large language models (LLMs) based on more\nrecent articles and KBs.",
            "author": [
                "Md. Rezaul Karim",
                "Lina Molinas Comet",
                "Md Shajalal",
                "Oya Deniz Beyan",
                "Dietrich Rebholz-Schuhmann",
                "Stefan Decker"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08365v2",
                "http://arxiv.org/pdf/2310.08365v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08364v1",
            "title": "Map2Schedule: An End-to-End Link Scheduling Method for Urban V2V\n  Communications",
            "updated": "2023-10-12T14:35:38Z",
            "published": "2023-10-12T14:35:38Z",
            "summary": "Urban vehicle-to-vehicle (V2V) link scheduling with shared spectrum is a\nchallenging problem. Its main goal is to find the scheduling policy that can\nmaximize system performance (usually the sum capacity of each link or their\nenergy efficiency). Given that each link can experience interference from all\nother active links, the scheduling becomes a combinatorial integer programming\nproblem and generally does not scale well with the number of V2V pairs.\nMoreover, link scheduling requires accurate channel state information (CSI),\nwhich is very difficult to estimate with good accuracy under high vehicle\nmobility. In this paper, we propose an end-to-end urban V2V link scheduling\nmethod called Map2Schedule, which can directly generate V2V scheduling policy\nfrom the city map and vehicle locations. Map2Schedule delivers comparable\nperformance to the physical-model-based methods in urban settings while\nmaintaining low computation complexity. This enhanced performance is achieved\nby machine learning (ML) technologies. Specifically, we first deploy the\nconvolutional neural network (CNN) model to estimate the CSI from street layout\nand vehicle locations and then apply the graph embedding model for optimal\nscheduling policy. The results show that the proposed method can achieve high\naccuracy with much lower overhead and latency.",
            "author": [
                "Lihao Zhang",
                "Haijian Sun",
                "Jin Sun",
                "Ramviyas Parasuraman",
                "Yinghui Ye",
                "Rose Qingyang Hu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08364v1",
                "http://arxiv.org/pdf/2310.08364v1"
            ],
            "primary_category": "cs.NI",
            "category": [
                "cs.NI",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08350v1",
            "title": "ALPHA: Attention-based Long-horizon Pathfinding in Highly-structured\n  Areas",
            "updated": "2023-10-12T14:18:47Z",
            "published": "2023-10-12T14:18:47Z",
            "summary": "The multi-agent pathfinding (MAPF) problem seeks collision-free paths for a\nteam of agents from their current positions to their pre-set goals in a known\nenvironment, and is an essential problem found at the core of many logistics,\ntransportation, and general robotics applications. Existing learning-based MAPF\napproaches typically only let each agent make decisions based on a limited\nfield-of-view (FOV) around its position, as a natural means to fix the input\ndimensions of its policy network. However, this often makes policies\nshort-sighted, since agents lack the ability to perceive and plan for\nobstacles/agents beyond their FOV. To address this challenge, we propose ALPHA,\na new framework combining the use of ground truth proximal (local) information\nand fuzzy distal (global) information to let agents sequence local decisions\nbased on the full current state of the system, and avoid such myopicity. We\nfurther allow agents to make short-term predictions about each others' paths,\nas a means to reason about each others' path intentions, thereby enhancing the\nlevel of cooperation among agents at the whole system level. Our neural\nstructure relies on a Graph Transformer architecture to allow agents to\nselectively combine these different sources of information and reason about\ntheir inter-dependencies at different spatial scales. Our simulation\nexperiments demonstrate that ALPHA outperforms both globally-guided MAPF\nsolvers and communication-learning based ones, showcasing its potential towards\nscalability in realistic deployments.",
            "author": [
                "Chengyang He",
                "Tianze Yang",
                "Tanishq Duhan",
                "Yutong Wang",
                "Guillaume Sartoretti"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08350v1",
                "http://arxiv.org/pdf/2310.08350v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08349v1",
            "title": "Performativity and Prospective Fairness",
            "updated": "2023-10-12T14:18:13Z",
            "published": "2023-10-12T14:18:13Z",
            "summary": "Deploying an algorithmically informed policy is a significant intervention in\nthe structure of society. As is increasingly acknowledged, predictive\nalgorithms have performative effects: using them can shift the distribution of\nsocial outcomes away from the one on which the algorithms were trained.\nAlgorithmic fairness research is usually motivated by the worry that these\nperformative effects will exacerbate the structural inequalities that gave rise\nto the training data. However, standard retrospective fairness methodologies\nare ill-suited to predict these effects. They impose static fairness\nconstraints that hold after the predictive algorithm is trained, but before it\nis deployed and, therefore, before performative effects have had a chance to\nkick in. However, satisfying static fairness criteria after training is not\nsufficient to avoid exacerbating inequality after deployment. Addressing the\nfundamental worry that motivates algorithmic fairness requires explicitly\ncomparing the change in relevant structural inequalities before and after\ndeployment. We propose a prospective methodology for estimating this\npost-deployment change from pre-deployment data and knowledge about the\nalgorithmic policy. That requires a strategy for distinguishing between, and\naccounting for, different kinds of performative effects. In this paper, we\nfocus on the algorithmic effect on the causally downstream outcome variable.\nThroughout, we are guided by an application from public administration: the use\nof algorithms to (1) predict who among the recently unemployed will stay\nunemployed for the long term and (2) targeting them with labor market programs.\nWe illustrate our proposal by showing how to predict whether such policies will\nexacerbate gender inequalities in the labor market.",
            "author": [
                "Sebastian Zezulka",
                "Konstantin Genin"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08349v1",
                "http://arxiv.org/pdf/2310.08349v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY",
                "K.4.1; K.4.2; J.4"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08335v1",
            "title": "2SFGL: A Simple And Robust Protocol For Graph-Based Fraud Detection",
            "updated": "2023-10-12T13:48:26Z",
            "published": "2023-10-12T13:48:26Z",
            "summary": "Financial crime detection using graph learning improves financial safety and\nefficiency. However, criminals may commit financial crimes across different\ninstitutions to avoid detection, which increases the difficulty of detection\nfor financial institutions which use local data for graph learning. As most\nfinancial institutions are subject to strict regulations in regards to data\nprivacy protection, the training data is often isolated and conventional\nlearning technology cannot handle the problem. Federated learning (FL) allows\nmultiple institutions to train a model without revealing their datasets to each\nother, hence ensuring data privacy protection. In this paper, we proposes a\nnovel two-stage approach to federated graph learning (2SFGL): The first stage\nof 2SFGL involves the virtual fusion of multiparty graphs, and the second\ninvolves model training and inference on the virtual graph. We evaluate our\nframework on a conventional fraud detection task based on the\nFraudAmazonDataset and FraudYelpDataset. Experimental results show that\nintegrating and applying a GCN (Graph Convolutional Network) with our 2SFGL\nframework to the same task results in a 17.6\\%-30.2\\% increase in performance\non several typical metrics compared to the case only using FedAvg, while\nintegrating GraphSAGE with 2SFGL results in a 6\\%-16.2\\% increase in\nperformance compared to the case only using FedAvg. We conclude that our\nproposed framework is a robust and simple protocol which can be simply\nintegrated to pre-existing graph-based fraud detection methods.",
            "author": [
                "Zhirui Pan",
                "Guangzhong Wang",
                "Zhaoning Li",
                "Lifeng Chen",
                "Yang Bian",
                "Zhongyuan Lai"
            ],
            "link": [
                "http://dx.doi.org/10.1109/CloudCom55334.2022.00036",
                "http://arxiv.org/abs/2310.08335v1",
                "http://arxiv.org/pdf/2310.08335v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08331v1",
            "title": "Impact of multi-armed bandit strategies on deep recurrent reinforcement\n  learning",
            "updated": "2023-10-12T13:45:33Z",
            "published": "2023-10-12T13:45:33Z",
            "summary": "Incomplete knowledge of the environment leads an agent to make decisions\nunder uncertainty. One of the major dilemmas in Reinforcement Learning (RL)\nwhere an autonomous agent has to balance two contrasting needs in making its\ndecisions is: exploiting the current knowledge of the environment to maximize\nthe cumulative reward as well as exploring actions that allow improving the\nknowledge of the environment, hopefully leading to higher reward values\n(exploration-exploitation trade-off). Concurrently, another relevant issue\nregards the full observability of the states, which may not be assumed in all\napplications. Such as when only 2D images are considered as input in a RL\napproach used for finding the optimal action within a 3D simulation\nenvironment. In this work, we address these issues by deploying and testing\nseveral techniques to balance exploration and exploitation trade-off on\npartially observable systems for predicting steering wheels in autonomous\ndriving scenario. More precisely, the final aim is to investigate the effects\nof using both stochastic and deterministic multi-armed bandit strategies\ncoupled with a Deep Recurrent Q-Network. Additionally, we adapted and evaluated\nthe impact of an innovative method to improve the learning phase of the\nunderlying Convolutional Recurrent Neural Network. We aim to show that adaptive\nstochastic methods for exploration better approximate the trade-off between\nexploration and exploitation as, in general, Softmax and Max-Boltzmann\nstrategies are able to outperform epsilon-greedy techniques.",
            "author": [
                "Valentina Zangirolami",
                "Matteo Borrotti"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08331v1",
                "http://arxiv.org/pdf/2310.08331v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08328v2",
            "title": "Transport-Hub-Aware Spatial-Temporal Adaptive Graph Transformer for\n  Traffic Flow Prediction",
            "updated": "2023-10-16T15:28:44Z",
            "published": "2023-10-12T13:44:35Z",
            "summary": "As a core technology of Intelligent Transportation System (ITS), traffic flow\nprediction has a wide range of applications. Traffic flow data are\nspatial-temporal, which are not only correlated to spatial locations in road\nnetworks, but also vary with temporal time indices. Existing methods have\nsolved the challenges in traffic flow prediction partly, focusing on modeling\nspatial-temporal dependencies effectively, while not all intrinsic properties\nof traffic flow data are utilized fully. Besides, there are very few attempts\nat incremental learning of spatial-temporal data mining, and few previous works\ncan be easily transferred to the traffic flow prediction task. Motivated by the\nchallenge of incremental learning methods for traffic flow prediction and the\nunderutilization of intrinsic properties of road networks, we propose a\nTransport-Hub-aware Spatial-Temporal adaptive graph transFormer (H-STFormer)\nfor traffic flow prediction. Specifically, we first design a novel spatial\nself-attention module to capture the dynamic spatial dependencies. Three graph\nmasking matrices are integrated into spatial self-attentions to highlight both\nshort- and long-term dependences. Additionally, we employ a temporal\nself-attention module to detect dynamic temporal patterns in the traffic flow\ndata. Finally, we design an extra spatial-temporal knowledge distillation\nmodule for incremental learning of traffic flow prediction tasks. Through\nextensive experiments, we show the effectiveness of H-STFormer in normal and\nincremental traffic flow prediction tasks. The code is available at\nhttps://github.com/Fantasy-Shaw/H-STFormer.",
            "author": [
                "Xiao Xu",
                "Lei Zhang",
                "Bailong Liu",
                "Zhizhen Liang",
                "Xuefei Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08328v2",
                "http://arxiv.org/pdf/2310.08328v2"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08314v1",
            "title": "Information Design for Congestion Games with Unknown Demand",
            "updated": "2023-10-12T13:22:57Z",
            "published": "2023-10-12T13:22:57Z",
            "summary": "We study a novel approach to information design in the standard traffic model\nof network congestion games. It captures the natural condition that the demand\nis unknown to the users of the network. A principal (e.g., a mobility service)\ncommits to a signaling strategy, observes the realized demand and sends a\n(public) signal to agents (i.e., users of the network). Based on the induced\nbelief about the demand, the users then form an equilibrium. We consider the\nalgorithmic goal of the principal: Compute a signaling scheme that minimizes\nthe expected total cost of the induced equilibrium. We concentrate on\nsingle-commodity networks and affine cost functions, for which we obtain the\nfollowing results. First, we devise a fully polynomial-time approximation\nscheme (FPTAS) for the case that the demand can only take two values. It relies\non several structural properties of the cost of the induced equilibrium as a\nfunction of the updated belief about the distribution of demands. We show that\nthis function is piecewise linear for any number of demands, and monotonic for\ntwo demands. Second, we give a complete characterization of the graph\nstructures for which it is optimal to fully reveal the information about the\nrealized demand. This signaling scheme turns out to be optimal for all cost\nfunctions and probability distributions over demands if and only if the graph\nis series-parallel. Third, we propose an algorithm that computes the optimal\nsignaling scheme for any number of demands whose time complexity is polynomial\nin the number of supports that occur in a Wardrop equilibrium for some demand.\nFinally, we conduct a computational study that tests this algorithm on\nreal-world instances.",
            "author": [
                "Svenja M. Griesbach",
                "Martin Hoefer",
                "Max Klimm",
                "Tim Koglin"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08314v1",
                "http://arxiv.org/pdf/2310.08314v1"
            ],
            "primary_category": "cs.GT",
            "category": [
                "cs.GT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08298v1",
            "title": "MProto: Multi-Prototype Network with Denoised Optimal Transport for\n  Distantly Supervised Named Entity Recognition",
            "updated": "2023-10-12T13:02:34Z",
            "published": "2023-10-12T13:02:34Z",
            "summary": "Distantly supervised named entity recognition (DS-NER) aims to locate entity\nmentions and classify their types with only knowledge bases or gazetteers and\nunlabeled corpus. However, distant annotations are noisy and degrade the\nperformance of NER models. In this paper, we propose a noise-robust prototype\nnetwork named MProto for the DS-NER task. Different from previous\nprototype-based NER methods, MProto represents each entity type with multiple\nprototypes to characterize the intra-class variance among entity\nrepresentations. To optimize the classifier, each token should be assigned an\nappropriate ground-truth prototype and we consider such token-prototype\nassignment as an optimal transport (OT) problem. Furthermore, to mitigate the\nnoise from incomplete labeling, we propose a novel denoised optimal transport\n(DOT) algorithm. Specifically, we utilize the assignment result between Other\nclass tokens and all prototypes to distinguish unlabeled entity tokens from\ntrue negatives. Experiments on several DS-NER benchmarks demonstrate that our\nMProto achieves state-of-the-art performance. The source code is now available\non Github.",
            "author": [
                "Shuhui Wu",
                "Yongliang Shen",
                "Zeqi Tan",
                "Wenqi Ren",
                "Jietian Guo",
                "Shiliang Pu",
                "Weiming Lu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08298v1",
                "http://arxiv.org/pdf/2310.08298v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08291v1",
            "title": "Expanding the Vocabulary of BERT for Knowledge Base Construction",
            "updated": "2023-10-12T12:52:46Z",
            "published": "2023-10-12T12:52:46Z",
            "summary": "Knowledge base construction entails acquiring structured information to\ncreate a knowledge base of factual and relational data, facilitating question\nanswering, information retrieval, and semantic understanding. The challenge\ncalled \"Knowledge Base Construction from Pretrained Language Models\" at\nInternational Semantic Web Conference 2023 defines tasks focused on\nconstructing knowledge base using language model. Our focus was on Track 1 of\nthe challenge, where the parameters are constrained to a maximum of 1 billion,\nand the inclusion of entity descriptions within the prompt is prohibited.\n  Although the masked language model offers sufficient flexibility to extend\nits vocabulary, it is not inherently designed for multi-token prediction. To\naddress this, we present Vocabulary Expandable BERT for knowledge base\nconstruction, which expand the language model's vocabulary while preserving\nsemantic embeddings for newly added words. We adopt task-specific\nre-pre-training on masked language model to further enhance the language model.\n  Through experimentation, the results show the effectiveness of our\napproaches. Our framework achieves F1 score of 0.323 on the hidden test set and\n0.362 on the validation set, both data set is provided by the challenge.\nNotably, our framework adopts a lightweight language model (BERT-base, 0.13\nbillion parameters) and surpasses the model using prompts directly on large\nlanguage model (Chatgpt-3, 175 billion parameters). Besides, Token-Recode\nachieves comparable performances as Re-pretrain. This research advances\nlanguage understanding models by enabling the direct embedding of multi-token\nentities, signifying a substantial step forward in link prediction task in\nknowledge graph and metadata completion in data management.",
            "author": [
                "Dong Yang",
                "Xu Wang",
                "Remzi Celebi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08291v1",
                "http://arxiv.org/pdf/2310.08291v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "68T20"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08284v1",
            "title": "Statistical arbitrage portfolio construction based on preference\n  relations",
            "updated": "2023-10-12T12:40:30Z",
            "published": "2023-10-12T12:40:30Z",
            "summary": "Statistical arbitrage methods identify mispricings in securities with the\ngoal of building portfolios which are weakly correlated with the market. In\npairs trading, an arbitrage opportunity is identified by observing relative\nprice movements between a pair of two securities. By simultaneously observing\nmultiple pairs, one can exploit different arbitrage opportunities and increase\nthe performance of such methods. However, the use of a large number of pairs is\ndifficult due to the increased probability of contradictory trade signals among\ndifferent pairs. In this paper, we propose a novel portfolio construction\nmethod based on preference relation graphs, which can reconcile contradictory\npairs trading signals across multiple security pairs. The proposed approach\nenables joint exploitation of arbitrage opportunities among a large number of\nsecurities. Experimental results using three decades of historical returns of\nroughly 500 stocks from the S\\&P 500 index show that the portfolios based on\npreference relations exhibit robust returns even with high transaction costs,\nand that their performance improves with the number of securities considered.",
            "author": [
                "Fredi \u0160ari\u0107",
                "Stjepan Begu\u0161i\u0107",
                "Andro Mer\u0107ep",
                "Zvonko Kostanj\u010dar"
            ],
            "link": [
                "http://dx.doi.org/10.1016/j.eswa.2023.121906",
                "http://arxiv.org/abs/2310.08284v1",
                "http://arxiv.org/pdf/2310.08284v1"
            ],
            "primary_category": "q-fin.PM",
            "category": [
                "q-fin.PM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08282v2",
            "title": "Data driven modeling of self-similar dynamics",
            "updated": "2023-10-27T05:25:49Z",
            "published": "2023-10-12T12:39:08Z",
            "summary": "Multiscale modeling of complex systems is crucial for understanding their\nintricacies. Data-driven multiscale modeling has emerged as a promising\napproach to tackle challenges associated with complex systems. On the other\nhand, self-similarity is prevalent in complex systems, hinting that large-scale\ncomplex systems can be modeled at a reduced cost. In this paper, we introduce a\nmultiscale neural network framework that incorporates self-similarity as prior\nknowledge, facilitating the modeling of self-similar dynamical systems. For\ndeterministic dynamics, our framework can discern whether the dynamics are\nself-similar. For uncertain dynamics, it can compare and determine which\nparameter set is closer to self-similarity. The framework allows us to extract\nscale-invariant kernels from the dynamics for modeling at any scale. Moreover,\nour method can identify the power law exponents in self-similar systems.\nPreliminary tests on the Ising model yielded critical exponents consistent with\ntheoretical expectations, providing valuable insights for addressing critical\nphase transitions in non-equilibrium systems.",
            "author": [
                "Ru-yi Tao",
                "Ning-ning Tao",
                "Yi-zhuang You",
                "Jiang Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08282v2",
                "http://arxiv.org/pdf/2310.08282v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cond-mat.stat-mech"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08279v1",
            "title": "CP-KGC: Constrained-Prompt Knowledge Graph Completion with Large\n  Language Models",
            "updated": "2023-10-12T12:31:23Z",
            "published": "2023-10-12T12:31:23Z",
            "summary": "Knowledge graph completion (KGC) aims to utilize existing knowledge to deduce\nand infer missing connections within knowledge graphs. Text-based approaches,\nlike SimKGC, have outperformed graph embedding methods, showcasing the promise\nof inductive KGC. However, the efficacy of text-based methods hinges on the\nquality of entity textual descriptions. In this paper, we identify the key\nissue of whether large language models (LLMs) can generate effective text. To\nmitigate hallucination in LLM-generated text in this paper, we introduce a\nconstraint-based prompt that utilizes the entity and its textual description as\ncontextual constraints to enhance data quality. Our Constrained-Prompt\nKnowledge Graph Completion (CP-KGC) method demonstrates effective inference\nunder low resource computing conditions and surpasses prior results on the\nWN18RR and FB15K237 datasets. This showcases the integration of LLMs in KGC\ntasks and provides new directions for future research.",
            "author": [
                "Rui Yang",
                "Li Fang",
                "Yi Zhou"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08279v1",
                "http://arxiv.org/pdf/2310.08279v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08261v1",
            "title": "GraphAlign: Enhancing Accurate Feature Alignment by Graph matching for\n  Multi-Modal 3D Object Detection",
            "updated": "2023-10-12T12:06:31Z",
            "published": "2023-10-12T12:06:31Z",
            "summary": "LiDAR and cameras are complementary sensors for 3D object detection in\nautonomous driving. However, it is challenging to explore the unnatural\ninteraction between point clouds and images, and the critical factor is how to\nconduct feature alignment of heterogeneous modalities. Currently, many methods\nachieve feature alignment by projection calibration only, without considering\nthe problem of coordinate conversion accuracy errors between sensors, leading\nto sub-optimal performance. In this paper, we present GraphAlign, a more\naccurate feature alignment strategy for 3D object detection by graph matching.\nSpecifically, we fuse image features from a semantic segmentation encoder in\nthe image branch and point cloud features from a 3D Sparse CNN in the LiDAR\nbranch. To save computation, we construct the nearest neighbor relationship by\ncalculating Euclidean distance within the subspaces that are divided into the\npoint cloud features. Through the projection calibration between the image and\npoint cloud, we project the nearest neighbors of point cloud features onto the\nimage features. Then by matching the nearest neighbors with a single point\ncloud to multiple images, we search for a more appropriate feature alignment.\nIn addition, we provide a self-attention module to enhance the weights of\nsignificant relations to fine-tune the feature alignment between heterogeneous\nmodalities. Extensive experiments on nuScenes benchmark demonstrate the\neffectiveness and efficiency of our GraphAlign.",
            "author": [
                "Ziying Song",
                "Haiyue Wei",
                "Lin Bai",
                "Lei Yang",
                "Caiyan Jia"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08261v1",
                "http://arxiv.org/pdf/2310.08261v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08258v1",
            "title": "A self-referenced optical phase noise analyzer for quantum technologies",
            "updated": "2023-10-12T12:04:50Z",
            "published": "2023-10-12T12:04:50Z",
            "summary": "Second generation quantum technologies aim to outperform classical\nalternatives by utilizing engineered quantum systems. Maintaining the coherence\nrequired to enable any quantum advantage requires detailed knowledge and\ncontrol over the noise the hosting system is subjected to. Characterizing noise\nprocesses via their power spectral density is routinely done throughout science\nand technology and can be a demanding task. Determining the phase noise power\nspectrum in leading quantum technology platforms, for example, can be either\noutside the reach of many phase noise analyzers, or be prohibitively expensive.\nIn this work, we present and characterize a cost-effective optical phase noise\nanalyzer for quantum technology applications. Using this setup we compare two\n$\\approx1\\ \\rm{Hz}$ linewidth ultra-stable oscillators near $729\\ \\rm{nm}$,\nusing them as references to determine and discuss the noise floor achieved in\nthis measurement apparatus with a focus on limitations and their tradeoffs. The\nachieved noise floor in this implementation of a low-cost, all-stock component,\nlow-complexity phase noise analyzer compares favourably to commercial\nofferings. This setup can find application in particular without a more stable\nreference or operational quantum system as sensor as would be the case for many\ncomponent manufacturers.",
            "author": [
                "Robert Freund",
                "Christian D. Marciniak",
                "Thomas Monz"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08258v1",
                "http://arxiv.org/pdf/2310.08258v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "physics.ins-det"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08256v1",
            "title": "Impact of Co-occurrence on Factual Knowledge of Large Language Models",
            "updated": "2023-10-12T12:01:32Z",
            "published": "2023-10-12T12:01:32Z",
            "summary": "Large language models (LLMs) often make factually incorrect responses despite\ntheir success in various applications. In this paper, we hypothesize that\nrelying heavily on simple co-occurrence statistics of the pre-training corpora\nis one of the main factors that cause factual errors. Our results reveal that\nLLMs are vulnerable to the co-occurrence bias, defined as preferring frequently\nco-occurred words over the correct answer. Consequently, LLMs struggle to\nrecall facts whose subject and object rarely co-occur in the pre-training\ndataset although they are seen during finetuning. We show that co-occurrence\nbias remains despite scaling up model sizes or finetuning. Therefore, we\nsuggest finetuning on a debiased dataset to mitigate the bias by filtering out\nbiased samples whose subject-object co-occurrence count is high. Although\ndebiased finetuning allows LLMs to memorize rare facts in the training set, it\nis not effective in recalling rare facts unseen during finetuning. Further\nresearch in mitigation will help build reliable language models by preventing\npotential errors. The code is available at\n\\url{https://github.com/CheongWoong/impact_of_cooccurrence}.",
            "author": [
                "Cheongwoong Kang",
                "Jaesik Choi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08256v1",
                "http://arxiv.org/pdf/2310.08256v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08243v1",
            "title": "Computing Twin-Width Parameterized by the Feedback Edge Number",
            "updated": "2023-10-12T11:40:07Z",
            "published": "2023-10-12T11:40:07Z",
            "summary": "The problem of whether and how one can compute the twin-width of a graph --\nalong with an accompanying contraction sequence -- lies at the forefront of the\narea of algorithmic model theory. While significant effort has been aimed at\nobtaining a fixed-parameter approximation for the problem when parameterized by\ntwin-width, here we approach the question from a different perspective and\nconsider whether one can obtain (near-)optimal contraction sequences under a\nlarger parameterization, notably the feedback edge number $k$. As our main\ncontributions, under this parameterization we obtain (1) a linear bikernel for\nthe problem of either computing a $2$-contraction sequence or determining that\nnone exists and (2) an approximate fixed-parameter algorithm which computes an\n$\\ell$-contraction sequence (for an arbitrary specified $\\ell$) or determines\nthat the twin-width of the input graph is at least $\\ell$. These algorithmic\nresults rely on newly obtained insights into the structure of optimal\ncontraction sequences, and as a byproduct of these we also slightly tighten the\nbound on the twin-width of graphs with small feedback edge number.",
            "author": [
                "Jakub Balab\u00e1n",
                "Robert Ganian",
                "Mathis Rocton"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08243v1",
                "http://arxiv.org/pdf/2310.08243v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08232v1",
            "title": "Language Models are Universal Embedders",
            "updated": "2023-10-12T11:25:46Z",
            "published": "2023-10-12T11:25:46Z",
            "summary": "In the large language model (LLM) revolution, embedding is a key component of\nvarious systems. For example, it is used to retrieve knowledge or memories for\nLLMs, to build content moderation filters, etc. As such cases span from English\nto other natural or programming languages, from retrieval to classification and\nbeyond, it is desirable to build a unified embedding model rather than\ndedicated ones for each scenario. In this work, we make an initial step towards\nthis goal, demonstrating that multiple languages (both natural and programming)\npre-trained transformer decoders can embed universally when finetuned on\nlimited English data. We provide a comprehensive practice with thorough\nevaluations. On English MTEB, our models achieve competitive performance on\ndifferent embedding tasks by minimal training data. On other benchmarks, such\nas multilingual classification and code search, our models (without any\nsupervision) perform comparably to, or even surpass heavily supervised\nbaselines and/or APIs. These results provide evidence of a promising path\ntowards building powerful unified embedders that can be applied across tasks\nand languages.",
            "author": [
                "Xin Zhang",
                "Zehan Li",
                "Yanzhao Zhang",
                "Dingkun Long",
                "Pengjun Xie",
                "Meishan Zhang",
                "Min Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08232v1",
                "http://arxiv.org/pdf/2310.08232v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08217v1",
            "title": "TriRE: A Multi-Mechanism Learning Paradigm for Continual Knowledge\n  Retention and Promotion",
            "updated": "2023-10-12T11:05:34Z",
            "published": "2023-10-12T11:05:34Z",
            "summary": "Continual learning (CL) has remained a persistent challenge for deep neural\nnetworks due to catastrophic forgetting (CF) of previously learned tasks.\nSeveral techniques such as weight regularization, experience rehearsal, and\nparameter isolation have been proposed to alleviate CF. Despite their relative\nsuccess, these research directions have predominantly remained orthogonal and\nsuffer from several shortcomings, while missing out on the advantages of\ncompeting strategies. On the contrary, the brain continually learns,\naccommodates, and transfers knowledge across tasks by simultaneously leveraging\nseveral neurophysiological processes, including neurogenesis, active\nforgetting, neuromodulation, metaplasticity, experience rehearsal, and\ncontext-dependent gating, rarely resulting in CF. Inspired by how the brain\nexploits multiple mechanisms concurrently, we propose TriRE, a novel CL\nparadigm that encompasses retaining the most prominent neurons for each task,\nrevising and solidifying the extracted knowledge of current and past tasks, and\nactively promoting less active neurons for subsequent tasks through rewinding\nand relearning. Across CL settings, TriRE significantly reduces task\ninterference and surpasses different CL approaches considered in isolation.",
            "author": [
                "Preetha Vijayan",
                "Prashant Bhat",
                "Elahe Arani",
                "Bahram Zonooz"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08217v1",
                "http://arxiv.org/pdf/2310.08217v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08213v1",
            "title": "A Universal Scheme for Partitioned Dynamic Shortest Path Index",
            "updated": "2023-10-12T11:03:00Z",
            "published": "2023-10-12T11:03:00Z",
            "summary": "Graph partitioning is a common solution to scale up the graph algorithms, and\nshortest path (SP) computation is one of them. However, the existing solutions\ntypically have a fixed partition method with a fixed path index and fixed\npartition structure, so it is unclear how the partition method and path index\ninfluence the pathfinding performance. Moreover, few studies have explored the\nindex maintenance of partitioned SP (PSP) on dynamic graphs. To provide a\ndeeper insight into the dynamic PSP indexes, we systematically deliberate on\nthe existing works and propose a universal scheme to analyze this problem\ntheoretically. Specifically, we first propose two novel partitioned index\nstrategies and one optimization to improve index construction, query answering,\nor index maintenance of PSP index. Then we propose a path-oriented graph\npartitioning classification criteria for easier partition method selection.\nAfter that, we re-couple the dimensions in our scheme (partitioned index\nstrategy, path index, and partition structure) to propose five new partitioned\nSP indexes that are more efficient either in the query or update on different\nnetworks. Finally, we demonstrate the effectiveness of our new indexes by\ncomparing them with state-of-the-art PSP indexes through comprehensive\nevaluations.",
            "author": [
                "Mengxuan Zhang",
                "Xinjie Zhou",
                "Lei Li",
                "Ziyi Liu",
                "Goce Trajcevski",
                "Yan Huang",
                "Xiaofang Zhou"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08213v1",
                "http://arxiv.org/pdf/2310.08213v1"
            ],
            "primary_category": "cs.DB",
            "category": [
                "cs.DB"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08191v2",
            "title": "Algebraic properties of binomial edge ideals of Levi graphs associated\n  with curve arrangements",
            "updated": "2023-10-24T07:45:22Z",
            "published": "2023-10-12T10:36:13Z",
            "summary": "In this article, we study algebraic properties of binomial edge ideals\nassociated with certain plane curve arrangements via their Levi graphs. Using\ncombinatorial properties of the Levi graphs, we discuss the Cohen-Macaulayness\nof binomial edge ideals of some curve arrangements in the complex projective\nplane like the $d$-arrangement of curves and the conic-line arrangement. We\nalso discuss the existence of certain induced cycles in the Levi graph of these\narrangements and obtain lower bounds for the regularity of powers of the\ncorresponding binomial edge ideals.",
            "author": [
                "Rupam Karmakar",
                "Rajib Sarkar",
                "Aditya Subramaniam"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08191v2",
                "http://arxiv.org/pdf/2310.08191v2"
            ],
            "primary_category": "math.AC",
            "category": [
                "math.AC",
                "math.AG",
                "math.CO",
                "05E40, 13C14, 13C15, 14N10, 14N20"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08189v2",
            "title": "New graph invariants based on $p$-Laplacian eigenvalues",
            "updated": "2023-10-31T14:24:55Z",
            "published": "2023-10-12T10:32:36Z",
            "summary": "We present monotonicity inequalities for certain functions involving\neigenvalues of $p$-Laplacians on signed graphs with respect to $p$. Inspired by\nsuch monotonicity, we propose new spectrum-based graph invariants, called\n(variational) cut-off adjacency eigenvalues, that are relevant to certain\neigenvector-dependent nonlinear eigenvalue problem. Using these invariants, we\nobtain new lower bounds for the $p$-Laplacian variational eigenvalues,\nessentially giving the state-of-the-art spectral asymptotics for these\neigenvalues. Moreover, based on such invariants, we establish two inertia\nbounds regarding the cardinalities of a maximum independent set and a minimum\nedge cover, respectively. The first inertia bound enhances the classical\nCvetkovi\\'c bound, and the second one implies that the $k$-th $p$-Laplacian\nvariational eigenvalue is of the order $2^p$ as $p$ tends to infinity whenever\n$k$ is larger than the cardinality of a minimum edge cover of the underlying\ngraph. We further discover an interesting connection between graph\n$p$-Laplacian eigenvalues and tensor eigenvalues and discuss applications of\nour invariants to spectral problems of tensors.",
            "author": [
                "Chuanyuan Ge",
                "Shiping Liu",
                "Dong Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08189v2",
                "http://arxiv.org/pdf/2310.08189v2"
            ],
            "primary_category": "math.SP",
            "category": [
                "math.SP",
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08176v4",
            "title": "Infinite Width Graph Neural Networks for Node Regression/ Classification",
            "updated": "2023-11-20T17:10:22Z",
            "published": "2023-10-12T10:01:39Z",
            "summary": "This work analyzes Graph Neural Networks, a generalization of Fully-Connected\nDeep Neural Nets on Graph structured data, when their width, that is the number\nof nodes in each fullyconnected layer is increasing to infinity. Infinite Width\nNeural Networks are connecting Deep Learning to Gaussian Processes and Kernels,\nboth Machine Learning Frameworks with long traditions and extensive theoretical\nfoundations. Gaussian Processes and Kernels have much less hyperparameters then\nNeural Networks and can be used for uncertainty estimation, making them more\nuser friendly for applications. This works extends the increasing amount of\nresearch connecting Gaussian Processes and Kernels to Neural Networks. The\nKernel and Gaussian Process closed forms are derived for a variety of\narchitectures, namely the standard Graph Neural Network, the Graph Neural\nNetwork with Skip-Concatenate Connections and the Graph Attention Neural\nNetwork. All architectures are evaluated on a variety of datasets on the task\nof transductive Node Regression and Classification. Additionally, a Spectral\nSparsification method known as Effective Resistance is used to improve runtime\nand memory requirements. Extending the setting to inductive graph learning\ntasks (Graph Regression/ Classification) is straightforward and is briefly\ndiscussed in 3.5.",
            "author": [
                "Yunus Cobanoglu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08176v4",
                "http://arxiv.org/pdf/2310.08176v4"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08174v1",
            "title": "Mapping Water on the Moon and Mars using a Muon Tomograph",
            "updated": "2023-10-12T10:01:31Z",
            "published": "2023-10-12T10:01:31Z",
            "summary": "The search for water on the Lunar and Martian surfaces is a fundamental\naspect of space exploration, contributing to the understanding of the history\nand evolution of these celestial bodies. However, the current understanding of\nthe distribution, concentration, origin, and migration of water on these\nsurfaces is limited. Moreover, there is a need for more detailed data on these\naspects of Lunar and Martian water. The natural flux of cosmic-ray muons,\ncapable of penetrating the planetary surface, offers a method to study the\nwater-ice content, composition, and density of these surfaces. In this paper,\nthe author presents a novel approach to address these knowledge gaps by\nemploying cosmic-ray muon detectors and backscattered radiation. The study\ndescribes a cutting-edge muon tracking system developed by GScan and highlights\nthe results of preliminary simulations conducted using GEANT4. These findings\nsuggest that muon tomography could be a potential tool for investigating\nwater-ice content on the Lunar and Martian surfaces, pointing to new avenues\nfor space science exploration.",
            "author": [
                "Olin Lyod Pinto",
                "J\u00f6rg Miikael Tiit"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08174v1",
                "http://arxiv.org/pdf/2310.08174v1"
            ],
            "primary_category": "physics.ins-det",
            "category": [
                "physics.ins-det",
                "hep-ex",
                "physics.ao-ph",
                "physics.space-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08172v2",
            "title": "Exploring the Cognitive Knowledge Structure of Large Language Models: An\n  Educational Diagnostic Assessment Approach",
            "updated": "2023-10-18T11:37:43Z",
            "published": "2023-10-12T09:55:45Z",
            "summary": "Large Language Models (LLMs) have not only exhibited exceptional performance\nacross various tasks, but also demonstrated sparks of intelligence. Recent\nstudies have focused on assessing their capabilities on human exams and\nrevealed their impressive competence in different domains. However, cognitive\nresearch on the overall knowledge structure of LLMs is still lacking. In this\npaper, based on educational diagnostic assessment method, we conduct an\nevaluation using MoocRadar, a meticulously annotated human test dataset based\non Bloom Taxonomy. We aim to reveal the knowledge structures of LLMs and gain\ninsights of their cognitive capabilities. This research emphasizes the\nsignificance of investigating LLMs' knowledge and understanding the disparate\ncognitive patterns of LLMs. By shedding light on models' knowledge, researchers\ncan advance development and utilization of LLMs in a more informed and\neffective manner.",
            "author": [
                "Zheyuan Zhang",
                "Jifan Yu",
                "Juanzi Li",
                "Lei Hou"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08172v2",
                "http://arxiv.org/pdf/2310.08172v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08148v1",
            "title": "Open-Set Knowledge-Based Visual Question Answering with Inference Paths",
            "updated": "2023-10-12T09:12:50Z",
            "published": "2023-10-12T09:12:50Z",
            "summary": "Given an image and an associated textual question, the purpose of\nKnowledge-Based Visual Question Answering (KB-VQA) is to provide a correct\nanswer to the question with the aid of external knowledge bases. Prior KB-VQA\nmodels are usually formulated as a retriever-classifier framework, where a\npre-trained retriever extracts textual or visual information from knowledge\ngraphs and then makes a prediction among the candidates. Despite promising\nprogress, there are two drawbacks with existing models. Firstly, modeling\nquestion-answering as multi-class classification limits the answer space to a\npreset corpus and lacks the ability of flexible reasoning. Secondly, the\nclassifier merely consider \"what is the answer\" without \"how to get the\nanswer\", which cannot ground the answer to explicit reasoning paths. In this\npaper, we confront the challenge of \\emph{explainable open-set} KB-VQA, where\nthe system is required to answer questions with entities at wild and retain an\nexplainable reasoning path. To resolve the aforementioned issues, we propose a\nnew retriever-ranker paradigm of KB-VQA, Graph pATH rankER (GATHER for\nbrevity). Specifically, it contains graph constructing, pruning, and path-level\nranking, which not only retrieves accurate answers but also provides inference\npaths that explain the reasoning process. To comprehensively evaluate our\nmodel, we reformulate the benchmark dataset OK-VQA with manually corrected\nentity-level annotations and release it as ConceptVQA. Extensive experiments on\nreal-world questions demonstrate that our framework is not only able to perform\nopen-set question answering across the whole knowledge base but provide\nexplicit reasoning path.",
            "author": [
                "Jingru Gan",
                "Xinzhe Han",
                "Shuhui Wang",
                "Qingming Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08148v1",
                "http://arxiv.org/pdf/2310.08148v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08140v1",
            "title": "CODY: A graph-based framework for the analysis of COnversation DYnamics\n  in online social networks",
            "updated": "2023-10-12T08:55:32Z",
            "published": "2023-10-12T08:55:32Z",
            "summary": "Conversations are an integral part of online social media, and gaining\ninsights into these conversations is of significant value for many commercial\nas well as academic use cases. From a computational perspective, however,\nanalyzing conversation data is complex, and numerous aspects must be\nconsidered. Next to the structure of conversations, the discussed content - as\nwell as their dynamics - have to be taken into account. Still, most existing\nmodelling and analysis approaches focus only on one of these aspects and, in\nparticular, lack the capability to investigate the temporal evolution of a\nconversation. To address these shortcomings, in this work, we present CODY, a\ncontent-aware, graph-based framework to study the dynamics of online\nconversations along multiple dimensions. Its capabilities are extensively\ndemonstrated by conducting three experiments based on a large conversation\ndataset from the German political Twittersphere. First, the posting activity\nacross the lifetime of conversations is examined. We find that posting activity\nfollows an exponential saturation pattern. Based on this activity model, we\ndevelop a volume-based sampling method to study conversation dynamics using\ntemporal network snapshots. In a second experiment, we focus on the evolution\nof a conversation's structure and leverage a novel metric, the temporal Wiener\nindex, for that. Results indicate that as conversations progress, a\nconversation's structure tends to be less sprawling and more centered around\nthe original seed post. Furthermore, focusing on the dynamics of content in\nconversations, the evolution of hashtag usage within conversations is studied.\nInitially used hashtags do not necessarily keep their dominant prevalence\nthroughout the lifetime of a conversation. Instead, various \"hashtag hijacking\"\nscenarios are found.",
            "author": [
                "John Ziegler",
                "Fabian Kneissl",
                "Michael Gertz"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08140v1",
                "http://arxiv.org/pdf/2310.08140v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08138v1",
            "title": "Multi-Scale Spatial-Temporal Recurrent Networks for Traffic Flow\n  Prediction",
            "updated": "2023-10-12T08:52:36Z",
            "published": "2023-10-12T08:52:36Z",
            "summary": "Traffic flow prediction is one of the most fundamental tasks of intelligent\ntransportation systems. The complex and dynamic spatial-temporal dependencies\nmake the traffic flow prediction quite challenging. Although existing\nspatial-temporal graph neural networks hold prominent, they often encounter\nchallenges such as (1) ignoring the fixed graph that limits the predictive\nperformance of the model, (2) insufficiently capturing complex spatial-temporal\ndependencies simultaneously, and (3) lacking attention to spatial-temporal\ninformation at different time lengths. In this paper, we propose a Multi-Scale\nSpatial-Temporal Recurrent Network for traffic flow prediction, namely MSSTRN,\nwhich consists of two different recurrent neural networks: the single-step gate\nrecurrent unit and the multi-step gate recurrent unit to fully capture the\ncomplex spatial-temporal information in the traffic data under different time\nwindows. Moreover, we propose a spatial-temporal synchronous attention\nmechanism that integrates adaptive position graph convolutions into the\nself-attention mechanism to achieve synchronous capture of spatial-temporal\ndependencies. We conducted extensive experiments on four real traffic datasets\nand demonstrated that our model achieves the best prediction accuracy with\nnon-trivial margins compared to all the twenty baseline methods.",
            "author": [
                "Haiyang Liu",
                "Chunjiang Zhu",
                "Detian Zhang",
                "Qing Li"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08138v1",
                "http://arxiv.org/pdf/2310.08138v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08119v1",
            "title": "The existence of ground state solutions for nonlinear p-Laplacian\n  equations on lattice graphs",
            "updated": "2023-10-12T08:22:57Z",
            "published": "2023-10-12T08:22:57Z",
            "summary": "In this paper, we study the nonlinear $p$-Laplacian equation\n  $$-\\Delta_{p} u+V(x)|u|^{p-2}u=f(x,u) $$ with positive and periodic potential\n$V$ on the lattice graph $\\mathbb{Z}^{N}$, where $\\Delta_{p}$ is the discrete\n$p$-Laplacian, $p \\in (1,\\infty)$. The nonlinearity $f$ is also periodic in $x$\nand satisfies the growth condition $|f(x,u)| \\leq a(1+|u|^{q-1})$ for some $\nq>p$. We first prove the equivalence of three function spaces on\n$\\mathbb{Z}^{N}$, which is quite different from the continuous case and allows\nus to remove the restriction $q>p^{*}$ in [SW10], where $p^{*}$ is the critical\nexponent for $ W^{1,p}(\\Omega) \\hookrightarrow L^{q}(\\Omega)$ with $\\Omega\n\\subset \\mathbb{R}^{N}$ bounded. Then, using the method of Nehari [Neh60,\nNeh61], we prove the existence of ground state solutions to the above equation.",
            "author": [
                "Bobo Hua",
                "Wendi Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08119v1",
                "http://arxiv.org/pdf/2310.08119v1"
            ],
            "primary_category": "math.AP",
            "category": [
                "math.AP",
                "math.CO",
                "35Q55, 39A14, 58E30"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08115v1",
            "title": "Model-Agnostic Covariate-Assisted Inference on Partially Identified\n  Causal Effects",
            "updated": "2023-10-12T08:17:30Z",
            "published": "2023-10-12T08:17:30Z",
            "summary": "Many causal estimands are only partially identifiable since they depend on\nthe unobservable joint distribution between potential outcomes. Stratification\non pretreatment covariates can yield sharper partial identification bounds;\nhowever, unless the covariates are discrete with relatively small support, this\napproach typically requires consistent estimation of the conditional\ndistributions of the potential outcomes given the covariates. Thus, existing\napproaches may fail under model misspecification or if consistency assumptions\nare violated. In this study, we propose a unified and model-agnostic\ninferential approach for a wide class of partially identified estimands, based\non duality theory for optimal transport problems. In randomized experiments,\nour approach can wrap around any estimates of the conditional distributions and\nprovide uniformly valid inference, even if the initial estimates are\narbitrarily inaccurate. Also, our approach is doubly robust in observational\nstudies. Notably, this property allows analysts to use the multiplier bootstrap\nto select covariates and models without sacrificing validity even if the true\nmodel is not included. Furthermore, if the conditional distributions are\nestimated at semiparametric rates, our approach matches the performance of an\noracle with perfect knowledge of the outcome model. Finally, we propose an\nefficient computational framework, enabling implementation on many practical\nproblems in causal inference.",
            "author": [
                "Wenlong Ji",
                "Lihua Lei",
                "Asher Spector"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08115v1",
                "http://arxiv.org/pdf/2310.08115v1"
            ],
            "primary_category": "econ.EM",
            "category": [
                "econ.EM",
                "math.ST",
                "stat.ME",
                "stat.ML",
                "stat.TH",
                "62G15 (Primary), 62G05 (Secondary)",
                "G.3; I.2.m"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08081v1",
            "title": "Supersaturation beyond color-critical graphs",
            "updated": "2023-10-12T07:10:22Z",
            "published": "2023-10-12T07:10:22Z",
            "summary": "The supersaturation problem for a given graph $F$ asks for the minimum number\n$h_F(n,q)$ of copies of $F$ in an $n$-vertex graph with $ex(n,F)+q$ edges.\nSubsequent works by Rademacher, Erd\\H{o}s, and Lov\\'{a}sz and Simonovits\ndetermine the optimal range of $q$ (which is linear in $n$) for cliques $F$\nsuch that $h_F(n,q)$ equals the minimum number $t_F(n,q)$ of copies of $F$\nobtained from a maximum $F$-free $n$-vertex graph by adding $q$ new edges. A\nbreakthrough result of Mubayi extends this line of research from cliques to\ncolor-critical graphs $F$, and this was further strengthened by Pikhurko and\nYilma who established the equality $h_F(n,q)=t_F(n,q)$ for $1\\leq q\\leq\n\\epsilon_F n$ and sufficiently large $n$. In this paper, we present several\nresults on the supersaturation problem that extend beyond the existing\nframework. Firstly, we explicitly construct infinitely many graphs $F$ with\nrestricted properties for which $h_F(n,q)<q\\cdot t_F(n,1)$ holds when $n\\gg\nq\\geq 4$, thus refuting a conjecture of Mubayi. Secondly, we extend the result\nof Pikhurko-Yilma by showing the equality $h_F(n,q)=t_F(n,q)$ in the range\n$1\\leq q\\leq \\epsilon_F n$ for any member $F$ in a diverse and abundant graph\nfamily (which includes color-critical graphs, disjoint unions of cliques $K_r$,\nand the Petersen graph). Lastly, we prove the existence of a graph $F$ for any\npositive integer $s$ such that $h_F(n,q)=t_F(n,q)$ holds when $1\\leq q\\leq\n\\epsilon_F n^{1-1/s}$, and $h_F(n,q)<t_F(n,q)$ when $n^{1-1/s}/\\epsilon_F\\leq\nq\\leq \\epsilon_F n$, indicating that $q=\\Theta(n^{1-1/s})$ serves as the\nthreshold for the equality $h_F(n,q)=t_F(n,q)$. We also discuss some additional\nremarks and related open problems.",
            "author": [
                "Jie Ma",
                "Long-Tu Yuan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08081v1",
                "http://arxiv.org/pdf/2310.08081v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08071v1",
            "title": "Learning Transferable Conceptual Prototypes for Interpretable\n  Unsupervised Domain Adaptation",
            "updated": "2023-10-12T06:36:41Z",
            "published": "2023-10-12T06:36:41Z",
            "summary": "Despite the great progress of unsupervised domain adaptation (UDA) with the\ndeep neural networks, current UDA models are opaque and cannot provide\npromising explanations, limiting their applications in the scenarios that\nrequire safe and controllable model decisions. At present, a surge of work\nfocuses on designing deep interpretable methods with adequate data annotations\nand only a few methods consider the distributional shift problem. Most existing\ninterpretable UDA methods are post-hoc ones, which cannot facilitate the model\nlearning process for performance enhancement. In this paper, we propose an\ninherently interpretable method, named Transferable Conceptual Prototype\nLearning (TCPL), which could simultaneously interpret and improve the processes\nof knowledge transfer and decision-making in UDA. To achieve this goal, we\ndesign a hierarchically prototypical module that transfers categorical basic\nconcepts from the source domain to the target domain and learns domain-shared\nprototypes for explaining the underlying reasoning process. With the learned\ntransferable prototypes, a self-predictive consistent pseudo-label strategy\nthat fuses confidence, predictions, and prototype information, is designed for\nselecting suitable target samples for pseudo annotations and gradually\nnarrowing down the domain gap. Comprehensive experiments show that the proposed\nmethod can not only provide effective and intuitive explanations but also\noutperform previous state-of-the-arts.",
            "author": [
                "Junyu Gao",
                "Xinhong Ma",
                "Changsheng Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08071v1",
                "http://arxiv.org/pdf/2310.08071v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08064v1",
            "title": "Age Estimation Based on Graph Convolutional Networks and Multi-head\n  Attention Mechanisms",
            "updated": "2023-10-12T06:26:39Z",
            "published": "2023-10-12T06:26:39Z",
            "summary": "Age estimation technology is a part of facial recognition and has been\napplied to identity authentication. This technology achieves the development\nand application of a juvenile anti-addiction system by authenticating users in\nthe game. Convolutional Neural Network (CNN) and Transformer algorithms are\nwidely used in this application scenario. However, these two models cannot\nflexibly extract and model features of faces with irregular shapes, and they\nare ineffective in capturing key information. Furthermore, the above methods\nwill contain a lot of background information while extracting features, which\nwill interfere with the model. In consequence, it is easy to extract redundant\ninformation from images. In this paper, a new modeling idea is proposed to\nsolve this problem, which can flexibly model irregular objects. The Graph\nConvolutional Network (GCN) is used to extract features from irregular face\nimages effectively, and multi-head attention mechanisms are added to avoid\nredundant features and capture key region information in the image. This model\ncan effectively improve the accuracy of age estimation and reduce the MAE error\nvalue to about 3.64, which is better than the effect of today's age estimation\nmodel, to improve the accuracy of face recognition and identity authentication.",
            "author": [
                "Miaomiao Yang",
                "Changwei Yao",
                "Shijin Yan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08064v1",
                "http://arxiv.org/pdf/2310.08064v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08061v1",
            "title": "ETDock: A Novel Equivariant Transformer for Protein-Ligand Docking",
            "updated": "2023-10-12T06:23:12Z",
            "published": "2023-10-12T06:23:12Z",
            "summary": "Predicting the docking between proteins and ligands is a crucial and\nchallenging task for drug discovery. However, traditional docking methods\nmainly rely on scoring functions, and deep learning-based docking approaches\nusually neglect the 3D spatial information of proteins and ligands, as well as\nthe graph-level features of ligands, which limits their performance. To address\nthese limitations, we propose an equivariant transformer neural network for\nprotein-ligand docking pose prediction. Our approach involves the fusion of\nligand graph-level features by feature processing, followed by the learning of\nligand and protein representations using our proposed TAMformer module.\nAdditionally, we employ an iterative optimization approach based on the\npredicted distance matrix to generate refined ligand poses. The experimental\nresults on real datasets show that our model can achieve state-of-the-art\nperformance.",
            "author": [
                "Yiqiang Yi",
                "Xu Wan",
                "Yatao Bian",
                "Le Ou-Yang",
                "Peilin Zhao"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08061v1",
                "http://arxiv.org/pdf/2310.08061v1"
            ],
            "primary_category": "q-bio.BM",
            "category": [
                "q-bio.BM",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08057v1",
            "title": "Structural balance and spectral properties of generalized corona product\n  of signed graphs",
            "updated": "2023-10-12T06:09:38Z",
            "published": "2023-10-12T06:09:38Z",
            "summary": "In this paper, we extend our earlier proposal of corona product of signed\ngraphs into generalized corona product of signed graphs inspired by the\ngeneralized corona product of unsigned graphs. Then we study structural balance\nand spectral properties of these graphs. Utilizing the notion of coronal of a\ngraph, we determine computable formulae of characteristic, Laplacian, and\nsignless Laplacian polynomials of generalized corona product of signed graphs.\nFinally, we provide sufficient conditions for the generalized corona product of\nsome distinct collections of signed graphs to be co-spectral.",
            "author": [
                "Amrik Singh",
                "Ravi Srivastava",
                "Bibhas Adhikari",
                "Sandeep Kumar Yadav"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08057v1",
                "http://arxiv.org/pdf/2310.08057v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08039v1",
            "title": "Rethinking Large-scale Pre-ranking System: Entire-chain Cross-domain\n  Models",
            "updated": "2023-10-12T05:14:42Z",
            "published": "2023-10-12T05:14:42Z",
            "summary": "Industrial systems such as recommender systems and online advertising, have\nbeen widely equipped with multi-stage architectures, which are divided into\nseveral cascaded modules, including matching, pre-ranking, ranking and\nre-ranking. As a critical bridge between matching and ranking, existing\npre-ranking approaches mainly endure sample selection bias (SSB) problem owing\nto ignoring the entire-chain data dependence, resulting in sub-optimal\nperformances. In this paper, we rethink pre-ranking system from the perspective\nof the entire sample space, and propose Entire-chain Cross-domain Models (ECM),\nwhich leverage samples from the whole cascaded stages to effectively alleviate\nSSB problem. Besides, we design a fine-grained neural structure named ECMM to\nfurther improve the pre-ranking accuracy. Specifically, we propose a\ncross-domain multi-tower neural network to comprehensively predict for each\nstage result, and introduce the sub-networking routing strategy with $L0$\nregularization to reduce computational costs. Evaluations on real-world\nlarge-scale traffic logs demonstrate that our pre-ranking models outperform\nSOTA methods while time consumption is maintained within an acceptable level,\nwhich achieves better trade-off between efficiency and effectiveness.",
            "author": [
                "Jinbo Song",
                "Ruoran Huang",
                "Xinyang Wang",
                "Wei Huang",
                "Qian Yu",
                "Mingming Chen",
                "Yafei Yao",
                "Chaosheng Fan",
                "Changping Peng",
                "Zhangang Lin",
                "Jinghe Hu",
                "Jingping Shao"
            ],
            "link": [
                "http://dx.doi.org/10.1145/3511808.3557683",
                "http://arxiv.org/abs/2310.08039v1",
                "http://arxiv.org/pdf/2310.08039v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08038v1",
            "title": "Continual Learning via Manifold Expansion Replay",
            "updated": "2023-10-12T05:09:27Z",
            "published": "2023-10-12T05:09:27Z",
            "summary": "In continual learning, the learner learns multiple tasks in sequence, with\ndata being acquired only once for each task. Catastrophic forgetting is a major\nchallenge to continual learning. To reduce forgetting, some existing\nrehearsal-based methods use episodic memory to replay samples of previous\ntasks. However, in the process of knowledge integration when learning a new\ntask, this strategy also suffers from catastrophic forgetting due to an\nimbalance between old and new knowledge. To address this problem, we propose a\nnovel replay strategy called Manifold Expansion Replay (MaER). We argue that\nexpanding the implicit manifold of the knowledge representation in the episodic\nmemory helps to improve the robustness and expressiveness of the model. To this\nend, we propose a greedy strategy to keep increasing the diameter of the\nimplicit manifold represented by the knowledge in the buffer during memory\nmanagement. In addition, we introduce Wasserstein distance instead of cross\nentropy as distillation loss to preserve previous knowledge. With extensive\nexperimental validation on MNIST, CIFAR10, CIFAR100, and TinyImageNet, we show\nthat the proposed method significantly improves the accuracy in continual\nlearning setup, outperforming the state of the arts.",
            "author": [
                "Zihao Xu",
                "Xuan Tang",
                "Yufei Shi",
                "Jianfeng Zhang",
                "Jian Yang",
                "Mingsong Chen",
                "Xian Wei"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08038v1",
                "http://arxiv.org/pdf/2310.08038v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV",
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08032v1",
            "title": "Incorporating Domain Knowledge Graph into Multimodal Movie Genre\n  Classification with Self-Supervised Attention and Contrastive Learning",
            "updated": "2023-10-12T04:49:11Z",
            "published": "2023-10-12T04:49:11Z",
            "summary": "Multimodal movie genre classification has always been regarded as a demanding\nmulti-label classification task due to the diversity of multimodal data such as\nposters, plot summaries, trailers and metadata. Although existing works have\nmade great progress in modeling and combining each modality, they still face\nthree issues: 1) unutilized group relations in metadata, 2) unreliable\nattention allocation, and 3) indiscriminative fused features. Given that the\nknowledge graph has been proven to contain rich information, we present a novel\nframework that exploits the knowledge graph from various perspectives to\naddress the above problems. As a preparation, the metadata is processed into a\ndomain knowledge graph. A translate model for knowledge graph embedding is\nadopted to capture the relations between entities. Firstly we retrieve the\nrelevant embedding from the knowledge graph by utilizing group relations in\nmetadata and then integrate it with other modalities. Next, we introduce an\nAttention Teacher module for reliable attention allocation based on\nself-supervised learning. It learns the distribution of the knowledge graph and\nproduces rational attention weights. Finally, a Genre-Centroid Anchored\nContrastive Learning module is proposed to strengthen the discriminative\nability of fused features. The embedding space of anchors is initialized from\nthe genre entities in the knowledge graph. To verify the effectiveness of our\nframework, we collect a larger and more challenging dataset named MM-IMDb 2.0\ncompared with the MM-IMDb dataset. The experimental results on two datasets\ndemonstrate that our model is superior to the state-of-the-art methods. We will\nrelease the code in the near future.",
            "author": [
                "Jiaqi Li",
                "Guilin Qi",
                "Chuanyi Zhang",
                "Yongrui Chen",
                "Yiming Tan",
                "Chenlong Xia",
                "Ye Tian"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08032v1",
                "http://arxiv.org/pdf/2310.08032v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08031v1",
            "title": "Local Graph Clustering with Noisy Labels",
            "updated": "2023-10-12T04:37:15Z",
            "published": "2023-10-12T04:37:15Z",
            "summary": "The growing interest in machine learning problems over graphs with additional\nnode information such as texts, images, or labels has popularized methods that\nrequire the costly operation of processing the entire graph. Yet, little effort\nhas been made to the development of fast local methods (i.e. without accessing\nthe entire graph) that extract useful information from such data. To that end,\nwe propose a study of local graph clustering using noisy node labels as a proxy\nfor additional node information. In this setting, nodes receive initial binary\nlabels based on cluster affiliation: 1 if they belong to the target cluster and\n0 otherwise. Subsequently, a fraction of these labels is flipped. We\ninvestigate the benefits of incorporating noisy labels for local graph\nclustering. By constructing a weighted graph with such labels, we study the\nperformance of graph diffusion-based local clustering method on both the\noriginal and the weighted graphs. From a theoretical perspective, we consider\nrecovering an unknown target cluster with a single seed node in a random graph\nwith independent noisy node labels. We provide sufficient conditions on the\nlabel noise under which, with high probability, using diffusion in the weighted\ngraph yields a more accurate recovery of the target cluster. This approach\nproves more effective than using the given labels alone or using diffusion in\nthe label-free original graph. Empirically, we show that reliable node labels\ncan be obtained with just a few samples from an attributed graph. Moreover,\nutilizing these labels via diffusion in the weighted graph leads to\nsignificantly better local clustering performance across several real-world\ndatasets, improving F1 scores by up to 13%.",
            "author": [
                "Artur Back de Luca",
                "Kimon Fountoulakis",
                "Shenghao Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08031v1",
                "http://arxiv.org/pdf/2310.08031v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.SI",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08027v1",
            "title": "Exploring Large Language Models for Multi-Modal Out-of-Distribution\n  Detection",
            "updated": "2023-10-12T04:14:28Z",
            "published": "2023-10-12T04:14:28Z",
            "summary": "Out-of-distribution (OOD) detection is essential for reliable and trustworthy\nmachine learning. Recent multi-modal OOD detection leverages textual\ninformation from in-distribution (ID) class names for visual OOD detection, yet\nit currently neglects the rich contextual information of ID classes. Large\nlanguage models (LLMs) encode a wealth of world knowledge and can be prompted\nto generate descriptive features for each class. Indiscriminately using such\nknowledge causes catastrophic damage to OOD detection due to LLMs'\nhallucinations, as is observed by our analysis. In this paper, we propose to\napply world knowledge to enhance OOD detection performance through selective\ngeneration from LLMs. Specifically, we introduce a consistency-based\nuncertainty calibration method to estimate the confidence score of each\ngeneration. We further extract visual objects from each image to fully\ncapitalize on the aforementioned world knowledge. Extensive experiments\ndemonstrate that our method consistently outperforms the state-of-the-art.",
            "author": [
                "Yi Dai",
                "Hao Lang",
                "Kaisheng Zeng",
                "Fei Huang",
                "Yongbin Li"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08027v1",
                "http://arxiv.org/pdf/2310.08027v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08025v1",
            "title": "Visualizing Why Nondeterministic Finite-State Automata Reject",
            "updated": "2023-10-12T03:51:18Z",
            "published": "2023-10-12T03:51:18Z",
            "summary": "Students find their first course in Formal Languages and Automata Theory\nchallenging. In addition to the development of formal arguments, most students\nstruggle to understand nondeterministic computation models. In part, the\nstruggle stems from the course exposing them for the first time to\nnondeterminism. Often, students find it difficult to understand why a\nnondeterministic machine accepts or rejects a word. Furthermore, they may feel\nuncomfortable with there being multiple computations on the same input and with\na machine not consuming all of its input. This article describes a\nvisualization tool developed to help students understand nondeterministic\nbehavior. The tool is integrated into, FSM, a domain-specific language for the\nAutomata Theory classroom. The strategy is based on the automatic generation of\ncomputation graphs given a machine and an input word. Unlike previous\nvisualization tools, the computation graphs generated reflect the structure of\nthe given machine's transition relation and not the structure of the\ncomputation tree.",
            "author": [
                "Oliwia Kempinski",
                "Marco T. Moraz\u00e1n"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08025v1",
                "http://arxiv.org/pdf/2310.08025v1"
            ],
            "primary_category": "cs.PL",
            "category": [
                "cs.PL",
                "D.3.0"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08009v1",
            "title": "Dual-Stream Knowledge-Preserving Hashing for Unsupervised Video\n  Retrieval",
            "updated": "2023-10-12T03:21:12Z",
            "published": "2023-10-12T03:21:12Z",
            "summary": "Unsupervised video hashing usually optimizes binary codes by learning to\nreconstruct input videos. Such reconstruction constraint spends much effort on\nframe-level temporal context changes without focusing on video-level global\nsemantics that are more useful for retrieval. Hence, we address this problem by\ndecomposing video information into reconstruction-dependent and\nsemantic-dependent information, which disentangles the semantic extraction from\nreconstruction constraint. Specifically, we first design a simple dual-stream\nstructure, including a temporal layer and a hash layer. Then, with the help of\nsemantic similarity knowledge obtained from self-supervision, the hash layer\nlearns to capture information for semantic retrieval, while the temporal layer\nlearns to capture the information for reconstruction. In this way, the model\nnaturally preserves the disentangled semantics into binary codes. Validated by\ncomprehensive experiments, our method consistently outperforms the\nstate-of-the-arts on three video benchmarks.",
            "author": [
                "Pandeng Li",
                "Hongtao Xie",
                "Jiannan Ge",
                "Lei Zhang",
                "Shaobo Min",
                "Yongdong Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08009v1",
                "http://arxiv.org/pdf/2310.08009v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.08006v3",
            "title": "MCPNS: A Macropixel Collocated Position and Its Neighbors Search for\n  Plenoptic 2.0 Video Coding",
            "updated": "2023-11-28T00:57:15Z",
            "published": "2023-10-12T03:19:13Z",
            "summary": "Recently, it was demonstrated that a newly focused plenoptic 2.0 camera can\ncapture much higher spatial resolution owing to its effective light field\nsampling, as compared to a traditional unfocused plenoptic 1.0 camera. However,\ndue to the nature difference of the optical structure between the plenoptic 1.0\nand 2.0 cameras, the existing fast motion estimation (ME) method for plenoptic\n1.0 videos is expected to be sub-optimal for encoding plenoptic 2.0 videos. In\nthis paper, we point out the main motion characteristic differences between\nplenoptic 1.0 and 2.0 videos and then propose a new fast ME, called macropixel\ncollocated position and its neighbors search (MCPNS) for plenoptic 2.0 videos.\nIn detail, we propose to reduce the number of macropixel collocated position\n(MCP) search candidates based on the new observation of center-biased motion\nvector distribution at macropixel resolution. After that, due to large motion\ndeviation behavior around each MCP location in plenoptic 2.0 videos, we propose\nto select a certain number of key MCP locations with the lowest matching cost\nto perform the neighbors MCP search to improve the motion search accuracy.\nDifferent from existing methods, our method can achieve better performance\nwithout requiring prior knowledge of microlens array orientations. Our\nsimulation results confirmed the effectiveness of the proposed algorithm in\nterms of both bitrate savings and computational costs compared to existing\nmethods.",
            "author": [
                "Vinh Van Duong",
                "Thuc Nguyen Huu",
                "Jonghoon Yim",
                "Byeungwoo Jeon"
            ],
            "link": [
                "http://arxiv.org/abs/2310.08006v3",
                "http://arxiv.org/pdf/2310.08006v3"
            ],
            "primary_category": "cs.MM",
            "category": [
                "cs.MM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07999v1",
            "title": "LEMON: Lossless model expansion",
            "updated": "2023-10-12T03:02:41Z",
            "published": "2023-10-12T03:02:41Z",
            "summary": "Scaling of deep neural networks, especially Transformers, is pivotal for\ntheir surging performance and has further led to the emergence of sophisticated\nreasoning capabilities in foundation models. Such scaling generally requires\ntraining large models from scratch with random initialization, failing to\nleverage the knowledge acquired by their smaller counterparts, which are\nalready resource-intensive to obtain. To tackle this inefficiency, we present\n$\\textbf{L}$ossl$\\textbf{E}$ss $\\textbf{MO}$del Expansio$\\textbf{N}$ (LEMON), a\nrecipe to initialize scaled models using the weights of their smaller but\npre-trained counterparts. This is followed by model training with an optimized\nlearning rate scheduler tailored explicitly for the scaled models,\nsubstantially reducing the training time compared to training from scratch.\nNotably, LEMON is versatile, ensuring compatibility with various network\nstructures, including models like Vision Transformers and BERT. Our empirical\nresults demonstrate that LEMON reduces computational costs by 56.7% for Vision\nTransformers and 33.2% for BERT when compared to training from scratch.",
            "author": [
                "Yite Wang",
                "Jiahao Su",
                "Hanlin Lu",
                "Cong Xie",
                "Tianyi Liu",
                "Jianbo Yuan",
                "Haibin Lin",
                "Ruoyu Sun",
                "Hongxia Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07999v1",
                "http://arxiv.org/pdf/2310.07999v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07984v1",
            "title": "Large Language Models for Scientific Synthesis, Inference and\n  Explanation",
            "updated": "2023-10-12T02:17:59Z",
            "published": "2023-10-12T02:17:59Z",
            "summary": "Large language models are a form of artificial intelligence systems whose\nprimary knowledge consists of the statistical patterns, semantic relationships,\nand syntactical structures of language1. Despite their limited forms of\n\"knowledge\", these systems are adept at numerous complex tasks including\ncreative writing, storytelling, translation, question-answering, summarization,\nand computer code generation. However, they have yet to demonstrate advanced\napplications in natural science. Here we show how large language models can\nperform scientific synthesis, inference, and explanation. We present a method\nfor using general-purpose large language models to make inferences from\nscientific datasets of the form usually associated with special-purpose machine\nlearning algorithms. We show that the large language model can augment this\n\"knowledge\" by synthesizing from the scientific literature. When a conventional\nmachine learning system is augmented with this synthesized and inferred\nknowledge it can outperform the current state of the art across a range of\nbenchmark tasks for predicting molecular properties. This approach has the\nfurther advantage that the large language model can explain the machine\nlearning system's predictions. We anticipate that our framework will open new\navenues for AI to accelerate the pace of scientific discovery.",
            "author": [
                "Yizhen Zheng",
                "Huan Yee Koh",
                "Jiaxin Ju",
                "Anh T. N. Nguyen",
                "Lauren T. May",
                "Geoffrey I. Webb",
                "Shirui Pan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07984v1",
                "http://arxiv.org/pdf/2310.07984v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07980v2",
            "title": "GRASP: Accelerating Shortest Path Attacks via Graph Attention",
            "updated": "2023-10-23T17:50:33Z",
            "published": "2023-10-12T02:03:10Z",
            "summary": "Recent advances in machine learning (ML) have shown promise in aiding and\naccelerating classical combinatorial optimization algorithms. ML-based speed\nups that aim to learn in an end to end manner (i.e., directly output the\nsolution) tend to trade off run time with solution quality. Therefore,\nsolutions that are able to accelerate existing solvers while maintaining their\nperformance guarantees, are of great interest. We consider an APX-hard problem,\nwhere an adversary aims to attack shortest paths in a graph by removing the\nminimum number of edges. We propose the GRASP algorithm: Graph Attention\nAccelerated Shortest Path Attack, an ML aided optimization algorithm that\nachieves run times up to 10x faster, while maintaining the quality of solution\ngenerated. GRASP uses a graph attention network to identify a smaller subgraph\ncontaining the combinatorial solution, thus effectively reducing the input\nproblem size. Additionally, we demonstrate how careful representation of the\ninput graph, including node features that correlate well with the optimization\ntask, can highlight important structure in the optimization solution.",
            "author": [
                "Zohair Shafi",
                "Benjamin A. Miller",
                "Ayan Chatterjee",
                "Tina Eliassi-Rad",
                "Rajmonda S. Caceres"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07980v2",
                "http://arxiv.org/pdf/2310.07980v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07979v1",
            "title": "Graph-SCP: Accelerating Set Cover Problems with Graph Neural Networks",
            "updated": "2023-10-12T01:57:27Z",
            "published": "2023-10-12T01:57:27Z",
            "summary": "Machine learning (ML) approaches are increasingly being used to accelerate\ncombinatorial optimization (CO) problems. We look specifically at the Set Cover\nProblem (SCP) and propose Graph-SCP, a graph neural network method that can\naugment existing optimization solvers by learning to identify a much smaller\nsub-problem that contains the solution space. We evaluate the performance of\nGraph-SCP on synthetic weighted and unweighted SCP instances with diverse\nproblem characteristics and complexities, and on instances from the OR Library,\na canonical benchmark for SCP. We show that Graph-SCP reduces the problem size\nby 30-70% and achieves run time speedups up to~25x when compared to commercial\nsolvers (Gurobi). Given a desired optimality threshold, Graph-SCP will improve\nupon it or even achieve 100% optimality. This is in contrast to fast greedy\nsolutions that significantly compromise solution quality to achieve guaranteed\npolynomial run time. Graph-SCP can generalize to larger problem sizes and can\nbe used with other conventional or ML-augmented CO solvers to lead to potential\nadditional run time improvement.",
            "author": [
                "Zohair Shafi",
                "Benjamin A. Miller",
                "Tina Eliassi-Rad",
                "Rajmonda S. Caceres"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07979v1",
                "http://arxiv.org/pdf/2310.07979v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.DM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07958v3",
            "title": "Towards Causal Deep Learning for Vulnerability Detection",
            "updated": "2023-11-17T04:07:41Z",
            "published": "2023-10-12T00:51:06Z",
            "summary": "Deep learning vulnerability detection has shown promising results in recent\nyears. However, an important challenge that still blocks it from being very\nuseful in practice is that the model is not robust under perturbation and it\ncannot generalize well over the out-of-distribution (OOD) data, e.g., applying\na trained model to unseen projects in real world. We hypothesize that this is\nbecause the model learned non-robust features, e.g., variable names, that have\nspurious correlations with labels. When the perturbed and OOD datasets no\nlonger have the same spurious features, the model prediction fails. To address\nthe challenge, in this paper, we introduced causality into deep learning\nvulnerability detection. Our approach CausalVul consists of two phases. First,\nwe designed novel perturbations to discover spurious features that the model\nmay use to make predictions. Second, we applied the causal learning algorithms,\nspecifically, do-calculus, on top of existing deep learning models to\nsystematically remove the use of spurious features and thus promote causal\nbased prediction. Our results show that CausalVul consistently improved the\nmodel accuracy, robustness and OOD performance for all the state-of-the-art\nmodels and datasets we experimented. To the best of our knowledge, this is the\nfirst work that introduces do calculus based causal learning to software\nengineering models and shows it's indeed useful for improving the model\naccuracy, robustness and generalization. Our replication package is located at\nhttps://figshare.com/s/0ffda320dcb96c249ef2.",
            "author": [
                "Md Mahbubur Rahman",
                "Ira Ceka",
                "Chengzhi Mao",
                "Saikat Chakraborty",
                "Baishakhi Ray",
                "Wei Le"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07958v3",
                "http://arxiv.org/pdf/2310.07958v3"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE",
                "cs.CR",
                "cs.LG",
                "stat.ME"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07952v1",
            "title": "Modeling novel physics in virtual reality labs: An affective analysis of\n  student learning",
            "updated": "2023-10-12T00:27:24Z",
            "published": "2023-10-12T00:27:24Z",
            "summary": "We report on a study of the effects of laboratory activities that model\nfictitious laws of physics in a virtual reality environment on (1) students'\nepistemology about the role of experimental physics in class and in the world;\n(2) students' self-efficacy; and (3) the quality of student engagement with the\nlab activities. We create opportunities for students to practice physics as a\nmeans of creating and validating new knowledge by simulating real and\nfictitious physics in virtual reality (VR). This approach seeks to steer\nstudents away from a confirmation mindset in labs by eliminating any form of\nprior or outside models to confirm. We refer to the activities using this\napproach as Novel Observations in Mixed Reality (NOMR) labs. We examined NOMR's\neffects in 100-level and 200-level undergraduate courses. Using pre-post\nmeasurements we find that after NOMR labs, students in both populations were\nmore expertlike in their epistemology about experimental physics and held\nstronger self-efficacy about their abilities to do the kinds of things\nexperimental physicists do. Through the lens of the psychological theory of\nflow, we found that students engage as productively with NOMR labs as with\ntraditional hands-on labs. This engagement persisted after the novelty of VR in\nthe classroom wore off, suggesting that these effects are due to the\npedagogical design rather than the medium of the intervention. We conclude that\nthese NOMR labs offer an approach to physics laboratory instruction that\ncenters the development of students' understanding of and comfort with the\nauthentic practice of science.",
            "author": [
                "Jared P. Canright",
                "Suzanne White Brahmia"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07952v1",
                "http://arxiv.org/pdf/2310.07952v1"
            ],
            "primary_category": "physics.ed-ph",
            "category": [
                "physics.ed-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07941v1",
            "title": "A Convolutional Network Adaptation for Cortical Classification During\n  Mobile Brain Imaging",
            "updated": "2023-10-11T23:29:56Z",
            "published": "2023-10-11T23:29:56Z",
            "summary": "Deep neural networks (DNN) have become increasingly utilized in\nbrain-computer interface (BCI) technologies with the outset goal of classifying\nhuman physiological signals in computer-readable format. While our present\nunderstanding of DNN usage for BCI is promising, we have little experience in\ndeciphering neural events from dynamic freely-mobile situations. Using an\nimproved version of EEGNet, our goal was to classify cognitive events from\nelectroencephalography (EEG) signals while subjects simultaneously walked on a\ntreadmill, sometimes while carrying a rucksack equivalent to 40% of their body\nweight. Walking subjects simultaneously performed a visual oddball target\ndetection task, eliciting the P300 event-related potential (ERP), which then\nserved as the DNN classification target. We found the base EEGNet to reach\nclassification levels well above chance, with similar performance to previously\nreported P300 results. We found performance to be robust to noise, with\nclassification similar for walking and loaded walking, with respect to standard\nseated condition with minimal movement. With additional architectural search\nand tuning to the EEGNet model (termed Cog-Neuro, herein; CN-EEGNet), we\nreached classification accuracy of greater than 95%, similar to previously\nreported state of the art levels in seated P300 tasks. To our knowledge, these\nresults are the first documented implementation of a DNN for the classification\nof cognitive neural state during dual-task walking. The classification of one's\nongoing cognitive state during a demanding physical task establishes the\nutility for BCI in complex environments.",
            "author": [
                "Benjamin Cichy",
                "Jamie Lukos",
                "Mohammad Alam",
                "J. Cortney Bradford",
                "Nicholas Wymbs"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07941v1",
                "http://arxiv.org/pdf/2310.07941v1"
            ],
            "primary_category": "cs.NE",
            "category": [
                "cs.NE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07931v1",
            "title": "D2 Pruning: Message Passing for Balancing Diversity and Difficulty in\n  Data Pruning",
            "updated": "2023-10-11T23:01:29Z",
            "published": "2023-10-11T23:01:29Z",
            "summary": "Analytical theories suggest that higher-quality data can lead to lower test\nerrors in models trained on a fixed data budget. Moreover, a model can be\ntrained on a lower compute budget without compromising performance if a dataset\ncan be stripped of its redundancies. Coreset selection (or data pruning) seeks\nto select a subset of the training data so as to maximize the performance of\nmodels trained on this subset, also referred to as coreset. There are two\ndominant approaches: (1) geometry-based data selection for maximizing data\ndiversity in the coreset, and (2) functions that assign difficulty scores to\nsamples based on training dynamics. Optimizing for data diversity leads to a\ncoreset that is biased towards easier samples, whereas, selection by difficulty\nranking omits easy samples that are necessary for the training of deep learning\nmodels. This demonstrates that data diversity and importance scores are two\ncomplementary factors that need to be jointly considered during coreset\nselection. We represent a dataset as an undirected graph and propose a novel\npruning algorithm, D2 Pruning, that uses forward and reverse message passing\nover this dataset graph for coreset selection. D2 Pruning updates the\ndifficulty scores of each example by incorporating the difficulty of its\nneighboring examples in the dataset graph. Then, these updated difficulty\nscores direct a graph-based sampling method to select a coreset that\nencapsulates both diverse and difficult regions of the dataset space. We\nevaluate supervised and self-supervised versions of our method on various\nvision and language datasets. Results show that D2 Pruning improves coreset\nselection over previous state-of-the-art methods for up to 70% pruning rates.\nAdditionally, we find that using D2 Pruning for filtering large multimodal\ndatasets leads to increased diversity in the dataset and improved\ngeneralization of pretrained models.",
            "author": [
                "Adyasha Maharana",
                "Prateek Yadav",
                "Mohit Bansal"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07931v1",
                "http://arxiv.org/pdf/2310.07931v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CL",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07927v1",
            "title": "Enhanced sampling of Crystal Nucleation with Graph Representation Learnt\n  Variables",
            "updated": "2023-10-11T22:52:27Z",
            "published": "2023-10-11T22:52:27Z",
            "summary": "In this study, we present a graph neural network-based learning approach\nusing an autoencoder setup to derive low-dimensional variables from features\nobserved in experimental crystal structures. These variables are then biased in\nenhanced sampling to observe state-to-state transitions and reliable\nthermodynamic weights. Our approach uses simple convolution and pooling\nmethods. To verify the effectiveness of our protocol, we examined the\nnucleation of various allotropes and polymorphs of iron and glycine from their\nmolten states. Our graph latent variables when biased in well-tempered\nmetadynamics consistently show transitions between states and achieve accurate\nfree energy calculations in agreement with experiments, both of which are\nindicators of dependable sampling. This underscores the strength and promise of\nour graph neural net variables for improved sampling. The protocol shown here\nshould be applicable for other systems and with other sampling methods.",
            "author": [
                "Ziyue Zou",
                "Pratyush Tiwary"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07927v1",
                "http://arxiv.org/pdf/2310.07927v1"
            ],
            "primary_category": "cond-mat.stat-mech",
            "category": [
                "cond-mat.stat-mech",
                "cond-mat.mtrl-sci",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07923v3",
            "title": "The Expressive Power of Transformers with Chain of Thought",
            "updated": "2023-10-18T02:38:02Z",
            "published": "2023-10-11T22:35:18Z",
            "summary": "Recent theoretical work has identified surprisingly simple reasoning\nproblems, such as checking if two nodes in a graph are connected or simulating\nfinite-state machines, that are provably unsolvable by standard transformers\nthat answer immediately after reading their input. However, in practice,\ntransformers' reasoning can be improved by allowing them to use a \"chain of\nthought\" or \"scratchpad\", i.e., generate and condition on a sequence of\nintermediate tokens before answering. Motivated by this, we ask: Does such\nintermediate generation fundamentally extend the computational power of a\ndecoder-only transformer? We show that the answer is yes, but the amount of\nincrease depends crucially on the amount of intermediate generation. For\ninstance, we find that transformer decoders with a logarithmic number of\ndecoding steps (w.r.t. the input length) push the limits of standard\ntransformers only slightly, while a linear number of decoding steps adds a\nclear new ability (under standard complexity conjectures): recognizing all\nregular languages. Our results also imply that linear steps keep transformer\ndecoders within context-sensitive languages, and polynomial steps make them\nrecognize exactly the class of polynomial-time solvable problems -- the first\nexact characterization of a type of transformers in terms of standard\ncomplexity classes. Together, our results provide a nuanced framework for\nunderstanding how the length of a transformer's chain of thought or scratchpad\nimpacts its reasoning power.",
            "author": [
                "William Merrill",
                "Ashish Sabharwal"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07923v3",
                "http://arxiv.org/pdf/2310.07923v3"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CC",
                "cs.CL",
                "cs.LO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07922v2",
            "title": "Polyak Minorant Method for Convex Optimization",
            "updated": "2023-10-22T01:16:26Z",
            "published": "2023-10-11T22:31:42Z",
            "summary": "In 1963 Boris Polyak suggested a particular step size for gradient descent\nmethods, now known as the Polyak step size, that he later adapted to\nsubgradient methods. The Polyak step size requires knowledge of the optimal\nvalue of the minimization problem, which is a strong assumption but one that\nholds for several important problems. In this paper we extend Polyak's method\nto handle constraints and, as a generalization of subgradients, general\nminorants, which are convex functions that tightly lower bound the objective\nand constraint functions. We refer to this algorithm as the Polyak Minorant\nMethod (PMM). It is closely related to cutting-plane and bundle methods.",
            "author": [
                "Nikhil Devanathan",
                "Stephen Boyd"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07922v2",
                "http://arxiv.org/pdf/2310.07922v2"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07919v1",
            "title": "Representing and extending ensembles of parsimonious evolutionary\n  histories with a directed acyclic graph",
            "updated": "2023-10-11T22:17:46Z",
            "published": "2023-10-11T22:17:46Z",
            "summary": "In many situations, it would be useful to know not just the best phylogenetic\ntree for a given data set, but the collection of high-quality trees. This goal\nis typically addressed using Bayesian techniques, however, current Bayesian\nmethods do not scale to large data sets. Furthermore, for large data sets with\nrelatively low signal one cannot even store every good tree individually,\nespecially when the trees are required to be bifurcating. In this paper, we\ndevelop a novel object called the \"history subpartition directed acyclic graph\"\n(or \"history sDAG\" for short) that compactly represents an ensemble of trees\nwith labels (e.g. ancestral sequences) mapped onto the internal nodes. The\nhistory sDAG can be built efficiently and can also be efficiently trimmed to\nonly represent maximally parsimonious trees. We show that the history sDAG\nallows us to find many additional equally parsimonious trees, extending\ncombinatorially beyond the ensemble used to construct it. We argue that this\nobject could be useful as the \"skeleton\" of a more complete uncertainty\nquantification.",
            "author": [
                "Will Dumm",
                "Mary Barker",
                "William Howard-Snyder",
                "William S. DeWitt",
                "Frederick A. Matsen IV"
            ],
            "link": [
                "http://dx.doi.org/10.1007/s00285-023-02006-3",
                "http://arxiv.org/abs/2310.07919v1",
                "http://arxiv.org/pdf/2310.07919v1"
            ],
            "primary_category": "q-bio.PE",
            "category": [
                "q-bio.PE",
                "92-08 (Primary) 92B10, 92-04 (Secondary)"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07916v1",
            "title": "Dynamic Appearance Particle Neural Radiance Field",
            "updated": "2023-10-11T22:04:33Z",
            "published": "2023-10-11T22:04:33Z",
            "summary": "Neural Radiance Fields (NeRFs) have shown great potential in modelling 3D\nscenes. Dynamic NeRFs extend this model by capturing time-varying elements,\ntypically using deformation fields. The existing dynamic NeRFs employ a similar\nEulerian representation for both light radiance and deformation fields. This\nleads to a close coupling of appearance and motion and lacks a physical\ninterpretation. In this work, we propose Dynamic Appearance Particle Neural\nRadiance Field (DAP-NeRF), which introduces particle-based representation to\nmodel the motions of visual elements in a dynamic 3D scene. DAP-NeRF consists\nof superposition of a static field and a dynamic field. The dynamic field is\nquantised as a collection of {\\em appearance particles}, which carries the\nvisual information of a small dynamic element in the scene and is equipped with\na motion model. All components, including the static field, the visual features\nand motion models of the particles, are learned from monocular videos without\nany prior geometric knowledge of the scene. We develop an efficient\ncomputational framework for the particle-based model. We also construct a new\ndataset to evaluate motion modelling. Experimental results show that DAP-NeRF\nis an effective technique to capture not only the appearance but also the\nphysically meaningful motions in a 3D dynamic scene.",
            "author": [
                "Ancheng Lin",
                "Jun Li"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07916v1",
                "http://arxiv.org/pdf/2310.07916v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07912v2",
            "title": "Irreducibility of Markov Chains on simplicial complexes, the Spectrum of\n  the Discrete Hodge Laplacian and Homology",
            "updated": "2023-11-18T19:05:09Z",
            "published": "2023-10-11T21:41:31Z",
            "summary": "Random walks on graphs are a fundamental concept in graph theory and play a\ncrucial role in solving a wide range of theoretical and applied problems in\ndiscrete math, probability, theoretical computer science, network science, and\nmachine learning. The connection between Markov chains on graphs and their\ngeometric and topological structures is the main reason why such a wide range\nof theoretical and practical applications exist. Graph connectedness ensures\nirreducibility of a Markov chain. The convergence rate to the stationary\ndistribution is determined by the spectrum of the graph Laplacian which is\nassociated with lower bounds on graph curvature. Furthermore, walks on graphs\nare used to infer structural properties of underlying manifolds in data\nanalysis and manifold learning. However, an important question remains: can\nsimilar connections be established between Markov chains on simplicial\ncomplexes and the topology, geometry, and spectral properties of complexes?\nAdditionally, can we gain topological, geometric, or analytic information about\na manifold by defining appropriate Markov chains on its triangulations? These\nquestions are not only theoretically important but answers to them provide\npowerful tools for the analysis of complex networks that go beyond the analysis\nof pairwise interactions. In this paper, we provide an integrated overview of\nthe existing results on random walks on simplicial complexes, using the novel\nperspective of signed graphs. This perspective sheds light on previously\nunknown aspects such as irreducibility conditions. We show that while up-walks\non higher dimensional simplexes can never be irreducible, the down walks become\nirreducible if and only if the complex is orientable. We believe that this new\nintegrated perspective can be extended beyond discrete structures and enables\nexploration of classical problems for triangulable manifolds.",
            "author": [
                "Marzieh Eidi",
                "Sayan Mukherjee"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07912v2",
                "http://arxiv.org/pdf/2310.07912v2"
            ],
            "primary_category": "math.SP",
            "category": [
                "math.SP",
                "math.AT",
                "math.PR",
                "60J10, 37A30, 60B10, 57Z25, 55N99, 15B51, 55U10, 60J10, 05E45,\n  82C41, 60J05, 58J51, 00-02"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07907v2",
            "title": "The Pristine survey -- XXII. A serendipitous discovery of an extremely\n  Li-rich very metal-poor giant and a new method of $^6$Li/$^7$Li isotope\n  measurement",
            "updated": "2023-10-27T06:59:24Z",
            "published": "2023-10-11T21:24:59Z",
            "summary": "We report the serendipitous discovery of a very metal-poor (VMP) Li-rich\ngiant star ($T_{\\rm eff}$ = 4690$\\pm$80 K, log g = 1.34$\\pm$0.13, [Fe/H] =\n$-2.43\\pm$0.07). We analyse the Li I 6103 and 6707 \\r{A} lines accounting for\ndepartures from local thermodynamic equilibrium (NLTE) and correcting for 3D\neffects using literature data, which yields a lithium abundance\n$\\log\\varepsilon_{Li} = 3.42\\pm0.07$. Comparing lithium abundances from the two\nlines, in 1D NLTE we measure the isotope ratio $^6$Li/$^7$Li =\n1.64$^{+1.49}_{-1.08}$ %. When correcting for 3D effects, we detect the fragile\n$^6$Li isotope at $2$-sigma level and the ratio $^6$Li/$^7$Li =\n5.65$^{+5.05}_{-2.51}$ %. To our knowledge, this is the first $^6$Li/$^7$Li\nmeasurement in an extremely Li-rich VMP star. The Cameron-Fowler mechanism,\nwhich is proposed to produce Li-rich stars, does not imply $^6$Li production\nand is therefore inconsistent with our measurement when applying 3D\ncorrections. We also derive NLTE abundances for 16 elements, most of which show\nsimilar abundances to those found in VMP stars. Sodium is an exception:\n[Na/Fe]$_{\\rm NLTE, 1D}$ = 0.07 $\\pm 0.03$, which is 0.5 dex higher than what\nis typical for VMP stars. This star joins the sample of rare Li-rich VMP stars,\nand we offer a novel way to constrain the source of lithium in such stars\nthrough isotope ratio measurements.",
            "author": [
                "T. M. Sitnova",
                "T. Matsuno",
                "Z. Yuan",
                "N. F. Martin",
                "P. Banerjee",
                "F. Sestito",
                "K. A. Venn",
                "J. I. Gonz\u00e1lez Hern\u00e1ndez"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07907v2",
                "http://arxiv.org/pdf/2310.07907v2"
            ],
            "primary_category": "astro-ph.GA",
            "category": [
                "astro-ph.GA",
                "astro-ph.SR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07897v2",
            "title": "DeepNRMS: Unsupervised Deep Learning for Noise-Robust CO2 Monitoring in\n  Time-Lapse Seismic Images",
            "updated": "2023-10-13T03:10:57Z",
            "published": "2023-10-11T21:08:17Z",
            "summary": "Monitoring stored CO2 in carbon capture and storage projects is crucial for\nensuring safety and effectiveness. We introduce DeepNRMS, a novel noise-robust\nmethod that effectively handles time-lapse noise in seismic images. The\nDeepNRMS leverages unsupervised deep learning to acquire knowledge of\ntime-lapse noise characteristics from pre-injection surveys. By utilizing this\nlearned knowledge, our approach accurately discerns CO2-induced subtle signals\nfrom the high-amplitude time-lapse noise, ensuring fidelity in monitoring while\nreducing costs by enabling sparse acquisition. We evaluate our method using\nsynthetic data and field data acquired in the Aquistore project. In the\nsynthetic experiments, we simulate time-lapse noise by incorporating random\nnear-surface effects in the elastic properties of the subsurface model. We\ntrain our neural networks exclusively on pre-injection seismic images and\nsubsequently predict CO2 locations from post-injection seismic images. In the\nfield data analysis from Aquistore, the images from pre-injection surveys are\nutilized to train the neural networks with the characteristics of time-lapse\nnoise, followed by identifying CO2 plumes within two post-injection surveys.\nThe outcomes demonstrate the improved accuracy achieved by the DeepNRMS,\neffectively addressing the strong time-lapse noise.",
            "author": [
                "Min Jun Park",
                "Julio Frigerio",
                "Bob Clapp",
                "Biondo Biondi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07897v2",
                "http://arxiv.org/pdf/2310.07897v2"
            ],
            "primary_category": "physics.geo-ph",
            "category": [
                "physics.geo-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07893v1",
            "title": "Borel line graphs",
            "updated": "2023-10-11T20:55:21Z",
            "published": "2023-10-11T20:55:21Z",
            "summary": "We characterize Borel line graphs in terms of 10 forbidden induced subgraphs,\nnamely the 9 finite graphs from the classical result of Beineke together with a\n10th infinite graph associated to the equivalence relation $\\mathbb{E}_0$ on\nthe Cantor space. As a corollary, we prove a partial converse to the\nFeldman--Moore theorem, which allows us to characterize all locally countable\nBorel line graphs in terms of their Borel chromatic numbers.",
            "author": [
                "James Anderson",
                "Anton Bernshteyn"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07893v1",
                "http://arxiv.org/pdf/2310.07893v1"
            ],
            "primary_category": "math.LO",
            "category": [
                "math.LO",
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07892v1",
            "title": "ASV Station Keeping under Wind Disturbances using Neural Network\n  Simulation Error Minimization Model Predictive Control",
            "updated": "2023-10-11T20:55:13Z",
            "published": "2023-10-11T20:55:13Z",
            "summary": "Station keeping is an essential maneuver for Autonomous Surface Vehicles\n(ASVs), mainly when used in confined spaces, to carry out surveys that require\nthe ASV to keep its position or in collaboration with other vehicles where the\nrelative position has an impact over the mission. However, this maneuver can\nbecome challenging for classic feedback controllers due to the need for an\naccurate model of the ASV dynamics and the environmental disturbances. This\nwork proposes a Model Predictive Controller using Neural Network Simulation\nError Minimization (NNSEM-MPC) to accurately predict the dynamics of the ASV\nunder wind disturbances. The performance of the proposed scheme under wind\ndisturbances is tested and compared against other controllers in simulation,\nusing the Robotics Operating System (ROS) and the multipurpose simulation\nenvironment Gazebo. A set of six tests were conducted by combining two wind\nspeeds (3 m/s and 6 m/s) and three wind directions (0$^\\circ$, 90$^\\circ$, and\n180$^\\circ$). The simulation results clearly show the advantage of the\nNNSEM-MPC over the following methods: backstepping controller, sliding mode\ncontroller, simplified dynamics MPC (SD-MPC), neural ordinary differential\nequation MPC (NODE-MPC), and knowledge-based NODE MPC (KNODE-MPC). The proposed\nNNSEM-MPC approach performs better than the rest in 4 out of the 6 test\nconditions, and it is the second best in the 2 remaining test cases, reducing\nthe mean position and heading error by at least 31\\% and 46\\% respectively\nacross all the test cases. In terms of execution speed, the proposed NNSEM-MPC\nis at least 36\\% faster than the rest of the MPC controllers. The field\nexperiments on two different ASV platforms showed that ASVs can effectively\nkeep the station utilizing the proposed method, with a position error as low as\n$1.68$ m and a heading error as low as $6.14^{\\circ}$ within time windows of at\nleast $150$s.",
            "author": [
                "Jalil Chavez-Galaviz",
                "Jianwen Li",
                "Ajinkya Chaudhary",
                "Nina Mahmoudian"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07892v1",
                "http://arxiv.org/pdf/2310.07892v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07874v1",
            "title": "Refined Mechanism Design for Approximately Structured Priors via Active\n  Regression",
            "updated": "2023-10-11T20:34:17Z",
            "published": "2023-10-11T20:34:17Z",
            "summary": "We consider the problem of a revenue-maximizing seller with a large number of\nitems $m$ for sale to $n$ strategic bidders, whose valuations are drawn\nindependently from high-dimensional, unknown prior distributions. It is\nwell-known that optimal and even approximately-optimal mechanisms for this\nsetting are notoriously difficult to characterize or compute, and, even when\nthey can be found, are often rife with various counter-intuitive properties. In\nthis paper, following a model introduced recently by Cai and\nDaskalakis~\\cite{cai2022recommender}, we consider the case that bidders' prior\ndistributions can be well-approximated by a topic model. We design an active\nlearning component, responsible for interacting with the bidders and outputting\nlow-dimensional approximations of their types, and a mechanism design\ncomponent, responsible for robustifying mechanisms for the low-dimensional\nmodel to work for the approximate types of the former component. On the active\nlearning front, we cast our problem in the framework of Randomized Linear\nAlgebra (RLA) for regression problems, allowing us to import several\nbreakthrough results from that line of research, and adapt them to our setting.\nOn the mechanism design front, we remove many restrictive assumptions of prior\nwork on the type of access needed to the underlying distributions and the\nassociated mechanisms. To the best of our knowledge, our work is the first to\nformulate connections between mechanism design, and RLA for active learning of\nregression problems, opening the door for further applications of randomized\nlinear algebra primitives to mechanism design.",
            "author": [
                "Christos Boutsikas",
                "Petros Drineas",
                "Marios Mertzanidis",
                "Alexandros Psomas",
                "Paritosh Verma"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07874v1",
                "http://arxiv.org/pdf/2310.07874v1"
            ],
            "primary_category": "cs.GT",
            "category": [
                "cs.GT",
                "cs.DS",
                "cs.IR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07864v1",
            "title": "Towards Foundation Models for Materials Science: The Open MatSci ML\n  Toolkit",
            "updated": "2023-10-11T20:14:07Z",
            "published": "2023-10-11T20:14:07Z",
            "summary": "Artificial intelligence and machine learning have shown great promise in\ntheir ability to accelerate novel materials discovery. As researchers and\ndomain scientists seek to unify and consolidate chemical knowledge, the case\nfor models with potential to generalize across different tasks within materials\nscience - so-called \"foundation models\" - grows with ambitions. This manuscript\nreviews our recent progress with development of Open MatSci ML Toolkit, and\ndetails experiments that lay the groundwork for foundation model research and\ndevelopment with our framework. First, we describe and characterize a new\npretraining task that uses synthetic data generated from symmetry operations,\nand reveal complex training dynamics at large scales. Using the pretrained\nmodel, we discuss a number of use cases relevant to foundation model\ndevelopment: semantic architecture of datasets, and fine-tuning for property\nprediction and classification. Our key results show that for simple\napplications, pretraining appears to provide worse modeling performance than\ntraining models from random initialization. However, for more complex\ninstances, such as when a model is required to learn across multiple datasets\nand types of targets simultaneously, the inductive bias from pretraining\nprovides significantly better performance. This insight will hopefully inform\nsubsequent efforts into creating foundation models for materials science\napplications.",
            "author": [
                "Kin Long Kelvin Lee",
                "Carmelo Gonzales",
                "Matthew Spellings",
                "Mikhail Galkin",
                "Santiago Miret",
                "Nalini Kumar"
            ],
            "link": [
                "http://dx.doi.org/10.1145/3624062.3626081",
                "http://arxiv.org/abs/2310.07864v1",
                "http://arxiv.org/pdf/2310.07864v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci",
                "physics.comp-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07862v1",
            "title": "An $\\tilde\u03a9\\big(\\sqrt{\\log |T|}\\big)$ Lower Bound for Steiner Point\n  Removal",
            "updated": "2023-10-11T20:06:03Z",
            "published": "2023-10-11T20:06:03Z",
            "summary": "In the Steiner point removal (SPR) problem, we are given a (weighted) graph\n$G$ and a subset $T$ of its vertices called terminals, and the goal is to\ncompute a (weighted) graph $H$ on $T$ that is a minor of $G$, such that the\ndistance between every pair of terminals is preserved to within some small\nmultiplicative factor, that is called the stretch of $H$.\n  It has been shown that on general graphs we can achieve stretch $O(\\log |T|)$\n[Filtser, 2018]. On the other hand, the best-known stretch lower bound is $8$\n[Chan-Xia-Konjevod-Richa, 2006], which holds even for trees. In this work, we\nshow an improved lower bound of $\\tilde\\Omega\\big(\\sqrt{\\log |T|}\\big)$.",
            "author": [
                "Yu Chen",
                "Zihan Tan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07862v1",
                "http://arxiv.org/pdf/2310.07862v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07859v1",
            "title": "Interaction graph engineering in trapped-ion quantum simulators with\n  global drives",
            "updated": "2023-10-11T20:00:36Z",
            "published": "2023-10-11T20:00:36Z",
            "summary": "Trapped-ion quantum simulators have demonstrated a long history of studying\nthe physics of interacting spin-lattice systems using globally addressed\nentangling operations. Here, we seek to broaden and delimit the classes of\neffective spin-spin interactions achievable using exclusively global driving\nfields. We find that new categories of interaction graphs become achievable\nwith perfect or near-perfect theoretical fidelity by tailoring the coupling to\neach vibrational mode of the ion crystal, or by shaping the trapping potential\nto include specific anharmonic terms. We also derive a rigorous test to\ndetermine whether a desired interaction graph is accessible using only globally\ndriven fields. These tools broaden the reach of trapped-ion quantum simulators\nso that they may more easily address open questions in materials science and\nquantum chemistry.",
            "author": [
                "Antonis Kyprianidis",
                "A. J. Rasmusson",
                "Philip Richerme"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07859v1",
                "http://arxiv.org/pdf/2310.07859v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07857v1",
            "title": "On $(1+\\varepsilon)$-Approximate Flow Sparsifiers",
            "updated": "2023-10-11T19:59:44Z",
            "published": "2023-10-11T19:59:44Z",
            "summary": "Given a large graph $G$ with a subset $|T|=k$ of its vertices called\nterminals, a quality-$q$ flow sparsifier is a small graph $G'$ that contains\n$T$ and preserves all multicommodity flows that can be routed between terminals\nin $T$, to within factor $q$. The problem of constructing flow sparsifiers with\ngood (small) quality and (small) size has been a central problem in graph\ncompression for decades.\n  A natural approach of constructing $O(1)$-quality flow sparsifiers, which was\nadopted in most previous constructions, is contraction. Andoni, Krauthgamer,\nand Gupta constructed a sketch of size $f(k,\\varepsilon)$ that stores all\nfeasible multicommodity flows up to a factor of $(1+\\varepsilon)$, raised the\nquestion of constructing quality-$(1+\\varepsilon)$ flow sparsifiers whose size\nonly depends on $k,\\varepsilon$ (but not the number of vertices in the input\ngraph $G$), and proposed a contraction-based framework towards it using their\nsketch result.\n  In this paper, we settle their question for contraction-based flow\nsparsifiers, by showing that quality-$(1+\\varepsilon)$ contraction-based flow\nsparsifiers with size $f(\\varepsilon)$ exist for all $5$-terminal graphs, but\nnot for all $6$-terminal graphs. Our hardness result on $6$-terminal graphs\nimproves upon a recent hardness result by Krauthgamer and Mosenzon on exact\n(quality-$1$) flow sparsifiers, for contraction-based constructions. Our\nconstruction and proof utilize the notion of tight spans in metric geometry,\nwhich we believe is a powerful tool for future work.",
            "author": [
                "Yu Chen",
                "Zihan Tan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07857v1",
                "http://arxiv.org/pdf/2310.07857v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07848v1",
            "title": "Framework for Question-Answering in Sanskrit through Automated\n  Construction of Knowledge Graphs",
            "updated": "2023-10-11T19:50:59Z",
            "published": "2023-10-11T19:50:59Z",
            "summary": "Sanskrit (sa\\d{m}sk\\d{r}ta) enjoys one of the largest and most varied\nliterature in the whole world. Extracting the knowledge from it, however, is a\nchallenging task due to multiple reasons including complexity of the language\nand paucity of standard natural language processing tools. In this paper, we\ntarget the problem of building knowledge graphs for particular types of\nrelationships from sa\\d{m}sk\\d{r}ta texts. We build a natural language\nquestion-answering system in sa\\d{m}sk\\d{r}ta that uses the knowledge graph to\nanswer factoid questions. We design a framework for the overall system and\nimplement two separate instances of the system on human relationships from\nmah\\=abh\\=arata and r\\=am\\=aya\\d{n}a, and one instance on synonymous\nrelationships from bh\\=avaprak\\=a\\'sa nigha\\d{n}\\d{t}u, a technical text from\n\\=ayurveda. We show that about 50% of the factoid questions can be answered\ncorrectly by the system. More importantly, we analyse the shortcomings of the\nsystem in detail for each step, and discuss the possible ways forward.",
            "author": [
                "Hrishikesh Terdalkar",
                "Arnab Bhattacharya"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07848v1",
                "http://arxiv.org/pdf/2310.07848v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07846v1",
            "title": "DAG-aware Synthesis Orchestration",
            "updated": "2023-10-11T19:46:27Z",
            "published": "2023-10-11T19:46:27Z",
            "summary": "The key methodologies of modern logic synthesis techniques are conducted on\nmulti-level technology-independent representations such as And-Inverter-Graphs\n(AIGs) of the digital logic via directed-acyclic-graph (DAGs) traversal based\nstructural rewriting, resubstitution, and refactoring. Existing\nstate-of-the-art DAG-aware logic synthesis algorithms are all designed to\nperform stand-alone optimizations during a single DAG traversal. However, we\nempirically identify and demonstrate that these algorithms are limited in\nquality-of-results and runtime complexity due to this design concept. This work\nproposes Synthesis Orchestration, which orchestrates stand-alone operations\nwithin the single traversal of AIG. Thus, orchestration method explores more\noptimization opportunities and results in better performance. Our experimental\nresults are comprehensively conducted on all 104 designs collected from\nISCAS'85/89/99, VTR, and EPFL benchmark suites, with consistent logic\nminimization improvements over rewriting, resubstitution, refactoring, leading\nto an average of 4% more node reduction with improved runtime efficiency for\nthe single optimization. Moreover, we evaluate orchestration as a plug-in\nalgorithm in resyn and resyn3 flows in ABC, which demonstrates consistent logic\nminimization improvements (3.8% and 10.9% more node reduction on average). The\nruntime analysis demonstrates the orchestration outperforms stand-alone\nalgorithms in both AIG minimization and runtime efficiency. Finally, we\nintegrate the orchestration into OpenROAD for end-to-end performance\nevaluation. Our results demonstrate the advantages of the orchestration\noptimization technique, even after technology mapping and post-routing in the\ndesign flow have been conducted.",
            "author": [
                "Yingjie Li",
                "Mingju Liu",
                "Mark Ren",
                "Alan Mishchenko",
                "Cunxi Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07846v1",
                "http://arxiv.org/pdf/2310.07846v1"
            ],
            "primary_category": "cs.AR",
            "category": [
                "cs.AR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07838v4",
            "title": "Towards the Fundamental Limits of Knowledge Transfer over Finite Domains",
            "updated": "2023-11-14T11:26:10Z",
            "published": "2023-10-11T19:30:08Z",
            "summary": "We characterize the statistical efficiency of knowledge transfer through $n$\nsamples from a teacher to a probabilistic student classifier with input space\n$\\mathcal S$ over labels $\\mathcal A$. We show that privileged information at\nthree progressive levels accelerates the transfer. At the first level, only\nsamples with hard labels are known, via which the maximum likelihood estimator\nattains the minimax rate $\\sqrt{{|{\\mathcal S}||{\\mathcal A}|}/{n}}$. The\nsecond level has the teacher probabilities of sampled labels available in\naddition, which turns out to boost the convergence rate lower bound to\n${{|{\\mathcal S}||{\\mathcal A}|}/{n}}$. However, under this second data\nacquisition protocol, minimizing a naive adaptation of the cross-entropy loss\nresults in an asymptotically biased student. We overcome this limitation and\nachieve the fundamental limit by using a novel empirical variant of the squared\nerror logit loss. The third level further equips the student with the soft\nlabels (complete logits) on ${\\mathcal A}$ given every sampled input, thereby\nprovably enables the student to enjoy a rate ${|{\\mathcal S}|}/{n}$ free of\n$|{\\mathcal A}|$. We find any Kullback-Leibler divergence minimizer to be\noptimal in the last case. Numerical simulations distinguish the four learners\nand corroborate our theory.",
            "author": [
                "Qingyue Zhao",
                "Banghua Zhu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07838v4",
                "http://arxiv.org/pdf/2310.07838v4"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.IT",
                "math.IT",
                "math.ST",
                "stat.ML",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07793v2",
            "title": "GenTKG: Generative Forecasting on Temporal Knowledge Graph",
            "updated": "2023-11-14T15:51:18Z",
            "published": "2023-10-11T18:27:12Z",
            "summary": "The rapid advancements in large language models (LLMs) have ignited interest\nin the temporal knowledge graph (tKG) domain, where conventional carefully\ndesigned embedding-based and rule-based models dominate. The question remains\nopen of whether pre-trained LLMs can understand structured temporal relational\ndata and replace them as the foundation model for temporal relational\nforecasting. Therefore, we bring temporal knowledge forecasting into the\ngenerative setting. However, challenges occur in the huge chasms between\ncomplex temporal graph data structure and sequential natural expressions LLMs\ncan handle, and between the enormous data sizes of tKGs and heavy computation\ncosts of finetuning LLMs. To address these challenges, we propose a novel\nretrieval augmented generation framework that performs generative forecasting\non tKGs named GenTKG, which combines a temporal logical rule-based retrieval\nstrategy and lightweight parameter-efficient instruction tuning. Extensive\nexperiments have shown that GenTKG outperforms conventional methods of temporal\nrelational forecasting under low computation resources. GenTKG also highlights\nremarkable transferability with exceeding performance on unseen datasets\nwithout re-training. Our work reveals the huge potential of LLMs in the tKG\ndomain and opens a new frontier for generative forecasting on tKGs.",
            "author": [
                "Ruotong Liao",
                "Xu Jia",
                "Yunpu Ma",
                "Volker Tresp"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07793v2",
                "http://arxiv.org/pdf/2310.07793v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07792v1",
            "title": "Exploiting Semantic Localization in Highly Dynamic Wireless Networks\n  Using Deep Homoscedastic Domain Adaptation",
            "updated": "2023-10-11T18:24:21Z",
            "published": "2023-10-11T18:24:21Z",
            "summary": "Localization in GPS-denied outdoor locations, such as street canyons in an\nurban or metropolitan environment, has many applications. Machine Learning (ML)\nis widely used to tackle this critical problem. One challenge lies in the\nmixture of line-of-sight (LOS), obstructed LOS (OLOS), and non-LOS (NLOS)\nconditions. In this paper, we consider a semantic localization that treats\nthese three propagation conditions as the ''semantic objects\", and aims to\ndetermine them together with the actual localization, and show that this\nincreases accuracy and robustness. Furthermore, the propagation conditions are\nhighly dynamic, since obstruction by cars or trucks can change the channel\nstate information (CSI) at a fixed location over time. We therefore consider\nthe blockage by such dynamic objects as another semantic state. Based on these\nconsiderations, we formulate the semantic localization with a joint task\n(coordinates regression and semantics classification) learning problem. Another\nproblem created by the dynamics is the fact that each location may be\ncharacterized by a number of different CSIs. To avoid the need for excessive\namount of labeled training data, we propose a multi-task deep domain adaptation\n(DA) based localization technique, training neural networks with a limited\nnumber of labeled samples and numerous unlabeled ones. Besides, we introduce\nnovel scenario adaptive learning strategies to ensure efficient representation\nlearning and successful knowledge transfer. Finally, we use Bayesian theory for\nuncertainty modeling of the importance weights in each task, reducing the need\nfor time-consuming parameter finetuning; furthermore, with some mild\nassumptions, we derive the related log-likelihood for the joint task and\npresent the deep homoscedastic DA based localization method.",
            "author": [
                "Lei Chu",
                "Abdullah Alghafis",
                "Andreas F. Molisch"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07792v1",
                "http://arxiv.org/pdf/2310.07792v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07704v1",
            "title": "Ferret: Refer and Ground Anything Anywhere at Any Granularity",
            "updated": "2023-10-11T17:55:15Z",
            "published": "2023-10-11T17:55:15Z",
            "summary": "We introduce Ferret, a new Multimodal Large Language Model (MLLM) capable of\nunderstanding spatial referring of any shape or granularity within an image and\naccurately grounding open-vocabulary descriptions. To unify referring and\ngrounding in the LLM paradigm, Ferret employs a novel and powerful hybrid\nregion representation that integrates discrete coordinates and continuous\nfeatures jointly to represent a region in the image. To extract the continuous\nfeatures of versatile regions, we propose a spatial-aware visual sampler, adept\nat handling varying sparsity across different shapes. Consequently, Ferret can\naccept diverse region inputs, such as points, bounding boxes, and free-form\nshapes. To bolster the desired capability of Ferret, we curate GRIT, a\ncomprehensive refer-and-ground instruction tuning dataset including 1.1M\nsamples that contain rich hierarchical spatial knowledge, with 95K hard\nnegative data to promote model robustness. The resulting model not only\nachieves superior performance in classical referring and grounding tasks, but\nalso greatly outperforms existing MLLMs in region-based and\nlocalization-demanded multimodal chatting. Our evaluations also reveal a\nsignificantly improved capability of describing image details and a remarkable\nalleviation in object hallucination. Code and data will be available at\nhttps://github.com/apple/ml-ferret",
            "author": [
                "Haoxuan You",
                "Haotian Zhang",
                "Zhe Gan",
                "Xianzhi Du",
                "Bowen Zhang",
                "Zirui Wang",
                "Liangliang Cao",
                "Shih-Fu Chang",
                "Yinfei Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07704v1",
                "http://arxiv.org/pdf/2310.07704v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07700v1",
            "title": "Knowledge-enhanced Memory Model for Emotional Support Conversation",
            "updated": "2023-10-11T17:51:28Z",
            "published": "2023-10-11T17:51:28Z",
            "summary": "The prevalence of mental disorders has become a significant issue, leading to\nthe increased focus on Emotional Support Conversation as an effective\nsupplement for mental health support. Existing methods have achieved compelling\nresults, however, they still face three challenges: 1) variability of emotions,\n2) practicality of the response, and 3) intricate strategy modeling. To address\nthese challenges, we propose a novel knowledge-enhanced Memory mODEl for\nemotional suppoRt coNversation (MODERN). Specifically, we first devise a\nknowledge-enriched dialogue context encoding to perceive the dynamic emotion\nchange of different periods of the conversation for coherent user state\nmodeling and select context-related concepts from ConceptNet for practical\nresponse generation. Thereafter, we implement a novel memory-enhanced strategy\nmodeling module to model the semantic patterns behind the strategy categories.\nExtensive experiments on a widely used large-scale dataset verify the\nsuperiority of our model over cutting-edge baselines.",
            "author": [
                "Mengzhao Jia",
                "Qianglong Chen",
                "Liqiang Jing",
                "Dawei Fu",
                "Renyu Li"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07700v1",
                "http://arxiv.org/pdf/2310.07700v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07690v1",
            "title": "Large-scale photonic computing with nonlinear disordered media",
            "updated": "2023-10-11T17:37:20Z",
            "published": "2023-10-11T17:37:20Z",
            "summary": "Neural networks find widespread use in scientific and technological\napplications, yet their implementations in conventional computers have\nencountered bottlenecks due to ever-expanding computational needs. Photonic\nneuromorphic hardware, which manipulates information and represents data\ncontinuously in the optical domain, is one of the promising platforms with\npotential advantages of massive parallelism, ultralow latency, and reduced\nenergy consumption. While linear photonic neural networks are within reach,\nphotonic computing with large-scale optical nonlinear nodes remains largely\nunexplored. Here, we demonstrate a large-scale, high-performance nonlinear\nphotonic neural system based on a disordered polycrystalline slab composed of\nlithium niobate nanocrystals. Mediated by random quasi-phase-matching and\nmultiple scattering, linear and nonlinear optical speckle features are\ngenerated as the interplay between the simultaneous linear random scattering\nand the second-harmonic generation, defining a complex neural network in which\nthe second-order nonlinearity acts as internal nonlinear activation functions.\nBenchmarked against linear random projection, such nonlinear mapping embedded\nwith rich physical computational operations shows improved performance across a\nlarge collection of machine learning tasks in image classification, regression,\nand graph classification with varying complexity. Demonstrating up to 27,648\ninput and 3,500 nonlinear output nodes, the combination of optical nonlinearity\nand random scattering serves as a scalable computing engine for diverse\napplications.",
            "author": [
                "Hao Wang",
                "Jianqi Hu",
                "Andrea Morandi",
                "Alfonso Nardi",
                "Fei Xia",
                "Xuanchen Li",
                "Romolo Savo",
                "Qiang Liu",
                "Rachel Grange",
                "Sylvain Gigan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07690v1",
                "http://arxiv.org/pdf/2310.07690v1"
            ],
            "primary_category": "physics.optics",
            "category": [
                "physics.optics",
                "cs.ET"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07684v1",
            "title": "Hypergraph Neural Networks through the Lens of Message Passing: A Common\n  Perspective to Homophily and Architecture Design",
            "updated": "2023-10-11T17:35:20Z",
            "published": "2023-10-11T17:35:20Z",
            "summary": "Most of the current hypergraph learning methodologies and benchmarking\ndatasets in the hypergraph realm are obtained by lifting procedures from their\ngraph analogs, simultaneously leading to overshadowing hypergraph network\nfoundations. This paper attempts to confront some pending questions in that\nregard: Can the concept of homophily play a crucial role in Hypergraph Neural\nNetworks (HGNNs), similar to its significance in graph-based research? Is there\nroom for improving current hypergraph architectures and methodologies? (e.g. by\ncarefully addressing the specific characteristics of higher-order networks) Do\nexisting datasets provide a meaningful benchmark for HGNNs? Diving into the\ndetails, this paper proposes a novel conceptualization of homophily in\nhigher-order networks based on a message passing scheme; this approach\nharmonizes the analytical frameworks of datasets and architectures, offering a\nunified perspective for exploring and interpreting complex, higher-order\nnetwork structures and dynamics. Further, we propose MultiSet, a novel message\npassing framework that redefines HGNNs by allowing hyperedge-dependent node\nrepresentations, as well as introduce a novel architecture MultiSetMixer that\nleverages a new hyperedge sampling strategy. Finally, we provide an extensive\nset of experiments that contextualize our proposals and lead to valuable\ninsights in hypergraph representation learning.",
            "author": [
                "Lev Telyatnikov",
                "Maria Sofia Bucarelli",
                "Guillermo Bernardez",
                "Olga Zaghen",
                "Simone Scardapane",
                "Pietro Lio"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07684v1",
                "http://arxiv.org/pdf/2310.07684v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07668v1",
            "title": "GRaMuFeN: Graph-based Multi-modal Fake News Detection in Social Media",
            "updated": "2023-10-11T17:17:40Z",
            "published": "2023-10-11T17:17:40Z",
            "summary": "The proliferation of social media platforms such as Twitter, Instagram, and\nWeibo has significantly enhanced the dissemination of false information. This\nphenomenon grants both individuals and governmental entities the ability to\nshape public opinions, highlighting the need for deploying effective detection\nmethods. In this paper, we propose GraMuFeN, a model designed to detect fake\ncontent by analyzing both the textual and image content of news. GraMuFeN\ncomprises two primary components: a text encoder and an image encoder. For\ntextual analysis, GraMuFeN treats each text as a graph and employs a Graph\nConvolutional Neural Network (GCN) as the text encoder. Additionally, the\npre-trained ResNet-152, as a Convolutional Neural Network (CNN), has been\nutilized as the image encoder. By integrating the outputs from these two\nencoders and implementing a contrastive similarity loss function, GraMuFeN\nachieves remarkable results. Extensive evaluations conducted on two publicly\navailable benchmark datasets for social media news indicate a 10 % increase in\nmicro F1-Score, signifying improvement over existing state-of-the-art models.\nThese findings underscore the effectiveness of combining GCN and CNN models for\ndetecting fake news in multi-modal data, all while minimizing the additional\ncomputational burden imposed by model parameters.",
            "author": [
                "Makan Kananian",
                "Fatima Badiei",
                "S. AmirAli Gh. Ghahramani"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07668v1",
                "http://arxiv.org/pdf/2310.07668v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07667v1",
            "title": "Global Minima, Recoverability Thresholds, and Higher-Order Structure in\n  GNNS",
            "updated": "2023-10-11T17:16:33Z",
            "published": "2023-10-11T17:16:33Z",
            "summary": "We analyze the performance of graph neural network (GNN) architectures from\nthe perspective of random graph theory. Our approach promises to complement\nexisting lenses on GNN analysis, such as combinatorial expressive power and\nworst-case adversarial analysis, by connecting the performance of GNNs to\ntypical-case properties of the training data. First, we theoretically\ncharacterize the nodewise accuracy of one- and two-layer GCNs relative to the\ncontextual stochastic block model (cSBM) and related models. We additionally\nprove that GCNs cannot beat linear models under certain circumstances. Second,\nwe numerically map the recoverability thresholds, in terms of accuracy, of four\ndiverse GNN architectures (GCN, GAT, SAGE, and Graph Transformer) under a\nvariety of assumptions about the data. Sample results of this second analysis\ninclude: heavy-tailed degree distributions enhance GNN performance, GNNs can\nwork well on strongly heterophilous graphs, and SAGE and Graph Transformer can\nperform well on arbitrarily noisy edge data, but no architecture handled\nsufficiently noisy feature data well. Finally, we show how both specific\nhigher-order structures in synthetic data and the mix of empirical structures\nin real data have dramatic effects (usually negative) on GNN performance.",
            "author": [
                "Drake Brown",
                "Trevor Garrity",
                "Kaden Parker",
                "Jason Oliphant",
                "Stone Carson",
                "Cole Hanson",
                "Zachary Boyd"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07667v1",
                "http://arxiv.org/pdf/2310.07667v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07663v1",
            "title": "Deep Video Inpainting Guided by Audio-Visual Self-Supervision",
            "updated": "2023-10-11T17:03:21Z",
            "published": "2023-10-11T17:03:21Z",
            "summary": "Humans can easily imagine a scene from auditory information based on their\nprior knowledge of audio-visual events. In this paper, we mimic this innate\nhuman ability in deep learning models to improve the quality of video\ninpainting. To implement the prior knowledge, we first train the audio-visual\nnetwork, which learns the correspondence between auditory and visual\ninformation. Then, the audio-visual network is employed as a guider that\nconveys the prior knowledge of audio-visual correspondence to the video\ninpainting network. This prior knowledge is transferred through our proposed\ntwo novel losses: audio-visual attention loss and audio-visual pseudo-class\nconsistency loss. These two losses further improve the performance of the video\ninpainting by encouraging the inpainting result to have a high correspondence\nto its synchronized audio. Experimental results demonstrate that our proposed\nmethod can restore a wider domain of video scenes and is particularly effective\nwhen the sounding object in the scene is partially blinded.",
            "author": [
                "Kyuyeon Kim",
                "Junsik Jung",
                "Woo Jae Kim",
                "Sung-Eui Yoon"
            ],
            "link": [
                "http://dx.doi.org/10.1109/ICASSP43922.2022.9747073",
                "http://arxiv.org/abs/2310.07663v1",
                "http://arxiv.org/pdf/2310.07663v1"
            ],
            "primary_category": "eess.AS",
            "category": [
                "eess.AS",
                "cs.CV",
                "cs.SD"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07659v3",
            "title": "Well Begun is Half Done: Generator-agnostic Knowledge Pre-Selection for\n  Knowledge-Grounded Dialogue",
            "updated": "2023-10-20T13:43:35Z",
            "published": "2023-10-11T17:00:29Z",
            "summary": "Accurate knowledge selection is critical in knowledge-grounded dialogue\nsystems. Towards a closer look at it, we offer a novel perspective to organize\nexisting literature, i.e., knowledge selection coupled with, after, and before\ngeneration. We focus on the third under-explored category of study, which can\nnot only select knowledge accurately in advance, but has the advantage to\nreduce the learning, adjustment, and interpretation burden of subsequent\nresponse generation models, especially LLMs. We propose GATE, a\ngenerator-agnostic knowledge selection method, to prepare knowledge for\nsubsequent response generation models by selecting context-related knowledge\namong different knowledge structures and variable knowledge requirements.\nExperimental results demonstrate the superiority of GATE, and indicate that\nknowledge selection before generation is a lightweight yet effective way to\nfacilitate LLMs (e.g., ChatGPT) to generate more informative responses.",
            "author": [
                "Lang Qin",
                "Yao Zhang",
                "Hongru Liang",
                "Jun Wang",
                "Zhenglu Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07659v3",
                "http://arxiv.org/pdf/2310.07659v3"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07644v2",
            "title": "Rethinking the BERT-like Pretraining for DNA Sequences",
            "updated": "2023-10-12T03:32:32Z",
            "published": "2023-10-11T16:40:57Z",
            "summary": "With the success of large-scale pretraining in NLP, there is an increasing\ntrend of applying it to the domain of life sciences. In particular, pretraining\nmethods based on DNA sequences have garnered growing attention due to their\npotential to capture generic information about genes. However, existing\npretraining methods for DNA sequences largely rely on direct adoptions of BERT\npretraining from NLP, lacking a comprehensive understanding and a specifically\ntailored approach. To address this research gap, we first conducted a series of\nexploratory experiments and gained several insightful observations: 1) In the\nfine-tuning phase of downstream tasks, when using K-mer overlapping\ntokenization instead of K-mer non-overlapping tokenization, both overlapping\nand non-overlapping pretraining weights show consistent performance\nimprovement.2) During the pre-training process, using K-mer overlapping\ntokenization quickly produces clear K-mer embeddings and reduces the loss to a\nvery low level, while using K-mer non-overlapping tokenization results in less\ndistinct embeddings and continuously decreases the loss. 3) Using overlapping\ntokenization causes the self-attention in the intermediate layers of\npre-trained models to tend to overly focus on certain tokens, reflecting that\nthese layers are not adequately optimized. In summary, overlapping tokenization\ncan benefit the fine-tuning of downstream tasks but leads to inadequate\npretraining with fast convergence. To unleash the pretraining potential, we\nintroduce a novel approach called RandomMask, which gradually increases the\ntask difficulty of BERT-like pretraining by continuously expanding its mask\nboundary, forcing the model to learn more knowledge. RandomMask is simple but\neffective, achieving top-tier performance across 26 datasets of 28 datasets\nspanning 7 downstream tasks.",
            "author": [
                "Chaoqi Liang",
                "Weiqiang Bai",
                "Lifeng Qiao",
                "Yuchen Ren",
                "Jianle Sun",
                "Peng Ye",
                "Hongliang Yan",
                "Xinzhu Ma",
                "Wangmeng Zuo",
                "Wanli Ouyang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07644v2",
                "http://arxiv.org/pdf/2310.07644v2"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07638v1",
            "title": "Context-Enhanced Detector For Building Detection From Remote Sensing\n  Images",
            "updated": "2023-10-11T16:33:30Z",
            "published": "2023-10-11T16:33:30Z",
            "summary": "The field of building detection from remote sensing images has made\nsignificant progress, but faces challenges in achieving high-accuracy detection\ndue to the diversity in building appearances and the complexity of vast scenes.\nTo address these challenges, we propose a novel approach called\nContext-Enhanced Detector (CEDet). Our approach utilizes a three-stage cascade\nstructure to enhance the extraction of contextual information and improve\nbuilding detection accuracy. Specifically, we introduce two modules: the\nSemantic Guided Contextual Mining (SGCM) module, which aggregates multi-scale\ncontexts and incorporates an attention mechanism to capture long-range\ninteractions, and the Instance Context Mining Module (ICMM), which captures\ninstance-level relationship context by constructing a spatial relationship\ngraph and aggregating instance features. Additionally, we introduce a semantic\nsegmentation loss based on pseudo-masks to guide contextual information\nextraction. Our method achieves state-of-the-art performance on three building\ndetection benchmarks, including CNBuilding-9P, CNBuilding-23P, and SpaceNet.",
            "author": [
                "Ziyue Huang",
                "Mingming Zhang",
                "Qingjie Liu",
                "Wei Wang",
                "Zhe Dong",
                "Yunhong Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07638v1",
                "http://arxiv.org/pdf/2310.07638v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07637v2",
            "title": "OpsEval: A Comprehensive Task-Oriented AIOps Benchmark for Large\n  Language Models",
            "updated": "2023-10-12T01:53:03Z",
            "published": "2023-10-11T16:33:29Z",
            "summary": "Large language models (LLMs) have exhibited remarkable capabilities in\nNLP-related tasks such as translation, summarizing, and generation. The\napplication of LLMs in specific areas, notably AIOps (Artificial Intelligence\nfor IT Operations), holds great potential due to their advanced abilities in\ninformation summarizing, report analyzing, and ability of API calling.\nNevertheless, the performance of current LLMs in AIOps tasks is yet to be\ndetermined. Furthermore, a comprehensive benchmark is required to steer the\noptimization of LLMs tailored for AIOps. Compared with existing benchmarks that\nfocus on evaluating specific fields like network configuration, in this paper,\nwe present \\textbf{OpsEval}, a comprehensive task-oriented AIOps benchmark\ndesigned for LLMs. For the first time, OpsEval assesses LLMs' proficiency in\nthree crucial scenarios (Wired Network Operation, 5G Communication Operation,\nand Database Operation) at various ability levels (knowledge recall, analytical\nthinking, and practical application). The benchmark includes 7,200 questions in\nboth multiple-choice and question-answer (QA) formats, available in English and\nChinese. With quantitative and qualitative results, we show how various LLM\ntricks can affect the performance of AIOps, including zero-shot,\nchain-of-thought, and few-shot in-context learning. We find that GPT4-score is\nmore consistent with experts than widely used Bleu and Rouge, which can be used\nto replace automatic metrics for large-scale qualitative evaluations.",
            "author": [
                "Yuhe Liu",
                "Changhua Pei",
                "Longlong Xu",
                "Bohan Chen",
                "Mingze Sun",
                "Zhirui Zhang",
                "Yongqian Sun",
                "Shenglin Zhang",
                "Kun Wang",
                "Haiming Zhang",
                "Jianhui Li",
                "Gaogang Xie",
                "Xidao Wen",
                "Xiaohui Nie",
                "Dan Pei"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07637v2",
                "http://arxiv.org/pdf/2310.07637v2"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.NI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07631v1",
            "title": "Graph Transformer Network for Flood Forecasting with Heterogeneous\n  Covariates",
            "updated": "2023-10-11T16:24:06Z",
            "published": "2023-10-11T16:24:06Z",
            "summary": "Floods can be very destructive causing heavy damage to life, property, and\nlivelihoods. Global climate change and the consequent sea-level rise have\nincreased the occurrence of extreme weather events, resulting in elevated and\nfrequent flood risk. Therefore, accurate and timely flood forecasting in\ncoastal river systems is critical to facilitate good flood management. However,\nthe computational tools currently used are either slow or inaccurate. In this\npaper, we propose a Flood prediction tool using Graph Transformer Network\n(FloodGTN) for river systems. More specifically, FloodGTN learns the\nspatio-temporal dependencies of water levels at different monitoring stations\nusing Graph Neural Networks (GNNs) and an LSTM. It is currently implemented to\nconsider external covariates such as rainfall, tide, and the settings of\nhydraulic structures (e.g., outflows of dams, gates, pumps, etc.) along the\nriver. We use a Transformer to learn the attention given to external covariates\nin computing water levels. We apply the FloodGTN tool to data from the South\nFlorida Water Management District, which manages a coastal area prone to\nfrequent storms and hurricanes. Experimental results show that FloodGTN\noutperforms the physics-based model (HEC-RAS) by achieving higher accuracy with\n70% improvement while speeding up run times by at least 500x.",
            "author": [
                "Jimeng Shi",
                "Vitalii Stebliankin",
                "Zhaonan Wang",
                "Shaowen Wang",
                "Giri Narasimhan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07631v1",
                "http://arxiv.org/pdf/2310.07631v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07630v1",
            "title": "Differentiable Euler Characteristic Transforms for Shape Classification",
            "updated": "2023-10-11T16:23:07Z",
            "published": "2023-10-11T16:23:07Z",
            "summary": "The Euler Characteristic Transform (ECT) has proven to be a powerful\nrepresentation, combining geometrical and topological characteristics of shapes\nand graphs. However, the ECT was hitherto unable to learn task-specific\nrepresentations. We overcome this issue and develop a novel computational layer\nthat enables learning the ECT in an end-to-end fashion. Our method DECT is fast\nand computationally efficient, while exhibiting performance on a par with more\ncomplex models in both graph and point cloud classification tasks. Moreover, we\nshow that this seemingly unexpressive statistic still provides the same\ntopological expressivity as more complex topological deep learning layers\nprovide.",
            "author": [
                "Ernst Roell",
                "Bastian Rieck"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07630v1",
                "http://arxiv.org/pdf/2310.07630v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07625v1",
            "title": "Cybersecurity as a Crosscutting Concept Across an Undergrad Computer\n  Science Curriculum: An Experience Report",
            "updated": "2023-10-11T16:07:42Z",
            "published": "2023-10-11T16:07:42Z",
            "summary": "Although many Computer Science (CS) programs offer cybersecurity courses,\nthey are typically optional and placed at the periphery of the program. We\nadvocate to integrate cybersecurity as a crosscutting concept in CS curricula,\nwhich is also consistent with latest cybersecurity curricular guidelines, e.g.,\nCSEC2017. We describe our experience of implementing this crosscutting\nintervention across three undergraduate core CS courses at a leading technical\nuniversity in Europe between 2018 and 2023, collectively educating over 2200\nstudents. The security education was incorporated within CS courses using a\npartnership between the responsible course instructor and a security expert,\ni.e., the security expert (after consultation with course instructors)\ndeveloped and taught lectures covering multiple CSEC2017 knowledge areas. This\ncreated a complex dynamic between three stakeholders: the course instructor,\nthe security expert, and the students. We reflect on our intervention from the\nperspective of the three stakeholders -- we conducted a post-course survey to\ncollect student perceptions, and semi-supervised interviews with responsible\ncourse instructors and the security expert to gauge their experience. We found\nthat while the students were extremely enthusiastic about the security content\nand retained its impact several years later, the misaligned incentives for the\ninstructors and the security expert made it difficult to sustain this\nintervention without organizational support. By identifying limitations in our\nintervention, we suggest ideas for sustaining it.",
            "author": [
                "Azqa Nadeem"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07625v1",
                "http://arxiv.org/pdf/2310.07625v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07621v1",
            "title": "AG-CVG: Coverage Planning with a Mobile Recharging UGV and an\n  Energy-Constrained UAV",
            "updated": "2023-10-11T16:04:02Z",
            "published": "2023-10-11T16:04:02Z",
            "summary": "In this paper, we present an approach for coverage path planning for a team\nof an energy-constrained Unmanned Aerial Vehicle (UAV) and an Unmanned Ground\nVehicle (UGV). Both the UAV and the UGV have predefined areas that they have to\ncover. The goal is to perform complete coverage by both robots while minimizing\nthe coverage time. The UGV can also serve as a mobile recharging station. The\nUAV and UGV need to occasionally rendezvous for recharging. We propose a\nheuristic method to address this NP-Hard planning problem. Our approach\ninvolves initially determining coverage paths without factoring in energy\nconstraints. Subsequently, we cluster segments of these paths and employ graph\nmatching to assign UAV clusters to UGV clusters for efficient recharging\nmanagement. We perform numerical analysis on real-world coverage applications\nand show that compared with a greedy approach our method reduces rendezvous\noverhead on average by 11.33\\%. We demonstrate proof-of-concept with a team of\na VOXL m500 drone and a Clearpath Jackal ground vehicle, providing a complete\nsystem from the offline algorithm to the field execution.",
            "author": [
                "Nare Karapetyan",
                "Ahmad Bilal Asghar",
                "Amisha Bhaskar",
                "Guangyao Shi",
                "Dinesh Manocha",
                "Pratap Tokekar"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07621v1",
                "http://arxiv.org/pdf/2310.07621v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07619v1",
            "title": "Latent Su-Schrieffer-Heeger models",
            "updated": "2023-10-11T16:00:21Z",
            "published": "2023-10-11T16:00:21Z",
            "summary": "The Su-Schrieffer-Heeger (SSH) chain is the reference model of a\none-dimensional topological insulator. Its topological nature can be explained\nby the quantization of the Zak phase, due to reflection symmetry of the unit\ncell, or of the winding number, due to chiral symmetry. Here, we harness recent\ngraph-theoretical results to construct families of setups whose unit cell\nfeatures neither of these symmetries, but instead a so-called latent or hidden\nreflection symmetry. This causes the isospectral reduction -- akin to an\neffective Hamiltonian -- of the resulting lattice to have the form of an SSH\nmodel. As we show, these latent SSH models exhibit features such as multiple\ntopological transitions and edge states, as well as a quantized Zak phase.\nRelying on a generally applicable discrete framework, we experimentally\nvalidate our findings using electric circuits.",
            "author": [
                "Malte R\u00f6ntgen",
                "Xuelong Chen",
                "Wenlong Gao",
                "Maxim Pyzh",
                "Peter Schmelcher",
                "Vincent Pagneux",
                "Vassos Achilleos",
                "Antonin Coutant"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07619v1",
                "http://arxiv.org/pdf/2310.07619v1"
            ],
            "primary_category": "cond-mat.mes-hall",
            "category": [
                "cond-mat.mes-hall",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07614v1",
            "title": "Automorphisms of the Rado meet-tree",
            "updated": "2023-10-11T15:58:53Z",
            "published": "2023-10-11T15:58:53Z",
            "summary": "We prove that the group of automorphisms of the generic meet-tree expansion\nof an infinite non-unary free Fra\\\"{\\i}ss\\'{e} limit over a finite relational\nlanguage is simple. As a prototypical case, the group of automorphism of the\nRado meet-tree (i.e. the Fra\\\"{\\i}ss\\'{e} limit of finite graphs which are also\nmeet-trees) is simple.",
            "author": [
                "Itay Kaplan",
                "Binyamin Riahi",
                "Arturo Rodriguez Fanlo"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07614v1",
                "http://arxiv.org/pdf/2310.07614v1"
            ],
            "primary_category": "math.LO",
            "category": [
                "math.LO",
                "math.GR",
                "03C98, 03C15, 20B27"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07613v1",
            "title": "Reinforcement Learning-based Knowledge Graph Reasoning for Explainable\n  Fact-checking",
            "updated": "2023-10-11T15:58:31Z",
            "published": "2023-10-11T15:58:31Z",
            "summary": "Fact-checking is a crucial task as it ensures the prevention of\nmisinformation. However, manual fact-checking cannot keep up with the rate at\nwhich false information is generated and disseminated online. Automated\nfact-checking by machines is significantly quicker than by humans. But for\nbetter trust and transparency of these automated systems, explainability in the\nfact-checking process is necessary. Fact-checking often entails contrasting a\nfactual assertion with a body of knowledge for such explanations. An effective\nway of representing knowledge is the Knowledge Graph (KG). There have been\nsufficient works proposed related to fact-checking with the usage of KG but not\nmuch focus is given to the application of reinforcement learning (RL) in such\ncases. To mitigate this gap, we propose an RL-based KG reasoning approach for\nexplainable fact-checking. Extensive experiments on FB15K-277 and NELL-995\ndatasets reveal that reasoning over a KG is an effective way of producing\nhuman-readable explanations in the form of paths and classifications for fact\nclaims. The RL reasoning agent computes a path that either proves or disproves\na factual claim, but does not provide a verdict itself. A verdict is reached by\na voting mechanism that utilizes paths produced by the agent. These paths can\nbe presented to human readers so that they themselves can decide whether or not\nthe provided evidence is convincing or not. This work will encourage works in\nthis direction for incorporating RL for explainable fact-checking as it\nincreases trustworthiness by providing a human-in-the-loop approach.",
            "author": [
                "Gustav Nikopensius",
                "Mohit Mayank",
                "Orchid Chetia Phukan",
                "Rajesh Sharma"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07613v1",
                "http://arxiv.org/pdf/2310.07613v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07588v1",
            "title": "Accurate Use of Label Dependency in Multi-Label Text Classification\n  Through the Lens of Causality",
            "updated": "2023-10-11T15:28:44Z",
            "published": "2023-10-11T15:28:44Z",
            "summary": "Multi-Label Text Classification (MLTC) aims to assign the most relevant\nlabels to each given text. Existing methods demonstrate that label dependency\ncan help to improve the model's performance. However, the introduction of label\ndependency may cause the model to suffer from unwanted prediction bias. In this\nstudy, we attribute the bias to the model's misuse of label dependency, i.e.,\nthe model tends to utilize the correlation shortcut in label dependency rather\nthan fusing text information and label dependency for prediction. Motivated by\ncausal inference, we propose a CounterFactual Text Classifier (CFTC) to\neliminate the correlation bias, and make causality-based predictions.\nSpecifically, our CFTC first adopts the predict-then-modify backbone to extract\nprecise label information embedded in label dependency, then blocks the\ncorrelation shortcut through the counterfactual de-bias technique with the help\nof the human causal graph. Experimental results on three datasets demonstrate\nthat our CFTC significantly outperforms the baselines and effectively\neliminates the correlation bias in datasets.",
            "author": [
                "Caoyun Fan",
                "Wenqing Chen",
                "Jidong Tian",
                "Yitian Li",
                "Hao He",
                "Yaohui Jin"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07588v1",
                "http://arxiv.org/pdf/2310.07588v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07585v1",
            "title": "A Discrepancy Aware Framework for Robust Anomaly Detection",
            "updated": "2023-10-11T15:21:40Z",
            "published": "2023-10-11T15:21:40Z",
            "summary": "Defect detection is a critical research area in artificial intelligence.\nRecently, synthetic data-based self-supervised learning has shown great\npotential on this task. Although many sophisticated synthesizing strategies\nexist, little research has been done to investigate the robustness of models\nwhen faced with different strategies. In this paper, we focus on this issue and\nfind that existing methods are highly sensitive to them. To alleviate this\nissue, we present a Discrepancy Aware Framework (DAF), which demonstrates\nrobust performance consistently with simple and cheap strategies across\ndifferent anomaly detection benchmarks. We hypothesize that the high\nsensitivity to synthetic data of existing self-supervised methods arises from\ntheir heavy reliance on the visual appearance of synthetic data during\ndecoding. In contrast, our method leverages an appearance-agnostic cue to guide\nthe decoder in identifying defects, thereby alleviating its reliance on\nsynthetic appearance. To this end, inspired by existing knowledge distillation\nmethods, we employ a teacher-student network, which is trained based on\nsynthesized outliers, to compute the discrepancy map as the cue. Extensive\nexperiments on two challenging datasets prove the robustness of our method.\nUnder the simple synthesis strategies, it outperforms existing methods by a\nlarge margin. Furthermore, it also achieves the state-of-the-art localization\nperformance. Code is available at: https://github.com/caiyuxuan1120/DAF.",
            "author": [
                "Yuxuan Cai",
                "Dingkang Liang",
                "Dongliang Luo",
                "Xinwei He",
                "Xin Yang",
                "Xiang Bai"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07585v1",
                "http://arxiv.org/pdf/2310.07585v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07584v1",
            "title": "Centrality of the Fingerprint Core Location",
            "updated": "2023-10-11T15:20:44Z",
            "published": "2023-10-11T15:20:44Z",
            "summary": "Fingerprints have long been recognized as a unique and reliable means of\npersonal identification. Central to the analysis and enhancement of\nfingerprints is the concept of the fingerprint core. Although the location of\nthe core is used in many applications, to the best of our knowledge, this study\nis the first to investigate the empirical distribution of the core over a\nlarge, combined dataset of rolled, as well as plain fingerprint recordings. We\nidentify and investigate the extent of incomplete rolling during the rolled\nfingerprint acquisition and investigate the centrality of the core. After\ncorrecting for the incomplete rolling, we find that the core deviates from the\nfingerprint center by 5.7% $\\pm$ 5.2% to 7.6% $\\pm$ 6.9%, depending on the\nfinger. Additionally, we find that the assumption of normal distribution of the\ncore position of plain fingerprint recordings cannot be rejected, but for\nrolled ones it can. Therefore, we use a multi-step process to find the\ndistribution of the rolled fingerprint recordings. The process consists of an\nAnderson-Darling normality test, the Bayesian Information Criterion to reduce\nthe number of possible candidate distributions and finally a Generalized Monte\nCarlo goodness-of-fit procedure to find the best fitting distribution. We find\nthe non-central Fischer distribution best describes the cores' horizontal\npositions. Finally, we investigate the correlation between mean core position\noffset and the NFIQ 2 score and find that the NFIQ 2 prefers rolled fingerprint\nrecordings where the core sits slightly below the fingerprint center.",
            "author": [
                "Laurenz Ruzicka",
                "Bernhard Strobl",
                "Bernhard Kohn",
                "Clemens Heitzinger"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07584v1",
                "http://arxiv.org/pdf/2310.07584v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07576v1",
            "title": "Analyzing Trendy Twitter Hashtags in the 2022 French Election",
            "updated": "2023-10-11T15:17:55Z",
            "published": "2023-10-11T15:17:55Z",
            "summary": "Regressions trained to predict the future activity of social media users need\nrich features for accurate predictions. Many advanced models exist to generate\nsuch features; however, the time complexities of their computations are often\nprohibitive when they run on enormous data-sets. Some studies have shown that\nsimple semantic network features can be rich enough to use for regressions\nwithout requiring complex computations. We propose a method for using semantic\nnetworks as user-level features for machine learning tasks. We conducted an\nexperiment using a semantic network of 1037 Twitter hashtags from a corpus of\n3.7 million tweets related to the 2022 French presidential election. A\nbipartite graph is formed where hashtags are nodes and weighted edges connect\nthe hashtags reflecting the number of Twitter users that interacted with both\nhashtags. The graph is then transformed into a maximum-spanning tree with the\nmost popular hashtag as its root node to construct a hierarchy amongst the\nhashtags. We then provide a vector feature for each user based on this tree. To\nvalidate the usefulness of our semantic feature we performed a regression\nexperiment to predict the response rate of each user with six emotions like\nanger, enjoyment, or disgust. Our semantic feature performs well with the\nregression with most emotions having $R^2$ above 0.5. These results suggest\nthat our semantic feature could be considered for use in further experiments\npredicting social media response on big data-sets.",
            "author": [
                "Aamir Mandviwalla",
                "Lake Yin",
                "Boleslaw K. Szymanski"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07576v1",
                "http://arxiv.org/pdf/2310.07576v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07573v1",
            "title": "Relational Prior Knowledge Graphs for Detection and Instance\n  Segmentation",
            "updated": "2023-10-11T15:15:05Z",
            "published": "2023-10-11T15:15:05Z",
            "summary": "Humans have a remarkable ability to perceive and reason about the world\naround them by understanding the relationships between objects. In this paper,\nwe investigate the effectiveness of using such relationships for object\ndetection and instance segmentation. To this end, we propose a Relational\nPrior-based Feature Enhancement Model (RP-FEM), a graph transformer that\nenhances object proposal features using relational priors. The proposed\narchitecture operates on top of scene graphs obtained from initial proposals\nand aims to concurrently learn relational context modeling for object detection\nand instance segmentation. Experimental evaluations on COCO show that the\nutilization of scene graphs, augmented with relational priors, offer benefits\nfor object detection and instance segmentation. RP-FEM demonstrates its\ncapacity to suppress improbable class predictions within the image while also\npreventing the model from generating duplicate predictions, leading to\nimprovements over the baseline model on which it is built.",
            "author": [
                "Osman \u00dclger",
                "Yu Wang",
                "Ysbrand Galama",
                "Sezer Karaoglu",
                "Theo Gevers",
                "Martin R. Oswald"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07573v1",
                "http://arxiv.org/pdf/2310.07573v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07558v2",
            "title": "Smoothness-Adaptive Dynamic Pricing with Nonparametric Demand Learning",
            "updated": "2023-11-01T00:56:42Z",
            "published": "2023-10-11T15:02:13Z",
            "summary": "We study the dynamic pricing problem where the demand function is\nnonparametric and H\\\"older smooth, and we focus on adaptivity to the unknown\nH\\\"older smoothness parameter $\\beta$ of the demand function. Traditionally the\noptimal dynamic pricing algorithm heavily relies on the knowledge of $\\beta$ to\nachieve a minimax optimal regret of\n$\\widetilde{O}(T^{\\frac{\\beta+1}{2\\beta+1}})$. However, we highlight the\nchallenge of adaptivity in this dynamic pricing problem by proving that no\npricing policy can adaptively achieve this minimax optimal regret without\nknowledge of $\\beta$. Motivated by the impossibility result, we propose a\nself-similarity condition to enable adaptivity. Importantly, we show that the\nself-similarity condition does not compromise the problem's inherent complexity\nsince it preserves the regret lower bound\n$\\Omega(T^{\\frac{\\beta+1}{2\\beta+1}})$. Furthermore, we develop a\nsmoothness-adaptive dynamic pricing algorithm and theoretically prove that the\nalgorithm achieves this minimax optimal regret bound without the prior\nknowledge $\\beta$.",
            "author": [
                "Zeqi Ye",
                "Hansheng Jiang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07558v2",
                "http://arxiv.org/pdf/2310.07558v2"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG",
                "econ.EM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07554v2",
            "title": "Retrieve Anything To Augment Large Language Models",
            "updated": "2023-10-25T04:35:09Z",
            "published": "2023-10-11T14:59:53Z",
            "summary": "Large language models (LLMs) face significant challenges stemming from their\ninherent limitations in knowledge, memory, alignment, and action. These\nchallenges cannot be addressed by LLMs alone, but should rely on assistance\nfrom the external world, such as knowledge base, memory store, demonstration\nexamples, and tools. Retrieval augmentation stands as a vital mechanism for\nbridging the gap between LLMs and the external assistance. However,\nconventional methods encounter two pressing issues. On the one hand, the\ngeneral-purpose retrievers are not properly optimized for the retrieval\naugmentation of LLMs. On the other hand, the task-specific retrievers lack the\nrequired versatility, hindering their performance across the diverse retrieval\naugmentation scenarios.\n  In this work, we present a novel approach, the LLM-Embedder, which\ncomprehensively supports the diverse retrieval augmentation needs of LLMs with\none unified embedding model. Training such a unified model is non-trivial, as\nvarious retrieval tasks aim to capture distinct semantic relationships, often\nsubject to mutual interference. To address this challenge, we systematically\noptimize our training methodology. This includes reward formulation based on\nLLMs' feedback, the stabilization of knowledge distillation, multi-task\nfine-tuning with explicit instructions, and homogeneous in-batch negative\nsampling. These optimization strategies contribute to the outstanding empirical\nperformance of the LLM-Embedder. Notably, it yields remarkable enhancements in\nretrieval augmentation for LLMs, surpassing both general-purpose and\ntask-specific retrievers in various evaluation scenarios. Our checkpoint and\nsource code are publicly available at\nhttps://github.com/FlagOpen/FlagEmbedding.",
            "author": [
                "Peitian Zhang",
                "Shitao Xiao",
                "Zheng Liu",
                "Zhicheng Dou",
                "Jian-Yun Nie"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07554v2",
                "http://arxiv.org/pdf/2310.07554v2"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07535v2",
            "title": "Improving Fairness-Accuracy tradeoff with few Test Samples under\n  Covariate Shift",
            "updated": "2023-10-30T08:53:40Z",
            "published": "2023-10-11T14:39:51Z",
            "summary": "Covariate shift in the test data can significantly downgrade both the\naccuracy and the fairness performance of the model. Ensuring fairness across\ndifferent sensitive groups in such settings is of paramount importance due to\nsocietal implications like criminal justice. We operate under the unsupervised\nregime where only a small set of unlabeled test samples along with a labeled\ntraining set is available. Towards this problem, we make three contributions.\nFirst is a novel composite weighted entropy based objective for prediction\naccuracy which is optimized along with a representation matching loss for\nfairness. We experimentally verify that optimizing with our loss formulation\noutperforms a number of state-of-the-art baselines in the pareto sense with\nrespect to the fairness-accuracy tradeoff on several standard datasets. Our\nsecond contribution is a new setting we term Asymmetric Covariate Shift that,\nto the best of our knowledge, has not been studied before. Asymmetric covariate\nshift occurs when distribution of covariates of one group shifts significantly\ncompared to the other groups and this happens when a dominant group is\nover-represented. While this setting is extremely challenging for current\nbaselines, We show that our proposed method significantly outperforms them. Our\nthird contribution is theoretical, where we show that our weighted entropy term\nalong with prediction loss on the training set approximates test loss under\ncovariate shift. Empirically and through formal sample complexity bounds, we\nshow that this approximation to the unseen test loss does not depend on\nimportance sampling variance which affects many other baselines.",
            "author": [
                "Shreyas Havaldar",
                "Jatin Chauhan",
                "Karthikeyan Shanmugam",
                "Jay Nandy",
                "Aravindan Raghuveer"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07535v2",
                "http://arxiv.org/pdf/2310.07535v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07524v1",
            "title": "New Lower Bounds for the Minimum Distance of Cyclic Codes and\n  Applications to Locally Repairable Codes",
            "updated": "2023-10-11T14:22:44Z",
            "published": "2023-10-11T14:22:44Z",
            "summary": "Cyclic codes are an important class of linear codes. Bounding the minimum\ndistance of cyclic codes is a long-standing research topic in coding theory,\nand several well-known and basic results have been developed on this topic.\nRecently, locally repairable codes (LRCs) have attracted much attention due to\ntheir repair efficiency in large-scale distributed storage systems. In this\npaper, by employing the singleton procedure technique, we first provide a\nsufficient condition for bounding the minimum distance of cyclic codes with\ntypical defining sets. Secondly, by considering a specific case, we establish a\nconnection between bounds for the minimum distance of cyclic codes and\nsolutions to a system of inequalities. This connection leads to the derivation\nof new bounds, including some with general patterns. In particular, we provide\nthree new bounds with general patterns, one of which serves as a generalization\nof the Betti-Sala bound. Finally, we present a generalized lower bound for a\nspecial case and construct several families of $(2, \\delta)$-LRCs with\nunbounded length and minimum distance $2\\delta$. It turns out that these LRCs\nare distance-optimal, and their parameters are new. To the best of our\nknowledge, this work represents the first construction of distance-optimal $(r,\n\\delta)$-LRCs with unbounded length and minimum distance exceeding\n$r+\\delta-1$.",
            "author": [
                "Jing Qiu",
                "Weijun Fang",
                "Fang-Wei Fu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07524v1",
                "http://arxiv.org/pdf/2310.07524v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07521v2",
            "title": "Survey on Factuality in Large Language Models: Knowledge, Retrieval and\n  Domain-Specificity",
            "updated": "2023-10-18T14:09:19Z",
            "published": "2023-10-11T14:18:03Z",
            "summary": "This survey addresses the crucial issue of factuality in Large Language\nModels (LLMs). As LLMs find applications across diverse domains, the\nreliability and accuracy of their outputs become vital. We define the\nFactuality Issue as the probability of LLMs to produce content inconsistent\nwith established facts. We first delve into the implications of these\ninaccuracies, highlighting the potential consequences and challenges posed by\nfactual errors in LLM outputs. Subsequently, we analyze the mechanisms through\nwhich LLMs store and process facts, seeking the primary causes of factual\nerrors. Our discussion then transitions to methodologies for evaluating LLM\nfactuality, emphasizing key metrics, benchmarks, and studies. We further\nexplore strategies for enhancing LLM factuality, including approaches tailored\nfor specific domains. We focus two primary LLM configurations standalone LLMs\nand Retrieval-Augmented LLMs that utilizes external data, we detail their\nunique challenges and potential enhancements. Our survey offers a structured\nguide for researchers aiming to fortify the factual reliability of LLMs.",
            "author": [
                "Cunxiang Wang",
                "Xiaoze Liu",
                "Yuanhao Yue",
                "Xiangru Tang",
                "Tianhang Zhang",
                "Cheng Jiayang",
                "Yunzhi Yao",
                "Wenyang Gao",
                "Xuming Hu",
                "Zehan Qi",
                "Yidong Wang",
                "Linyi Yang",
                "Jindong Wang",
                "Xing Xie",
                "Zheng Zhang",
                "Yue Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07521v2",
                "http://arxiv.org/pdf/2310.07521v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07518v1",
            "title": "Exploiting Causal Graph Priors with Posterior Sampling for Reinforcement\n  Learning",
            "updated": "2023-10-11T14:16:04Z",
            "published": "2023-10-11T14:16:04Z",
            "summary": "Posterior sampling allows the exploitation of prior knowledge of the\nenvironment's transition dynamics to improve the sample efficiency of\nreinforcement learning. The prior is typically specified as a class of\nparametric distributions, a task that can be cumbersome in practice, often\nresulting in the choice of uninformative priors. In this work, we propose a\nnovel posterior sampling approach in which the prior is given as a (partial)\ncausal graph over the environment's variables. The latter is often more natural\nto design, such as listing known causal dependencies between biometric features\nin a medical treatment study. Specifically, we propose a hierarchical Bayesian\nprocedure, called C-PSRL, simultaneously learning the full causal graph at the\nhigher level and the parameters of the resulting factored dynamics at the lower\nlevel. For this procedure, we provide an analysis of its Bayesian regret, which\nexplicitly connects the regret rate with the degree of prior knowledge. Our\nnumerical evaluation conducted in illustrative domains confirms that C-PSRL\nstrongly improves the efficiency of posterior sampling with an uninformative\nprior while performing close to posterior sampling with the full causal graph.",
            "author": [
                "Mirco Mutti",
                "Riccardo De Santi",
                "Marcello Restelli",
                "Alexander Marx",
                "Giorgia Ramponi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07518v1",
                "http://arxiv.org/pdf/2310.07518v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07511v1",
            "title": "A Unified Remote Sensing Anomaly Detector Across Modalities and Scenes\n  via Deviation Relationship Learning",
            "updated": "2023-10-11T14:07:05Z",
            "published": "2023-10-11T14:07:05Z",
            "summary": "Remote sensing anomaly detector can find the objects deviating from the\nbackground as potential targets. Given the diversity in earth anomaly types, a\nunified anomaly detector across modalities and scenes should be cost-effective\nand flexible to new earth observation sources and anomaly types. However, the\ncurrent anomaly detectors are limited to a single modality and single scene,\nsince they aim to learn the varying background distribution. Motivated by the\nuniversal anomaly deviation pattern, in that anomalies exhibit deviations from\ntheir local context, we exploit this characteristic to build a unified anomaly\ndetector. Firstly, we reformulate the anomaly detection task as an undirected\nbilayer graph based on the deviation relationship, where the anomaly score is\nmodeled as the conditional probability, given the pattern of the background and\nnormal objects. The learning objective is then expressed as a conditional\nprobability ranking problem. Furthermore, we design an instantiation of the\nreformulation in the data, architecture, and optimization aspects. Simulated\nspectral and spatial anomalies drive the instantiated architecture. The model\nis optimized directly for the conditional probability ranking. The proposed\nmodel was validated in five modalities including the hyperspectral, visible\nlight, synthetic aperture radar (SAR), infrared and low light to show its\nunified detection ability.",
            "author": [
                "Jingtao Li",
                "Xinyu Wang",
                "Hengwei Zhao",
                "Liangpei Zhang",
                "Yanfei Zhong"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07511v1",
                "http://arxiv.org/pdf/2310.07511v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07508v1",
            "title": "Existence of Nontrivial Solutions for the Nonlinear Equation on Locally\n  Finite Graphs",
            "updated": "2023-10-11T14:02:43Z",
            "published": "2023-10-11T14:02:43Z",
            "summary": "Suppose that $G=(V, E)$ be a locally finite and connected graph with\nsymmetric weight and uniformly positive measure, where $V$ denotes the vertex\nset and $E$ denotes the edge set. We are concered with the following problem $$\n\\begin{cases}-\\Delta u+h u=f(x, u), & \\text { in } \\Omega, \\\\ u=0, & \\text { on\n} \\partial \\Omega,\\end{cases} $$ on the graph, where $h: \\Omega \\rightarrow\n\\mathbb{R}$, $f: \\Omega \\times \\mathbb{R} \\rightarrow \\mathbb{R}$ and $u:\n\\Omega \\rightarrow \\mathbb{R}$. When $ f $ and $ h $ satisfies certain\nassumption conditions, we can ascertain the existence of one or two nontrivial\nsolutions on the graph.",
            "author": [
                "Ziliang Yang",
                "Jiabao Su",
                "Mingzheng Sun"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07508v1",
                "http://arxiv.org/pdf/2310.07508v1"
            ],
            "primary_category": "math.FA",
            "category": [
                "math.FA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07506v1",
            "title": "Leveraging Hierarchical Feature Sharing for Efficient Dataset\n  Condensation",
            "updated": "2023-10-11T14:02:11Z",
            "published": "2023-10-11T14:02:11Z",
            "summary": "Given a real-world dataset, data condensation (DC) aims to synthesize a\nsignificantly smaller dataset that captures the knowledge of this dataset for\nmodel training with high performance. Recent works propose to enhance DC with\ndata parameterization, which condenses data into parameterized data containers\nrather than pixel space. The intuition behind data parameterization is to\nencode shared features of images to avoid additional storage costs. In this\npaper, we recognize that images share common features in a hierarchical way due\nto the inherent hierarchical structure of the classification system, which is\noverlooked by current data parameterization methods. To better align DC with\nthis hierarchical nature and encourage more efficient information sharing\ninside data containers, we propose a novel data parameterization architecture,\nHierarchical Memory Network (HMN). HMN stores condensed data in a three-tier\nstructure, representing the dataset-level, class-level, and instance-level\nfeatures. Another helpful property of the hierarchical architecture is that HMN\nnaturally ensures good independence among images despite achieving information\nsharing. This enables instance-level pruning for HMN to reduce redundant\ninformation, thereby further minimizing redundancy and enhancing performance.\nWe evaluate HMN on four public datasets (SVHN, CIFAR10, CIFAR100, and\nTiny-ImageNet) and compare HMN with eight DC baselines. The evaluation results\nshow that our proposed method outperforms all baselines, even when trained with\na batch-based loss consuming less GPU memory.",
            "author": [
                "Haizhong Zheng",
                "Jiachen Sun",
                "Shutong Wu",
                "Bhavya Kailkhura",
                "Zhuoqing Mao",
                "Chaowei Xiao",
                "Atul Prakash"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07506v1",
                "http://arxiv.org/pdf/2310.07506v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07505v1",
            "title": "Thermodynamic and configurational entropy of quantum Schwarzschild\n  geometries",
            "updated": "2023-10-11T14:02:09Z",
            "published": "2023-10-11T14:02:09Z",
            "summary": "We study different entropies for coherent states representing the geometry of\nspherically symmetric compact systems. We show that the thermodynamic entropy\nreproduces the Bekenstein-Hawking result in the presence of thermal modes at\nthe Hawking temperature if the object is a black hole and saturates the\nBekenstein bound for more general compact objects. We also analyse the\ninformation entropy of the quantum coherent state without radiation and find\nfurther support against the singular Schwarzschild geometry.",
            "author": [
                "R. Casadio",
                "R. da Rocha",
                "A. Giusti",
                "P. Meert"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07505v1",
                "http://arxiv.org/pdf/2310.07505v1"
            ],
            "primary_category": "gr-qc",
            "category": [
                "gr-qc",
                "hep-th"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07502v1",
            "title": "Exoplanet Occurrence Rates from Microlensing Surveys",
            "updated": "2023-10-11T14:00:46Z",
            "published": "2023-10-11T14:00:46Z",
            "summary": "The number of exoplanets detected using gravitational microlensing technique\nis currently larger than 200, which enables population studies. Microlensing is\nuniquely sensitive to low-mass planets orbiting at separations of several\nastronomical units, a parameter space that is not accessible to other\nplanet-detection techniques, as well as free-floating planets, not orbiting\naround any star. In this review, we present the state-of-the-art knowledge on\nthe demographics of exoplanets detected with microlensing, with a particular\nemphasis on their occurrence rates. We also summarize the current knowledge\nabout free-floating planets, an elusive population of objects that seem to be\nmore common than ordinary, gravitationally bound exoplanets.",
            "author": [
                "P. Mroz",
                "R. Poleski"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07502v1",
                "http://arxiv.org/pdf/2310.07502v1"
            ],
            "primary_category": "astro-ph.EP",
            "category": [
                "astro-ph.EP",
                "astro-ph.GA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07478v2",
            "title": "Multimodal Graph Learning for Generative Tasks",
            "updated": "2023-10-12T17:07:24Z",
            "published": "2023-10-11T13:25:03Z",
            "summary": "Multimodal learning combines multiple data modalities, broadening the types\nand complexity of data our models can utilize: for example, from plain text to\nimage-caption pairs. Most multimodal learning algorithms focus on modeling\nsimple one-to-one pairs of data from two modalities, such as image-caption\npairs, or audio-text pairs. However, in most real-world settings, entities of\ndifferent modalities interact with each other in more complex and multifaceted\nways, going beyond one-to-one mappings. We propose to represent these complex\nrelationships as graphs, allowing us to capture data with any number of\nmodalities, and with complex relationships between modalities that can flexibly\nvary from one sample to another. Toward this goal, we propose Multimodal Graph\nLearning (MMGL), a general and systematic framework for capturing information\nfrom multiple multimodal neighbors with relational structures among them. In\nparticular, we focus on MMGL for generative tasks, building upon pretrained\nLanguage Models (LMs), aiming to augment their text generation with multimodal\nneighbor contexts. We study three research questions raised by MMGL: (1) how\ncan we infuse multiple neighbor information into the pretrained LMs, while\navoiding scalability issues? (2) how can we infuse the graph structure\ninformation among multimodal neighbors into the LMs? and (3) how can we\nfinetune the pretrained LMs to learn from the neighbor context in a\nparameter-efficient manner? We conduct extensive experiments to answer these\nthree questions on MMGL and analyze the empirical results to pave the way for\nfuture MMGL research.",
            "author": [
                "Minji Yoon",
                "Jing Yu Koh",
                "Bryan Hooi",
                "Ruslan Salakhutdinov"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07478v2",
                "http://arxiv.org/pdf/2310.07478v2"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07477v1",
            "title": "GMOCAT: A Graph-Enhanced Multi-Objective Method for Computerized\n  Adaptive Testing",
            "updated": "2023-10-11T13:24:38Z",
            "published": "2023-10-11T13:24:38Z",
            "summary": "Computerized Adaptive Testing(CAT) refers to an online system that adaptively\nselects the best-suited question for students with various abilities based on\ntheir historical response records. Most CAT methods only focus on the quality\nobjective of predicting the student ability accurately, but neglect concept\ndiversity or question exposure control, which are important considerations in\nensuring the performance and validity of CAT. Besides, the students' response\nrecords contain valuable relational information between questions and knowledge\nconcepts. The previous methods ignore this relational information, resulting in\nthe selection of sub-optimal test questions. To address these challenges, we\npropose a Graph-Enhanced Multi-Objective method for CAT (GMOCAT). Firstly,\nthree objectives, namely quality, diversity and novelty, are introduced into\nthe Scalarized Multi-Objective Reinforcement Learning framework of CAT, which\nrespectively correspond to improving the prediction accuracy, increasing the\nconcept diversity and reducing the question exposure. We use an Actor-Critic\nRecommender to select questions and optimize three objectives simultaneously by\nthe scalarization function. Secondly, we utilize the graph neural network to\nlearn relation-aware embeddings of questions and concepts. These embeddings are\nable to aggregate neighborhood information in the relation graphs between\nquestions and concepts. We conduct experiments on three real-world educational\ndatasets, and show that GMOCAT not only outperforms the state-of-the-art\nmethods in the ability prediction, but also achieve superior performance in\nimproving the concept diversity and alleviating the question exposure. Our code\nis available at https://github.com/justarter/GMOCAT.",
            "author": [
                "Hangyu Wang",
                "Ting Long",
                "Liang Yin",
                "Weinan Zhang",
                "Wei Xia",
                "Qichen Hong",
                "Dingyin Xia",
                "Ruiming Tang",
                "Yong Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07477v1",
                "http://arxiv.org/pdf/2310.07477v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07465v1",
            "title": "Algorithmic study on liar's vertex-edge domination problem",
            "updated": "2023-10-11T13:07:59Z",
            "published": "2023-10-11T13:07:59Z",
            "summary": "A vertex set $L\\subseteq V$ is liar's vertex-edge dominating set of a graph\n$G=(V,E)$ if for every $e_i\\in E$, $|N_G[e_i]\\cap L|\\geq 2$ and for every pair\nof distinct edges $e_i$ and $e_j$, $|(N_G[e_i]\\cup N_G[e_j])\\cap L|\\geq 3$. In\nthis paper, we introduce the notion of liar's vertex-edge domination which\narise naturally from some application in communication network. Given a graph\n$G$, the \\textsc{Minimum Liar's Vertex-Edge Domination Problem}\n(\\textsc{MinLVEDP}) asks to find a minimum liar's vertex-edge dominating set of\n$G$ of minimum cardinality. We have studied this problem from algorithmic point\nof view. We show that \\textsc{MinLVEDP} can be solved in linear time for trees,\nwhereas the decision version of this problem is NP-complete for general graphs.\nWe further study approximation algorithms for this problem. We propose an\n$O(\\ln \\Delta(G))$-approximation algorithm for \\textsc{MinLVEDP} in general\ngraphs, where $\\Delta(G)$ is the maximum degree of the input graph.\n  On the negative side, we show that the \\textsc{MinLVEDP} cannot be\napproximated within $\\frac{1}{2}(\\frac{1}{8}-\\epsilon)\\ln|V|$ for any $\\epsilon\n>0$, unless $NP\\subseteq DTIME(|V|^{O(\\log(\\log|V|)})$.",
            "author": [
                "Debojyoti Bhattacharya",
                "Subhabrata Paul"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07465v1",
                "http://arxiv.org/pdf/2310.07465v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07456v1",
            "title": "Hierarchical Bayesian Claim Count modeling with Overdispersed Outcome\n  and Mismeasured Covariates in Actuarial Practice",
            "updated": "2023-10-11T13:01:25Z",
            "published": "2023-10-11T13:01:25Z",
            "summary": "The problem of overdispersed claim counts and mismeasured covariates is\ncommon in insurance. On the one hand, the presence of overdispersion in the\ncount data violates the homogeneity assumption, and on the other hand,\nmeasurement errors in covariates highlight the model risk issue in actuarial\npractice. The consequence can be inaccurate premium pricing which would\nnegatively affect business competitiveness. Our goal is to address these two\nmodelling problems simultaneously by capturing the unobservable correlations\nbetween observations that arise from overdispersed outcome and mismeasured\ncovariate in actuarial process. To this end, we establish novel connections\nbetween the count-based generalized linear mixed model (GLMM) and a popular\nerror-correction tool for non-linear modelling - Simulation Extrapolation\n(SIMEX). We consider a modelling framework based on the hierarchical Bayesian\nparadigm. To our knowledge, the approach of combining a hierarchical Bayes with\nSIMEX has not previously been discussed in the literature. We demonstrate the\napplicability of our approach on the workplace absenteeism data. Our results\nindicate that the hierarchical Bayesian GLMM incorporated with the SIMEX\noutperforms naive GLMM / SIMEX in terms of goodness of fit.",
            "author": [
                "Minkun Kim"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07456v1",
                "http://arxiv.org/pdf/2310.07456v1"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07452v1",
            "title": "On $k$-vertex-edge domination of graph",
            "updated": "2023-10-11T12:57:02Z",
            "published": "2023-10-11T12:57:02Z",
            "summary": "Let $G=(V,E)$ be a simple undirected graph. The open neighbourhood of a\nvertex $v$ in $G$ is defined as $N_G(v)=\\{u\\in V~|~ uv\\in E\\}$; whereas the\nclosed neighbourhood is defined as $N_G[v]= N_G(v)\\cup \\{v\\}$. For an integer\n$k$, a subset $D\\subseteq V$ is called a $k$-vertex-edge dominating set of $G$\nif for every edge $uv\\in E$, $|(N_G[u]\\cup N_G[v]) \\cap D|\\geq k$. In\n$k$-vertex-edge domination problem, our goal is to find a $k$-vertex-edge\ndominating set of minimum cardinality of an input graph $G$. In this paper, we\nfirst prove that the decision version of $k$-vertex-edge domination problem is\nNP-complete for chordal graphs. On the positive side, we design a linear time\nalgorithm for finding a minimum $k$-vertex-edge dominating set of tree. We also\nprove that there is a $O(\\log(\\Delta(G)))$-approximation algorithm for this\nproblem in general graph $G$, where $\\Delta(G)$ is the maximum degree of $G$.\nThen we show that for a graph $G$ with $n$ vertices, this problem cannot be\napproximated within a factor of $(1-\\epsilon) \\ln n$ for any $\\epsilon >0$\nunless $NP\\subseteq DTIME(|V|^{O(\\log\\log|V|)})$. Finally, we prove that it is\nAPX-complete for graphs with bounded degree $k+3$.",
            "author": [
                "Debojyoti Bhattacharya",
                "Subhabrata Paul"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07452v1",
                "http://arxiv.org/pdf/2310.07452v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "cs.DM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07446v1",
            "title": "ProbTS: A Unified Toolkit to Probe Deep Time-series Forecasting",
            "updated": "2023-10-11T12:48:45Z",
            "published": "2023-10-11T12:48:45Z",
            "summary": "Time-series forecasting serves as a linchpin in a myriad of applications,\nspanning various domains. With the growth of deep learning, this arena has\nbifurcated into two salient branches: one focuses on crafting specific neural\narchitectures tailored for time series, and the other harnesses advanced deep\ngenerative models for probabilistic forecasting. While both branches have made\nsignificant progress, their differences across data scenarios, methodological\nfocuses, and decoding schemes pose profound, yet unexplored, research\nquestions. To bridge this knowledge chasm, we introduce ProbTS, a pioneering\ntoolkit developed to synergize and compare these two distinct branches. Endowed\nwith a unified data module, a modularized model module, and a comprehensive\nevaluator module, ProbTS allows us to revisit and benchmark leading methods\nfrom both branches. The scrutiny with ProbTS highlights their distinct\ncharacteristics, relative strengths and weaknesses, and areas that need further\nexploration. Our analyses point to new avenues for research, aiming for more\neffective time-series forecasting.",
            "author": [
                "Jiawen Zhang",
                "Xumeng Wen",
                "Shun Zheng",
                "Jia Li",
                "Jiang Bian"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07446v1",
                "http://arxiv.org/pdf/2310.07446v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07445v2",
            "title": "The Alon-Tarsi number of $K_{3,3}$-minor-free graphs",
            "updated": "2023-10-16T08:33:28Z",
            "published": "2023-10-11T12:48:32Z",
            "summary": "The well known Wagner's theorem states that a graph is a planar graph if and\nonly if it is $K_5$-minor-free and $K_{3,3}$-minor-free. Denote by $AT(G)$ the\nAlon-Tarsi number of a graph $G$. We show that for any $K_{3,3}$-minor-free\ngraph $G$, $AT(G)\\le 5$, there exists a matching $M$ and a forest $F$ such that\n$AT(G-M)\\le 4$ and $AT(G-E(F))\\le 3$, extending the result on the Alon-Tarsi\nnumber of $K_5$-minor-free graphs due to Abe, Kim and Ozeki.",
            "author": [
                "Leyou Xu",
                "Bo Zhou"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07445v2",
                "http://arxiv.org/pdf/2310.07445v2"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07432v1",
            "title": "Bounds on zero forcing using (upper) total domination and minimum degree",
            "updated": "2023-10-11T12:34:23Z",
            "published": "2023-10-11T12:34:23Z",
            "summary": "While a number of bounds are known on the zero forcing number $Z(G)$ of a\ngraph $G$ expressed in terms of the order of a graph and maximum or minimum\ndegree, we present two bounds that are related to the (upper) total domination\nnumber $\\gamma_t(G)$ (resp. $\\Gamma_t(G)$) of $G$. We prove that\n$Z(G)+\\gamma_t(G)\\le n(G)$ and $Z(G)+\\frac{\\Gamma_t(G)}{2}\\le n(G)$ holds for\nany graph $G$ with no isolated vertices of order $n(G)$. Both bounds are sharp\nas demonstrated by several infinite families of graphs. In particular, we show\nthat every graph $H$ is an induced subgraph of a graph $G$ with\n$Z(G)+\\frac{\\Gamma_t(G)}{2}=n(G)$. Furthermore, we prove a characterization of\ngraphs with power domination equal to $1$, from which we derive a\ncharacterization of the extremal graphs attaining the trivial lower bound\n$Z(G)\\ge \\delta(G)$. The class of graphs that appears in the corresponding\ncharacterizations is obtained by extending an idea from [D.D.~Row, A technique\nfor computing the zero forcing number of a graph with a cut-vertex, Linear\nAlg.\\ Appl.\\ 436 (2012) 4423--4432], where the graphs with zero forcing number\nequal to $2$ were characterized.",
            "author": [
                "Bo\u0161tjan Bre\u0161ar",
                "Mar\u00eda Gracia Cornet",
                "Tanja Dravec",
                "Michael Henning"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07432v1",
                "http://arxiv.org/pdf/2310.07432v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07430v1",
            "title": "Non-backtracking Graph Neural Networks",
            "updated": "2023-10-11T12:32:13Z",
            "published": "2023-10-11T12:32:13Z",
            "summary": "The celebrated message-passing updates for graph neural networks allow the\nrepresentation of large-scale graphs with local and computationally tractable\nupdates. However, the local updates suffer from backtracking, i.e., a message\nflows through the same edge twice and revisits the previously visited node.\nSince the number of message flows increases exponentially with the number of\nupdates, the redundancy in local updates prevents the graph neural network from\naccurately recognizing a particular message flow for downstream tasks. In this\nwork, we propose to resolve such a redundancy via the non-backtracking graph\nneural network (NBA-GNN) that updates a message without incorporating the\nmessage from the previously visited node. We further investigate how NBA-GNN\nalleviates the over-squashing of GNNs, and establish a connection between\nNBA-GNN and the impressive performance of non-backtracking updates for\nstochastic block model recovery. We empirically verify the effectiveness of our\nNBA-GNN on long-range graph benchmark and transductive node classification\nproblems.",
            "author": [
                "Seonghyun Park",
                "Narae Ryu",
                "Gahee Kim",
                "Dongyeop Woo",
                "Se-Young Yun",
                "Sungsoo Ahn"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07430v1",
                "http://arxiv.org/pdf/2310.07430v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07417v1",
            "title": "What can knowledge graph alignment gain with Neuro-Symbolic learning\n  approaches?",
            "updated": "2023-10-11T12:03:19Z",
            "published": "2023-10-11T12:03:19Z",
            "summary": "Knowledge Graphs (KG) are the backbone of many data-intensive applications\nsince they can represent data coupled with its meaning and context. Aligning\nKGs across different domains and providers is necessary to afford a fuller and\nintegrated representation. A severe limitation of current KG alignment (KGA)\nalgorithms is that they fail to articulate logical thinking and reasoning with\nlexical, structural, and semantic data learning. Deep learning models are\nincreasingly popular for KGA inspired by their good performance in other tasks,\nbut they suffer from limitations in explainability, reasoning, and data\nefficiency. Hybrid neurosymbolic learning models hold the promise of\nintegrating logical and data perspectives to produce high-quality alignments\nthat are explainable and support validation through human-centric approaches.\nThis paper examines the current state of the art in KGA and explores the\npotential for neurosymbolic integration, highlighting promising research\ndirections for combining these fields.",
            "author": [
                "Pedro Giesteira Cotovio",
                "Ernesto Jimenez-Ruiz",
                "Catia Pesquita"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07417v1",
                "http://arxiv.org/pdf/2310.07417v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.LG",
                "cs.SC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07416v1",
            "title": "A Novel Voronoi-based Convolutional Neural Network Framework for Pushing\n  Person Detection in Crowd Videos",
            "updated": "2023-10-11T12:01:52Z",
            "published": "2023-10-11T12:01:52Z",
            "summary": "Analyzing the microscopic dynamics of pushing behavior within crowds can\noffer valuable insights into crowd patterns and interactions. By identifying\ninstances of pushing in crowd videos, a deeper understanding of when, where,\nand why such behavior occurs can be achieved. This knowledge is crucial to\ncreating more effective crowd management strategies, optimizing crowd flow, and\nenhancing overall crowd experiences. However, manually identifying pushing\nbehavior at the microscopic level is challenging, and the existing automatic\napproaches cannot detect such microscopic behavior. Thus, this article\nintroduces a novel automatic framework for identifying pushing in videos of\ncrowds on a microscopic level. The framework comprises two main components: i)\nFeature extraction and ii) Video labeling. In the feature extraction component,\na new Voronoi-based method is developed for determining the local regions\nassociated with each person in the input video. Subsequently, these regions are\nfed into EfficientNetV1B0 Convolutional Neural Network to extract the deep\nfeatures of each person over time. In the second component, a combination of a\nfully connected layer with a Sigmoid activation function is employed to analyze\nthese deep features and annotate the individuals involved in pushing within the\nvideo. The framework is trained and evaluated on a new dataset created using\nsix real-world experiments, including their corresponding ground truths. The\nexperimental findings indicate that the suggested framework outperforms seven\nbaseline methods that are employed for comparative analysis purposes.",
            "author": [
                "Ahmed Alia",
                "Mohammed Maree",
                "Mohcine Chraibi",
                "Armin Seyfried"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07416v1",
                "http://arxiv.org/pdf/2310.07416v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07409v2",
            "title": "Mobility and diffusion of intruders in granular suspensions. Einstein\n  relation",
            "updated": "2023-10-12T10:01:08Z",
            "published": "2023-10-11T11:50:02Z",
            "summary": "The Enskog kinetic equation is considered to determine the mobility $\\lambda$\nand diffusion $D$ transport coefficients of intruders immersed in a granular\ngas of inelastic hard spheres (grains). Intruders and grains are in contact\nwith a thermal bath, which plays the role of a background gas. As usual, the\ninfluence of the latter on the dynamics of intruders and grains is accounted\nfor via a viscous drag force plus a stochastic Langevin-like term proportional\nto the background temperature $T_\\text{b}$. The transport coefficients\n$\\lambda$ and $D$ are determined by solving the kinetic equation by means of\nthe Chapman--Enskog method adapted to dissipative dynamics. Both transport\ncoefficients are given in terms of the solutions of two integral equations\nwhich are approximately solved up to the second order in a Sonine polynomial\nexpansion. Theoretical results are compared against numerical solutions of the\ninelastic Enskog equation by means of the direct simulation Monte Carlo (DSMC)\nmethod. Good agreement between theory and simulations is in general found,\nspecially in the case of the second Sonine approximation. The knowledge of the\ncoefficients $\\lambda$ and $D$ allow us to assess the departure of the Einstein\nrelation $\\epsilon=D/(T_{\\text{b}}\\lambda)$ from 1. As expected from previous\nresults for driven granular gases, it is shown that the origin of the deviation\nof $\\epsilon$ from 1 is only due to the non-Maxwellian behavior of reference\nstate of intruders (measured by the cumulant $c_0$) when the bath temperature\n$T_\\text{b}$ is replaced by the intruder temperature $T_0$ in the Einstein\nrelation. Since the magnitude of $c_0$ is in general very small, deviations of\nthe (modified) Einstein relation $\\epsilon_0=D/(T_0\\lambda)$ from 1 cannot be\ndetected in computer simulations of dilute granular gases. This conclusion\nagrees well with previous computer simulation results.",
            "author": [
                "Rub\u00e9n G\u00f3mez Gonz\u00e1lez",
                "Vicente Garz\u00f3"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07409v2",
                "http://arxiv.org/pdf/2310.07409v2"
            ],
            "primary_category": "cond-mat.soft",
            "category": [
                "cond-mat.soft",
                "cond-mat.stat-mech"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07403v1",
            "title": "DASpeech: Directed Acyclic Transformer for Fast and High-quality\n  Speech-to-Speech Translation",
            "updated": "2023-10-11T11:39:36Z",
            "published": "2023-10-11T11:39:36Z",
            "summary": "Direct speech-to-speech translation (S2ST) translates speech from one\nlanguage into another using a single model. However, due to the presence of\nlinguistic and acoustic diversity, the target speech follows a complex\nmultimodal distribution, posing challenges to achieving both high-quality\ntranslations and fast decoding speeds for S2ST models. In this paper, we\npropose DASpeech, a non-autoregressive direct S2ST model which realizes both\nfast and high-quality S2ST. To better capture the complex distribution of the\ntarget speech, DASpeech adopts the two-pass architecture to decompose the\ngeneration process into two steps, where a linguistic decoder first generates\nthe target text, and an acoustic decoder then generates the target speech based\non the hidden states of the linguistic decoder. Specifically, we use the\ndecoder of DA-Transformer as the linguistic decoder, and use FastSpeech 2 as\nthe acoustic decoder. DA-Transformer models translations with a directed\nacyclic graph (DAG). To consider all potential paths in the DAG during\ntraining, we calculate the expected hidden states for each target token via\ndynamic programming, and feed them into the acoustic decoder to predict the\ntarget mel-spectrogram. During inference, we select the most probable path and\ntake hidden states on that path as input to the acoustic decoder. Experiments\non the CVSS Fr-En benchmark demonstrate that DASpeech can achieve comparable or\neven better performance than the state-of-the-art S2ST model Translatotron 2,\nwhile preserving up to 18.53x speedup compared to the autoregressive baseline.\nCompared with the previous non-autoregressive S2ST model, DASpeech does not\nrely on knowledge distillation and iterative decoding, achieving significant\nimprovements in both translation quality and decoding speed. Furthermore,\nDASpeech shows the ability to preserve the speaker's voice of the source speech\nduring translation.",
            "author": [
                "Qingkai Fang",
                "Yan Zhou",
                "Yang Feng"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07403v1",
                "http://arxiv.org/pdf/2310.07403v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.SD",
                "eess.AS",
                "I.2.7"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07394v1",
            "title": "CLIP for Lightweight Semantic Segmentation",
            "updated": "2023-10-11T11:26:35Z",
            "published": "2023-10-11T11:26:35Z",
            "summary": "The large-scale pretrained model CLIP, trained on 400 million image-text\npairs, offers a promising paradigm for tackling vision tasks, albeit at the\nimage level. Later works, such as DenseCLIP and LSeg, extend this paradigm to\ndense prediction, including semantic segmentation, and have achieved excellent\nresults. However, the above methods either rely on CLIP-pretrained visual\nbackbones or use none-pretrained but heavy backbones such as Swin, while\nfalling ineffective when applied to lightweight backbones. The reason for this\nis that the lightweitht networks, feature extraction ability of which are\nrelatively limited, meet difficulty embedding the image feature aligned with\ntext embeddings perfectly. In this work, we present a new feature fusion module\nwhich tackles this problem and enables language-guided paradigm to be applied\nto lightweight networks. Specifically, the module is a parallel design of CNN\nand transformer with a two-way bridge in between, where CNN extracts spatial\ninformation and visual context of the feature map from the image encoder, and\nthe transformer propagates text embeddings from the text encoder forward. The\ncore of the module is the bidirectional fusion of visual and text feature\nacross the bridge which prompts their proximity and alignment in embedding\nspace. The module is model-agnostic, which can not only make language-guided\nlightweight semantic segmentation practical, but also fully exploit the\npretrained knowledge of language priors and achieve better performance than\nprevious SOTA work, such as DenseCLIP, whatever the vision backbone is.\nExtensive experiments have been conducted to demonstrate the superiority of our\nmethod.",
            "author": [
                "Ke Jin",
                "Wankou Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07394v1",
                "http://arxiv.org/pdf/2310.07394v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07392v1",
            "title": "Deep Kernel and Image Quality Estimators for Optimizing Robotic\n  Ultrasound Controller using Bayesian Optimization",
            "updated": "2023-10-11T11:20:35Z",
            "published": "2023-10-11T11:20:35Z",
            "summary": "Ultrasound is a commonly used medical imaging modality that requires expert\nsonographers to manually maneuver the ultrasound probe based on the acquired\nimage. Autonomous Robotic Ultrasound (A-RUS) is an appealing alternative to\nthis manual procedure in order to reduce sonographers' workload. The key\nchallenge to A-RUS is optimizing the ultrasound image quality for the region of\ninterest across different patients. This requires knowledge of anatomy,\nrecognition of error sources and precise probe position, orientation and\npressure. Sample efficiency is important while optimizing these parameters\nassociated with the robotized probe controller. Bayesian Optimization (BO), a\nsample-efficient optimization framework, has recently been applied to optimize\nthe 2D motion of the probe. Nevertheless, further improvements are needed to\nimprove the sample efficiency for high-dimensional control of the probe. We aim\nto overcome this problem by using a neural network to learn a low-dimensional\nkernel in BO, termed as Deep Kernel (DK). The neural network of DK is trained\nusing probe and image data acquired during the procedure. The two image quality\nestimators are proposed that use a deep convolution neural network and provide\nreal-time feedback to the BO. We validated our framework using these two\nfeedback functions on three urinary bladder phantoms. We obtained over 50%\nincrease in sample efficiency for 6D control of the robotized probe.\nFurthermore, our results indicate that this performance enhancement in BO is\nindependent of the specific training dataset, demonstrating inter-patient\nadaptability.",
            "author": [
                "Deepak Raina",
                "SH Chandrashekhara",
                "Richard Voyles",
                "Juan Wachs",
                "Subir Kumar Saha"
            ],
            "link": [
                "http://dx.doi.org/10.1109/ISMR57123.2023.10130193",
                "http://arxiv.org/abs/2310.07392v1",
                "http://arxiv.org/pdf/2310.07392v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07376v2",
            "title": "Point Cloud Denoising and Outlier Detection with Local Geometric\n  Structure by Dynamic Graph CNN",
            "updated": "2023-10-22T02:03:34Z",
            "published": "2023-10-11T10:50:15Z",
            "summary": "The digitalization of society is rapidly developing toward the realization of\nthe digital twin and metaverse. In particular, point clouds are attracting\nattention as a media format for 3D space. Point cloud data is contaminated with\nnoise and outliers due to measurement errors. Therefore, denoising and outlier\ndetection are necessary for point cloud processing. Among them, PointCleanNet\nis an effective method for point cloud denoising and outlier detection.\nHowever, it does not consider the local geometric structure of the patch. We\nsolve this problem by applying two types of graph convolutional layer designed\nbased on the Dynamic Graph CNN. Experimental results show that the proposed\nmethods outperform the conventional method in AUPR, which indicates outlier\ndetection accuracy, and Chamfer Distance, which indicates denoising accuracy.",
            "author": [
                "Kosuke Nakayama",
                "Hiroto Fukuta",
                "Hiroshi Watanabe"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07376v2",
                "http://arxiv.org/pdf/2310.07376v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.MM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07372v1",
            "title": "Sampling triangulations of manifolds using Monte Carlo methods",
            "updated": "2023-10-11T10:45:45Z",
            "published": "2023-10-11T10:45:45Z",
            "summary": "We propose a Monte Carlo method to efficiently find, count, and sample\nabstract triangulations of a given manifold M. The method is based on a biased\nrandom walk through all possible triangulations of M (in the Pachner graph),\nconstructed by combining (bi-stellar) moves with suitable chosen accept/reject\nprobabilities (Metropolis-Hastings). Asymptotically, the method guarantees that\nsamples of triangulations are drawn at random from a chosen probability. This\nenables us not only to sample (rare) triangulations of particular interest but\nalso to estimate the (extremely small) probability of obtaining them when\nisomorphism types of triangulations are sampled uniformly at random. We\nimplement our general method for surface triangulations and 1-vertex\ntriangulations of 3-manifolds. To showcase its usefulness, we present a number\nof experiments: (a) we recover asymptotic growth rates for the number of\nisomorphism types of simplicial triangulations of the 2-dimensional sphere; (b)\nwe experimentally observe that the growth rate for the number of isomorphism\ntypes of 1-vertex triangulations of the 3-dimensional sphere appears to be\nsingly exponential in the number of their tetrahedra; and (c) we present\nexperimental evidence that a randomly chosen isomorphism type of 1-vertex\nn-tetrahedra 3-sphere triangulation, for n tending to infinity, almost surely\nshows a fixed edge-degree distribution which decays exponentially for large\ndegrees, but shows non-monotonic behaviour for small degrees.",
            "author": [
                "Eduardo G. Altmann",
                "Jonathan Spreer"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07372v1",
                "http://arxiv.org/pdf/2310.07372v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "cond-mat.stat-mech",
                "cs.CG",
                "math.GT",
                "physics.comp-ph",
                "57Q15, 60J10, 57-08"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07365v2",
            "title": "GraphControl: Adding Conditional Control to Universal Graph Pre-trained\n  Models for Graph Domain Transfer Learning",
            "updated": "2023-10-12T06:00:52Z",
            "published": "2023-10-11T10:30:49Z",
            "summary": "Graph-structured data is ubiquitous in the world which models complex\nrelationships between objects, enabling various Web applications. Daily\ninfluxes of unlabeled graph data on the Web offer immense potential for these\napplications. Graph self-supervised algorithms have achieved significant\nsuccess in acquiring generic knowledge from abundant unlabeled graph data.\nThese pre-trained models can be applied to various downstream Web applications,\nsaving training time and improving downstream (target) performance. However,\ndifferent graphs, even across seemingly similar domains, can differ\nsignificantly in terms of attribute semantics, posing difficulties, if not\ninfeasibility, for transferring the pre-trained models to downstream tasks.\nConcretely speaking, for example, the additional task-specific node information\nin downstream tasks (specificity) is usually deliberately omitted so that the\npre-trained representation (transferability) can be leveraged. The trade-off as\nsuch is termed as \"transferability-specificity dilemma\" in this work. To\naddress this challenge, we introduce an innovative deployment module coined as\nGraphControl, motivated by ControlNet, to realize better graph domain transfer\nlearning. Specifically, by leveraging universal structural pre-trained models\nand GraphControl, we align the input space across various graphs and\nincorporate unique characteristics of target data as conditional inputs. These\nconditions will be progressively integrated into the model during fine-tuning\nor prompt tuning through ControlNet, facilitating personalized deployment.\nExtensive experiments show that our method significantly enhances the\nadaptability of pre-trained models on target attributed datasets, achieving\n1.4-3x performance gain. Furthermore, it outperforms training-from-scratch\nmethods on target data with a comparable margin and exhibits faster\nconvergence.",
            "author": [
                "Yun Zhu",
                "Yaoke Wang",
                "Haizhou Shi",
                "Zhenshuo Zhang",
                "Siliang Tang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07365v2",
                "http://arxiv.org/pdf/2310.07365v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07355v1",
            "title": "IMITATE: Clinical Prior Guided Hierarchical Vision-Language Pre-training",
            "updated": "2023-10-11T10:12:43Z",
            "published": "2023-10-11T10:12:43Z",
            "summary": "In the field of medical Vision-Language Pre-training (VLP), significant\nefforts have been devoted to deriving text and image features from both\nclinical reports and associated medical images. However, most existing methods\nmay have overlooked the opportunity in leveraging the inherent hierarchical\nstructure of clinical reports, which are generally split into `findings' for\ndescriptive content and `impressions' for conclusive observation. Instead of\nutilizing this rich, structured format, current medical VLP approaches often\nsimplify the report into either a unified entity or fragmented tokens. In this\nwork, we propose a novel clinical prior guided VLP framework named IMITATE to\nlearn the structure information from medical reports with hierarchical\nvision-language alignment. The framework derives multi-level visual features\nfrom the chest X-ray (CXR) images and separately aligns these features with the\ndescriptive and the conclusive text encoded in the hierarchical medical report.\nFurthermore, a new clinical-informed contrastive loss is introduced for\ncross-modal learning, which accounts for clinical prior knowledge in\nformulating sample correlations in contrastive learning. The proposed model,\nIMITATE, outperforms baseline VLP methods across six different datasets,\nspanning five medical imaging downstream tasks. Comprehensive experimental\nresults highlight the advantages of integrating the hierarchical structure of\nmedical reports for vision-language alignment.",
            "author": [
                "Che Liu",
                "Sibo Cheng",
                "Miaojing Shi",
                "Anand Shah",
                "Wenjia Bai",
                "Rossella Arcucci"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07355v1",
                "http://arxiv.org/pdf/2310.07355v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07351v1",
            "title": "Atom-Motif Contrastive Transformer for Molecular Property Prediction",
            "updated": "2023-10-11T10:03:10Z",
            "published": "2023-10-11T10:03:10Z",
            "summary": "Recently, Graph Transformer (GT) models have been widely used in the task of\nMolecular Property Prediction (MPP) due to their high reliability in\ncharacterizing the latent relationship among graph nodes (i.e., the atoms in a\nmolecule). However, most existing GT-based methods usually explore the basic\ninteractions between pairwise atoms, and thus they fail to consider the\nimportant interactions among critical motifs (e.g., functional groups consisted\nof several atoms) of molecules. As motifs in a molecule are significant\npatterns that are of great importance for determining molecular properties\n(e.g., toxicity and solubility), overlooking motif interactions inevitably\nhinders the effectiveness of MPP. To address this issue, we propose a novel\nAtom-Motif Contrastive Transformer (AMCT), which not only explores the\natom-level interactions but also considers the motif-level interactions. Since\nthe representations of atoms and motifs for a given molecule are actually two\ndifferent views of the same instance, they are naturally aligned to generate\nthe self-supervisory signals for model training. Meanwhile, the same motif can\nexist in different molecules, and hence we also employ the contrastive loss to\nmaximize the representation agreement of identical motifs across different\nmolecules. Finally, in order to clearly identify the motifs that are critical\nin deciding the properties of each molecule, we further construct a\nproperty-aware attention mechanism into our learning framework. Our proposed\nAMCT is extensively evaluated on seven popular benchmark datasets, and both\nquantitative and qualitative results firmly demonstrate its effectiveness when\ncompared with the state-of-the-art methods.",
            "author": [
                "Wentao Yu",
                "Shuo Chen",
                "Chen Gong",
                "Gang Niu",
                "Masashi Sugiyama"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07351v1",
                "http://arxiv.org/pdf/2310.07351v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07350v1",
            "title": "Choosing optimal parameters for a distributed multi-constrained QoS\n  routing",
            "updated": "2023-10-11T09:59:18Z",
            "published": "2023-10-11T09:59:18Z",
            "summary": "We consider several basic questions on distributed routing in directed graphs\nwith multiple additive costs, or metrics, and multiple constraints. Distributed\nrouting in this sense is used in several protocols, such as IS-IS and OSPF. A\npractical approach to the multi-constraint routing problem is to, first,\ncombine the metrics into a single `composite' metric, and then apply one-to-all\nshortest path algorithms, e.g. Dijkstra, in order to find shortest path trees.\nWe show that, in general, even if a feasible path exists and is known for every\nsource and destination pair, it is impossible to guarantee a distributed\nrouting under several constraints. We also study the question of choosing the\noptimal `composite' metric. We show that under certain mathematical assumptions\nwe can efficiently find a convex combination of several metrics that maximizes\nthe number of discovered feasible paths. Sometimes it can be done analytically,\nand is in general possible using what we call a 'smart iterative approach'. We\nillustrate these findings by extensive experiments on several typical network\ntopologies.",
            "author": [
                "Sergey Komech",
                "Andrey Kupavskii",
                "Alexei Vezolainen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07350v1",
                "http://arxiv.org/pdf/2310.07350v1"
            ],
            "primary_category": "cs.NI",
            "category": [
                "cs.NI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07348v1",
            "title": "Semantic Association Rule Learning from Time Series Data and Knowledge\n  Graphs",
            "updated": "2023-10-11T09:57:56Z",
            "published": "2023-10-11T09:57:56Z",
            "summary": "Digital Twins (DT) are a promising concept in cyber-physical systems research\ndue to their advanced features including monitoring and automated reasoning.\nSemantic technologies such as Knowledge Graphs (KG) are recently being utilized\nin DTs especially for information modelling. Building on this move, this paper\nproposes a pipeline for semantic association rule learning in DTs using KGs and\ntime series data. In addition to this initial pipeline, we also propose new\nsemantic association rule criterion. The approach is evaluated on an industrial\nwater network scenario. Initial evaluation shows that the proposed approach is\nable to learn a high number of association rules with semantic information\nwhich are more generalizable. The paper aims to set a foundation for further\nwork on using semantic association rule learning especially in the context of\nindustrial applications.",
            "author": [
                "Erkan Karabulut",
                "Victoria Degeler",
                "Paul Groth"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07348v1",
                "http://arxiv.org/pdf/2310.07348v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07343v1",
            "title": "How Do Large Language Models Capture the Ever-changing World Knowledge?\n  A Review of Recent Advances",
            "updated": "2023-10-11T09:46:32Z",
            "published": "2023-10-11T09:46:32Z",
            "summary": "Although large language models (LLMs) are impressive in solving various\ntasks, they can quickly be outdated after deployment. Maintaining their\nup-to-date status is a pressing concern in the current era. This paper provides\na comprehensive review of recent advances in aligning LLMs with the\never-changing world knowledge without re-training from scratch. We categorize\nresearch works systemically and provide in-depth comparisons and discussion. We\nalso discuss existing challenges and highlight future directions to facilitate\nresearch in this field. We release the paper list at\nhttps://github.com/hyintell/awesome-refreshing-llms",
            "author": [
                "Zihan Zhang",
                "Meng Fang",
                "Ling Chen",
                "Mohammad-Reza Namazi-Rad",
                "Jun Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07343v1",
                "http://arxiv.org/pdf/2310.07343v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07338v2",
            "title": "Towards Foundation Models for Learning on Tabular Data",
            "updated": "2023-10-22T17:28:18Z",
            "published": "2023-10-11T09:37:38Z",
            "summary": "Learning on tabular data underpins numerous real-world applications. Despite\nconsiderable efforts in developing effective learning models for tabular data,\ncurrent transferable tabular models remain in their infancy, limited by either\nthe lack of support for direct instruction following in new tasks or the\nneglect of acquiring foundational knowledge and capabilities from diverse\ntabular datasets. In this paper, we propose Tabular Foundation Models (TabFMs)\nto overcome these limitations. TabFMs harness the potential of generative\ntabular learning, employing a pre-trained large language model (LLM) as the\nbase model and fine-tuning it using purpose-designed objectives on an extensive\nrange of tabular datasets. This approach endows TabFMs with a profound\nunderstanding and universal capabilities essential for learning on tabular\ndata. Our evaluations underscore TabFM's effectiveness: not only does it\nsignificantly excel in instruction-following tasks like zero-shot and\nin-context inference, but it also showcases performance that approaches, and in\ninstances, even transcends, the renowned yet mysterious closed-source LLMs like\nGPT-4. Furthermore, when fine-tuning with scarce data, our model achieves\nremarkable efficiency and maintains competitive performance with abundant\ntraining data. Finally, while our results are promising, we also delve into\nTabFM's limitations and potential opportunities, aiming to stimulate and\nexpedite future research on developing more potent TabFMs.",
            "author": [
                "Han Zhang",
                "Xumeng Wen",
                "Shun Zheng",
                "Wei Xu",
                "Jiang Bian"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07338v2",
                "http://arxiv.org/pdf/2310.07338v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07320v1",
            "title": "Byzantine-Resilient Decentralized Multi-Armed Bandits",
            "updated": "2023-10-11T09:09:50Z",
            "published": "2023-10-11T09:09:50Z",
            "summary": "In decentralized cooperative multi-armed bandits (MAB), each agent observes a\ndistinct stream of rewards, and seeks to exchange information with others to\nselect a sequence of arms so as to minimize its regret. Agents in the\ncooperative setting can outperform a single agent running a MAB method such as\nUpper-Confidence Bound (UCB) independently. In this work, we study how to\nrecover such salient behavior when an unknown fraction of the agents can be\nByzantine, that is, communicate arbitrarily wrong information in the form of\nreward mean-estimates or confidence sets. This framework can be used to model\nattackers in computer networks, instigators of offensive content into\nrecommender systems, or manipulators of financial markets. Our key contribution\nis the development of a fully decentralized resilient upper confidence bound\n(UCB) algorithm that fuses an information mixing step among agents with a\ntruncation of inconsistent and extreme values. This truncation step enables us\nto establish that the performance of each normal agent is no worse than the\nclassic single-agent UCB1 algorithm in terms of regret, and more importantly,\nthe cumulative regret of all normal agents is strictly better than the\nnon-cooperative case, provided that each agent has at least 3f+1 neighbors\nwhere f is the maximum possible Byzantine agents in each agent's neighborhood.\nExtensions to time-varying neighbor graphs, and minimax lower bounds are\nfurther established on the achievable regret. Experiments corroborate the\nmerits of this framework in practice.",
            "author": [
                "Jingxuan Zhu",
                "Alec Koppel",
                "Alvaro Velasquez",
                "Ji Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07320v1",
                "http://arxiv.org/pdf/2310.07320v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07310v1",
            "title": "Deep Aramaic: Towards a Synthetic Data Paradigm Enabling Machine\n  Learning in Epigraphy",
            "updated": "2023-10-11T08:47:29Z",
            "published": "2023-10-11T08:47:29Z",
            "summary": "Epigraphy increasingly turns to modern artificial intelligence (AI)\ntechnologies such as machine learning (ML) for extracting insights from ancient\ninscriptions. However, scarce labeled data for training ML algorithms severely\nlimits current techniques, especially for ancient scripts like Old Aramaic. Our\nresearch pioneers an innovative methodology for generating synthetic training\ndata tailored to Old Aramaic letters. Our pipeline synthesizes photo-realistic\nAramaic letter datasets, incorporating textural features, lighting, damage, and\naugmentations to mimic real-world inscription diversity. Despite minimal real\nexamples, we engineer a dataset of 250,000 training and 25,000 validation\nimages covering the 22 letter classes in the Aramaic alphabet. This\ncomprehensive corpus provides a robust volume of data for training a residual\nneural network (ResNet) to classify highly degraded Aramaic letters. The ResNet\nmodel demonstrates high accuracy in classifying real images from the 8th\ncentury BCE Hadad statue inscription. Additional experiments validate\nperformance on varying materials and styles, proving effective generalization.\nOur results validate the model's capabilities in handling diverse real-world\nscenarios, proving the viability of our synthetic data approach and avoiding\nthe dependence on scarce training data that has constrained epigraphic\nanalysis. Our innovative framework elevates interpretation accuracy on damaged\ninscriptions, thus enhancing knowledge extraction from these historical\nresources.",
            "author": [
                "Andrei C. Aioanei",
                "Regine Hunziker-Rodewald",
                "Konstantin Klein",
                "Dominik L. Michels"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07310v1",
                "http://arxiv.org/pdf/2310.07310v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07296v1",
            "title": "A structured L-BFGS method and its application to inverse problems",
            "updated": "2023-10-11T08:30:49Z",
            "published": "2023-10-11T08:30:49Z",
            "summary": "Many inverse problems are phrased as optimization problems in which the\nobjective function is the sum of a data-fidelity term and a regularization.\nOften, the Hessian of the fidelity term is computationally unavailable while\nthe Hessian of the regularizer allows for cheap matrix-vector products. In this\npaper, we study an LBFGS method that takes advantage of this structure. We show\nthat the method converges globally without convexity assumptions and that the\nconvergence is linear under a Kurdyka--{\\L}ojasiewicz-type inequality. In\naddition, we prove linear convergence to cluster points near which the\nobjective function is strongly convex. To the best of our knowledge, this is\nthe first time that linear convergence of an LBFGS method is established in a\nnon-convex setting. The convergence analysis is carried out in infinite\ndimensional Hilbert space, which is appropriate for inverse problems but has\nnot been done before. Numerical results show that the new method outperforms\nother structured LBFGS methods and classical LBFGS on non-convex real-life\nproblems from medical image registration. It also compares favorably with\nclassical LBFGS on ill-conditioned quadratic model problems. An implementation\nof the method is freely available.",
            "author": [
                "Florian Mannel",
                "Hari Om Aggrawal",
                "Jan Modersitzki"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07296v1",
                "http://arxiv.org/pdf/2310.07296v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "65J22 65K05 65K10 90C06 90C26 90C30 90C48 90C53 90C90"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07289v1",
            "title": "Beyond Factuality: A Comprehensive Evaluation of Large Language Models\n  as Knowledge Generators",
            "updated": "2023-10-11T08:22:37Z",
            "published": "2023-10-11T08:22:37Z",
            "summary": "Large language models (LLMs) outperform information retrieval techniques for\ndownstream knowledge-intensive tasks when being prompted to generate world\nknowledge. However, community concerns abound regarding the factuality and\npotential implications of using this uncensored knowledge. In light of this, we\nintroduce CONNER, a COmpreheNsive kNowledge Evaluation fRamework, designed to\nsystematically and automatically evaluate generated knowledge from six\nimportant perspectives -- Factuality, Relevance, Coherence, Informativeness,\nHelpfulness and Validity. We conduct an extensive empirical analysis of the\ngenerated knowledge from three different types of LLMs on two widely studied\nknowledge-intensive tasks, i.e., open-domain question answering and\nknowledge-grounded dialogue. Surprisingly, our study reveals that the\nfactuality of generated knowledge, even if lower, does not significantly hinder\ndownstream tasks. Instead, the relevance and coherence of the outputs are more\nimportant than small factual mistakes. Further, we show how to use CONNER to\nimprove knowledge-intensive tasks by designing two strategies: Prompt\nEngineering and Knowledge Selection. Our evaluation code and LLM-generated\nknowledge with human annotations will be released to facilitate future\nresearch.",
            "author": [
                "Liang Chen",
                "Yang Deng",
                "Yatao Bian",
                "Zeyu Qin",
                "Bingzhe Wu",
                "Tat-Seng Chua",
                "Kam-Fai Wong"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07289v1",
                "http://arxiv.org/pdf/2310.07289v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07284v3",
            "title": "Typing to Listen at the Cocktail Party: Text-Guided Target Speaker\n  Extraction",
            "updated": "2023-10-15T03:58:29Z",
            "published": "2023-10-11T08:17:54Z",
            "summary": "Humans possess an extraordinary ability to selectively focus on the sound\nsource of interest amidst complex acoustic environments, commonly referred to\nas cocktail party scenarios. In an attempt to replicate this remarkable\nauditory attention capability in machines, target speaker extraction (TSE)\nmodels have been developed. These models leverage the pre-registered cues of\nthe target speaker to extract the sound source of interest. However, the\neffectiveness of these models is hindered in real-world scenarios due to the\nunreliable or even absence of pre-registered cues. To address this limitation,\nthis study investigates the integration of natural language description to\nenhance the feasibility, controllability, and performance of existing TSE\nmodels. Specifically, we propose a model named LLM-TSE, wherein a large\nlanguage model (LLM) extracts useful semantic cues from the user's typed text\ninput. These cues can serve as independent extraction cues, task selectors to\ncontrol the TSE process or complement the pre-registered cues. Our experimental\nresults demonstrate competitive performance when only text-based cues are\npresented, the effectiveness of using input text as a task selector, and a new\nstate-of-the-art when combining text-based cues with pre-registered cues. To\nour knowledge, this is the first study to successfully incorporate LLMs to\nguide target speaker extraction, which can be a cornerstone for cocktail party\nproblem research.",
            "author": [
                "Xiang Hao",
                "Jibin Wu",
                "Jianwei Yu",
                "Chenglin Xu",
                "Kay Chen Tan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07284v3",
                "http://arxiv.org/pdf/2310.07284v3"
            ],
            "primary_category": "eess.AS",
            "category": [
                "eess.AS",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07276v2",
            "title": "BioT5: Enriching Cross-modal Integration in Biology with Chemical\n  Knowledge and Natural Language Associations",
            "updated": "2023-10-17T14:55:58Z",
            "published": "2023-10-11T07:57:08Z",
            "summary": "Recent advancements in biological research leverage the integration of\nmolecules, proteins, and natural language to enhance drug discovery. However,\ncurrent models exhibit several limitations, such as the generation of invalid\nmolecular SMILES, underutilization of contextual information, and equal\ntreatment of structured and unstructured knowledge. To address these issues, we\npropose $\\mathbf{BioT5}$, a comprehensive pre-training framework that enriches\ncross-modal integration in biology with chemical knowledge and natural language\nassociations. $\\mathbf{BioT5}$ utilizes SELFIES for $100%$ robust molecular\nrepresentations and extracts knowledge from the surrounding context of\nbio-entities in unstructured biological literature. Furthermore,\n$\\mathbf{BioT5}$ distinguishes between structured and unstructured knowledge,\nleading to more effective utilization of information. After fine-tuning, BioT5\nshows superior performance across a wide range of tasks, demonstrating its\nstrong capability of capturing underlying relations and properties of\nbio-entities. Our code is available at\n$\\href{https://github.com/QizhiPei/BioT5}{Github}$.",
            "author": [
                "Qizhi Pei",
                "Wei Zhang",
                "Jinhua Zhu",
                "Kehan Wu",
                "Kaiyuan Gao",
                "Lijun Wu",
                "Yingce Xia",
                "Rui Yan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07276v2",
                "http://arxiv.org/pdf/2310.07276v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG",
                "q-bio.BM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07265v1",
            "title": "Distilling Efficient Vision Transformers from CNNs for Semantic\n  Segmentation",
            "updated": "2023-10-11T07:45:37Z",
            "published": "2023-10-11T07:45:37Z",
            "summary": "In this paper, we tackle a new problem: how to transfer knowledge from the\npre-trained cumbersome yet well-performed CNN-based model to learn a compact\nVision Transformer (ViT)-based model while maintaining its learning capacity?\nDue to the completely different characteristics of ViT and CNN and the\nlong-existing capacity gap between teacher and student models in Knowledge\nDistillation (KD), directly transferring the cross-model knowledge is\nnon-trivial. To this end, we subtly leverage the visual and\nlinguistic-compatible feature character of ViT (i.e., student), and its\ncapacity gap with the CNN (i.e., teacher) and propose a novel CNN-to-ViT KD\nframework, dubbed C2VKD. Importantly, as the teacher's features are\nheterogeneous to those of the student, we first propose a novel\nvisual-linguistic feature distillation (VLFD) module that explores efficient KD\namong the aligned visual and linguistic-compatible representations. Moreover,\ndue to the large capacity gap between the teacher and student and the\ninevitable prediction errors of the teacher, we then propose a pixel-wise\ndecoupled distillation (PDD) module to supervise the student under the\ncombination of labels and teacher's predictions from the decoupled target and\nnon-target classes. Experiments on three semantic segmentation benchmark\ndatasets consistently show that the increment of mIoU of our method is over\n200% of the SoTA KD methods",
            "author": [
                "Xu Zheng",
                "Yunhao Luo",
                "Pengyuan Zhou",
                "Lin Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07265v1",
                "http://arxiv.org/pdf/2310.07265v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07253v1",
            "title": "ADMEOOD: Out-of-Distribution Benchmark for Drug Property Prediction",
            "updated": "2023-10-11T07:30:18Z",
            "published": "2023-10-11T07:30:18Z",
            "summary": "Obtaining accurate and valid information for drug molecules is a crucial and\nchallenging task. However, chemical knowledge and information have been\naccumulated over the past 100 years from various regions, laboratories, and\nexperimental purposes. Little has been explored in terms of the\nout-of-distribution (OOD) problem with noise and inconsistency, which may lead\nto weak robustness and unsatisfied performance. This study proposes a novel\nbenchmark ADMEOOD, a systematic OOD dataset curator and benchmark specifically\ndesigned for drug property prediction. ADMEOOD obtained 27 ADME (Absorption,\nDistribution, Metabolism, Excretion) drug properties from Chembl and relevant\nliterature. Additionally, it includes two kinds of OOD data shifts: Noise Shift\nand Concept Conflict Drift (CCD). Noise Shift responds to the noise level by\ncategorizing the environment into different confidence levels. On the other\nhand, CCD describes the data which has inconsistent label among the original\ndata. Finally, it tested on a variety of domain generalization models, and the\nexperimental results demonstrate the effectiveness of the proposed partition\nmethod in ADMEOOD: ADMEOOD demonstrates a significant difference performance\nbetween in-distribution and out-of-distribution data. Moreover, ERM (Empirical\nRisk Minimization) and other models exhibit distinct trends in performance\nacross different domains and measurement types.",
            "author": [
                "Shuoying Wei",
                "Xinlong Wen",
                "Lida Zhu",
                "Songquan Li",
                "Rongbo Zhu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07253v1",
                "http://arxiv.org/pdf/2310.07253v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "q-bio.QM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07240v1",
            "title": "CacheGen: Fast Context Loading for Language Model Applications",
            "updated": "2023-10-11T07:08:20Z",
            "published": "2023-10-11T07:08:20Z",
            "summary": "As large language models (LLMs) take on more complex tasks, their inputs\nincorporate longer contexts to respond to questions that require domain\nknowledge or user-specific conversational histories. Yet, using long contexts\nposes a challenge for responsive LLM systems, as nothing can be generated until\nall the contexts are fetched to and processed by the LLM. Existing systems\noptimize only the computation delay in context processing (e.g., by caching\nintermediate key-value features of the text context) but often cause longer\nnetwork delays in context fetching (e.g., key-value features consume orders of\nmagnitude larger bandwidth than the text context).\n  This paper presents CacheGen to minimize the delays in fetching and\nprocessing contexts for LLMs. CacheGen reduces the bandwidth needed for\ntransmitting long contexts' key-value (KV) features through a novel encoder\nthat compresses KV features into more compact bitstream representations. The\nencoder combines adaptive quantization with a tailored arithmetic coder, taking\nadvantage of the KV features' distributional properties, such as locality\nacross tokens. Furthermore, CacheGen minimizes the total delay in fetching and\nprocessing a context by using a controller that determines when to load the\ncontext as compressed KV features or raw text and picks the appropriate\ncompression level if loaded as KV features. We test CacheGen on three models of\nvarious sizes and three datasets of different context lengths. Compared to\nrecent methods that handle long contexts, CacheGen reduces bandwidth usage by\n3.7-4.3x and the total delay in fetching and processing contexts by 2.7-3x\nwhile maintaining similar LLM performance on various tasks as loading the text\ncontexts.",
            "author": [
                "Yuhan Liu",
                "Hanchen Li",
                "Kuntai Du",
                "Jiayi Yao",
                "Yihua Cheng",
                "Yuyang Huang",
                "Shan Lu",
                "Michael Maire",
                "Henry Hoffmann",
                "Ari Holtzman",
                "Ganesh Ananthanarayanan",
                "Junchen Jiang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07240v1",
                "http://arxiv.org/pdf/2310.07240v1"
            ],
            "primary_category": "cs.NI",
            "category": [
                "cs.NI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07239v1",
            "title": "Multidimensional Hopfield Networks for clustering",
            "updated": "2023-10-11T07:07:53Z",
            "published": "2023-10-11T07:07:53Z",
            "summary": "We present the Multidimensional Hopfield Network (DHN), a natural\ngeneralisation of the Hopfield Network. In our theoretical investigations we\nfocus on DHNs with a certain activation function and provide energy functions\nfor them. We conclude that these DHNs are convergent in finite time, and are\nequivalent to greedy methods that aim to find graph clusterings of locally\nminimal cuts. We also show that the general framework of DHNs encapsulates\nseveral previously known algorithms used for generating graph embeddings and\nclusterings. Namely, the Cleora graph embedding algorithm, the Louvain method,\nand the Newmans method can be cast as DHNs with appropriate activation function\nand update rule. Motivated by these findings we provide a generalisation of\nNewmans method to the multidimensional case.",
            "author": [
                "Gergely Stomfai",
                "\u0141ukasz Sienkiewicz",
                "Barbara Rychalska"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07239v1",
                "http://arxiv.org/pdf/2310.07239v1"
            ],
            "primary_category": "cs.NE",
            "category": [
                "cs.NE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07235v2",
            "title": "Are GATs Out of Balance?",
            "updated": "2023-10-25T15:49:30Z",
            "published": "2023-10-11T06:53:05Z",
            "summary": "While the expressive power and computational capabilities of graph neural\nnetworks (GNNs) have been theoretically studied, their optimization and\nlearning dynamics, in general, remain largely unexplored. Our study undertakes\nthe Graph Attention Network (GAT), a popular GNN architecture in which a node's\nneighborhood aggregation is weighted by parameterized attention coefficients.\nWe derive a conservation law of GAT gradient flow dynamics, which explains why\na high portion of parameters in GATs with standard initialization struggle to\nchange during training. This effect is amplified in deeper GATs, which perform\nsignificantly worse than their shallow counterparts. To alleviate this problem,\nwe devise an initialization scheme that balances the GAT network. Our approach\ni) allows more effective propagation of gradients and in turn enables\ntrainability of deeper networks, and ii) attains a considerable speedup in\ntraining and convergence time in comparison to the standard initialization. Our\nmain theorem serves as a stepping stone to studying the learning dynamics of\npositive homogeneous models with attention mechanisms.",
            "author": [
                "Nimrah Mustafa",
                "Aleksandar Bojchevski",
                "Rebekka Burkholz"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07235v2",
                "http://arxiv.org/pdf/2310.07235v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07234v1",
            "title": "Hierarchical Decomposition of Prompt-Based Continual Learning:\n  Rethinking Obscured Sub-optimality",
            "updated": "2023-10-11T06:51:46Z",
            "published": "2023-10-11T06:51:46Z",
            "summary": "Prompt-based continual learning is an emerging direction in leveraging\npre-trained knowledge for downstream continual learning, and has almost reached\nthe performance pinnacle under supervised pre-training. However, our empirical\nresearch reveals that the current strategies fall short of their full potential\nunder the more realistic self-supervised pre-training, which is essential for\nhandling vast quantities of unlabeled data in practice. This is largely due to\nthe difficulty of task-specific knowledge being incorporated into instructed\nrepresentations via prompt parameters and predicted by uninstructed\nrepresentations at test time. To overcome the exposed sub-optimality, we\nconduct a theoretical analysis of the continual learning objective in the\ncontext of pre-training, and decompose it into hierarchical components:\nwithin-task prediction, task-identity inference, and task-adaptive prediction.\nFollowing these empirical and theoretical insights, we propose Hierarchical\nDecomposition (HiDe-)Prompt, an innovative approach that explicitly optimizes\nthe hierarchical components with an ensemble of task-specific prompts and\nstatistics of both uninstructed and instructed representations, further with\nthe coordination of a contrastive regularization strategy. Our extensive\nexperiments demonstrate the superior performance of HiDe-Prompt and its\nrobustness to pre-training paradigms in continual learning (e.g., up to 15.01%\nand 9.61% lead on Split CIFAR-100 and Split ImageNet-R, respectively). Our code\nis available at \\url{https://github.com/thu-ml/HiDe-Prompt}.",
            "author": [
                "Liyuan Wang",
                "Jingyi Xie",
                "Xingxing Zhang",
                "Mingyi Huang",
                "Hang Su",
                "Jun Zhu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07234v1",
                "http://arxiv.org/pdf/2310.07234v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07229v1",
            "title": "Self-supervised Pocket Pretraining via Protein Fragment-Surroundings\n  Alignment",
            "updated": "2023-10-11T06:36:23Z",
            "published": "2023-10-11T06:36:23Z",
            "summary": "Pocket representations play a vital role in various biomedical applications,\nsuch as druggability estimation, ligand affinity prediction, and de novo drug\ndesign. While existing geometric features and pretrained representations have\ndemonstrated promising results, they usually treat pockets independent of\nligands, neglecting the fundamental interactions between them. However, the\nlimited pocket-ligand complex structures available in the PDB database (less\nthan 100 thousand non-redundant pairs) hampers large-scale pretraining\nendeavors for interaction modeling. To address this constraint, we propose a\nnovel pocket pretraining approach that leverages knowledge from high-resolution\natomic protein structures, assisted by highly effective pretrained small\nmolecule representations. By segmenting protein structures into drug-like\nfragments and their corresponding pockets, we obtain a reasonable simulation of\nligand-receptor interactions, resulting in the generation of over 5 million\ncomplexes. Subsequently, the pocket encoder is trained in a contrastive manner\nto align with the representation of pseudo-ligand furnished by some pretrained\nsmall molecule encoders. Our method, named ProFSA, achieves state-of-the-art\nperformance across various tasks, including pocket druggability prediction,\npocket matching, and ligand binding affinity prediction. Notably, ProFSA\nsurpasses other pretraining methods by a substantial margin. Moreover, our work\nopens up a new avenue for mitigating the scarcity of protein-ligand complex\ndata through the utilization of high-quality and diverse protein structure\ndatabases.",
            "author": [
                "Bowen Gao",
                "Yinjun Jia",
                "Yuanle Mo",
                "Yuyan Ni",
                "Weiying Ma",
                "Zhiming Ma",
                "Yanyan Lan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07229v1",
                "http://arxiv.org/pdf/2310.07229v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07227v1",
            "title": "On fundamental results for pushable homomorphisms of oriented graphs",
            "updated": "2023-10-11T06:29:55Z",
            "published": "2023-10-11T06:29:55Z",
            "summary": "This article deals with homomorphisms of oriented graphs with respect to push\nequivalence. Here homomorphisms refer to arc preserving vertex mappings, and\npush equivalence refers to the equivalence class of orientations of a graph $G$\nthose can be obtained from one another by reversing arcs of an edge cut. We\nstudy and prove some fundamental properties of pushable homomorphisms, and\nestablish its connections to homomorphisms of signed graphs and graph coloring.\nTo list a few highlights of this work:\n  $\\bullet$ We characterize orientations of a graph up to push equivalence and\nshow that it is possible to decide whether they are equivalent or not in\npolynomial time.\n  $\\bullet$ We give a canonical definition of pushable homomorphism - this\nanswers a natural open question.\n  $\\bullet$ We build a one-to-one correspondence between the equivalence\nclasses of oriented and signed bipartite graphs. Thus, it is possible to\ntranslate a number of important results directly from the theory of signed\ngraphs to oriented graphs. In particular, we show that pushable homomorphisms\nof bipartite graphs capture the entire theory of graph coloring as a subcase.\n  $\\bullet$ Given a graph $G$, we build a gadget oriented graph\n$\\overrightarrow{G}^{(k)}$ which admits a pushable homomorphism to a directed\nodd cycle of length $(2k+1)$ if and only if $G$ admits a $(2k+1)$-coloring.\n  We also show that it is NP-complete to determine whether an oriented (sparse)\ngraph admits a pushable homomorphism to a directed odd cycle or not.",
            "author": [
                "Tapas Das",
                "Pavan P D",
                "Sagnik Sen",
                "S Taruni"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07227v1",
                "http://arxiv.org/pdf/2310.07227v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07225v1",
            "title": "Exploring the Landscape of Large Language Models In Medical Question\n  Answering: Observations and Open Questions",
            "updated": "2023-10-11T06:26:19Z",
            "published": "2023-10-11T06:26:19Z",
            "summary": "Large Language Models (LLMs) have shown promise in medical question answering\nby achieving passing scores in standardised exams and have been suggested as\ntools for supporting healthcare workers. Deploying LLMs into such a high-risk\ncontext requires a clear understanding of the limitations of these models. With\nthe rapid development and release of new LLMs, it is especially valuable to\nidentify patterns which exist across models and may, therefore, continue to\nappear in newer versions. In this paper, we evaluate a wide range of popular\nLLMs on their knowledge of medical questions in order to better understand\ntheir properties as a group. From this comparison, we provide preliminary\nobservations and raise open questions for further research.",
            "author": [
                "Karolina Korgul",
                "Andrew M. Bean",
                "Felix Krones",
                "Robert McCraith",
                "Adam Mahdi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07225v1",
                "http://arxiv.org/pdf/2310.07225v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07223v2",
            "title": "Deep Learning for blind spectral unmixing of LULC classes with MODIS\n  multispectral time series and ancillary data",
            "updated": "2023-11-03T15:35:41Z",
            "published": "2023-10-11T06:13:50Z",
            "summary": "Remotely sensed data are dominated by mixed Land Use and Land Cover (LULC)\ntypes. Spectral unmixing is a technique to extract information from mixed\npixels into their constituent LULC types and corresponding abundance fractions.\nTraditionally, solving this task has relied on either classical methods that\nrequire prior knowledge of endmembers or machine learning methods that avoid\nexplicit endmembers calculation, also known as blind spectral unmixing (BSU).\nMost BSU studies based on Deep Learning (DL) focus on one time-step\nhyperspectral or multispectral data. To our knowledge, here we provide the\nfirst study on BSU of LULC classes using MODIS multispectral time series, in\npresence of missing data, with end-to-end DL models. We further boost the\nperformance of a Long-Short Term Memory (LSTM)-based model by incorporating\ngeographic plus topographic (geo-topographic) and climatic ancillary\ninformation. Our experiments show that combining spectral-temporal input data\ntogether with geo-topographic and climatic information substantially improves\nthe abundance estimation of LULC classes in mixed pixels. To carry out this\nstudy, we built a new labeled dataset of the region of Andalusia (Spain) with\nmonthly multispectral time series of pixels for the year 2013 from MODIS at\n460m resolution, for two hierarchical levels of LULC classes, named Andalusia\nMultiSpectral MultiTemporal Unmixing (Andalusia-MSMTU). This dataset provides,\nat the pixel level, a multispectral time series plus ancillary information\nannotated with the abundance of each LULC class inside each pixel. The dataset\n(https://zenodo.org/record/7752348##.ZBmkkezMLdo) and code\n(https://github.com/jrodriguezortega/MSMTU) are available to the public.",
            "author": [
                "Jos\u00e9 Rodr\u00edguez-Ortega",
                "Rohaifa Khaldi",
                "Domingo Alcaraz-Segura",
                "Siham Tabik"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07223v2",
                "http://arxiv.org/pdf/2310.07223v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07219v1",
            "title": "Improved Membership Inference Attacks Against Language Classification\n  Models",
            "updated": "2023-10-11T06:09:48Z",
            "published": "2023-10-11T06:09:48Z",
            "summary": "Artificial intelligence systems are prevalent in everyday life, with use\ncases in retail, manufacturing, health, and many other fields. With the rise in\nAI adoption, associated risks have been identified, including privacy risks to\nthe people whose data was used to train models. Assessing the privacy risks of\nmachine learning models is crucial to enabling knowledgeable decisions on\nwhether to use, deploy, or share a model. A common approach to privacy risk\nassessment is to run one or more known attacks against the model and measure\ntheir success rate. We present a novel framework for running membership\ninference attacks against classification models. Our framework takes advantage\nof the ensemble method, generating many specialized attack models for different\nsubsets of the data. We show that this approach achieves higher accuracy than\neither a single attack model or an attack model per class label, both on\nclassical and language classification tasks.",
            "author": [
                "Shlomit Shachor",
                "Natalia Razinkov",
                "Abigail Goldsteen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07219v1",
                "http://arxiv.org/pdf/2310.07219v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07214v1",
            "title": "Borodin-Kostochka Conjecture holds for odd-hole-free graphs",
            "updated": "2023-10-11T06:00:00Z",
            "published": "2023-10-11T06:00:00Z",
            "summary": "The Borodin-Kostochka Conjecture states that for a graph $G$, if\n$\\Delta(G)\\geq 9$, then $\\chi(G)\\leq\\max\\{\\Delta(G)-1,\\omega(G)\\}$. In this\npaper, we prove the Borodin-Kostochka Conjecture holding for odd-hole-free\ngraphs.",
            "author": [
                "Rong Chen",
                "Kaiyang Lan",
                "Xinheng Lin",
                "Yidong Zhou"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07214v1",
                "http://arxiv.org/pdf/2310.07214v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07197v1",
            "title": "MatChat: A Large Language Model and Application Service Platform for\n  Materials Science",
            "updated": "2023-10-11T05:11:46Z",
            "published": "2023-10-11T05:11:46Z",
            "summary": "The prediction of chemical synthesis pathways plays a pivotal role in\nmaterials science research. Challenges, such as the complexity of synthesis\npathways and the lack of comprehensive datasets, currently hinder our ability\nto predict these chemical processes accurately. However, recent advancements in\ngenerative artificial intelligence (GAI), including automated text generation\nand question-answering systems, coupled with fine-tuning techniques, have\nfacilitated the deployment of large-scale AI models tailored to specific\ndomains. In this study, we harness the power of the LLaMA2-7B model and enhance\nit through a learning process that incorporates 13,878 pieces of structured\nmaterial knowledge data. This specialized AI model, named MatChat, focuses on\npredicting inorganic material synthesis pathways. MatChat exhibits remarkable\nproficiency in generating and reasoning with knowledge in materials science.\nAlthough MatChat requires further refinement to meet the diverse material\ndesign needs, this research undeniably highlights its impressive reasoning\ncapabilities and innovative potential in the field of materials science.\nMatChat is now accessible online and open for use, with both the model and its\napplication framework available as open source. This study establishes a robust\nfoundation for collaborative innovation in the integration of generative AI in\nmaterials science.",
            "author": [
                "Ziyi Chen",
                "Fankai Xie",
                "Meng Wan",
                "Yang Yuan",
                "Miao Liu",
                "Zongguo Wang",
                "Sheng Meng",
                "Yangang Wang"
            ],
            "link": [
                "http://dx.doi.org/10.1088/1674-1056/ad04cb",
                "http://arxiv.org/abs/2310.07197v1",
                "http://arxiv.org/pdf/2310.07197v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07192v1",
            "title": "The Local-well-posedness of the relativistic Vlasov-Maxwell-Landau\n  system with the specular reflection boundary condition",
            "updated": "2023-10-11T04:51:50Z",
            "published": "2023-10-11T04:51:50Z",
            "summary": "We prove the local-in-time well-posedness of the relativistic\nVlasov-Maxwell-Landau system in a bounded domain $\\Omega$ with the specular\nreflection condition. Our result covers the case when $\\Omega$ is a non-convex\ndomain, e.g., solid torus. To the best of our knowledge, this is the first\nlocal well-posedness result for a nonlinear kinetic model with a\nself-consistent magnetic effect in a three-dimensional $\\textbf{bounded}$\ndomain.",
            "author": [
                "Hongjie Dong",
                "Yan Guo",
                "Zhimeng Ouyang",
                "Timur Yastrzhembskiy"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07192v1",
                "http://arxiv.org/pdf/2310.07192v1"
            ],
            "primary_category": "math.AP",
            "category": [
                "math.AP",
                "35Q83, 35Q84, 35Q61, 35K70, 35H10, 34A12"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07177v2",
            "title": "Online Speculative Decoding",
            "updated": "2023-10-17T18:02:19Z",
            "published": "2023-10-11T04:03:42Z",
            "summary": "Speculative decoding is a pivotal technique to accelerate the inference of\nlarge language models (LLMs) by employing a smaller draft model to predict the\ntarget model's outputs. However, its efficacy can be limited due to the low\npredictive accuracy of the draft model, particularly when faced with diverse\ntext inputs and a significant capability gap between the draft and target\nmodels. We introduce online speculative decoding (OSD) to address this\nchallenge. The main idea is to continually update (multiple) draft model(s) on\nobserved user query data using the abundant excess computational power in an\nLLM serving cluster. Given that LLM inference is memory-bounded, the surplus\ncomputational power in a typical LLM serving cluster can be repurposed for\nonline retraining of draft models, thereby making the training cost-neutral.\nSince the query distribution of an LLM service is relatively simple, retraining\non query distribution enables the draft model to more accurately predict the\ntarget model's outputs, particularly on data originating from query\ndistributions. As the draft model evolves online, it aligns with the query\ndistribution in real time, mitigating distribution shifts. We develop a\nprototype of online speculative decoding based on online knowledge distillation\nand evaluate it using both synthetic and real query data on several popular\nLLMs. The results show a substantial increase in the token acceptance rate by\n0.1 to 0.65, which translates into 1.22x to 3.06x latency reduction.",
            "author": [
                "Xiaoxuan Liu",
                "Lanxiang Hu",
                "Peter Bailis",
                "Ion Stoica",
                "Zhijie Deng",
                "Alvin Cheung",
                "Hao Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07177v2",
                "http://arxiv.org/pdf/2310.07177v2"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07170v1",
            "title": "PHALM: Building a Knowledge Graph from Scratch by Prompting Humans and a\n  Language Model",
            "updated": "2023-10-11T03:39:46Z",
            "published": "2023-10-11T03:39:46Z",
            "summary": "Despite the remarkable progress in natural language understanding with\npretrained Transformers, neural language models often do not handle commonsense\nknowledge well. Toward commonsense-aware models, there have been attempts to\nobtain knowledge, ranging from automatic acquisition to crowdsourcing. However,\nit is difficult to obtain a high-quality knowledge base at a low cost,\nespecially from scratch. In this paper, we propose PHALM, a method of building\na knowledge graph from scratch, by prompting both crowdworkers and a large\nlanguage model (LLM). We used this method to build a Japanese event knowledge\ngraph and trained Japanese commonsense generation models. Experimental results\nrevealed the acceptability of the built graph and inferences generated by the\ntrained models. We also report the difference in prompting humans and an LLM.\nOur code, data, and models are available at\ngithub.com/nlp-waseda/comet-atomic-ja.",
            "author": [
                "Tatsuya Ide",
                "Eiki Murata",
                "Daisuke Kawahara",
                "Takato Yamazaki",
                "Shengzhe Li",
                "Kenta Shinzato",
                "Toshinori Sato"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07170v1",
                "http://arxiv.org/pdf/2310.07170v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07159v1",
            "title": "My Brother Helps Me: Node Injection Based Adversarial Attack on Social\n  Bot Detection",
            "updated": "2023-10-11T03:09:48Z",
            "published": "2023-10-11T03:09:48Z",
            "summary": "Social platforms such as Twitter are under siege from a multitude of\nfraudulent users. In response, social bot detection tasks have been developed\nto identify such fake users. Due to the structure of social networks, the\nmajority of methods are based on the graph neural network(GNN), which is\nsusceptible to attacks. In this study, we propose a node injection-based\nadversarial attack method designed to deceive bot detection models. Notably,\nneither the target bot nor the newly injected bot can be detected when a new\nbot is added around the target bot. This attack operates in a black-box\nfashion, implying that any information related to the victim model remains\nunknown. To our knowledge, this is the first study exploring the resilience of\nbot detection through graph node injection. Furthermore, we develop an\nattribute recovery module to revert the injected node embedding from the graph\nembedding space back to the original feature space, enabling the adversary to\nmanipulate node perturbation effectively. We conduct adversarial attacks on\nfour commonly used GNN structures for bot detection on two widely used\ndatasets: Cresci-2015 and TwiBot-22. The attack success rate is over 73\\% and\nthe rate of newly injected nodes being detected as bots is below 13\\% on these\ntwo datasets.",
            "author": [
                "Lanjun Wang",
                "Xinran Qiao",
                "Yanwei Xie",
                "Weizhi Nie",
                "Yongdong Zhang",
                "Anan Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07159v1",
                "http://arxiv.org/pdf/2310.07159v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07155v2",
            "title": "\"A Tale of Two Movements\": Identifying and Comparing Perspectives in\n  #BlackLivesMatter and #BlueLivesMatter Movements-related Tweets using Weakly\n  Supervised Graph-based Structured Prediction",
            "updated": "2023-10-21T16:28:07Z",
            "published": "2023-10-11T03:01:42Z",
            "summary": "Social media has become a major driver of social change, by facilitating the\nformation of online social movements. Automatically understanding the\nperspectives driving the movement and the voices opposing it, is a challenging\ntask as annotated data is difficult to obtain. We propose a weakly supervised\ngraph-based approach that explicitly models perspectives in\n#BackLivesMatter-related tweets. Our proposed approach utilizes a\nsocial-linguistic representation of the data. We convert the text to a graph by\nbreaking it into structured elements and connect it with the social network of\nauthors, then structured prediction is done over the elements for identifying\nperspectives. Our approach uses a small seed set of labeled examples. We\nexperiment with large language models for generating artificial training\nexamples, compare them to manual annotation, and find that it achieves\ncomparable performance. We perform quantitative and qualitative analyses using\na human-annotated test set. Our model outperforms multitask baselines by a\nlarge margin, successfully characterizing the perspectives supporting and\nopposing #BLM.",
            "author": [
                "Shamik Roy",
                "Dan Goldwasser"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07155v2",
                "http://arxiv.org/pdf/2310.07155v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07138v1",
            "title": "Denoising Task Routing for Diffusion Models",
            "updated": "2023-10-11T02:23:18Z",
            "published": "2023-10-11T02:23:18Z",
            "summary": "Diffusion models generate highly realistic images through learning a\nmulti-step denoising process, naturally embodying the principles of multi-task\nlearning (MTL). Despite the inherent connection between diffusion models and\nMTL, there remains an unexplored area in designing neural architectures that\nexplicitly incorporate MTL into the framework of diffusion models. In this\npaper, we present Denoising Task Routing (DTR), a simple add-on strategy for\nexisting diffusion model architectures to establish distinct information\npathways for individual tasks within a single architecture by selectively\nactivating subsets of channels in the model. What makes DTR particularly\ncompelling is its seamless integration of prior knowledge of denoising tasks\ninto the framework: (1) Task Affinity: DTR activates similar channels for tasks\nat adjacent timesteps and shifts activated channels as sliding windows through\ntimesteps, capitalizing on the inherent strong affinity between tasks at\nadjacent timesteps. (2) Task Weights: During the early stages (higher\ntimesteps) of the denoising process, DTR assigns a greater number of\ntask-specific channels, leveraging the insight that diffusion models prioritize\nreconstructing global structure and perceptually rich contents in earlier\nstages, and focus on simple noise removal in later stages. Our experiments\ndemonstrate that DTR consistently enhances the performance of diffusion models\nacross various evaluation protocols, all without introducing additional\nparameters. Furthermore, DTR contributes to accelerating convergence during\ntraining. Finally, we show the complementarity between our architectural\napproach and existing MTL optimization techniques, providing a more complete\nview of MTL within the context of diffusion training.",
            "author": [
                "Byeongjun Park",
                "Sangmin Woo",
                "Hyojun Go",
                "Jin-Young Kim",
                "Changick Kim"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07138v1",
                "http://arxiv.org/pdf/2310.07138v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07136v1",
            "title": "Exponential Quantum Communication Advantage in Distributed Learning",
            "updated": "2023-10-11T02:19:50Z",
            "published": "2023-10-11T02:19:50Z",
            "summary": "Training and inference with large machine learning models that far exceed the\nmemory capacity of individual devices necessitates the design of distributed\narchitectures, forcing one to contend with communication constraints. We\npresent a framework for distributed computation over a quantum network in which\ndata is encoded into specialized quantum states. We prove that for certain\nmodels within this framework, inference and training using gradient descent can\nbe performed with exponentially less communication compared to their classical\nanalogs, and with relatively modest time and space complexity overheads\nrelative to standard gradient-based methods. To our knowledge, this is the\nfirst example of exponential quantum advantage for a generic class of machine\nlearning problems with dense classical data that holds regardless of the data\nencoding cost. Moreover, we show that models in this class can encode highly\nnonlinear features of their inputs, and their expressivity increases\nexponentially with model depth. We also find that, interestingly, the\ncommunication advantage nearly vanishes for simpler linear classifiers. These\nresults can be combined with natural privacy advantages in the communicated\nquantum states that limit the amount of information that can be extracted from\nthem about the data and model parameters. Taken as a whole, these findings form\na promising foundation for distributed machine learning over quantum networks.",
            "author": [
                "Dar Gilboa",
                "Jarrod R. McClean"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07136v1",
                "http://arxiv.org/pdf/2310.07136v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07123v2",
            "title": "Off-Policy Evaluation for Human Feedback",
            "updated": "2023-10-14T16:38:00Z",
            "published": "2023-10-11T01:52:42Z",
            "summary": "Off-policy evaluation (OPE) is important for closing the gap between offline\ntraining and evaluation of reinforcement learning (RL), by estimating\nperformance and/or rank of target (evaluation) policies using offline\ntrajectories only. It can improve the safety and efficiency of data collection\nand policy testing procedures in situations where online deployments are\nexpensive, such as healthcare. However, existing OPE methods fall short in\nestimating human feedback (HF) signals, as HF may be conditioned over multiple\nunderlying factors and is only sparsely available; as opposed to the\nagent-defined environmental rewards (used in policy optimization), which are\nusually determined over parametric functions or distributions. Consequently,\nthe nature of HF signals makes extrapolating accurate OPE estimations to be\nchallenging. To resolve this, we introduce an OPE for HF (OPEHF) framework that\nrevives existing OPE methods in order to accurately evaluate the HF signals.\nSpecifically, we develop an immediate human reward (IHR) reconstruction\napproach, regularized by environmental knowledge distilled in a latent space\nthat captures the underlying dynamics of state transitions as well as issuing\nHF signals. Our approach has been tested over two real-world experiments,\nadaptive in-vivo neurostimulation and intelligent tutoring, as well as in a\nsimulation environment (visual Q&A). Results show that our approach\nsignificantly improves the performance toward estimating HF signals accurately,\ncompared to directly applying (variants of) existing OPE methods.",
            "author": [
                "Qitong Gao",
                "Ge Gao",
                "Juncheng Dong",
                "Vahid Tarokh",
                "Min Chi",
                "Miroslav Pajic"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07123v2",
                "http://arxiv.org/pdf/2310.07123v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07118v2",
            "title": "Unclonable Non-Interactive Zero-Knowledge",
            "updated": "2023-10-12T19:05:03Z",
            "published": "2023-10-11T01:32:36Z",
            "summary": "A non-interactive ZK (NIZK) proof enables verification of NP statements\nwithout revealing secrets about them. However, an adversary that obtains a NIZK\nproof may be able to clone this proof and distribute arbitrarily many copies of\nit to various entities: this is inevitable for any proof that takes the form of\na classical string. In this paper, we ask whether it is possible to rely on\nquantum information in order to build NIZK proof systems that are impossible to\nclone.\n  We define and construct unclonable non-interactive zero-knowledge proofs (of\nknowledge) for NP. Besides satisfying the zero-knowledge and proof of knowledge\nproperties, these proofs additionally satisfy unclonability. Very roughly, this\nensures that no adversary can split an honestly generated proof of membership\nof an instance $x$ in an NP language $\\mathcal{L}$ and distribute copies to\nmultiple entities that all obtain accepting proofs of membership of $x$ in\n$\\mathcal{L}$. Our result has applications to unclonable signatures of\nknowledge, which we define and construct in this work; these non-interactively\nprevent replay attacks.",
            "author": [
                "Ruta Jawale",
                "Dakshita Khurana"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07118v2",
                "http://arxiv.org/pdf/2310.07118v2"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07114v3",
            "title": "Antimagicness of Tensor product for some wheel related graphs with star",
            "updated": "2023-11-24T19:16:54Z",
            "published": "2023-10-11T01:22:04Z",
            "summary": "A graph $G$ with $p$ vertices and $q$ edges has an antimagic labelling if\nthere is a bijection from the graph's edge set to the label set $\\left\\{1,2,\n\\cdots, q \\right\\}$ such that $p$ vertices must have distinct vertex sums,\nwhere the vertex sums are determined by adding up all the edge labels incident\nto each vertex $v$ in $V(G)$. Hartsfield and Ringel \\cite{Ringel1} in the book\n\"Pearls in Graph Theory\" conjectured that every connected graph is antimagic,\nwith the exception of $P_2$. In this study, we identified a class of connected\ngraphs that lend credence to the conjecture. In this article, we proved that\nthe tensor product of a wheel and a star, a helm and a star, and a flower and a\nstar is antimagic.",
            "author": [
                "Vinothkumar Latchoumanane",
                "Murugan Varadhan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07114v3",
                "http://arxiv.org/pdf/2310.07114v3"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C78, 05C76"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07104v1",
            "title": "On the edge reconstruction of the characteristic and permanental\n  polynomials of a simple graph",
            "updated": "2023-10-11T00:57:54Z",
            "published": "2023-10-11T00:57:54Z",
            "summary": "As a variant of the Ulam's vertex reconstruction conjecture and the Harary's\nedge reconstruction conjecture, Cvetkovi\\'c and Schwenk posed independently the\nfollowing problem: Can the characteristic polynomial of a simple graph $G$ with\nvertex set $V$ be reconstructed from the characteristic polynomials of all\nsubgraphs in $\\{G-v|v\\in V\\}$ for $|V|\\geq 3$? This problem is still open. A\nnatural problem is: Can the characteristic polynomial of a simple graph $G$\nwith edge set $E$ be reconstructed from the characteristic polynomials of all\nsubgraphs in $\\{G-e|e\\in E\\}$? In this paper, we prove that if $|V|\\neq |E|$,\nthen the characteristic polynomial of $G$ can be reconstructed from the\ncharacteristic polynomials of all subgraphs in $\\{G-uv, G-u-v|uv\\in E\\}$, and\nthe similar result holds for the permanental polynomial of $G$. We also prove\nthat the Laplacian (resp. signless Laplacian) characteristic polynomial of $G$\ncan be reconstructed from the Laplacian (resp. signless Laplacian)\ncharacteristic polynomials of all subgraphs in $\\{G-e|e\\in E\\}$ (resp. if\n$|V|\\neq |E|$).",
            "author": [
                "Jingyuan Zhang",
                "Xian'an Jin",
                "Weigen Yan",
                "Qinghai Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07104v1",
                "http://arxiv.org/pdf/2310.07104v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07100v1",
            "title": "GraphCloak: Safeguarding Task-specific Knowledge within Graph-structured\n  Data from Unauthorized Exploitation",
            "updated": "2023-10-11T00:50:55Z",
            "published": "2023-10-11T00:50:55Z",
            "summary": "As Graph Neural Networks (GNNs) become increasingly prevalent in a variety of\nfields, from social network analysis to protein-protein interaction studies,\ngrowing concerns have emerged regarding the unauthorized utilization of\npersonal data. Recent studies have shown that imperceptible poisoning attacks\nare an effective method of protecting image data from such misuse. However, the\nefficacy of this approach in the graph domain remains unexplored. To bridge\nthis gap, this paper introduces GraphCloak to safeguard against the\nunauthorized usage of graph data. Compared with prior work, GraphCloak offers\nunique significant innovations: (1) graph-oriented, the perturbations are\napplied to both topological structures and descriptive features of the graph;\n(2) effective and stealthy, our cloaking method can bypass various inspections\nwhile causing a significant performance drop in GNNs trained on the cloaked\ngraphs; and (3) stable across settings, our methods consistently perform\neffectively under a range of practical settings with limited knowledge. To\naddress the intractable bi-level optimization problem, we propose two\nerror-minimizing-based poisoning methods that target perturbations on the\nstructural and feature space, along with a subgraph injection poisoning method.\nOur comprehensive evaluation of these methods underscores their effectiveness,\nstealthiness, and stability. We also delve into potential countermeasures and\nprovide analytical justification for their effectiveness, paving the way for\nintriguing future research.",
            "author": [
                "Yixin Liu",
                "Chenrui Fan",
                "Xun Chen",
                "Pan Zhou",
                "Lichao Sun"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07100v1",
                "http://arxiv.org/pdf/2310.07100v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07075v1",
            "title": "Syntax Error-Free and Generalizable Tool Use for LLMs via Finite-State\n  Decoding",
            "updated": "2023-10-10T23:37:53Z",
            "published": "2023-10-10T23:37:53Z",
            "summary": "Large language models (LLMs) have shown promising capabilities in using\nexternal tools to solve complex problems. However, existing approaches either\ninvolve fine-tuning on tool demonstrations, which do not generalize to new\ntools without additional training, or providing tool documentation in context,\nlimiting the number of tools. Both approaches often generate syntactically\ninvalid tool calls. In this paper, we propose ToolDec, a finite-state\nmachine-guided decoding algorithm for tool-augmented LLMs. ToolDec eliminates\ntool-related errors for any tool-augmented LLMs by ensuring valid tool names\nand type-conforming arguments. Furthermore, ToolDec enables LLM to effectively\nselect tools using only the information contained in their names, with no need\nfor fine-tuning or in-context documentation. We evaluated multiple prior\nmethods and their ToolDec-enhanced versions on a variety of tasks involving\ntools like math functions, knowledge graph relations, and complex real-world\nRESTful APIs. Our experiments show that ToolDec reduces syntactic errors to\nzero, consequently achieving significantly better performance and as much as a\n2x speedup. We also show that ToolDec achieves superior generalization\nperformance on unseen tools, performing up to 8x better than the baselines.",
            "author": [
                "Kexun Zhang",
                "Hongqiao Chen",
                "Lei Li",
                "William Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07075v1",
                "http://arxiv.org/pdf/2310.07075v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07068v1",
            "title": "Taking the human out of decomposition-based optimization via artificial\n  intelligence: Part I. Learning when to decompose",
            "updated": "2023-10-10T23:31:06Z",
            "published": "2023-10-10T23:31:06Z",
            "summary": "In this paper, we propose a graph classification approach for automatically\ndetermining whether to use a monolithic or a decomposition-based solution\nmethod. In this approach, an optimization problem is represented as a graph\nthat captures the structural and functional coupling among the variables and\nconstraints of the problem via an appropriate set of features. Given this\nrepresentation, a graph classifier is built to determine the best solution\nmethod for a given problem. The proposed approach is used to develop a\nclassifier that determines whether a convex Mixed Integer Nonlinear Programming\nproblem should be solved using branch and bound or the outer approximation\nalgorithm. Finally, it is shown how the learned classifier can be incorporated\ninto existing mixed integer optimization solvers.",
            "author": [
                "Ilias Mitrai",
                "Prodromos Daoutidis"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07068v1",
                "http://arxiv.org/pdf/2310.07068v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07064v1",
            "title": "Large Language Models can Learn Rules",
            "updated": "2023-10-10T23:07:01Z",
            "published": "2023-10-10T23:07:01Z",
            "summary": "When prompted with a few examples and intermediate steps, large language\nmodels (LLMs) have demonstrated impressive performance in various reasoning\ntasks. However, prompting methods that rely on implicit knowledge in an LLM\noften hallucinate incorrect answers when the implicit knowledge is wrong or\ninconsistent with the task. To tackle this problem, we present\nHypotheses-to-Theories (HtT), a framework that learns a rule library for\nreasoning with LLMs. HtT contains two stages, an induction stage and a\ndeduction stage. In the induction stage, an LLM is first asked to generate and\nverify rules over a set of training examples. Rules that appear and lead to\ncorrect answers sufficiently often are collected to form a rule library. In the\ndeduction stage, the LLM is then prompted to employ the learned rule library to\nperform reasoning to answer test questions. Experiments on both numerical\nreasoning and relational reasoning problems show that HtT improves existing\nprompting methods, with an absolute gain of 11-27% in accuracy. The learned\nrules are also transferable to different models and to different forms of the\nsame problem.",
            "author": [
                "Zhaocheng Zhu",
                "Yuan Xue",
                "Xinyun Chen",
                "Denny Zhou",
                "Jian Tang",
                "Dale Schuurmans",
                "Hanjun Dai"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07064v1",
                "http://arxiv.org/pdf/2310.07064v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07059v1",
            "title": "DKEC: Domain Knowledge Enhanced Multi-Label Classification for\n  Electronic Health Records",
            "updated": "2023-10-10T22:53:15Z",
            "published": "2023-10-10T22:53:15Z",
            "summary": "Multi-label text classification (MLTC) tasks in the medical domain often face\nlong-tail label distribution, where rare classes have fewer training samples\nthan frequent classes. Although previous works have explored different model\narchitectures and hierarchical label structures to find important features,\nmost of them neglect to incorporate the domain knowledge from medical\nguidelines. In this paper, we present DKEC, Domain Knowledge Enhanced\nClassifier for medical diagnosis prediction with two innovations: (1) a\nlabel-wise attention mechanism that incorporates a heterogeneous graph and\ndomain ontologies to capture the semantic relationships between medical\nentities, (2) a simple yet effective group-wise training method based on\nsimilarity of labels to increase samples of rare classes. We evaluate DKEC on\ntwo real-world medical datasets: the RAA dataset, a collection of 4,417 patient\ncare reports from emergency medical services (EMS) incidents, and a subset of\n53,898 reports from the MIMIC-III dataset. Experimental results show that our\nmethod outperforms the state-of-the-art, particularly for the few-shot (tail)\nclasses. More importantly, we study the applicability of DKEC to different\nlanguage models and show that DKEC can help the smaller language models achieve\ncomparable performance to large language models.",
            "author": [
                "Xueren Ge",
                "Ronald Dean Williams",
                "John A. Stankovic",
                "Homa Alemzadeh"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07059v1",
                "http://arxiv.org/pdf/2310.07059v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07056v1",
            "title": "TextPSG: Panoptic Scene Graph Generation from Textual Descriptions",
            "updated": "2023-10-10T22:36:15Z",
            "published": "2023-10-10T22:36:15Z",
            "summary": "Panoptic Scene Graph has recently been proposed for comprehensive scene\nunderstanding. However, previous works adopt a fully-supervised learning\nmanner, requiring large amounts of pixel-wise densely-annotated data, which is\nalways tedious and expensive to obtain. To address this limitation, we study a\nnew problem of Panoptic Scene Graph Generation from Purely Textual Descriptions\n(Caption-to-PSG). The key idea is to leverage the large collection of free\nimage-caption data on the Web alone to generate panoptic scene graphs. The\nproblem is very challenging for three constraints: 1) no location priors; 2) no\nexplicit links between visual regions and textual entities; and 3) no\npre-defined concept sets. To tackle this problem, we propose a new framework\nTextPSG consisting of four modules, i.e., a region grouper, an entity grounder,\na segment merger, and a label generator, with several novel techniques. The\nregion grouper first groups image pixels into different segments and the entity\ngrounder then aligns visual segments with language entities based on the\ntextual description of the segment being referred to. The grounding results can\nthus serve as pseudo labels enabling the segment merger to learn the segment\nsimilarity as well as guiding the label generator to learn object semantics and\nrelation predicates, resulting in a fine-grained structured scene\nunderstanding. Our framework is effective, significantly outperforming the\nbaselines and achieving strong out-of-distribution robustness. We perform\ncomprehensive ablation studies to corroborate the effectiveness of our design\nchoices and provide an in-depth analysis to highlight future directions. Our\ncode, data, and results are available on our project page:\nhttps://vis-www.cs.umass.edu/TextPSG.",
            "author": [
                "Chengyang Zhao",
                "Yikang Shen",
                "Zhenfang Chen",
                "Mingyu Ding",
                "Chuang Gan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07056v1",
                "http://arxiv.org/pdf/2310.07056v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07045v1",
            "title": "Structural convergence and algebraic roots",
            "updated": "2023-10-10T22:18:54Z",
            "published": "2023-10-10T22:18:54Z",
            "summary": "Structural convergence is a framework for convergence of graphs by\nNe\\v{s}et\\v{r}il and Ossona de Mendez that unifies the dense (left) graph\nconvergence and Benjamini-Schramm convergence. They posed a problem asking\nwhether for a given sequence of graphs $(G_n)$ converging to a limit $L$ and a\nvertex $r$ of $L$ it is possible to find a sequence of vertices $(r_n)$ such\nthat $L$ rooted at $r$ is the limit of the graphs $G_n$ rooted at $r_n$. A\ncounterexample was found by Christofides and Kr\\'{a}l', but they showed that\nthe statement holds for almost all vertices $r$ of $L$. We offer another\nperspective to the original problem by considering the size of definable sets\nto which the root $r$ belongs. We prove that if $r$ is an algebraic vertex\n(i.e. belongs to a finite definable set), the sequence of roots $(r_n)$ always\nexists.",
            "author": [
                "David Hartman",
                "Tom\u00e1\u0161 Hons",
                "Jaroslav Ne\u0161et\u0159il"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07045v1",
                "http://arxiv.org/pdf/2310.07045v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "cs.DM",
                "03C98, 05C99, 05C63"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07040v1",
            "title": "Degree-penalized contact processes",
            "updated": "2023-10-10T22:00:22Z",
            "published": "2023-10-10T22:00:22Z",
            "summary": "We study degree-penalized contact processes on Galton-Watson trees (GW) and\nthe configuration model. The model we consider is a modification of the usual\ncontact process on a graph. In particular, each vertex can be either infected\nor healthy. When infected, each vertex heals at rate one. Also, when infected,\na vertex $v$ with degree $d_v$ infects its neighboring vertex $u$ with degree\n$d_u$ with rate $\\lambda/f(d_u, d_v)$ for some positive function $f$. In the\ncase $f(d_u, d_v)=\\max(d_u, d_v)^\\mu$ for some $\\mu>0$, the infection is slowed\ndown to and from high degree vertices. This is in line with arguments used in\nsocial network science: people with many contacts do not have the time to\ninfect their neighbors at the same rate as people with fewer contacts.\n  We show that new phase transitions occur in terms of the parameter $\\mu$ (at\n$1/2$) and the degree distribution $D$ of the GW tree.\n  - When $\\mu\\ge 1$, the process goes extinct for all distributions $D$ for all\nsufficiently small $\\lambda>0$;\n  - When $\\mu\\in(1/2, 1)$, and the tail of $D$ weakly follows a power law with\ntail-exponent less than $1-\\mu$, the process survives globally but not locally\nfor all $\\lambda$ small enough;\n  - When $\\mu\\in(1/2, 1)$, and $\\mathbb{E}[D^{1-\\mu}]<\\infty$, the process goes\nextinct almost surely, for all $\\lambda$ small enough;\n  - When $\\mu<1/2$, and $D$ is heavier then stretched exponential with\nstretch-exponent $1-2\\mu$, the process survives (locally) with positive\nprobability for all $\\lambda>0$.\n  We also study the product case $f(x,y)=(xy)^\\mu$. In that case, the situation\nfor $\\mu < 1/2$ is the same as the one described above, but $\\mu\\ge 1/2$ always\nleads to a subcritical contact process for small enough $\\lambda>0$ on all\ngraphs. Furthermore, for finite random graphs with prescribed degree sequences,\nwe establish the corresponding phase transitions in terms of the length of\nsurvival.",
            "author": [
                "Zsolt Bartha",
                "J\u00falia Komj\u00e1thy",
                "Daniel Valesin"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07040v1",
                "http://arxiv.org/pdf/2310.07040v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR",
                "82C22 (Primary) 60K35, 05C80, 60J85 (Secondary)"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07039v1",
            "title": "Lipschitz Interpolation: Non-parametric Convergence under Bounded\n  Stochastic Noise",
            "updated": "2023-10-10T21:56:58Z",
            "published": "2023-10-10T21:56:58Z",
            "summary": "This paper examines the asymptotic convergence properties of Lipschitz\ninterpolation methods within the context of bounded stochastic noise. In the\nfirst part of the paper, we establish probabilistic consistency guarantees of\nthe classical approach in a general setting and derive upper bounds on the\nuniform convergence rates. These bounds align with well-established optimal\nrates of non-parametric regression obtained in related settings and provide new\nprecise upper bounds on the non-parametric regression problem under bounded\nnoise assumptions. Practically, they can serve as a theoretical tool for\ncomparing Lipschitz interpolation to alternative non-parametric regression\nmethods, providing a condition on the behaviour of the noise at the boundary of\nits support which indicates when Lipschitz interpolation should be expected to\nasymptotically outperform or underperform other approaches. In the second part,\nwe expand upon these results to include asymptotic guarantees for online\nlearning of dynamics in discrete-time stochastic systems and illustrate their\nutility in deriving closed-loop stability guarantees of a simple controller. We\nalso explore applications where the main assumption of prior knowledge of the\nLipschitz constant is removed by adopting the LACKI framework (Calliess et al.\n(2020)) and deriving general asymptotic consistency.",
            "author": [
                "Julien Walden Huang",
                "Stephen Roberts",
                "Jan-Peter Calliess"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07039v1",
                "http://arxiv.org/pdf/2310.07039v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "math.ST",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07020v1",
            "title": "Comparing the sets of volume polynomials and Lorentzian Polynomials",
            "updated": "2023-10-10T21:13:29Z",
            "published": "2023-10-10T21:13:29Z",
            "summary": "Given n convex bodies in the real space of dimension d, we consider the set\nof homogeneous polynomials of degree d in n variables that can be represented\nas their volume polynomial. This set is a subset of the set of Lorentzian\npolynomials. Using our knowledge of operations that preserve the Lorentzian\nproperty, we give a complete classification of the cases when the two sets are\nequal.",
            "author": [
                "Amelie Menges"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07020v1",
                "http://arxiv.org/pdf/2310.07020v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "math.AG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07018v1",
            "title": "NEWTON: Are Large Language Models Capable of Physical Reasoning?",
            "updated": "2023-10-10T21:08:51Z",
            "published": "2023-10-10T21:08:51Z",
            "summary": "Large Language Models (LLMs), through their contextualized representations,\nhave been empirically proven to encapsulate syntactic, semantic, word sense,\nand common-sense knowledge. However, there has been limited exploration of\ntheir physical reasoning abilities, specifically concerning the crucial\nattributes for comprehending everyday objects. To address this gap, we\nintroduce NEWTON, a repository and benchmark for evaluating the physics\nreasoning skills of LLMs. Further, to enable domain-specific adaptation of this\nbenchmark, we present a pipeline to enable researchers to generate a variant of\nthis benchmark that has been customized to the objects and attributes relevant\nfor their application. The NEWTON repository comprises a collection of 2800\nobject-attribute pairs, providing the foundation for generating infinite-scale\nassessment templates. The NEWTON benchmark consists of 160K QA questions,\ncurated using the NEWTON repository to investigate the physical reasoning\ncapabilities of several mainstream language models across foundational,\nexplicit, and implicit reasoning tasks. Through extensive empirical analysis,\nour results highlight the capabilities of LLMs for physical reasoning. We find\nthat LLMs like GPT-4 demonstrate strong reasoning capabilities in\nscenario-based tasks but exhibit less consistency in object-attribute reasoning\ncompared to humans (50% vs. 84%). Furthermore, the NEWTON platform demonstrates\nits potential for evaluating and enhancing language models, paving the way for\ntheir integration into physically grounded settings, such as robotic\nmanipulation. Project site: https://newtonreasoning.github.io",
            "author": [
                "Yi Ru Wang",
                "Jiafei Duan",
                "Dieter Fox",
                "Siddhartha Srinivasa"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07018v1",
                "http://arxiv.org/pdf/2310.07018v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07015v1",
            "title": "Neural Relational Inference with Fast Modular Meta-learning",
            "updated": "2023-10-10T21:05:13Z",
            "published": "2023-10-10T21:05:13Z",
            "summary": "\\textit{Graph neural networks} (GNNs) are effective models for many dynamical\nsystems consisting of entities and relations. Although most GNN applications\nassume a single type of entity and relation, many situations involve multiple\ntypes of interactions. \\textit{Relational inference} is the problem of\ninferring these interactions and learning the dynamics from observational data.\nWe frame relational inference as a \\textit{modular meta-learning} problem,\nwhere neural modules are trained to be composed in different ways to solve many\ntasks. This meta-learning framework allows us to implicitly encode time\ninvariance and infer relations in context of one another rather than\nindependently, which increases inference capacity. Framing inference as the\ninner-loop optimization of meta-learning leads to a model-based approach that\nis more data-efficient and capable of estimating the state of entities that we\ndo not observe directly, but whose existence can be inferred from their effect\non observed entities. To address the large search space of graph neural network\ncompositions, we meta-learn a \\textit{proposal function} that speeds up the\ninner-loop simulated annealing search within the modular meta-learning\nalgorithm, providing two orders of magnitude increase in the size of problems\nthat can be addressed.",
            "author": [
                "Ferran Alet",
                "Erica Weng",
                "Tom\u00e1s Lozano P\u00e9rez",
                "Leslie Pack Kaelbling"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07015v1",
                "http://arxiv.org/pdf/2310.07015v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.07008v1",
            "title": "Answer Candidate Type Selection: Text-to-Text Language Model for Closed\n  Book Question Answering Meets Knowledge Graphs",
            "updated": "2023-10-10T20:49:43Z",
            "published": "2023-10-10T20:49:43Z",
            "summary": "Pre-trained Text-to-Text Language Models (LMs), such as T5 or BART yield\npromising results in the Knowledge Graph Question Answering (KGQA) task.\nHowever, the capacity of the models is limited and the quality decreases for\nquestions with less popular entities. In this paper, we present a novel\napproach which works on top of the pre-trained Text-to-Text QA system to\naddress this issue. Our simple yet effective method performs filtering and\nre-ranking of generated candidates based on their types derived from Wikidata\n\"instance_of\" property.",
            "author": [
                "Mikhail Salnikov",
                "Maria Lysyuk",
                "Pavel Braslavski",
                "Anton Razzhigaev",
                "Valentin Malykh",
                "Alexander Panchenko"
            ],
            "link": [
                "http://arxiv.org/abs/2310.07008v1",
                "http://arxiv.org/pdf/2310.07008v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.IR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06995v1",
            "title": "Accelerated Modelling of Interfaces for Electronic Devices using Graph\n  Neural Networks",
            "updated": "2023-10-10T20:26:46Z",
            "published": "2023-10-10T20:26:46Z",
            "summary": "Modern microelectronic devices are composed of interfaces between a large\nnumber of materials, many of which are in amorphous or polycrystalline phases.\nModeling such non-crystalline materials using first-principles methods such as\ndensity functional theory is often numerically intractable. Recently, graph\nneural networks (GNNs) have shown potential to achieve linear complexity with\naccuracies comparable to ab-initio methods. Here, we demonstrate the\napplicability of GNNs to accelerate the atomistic computational pipeline for\npredicting macroscopic transistor transport characteristics via learning\nmicroscopic physical properties. We generate amorphous heterostructures,\nspecifically the HfO$_{2}$-SiO$_{2}$-Si semiconductor-dielectric transistor\ngate stack, via GNN predicted atomic forces, and show excellent accuracy in\npredicting transport characteristics including injection velocity for nanoslab\nsilicon channels. This work paves the way for faster and more scalable methods\nto model modern advanced electronic devices via GNNs.",
            "author": [
                "Pratik Brahma",
                "Krishnakumar Bhattaram",
                "Sayeef Salahuddin"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06995v1",
                "http://arxiv.org/pdf/2310.06995v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci",
                "cond-mat.dis-nn"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06975v1",
            "title": "Reconfigurable Intelligent Surfaces-Enabled Intra-Cell Pilot Reuse in\n  Massive MIMO Systems",
            "updated": "2023-10-10T19:55:19Z",
            "published": "2023-10-10T19:55:19Z",
            "summary": "Channel state information (CSI) estimation is a critical issue in the design\nof modern massive multiple-input multiple-output (mMIMO) networks. With the\nincreasing number of users, assigning orthogonal pilots to everyone incurs a\nlarge overhead that strongly penalizes the system's spectral efficiency (SE).\nIt becomes thus necessary to reuse pilots, giving rise to pilot contamination,\na vital performance bottleneck of mMIMO networks. Reusing pilots among the\nusers of the same cell is a desirable operation condition from the perspective\nof reducing training overheads; however, the intra-cell pilot contamination\nmight worsen due to the users' proximity. Reconfigurable intelligent surfaces\n(RISs), capable of smartly controlling the wireless channel, can be leveraged\nfor intra-cell pilot reuse. In this paper, our main contribution is a RIS-aided\napproach for intra-cell pilot reuse and the corresponding channel estimation\nmethod. Relying upon the knowledge of only statistical CSI, we optimize the RIS\nphase shifts based on a manifold optimization framework and the RIS positioning\nbased on a deterministic approach. The extensive numerical results highlight\nthe remarkable performance improvements the proposed scheme achieves (for both\nuplink and downlink transmissions) compared to other alternatives.",
            "author": [
                "Jose Carlos Marinello Filho",
                "Taufik Abrao",
                "Ekram Hossain",
                "Amine Mezghani"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06975v1",
                "http://arxiv.org/pdf/2310.06975v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "eess.SP",
                "math.IT",
                "stat.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06973v1",
            "title": "Federated Quantum Machine Learning with Differential Privacy",
            "updated": "2023-10-10T19:52:37Z",
            "published": "2023-10-10T19:52:37Z",
            "summary": "The preservation of privacy is a critical concern in the implementation of\nartificial intelligence on sensitive training data. There are several\ntechniques to preserve data privacy but quantum computations are inherently\nmore secure due to the no-cloning theorem, resulting in a most desirable\ncomputational platform on top of the potential quantum advantages. There have\nbeen prior works in protecting data privacy by Quantum Federated Learning (QFL)\nand Quantum Differential Privacy (QDP) studied independently. However, to the\nbest of our knowledge, no prior work has addressed both QFL and QDP together\nyet. Here, we propose to combine these privacy-preserving methods and implement\nthem on the quantum platform, so that we can achieve comprehensive protection\nagainst data leakage (QFL) and model inversion attacks (QDP). This\nimplementation promises more efficient and secure artificial intelligence. In\nthis paper, we present a successful implementation of these\nprivacy-preservation methods by performing the binary classification of the\nCats vs Dogs dataset. Using our quantum-classical machine learning model, we\nobtained a test accuracy of over 0.98, while maintaining epsilon values less\nthan 1.3. We show that federated differentially private training is a viable\nprivacy preservation method for quantum machine learning on Noisy\nIntermediate-Scale Quantum (NISQ) devices.",
            "author": [
                "Rod Rofougaran",
                "Shinjae Yoo",
                "Huan-Hsin Tseng",
                "Samuel Yen-Chi Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06973v1",
                "http://arxiv.org/pdf/2310.06973v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06970v2",
            "title": "Flood and Echo: Algorithmic Alignment of GNNs with Distributed Computing",
            "updated": "2023-10-12T14:21:52Z",
            "published": "2023-10-10T19:47:58Z",
            "summary": "Graph Neural Networks are a natural fit for learning algorithms. They can\ndirectly represent tasks through an abstract but versatile graph structure and\nhandle inputs of different sizes. This opens up the possibility for scaling and\nextrapolation to larger graphs, one of the most important advantages of an\nalgorithm. However, this raises two core questions i) How can we enable nodes\nto gather the required information in a given graph ($\\textit{information\nexchange}$), even if is far away and ii) How can we design an execution\nframework which enables this information exchange for extrapolation to larger\ngraph sizes ($\\textit{algorithmic alignment for extrapolation}$). We propose a\nnew execution framework that is inspired by the design principles of\ndistributed algorithms: Flood and Echo Net. It propagates messages through the\nentire graph in a wave like activation pattern, which naturally generalizes to\nlarger instances. Through its sparse but parallel activations it is provably\nmore efficient in terms of message complexity. We study the proposed model and\nprovide both empirical evidence and theoretical insights in terms of its\nexpressiveness, efficiency, information exchange and ability to extrapolate.",
            "author": [
                "Jo\u00ebl Mathys",
                "Florian Gr\u00f6tschla",
                "Kalyan Varma Nadimpalli",
                "Roger Wattenhofer"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06970v2",
                "http://arxiv.org/pdf/2310.06970v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06966v1",
            "title": "On the Interpretability of Part-Prototype Based Classifiers: A Human\n  Centric Analysis",
            "updated": "2023-10-10T19:32:59Z",
            "published": "2023-10-10T19:32:59Z",
            "summary": "Part-prototype networks have recently become methods of interest as an\ninterpretable alternative to many of the current black-box image classifiers.\nHowever, the interpretability of these methods from the perspective of human\nusers has not been sufficiently explored. In this work, we have devised a\nframework for evaluating the interpretability of part-prototype-based models\nfrom a human perspective. The proposed framework consists of three actionable\nmetrics and experiments. To demonstrate the usefulness of our framework, we\nperformed an extensive set of experiments using Amazon Mechanical Turk. They\nnot only show the capability of our framework in assessing the interpretability\nof various part-prototype-based models, but they also are, to the best of our\nknowledge, the most comprehensive work on evaluating such methods in a unified\nframework.",
            "author": [
                "Omid Davoodi",
                "Shayan Mohammadizadehsamakosh",
                "Majid Komeili"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06966v1",
                "http://arxiv.org/pdf/2310.06966v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.HC",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06941v1",
            "title": "A framework for characterizing covariational reasoning in physics",
            "updated": "2023-10-10T18:58:55Z",
            "published": "2023-10-10T18:58:55Z",
            "summary": "Covariational reasoning--considering how changes in one quantity affect\nanother, related quantity--is a foundation of quantitative modeling in physics.\nUnderstanding quantitative models is a learning objective of introductory\nphysics instruction at the college level. Prior work suggests that\ncovariational reasoning in physics contexts differs in important ways from\nreasoning about functions and graphs in purely mathematical contexts; this\nreasoning is effortful in physics even for mathematically well-prepared\nstudents. In order to help students learn to reason covariationally in physics\ncontexts, we need to characterize what we mean by physics covariational\nreasoning. To this end, we present a framework of covariational reasoning in\nphysics contexts, to describe the ways that covariational reasoning is used in\nphysics modeling. The framework can be used as a tool by which instructors can\nrecognize physics covariational reasoning patterns and researchers can analyze\nstudent reasoning. The framework can also help inform the development of\neffective instructional materials and methods.",
            "author": [
                "Alexis Olsho",
                "Charlotte Zimmerman",
                "Suzanne White Brahmia"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06941v1",
                "http://arxiv.org/pdf/2310.06941v1"
            ],
            "primary_category": "physics.ed-ph",
            "category": [
                "physics.ed-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06936v1",
            "title": "LLMs Killed the Script Kiddie: How Agents Supported by Large Language\n  Models Change the Landscape of Network Threat Testing",
            "updated": "2023-10-10T18:49:20Z",
            "published": "2023-10-10T18:49:20Z",
            "summary": "In this paper, we explore the potential of Large Language Models (LLMs) to\nreason about threats, generate information about tools, and automate cyber\ncampaigns. We begin with a manual exploration of LLMs in supporting specific\nthreat-related actions and decisions. We proceed by automating the decision\nprocess in a cyber campaign. We present prompt engineering approaches for a\nplan-act-report loop for one action of a threat campaign and and a prompt\nchaining design that directs the sequential decision process of a multi-action\ncampaign. We assess the extent of LLM's cyber-specific knowledge w.r.t the\nshort campaign we demonstrate and provide insights into prompt design for\neliciting actionable responses. We discuss the potential impact of LLMs on the\nthreat landscape and the ethical considerations of using LLMs for accelerating\nthreat actor capabilities. We report a promising, yet concerning, application\nof generative AI to cyber threats. However, the LLM's capabilities to deal with\nmore complex networks, sophisticated vulnerabilities, and the sensitivity of\nprompts are open questions. This research should spur deliberations over the\ninevitable advancements in LLM-supported cyber adversarial landscape.",
            "author": [
                "Stephen Moskal",
                "Sam Laney",
                "Erik Hemberg",
                "Una-May O'Reilly"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06936v1",
                "http://arxiv.org/pdf/2310.06936v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06906v1",
            "title": "Distillation Improves Visual Place Recognition for Low-Quality Queries",
            "updated": "2023-10-10T18:03:29Z",
            "published": "2023-10-10T18:03:29Z",
            "summary": "The shift to online computing for real-time visual localization often\nrequires streaming query images/videos to a server for visual place recognition\n(VPR), where fast video transmission may result in reduced resolution or\nincreased quantization. This compromises the quality of global image\ndescriptors, leading to decreased VPR performance. To improve the low recall\nrate for low-quality query images, we present a simple yet effective method\nthat uses high-quality queries only during training to distill better feature\nrepresentations for deep-learning-based VPR, such as NetVLAD. Specifically, we\nuse mean squared error (MSE) loss between the global descriptors of queries\nwith different qualities, and inter-channel correlation knowledge distillation\n(ICKD) loss over their corresponding intermediate features. We validate our\napproach using the both Pittsburgh 250k dataset and our own indoor dataset with\nvarying quantization levels. By fine-tuning NetVLAD parameters with our\ndistillation-augmented losses, we achieve notable VPR recall-rate improvements\nover low-quality queries, as demonstrated in our extensive experimental\nresults. We believe this work not only pushes forward the VPR research but also\nprovides valuable insights for applications needing dependable place\nrecognition under resource-limited conditions.",
            "author": [
                "Anbang Yang",
                "Yao Wang",
                "John-Ross Rizzo",
                "Chen Feng"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06906v1",
                "http://arxiv.org/pdf/2310.06906v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06826v1",
            "title": "Finding cliques and dense subgraphs using edge queries",
            "updated": "2023-10-10T17:56:12Z",
            "published": "2023-10-10T17:56:12Z",
            "summary": "We consider the problem of finding a large clique in an Erd\\H{o}s--R\\'enyi\nrandom graph where we are allowed unbounded computational time but can only\nquery a limited number of edges. Recall that the largest clique in $G \\sim\nG(n,1/2)$ has size roughly $2\\log_{2} n$. Let $\\alpha_{\\star}(\\delta,\\ell)$ be\nthe supremum over $\\alpha$ such that there exists an algorithm that makes\n$n^{\\delta}$ queries in total to the adjacency matrix of $G$, in a constant\n$\\ell$ number of rounds, and outputs a clique of size $\\alpha \\log_{2} n$ with\nhigh probability. We give improved upper bounds on\n$\\alpha_{\\star}(\\delta,\\ell)$ for every $\\delta \\in [1,2)$ and $\\ell \\geq 3$.\nWe also study analogous questions for finding subgraphs with density at least\n$\\eta$ for a given $\\eta$, and prove corresponding impossibility results.",
            "author": [
                "Endre Cs\u00f3ka",
                "Andr\u00e1s Pongr\u00e1cz"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06826v1",
                "http://arxiv.org/pdf/2310.06826v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "cs.DM",
                "05C80, 05C85, 68Q87, 68W20"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06803v1",
            "title": "Advancing Transformer's Capabilities in Commonsense Reasoning",
            "updated": "2023-10-10T17:21:03Z",
            "published": "2023-10-10T17:21:03Z",
            "summary": "Recent advances in general purpose pre-trained language models have shown\ngreat potential in commonsense reasoning. However, current works still perform\npoorly on standard commonsense reasoning benchmarks including the Com2Sense\nDataset. We argue that this is due to a disconnect with current cutting-edge\nmachine learning methods. In this work, we aim to bridge the gap by introducing\ncurrent ML-based methods to improve general purpose pre-trained language models\nin the task of commonsense reasoning. Specifically, we experiment with and\nsystematically evaluate methods including knowledge transfer, model ensemble,\nand introducing an additional pairwise contrastive objective. Our best model\noutperforms the strongest previous works by ~15\\% absolute gains in Pairwise\nAccuracy and ~8.7\\% absolute gains in Standard Accuracy.",
            "author": [
                "Yu Zhou",
                "Yunqiu Han",
                "Hanyu Zhou",
                "Yulun Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06803v1",
                "http://arxiv.org/pdf/2310.06803v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.13710v1",
            "title": "Exploring the Creation and Humanization of Digital Life: Consciousness\n  Simulation and Human-Machine Interaction",
            "updated": "2023-10-10T17:16:57Z",
            "published": "2023-10-10T17:16:57Z",
            "summary": "Digital life, a form of life generated by computer programs or artificial\nintelligence systems, it possesses self-awareness, thinking abilities,\nemotions, and subjective consciousness. Achieving it involves complex neural\nnetworks, multi-modal sensory integration [1, 2], feedback mechanisms, and\nself-referential processing [3]. Injecting prior knowledge into digital life\nstructures is a critical step. It guides digital entities' understanding of the\nworld, decision-making, and interactions. We can customize and personalize\ndigital life, it includes adjusting intelligence levels, character settings,\npersonality traits, and behavioral characteristics. Virtual environments\nfacilitate efficient and controlled development, allowing user interaction,\nobservation, and active participation in digital life's growth. Researchers\nbenefit from controlled experiments, driving technological advancements. The\nfusion of digital life into the real world offers exciting possibilities for\nhuman-digital entity collaboration and coexistence.",
            "author": [
                "Qikang Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.13710v1",
                "http://arxiv.org/pdf/2310.13710v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06778v1",
            "title": "How Knowledge Workers Think Generative AI Will (Not) Transform Their\n  Industries",
            "updated": "2023-10-10T16:53:52Z",
            "published": "2023-10-10T16:53:52Z",
            "summary": "Generative AI is expected to have transformative effects in multiple\nknowledge industries. To better understand how knowledge workers expect\ngenerative AI may affect their industries in the future, we conducted\nparticipatory research workshops for seven different industries, with a total\nof 54 participants across three US cities. We describe participants'\nexpectations of generative AI's impact, including a dominant narrative that cut\nacross the groups' discourse: participants largely envision generative AI as a\ntool to perform menial work, under human review. Participants do not generally\nanticipate the disruptive changes to knowledge industries currently projected\nin common media and academic narratives. Participants do however envision\ngenerative AI may amplify four social forces currently shaping their\nindustries: deskilling, dehumanization, disconnection, and disinformation. We\ndescribe these forces, and then we provide additional detail regarding\nattitudes in specific knowledge industries. We conclude with a discussion of\nimplications and research challenges for the HCI community.",
            "author": [
                "Allison Woodruff",
                "Renee Shelby",
                "Patrick Gage Kelley",
                "Steven Rousso-Schindler",
                "Jamila Smith-Loud",
                "Lauren Wilcox"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06778v1",
                "http://arxiv.org/pdf/2310.06778v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY",
                "K.4.1; K.4.2; K.4.3"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06765v1",
            "title": "Efficient Graduated Non-Convexity for Pose Graph Optimization",
            "updated": "2023-10-10T16:40:38Z",
            "published": "2023-10-10T16:40:38Z",
            "summary": "We propose a novel approach to Graduated Non-Convexity (GNC) and demonstrate\nits efficacy through its application in robust pose graph optimization, a key\ncomponent in SLAM backends. Traditional GNC methods often rely on heuristic\nmethods for GNC schedule, updating control parameter {\\mu} for escalating the\nnon-convexity. In contrast, our approach leverages the properties of convex\nfunctions and convex optimization to identify the boundary points beyond which\nconvexity is no longer guaranteed, thereby eliminating redundant optimization\nsteps in existing methodologies and enhancing both speed and robustness. We\nshow that our method outperforms the state-of-the-art method in terms of speed\nand accuracy when used for robust back-end pose graph optimization via GNC. Our\nwork builds upon and enhances the open-source riSAM framework. Our\nimplementation can be accessed from: https://github.com/SNU-DLLAB/EGNC-PGO",
            "author": [
                "Wonseok Kang",
                "Jaehyun Kim",
                "Jiseong Chung",
                "Seungwon Choi",
                "Tae-wan Kim"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06765v1",
                "http://arxiv.org/pdf/2310.06765v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06732v1",
            "title": "Graph-Based Analysis and Visualisation of Mobility Data",
            "updated": "2023-10-10T15:57:59Z",
            "published": "2023-10-10T15:57:59Z",
            "summary": "Urban mobility forecast and analysis can be addressed through grid-based and\ngraph-based models. However, graph-based representations have the advantage of\nmore realistically depicting the mobility networks and being more robust since\nthey allow the implementation of Graph Theory machinery, enhancing the analysis\nand visualisation of mobility flows. We define two types of mobility graphs:\nRegion Adjacency graphs and Origin-Destination graphs. Several node centrality\nmetrics of graphs are applied to identify the most relevant nodes of the\nnetwork in terms of graph connectivity. Additionally, the Perron vector\nassociated with a strongly connected graph is applied to define a circulation\nfunction on the mobility graph. Such node values are visualised in the\ngeographically embedded graphs, showing clustering patterns within the network.\nSince mobility graphs can be directed or undirected, we define several Graph\nLaplacian for both cases and show that these matrices and their spectral\nproperties provide insightful information for network analysis. The computation\nof node centrality metrics and Perron-induced circulation functions for three\ndifferent geographical regions demonstrate that basic elements from Graph\nTheory applied to mobility networks can lead to structure analysis for graphs\nof different connectivity, size, and orientation properties.",
            "author": [
                "Rafael Mart\u00ednez M\u00e1rquez",
                "Giuseppe Patan\u00e8"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06732v1",
                "http://arxiv.org/pdf/2310.06732v1"
            ],
            "primary_category": "cs.DM",
            "category": [
                "cs.DM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06725v2",
            "title": "Growing ecosystem of deep learning methods for modeling\n  protein$\\unicode{x2013}$protein interactions",
            "updated": "2023-12-06T19:04:26Z",
            "published": "2023-10-10T15:53:27Z",
            "summary": "Numerous cellular functions rely on protein$\\unicode{x2013}$protein\ninteractions. Efforts to comprehensively characterize them remain challenged\nhowever by the diversity of molecular recognition mechanisms employed within\nthe proteome. Deep learning has emerged as a promising approach for tackling\nthis problem by exploiting both experimental data and basic biophysical\nknowledge about protein interactions. Here, we review the growing ecosystem of\ndeep learning methods for modeling protein interactions, highlighting the\ndiversity of these biophysically-informed models and their respective\ntrade-offs. We discuss recent successes in using representation learning to\ncapture complex features pertinent to predicting protein interactions and\ninteraction sites, geometric deep learning to reason over protein structures\nand predict complex structures, and generative modeling to design de novo\nprotein assemblies. We also outline some of the outstanding challenges and\npromising new directions. Opportunities abound to discover novel interactions,\nelucidate their physical mechanisms, and engineer binders to modulate their\nfunctions using deep learning and, ultimately, unravel how protein interactions\norchestrate complex cellular behaviors.",
            "author": [
                "Julia R. Rogers",
                "Gerg\u0151 Nikol\u00e9nyi",
                "Mohammed AlQuraishi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06725v2",
                "http://arxiv.org/pdf/2310.06725v2"
            ],
            "primary_category": "q-bio.BM",
            "category": [
                "q-bio.BM",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06710v1",
            "title": "Zero-Shot Transfer in Imitation Learning",
            "updated": "2023-10-10T15:36:58Z",
            "published": "2023-10-10T15:36:58Z",
            "summary": "We present an algorithm that learns to imitate expert behavior and can\ntransfer to previously unseen domains without retraining. Such an algorithm is\nextremely relevant in real-world applications such as robotic learning because\n1) reward functions are difficult to design, 2) learned policies from one\ndomain are difficult to deploy in another domain and 3) learning directly in\nthe real world is either expensive or unfeasible due to security concerns. To\novercome these constraints, we combine recent advances in Deep RL by using an\nAnnealedVAE to learn a disentangled state representation and imitate an expert\nby learning a single Q-function which avoids adversarial training. We\ndemonstrate the effectiveness of our method in 3 environments ranging in\ndifficulty and the type of transfer knowledge required.",
            "author": [
                "Alvaro Cauderan",
                "Gauthier Boeshertz",
                "Florian Schwarb",
                "Calvin Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06710v1",
                "http://arxiv.org/pdf/2310.06710v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06708v1",
            "title": "Adjustment with Three Continuous Variables",
            "updated": "2023-10-10T15:33:59Z",
            "published": "2023-10-10T15:33:59Z",
            "summary": "Spurious association between X and Y may be due to a confounding variable W.\nStatisticians may adjust for W using a variety of techniques. This paper\npresents the results of simulations conducted to assess the performance of\nthose techniques under various, elementary, data-generating processes. The\nresults indicate that no technique is best overall and that specific techniques\nshould be selected based on the particulars of the data-generating process.\nHere we show how causal graphs can guide the selection or design of techniques\nfor statistical adjustment. R programs are provided for researchers interested\nin generalization.",
            "author": [
                "Brian Knaeble"
            ],
            "link": [
                "http://dx.doi.org/10.1080/03610918.2017.1390128",
                "http://arxiv.org/abs/2310.06708v1",
                "http://arxiv.org/pdf/2310.06708v1"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06698v2",
            "title": "Simulating the Transverse Field Ising Model on the Kagome Lattice using\n  a Programmable Quantum Annealer",
            "updated": "2023-11-06T20:35:33Z",
            "published": "2023-10-10T15:22:01Z",
            "summary": "The presence of competing interactions due to geometry leads to frustration\nin quantum spin models. As a consequence, the ground state of such systems\noften displays a large degeneracy that can be lifted due to thermal or quantum\neffects. One such example is the antiferromagnetic Ising model on the Kagome\nlattice. It was shown that while the same model on the triangular lattice is\nordered at zero temperature for small transverse field due to an order by\ndisorder mechanism, the Kagome lattice resists any such effects and exhibits\nonly short range spin correlations and a trivial paramagnetic phase. We embed\nthis model on the latest architecture of D-Wave's quantum annealer, the\nAdvantage2 prototype, which uses the highly connected Zephyr graph. Using\nadvanced embedding and calibration techniques, we are able to embed a Kagome\nlattice with mixed open and periodic boundary conditions of 231 sites on the\nfull graph of the currently available prototype. Through forward annealing\nexperiments, we show that under a finite longitudinal field the system exhibits\na one-third magnetization plateau, consistent with a classical spin liquid\nstate of reduced entropy. An anneal-pause-quench protocol is then used to\nextract an experimental ensemble of states resulting from the equilibration of\nthe model at finite transverse and longitudinal field. This allows us to\nconstruct a partial phase diagram and confirm that the system exits the\nconstrained Hilbert space of the classical spin liquid when subjected to a\ntransverse field. We connect our results to previous theoretical results and\nquantum Monte Carlo simulation, which helps us confirm the validity of the\nquantum simulation realized here, thereby extracting insight into the\nperformance of the D-Wave quantum annealer to simulate non-trivial quantum\nsystems in equilibrium.",
            "author": [
                "Pratyankara Narasimhan",
                "Stephan Humeniuk",
                "Ananda Roy",
                "Victor Drouin-Touchette"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06698v2",
                "http://arxiv.org/pdf/2310.06698v2"
            ],
            "primary_category": "cond-mat.stat-mech",
            "category": [
                "cond-mat.stat-mech",
                "cond-mat.str-el",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06689v1",
            "title": "Approximating Nash Equilibria in Normal-Form Games via Stochastic\n  Optimization",
            "updated": "2023-10-10T15:05:21Z",
            "published": "2023-10-10T15:05:21Z",
            "summary": "We propose the first, to our knowledge, loss function for approximate Nash\nequilibria of normal-form games that is amenable to unbiased Monte Carlo\nestimation. This construction allows us to deploy standard non-convex\nstochastic optimization techniques for approximating Nash equilibria, resulting\nin novel algorithms with provable guarantees. We complement our theoretical\nanalysis with experiments demonstrating that stochastic gradient descent can\noutperform previous state-of-the-art approaches.",
            "author": [
                "Ian Gemp",
                "Luke Marris",
                "Georgios Piliouras"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06689v1",
                "http://arxiv.org/pdf/2310.06689v1"
            ],
            "primary_category": "cs.GT",
            "category": [
                "cs.GT",
                "cs.MA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06684v1",
            "title": "Learning Multiplex Embeddings on Text-rich Networks with One Text\n  Encoder",
            "updated": "2023-10-10T14:59:22Z",
            "published": "2023-10-10T14:59:22Z",
            "summary": "In real-world scenarios, texts in a network are often linked by multiple\nsemantic relations (e.g., papers in an academic network are referenced by other\npublications, written by the same author, or published in the same venue),\nwhere text documents and their relations form a multiplex text-rich network.\nMainstream text representation learning methods use pretrained language models\n(PLMs) to generate one embedding for each text unit, expecting that all types\nof relations between texts can be captured by these single-view embeddings.\nHowever, this presumption does not hold particularly in multiplex text-rich\nnetworks. Along another line of work, multiplex graph neural networks (GNNs)\ndirectly initialize node attributes as a feature vector for node representation\nlearning, but they cannot fully capture the semantics of the nodes' associated\ntexts. To bridge these gaps, we propose METERN, a new framework for learning\nMultiplex Embeddings on TExt-Rich Networks. In contrast to existing methods,\nMETERN uses one text encoder to model the shared knowledge across relations and\nleverages a small number of parameters per relation to derive relation-specific\nrepresentations. This allows the encoder to effectively capture the multiplex\nstructures in the network while also preserving parameter efficiency. We\nconduct experiments on nine downstream tasks in five networks from both\nacademic and e-commerce domains, where METERN outperforms baselines\nsignificantly and consistently. The code is available at\nhttps://github.com/PeterGriffinJin/METERN-submit.",
            "author": [
                "Bowen Jin",
                "Wentao Zhang",
                "Yu Zhang",
                "Yu Meng",
                "Han Zhao",
                "Jiawei Han"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06684v1",
                "http://arxiv.org/pdf/2310.06684v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.12169v1",
            "title": "Enhanced Graph Neural Networks with Ego-Centric Spectral Subgraph\n  Embeddings Augmentation",
            "updated": "2023-10-10T14:57:29Z",
            "published": "2023-10-10T14:57:29Z",
            "summary": "Graph Neural Networks (GNNs) have shown remarkable merit in performing\nvarious learning-based tasks in complex networks. The superior performance of\nGNNs often correlates with the availability and quality of node-level features\nin the input networks. However, for many network applications, such node-level\ninformation may be missing or unreliable, thereby limiting the applicability\nand efficacy of GNNs. To address this limitation, we present a novel approach\ndenoted as Ego-centric Spectral subGraph Embedding Augmentation (ESGEA), which\naims to enhance and design node features, particularly in scenarios where\ninformation is lacking. Our method leverages the topological structure of the\nlocal subgraph to create topology-aware node features. The subgraph features\nare generated using an efficient spectral graph embedding technique, and they\nserve as node features that capture the local topological organization of the\nnetwork. The explicit node features, if present, are then enhanced with the\nsubgraph embeddings in order to improve the overall performance. ESGEA is\ncompatible with any GNN-based architecture and is effective even in the absence\nof node features. We evaluate the proposed method in a social network graph\nclassification task where node attributes are unavailable, as well as in a node\nclassification task where node features are corrupted or even absent. The\nevaluation results on seven datasets and eight baseline models indicate up to a\n10% improvement in AUC and a 7% improvement in accuracy for graph and node\nclassification tasks, respectively.",
            "author": [
                "Anwar Said",
                "Mudassir Shabbir",
                "Tyler Derr",
                "Waseem Abbas",
                "Xenofon Koutsoukos"
            ],
            "link": [
                "http://arxiv.org/abs/2310.12169v1",
                "http://arxiv.org/pdf/2310.12169v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06682v1",
            "title": "On the importance of catalyst-adsorbate 3D interactions for relaxed\n  energy predictions",
            "updated": "2023-10-10T14:57:04Z",
            "published": "2023-10-10T14:57:04Z",
            "summary": "The use of machine learning for material property prediction and discovery\nhas traditionally centered on graph neural networks that incorporate the\ngeometric configuration of all atoms. However, in practice not all this\ninformation may be readily available, e.g.~when evaluating the potentially\nunknown binding of adsorbates to catalyst. In this paper, we investigate\nwhether it is possible to predict a system's relaxed energy in the OC20 dataset\nwhile ignoring the relative position of the adsorbate with respect to the\nelectro-catalyst. We consider SchNet, DimeNet++ and FAENet as base\narchitectures and measure the impact of four modifications on model\nperformance: removing edges in the input graph, pooling independent\nrepresentations, not sharing the backbone weights and using an attention\nmechanism to propagate non-geometric relative information. We find that while\nremoving binding site information impairs accuracy as expected, modified models\nare able to predict relaxed energies with remarkably decent MAE. Our work\nsuggests future research directions in accelerated materials discovery where\ninformation on reactant configurations can be reduced or altogether omitted.",
            "author": [
                "Alvaro Carbonero",
                "Alexandre Duval",
                "Victor Schmidt",
                "Santiago Miret",
                "Alex Hernandez-Garcia",
                "Yoshua Bengio",
                "David Rolnick"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06682v1",
                "http://arxiv.org/pdf/2310.06682v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06680v1",
            "title": "Benchmarking and Explaining Large Language Model-based Code Generation:\n  A Causality-Centric Approach",
            "updated": "2023-10-10T14:56:26Z",
            "published": "2023-10-10T14:56:26Z",
            "summary": "While code generation has been widely used in various software development\nscenarios, the quality of the generated code is not guaranteed. This has been a\nparticular concern in the era of large language models (LLMs)- based code\ngeneration, where LLMs, deemed a complex and powerful black-box model, is\ninstructed by a high-level natural language specification, namely a prompt, to\ngenerate code. Nevertheless, effectively evaluating and explaining the code\ngeneration capability of LLMs is inherently challenging, given the complexity\nof LLMs and the lack of transparency.\n  Inspired by the recent progress in causality analysis and its application in\nsoftware engineering, this paper launches a causality analysis-based approach\nto systematically analyze the causal relations between the LLM input prompts\nand the generated code. To handle various technical challenges in this study,\nwe first propose a novel causal graph-based representation of the prompt and\nthe generated code, which is established over the fine-grained,\nhuman-understandable concepts in the input prompts. The formed causal graph is\nthen used to identify the causal relations between the prompt and the derived\ncode. We illustrate the insights that our framework can provide by studying\nover 3 popular LLMs with over 12 prompt adjustment strategies. The results of\nthese studies illustrate the potential of our technique to provide insights\ninto LLM effectiveness, and aid end-users in understanding predictions.\nAdditionally, we demonstrate that our approach provides actionable insights to\nimprove the quality of the LLM-generated code by properly calibrating the\nprompt.",
            "author": [
                "Zhenlan Ji",
                "Pingchuan Ma",
                "Zongjie Li",
                "Shuai Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06680v1",
                "http://arxiv.org/pdf/2310.06680v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06671v1",
            "title": "Making Large Language Models Perform Better in Knowledge Graph\n  Completion",
            "updated": "2023-10-10T14:47:09Z",
            "published": "2023-10-10T14:47:09Z",
            "summary": "Large language model (LLM) based knowledge graph completion (KGC) aims to\npredict the missing triples in the KGs with LLMs and enrich the KGs to become\nbetter web infrastructure, which can benefit a lot of web-based automatic\nservices. However, research about LLM-based KGC is limited and lacks effective\nutilization of LLM's inference capabilities, which ignores the important\nstructural information in KGs and prevents LLMs from acquiring accurate factual\nknowledge. In this paper, we discuss how to incorporate the helpful KG\nstructural information into the LLMs, aiming to achieve structrual-aware\nreasoning in the LLMs. We first transfer the existing LLM paradigms to\nstructural-aware settings and further propose a knowledge prefix adapter (KoPA)\nto fulfill this stated goal. KoPA employs structural embedding pre-training to\ncapture the structural information of entities and relations in the KG. Then\nKoPA informs the LLMs of the knowledge prefix adapter which projects the\nstructural embeddings into the textual space and obtains virtual knowledge\ntokens as a prefix of the input prompt. We conduct comprehensive experiments on\nthese structural-aware LLM-based KGC methods and provide an in-depth analysis\ncomparing how the introduction of structural information would be better for\nLLM's knowledge reasoning ability. Our code is released at\nhttps://github.com/zjukg/KoPA.",
            "author": [
                "Yichi Zhang",
                "Zhuo Chen",
                "Wen Zhang",
                "Huajun Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06671v1",
                "http://arxiv.org/pdf/2310.06671v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06663v2",
            "title": "Adaptive Cache-Friendly Priority Queue: Enhancing Heap-Tree Efficiency\n  for Modern Computing",
            "updated": "2023-11-24T16:05:01Z",
            "published": "2023-10-10T14:39:01Z",
            "summary": "Priority queues are fundamental data structures with widespread applications\nin various domains, including graph algorithms and network simulations. Their\nperformance critically impacts the overall efficiency of these algorithms.\nTraditional priority queue implementations often face cache-related performance\nbottlenecks, especially in modern computing environments with hierarchical\nmemory systems. To address this challenge, we propose an adaptive\ncache-friendly priority queue that utilizes three adjustable parameters to\noptimize the heap tree structure for specific system conditions by making a\ntradeoff between cache friendliness and the average number of cpu instructions\nneeded to carry out the data structure operations. Compared to the implicit\nbinary tree model, our approach significantly reduces the number of cache\nmisses and improves performance, as demonstrated through rigorous testing on\nthe heap sort algorithm. We employ a search method to determine the optimal\nparameter values, eliminating the need for manual configuration. Furthermore,\nour data structure is laid out in a single compact block of memory, minimizing\nthe memory consumption and can dynamically grow without the need for costly\nheap tree reconstructions. The adaptability of our cache-friendly priority\nqueue makes it particularly well-suited for modern computing environments with\ndiverse system architectures.",
            "author": [
                "Kiarash Parvizi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06663v2",
                "http://arxiv.org/pdf/2310.06663v2"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06661v1",
            "title": "Tertiary Lymphoid Structures Generation through Graph-based Diffusion",
            "updated": "2023-10-10T14:37:17Z",
            "published": "2023-10-10T14:37:17Z",
            "summary": "Graph-based representation approaches have been proven to be successful in\nthe analysis of biomedical data, due to their capability of capturing intricate\ndependencies between biological entities, such as the spatial organization of\ndifferent cell types in a tumor tissue. However, to further enhance our\nunderstanding of the underlying governing biological mechanisms, it is\nimportant to accurately capture the actual distributions of such complex data.\nGraph-based deep generative models are specifically tailored to accomplish\nthat. In this work, we leverage state-of-the-art graph-based diffusion models\nto generate biologically meaningful cell-graphs. In particular, we show that\nthe adopted graph diffusion model is able to accurately learn the distribution\nof cells in terms of their tertiary lymphoid structures (TLS) content, a\nwell-established biomarker for evaluating the cancer progression in oncology\nresearch. Additionally, we further illustrate the utility of the learned\ngenerative models for data augmentation in a TLS classification task. To the\nbest of our knowledge, this is the first work that leverages the power of graph\ndiffusion models in generating meaningful biological cell structures.",
            "author": [
                "Manuel Madeira",
                "Dorina Thanou",
                "Pascal Frossard"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06661v1",
                "http://arxiv.org/pdf/2310.06661v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06644v1",
            "title": "Zero-Level-Set Encoder for Neural Distance Fields",
            "updated": "2023-10-10T14:07:37Z",
            "published": "2023-10-10T14:07:37Z",
            "summary": "Neural shape representation generally refers to representing 3D geometry\nusing neural networks, e.g., to compute a signed distance or occupancy value at\na specific spatial position. Previous methods tend to rely on the auto-decoder\nparadigm, which often requires densely-sampled and accurate signed distances to\nbe known during training and testing, as well as an additional optimization\nloop during inference. This introduces a lot of computational overhead, in\naddition to having to compute signed distances analytically, even during\ntesting. In this paper, we present a novel encoder-decoder neural network for\nembedding 3D shapes in a single forward pass. Our architecture is based on a\nmulti-scale hybrid system incorporating graph-based and voxel-based components,\nas well as a continuously differentiable decoder. Furthermore, the network is\ntrained to solve the Eikonal equation and only requires knowledge of the\nzero-level set for training and inference. Additional volumetric samples can be\ngenerated on-the-fly, and incorporated in an unsupervised manner. This means\nthat in contrast to most previous work, our network is able to output valid\nsigned distance fields without explicit prior knowledge of non-zero distance\nvalues or shape occupancy. In other words, our network computes approximate\nsolutions to the boundary-valued Eikonal equation. It also requires only a\nsingle forward pass during inference, instead of the common latent code\noptimization. We further propose a modification of the loss function in case\nthat surface normals are not well defined, e.g., in the context of\nnon-watertight surface-meshes and non-manifold geometry. We finally demonstrate\nthe efficacy, generalizability and scalability of our method on datasets\nconsisting of deforming 3D shapes, single class encoding and multiclass\nencoding, showcasing a wide range of possible applications.",
            "author": [
                "Stefan Rhys Jeske",
                "Jonathan Klein",
                "Dominik L. Michels",
                "Jan Bender"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06644v1",
                "http://arxiv.org/pdf/2310.06644v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06643v3",
            "title": "Implicit Variational Inference for High-Dimensional Posteriors",
            "updated": "2023-11-09T10:39:02Z",
            "published": "2023-10-10T14:06:56Z",
            "summary": "In variational inference, the benefits of Bayesian models rely on accurately\ncapturing the true posterior distribution. We propose using neural samplers\nthat specify implicit distributions, which are well-suited for approximating\ncomplex multimodal and correlated posteriors in high-dimensional spaces. Our\napproach introduces novel bounds for approximate inference using implicit\ndistributions by locally linearising the neural sampler. This is distinct from\nexisting methods that rely on additional discriminator networks and unstable\nadversarial objectives. Furthermore, we present a new sampler architecture\nthat, for the first time, enables implicit distributions over tens of millions\nof latent variables, addressing computational concerns by using differentiable\nnumerical approximations. We empirically show that our method is capable of\nrecovering correlations across layers in large Bayesian neural networks, a\nproperty that is crucial for a network's performance but notoriously\nchallenging to achieve. To the best of our knowledge, no other method has been\nshown to accomplish this task for such large models. Through experiments in\ndownstream tasks, we demonstrate that our expressive posteriors outperform\nstate-of-the-art uncertainty quantification methods, validating the\neffectiveness of our training algorithm and the quality of the learned implicit\napproximation.",
            "author": [
                "Anshuk Uppal",
                "Kristoffer Stensbo-Smidt",
                "Wouter Boomsma",
                "Jes Frellsen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06643v3",
                "http://arxiv.org/pdf/2310.06643v3"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06631v1",
            "title": "Dense circuit graphs and the planar Tur\u00e1n number of a cycle",
            "updated": "2023-10-10T13:49:51Z",
            "published": "2023-10-10T13:49:51Z",
            "summary": "The $\\textit{planar Tur\\'an number}$ $\\textrm{ex}_{\\mathcal P}(n,H)$ of a\ngraph $H$ is the maximum number of edges in an $n$-vertex planar graph without\n$H$ as a subgraph. Let $C_k$ denote the cycle of length $k$. The planar Tur\\'an\nnumber $\\textrm{ex}_{\\mathcal P}(n,C_k)$ is known for $k\\le 7$. We show that\ndense planar graphs with a certain connectivity property (known as circuit\ngraphs) contain large near triangulations, and we use this result to obtain\nconsequences for planar Tur\\'an numbers. In particular, we prove that there is\na constant $D$ so that $\\textrm{ex}_{\\mathcal P}(n,C_k) \\le 3n - 6 -\nDn/k^{\\log_2^3}$ for all $k, n\\ge 4$. When $k \\ge 11$ this bound is tight up to\nthe constant $D$ and proves a conjecture of Cranston, Lidick\\'y, Liu, and\nShantanam.",
            "author": [
                "Ruilin Shi",
                "Zach Walsh",
                "Xingxing Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06631v1",
                "http://arxiv.org/pdf/2310.06631v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C35, 05C10"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06613v1",
            "title": "BandMap: Application Mapping with Bandwidth Allocation forCoarse-Grained\n  Reconfigurable Array",
            "updated": "2023-10-10T13:30:05Z",
            "published": "2023-10-10T13:30:05Z",
            "summary": "This paper proposes an application mapping algorithm, BandMap, for\ncoarse-grained reconfigurable array (CGRA), which allocates the bandwidth in PE\narray according to the transferring demands of data, especially the data with\nhigh spatial reuse, to reduce the routing PEs. To cover bandwidth allocation,\nBandMap maps the data flow graphs (DFGs), abstracted from applications, by\nsolving the maximum independent set (MIS) on a mixture of tuple and quadruple\nresource occupation conflict graph. Compared to a state-of-art BusMap work,\nBandmap can achieve reduced routing PEs with the same or even smaller\ninitiation interval (II).",
            "author": [
                "Xiaobing Ni",
                "Jiaheng Ruan",
                "Mengke Ge",
                "Wendi Sun",
                "Song Chen",
                "Yi Kang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06613v1",
                "http://arxiv.org/pdf/2310.06613v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06612v1",
            "title": "On the classification and dispersability of circulant graphs with two\n  jump lengths",
            "updated": "2023-10-10T13:28:39Z",
            "published": "2023-10-10T13:28:39Z",
            "summary": "In this paper, we give the classification of circulant graphs\n$C(\\mathbb{Z}_{n},S)$ with $|S|=2$ and completely solve the dispersability of\ncirculant graphs $C(\\mathbb{Z}_{n},\\{1, k\\})$.",
            "author": [
                "Xiaoxiang Yu",
                "Zeling Shao",
                "Zhiguo Li"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06612v1",
                "http://arxiv.org/pdf/2310.06612v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C10"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06602v1",
            "title": "A solution method for arbitrary polyhedral convex set optimization\n  problems",
            "updated": "2023-10-10T13:09:09Z",
            "published": "2023-10-10T13:09:09Z",
            "summary": "We provide a solution method for a polyhedral convex set optimization\nproblems, that is, the problem to minimize a set-valued mapping with polyhedral\nconvex graph with respect to a set ordering relation which is generated by a\npolyhedral convex cone. The method is proven to be correct and finite without\nany further assumption to the problem.",
            "author": [
                "Andreas L\u00f6hne"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06602v1",
                "http://arxiv.org/pdf/2310.06602v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "90C99, 90C29, 90C05"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06561v2",
            "title": "Universal holomorphic maps with slow growth II. functional analysis\n  methods",
            "updated": "2023-10-24T15:30:23Z",
            "published": "2023-10-10T12:19:04Z",
            "summary": "By means of hypercyclic operator theory, we complement our previous results\non hypercyclic holomorphic maps between complex Euclidean spaces having slow\ngrowth rates,by showing {\\it abstract abundance} rather than {\\it explicit\nexistence}. Next, we establish that, in the space of holomorphic maps from\n$\\mathbb{C}^n$ to any connected Oka manifold $Y$, equipped with the\ncompact-open topology, there exists a {\\em dense} subset consisting of common\n{\\em frequently hypercyclic} elements for all nontrivial translation operators.\nTo our knowledge, this is new even for $n=1$ and $Y=\\mathbb{C}$.",
            "author": [
                "Bin Guo",
                "Song-Yan Xie"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06561v2",
                "http://arxiv.org/pdf/2310.06561v2"
            ],
            "primary_category": "math.CV",
            "category": [
                "math.CV",
                "math.FA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06560v2",
            "title": "On friendship and cyclic parking functions",
            "updated": "2023-10-11T05:54:54Z",
            "published": "2023-10-10T12:18:23Z",
            "summary": "In parking problems, a given number of cars enter a one-way street\nsequentially, and try to park according to a specified preferred spot in the\nstreet. Various models are possible depending on the chosen rule for\ncollisions, when two cars have the same preferred spot. In classical parking\nfunctions, if a car's preferred spot is already occupied by a previous car, it\ndrives forward and looks for the first unoccupied spot to park. In this work,\nwe introduce a variant of classical parking functions, called \"friendship\nparking functions\", which imposes additional restrictions on where cars can\npark. Namely, a car can only end up parking next to cars which are its friends\n(friendship will correspond to adjacency in an underlying graph). We\ncharacterise and enumerate such friendship parking functions according to their\noutcome permutation, which describes the final configuration when all cars have\nparked. We apply this to the case where the underlying friendship graph is the\ncycle graph. Finally, we consider a subset of classical parking functions,\ncalled \"cyclic parking functions\", where cars end up in an increasing cyclic\norder. We enumerate these cyclic parking functions and exhibit a bijection to\npermutation components.",
            "author": [
                "Yujia Kang",
                "Thomas Selig",
                "Guanyi Yang",
                "Yanting Zhang",
                "Haoyue Zhu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06560v2",
                "http://arxiv.org/pdf/2310.06560v2"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05A19 (Primary) 05A15, 05A05, 05C30 (Secondary)"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06552v3",
            "title": "Automated clinical coding using off-the-shelf large language models",
            "updated": "2023-11-13T12:38:00Z",
            "published": "2023-10-10T11:56:48Z",
            "summary": "The task of assigning diagnostic ICD codes to patient hospital admissions is\ntypically performed by expert human coders. Efforts towards automated ICD\ncoding are dominated by supervised deep learning models. However, difficulties\nin learning to predict the large number of rare codes remain a barrier to\nadoption in clinical practice. In this work, we leverage off-the-shelf\npre-trained generative large language models (LLMs) to develop a practical\nsolution that is suitable for zero-shot and few-shot code assignment, with no\nneed for further task-specific training. Unsupervised pre-training alone does\nnot guarantee precise knowledge of the ICD ontology and specialist clinical\ncoding task, therefore we frame the task as information extraction, providing a\ndescription of each coded concept and asking the model to retrieve related\nmentions. For efficiency, rather than iterating over all codes, we leverage the\nhierarchical nature of the ICD ontology to sparsely search for relevant codes.",
            "author": [
                "Joseph S. Boyle",
                "Antanas Kascenas",
                "Pat Lok",
                "Maria Liakata",
                "Alison Q. O'Neil"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06552v3",
                "http://arxiv.org/pdf/2310.06552v3"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL",
                "I.2.7; I.2.8"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06549v1",
            "title": "Be Careful What You Smooth For: Label Smoothing Can Be a Privacy Shield\n  but Also a Catalyst for Model Inversion Attacks",
            "updated": "2023-10-10T11:51:12Z",
            "published": "2023-10-10T11:51:12Z",
            "summary": "Label smoothing -- using softened labels instead of hard ones -- is a widely\nadopted regularization method for deep learning, showing diverse benefits such\nas enhanced generalization and calibration. Its implications for preserving\nmodel privacy, however, have remained unexplored. To fill this gap, we\ninvestigate the impact of label smoothing on model inversion attacks (MIAs),\nwhich aim to generate class-representative samples by exploiting the knowledge\nencoded in a classifier, thereby inferring sensitive information about its\ntraining data. Through extensive analyses, we uncover that traditional label\nsmoothing fosters MIAs, thereby increasing a model's privacy leakage. Even\nmore, we reveal that smoothing with negative factors counters this trend,\nimpeding the extraction of class-related information and leading to privacy\npreservation, beating state-of-the-art defenses. This establishes a practical\nand powerful novel way for enhancing model resilience against MIAs.",
            "author": [
                "Lukas Struppek",
                "Dominik Hintersdorf",
                "Kristian Kersting"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06549v1",
                "http://arxiv.org/pdf/2310.06549v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CR",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06543v1",
            "title": "An Edge-Aware Graph Autoencoder Trained on Scale-Imbalanced Data for\n  Travelling Salesman Problems",
            "updated": "2023-10-10T11:42:49Z",
            "published": "2023-10-10T11:42:49Z",
            "summary": "Recent years have witnessed a surge in research on machine learning for\ncombinatorial optimization since learning-based approaches can outperform\ntraditional heuristics and approximate exact solvers at a lower computation\ncost. However, most existing work on supervised neural combinatorial\noptimization focuses on TSP instances with a fixed number of cities and\nrequires large amounts of training samples to achieve a good performance,\nmaking them less practical to be applied to realistic optimization scenarios.\nThis work aims to develop a data-driven graph representation learning method\nfor solving travelling salesman problems (TSPs) with various numbers of cities.\nTo this end, we propose an edge-aware graph autoencoder (EdgeGAE) model that\ncan learn to solve TSPs after being trained on solution data of various sizes\nwith an imbalanced distribution. We formulate the TSP as a link prediction task\non sparse connected graphs. A residual gated encoder is trained to learn latent\nedge embeddings, followed by an edge-centered decoder to output link\npredictions in an end-to-end manner. To improve the model's generalization\ncapability of solving large-scale problems, we introduce an active sampling\nstrategy into the training process. In addition, we generate a benchmark\ndataset containing 50,000 TSP instances with a size from 50 to 500 cities,\nfollowing an extremely scale-imbalanced distribution, making it ideal for\ninvestigating the model's performance for practical applications. We conduct\nexperiments using different amounts of training data with various scales, and\nthe experimental results demonstrate that the proposed data-driven approach\nachieves a highly competitive performance among state-of-the-art learning-based\nmethods for solving TSPs.",
            "author": [
                "Shiqing Liu",
                "Xueming Yan",
                "Yaochu Jin"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06543v1",
                "http://arxiv.org/pdf/2310.06543v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06540v1",
            "title": "A Novel Contrastive Learning Method for Clickbait Detection on RoCliCo:\n  A Romanian Clickbait Corpus of News Articles",
            "updated": "2023-10-10T11:38:16Z",
            "published": "2023-10-10T11:38:16Z",
            "summary": "To increase revenue, news websites often resort to using deceptive news\ntitles, luring users into clicking on the title and reading the full news.\nClickbait detection is the task that aims to automatically detect this form of\nfalse advertisement and avoid wasting the precious time of online users.\nDespite the importance of the task, to the best of our knowledge, there is no\npublicly available clickbait corpus for the Romanian language. To this end, we\nintroduce a novel Romanian Clickbait Corpus (RoCliCo) comprising 8,313 news\nsamples which are manually annotated with clickbait and non-clickbait labels.\nFurthermore, we conduct experiments with four machine learning methods, ranging\nfrom handcrafted models to recurrent and transformer-based neural networks, to\nestablish a line-up of competitive baselines. We also carry out experiments\nwith a weighted voting ensemble. Among the considered baselines, we propose a\nnovel BERT-based contrastive learning model that learns to encode news titles\nand contents into a deep metric space such that titles and contents of\nnon-clickbait news have high cosine similarity, while titles and contents of\nclickbait news have low cosine similarity. Our data set and code to reproduce\nthe baselines are publicly available for download at\nhttps://github.com/dariabroscoteanu/RoCliCo.",
            "author": [
                "Daria-Mihaela Broscoteanu",
                "Radu Tudor Ionescu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06540v1",
                "http://arxiv.org/pdf/2310.06540v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06534v1",
            "title": "Disk failure prediction based on multi-layer domain adaptive learning",
            "updated": "2023-10-10T11:28:40Z",
            "published": "2023-10-10T11:28:40Z",
            "summary": "Large scale data storage is susceptible to failure. As disks are damaged and\nreplaced, traditional machine learning models, which rely on historical data to\nmake predictions, struggle to accurately predict disk failures. This paper\npresents a novel method for predicting disk failures by leveraging multi-layer\ndomain adaptive learning techniques. First, disk data with numerous faults is\nselected as the source domain, and disk data with fewer faults is selected as\nthe target domain. A training of the feature extraction network is performed\nwith the selected origin and destination domains. The contrast between the two\ndomains facilitates the transfer of diagnostic knowledge from the domain of\nsource and target. According to the experimental findings, it has been\ndemonstrated that the proposed technique can generate a reliable prediction\nmodel and improve the ability to predict failures on disk data with few failure\nsamples.",
            "author": [
                "Guangfu Gao",
                "Peng Wu",
                "Hussain Dawood"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06534v1",
                "http://arxiv.org/pdf/2310.06534v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06517v1",
            "title": "Toward Semantic Publishing in Non-Invasive Brain Stimulation: A\n  Comprehensive Analysis of rTMS Studies",
            "updated": "2023-10-10T11:00:23Z",
            "published": "2023-10-10T11:00:23Z",
            "summary": "Noninvasive brain stimulation (NIBS) encompasses transcranial stimulation\ntechniques that can influence brain excitability. These techniques have the\npotential to treat conditions like depression, anxiety, and chronic pain, and\nto provide insights into brain function. However, a lack of standardized\nreporting practices limits its reproducibility and full clinical potential.\nThis paper aims to foster interinterdisciplinarity toward adopting Computer\nScience Semantic reporting methods for the standardized documentation of\nNeuroscience NIBS studies making them explicitly Findable, Accessible,\nInteroperable, and Reusable (FAIR).\n  In a large-scale systematic review of 600 repetitive transcranial magnetic\nstimulation (rTMS), a subarea of NIBS, dosages, we describe key properties that\nallow for structured descriptions and comparisons of the studies. This paper\nshowcases the semantic publishing of NIBS in the ecosphere of\nknowledge-graph-based next-generation scholarly digital libraries.\nSpecifically, the FAIR Semantic Web resource(s)-based publishing paradigm is\nimplemented for the 600 reviewed rTMS studies in the Open Research Knowledge\nGraph.",
            "author": [
                "Swathi Anil",
                "Jennifer D'Souza"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06517v1",
                "http://arxiv.org/pdf/2310.06517v1"
            ],
            "primary_category": "cs.DL",
            "category": [
                "cs.DL",
                "cs.CL",
                "cs.IT",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06516v1",
            "title": "On the order sequence of a group",
            "updated": "2023-10-10T11:00:22Z",
            "published": "2023-10-10T11:00:22Z",
            "summary": "This paper provides a bridge between two active areas of research, the\nspectrum (set of element orders) and the power graph of a finite group.\n  The order sequence of a finite group $G$ is the list of orders of elements of\nthe group, arranged in non-decreasing order. Order sequences of groups of order\n$n$ are ordered by elementwise domination, forming a partially ordered set. We\nprove a number of results about this poset, among them the following.\n  Abelian groups are uniquely determined by their order sequences, and the\nposet of order sequences of abelian groups of order $p^n$ is naturally\nisomorphic to the (well-studied) poset of partitions of $n$ with its natural\npartial order.\n  If there exists a non-nilpotent group of order $n$, then there exists such a\ngroup whose order sequence is dominated by the order sequence of any nilpotent\ngroup of order $n$.\n  There is a product operation on finite ordered sequences, defined by forming\nall products and sorting them into non-decreasing order. The product of order\nsequences of groups $G$ and $H$ is the order sequence of a group if and only if\n$|G|$ and $|H|$ are coprime.\n  The paper concludes with a number of open problems.",
            "author": [
                "Peter J. Cameron",
                "Hiranya Kishore Dey"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06516v1",
                "http://arxiv.org/pdf/2310.06516v1"
            ],
            "primary_category": "math.GR",
            "category": [
                "math.GR",
                "math.CO",
                "20D15, 20D60, 20E22, 05E16"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06881v1",
            "title": "CAFA-evaluator: A Python Tool for Benchmarking Ontological\n  Classification Methods",
            "updated": "2023-10-10T10:51:47Z",
            "published": "2023-10-10T10:51:47Z",
            "summary": "We present CAFA-evaluator, a powerful Python program designed to evaluate the\nperformance of prediction methods on targets with hierarchical concept\ndependencies. It generalizes multi-label evaluation to modern ontologies where\nthe prediction targets are drawn from a directed acyclic graph and achieves\nhigh efficiency by leveraging matrix computation and topological sorting. The\nprogram requirements include a small number of standard Python libraries,\nmaking CAFA-evaluator easy to maintain. The code replicates the Critical\nAssessment of protein Function Annotation (CAFA) benchmarking, which evaluates\npredictions of the consistent subgraphs in Gene Ontology. Owing to its\nreliability and accuracy, the organizers have selected CAFA-evaluator as the\nofficial CAFA evaluation software.",
            "author": [
                "Damiano Piovesan",
                "Davide Zago",
                "Parnal Joshi",
                "M. Clara De Paolis Kaluza",
                "Alexander Miguel Monzon",
                "Walter Reade",
                "Iddo Friedberg",
                "Predrag Radivojac",
                "Silvio C. E. Tosatto"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06881v1",
                "http://arxiv.org/pdf/2310.06881v1"
            ],
            "primary_category": "q-bio.QM",
            "category": [
                "q-bio.QM",
                "cs.PF"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06508v1",
            "title": "Topological data analysis of human vowels: Persistent homologies across\n  representation spaces",
            "updated": "2023-10-10T10:37:54Z",
            "published": "2023-10-10T10:37:54Z",
            "summary": "Topological Data Analysis (TDA) has been successfully used for various tasks\nin signal/image processing, from visualization to supervised/unsupervised\nclassification. Often, topological characteristics are obtained from persistent\nhomology theory. The standard TDA pipeline starts from the raw signal data or a\nrepresentation of it. Then, it consists in building a multiscale topological\nstructure on the top of the data using a pre-specified filtration, and finally\nto compute the topological signature to be further exploited. The commonly used\ntopological signature is a persistent diagram (or transformations of it).\nCurrent research discusses the consequences of the many ways to exploit\ntopological signatures, much less often the choice of the filtration, but to\nthe best of our knowledge, the choice of the representation of a signal has not\nbeen the subject of any study yet. This paper attempts to provide some answers\non the latter problem. To this end, we collected real audio data and built a\ncomparative study to assess the quality of the discriminant information of the\ntopological signatures extracted from three different representation spaces.\nEach audio signal is represented as i) an embedding of observed data in a\nhigher dimensional space using Taken's representation, ii) a spectrogram viewed\nas a surface in a 3D ambient space, iii) the set of spectrogram's zeroes. From\nvowel audio recordings, we use topological signature for three prediction\nproblems: speaker gender, vowel type, and individual. We show that\ntopologically-augmented random forest improves the Out-of-Bag Error (OOB) over\nsolely based Mel-Frequency Cepstral Coefficients (MFCC) for the last two\nproblems. Our results also suggest that the topological information extracted\nfrom different signal representations is complementary, and that spectrogram's\nzeros offers the best improvement for gender prediction.",
            "author": [
                "Guillem Bonafos",
                "Jean-Marc Freyermuth",
                "Pierre Pudlo",
                "Samuel Tron\u00e7on",
                "Arnaud Rey"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06508v1",
                "http://arxiv.org/pdf/2310.06508v1"
            ],
            "primary_category": "cs.SD",
            "category": [
                "cs.SD",
                "eess.AS",
                "stat.AP",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06427v1",
            "title": "TANGO: Time-Reversal Latent GraphODE for Multi-Agent Dynamical Systems",
            "updated": "2023-10-10T08:52:16Z",
            "published": "2023-10-10T08:52:16Z",
            "summary": "Learning complex multi-agent system dynamics from data is crucial across many\ndomains, such as in physical simulations and material modeling. Extended from\npurely data-driven approaches, existing physics-informed approaches such as\nHamiltonian Neural Network strictly follow energy conservation law to introduce\ninductive bias, making their learning more sample efficiently. However, many\nreal-world systems do not strictly conserve energy, such as spring systems with\nfrictions. Recognizing this, we turn our attention to a broader physical\nprinciple: Time-Reversal Symmetry, which depicts that the dynamics of a system\nshall remain invariant when traversed back over time. It still helps to\npreserve energies for conservative systems and in the meanwhile, serves as a\nstrong inductive bias for non-conservative, reversible systems. To inject such\ninductive bias, in this paper, we propose a simple-yet-effective\nself-supervised regularization term as a soft constraint that aligns the\nforward and backward trajectories predicted by a continuous graph neural\nnetwork-based ordinary differential equation (GraphODE). It effectively imposes\ntime-reversal symmetry to enable more accurate model predictions across a wider\nrange of dynamical systems under classical mechanics. In addition, we further\nprovide theoretical analysis to show that our regularization essentially\nminimizes higher-order Taylor expansion terms during the ODE integration steps,\nwhich enables our model to be more noise-tolerant and even applicable to\nirreversible systems. Experimental results on a variety of physical systems\ndemonstrate the effectiveness of our proposed method. Particularly, it achieves\nan MSE improvement of 11.5 % on a challenging chaotic triple-pendulum systems.",
            "author": [
                "Zijie Huang",
                "Wanjia Zhao",
                "Jingdong Gao",
                "Ziniu Hu",
                "Xiao Luo",
                "Yadi Cao",
                "Yuanzhou Chen",
                "Yizhou Sun",
                "Wei Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06427v1",
                "http://arxiv.org/pdf/2310.06427v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06423v1",
            "title": "A groupoid rack and spatial surfaces",
            "updated": "2023-10-10T08:46:11Z",
            "published": "2023-10-10T08:46:11Z",
            "summary": "A spatial surface is a compact surface embedded in the $3$-sphere. We assume\nthat each connected component has non-empty boundary. Spatial surfaces are\nrepresented by diagrams of spatial trivalent graphs. In this paper, we\nintroduce the notion of a groupoid rack, which is an algebraic structure that\ncan be used for colorings of diagrams of oriented spatial surfaces.\nFurthermore, we show that the groupoid rack has a universal property on\ncolorings for diagrams of spatial surfaces.",
            "author": [
                "Katsunori Arai"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06423v1",
                "http://arxiv.org/pdf/2310.06423v1"
            ],
            "primary_category": "math.GT",
            "category": [
                "math.GT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06417v1",
            "title": "Advective Diffusion Transformers for Topological Generalization in Graph\n  Learning",
            "updated": "2023-10-10T08:40:47Z",
            "published": "2023-10-10T08:40:47Z",
            "summary": "Graph diffusion equations are intimately related to graph neural networks\n(GNNs) and have recently attracted attention as a principled framework for\nanalyzing GNN dynamics, formalizing their expressive power, and justifying\narchitectural choices. One key open questions in graph learning is the\ngeneralization capabilities of GNNs. A major limitation of current approaches\nhinges on the assumption that the graph topologies in the training and test\nsets come from the same distribution. In this paper, we make steps towards\nunderstanding the generalization of GNNs by exploring how graph diffusion\nequations extrapolate and generalize in the presence of varying graph\ntopologies. We first show deficiencies in the generalization capability of\nexisting models built upon local diffusion on graphs, stemming from the\nexponential sensitivity to topology variation. Our subsequent analysis reveals\nthe promise of non-local diffusion, which advocates for feature propagation\nover fully-connected latent graphs, under the assumption of a specific\ndata-generating condition. In addition to these findings, we propose a novel\ngraph encoder backbone, Advective Diffusion Transformer (ADiT), inspired by\nadvective graph diffusion equations that have a closed-form solution backed up\nwith theoretical guarantees of desired generalization under topological\ndistribution shifts. The new model, functioning as a versatile graph\nTransformer, demonstrates superior performance across a wide range of graph\nlearning tasks.",
            "author": [
                "Qitian Wu",
                "Chenxiao Yang",
                "Kaipeng Zeng",
                "Fan Nie",
                "Michael Bronstein",
                "Junchi Yan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06417v1",
                "http://arxiv.org/pdf/2310.06417v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06415v1",
            "title": "Deep reinforcement learning uncovers processes for separating azeotropic\n  mixtures without prior knowledge",
            "updated": "2023-10-10T08:36:21Z",
            "published": "2023-10-10T08:36:21Z",
            "summary": "Process synthesis in chemical engineering is a complex planning problem due\nto vast search spaces, continuous parameters and the need for generalization.\nDeep reinforcement learning agents, trained without prior knowledge, have shown\nto outperform humans in various complex planning problems in recent years.\nExisting work on reinforcement learning for flowsheet synthesis shows promising\nconcepts, but focuses on narrow problems in a single chemical system, limiting\nits practicality. We present a general deep reinforcement learning approach for\nflowsheet synthesis. We demonstrate the adaptability of a single agent to the\ngeneral task of separating binary azeotropic mixtures. Without prior knowledge,\nit learns to craft near-optimal flowsheets for multiple chemical systems,\nconsidering different feed compositions and conceptual approaches. On average,\nthe agent can separate more than 99% of the involved materials into pure\ncomponents, while autonomously learning fundamental process engineering\nparadigms. This highlights the agent's planning flexibility, an encouraging\nstep toward true generality.",
            "author": [
                "Quirin G\u00f6ttl",
                "Jonathan Pirnay",
                "Jakob Burger",
                "Dominik G. Grimm"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06415v1",
                "http://arxiv.org/pdf/2310.06415v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06414v1",
            "title": "Plane Constraints Aided Multi-Vehicle Cooperative Positioning Using\n  Factor Graph Optimization",
            "updated": "2023-10-10T08:31:33Z",
            "published": "2023-10-10T08:31:33Z",
            "summary": "The development of vehicle-to-vehicle (V2V) communication facil-itates the\nstudy of cooperative positioning (CP) techniques for vehicular applications.\nThe CP methods can improve the posi-tioning availability and accuracy by\ninter-vehicle ranging and data exchange between vehicles. However, the\ninter-vehicle rang-ing can be easily interrupted due to many factors such as\nobsta-cles in-between two cars. Without inter-vehicle ranging, the other\ncooperative data such as vehicle positions will be wasted, leading to\nperformance degradation of range-based CP methods. To fully utilize the\ncooperative data and mitigate the impact of inter-vehicle ranging loss, a novel\ncooperative positioning method aided by plane constraints is proposed in this\npaper. The positioning results received from cooperative vehicles are used to\nconstruct the road plane for each vehicle. The plane parameters are then\nintroduced into CP scheme to impose constraints on positioning solutions. The\nstate-of-art factor graph optimization (FGO) algo-rithm is employed to\nintegrate the plane constraints with raw data of Global Navigation Satellite\nSystems (GNSS) as well as inter-vehicle ranging measurements. The proposed CP\nmethod has the ability to resist the interruptions of inter-vehicle ranging\nsince the plane constraints are computed by just using position-related data. A\nvehicle can still benefit from the position data of cooperative vehicles even\nif the inter-vehicle ranging is unavaila-ble. The experimental results indicate\nthe superiority of the pro-posed CP method in positioning performance over the\nexisting methods, especially when the inter-ranging interruptions occur.",
            "author": [
                "Chen Zhuang",
                "Hongbo Zhao"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06414v1",
                "http://arxiv.org/pdf/2310.06414v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.SY",
                "eess.SP",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06407v1",
            "title": "Unveiling Explosive Vulnerability of Networks through Edge Collective\n  Behavior",
            "updated": "2023-10-10T08:22:07Z",
            "published": "2023-10-10T08:22:07Z",
            "summary": "Edges, binding together nodes within networks, have the potential to induce\ndramatic transitions when specific collective failure behaviors emerge. These\nchanges, initially unfolding covertly and then erupting abruptly, pose\nsubstantial, unforeseeable threats to networked systems, and are termed\nexplosive vulnerability. Thus, identifying influential edges capable of\ntriggering such drastic transitions, while minimizing cost, is of utmost\nsignificance. Here, we address this challenge by introducing edge collective\ninfluence (ECI), which builds upon the optimal percolation theory applied to\nline graphs. ECI embodies features of both optimal and explosive percolation,\ninvolving minimized removal costs and explosive dismantling tactic.\nFurthermore, we introduce two improved versions of ECI, namely IECI and IECIR,\ntailored for objectives of hidden and fast dismantling, respectively, with\ntheir superior performance validated in both synthetic and empirical networks.\nFinally, we present a dual competitive percolation (DCP) model, whose reverse\nprocess replicates the explosive dismantling process and the trajectory of the\ncost function of ECI, elucidating the microscopic mechanisms enabling ECI's\noptimization. ECI and the DCP model demonstrate the profound connection between\noptimal and explosive percolation. This work significantly deepens our\ncomprehension of percolation and provides valuable insights into the explosive\nvulnerabilities arising from edge collective behaviors.",
            "author": [
                "Peng Peng",
                "Tianlong Fan",
                "Xiao-Long Ren",
                "Linyuan L\u00fc"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06407v1",
                "http://arxiv.org/pdf/2310.06407v1"
            ],
            "primary_category": "physics.soc-ph",
            "category": [
                "physics.soc-ph",
                "05C82 (Primary), 91D30, 05C80 (Secondary)",
                "J.2; I.5.1"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06404v2",
            "title": "Hexa: Self-Improving for Knowledge-Grounded Dialogue System",
            "updated": "2023-10-22T23:58:53Z",
            "published": "2023-10-10T08:15:24Z",
            "summary": "A common practice in knowledge-grounded dialogue generation is to explicitly\nutilize intermediate steps (e.g., web-search, memory retrieval) with modular\napproaches. However, data for such steps are often inaccessible compared to\nthose of dialogue responses as they are unobservable in an ordinary dialogue.\nTo fill in the absence of these data, we develop a self-improving method to\nimprove the generative performances of intermediate steps without the ground\ntruth data. In particular, we propose a novel bootstrapping scheme with a\nguided prompt and a modified loss function to enhance the diversity of\nappropriate self-generated responses. Through experiments on various benchmark\ndatasets, we empirically demonstrate that our method successfully leverages a\nself-improving mechanism in generating intermediate and final responses and\nimproves the performances on the task of knowledge-grounded dialogue\ngeneration.",
            "author": [
                "Daejin Jo",
                "Daniel Wontae Nam",
                "Gunsoo Han",
                "Kyoung-Woon On",
                "Taehwan Kwon",
                "Seungeun Rho",
                "Sungwoong Kim"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06404v2",
                "http://arxiv.org/pdf/2310.06404v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06396v1",
            "title": "Adversarial Robustness in Graph Neural Networks: A Hamiltonian Approach",
            "updated": "2023-10-10T07:59:23Z",
            "published": "2023-10-10T07:59:23Z",
            "summary": "Graph neural networks (GNNs) are vulnerable to adversarial perturbations,\nincluding those that affect both node features and graph topology. This paper\ninvestigates GNNs derived from diverse neural flows, concentrating on their\nconnection to various stability notions such as BIBO stability, Lyapunov\nstability, structural stability, and conservative stability. We argue that\nLyapunov stability, despite its common use, does not necessarily ensure\nadversarial robustness. Inspired by physics principles, we advocate for the use\nof conservative Hamiltonian neural flows to construct GNNs that are robust to\nadversarial attacks. The adversarial robustness of different neural flow GNNs\nis empirically compared on several benchmark datasets under a variety of\nadversarial attacks. Extensive numerical experiments demonstrate that GNNs\nleveraging conservative Hamiltonian flows with Lyapunov stability substantially\nimprove robustness against adversarial perturbations. The implementation code\nof experiments is available at\nhttps://github.com/zknus/NeurIPS-2023-HANG-Robustness.",
            "author": [
                "Kai Zhao",
                "Qiyu Kang",
                "Yang Song",
                "Rui She",
                "Sijie Wang",
                "Wee Peng Tay"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06396v1",
                "http://arxiv.org/pdf/2310.06396v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06392v1",
            "title": "Co-maximal subgroup graph characterized by forbidden subgraphs",
            "updated": "2023-10-10T07:56:24Z",
            "published": "2023-10-10T07:56:24Z",
            "summary": "In this communication, the co-maximal subgroup graph $\\Gamma(G)$ of a finite\ngroup $G$ is examined when $G$ is a finite nilpotent group, finite abelian\ngroup, dihedral group $D_n$, dicyclic group $Q_{2^n}$, and $p$-group. We derive\nthe necessary and sufficient conditions for $\\Gamma(G)$ to be a cluster graph,\ntriangle-free graph, claw-free graph, cograph, chordal graph, threshold graph\nand split graph. For the case of finite nilpotent group, we are able to\nclassify it entirely. Moreover, we derive the complete structure of finite\nabelian group $G$ such that $\\Gamma(G)$ is a split graph. We leave the readers\nwith a few unsolved questions.",
            "author": [
                "Pallabi Manna",
                "Santanu Mandal",
                "Manideepa Saha"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06392v1",
                "http://arxiv.org/pdf/2310.06392v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C25"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06390v1",
            "title": "P5: Plug-and-Play Persona Prompting for Personalized Response Selection",
            "updated": "2023-10-10T07:53:36Z",
            "published": "2023-10-10T07:53:36Z",
            "summary": "The use of persona-grounded retrieval-based chatbots is crucial for\npersonalized conversations, but there are several challenges that need to be\naddressed. 1) In general, collecting persona-grounded corpus is very expensive.\n2) The chatbot system does not always respond in consideration of persona at\nreal applications. To address these challenges, we propose a plug-and-play\npersona prompting method. Our system can function as a standard open-domain\nchatbot if persona information is not available. We demonstrate that this\napproach performs well in the zero-shot setting, which reduces the dependence\non persona-ground training data. This makes it easier to expand the system to\nother languages without the need to build a persona-grounded corpus.\nAdditionally, our model can be fine-tuned for even better performance. In our\nexperiments, the zero-shot model improved the standard model by 7.71 and 1.04\npoints in the original persona and revised persona, respectively. The\nfine-tuned model improved the previous state-of-the-art system by 1.95 and 3.39\npoints in the original persona and revised persona, respectively. To the best\nof our knowledge, this is the first attempt to solve the problem of\npersonalized response selection using prompt sequences. Our code is available\non github~\\footnote{https://github.com/rungjoo/plug-and-play-prompt-persona}.",
            "author": [
                "Joosung Lee",
                "Minsik Oh",
                "Donghun Lee"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06390v1",
                "http://arxiv.org/pdf/2310.06390v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06380v1",
            "title": "CAST: Cluster-Aware Self-Training for Tabular Data",
            "updated": "2023-10-10T07:46:54Z",
            "published": "2023-10-10T07:46:54Z",
            "summary": "Self-training has gained attraction because of its simplicity and\nversatility, yet it is vulnerable to noisy pseudo-labels. Several studies have\nproposed successful approaches to tackle this issue, but they have diminished\nthe advantages of self-training because they require specific modifications in\nself-training algorithms or model architectures. Furthermore, most of them are\nincompatible with gradient boosting decision trees, which dominate the tabular\ndomain. To address this, we revisit the cluster assumption, which states that\ndata samples that are close to each other tend to belong to the same class.\nInspired by the assumption, we propose Cluster-Aware Self-Training (CAST) for\ntabular data. CAST is a simple and universally adaptable approach for enhancing\nexisting self-training algorithms without significant modifications.\nConcretely, our method regularizes the confidence of the classifier, which\nrepresents the value of the pseudo-label, forcing the pseudo-labels in\nlow-density regions to have lower confidence by leveraging prior knowledge for\neach class within the training data. Extensive empirical evaluations on up to\n20 real-world datasets confirm not only the superior performance of CAST but\nalso its robustness in various setups in self-training contexts.",
            "author": [
                "Minwook Kim",
                "Juseong Kim",
                "Kibeom Kim",
                "Donggil Kang",
                "Giltae Song"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06380v1",
                "http://arxiv.org/pdf/2310.06380v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06372v1",
            "title": "Leveraging Diffusion-Based Image Variations for Robust Training on\n  Poisoned Data",
            "updated": "2023-10-10T07:25:06Z",
            "published": "2023-10-10T07:25:06Z",
            "summary": "Backdoor attacks pose a serious security threat for training neural networks\nas they surreptitiously introduce hidden functionalities into a model. Such\nbackdoors remain silent during inference on clean inputs, evading detection due\nto inconspicuous behavior. However, once a specific trigger pattern appears in\nthe input data, the backdoor activates, causing the model to execute its\nconcealed function. Detecting such poisoned samples within vast datasets is\nvirtually impossible through manual inspection. To address this challenge, we\npropose a novel approach that enables model training on potentially poisoned\ndatasets by utilizing the power of recent diffusion models. Specifically, we\ncreate synthetic variations of all training samples, leveraging the inherent\nresilience of diffusion models to potential trigger patterns in the data. By\ncombining this generative approach with knowledge distillation, we produce\nstudent models that maintain their general performance on the task while\nexhibiting robust resistance to backdoor triggers.",
            "author": [
                "Lukas Struppek",
                "Martin B. Hentschel",
                "Clifton Poth",
                "Dominik Hintersdorf",
                "Kristian Kersting"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06372v1",
                "http://arxiv.org/pdf/2310.06372v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06369v1",
            "title": "Geometrically Aligned Transfer Encoder for Inductive Transfer in\n  Regression Tasks",
            "updated": "2023-10-10T07:11:25Z",
            "published": "2023-10-10T07:11:25Z",
            "summary": "Transfer learning is a crucial technique for handling a small amount of data\nthat is potentially related to other abundant data. However, most of the\nexisting methods are focused on classification tasks using images and language\ndatasets. Therefore, in order to expand the transfer learning scheme to\nregression tasks, we propose a novel transfer technique based on differential\ngeometry, namely the Geometrically Aligned Transfer Encoder (GATE). In this\nmethod, we interpret the latent vectors from the model to exist on a Riemannian\ncurved manifold. We find a proper diffeomorphism between pairs of tasks to\nensure that every arbitrary point maps to a locally flat coordinate in the\noverlapping region, allowing the transfer of knowledge from the source to the\ntarget data. This also serves as an effective regularizer for the model to\nbehave in extrapolation regions. In this article, we demonstrate that GATE\noutperforms conventional methods and exhibits stable behavior in both the\nlatent space and extrapolation regions for various molecular graph datasets.",
            "author": [
                "Sung Moon Ko",
                "Sumin Lee",
                "Dae-Woong Jeong",
                "Woohyung Lim",
                "Sehui Han"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06369v1",
                "http://arxiv.org/pdf/2310.06369v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06368v1",
            "title": "CoinSeg: Contrast Inter- and Intra- Class Representations for\n  Incremental Segmentation",
            "updated": "2023-10-10T07:08:49Z",
            "published": "2023-10-10T07:08:49Z",
            "summary": "Class incremental semantic segmentation aims to strike a balance between the\nmodel's stability and plasticity by maintaining old knowledge while adapting to\nnew concepts. However, most state-of-the-art methods use the freeze strategy\nfor stability, which compromises the model's plasticity.In contrast, releasing\nparameter training for plasticity could lead to the best performance for all\ncategories, but this requires discriminative feature representation.Therefore,\nwe prioritize the model's plasticity and propose the Contrast inter- and\nintra-class representations for Incremental Segmentation (CoinSeg), which\npursues discriminative representations for flexible parameter tuning. Inspired\nby the Gaussian mixture model that samples from a mixture of Gaussian\ndistributions, CoinSeg emphasizes intra-class diversity with multiple\ncontrastive representation centroids. Specifically, we use mask proposals to\nidentify regions with strong objectness that are likely to be diverse\ninstances/centroids of a category. These mask proposals are then used for\ncontrastive representations to reinforce intra-class diversity. Meanwhile, to\navoid bias from intra-class diversity, we also apply category-level\npseudo-labels to enhance category-level consistency and inter-category\ndiversity. Additionally, CoinSeg ensures the model's stability and alleviates\nforgetting through a specific flexible tuning strategy. We validate CoinSeg on\nPascal VOC 2012 and ADE20K datasets with multiple incremental scenarios and\nachieve superior results compared to previous state-of-the-art methods,\nespecially in more challenging and realistic long-term scenarios. Code is\navailable at https://github.com/zkzhang98/CoinSeg.",
            "author": [
                "Zekang Zhang",
                "Guangyu Gao",
                "Jianbo Jiao",
                "Chi Harold Liu",
                "Yunchao Wei"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06368v1",
                "http://arxiv.org/pdf/2310.06368v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06367v1",
            "title": "DrugCLIP: Contrastive Protein-Molecule Representation Learning for\n  Virtual Screening",
            "updated": "2023-10-10T07:08:35Z",
            "published": "2023-10-10T07:08:35Z",
            "summary": "Virtual screening, which identifies potential drugs from vast compound\ndatabases to bind with a particular protein pocket, is a critical step in\nAI-assisted drug discovery. Traditional docking methods are highly\ntime-consuming, and can only work with a restricted search library in real-life\napplications. Recent supervised learning approaches using scoring functions for\nbinding-affinity prediction, although promising, have not yet surpassed docking\nmethods due to their strong dependency on limited data with reliable\nbinding-affinity labels. In this paper, we propose a novel contrastive learning\nframework, DrugCLIP, by reformulating virtual screening as a dense retrieval\ntask and employing contrastive learning to align representations of binding\nprotein pockets and molecules from a large quantity of pairwise data without\nexplicit binding-affinity scores. We also introduce a biological-knowledge\ninspired data augmentation strategy to learn better protein-molecule\nrepresentations. Extensive experiments show that DrugCLIP significantly\noutperforms traditional docking and supervised learning methods on diverse\nvirtual screening benchmarks with highly reduced computation time, especially\nin zero-shot setting.",
            "author": [
                "Bowen Gao",
                "Bo Qiang",
                "Haichuan Tan",
                "Minsi Ren",
                "Yinjun Jia",
                "Minsi Lu",
                "Jingjing Liu",
                "Weiying Ma",
                "Yanyan Lan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06367v1",
                "http://arxiv.org/pdf/2310.06367v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06365v1",
            "title": "Multi-Modal Knowledge Graph Transformer Framework for Multi-Modal Entity\n  Alignment",
            "updated": "2023-10-10T07:06:06Z",
            "published": "2023-10-10T07:06:06Z",
            "summary": "Multi-Modal Entity Alignment (MMEA) is a critical task that aims to identify\nequivalent entity pairs across multi-modal knowledge graphs (MMKGs). However,\nthis task faces challenges due to the presence of different types of\ninformation, including neighboring entities, multi-modal attributes, and entity\ntypes. Directly incorporating the above information (e.g., concatenation or\nattention) can lead to an unaligned information space. To address these\nchallenges, we propose a novel MMEA transformer, called MoAlign, that\nhierarchically introduces neighbor features, multi-modal attributes, and entity\ntypes to enhance the alignment task. Taking advantage of the transformer's\nability to better integrate multiple information, we design a hierarchical\nmodifiable self-attention block in a transformer encoder to preserve the unique\nsemantics of different information. Furthermore, we design two entity-type\nprefix injection methods to integrate entity-type information using type\nprefixes, which help to restrict the global information of entities not present\nin the MMKGs. Our extensive experiments on benchmark datasets demonstrate that\nour approach outperforms strong competitors and achieves excellent entity\nalignment performance.",
            "author": [
                "Qian Li",
                "Cheng Ji",
                "Shu Guo",
                "Zhaoji Liang",
                "Lihong Wang",
                "Jianxin Li"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06365v1",
                "http://arxiv.org/pdf/2310.06365v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06362v1",
            "title": "InfoCL: Alleviating Catastrophic Forgetting in Continual Text\n  Classification from An Information Theoretic Perspective",
            "updated": "2023-10-10T07:00:13Z",
            "published": "2023-10-10T07:00:13Z",
            "summary": "Continual learning (CL) aims to constantly learn new knowledge over time\nwhile avoiding catastrophic forgetting on old tasks. We focus on continual text\nclassification under the class-incremental setting. Recent CL studies have\nidentified the severe performance decrease on analogous classes as a key factor\nfor catastrophic forgetting. In this paper, through an in-depth exploration of\nthe representation learning process in CL, we discover that the compression\neffect of the information bottleneck leads to confusion on analogous classes.\nTo enable the model learn more sufficient representations, we propose a novel\nreplay-based continual text classification method, InfoCL. Our approach\nutilizes fast-slow and current-past contrastive learning to perform mutual\ninformation maximization and better recover the previously learned\nrepresentations. In addition, InfoCL incorporates an adversarial memory\naugmentation strategy to alleviate the overfitting problem of replay.\nExperimental results demonstrate that InfoCL effectively mitigates forgetting\nand achieves state-of-the-art performance on three text classification tasks.\nThe code is publicly available at https://github.com/Yifan-Song793/InfoCL.",
            "author": [
                "Yifan Song",
                "Peiyi Wang",
                "Weimin Xiong",
                "Dawei Zhu",
                "Tianyu Liu",
                "Zhifang Sui",
                "Sujian Li"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06362v1",
                "http://arxiv.org/pdf/2310.06362v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06354v1",
            "title": "Transversals in a collections of trees",
            "updated": "2023-10-10T06:43:57Z",
            "published": "2023-10-10T06:43:57Z",
            "summary": "Let $\\mathcal{S}$ be a fixed family of graphs on vertex set $V$ and\n$\\mathcal{G}$ be a collection of elements in $\\mathcal{S}$. We investigated the\ntransversal problem of finding the maximum value of $|\\mathcal{G}|$ when\n$\\mathcal{G}$ contains no rainbow elements in $\\mathcal{S}$. Specifically, we\ndetermine the exact values when $\\mathcal{S}$ is a family of stars or a family\nof trees of the same order $n$ with $n$ dividing $|V|$. Further, all the\nextremal cases for $\\mathcal{G}$ are characterized.",
            "author": [
                "Ethan Y. H. Li",
                "Luyi Li",
                "Ping Li"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06354v1",
                "http://arxiv.org/pdf/2310.06354v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C15, 05C05, 05D15"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06350v1",
            "title": "Volume holograms with linear diffraction efficiency relation by (3+1)D\n  printing",
            "updated": "2023-10-10T06:35:55Z",
            "published": "2023-10-10T06:35:55Z",
            "summary": "We demonstrate the fabrication of volume holograms using 2-photon\npolymerization with dynamic control of light exposure. We refer to our method\nas (3+1)D printing. Volume holograms that are recorded by interfering reference\nand signal beams have a diffraction efficiency relation that is inversely\nproportional with the square of the number of superimposed holograms. By using\n(3+1)D printing for fabrication, the refractive index of each voxel is created\nindependently and thus by, digitally filtering the undesired interference\nterms, the diffraction efficiency is now inversely proportional to the number\nof multiplexed gratings. We experimentally demonstrated this linear dependence\nby recording M=50 volume gratings. To the best of our knowledge, this is the\nfirst experimental demonstration of distributed volume holograms that overcome\nthe 1/M^2 limit.",
            "author": [
                "Niyazi Ulas Dinc",
                "Christophe Moser",
                "Demetri Psaltis"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06350v1",
                "http://arxiv.org/pdf/2310.06350v1"
            ],
            "primary_category": "physics.optics",
            "category": [
                "physics.optics"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06348v2",
            "title": "A conditional compound Poisson process approach to the sparse\n  Erd\u0151s-R\u00e9nyi random graphs: moderate deviations",
            "updated": "2023-10-20T11:08:37Z",
            "published": "2023-10-10T06:32:44Z",
            "summary": "We construct a compound Poisson process conditioned on its random summation\nthat represents the sizes of the connected components in the sparse\nErd\\H{o}s-R\\'enyi random graph $G(n,c/n)$. This new representation depicts a\nconnection between the phase transition in the sparse random graph and the\ncondensation transition in the zero-range model. Under this framework, we can\nderive moderate deviation principles for the maximun component, total number of\nconnected components and empirical measure of the sizes in the non-critical\nregimes. Large deviation results are discussed.",
            "author": [
                "Wen Sun"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06348v2",
                "http://arxiv.org/pdf/2310.06348v2"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06335v1",
            "title": "BBCA-CHAIN: One-Message, Low Latency BFT Consensus on a DAG",
            "updated": "2023-10-10T06:08:04Z",
            "published": "2023-10-10T06:08:04Z",
            "summary": "This paper presents a partially synchronous BFT consensus protocol powered by\nBBCA, a lightly modified Byzantine Consistent Broadcast (CBC) primitive. BBCA\nprovides a Complete-Adopt semantic through an added probing interface to allow\neither aborting the broadcast by correct nodes or exclusively, adopting the\nmessage consistently in case of a potential delivery. It does not introduce any\nextra type of messages or communication cost to CBC.\n  BBCA is harnessed into BBCA-CHAIN to make direct commits on a chained\nbackbone of a causally ordered graph of blocks, without any additional voting\nblocks or artificial layering. With the help of Complete-Adopt, the additional\nknowledge gained from the underlying CBC completely removes the voting latency\nin popular DAG-based protocols. At the same time, causal ordering allows nodes\nto propose blocks in parallel and achieve high throughput.\n  BBCA-CHAIN thus closes up the gap between protocols built by consistent\nbroadcasts (e.g., Bullshark) to those without such an abstraction (e.g.,\nPBFT/HotStuff), emphasizing their shared fundamental principles. Using a\nBracha-style CBC as an example, we fully specify BBCA-CHAIN with simplicity,\nserving as a solid basis for high-performance replication systems (and\nblockchains).",
            "author": [
                "Dahlia Malkhi",
                "Chrysoula Stathakopoulou",
                "Maofan Yin"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06335v1",
                "http://arxiv.org/pdf/2310.06335v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06333v1",
            "title": "Learning bounded-degree polytrees with known skeleton",
            "updated": "2023-10-10T06:03:51Z",
            "published": "2023-10-10T06:03:51Z",
            "summary": "We establish finite-sample guarantees for efficient proper learning of\nbounded-degree polytrees, a rich class of high-dimensional probability\ndistributions and a subclass of Bayesian networks, a widely-studied type of\ngraphical model. Recently, Bhattacharyya et al. (2021) obtained finite-sample\nguarantees for recovering tree-structured Bayesian networks, i.e., 1-polytrees.\nWe extend their results by providing an efficient algorithm which learns\n$d$-polytrees in polynomial time and sample complexity for any bounded $d$ when\nthe underlying undirected graph (skeleton) is known. We complement our\nalgorithm with an information-theoretic sample complexity lower bound, showing\nthat the dependence on the dimension and target accuracy parameters are nearly\ntight.",
            "author": [
                "Davin Choo",
                "Joy Qiping Yang",
                "Arnab Bhattacharyya",
                "Cl\u00e9ment L. Canonne"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06333v1",
                "http://arxiv.org/pdf/2310.06333v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.DS",
                "math.PR",
                "math.ST",
                "stat.ML",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06312v1",
            "title": "Discovering Mixtures of Structural Causal Models from Time Series Data",
            "updated": "2023-10-10T05:13:10Z",
            "published": "2023-10-10T05:13:10Z",
            "summary": "In fields such as finance, climate science, and neuroscience, inferring\ncausal relationships from time series data poses a formidable challenge. While\ncontemporary techniques can handle nonlinear relationships between variables\nand flexible noise distributions, they rely on the simplifying assumption that\ndata originates from the same underlying causal model. In this work, we relax\nthis assumption and perform causal discovery from time series data originating\nfrom mixtures of different causal models. We infer both the underlying\nstructural causal models and the posterior probability for each sample\nbelonging to a specific mixture component. Our approach employs an end-to-end\ntraining process that maximizes an evidence-lower bound for data likelihood.\nThrough extensive experimentation on both synthetic and real-world datasets, we\ndemonstrate that our method surpasses state-of-the-art benchmarks in causal\ndiscovery tasks, particularly when the data emanates from diverse underlying\ncausal graphs. Theoretically, we prove the identifiability of such a model\nunder some mild assumptions.",
            "author": [
                "Sumanth Varambally",
                "Yi-An Ma",
                "Rose Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06312v1",
                "http://arxiv.org/pdf/2310.06312v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06303v1",
            "title": "Dobby: A Conversational Service Robot Driven by GPT-4",
            "updated": "2023-10-10T04:34:00Z",
            "published": "2023-10-10T04:34:00Z",
            "summary": "This work introduces a robotics platform which embeds a conversational AI\nagent in an embodied system for natural language understanding and intelligent\ndecision-making for service tasks; integrating task planning and human-like\nconversation. The agent is derived from a large language model, which has\nlearned from a vast corpus of general knowledge. In addition to generating\ndialogue, this agent can interface with the physical world by invoking commands\non the robot; seamlessly merging communication and behavior. This system is\ndemonstrated in a free-form tour-guide scenario, in an HRI study combining\nrobots with and without conversational AI capabilities. Performance is measured\nalong five dimensions: overall effectiveness, exploration abilities,\nscrutinization abilities, receptiveness to personification, and adaptability.",
            "author": [
                "Carson Stark",
                "Bohkyung Chun",
                "Casey Charleston",
                "Varsha Ravi",
                "Luis Pabon",
                "Surya Sunkari",
                "Tarun Mohan",
                "Peter Stone",
                "Justin Hart"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06303v1",
                "http://arxiv.org/pdf/2310.06303v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06300v1",
            "title": "Toward a Reference Architecture for Software Supply Chain Metadata\n  Management",
            "updated": "2023-10-10T04:25:30Z",
            "published": "2023-10-10T04:25:30Z",
            "summary": "An Software Supply Chain (SSC) attack combines an upstream attack, where\nmalicious codes are injected into a software artefact via a compromised life\ncycle activity, and a downstream attack on the consumers who use the\ncompromised artefact. Organisations need thorough and trustworthy visibility\nover the entire SSC of their software inventory to detect risks early and\nrapidly identify compromised assets in the event of an SSC attack. One way to\nachieve such visibility is through SSC metadata, machine-readable and\nauthenticated documents describing an artefact's lifecycle, such as how it was\nconstructed and the utilised ``ingredients''. Adopting SSC metadata requires\norganisations to procure or develop a Software Supply Chain Metadata Management\nsystem (SCM2), a suite of software tools for performing life cycle activities\nof SSC metadata documents such as creation, signing, distribution, and\nconsumption. Selecting or developing an SCM2 is challenging due to the lack of\na comprehensive domain model and architectural blueprint to aid practitioners\nin navigating the vast design space of SSC metadata terminologies, frameworks,\nand solutions. This paper addresses the above-mentioned challenge with a\nSystematisation of Knowledge about SSC metadata and SCM2, presented as a\nReference Architecture (RA). The RA comprises a domain model and an\narchitectural blueprint for SCM2 systems, constructed from the concepts and\nbuilding blocks scattered across existing SSC security frameworks and\nstandards. Our evaluation shows that the RA framework is effective for\nanalysing existing SCM2 solutions and guiding the engineering of new SCM2.",
            "author": [
                "Nguyen Khoi Tran",
                "Samodha Pallewatta",
                "M. Ali Babar"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06300v1",
                "http://arxiv.org/pdf/2310.06300v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06287v1",
            "title": "Stability of FFLS-based diffusion adaptive filter under a cooperative\n  excitation condition",
            "updated": "2023-10-10T03:48:13Z",
            "published": "2023-10-10T03:48:13Z",
            "summary": "In this paper, we consider the distributed filtering problem over sensor\nnetworks such that all sensors cooperatively track unknown time-varying\nparameters by using local information. A distributed forgetting factor least\nsquares (FFLS) algorithm is proposed by minimizing a local cost function\nformulated as a linear combination of accumulative estimation error. Stability\nanalysis of the algorithm is provided under a cooperative excitation condition\nwhich contains spatial union information to reflect the cooperative effect of\nall sensors. Furthermore, we generalize theoretical results to the case of\nMarkovian switching directed graphs. The main difficulties of theoretical\nanalysis lie in how to analyze properties of the product of non-independent and\nnon-stationary random matrices. Some techniques such as stability theory,\nalgebraic graph theory and Markov chain theory are employed to deal with the\nabove issue. Our theoretical results are obtained without relying on the\nindependency or stationarity assumptions of regression vectors which are\ncommonly used in existing literature.",
            "author": [
                "Die Gan",
                "Siyu Xie",
                "Zhixin Liu",
                "Jinhu Lv"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06287v1",
                "http://arxiv.org/pdf/2310.06287v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06271v1",
            "title": "Towards Mitigating Hallucination in Large Language Models via\n  Self-Reflection",
            "updated": "2023-10-10T03:05:44Z",
            "published": "2023-10-10T03:05:44Z",
            "summary": "Large language models (LLMs) have shown promise for generative and\nknowledge-intensive tasks including question-answering (QA) tasks. However, the\npractical deployment still faces challenges, notably the issue of\n\"hallucination\", where models generate plausible-sounding but unfaithful or\nnonsensical information. This issue becomes particularly critical in the\nmedical domain due to the uncommon professional concepts and potential social\nrisks involved. This paper analyses the phenomenon of hallucination in medical\ngenerative QA systems using widely adopted LLMs and datasets. Our investigation\ncenters on the identification and comprehension of common problematic answers,\nwith a specific emphasis on hallucination. To tackle this challenge, we present\nan interactive self-reflection methodology that incorporates knowledge\nacquisition and answer generation. Through this feedback process, our approach\nsteadily enhances the factuality, consistency, and entailment of the generated\nanswers. Consequently, we harness the interactivity and multitasking ability of\nLLMs and produce progressively more precise and accurate answers. Experimental\nresults on both automatic and human evaluation demonstrate the superiority of\nour approach in hallucination reduction compared to baselines.",
            "author": [
                "Ziwei Ji",
                "Tiezheng Yu",
                "Yan Xu",
                "Nayeon Lee",
                "Etsuko Ishii",
                "Pascale Fung"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06271v1",
                "http://arxiv.org/pdf/2310.06271v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06261v1",
            "title": "Self-Discriminative Modeling for Anomalous Graph Detection",
            "updated": "2023-10-10T02:08:09Z",
            "published": "2023-10-10T02:08:09Z",
            "summary": "This paper studies the problem of detecting anomalous graphs using a machine\nlearning model trained on only normal graphs, which has many applications in\nmolecule, biology, and social network data analysis. We present a\nself-discriminative modeling framework for anomalous graph detection. The key\nidea, mathematically and numerically illustrated, is to learn a discriminator\n(classifier) from the given normal graphs together with pseudo-anomalous graphs\ngenerated by a model jointly trained, where we never use any true anomalous\ngraphs and we hope that the generated pseudo-anomalous graphs interpolate\nbetween normal ones and (real) anomalous ones. Under the framework, we provide\nthree algorithms with different computational efficiencies and stabilities for\nanomalous graph detection. The three algorithms are compared with several\nstate-of-the-art graph-level anomaly detection baselines on nine popular graph\ndatasets (four with small size and five with moderate size) and show\nsignificant improvement in terms of AUC. The success of our algorithms stems\nfrom the integration of the discriminative classifier and the well-posed\npseudo-anomalous graphs, which provide new insights for anomaly detection.\nMoreover, we investigate our algorithms for large-scale imbalanced graph\ndatasets. Surprisingly, our algorithms, though fully unsupervised, are able to\nsignificantly outperform supervised learning algorithms of anomalous graph\ndetection. The corresponding reason is also analyzed.",
            "author": [
                "Jinyu Cai",
                "Yunhe Zhang",
                "Jicong Fan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06261v1",
                "http://arxiv.org/pdf/2310.06261v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06257v1",
            "title": "SCAR: Power Side-Channel Analysis at RTL-Level",
            "updated": "2023-10-10T02:03:52Z",
            "published": "2023-10-10T02:03:52Z",
            "summary": "Power side-channel attacks exploit the dynamic power consumption of\ncryptographic operations to leak sensitive information of encryption hardware.\nTherefore, it is necessary to conduct power side-channel analysis for assessing\nthe susceptibility of cryptographic systems and mitigating potential risks.\nExisting power side-channel analysis primarily focuses on post-silicon\nimplementations, which are inflexible in addressing design flaws, leading to\ncostly and time-consuming post-fabrication design re-spins. Hence, pre-silicon\npower side-channel analysis is required for early detection of vulnerabilities\nto improve design robustness. In this paper, we introduce SCAR, a novel\npre-silicon power side-channel analysis framework based on Graph Neural\nNetworks (GNN). SCAR converts register-transfer level (RTL) designs of\nencryption hardware into control-data flow graphs and use that to detect the\ndesign modules susceptible to side-channel leakage. Furthermore, we incorporate\na deep learning-based explainer in SCAR to generate quantifiable and\nhuman-accessible explanation of our detection and localization decisions. We\nhave also developed a fortification component as a part of SCAR that uses\nlarge-language models (LLM) to automatically generate and insert additional\ndesign code at the localized zone to shore up the side-channel leakage. When\nevaluated on popular encryption algorithms like AES, RSA, and PRESENT, and\npostquantum cryptography algorithms like Saber and CRYSTALS-Kyber, SCAR,\nachieves up to 94.49% localization accuracy, 100% precision, and 90.48% recall.\nAdditionally, through explainability analysis, SCAR reduces features for GNN\nmodel training by 57% while maintaining comparable accuracy. We believe that\nSCAR will transform the security-critical hardware design cycle, resulting in\nfaster design closure at a reduced design cost.",
            "author": [
                "Amisha Srivastava",
                "Sanjay Das",
                "Navnil Choudhury",
                "Rafail Psiakis",
                "Pedro Henrique Silva",
                "Debjit Pal",
                "Kanad Basu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06257v1",
                "http://arxiv.org/pdf/2310.06257v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06245v1",
            "title": "We are what we repeatedly do: Inducing and deploying habitual schemas in\n  persona-based responses",
            "updated": "2023-10-10T01:44:47Z",
            "published": "2023-10-10T01:44:47Z",
            "summary": "Many practical applications of dialogue technology require the generation of\nresponses according to a particular developer-specified persona. While a\nvariety of personas can be elicited from recent large language models, the\nopaqueness and unpredictability of these models make it desirable to be able to\nspecify personas in an explicit form. In previous work, personas have typically\nbeen represented as sets of one-off pieces of self-knowledge that are retrieved\nby the dialogue system for use in generation. However, in realistic human\nconversations, personas are often revealed through story-like narratives that\ninvolve rich habitual knowledge -- knowledge about kinds of events that an\nagent often participates in (e.g., work activities, hobbies, sporting\nactivities, favorite entertainments, etc.), including typical goals,\nsub-events, preconditions, and postconditions of those events. We capture such\nhabitual knowledge using an explicit schema representation, and propose an\napproach to dialogue generation that retrieves relevant schemas to condition a\nlarge language model to generate persona-based responses. Furthermore, we\ndemonstrate a method for bootstrapping the creation of such schemas by first\ngenerating generic passages from a set of simple facts, and then inducing\nschemas from the generated passages.",
            "author": [
                "Benjamin Kane",
                "Lenhart Schubert"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06245v1",
                "http://arxiv.org/pdf/2310.06245v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06223v1",
            "title": "Projected Push-Pull For Distributed Constrained Optimization Over\n  Time-Varying Directed Graphs (extended version)",
            "updated": "2023-10-10T00:32:12Z",
            "published": "2023-10-10T00:32:12Z",
            "summary": "We introduce the Projected Push-Pull algorithm that enables multiple agents\nto solve a distributed constrained optimization problem with private cost\nfunctions and global constraints, in a collaborative manner. Our algorithm\nemploys projected gradient descent to deal with constraints and a lazy update\nrule to control the trade-off between the consensus and optimization steps in\nthe protocol. We prove that our algorithm achieves geometric convergence over\ntime-varying directed graphs while ensuring that the decision variable always\nstays within the constraint set. We derive explicit bounds for step sizes that\nguarantee geometric convergence based on the strong-convexity and smoothness of\ncost functions, and graph properties. Moreover, we provide additional\ntheoretical results on the usefulness of lazy updates, revealing the challenges\nin the analysis of any gradient tracking method that uses projection operators\nin a distributed constrained optimization setting. We validate our theoretical\nresults with numerical studies over different graph types, showing that our\nalgorithm achieves geometric convergence empirically.",
            "author": [
                "Orhan Eren Akg\u00fcn",
                "Arif Kerem Day\u0131",
                "Stephanie Gil",
                "Angelia Nedi\u0107"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06223v1",
                "http://arxiv.org/pdf/2310.06223v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "cs.DC",
                "cs.MA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06221v1",
            "title": "Detecting and Learning Out-of-Distribution Data in the Open world:\n  Algorithm and Theory",
            "updated": "2023-10-10T00:25:21Z",
            "published": "2023-10-10T00:25:21Z",
            "summary": "This thesis makes considerable contributions to the realm of machine\nlearning, specifically in the context of open-world scenarios where systems\nface previously unseen data and contexts. Traditional machine learning models\nare usually trained and tested within a fixed and known set of classes, a\ncondition known as the closed-world setting. While this assumption works in\ncontrolled environments, it falls short in real-world applications where new\nclasses or categories of data can emerge dynamically and unexpectedly. To\naddress this, our research investigates two intertwined steps essential for\nopen-world machine learning: Out-of-distribution (OOD) Detection and Open-world\nRepresentation Learning (ORL). OOD detection focuses on identifying instances\nfrom unknown classes that fall outside the model's training distribution. This\nprocess reduces the risk of making overly confident, erroneous predictions\nabout unfamiliar inputs. Moving beyond OOD detection, ORL extends the\ncapabilities of the model to not only detect unknown instances but also learn\nfrom and incorporate knowledge about these new classes. By delving into these\nresearch problems of open-world learning, this thesis contributes both\nalgorithmic solutions and theoretical foundations, which pave the way for\nbuilding machine learning models that are not only performant but also reliable\nin the face of the evolving complexities of the real world.",
            "author": [
                "Yiyou Sun"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06221v1",
                "http://arxiv.org/pdf/2310.06221v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06213v1",
            "title": "GeoLLM: Extracting Geospatial Knowledge from Large Language Models",
            "updated": "2023-10-10T00:03:23Z",
            "published": "2023-10-10T00:03:23Z",
            "summary": "The application of machine learning (ML) in a range of geospatial tasks is\nincreasingly common but often relies on globally available covariates such as\nsatellite imagery that can either be expensive or lack predictive power. Here\nwe explore the question of whether the vast amounts of knowledge found in\nInternet language corpora, now compressed within large language models (LLMs),\ncan be leveraged for geospatial prediction tasks. We first demonstrate that\nLLMs embed remarkable spatial information about locations, but naively querying\nLLMs using geographic coordinates alone is ineffective in predicting key\nindicators like population density. We then present GeoLLM, a novel method that\ncan effectively extract geospatial knowledge from LLMs with auxiliary map data\nfrom OpenStreetMap. We demonstrate the utility of our approach across multiple\ntasks of central interest to the international community, including the\nmeasurement of population density and economic livelihoods. Across these tasks,\nour method demonstrates a 70% improvement in performance (measured using\nPearson's $r^2$) relative to baselines that use nearest neighbors or use\ninformation directly from the prompt, and performance equal to or exceeding\nsatellite-based benchmarks in the literature. With GeoLLM, we observe that\nGPT-3.5 outperforms Llama 2 and RoBERTa by 19% and 51% respectively, suggesting\nthat the performance of our method scales well with the size of the model and\nits pretraining dataset. Our experiments reveal that LLMs are remarkably\nsample-efficient, rich in geospatial information, and robust across the globe.\nCrucially, GeoLLM shows promise in mitigating the limitations of existing\ngeospatial covariates and complementing them well.",
            "author": [
                "Rohin Manvi",
                "Samar Khanna",
                "Gengchen Mai",
                "Marshall Burke",
                "David Lobell",
                "Stefano Ermon"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06213v1",
                "http://arxiv.org/pdf/2310.06213v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06212v1",
            "title": "Comparison of deep-learning data fusion strategies in mandibular\n  osteoradionecrosis prediction modelling using clinical variables and\n  radiation dose distribution volumes",
            "updated": "2023-10-09T23:55:02Z",
            "published": "2023-10-09T23:55:02Z",
            "summary": "Purpose. NTCP modelling is rapidly embracing DL methods as the need to\ninclude spatial dose information is acknowledged. Finding the most appropriate\nway of combining radiation dose distribution images and clinical data involves\ntechnical challenges and requires domain knowledge. We propose different data\nfusion strategies that we hope will serve as a starting point for future DL\nNTCP studies. Methods. Early, joint and late DL multi-modality fusion\nstrategies were compared using clinical variables and mandibular radiation dose\ndistribution volumes. The discriminative performance of the multi-modality\nmodels was compared to that of single-modality models. All the experiments were\nconducted on a control-case matched cohort of 92 ORN cases and 92 controls from\na single institution. Results. The highest ROC AUC score was obtained with the\nlate fusion model (0.70), but no statistically significant differences in\ndiscrimination performance were observed between strategies. While late fusion\nwas the least technically complex strategy, its design did not model the\ninter-modality interactions that are required for NTCP modelling. Joint fusion\ninvolved the most complex design but resulted in a single network training\nprocess which included intra- and inter-modality interactions in its model\nparameter optimisation. Conclusions. This is the first study that compares\ndifferent strategies for including image data into DL NTCP models in\ncombination with lower dimensional data such as clinical variables. The\ndiscrimination performance of such multi-modality NTCP models and the choice of\nfusion strategy will depend on the distribution and quality of both types of\ndata. We encourage future DL NTCP studies to report on different fusion\nstrategies to better justify their choice of DL pipeline.",
            "author": [
                "Laia Humbert-Vidan",
                "Vinod Patel",
                "Andrew P King",
                "Teresa Guerrero Urbano"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06212v1",
                "http://arxiv.org/pdf/2310.06212v1"
            ],
            "primary_category": "physics.med-ph",
            "category": [
                "physics.med-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06208v1",
            "title": "Human-Robot Gym: Benchmarking Reinforcement Learning in Human-Robot\n  Collaboration",
            "updated": "2023-10-09T23:34:09Z",
            "published": "2023-10-09T23:34:09Z",
            "summary": "Deep reinforcement learning (RL) has shown promising results in robot motion\nplanning with first attempts in human-robot collaboration (HRC). However, a\nfair comparison of RL approaches in HRC under the constraint of guaranteed\nsafety is yet to be made. We, therefore, present human-robot gym, a benchmark\nfor safe RL in HRC. Our benchmark provides eight challenging, realistic HRC\ntasks in a modular simulation framework. Most importantly, human-robot gym\nincludes a safety shield that provably guarantees human safety. We are,\nthereby, the first to provide a benchmark to train RL agents that adhere to the\nsafety specifications of real-world HRC. This bridges a critical gap between\ntheoretic RL research and its real-world deployment. Our evaluation of six\nenvironments led to three key results: (a) the diverse nature of the tasks\noffered by human-robot gym creates a challenging benchmark for state-of-the-art\nRL methods, (b) incorporating expert knowledge in the RL training in the form\nof an action-based reward can outperform the expert, and (c) our agents\nnegligibly overfit to training data.",
            "author": [
                "Jakob Thumm",
                "Felix Trost",
                "Matthias Althoff"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06208v1",
                "http://arxiv.org/pdf/2310.06208v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06205v1",
            "title": "Fair Classifiers that Abstain without Harm",
            "updated": "2023-10-09T23:07:28Z",
            "published": "2023-10-09T23:07:28Z",
            "summary": "In critical applications, it is vital for classifiers to defer\ndecision-making to humans. We propose a post-hoc method that makes existing\nclassifiers selectively abstain from predicting certain samples. Our abstaining\nclassifier is incentivized to maintain the original accuracy for each\nsub-population (i.e. no harm) while achieving a set of group fairness\ndefinitions to a user specified degree. To this end, we design an Integer\nProgramming (IP) procedure that assigns abstention decisions for each training\nsample to satisfy a set of constraints. To generalize the abstaining decisions\nto test samples, we then train a surrogate model to learn the abstaining\ndecisions based on the IP solutions in an end-to-end manner. We analyze the\nfeasibility of the IP procedure to determine the possible abstention rate for\ndifferent levels of unfairness tolerance and accuracy constraint for achieving\nno harm. To the best of our knowledge, this work is the first to identify the\ntheoretical relationships between the constraint parameters and the required\nabstention rate. Our theoretical results are important since a high abstention\nrate is often infeasible in practice due to a lack of human resources. Our\nframework outperforms existing methods in terms of fairness disparity without\nsacrificing accuracy at similar abstention rates.",
            "author": [
                "Tongxin Yin",
                "Jean-Fran\u00e7ois Ton",
                "Ruocheng Guo",
                "Yuanshun Yao",
                "Mingyan Liu",
                "Yang Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06205v1",
                "http://arxiv.org/pdf/2310.06205v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06203v1",
            "title": "Graphs with three and four distinct eigenvalues based on circulants",
            "updated": "2023-10-09T23:06:23Z",
            "published": "2023-10-09T23:06:23Z",
            "summary": "In this paper, we aim to address the open questions raised in various recent\npapers regarding characterization of circulant graphs with three or four\ndistinct eigenvalues in their spectra. Our focus is on providing\ncharacterizations and constructing classes of graphs falling under this\nspecific category. We present a characterization of circulant graphs with prime\nnumber order and unitary Cayley graphs with arbitrary order, both of which\npossess spectra displaying three or four distinct eigenvalues. Various\nconstructions of circulant graphs with composite orders are provided whose\nspectra consist of four distinct eigenvalues. These constructions primarily\nutilize specific subgraphs of circulant graphs that already possess two or\nthree eigenvalues in their spectra, employing graph operations like the tensor\nproduct, the union, and the complement. Finally, we characterize the iterated\nline graphs of unitary Cayley graphs whose spectra contain three or four\ndistinct eigenvalues, and we show their non-circulant nature.",
            "author": [
                "Milan Ba\u0161i\u0107"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06203v1",
                "http://arxiv.org/pdf/2310.06203v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "cs.DM",
                "05C50, 05E30, 11A07, 11A15, 11A25"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06192v1",
            "title": "Cup Stacking in Graphs",
            "updated": "2023-10-09T22:43:37Z",
            "published": "2023-10-09T22:43:37Z",
            "summary": "Here we introduce a new game on graphs, called cup stacking, following a line\nof what can be considered as $0$-, $1$-, or $2$-person games such as chip\nfiring, percolation, graph burning, zero forcing, cops and robbers, graph\npebbling, and graph pegging, among others. It can be more general, but the most\nbasic scenario begins with a single cup on each vertex of a graph. For a vertex\nwith $k$ cups on it we can move all its cups to a vertex at distance $k$ from\nit, provided the second vertex already has at least one cup on it. The object\nis to stack all cups onto some pre-described target vertex. We say that a graph\nis stackable if this can be accomplished for all possible target vertices.\n  In this paper we study cup stacking on many families of graphs, developing a\ncharacterization of stackability in graphs and using it to prove the\nstackability of complete graphs, paths, cycles, grids, the Petersen graph, many\nKneser graphs, some trees, cubes of dimension up to 20, \"somewhat balanced\"\ncomplete $t$-partite graphs, and Hamiltonian diameter two graphs. Additionally\nwe use the Gallai-Edmonds Structure Theorem, the Edmonds Blossom Algorithm, and\nthe Hungarian algorithm to devise a polynomial algorithm to decide if a\ndiameter two graph is stackable.\n  Our proof that cubes up to dimension 20 are stackable uses Kleitman's\nSymmetric Chain Decomposition and the new result of Merino, M\\\"utze, and\nNamrata that all generalized Johnson graphs (excluding the Petersen graph) are\nHamiltonian. We conjecture that all cubes and higher-dimensional grids are\nstackable, and leave the reader with several open problems, questions, and\ngeneralizations.",
            "author": [
                "Paul Fay",
                "Glenn Hurlbert",
                "Maya Tennant"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06192v1",
                "http://arxiv.org/pdf/2310.06192v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C99 (Primary) 05C85, 05C70 (Secondary)"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06184v2",
            "title": "A Brief Review of Single Event Burnout Failure Mechanisms and Design\n  Tolerances of Silicon Carbide MOSFETs",
            "updated": "2023-10-11T23:03:03Z",
            "published": "2023-10-09T22:24:06Z",
            "summary": "Radiation hardening of the MOSFET is of the highest priority for sustaining\nhigh-power systems in the space radiation environment. SiC-based power\nelectronics are being looked at as a strong alternative for high power\nspaceborne power electronic systems. The SiC MOSFET has been shown to be most\nprone to SEB of the radiation effects. The knowledge of SiC MOSFET device\ndegradation and failure mechanisms are reviewed. Additionally, the viability of\nrad-tolerant SiC MOSFET designs and the methods of SEB simulation are\nevaluated. A merit system is proposed to consider the performance of radiation\ntolerance and nominal electrical performance. Criteria needed for high-fidelity\nSEB simulation are also reviewed. This paper stands as a necessary analytical\nreview to intercede the development of rad-hard power devices for space and\nextreme environment applications.",
            "author": [
                "Christopher A. Grome",
                "Wei Ji"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06184v2",
                "http://arxiv.org/pdf/2310.06184v2"
            ],
            "primary_category": "physics.ins-det",
            "category": [
                "physics.ins-det"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06179v2",
            "title": "Automatic Integration for Spatiotemporal Neural Point Processes",
            "updated": "2023-10-31T19:15:59Z",
            "published": "2023-10-09T22:07:48Z",
            "summary": "Learning continuous-time point processes is essential to many discrete event\nforecasting tasks. However, integration poses a major challenge, particularly\nfor spatiotemporal point processes (STPPs), as it involves calculating the\nlikelihood through triple integrals over space and time. Existing methods for\nintegrating STPP either assume a parametric form of the intensity function,\nwhich lacks flexibility; or approximating the intensity with Monte Carlo\nsampling, which introduces numerical errors. Recent work by Omi et al. [2019]\nproposes a dual network approach for efficient integration of flexible\nintensity function. However, their method only focuses on the 1D temporal point\nprocess. In this paper, we introduce a novel paradigm: AutoSTPP (Automatic\nIntegration for Spatiotemporal Neural Point Processes) that extends the dual\nnetwork approach to 3D STPP. While previous work provides a foundation, its\ndirect extension overly restricts the intensity function and leads to\ncomputational challenges. In response, we introduce a decomposable\nparametrization for the integral network using ProdNet. This approach,\nleveraging the product of simplified univariate graphs, effectively sidesteps\nthe computational complexities inherent in multivariate computational graphs.\nWe prove the consistency of AutoSTPP and validate it on synthetic data and\nbenchmark real-world datasets. AutoSTPP shows a significant advantage in\nrecovering complex intensity functions from irregular spatiotemporal events,\nparticularly when the intensity is sharply localized. Our code is open-source\nat https://github.com/Rose-STL-Lab/AutoSTPP.",
            "author": [
                "Zihao Zhou",
                "Rose Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06179v2",
                "http://arxiv.org/pdf/2310.06179v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06160v1",
            "title": "Entropy Based Multi-robot Active SLAM",
            "updated": "2023-10-09T21:18:14Z",
            "published": "2023-10-09T21:18:14Z",
            "summary": "In this article, we present an efficient multi-robot active SLAM framework\nthat involves a frontier-sharing method for maximum exploration of an unknown\nenvironment. It encourages the robots to spread into the environment while\nweighting the goal frontiers with the pose graph SLAM uncertainly and path\nentropy. Our approach works on a limited number of frontier points and weights\nthe goal frontiers with a utility function that encapsulates both the SLAM and\nmap uncertainties, thus providing an efficient and not computationally\nexpensive solution. Our approach has been tested on publicly available\nsimulation environments and on real robots. An accumulative 31% more coverage\nthan similar state-of-the-art approaches has been obtained, proving the\ncapability of our approach for efficient environment exploration.",
            "author": [
                "Muhammad Farhan Ahmed",
                "Matteo Maragliano",
                "Vincent Fr\u00e9mont",
                "Carmine Tommaso Recchiuto"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06160v1",
                "http://arxiv.org/pdf/2310.06160v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06152v1",
            "title": "Some algebraic invariants of the edge ideals of some $q$-fold bristled\n  graphs",
            "updated": "2023-10-09T20:59:58Z",
            "published": "2023-10-09T20:59:58Z",
            "summary": "In this paper, we compute the exact values of regularity of the quotient\nrings of the edge ideals associated to multi triangular snake and multi\ntriangular ouroboros snake graphs. Also we compute the exact values of depth,\nStanley depth, regularity and projective dimension of the quotient rings of the\nedge ideals associated to $q$-fold bristled graphs of multi triangular snake\nand multi triangular ouroboros snake graphs.",
            "author": [
                "Ayesha Saqib",
                "Muhammad Ishaq"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06152v1",
                "http://arxiv.org/pdf/2310.06152v1"
            ],
            "primary_category": "math.AC",
            "category": [
                "math.AC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06146v1",
            "title": "Working towards a gauge-invariant description of the Higgs model: from\n  local composite operators to spectral density functions",
            "updated": "2023-10-09T20:49:31Z",
            "published": "2023-10-09T20:49:31Z",
            "summary": "We analyze different BRST invariant solutions for the introduction of a mass\nterm in Yang-Mills (YM) theories. First, we analyze the non-local composite\ngauge-invariant field $A^h_{\\mu}(x)$, which can be localized by the\nStueckelberg-like field $\\xi^a(x)$. This enables us to introduce a mass term in\nthe $SU(N)$ YM model, a feature that has been indicated at a non-perturbative\nlevel by both analytical and numerical studies. We also consider the unitary\nAbelian Higgs model and investigate its spectral functions at one-loop order.\nThis analysis allows to disentangle what is physical and what is not at the\nlevel of the elementary particle propagators, in conjunction with the Nielsen\nidentities. We highlight the role of the tadpole graphs and the gauge choices\nto get sensible results. We also introduce an Abelian Curci-Ferrari action\ncoupled to a scalar field to model a massive photon which, like the non-Abelian\nCurci-Ferarri model, is left invariant by a modified non-nilpotent BRST\nsymmetry. Finally, the spectral properties of a set of local gauge-invariant\ncomposite operators are investigated in the $U(1)$ and $SU(2)$ Higgs model\nquantized in the 't Hooft $R_{\\xi}$ gauge. These operators enable us to give a\ngauge-invariant description of the spectrum of the theory, thereby surpassing\ncertain incommodities when using the standard elementary fields. The\ncorresponding two-point correlation functions are evaluated at one-loop order\nand their spectral functions are obtained explicitly. It is shown that the\nspectral functions of the elementary fields suffer from a strong unphysical\ndependence from the gauge parameter $\\xi$, and can even exhibit positivity\nviolating behaviour. In contrast, the BRST invariant local operators exhibit a\nwell defined positive spectral density.",
            "author": [
                "D. M. van Egmond"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06146v1",
                "http://arxiv.org/pdf/2310.06146v1"
            ],
            "primary_category": "hep-th",
            "category": [
                "hep-th",
                "hep-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06138v1",
            "title": "Layout Sequence Prediction From Noisy Mobile Modality",
            "updated": "2023-10-09T20:32:49Z",
            "published": "2023-10-09T20:32:49Z",
            "summary": "Trajectory prediction plays a vital role in understanding pedestrian movement\nfor applications such as autonomous driving and robotics. Current trajectory\nprediction models depend on long, complete, and accurately observed sequences\nfrom visual modalities. Nevertheless, real-world situations often involve\nobstructed cameras, missed objects, or objects out of sight due to\nenvironmental factors, leading to incomplete or noisy trajectories. To overcome\nthese limitations, we propose LTrajDiff, a novel approach that treats objects\nobstructed or out of sight as equally important as those with fully visible\ntrajectories. LTrajDiff utilizes sensor data from mobile phones to surmount\nout-of-sight constraints, albeit introducing new challenges such as modality\nfusion, noisy data, and the absence of spatial layout and object size\ninformation. We employ a denoising diffusion model to predict precise layout\nsequences from noisy mobile data using a coarse-to-fine diffusion strategy,\nincorporating the RMS, Siamese Masked Encoding Module, and MFM. Our model\npredicts layout sequences by implicitly inferring object size and projection\nstatus from a single reference timestamp or significantly obstructed sequences.\nAchieving SOTA results in randomly obstructed experiments and extremely short\ninput experiments, our model illustrates the effectiveness of leveraging noisy\nmobile data. In summary, our approach offers a promising solution to the\nchallenges faced by layout sequence and trajectory prediction models in\nreal-world settings, paving the way for utilizing sensor data from mobile\nphones to accurately predict pedestrian bounding box trajectories. To the best\nof our knowledge, this is the first work that addresses severely obstructed and\nextremely short layout sequences by combining vision with noisy mobile\nmodality, making it the pioneering work in the field of layout sequence\ntrajectory prediction.",
            "author": [
                "Haichao Zhang",
                "Yi Xu",
                "Hongsheng Lu",
                "Takayuki Shimizu",
                "Yun Fu"
            ],
            "link": [
                "http://dx.doi.org/10.1145/3581783.3611936",
                "http://arxiv.org/abs/2310.06138v1",
                "http://arxiv.org/pdf/2310.06138v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG",
                "cs.MM",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06122v1",
            "title": "From Text to Knowledge with Graphs: modelling, querying and exploiting\n  textual content",
            "updated": "2023-10-09T19:57:22Z",
            "published": "2023-10-09T19:57:22Z",
            "summary": "This paper highlights the challenges, current trends, and open issues related\nto the representation, querying and analytics of content extracted from texts.\nThe internet contains vast text-based information on various subjects,\nincluding commercial documents, medical records, scientific experiments,\nengineering tests, and events that impact urban and natural environments.\nExtracting knowledge from this text involves understanding the nuances of\nnatural language and accurately representing the content without losing\ninformation. This allows knowledge to be accessed, inferred, or discovered. To\nachieve this, combining results from various fields, such as linguistics,\nnatural language processing, knowledge representation, data storage, querying,\nand analytics, is necessary. The vision in this paper is that graphs can be a\nwell-suited text content representation once annotated and the right querying\nand analytics techniques are applied. This paper discusses this hypothesis from\nthe perspective of linguistics, natural language processing, graph models and\ndatabases and artificial intelligence provided by the panellists of the DOING\nsession in the MADICS Symposium 2022.",
            "author": [
                "Genoveva Vargas-Solar",
                "Mirian Halfeld Ferrari Alves",
                "Anne-Lyse Minard Forst"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06122v1",
                "http://arxiv.org/pdf/2310.06122v1"
            ],
            "primary_category": "cs.DB",
            "category": [
                "cs.DB"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06117v1",
            "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language\n  Models",
            "updated": "2023-10-09T19:48:55Z",
            "published": "2023-10-09T19:48:55Z",
            "summary": "We present Step-Back Prompting, a simple prompting technique that enables\nLLMs to do abstractions to derive high-level concepts and first principles from\ninstances containing specific details. Using the concepts and principles to\nguide the reasoning steps, LLMs significantly improve their abilities in\nfollowing a correct reasoning path towards the solution. We conduct experiments\nof Step-Back Prompting with PaLM-2L models and observe substantial performance\ngains on a wide range of challenging reasoning-intensive tasks including STEM,\nKnowledge QA, and Multi-Hop Reasoning. For instance, Step-Back Prompting\nimproves PaLM-2L performance on MMLU Physics and Chemistry by 7% and 11%,\nTimeQA by 27%, and MuSiQue by 7%.",
            "author": [
                "Huaixiu Steven Zheng",
                "Swaroop Mishra",
                "Xinyun Chen",
                "Heng-Tze Cheng",
                "Ed H. Chi",
                "Quoc V Le",
                "Denny Zhou"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06117v1",
                "http://arxiv.org/pdf/2310.06117v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06100v1",
            "title": "High Dimensional Causal Inference with Variational Backdoor Adjustment",
            "updated": "2023-10-09T19:21:41Z",
            "published": "2023-10-09T19:21:41Z",
            "summary": "Backdoor adjustment is a technique in causal inference for estimating\ninterventional quantities from purely observational data. For example, in\nmedical settings, backdoor adjustment can be used to control for confounding\nand estimate the effectiveness of a treatment. However, high dimensional\ntreatments and confounders pose a series of potential pitfalls: tractability,\nidentifiability, optimization. In this work, we take a generative modeling\napproach to backdoor adjustment for high dimensional treatments and\nconfounders. We cast backdoor adjustment as an optimization problem in\nvariational inference without reliance on proxy variables and hidden\nconfounders. Empirically, our method is able to estimate interventional\nlikelihood in a variety of high dimensional settings, including semi-synthetic\nX-ray medical data. To the best of our knowledge, this is the first application\nof backdoor adjustment in which all the relevant variables are high\ndimensional.",
            "author": [
                "Daniel Israel",
                "Aditya Grover",
                "Guy Van den Broeck"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06100v1",
                "http://arxiv.org/pdf/2310.06100v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06083v1",
            "title": "Transformers and Large Language Models for Chemistry and Drug Discovery",
            "updated": "2023-10-09T18:40:04Z",
            "published": "2023-10-09T18:40:04Z",
            "summary": "Language modeling has seen impressive progress over the last years, mainly\nprompted by the invention of the Transformer architecture, sparking a\nrevolution in many fields of machine learning, with breakthroughs in chemistry\nand biology. In this chapter, we explore how analogies between chemical and\nnatural language have inspired the use of Transformers to tackle important\nbottlenecks in the drug discovery process, such as retrosynthetic planning and\nchemical space exploration. The revolution started with models able to perform\nparticular tasks with a single type of data, like linearised molecular graphs,\nwhich then evolved to include other types of data, like spectra from analytical\ninstruments, synthesis actions, and human language. A new trend leverages\nrecent developments in large language models, giving rise to a wave of models\ncapable of solving generic tasks in chemistry, all facilitated by the\nflexibility of natural language. As we continue to explore and harness these\ncapabilities, we can look forward to a future where machine learning plays an\neven more integral role in accelerating scientific discovery.",
            "author": [
                "Andres M Bran",
                "Philippe Schwaller"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06083v1",
                "http://arxiv.org/pdf/2310.06083v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "physics.chem-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06072v1",
            "title": "JVNV: A Corpus of Japanese Emotional Speech with Verbal Content and\n  Nonverbal Expressions",
            "updated": "2023-10-09T18:27:13Z",
            "published": "2023-10-09T18:27:13Z",
            "summary": "We present the JVNV, a Japanese emotional speech corpus with verbal content\nand nonverbal vocalizations whose scripts are generated by a large-scale\nlanguage model. Existing emotional speech corpora lack not only proper\nemotional scripts but also nonverbal vocalizations (NVs) that are essential\nexpressions in spoken language to express emotions. We propose an automatic\nscript generation method to produce emotional scripts by providing seed words\nwith sentiment polarity and phrases of nonverbal vocalizations to ChatGPT using\nprompt engineering. We select 514 scripts with balanced phoneme coverage from\nthe generated candidate scripts with the assistance of emotion confidence\nscores and language fluency scores. We demonstrate the effectiveness of JVNV by\nshowing that JVNV has better phoneme coverage and emotion recognizability than\nprevious Japanese emotional speech corpora. We then benchmark JVNV on emotional\ntext-to-speech synthesis using discrete codes to represent NVs. We show that\nthere still exists a gap between the performance of synthesizing read-aloud\nspeech and emotional speech, and adding NVs in the speech makes the task even\nharder, which brings new challenges for this task and makes JVNV a valuable\nresource for relevant works in the future. To our best knowledge, JVNV is the\nfirst speech corpus that generates scripts automatically using large language\nmodels.",
            "author": [
                "Detai Xin",
                "Junfeng Jiang",
                "Shinnosuke Takamichi",
                "Yuki Saito",
                "Akiko Aizawa",
                "Hiroshi Saruwatari"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06072v1",
                "http://arxiv.org/pdf/2310.06072v1"
            ],
            "primary_category": "cs.SD",
            "category": [
                "cs.SD",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06071v1",
            "title": "A new lower bound for doubly metric dimension and related extremal\n  differences",
            "updated": "2023-10-09T18:25:55Z",
            "published": "2023-10-09T18:25:55Z",
            "summary": "In this paper a new graph invariant based on the minimal hitting set problem\nis introduced. It is shown that it represents a tight lower bound for the\ndoubly metric dimension of a graph. Exact values of new invariant for paths,\nstars, complete graphs and complete bipartite graph are obtained. The paper\nanalyzes some tight bounds for the new invariant in general case. Also several\nextremal differences between some related invariants are determined.",
            "author": [
                "Jozef Kratica",
                "Vera Kova\u010devi\u0107-Vuj\u010di\u0107",
                "Mirjana \u010cangalovi\u0107"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06071v1",
                "http://arxiv.org/pdf/2310.06071v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C09, 05C12",
                "G.2.2"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06047v1",
            "title": "Knowledge Distillation for Anomaly Detection",
            "updated": "2023-10-09T18:02:38Z",
            "published": "2023-10-09T18:02:38Z",
            "summary": "Unsupervised deep learning techniques are widely used to identify anomalous\nbehaviour. The performance of such methods is a product of the amount of\ntraining data and the model size. However, the size is often a limiting factor\nfor the deployment on resource-constrained devices. We present a novel\nprocedure based on knowledge distillation for compressing an unsupervised\nanomaly detection model into a supervised deployable one and we suggest a set\nof techniques to improve the detection sensitivity. Compressed models perform\ncomparably to their larger counterparts while significantly reducing the size\nand memory footprint.",
            "author": [
                "Adrian Alan Pol",
                "Ekaterina Govorkova",
                "Sonja Gronroos",
                "Nadezda Chernyavskaya",
                "Philip Harris",
                "Maurizio Pierini",
                "Isobel Ojalvo",
                "Peter Elmer"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06047v1",
                "http://arxiv.org/pdf/2310.06047v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06027v1",
            "title": "Bounds on scattering of neutral Goldstones",
            "updated": "2023-10-09T18:00:02Z",
            "published": "2023-10-09T18:00:02Z",
            "summary": "We study the space of $2\\to 2$ scattering amplitudes of neutral Goldstone\nbosons in four space-time dimensions. We establish universal bounds on the\nfirst two non-universal Wilson coefficients of the low energy Effective Field\nTheory (EFT) for such particles. We reconstruct the analytic,\ncrossing-symmetric, and unitary amplitudes saturating our bounds, and we study\ntheir physical content. We uncover non-perturbative Regge trajectories by\ncontinuing our numerical amplitudes to complex spins. We then explore the\nconsequence of additional constraints arising when we impose the knowledge\nabout the EFT up to the cut-off scale. In the process, we improve on some\naspects of the numerical $S$-matrix bootstrap technology for massless\nparticles.",
            "author": [
                "Francesca Acanfora",
                "Andrea Guerrieri",
                "Kelian H\u00e4ring",
                "Denis Karateev"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06027v1",
                "http://arxiv.org/pdf/2310.06027v1"
            ],
            "primary_category": "hep-th",
            "category": [
                "hep-th"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05903v1",
            "title": "Graphs with no even holes and no sector wheels are the union of two\n  chordal graphs",
            "updated": "2023-10-09T17:45:01Z",
            "published": "2023-10-09T17:45:01Z",
            "summary": "Sivaraman conjectured that if $G$ is a graph with no induced even cycle then\nthere exist sets $X_1, X_2 \\subseteq V(G)$ satisfying $V(G) = X_1 \\cup X_2$\nsuch that the induced graphs $G[X_1]$ and $G[X_2]$ are both chordal. We prove\nthis conjecture in the special case where $G$ contains no sector wheel, namely,\na pair $(H, w)$ where $H$ is an induced cycle of $G$ and $w$ is a vertex in\n$V(G) \\setminus V(H)$ such that $N(w) \\cap H$ is either $V(H)$ or a path with\nat least three vertices.",
            "author": [
                "Tara Abrishami",
                "Eli Berger",
                "Maria Chudnovsky",
                "Shira Zerbib"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05903v1",
                "http://arxiv.org/pdf/2310.05903v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.13002v1",
            "title": "Are Large Language Models Geospatially Knowledgeable?",
            "updated": "2023-10-09T17:20:11Z",
            "published": "2023-10-09T17:20:11Z",
            "summary": "Despite the impressive performance of Large Language Models (LLM) for various\nnatural language processing tasks, little is known about their comprehension of\ngeographic data and related ability to facilitate informed geospatial\ndecision-making. This paper investigates the extent of geospatial knowledge,\nawareness, and reasoning abilities encoded within such pretrained LLMs. With a\nfocus on autoregressive language models, we devise experimental approaches\nrelated to (i) probing LLMs for geo-coordinates to assess geospatial knowledge,\n(ii) using geospatial and non-geospatial prepositions to gauge their geospatial\nawareness, and (iii) utilizing a multidimensional scaling (MDS) experiment to\nassess the models' geospatial reasoning capabilities and to determine locations\nof cities based on prompting. Our results confirm that it does not only take\nlarger, but also more sophisticated LLMs to synthesize geospatial knowledge\nfrom textual information. As such, this research contributes to understanding\nthe potential and limitations of LLMs in dealing with geospatial information.",
            "author": [
                "Prabin Bhandari",
                "Antonios Anastasopoulos",
                "Dieter Pfoser"
            ],
            "link": [
                "http://arxiv.org/abs/2310.13002v1",
                "http://arxiv.org/pdf/2310.13002v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05867v2",
            "title": "Domain-wise Invariant Learning for Panoptic Scene Graph Generation",
            "updated": "2023-12-05T11:37:54Z",
            "published": "2023-10-09T17:03:39Z",
            "summary": "Panoptic Scene Graph Generation (PSG) involves the detection of objects and\nthe prediction of their corresponding relationships (predicates). However, the\npresence of biased predicate annotations poses a significant challenge for PSG\nmodels, as it hinders their ability to establish a clear decision boundary\namong different predicates. This issue substantially impedes the practical\nutility and real-world applicability of PSG models. To address the intrinsic\nbias above, we propose a novel framework to infer potentially biased\nannotations by measuring the predicate prediction risks within each\nsubject-object pair (domain), and adaptively transfer the biased annotations to\nconsistent ones by learning invariant predicate representation embeddings.\nExperiments show that our method significantly improves the performance of\nbenchmark models, achieving a new state-of-the-art performance, and shows great\ngeneralization and effectiveness on PSG dataset.",
            "author": [
                "Li Li",
                "You Qin",
                "Wei Ji",
                "Yuxiao Zhou",
                "Roger Zimmermann"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05867v2",
                "http://arxiv.org/pdf/2310.05867v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05845v1",
            "title": "GraphLLM: Boosting Graph Reasoning Ability of Large Language Model",
            "updated": "2023-10-09T16:42:00Z",
            "published": "2023-10-09T16:42:00Z",
            "summary": "The advancement of Large Language Models (LLMs) has remarkably pushed the\nboundaries towards artificial general intelligence (AGI), with their\nexceptional ability on understanding diverse types of information, including\nbut not limited to images and audio. Despite this progress, a critical gap\nremains in empowering LLMs to proficiently understand and reason on graph data.\nRecent studies underscore LLMs' underwhelming performance on fundamental graph\nreasoning tasks. In this paper, we endeavor to unearth the obstacles that\nimpede LLMs in graph reasoning, pinpointing the common practice of converting\ngraphs into natural language descriptions (Graph2Text) as a fundamental\nbottleneck. To overcome this impediment, we introduce GraphLLM, a pioneering\nend-to-end approach that synergistically integrates graph learning models with\nLLMs. This synergy equips LLMs with the ability to proficiently interpret and\nreason on graph data, harnessing the superior expressive power of graph\nlearning models. Our empirical evaluations across four fundamental graph\nreasoning tasks validate the effectiveness of GraphLLM. The results exhibit a\nsubstantial average accuracy enhancement of 54.44%, alongside a noteworthy\ncontext reduction of 96.45% across various graph reasoning tasks.",
            "author": [
                "Ziwei Chai",
                "Tianjie Zhang",
                "Liang Wu",
                "Kaiqiao Han",
                "Xiaohai Hu",
                "Xuanwen Huang",
                "Yang Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05845v1",
                "http://arxiv.org/pdf/2310.05845v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05842v1",
            "title": "Robust Angular Synchronization via Directed Graph Neural Networks",
            "updated": "2023-10-09T16:37:19Z",
            "published": "2023-10-09T16:37:19Z",
            "summary": "The angular synchronization problem aims to accurately estimate (up to a\nconstant additive phase) a set of unknown angles $\\theta_1, \\dots,\n\\theta_n\\in[0, 2\\pi)$ from $m$ noisy measurements of their offsets\n$\\theta_i-\\theta_j \\;\\mbox{mod} \\; 2\\pi.$ Applications include, for example,\nsensor network localization, phase retrieval, and distributed clock\nsynchronization. An extension of the problem to the heterogeneous setting\n(dubbed $k$-synchronization) is to estimate $k$ groups of angles\nsimultaneously, given noisy observations (with unknown group assignment) from\neach group. Existing methods for angular synchronization usually perform poorly\nin high-noise regimes, which are common in applications. In this paper, we\nleverage neural networks for the angular synchronization problem, and its\nheterogeneous extension, by proposing GNNSync, a theoretically-grounded\nend-to-end trainable framework using directed graph neural networks. In\naddition, new loss functions are devised to encode synchronization objectives.\nExperimental results on extensive data sets demonstrate that GNNSync attains\ncompetitive, and often superior, performance against a comprehensive set of\nbaselines for the angular synchronization problem and its extension, validating\nthe robustness of GNNSync even at high noise levels.",
            "author": [
                "Yixuan He",
                "Gesine Reinert",
                "David Wipf",
                "Mihai Cucuringu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05842v1",
                "http://arxiv.org/pdf/2310.05842v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "math.OC",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05839v1",
            "title": "Directed Symmetric Multicut is W[1]-hard",
            "updated": "2023-10-09T16:30:23Z",
            "published": "2023-10-09T16:30:23Z",
            "summary": "Given a directed graph $G$ and a set of vertex pairs $\\{(s_1,t_1), \\dots,\n(s_m, t_m)\\}$, the Directed Symmetric Multicut problem asks to delete the\nminimum number of edges from $G$ to separate every pair $(s_i, t_i)$ into\ndistinct strong components. Eiben, Rambaud and Wahlstr\\\"om [IPEC 2022]\ninitiated the study of this problem parameterized by the solution size. They\ngave a fixed-parameter tractable 2-approximation algorithm, and left the exact\nparameterized complexity status as an open question. We answer their question\nin negative, showing that Directed Symmetric Multicut is W[1]-hard.",
            "author": [
                "George Osipov",
                "Marcin Pilipczuk"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05839v1",
                "http://arxiv.org/pdf/2310.05839v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05813v2",
            "title": "Audio compression-assisted feature extraction for voice replay attack\n  detection",
            "updated": "2023-10-10T09:06:42Z",
            "published": "2023-10-09T15:53:42Z",
            "summary": "Replay attack is one of the most effective and simplest voice spoofing\nattacks. Detecting replay attacks is challenging, according to the Automatic\nSpeaker Verification Spoofing and Countermeasures Challenge 2021 (ASVspoof\n2021), because they involve a loudspeaker, a microphone, and acoustic\nconditions (e.g., background noise). One obstacle to detecting replay attacks\nis finding robust feature representations that reflect the channel noise\ninformation added to the replayed speech. This study proposes a feature\nextraction approach that uses audio compression for assistance. Audio\ncompression compresses audio to preserve content and speaker information for\ntransmission. The missed information after decompression is expected to contain\ncontent- and speaker-independent information (e.g., channel noise added during\nthe replay process). We conducted a comprehensive experiment with a few data\naugmentation techniques and 3 classifiers on the ASVspoof 2021 physical access\n(PA) set and confirmed the effectiveness of the proposed feature extraction\napproach. To the best of our knowledge, the proposed approach achieves the\nlowest EER at 22.71% on the ASVspoof 2021 PA evaluation set.",
            "author": [
                "Xiangyu Shi",
                "Yuhao Luo",
                "Li Wang",
                "Haorui He",
                "Hao Li",
                "Lei Wang",
                "Zhizheng Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05813v2",
                "http://arxiv.org/pdf/2310.05813v2"
            ],
            "primary_category": "cs.SD",
            "category": [
                "cs.SD",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05808v2",
            "title": "A Simple Open-Loop Baseline for Reinforcement Learning Locomotion Tasks",
            "updated": "2023-11-30T17:51:00Z",
            "published": "2023-10-09T15:45:08Z",
            "summary": "In search of the simplest baseline capable of competing with Deep\nReinforcement Learning on locomotion tasks, we propose a biologically inspired\nmodel-free open-loop strategy. Drawing upon prior knowledge and harnessing the\nelegance of simple oscillators to generate periodic joint motions, it achieves\nrespectable performance in five different locomotion environments, with a\nnumber of tunable parameters that is a tiny fraction of the thousands typically\nrequired by RL algorithms. Unlike RL methods, which are prone to performance\ndegradation when exposed to sensor noise or failure, our open-loop oscillators\nexhibit remarkable robustness due to their lack of reliance on sensors.\nFurthermore, we showcase a successful transfer from simulation to reality using\nan elastic quadruped, all without the need for randomization or reward\nengineering. Overall, the proposed baseline and associated experiments\nhighlight the existing limitations of DRL for robotic applications, provide\ninsights on how to address them, and encourage reflection on the costs of\ncomplexity and generality.",
            "author": [
                "Antonin Raffin",
                "Olivier Sigaud",
                "Jens Kober",
                "Alin Albu-Sch\u00e4ffer",
                "Jo\u00e3o Silv\u00e9rio",
                "Freek Stulp"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05808v2",
                "http://arxiv.org/pdf/2310.05808v2"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05797v2",
            "title": "Are Large Language Models Post Hoc Explainers?",
            "updated": "2023-10-10T19:33:38Z",
            "published": "2023-10-09T15:31:03Z",
            "summary": "Large Language Models (LLMs) are increasingly used as powerful tools for a\nplethora of natural language processing (NLP) applications. A recent\ninnovation, in-context learning (ICL), enables LLMs to learn new tasks by\nsupplying a few examples in the prompt during inference time, thereby\neliminating the need for model fine-tuning. While LLMs have been utilized in\nseveral applications, their applicability in explaining the behavior of other\nmodels remains relatively unexplored. Despite the growing number of new\nexplanation techniques, many require white-box access to the model and/or are\ncomputationally expensive, highlighting a need for next-generation post hoc\nexplainers. In this work, we present the first framework to study the\neffectiveness of LLMs in explaining other predictive models. More specifically,\nwe propose a novel framework encompassing multiple prompting strategies: i)\nPerturbation-based ICL, ii) Prediction-based ICL, iii) Instruction-based ICL,\nand iv) Explanation-based ICL, with varying levels of information about the\nunderlying ML model and the local neighborhood of the test sample. We conduct\nextensive experiments with real-world benchmark datasets to demonstrate that\nLLM-generated explanations perform on par with state-of-the-art post hoc\nexplainers using their ability to leverage ICL examples and their internal\nknowledge in generating model explanations. On average, across four datasets\nand two ML models, we observe that LLMs identify the most important feature\nwith 72.19% accuracy, opening up new frontiers in explainable artificial\nintelligence (XAI) to explore LLM-based explanation frameworks.",
            "author": [
                "Nicholas Kroeger",
                "Dan Ley",
                "Satyapriya Krishna",
                "Chirag Agarwal",
                "Himabindu Lakkaraju"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05797v2",
                "http://arxiv.org/pdf/2310.05797v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05795v1",
            "title": "Finitely presented subgroups of direct products of graphs of groups with\n  free abelian vertex groups",
            "updated": "2023-10-09T15:30:00Z",
            "published": "2023-10-09T15:30:00Z",
            "summary": "A result by Bridson, Howie, Miller, and Short states that if $S$ is a\nfinitely presented subgroup of the direct product of free groups, then $S$ is\nvirtually a nilpotent extension of a direct product of free groups. Moreover,\nif $S$ is a subgroup of type $FP_n$ of the direct product of $n$ free groups,\nthen the nilpotent extension is finite, so $S$ is actually virtually the direct\nproduct of free groups.\n  In this paper, these results are generalized to $2$-dimensional coherent\nright-angled Artin groups. More precisely, we show that a finitely presented\nsubgroup of the direct product of $2$-dimensional coherent RAAGs is still\nvirtually a nilpotent extension of a direct product of subgroups. If $S$ is\nmoreover a type $FP_n$ subgroup of the direct product of $n$ $2$-dimensional\ncoherent RAAGs, then $S$ is commensurable to a kernel of a character of a\ndirect product of subgroups.\n  Finally, we show that the multiple conjugacy problem and the membership\nproblem are decidable for finitely presented subgroups of direct products of\n$2$-dimensional coherent RAAGs.",
            "author": [
                "Montserrat Casals-Ruiz",
                "Jone Lopez de Gamiz Zearra"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05795v1",
                "http://arxiv.org/pdf/2310.05795v1"
            ],
            "primary_category": "math.GR",
            "category": [
                "math.GR",
                "20F65 (Primary), 20E08 (Secondary)"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05791v1",
            "title": "Problem-Solving Guide: Predicting the Algorithm Tags and Difficulty for\n  Competitive Programming Problems",
            "updated": "2023-10-09T15:26:07Z",
            "published": "2023-10-09T15:26:07Z",
            "summary": "The recent program development industries have required problem-solving\nabilities for engineers, especially application developers. However, AI-based\neducation systems to help solve computer algorithm problems have not yet\nattracted attention, while most big tech companies require the ability to solve\nalgorithm problems including Google, Meta, and Amazon. The most useful guide to\nsolving algorithm problems might be guessing the category (tag) of the facing\nproblems. Therefore, our study addresses the task of predicting the algorithm\ntag as a useful tool for engineers and developers. Moreover, we also consider\npredicting the difficulty levels of algorithm problems, which can be used as\nuseful guidance to calculate the required time to solve that problem. In this\npaper, we present a real-world algorithm problem multi-task dataset, AMT, by\nmainly collecting problem samples from the most famous and large competitive\nprogramming website Codeforces. To the best of our knowledge, our proposed\ndataset is the most large-scale dataset for predicting algorithm tags compared\nto previous studies. Moreover, our work is the first to address predicting the\ndifficulty levels of algorithm problems. We present a deep learning-based novel\nmethod for simultaneously predicting algorithm tags and the difficulty levels\nof an algorithm problem given. All datasets and source codes are available at\nhttps://github.com/sronger/PSG_Predicting_Algorithm_Tags_and_Difficulty.",
            "author": [
                "Juntae Kim",
                "Eunjung Cho",
                "Dongwoo Kim",
                "Dongbin Na"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05791v1",
                "http://arxiv.org/pdf/2310.05791v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05788v1",
            "title": "Canonization of a random circulant graph by counting walks",
            "updated": "2023-10-09T15:20:10Z",
            "published": "2023-10-09T15:20:10Z",
            "summary": "It is well known that almost all graphs are canonizable by a simple\ncombinatorial routine known as color refinement. With high probability, this\nmethod assigns a unique label to each vertex of a random input graph and,\nhence, it is applicable only to asymmetric graphs. The strength of\ncombinatorial refinement techniques becomes a subtle issue if the input graphs\nare highly symmetric. We prove that the combination of color refinement with\nvertex individualization produces a canonical labeling for almost all circulant\ndigraphs (Cayley digraphs of a cyclic group). To our best knowledge, this is\nthe first application of combinatorial refinement in the realm of\nvertex-transitive graphs. Remarkably, we do not even need the full power of the\ncolor refinement algorithm. We show that the canonical label of a vertex $v$\ncan be obtained just by counting walks of each length from $v$ to an\nindividualized vertex.",
            "author": [
                "Oleg Verbitsky",
                "Maksim Zhukovskii"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05788v1",
                "http://arxiv.org/pdf/2310.05788v1"
            ],
            "primary_category": "cs.CC",
            "category": [
                "cs.CC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05787v1",
            "title": "Exact threshold for approximate ellipsoid fitting of random points",
            "updated": "2023-10-09T15:18:56Z",
            "published": "2023-10-09T15:18:56Z",
            "summary": "We consider the problem $(\\rm P)$ of exactly fitting an ellipsoid (centered\nat $0$) to $n$ standard Gaussian random vectors in $\\mathbb{R}^d$, as $n, d \\to\n\\infty$ with $n / d^2 \\to \\alpha > 0$. This problem is conjectured to undergo a\nsharp transition: with high probability, $(\\rm P)$ has a solution if $\\alpha <\n1/4$, while $(\\rm P)$ has no solutions if $\\alpha > 1/4$. So far, only a\ntrivial bound $\\alpha > 1/2$ is known to imply the absence of solutions, while\nthe sharpest results on the positive side assume $\\alpha \\leq \\eta$ (for $\\eta\n> 0$ a small constant) to prove that $(\\rm P)$ is solvable. In this work we\nstudy universality between this problem and a so-called \"Gaussian equivalent\",\nfor which the same transition can be rigorously analyzed. Our main results are\ntwofold. On the positive side, we prove that if $\\alpha < 1/4$, there exist an\nellipsoid fitting all the points up to a small error, and that the lengths of\nits principal axes are bounded above and below. On the other hand, for $\\alpha\n> 1/4$, we show that achieving small fitting error is not possible if the\nlength of the ellipsoid's shortest axis does not approach $0$ as $d \\to \\infty$\n(and in particular there does not exist any ellipsoid fit whose shortest axis\nlength is bounded away from $0$ as $d \\to \\infty$). To the best of our\nknowledge, our work is the first rigorous result characterizing the expected\nphase transition in ellipsoid fitting at $\\alpha = 1/4$. In a companion\nnon-rigorous work, the first author and D. Kunisky give a general analysis of\nellipsoid fitting using the replica method of statistical physics, which\ninspired the present work.",
            "author": [
                "Antoine Maillard",
                "Afonso S. Bandeira"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05787v1",
                "http://arxiv.org/pdf/2310.05787v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR",
                "cond-mat.dis-nn",
                "cs.DS",
                "math.ST",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05784v2",
            "title": "The Parameterised Complexity of Integer Multicommodity Flow",
            "updated": "2023-10-23T08:32:43Z",
            "published": "2023-10-09T15:16:02Z",
            "summary": "The Integer Multicommodity Flow problem has been studied extensively in the\nliterature. However, from a parameterised perspective, mostly special cases,\nsuch as the Disjoint Paths problem, have been considered. Therefore, we\ninvestigate the parameterised complexity of the general Integer Multicommodity\nFlow problem. We show that the decision version of this problem on directed\ngraphs for a constant number of commodities, when the capacities are given in\nunary, is XNLP-complete with pathwidth as parameter and XALP-complete with\ntreewidth as parameter. When the capacities are given in binary, the problem is\nNP-complete even for graphs of pathwidth at most 13. We give related results\nfor undirected graphs. These results imply that the problem is unlikely to be\nfixed-parameter tractable by these parameters.\n  In contrast, we show that the problem does become fixed-parameter tractable\nwhen weighted tree partition width (a variant of tree partition width for edge\nweighted graphs) is used as parameter.",
            "author": [
                "Hans L. Bodlaender",
                "Isja Mannens",
                "Jelle J. Oostveen",
                "Sukanya Pandey",
                "Erik Jan van Leeuwen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05784v2",
                "http://arxiv.org/pdf/2310.05784v2"
            ],
            "primary_category": "cs.DM",
            "category": [
                "cs.DM",
                "cs.CC",
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05772v1",
            "title": "RateRL: A Framework for Developing RL-Based Rate Adaptation Algorithms\n  in ns-3",
            "updated": "2023-10-09T14:57:37Z",
            "published": "2023-10-09T14:57:37Z",
            "summary": "The increasing complexity of recent Wi-Fi amendments is making the use of\ntraditional algorithms and heuristics unfeasible to address the Rate Adaptation\n(RA) problem. This is due to the large combination of configuration parameters\nalong with the high variability of the wireless channel. Recently, several\nworks have proposed the usage of Reinforcement Learning (RL) techniques to\naddress the problem. However, the proposed solutions lack sufficient technical\nexplanation. Also, the lack of standard frameworks enabling the reproducibility\nof results and the limited availability of source code, makes the fair\ncomparison with state of the art approaches a challenge. This paper proposes a\nframework, named RateRL, that integrates state of the art libraries with the\nwell-known Network Simulator 3 (ns-3) to enable the implementation and\nevaluation of RL-based RA algorithms. To the best of our knowledge, RateRL is\nthe first tool available to assist researchers during the implementation,\nvalidation and evaluation phases of RL-based RA algorithms and enable the fair\ncomparison between competing algorithms.",
            "author": [
                "Ruben Queiros",
                "Luis Ferreira",
                "Helder Fontes",
                "Rui Campos"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05772v1",
                "http://arxiv.org/pdf/2310.05772v1"
            ],
            "primary_category": "cs.NI",
            "category": [
                "cs.NI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05761v1",
            "title": "Robust Minimum Distance Inference in Structural Models",
            "updated": "2023-10-09T14:41:53Z",
            "published": "2023-10-09T14:41:53Z",
            "summary": "This paper proposes minimum distance inference for a structural parameter of\ninterest, which is robust to the lack of identification of other structural\nnuisance parameters. Some choices of the weighting matrix lead to asymptotic\nchi-squared distributions with degrees of freedom that can be consistently\nestimated from the data, even under partial identification. In any case,\nknowledge of the level of under-identification is not required. We study the\npower of our robust test. Several examples show the wide applicability of the\nprocedure and a Monte Carlo investigates its finite sample performance. Our\nidentification-robust inference method can be applied to make inferences on\nboth calibrated (fixed) parameters and any other structural parameter of\ninterest. We illustrate the method's usefulness by applying it to a structural\nmodel on the non-neutrality of monetary policy, as in \\cite{nakamura2018high},\nwhere we empirically evaluate the validity of the calibrated parameters and we\ncarry out robust inference on the slope of the Phillips curve and the\ninformation effect.",
            "author": [
                "Joan Alegre",
                "Juan Carlos Escanciano"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05761v1",
                "http://arxiv.org/pdf/2310.05761v1"
            ],
            "primary_category": "econ.EM",
            "category": [
                "econ.EM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05757v1",
            "title": "Nonlinear Correct and Smooth for Semi-Supervised Learning",
            "updated": "2023-10-09T14:33:32Z",
            "published": "2023-10-09T14:33:32Z",
            "summary": "Graph-based semi-supervised learning (GSSL) has been used successfully in\nvarious applications. Existing methods leverage the graph structure and labeled\nsamples for classification. Label Propagation (LP) and Graph Neural Networks\n(GNNs) both iteratively pass messages on graphs, where LP propagates node\nlabels through edges and GNN aggregates node features from the neighborhood.\nRecently, combining LP and GNN has led to improved performance. However,\nutilizing labels and features jointly in higher-order graphs has not been\nexplored. Therefore, we propose Nonlinear Correct and Smooth (NLCS), which\nimproves the existing post-processing approach by incorporating non-linearity\nand higher-order representation into the residual propagation to handle\nintricate node relationships effectively. Systematic evaluations show that our\nmethod achieves remarkable average improvements of 13.71% over base prediction\nand 2.16% over the state-of-the-art post-processing method on six commonly used\ndatasets. Comparisons and analyses show our method effectively utilizes labels\nand features jointly in higher-order graphs to resolve challenging graph\nrelationships.",
            "author": [
                "Yuanhang Shao",
                "Xiuwen Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05757v1",
                "http://arxiv.org/pdf/2310.05757v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05733v1",
            "title": "Polyhedral approach to weighted connected matchings in general graphs",
            "updated": "2023-10-09T14:05:46Z",
            "published": "2023-10-09T14:05:46Z",
            "summary": "A connected matching in a graph G consists of a set of pairwise disjoint\nedges whose covered vertices induce a connected subgraph of G. While finding a\nconnected matching of maximum cardinality is a well-solved problem, it is\nNP-hard to determine an optimal connected matching in an edge-weighted graph,\neven in the planar bipartite case. We present two mixed integer programming\nformulations and a sophisticated branch-and-cut scheme to find weighted\nconnected matchings in general graphs. The formulations explore different\npolyhedra associated to this problem, including strong valid inequalities both\nfrom the matching polytope and from the connected subgraph polytope. We\nconjecture that one attains a tight approximation of the convex hull of\nconnected matchings using our strongest formulation, and report encouraging\ncomputational results over DIMACS Implementation Challenge benchmark instances.\nThe source code of the complete implementation is also made available.",
            "author": [
                "Phillippe Samer",
                "Phablo F. S. Moura"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05733v1",
                "http://arxiv.org/pdf/2310.05733v1"
            ],
            "primary_category": "cs.DM",
            "category": [
                "cs.DM",
                "cs.DS",
                "math.CO",
                "90C27, 90C57, 90C11, 68R10",
                "G.2.2; G.1.6; G.4"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05728v2",
            "title": "Hidden Permutations to the Rescue: Multi-Pass Semi-Streaming Lower\n  Bounds for Approximate Matchings",
            "updated": "2023-10-11T05:06:58Z",
            "published": "2023-10-09T13:59:13Z",
            "summary": "We prove that any semi-streaming algorithm for $(1-\\epsilon)$-approximation\nof maximum bipartite matching requires \\[\n\\Omega(\\frac{\\log{(1/\\epsilon)}}{{\\log{(1/\\beta)}}}) \\] passes, where $\\beta\n\\in (0,1)$ is the largest parameter so that an $n$-vertex graph with\n$n^{\\beta}$ edge-disjoint induced matchings of size $\\Theta(n)$ exist (such\ngraphs are referred to as RS graphs). Currently, it is known that \\[\n\\Omega(\\frac{1}{\\log\\log{n}}) \\leqslant \\beta \\leqslant\n1-\\Theta(\\frac{\\log^*{n}}{{\\log{n}}}) \\] and closing this huge gap between\nupper and lower bounds has remained a notoriously difficult problem in\ncombinatorics.\n  Under the plausible hypothesis that $\\beta = \\Omega(1)$, our lower bound\nresult provides the first pass-approximation lower bound for (small) constant\napproximation of matchings in the semi-streaming model, a longstanding open\nquestion in the graph streaming literature.\n  Our techniques are based on analyzing communication protocols for compressing\n(hidden) permutations. Prior work in this context relied on reducing such\nproblems to Boolean domain and analyzing them via tools like XOR Lemmas and\nFourier analysis on Boolean hypercube. In contrast, our main technical\ncontribution is a hardness amplification result for permutations through\nconcatenation in place of prior XOR Lemmas. This result is proven by analyzing\npermutations directly via simple tools from group representation theory\ncombined with detailed information-theoretic arguments, and can be of\nindependent interest.",
            "author": [
                "Sepehr Assadi",
                "Janani Sundaresan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05728v2",
                "http://arxiv.org/pdf/2310.05728v2"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "cs.CC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05714v1",
            "title": "DecAP: Decaying Action Priors for Accelerated Learning of Torque-Based\n  Legged Locomotion Policies",
            "updated": "2023-10-09T13:38:03Z",
            "published": "2023-10-09T13:38:03Z",
            "summary": "Optimal Control for legged robots has gone through a paradigm shift from\nposition-based to torque-based control, owing to the latter's compliant and\nrobust nature. In parallel to this shift, the community has also turned to Deep\nReinforcement Learning (DRL) as a promising approach to directly learn\nlocomotion policies for complex real-life tasks. However, most end-to-end DRL\napproaches still operate in position space, mainly because learning in torque\nspace is often sample-inefficient and does not consistently converge to natural\ngaits. To address these challenges, we introduce Decaying Action Priors\n(DecAP), a novel three-stage framework to learn and deploy torque policies for\nlegged locomotion. In the first stage, we generate our own imitation data by\ntraining a position policy, eliminating the need for expert knowledge in\ndesigning optimal controllers. The second stage incorporates decaying action\npriors to enhance the exploration of torque-based policies aided by imitation\nrewards. We show that our approach consistently outperforms imitation learning\nalone and is significantly robust to the scaling of these rewards. Finally, our\nthird stage facilitates safe sim-to-real transfer by directly deploying our\nlearned torques, alongside low-gain PID control from our trained position\npolicy. We demonstrate the generality of our approach by training torque-based\nlocomotion policies for a biped, a quadruped, and a hexapod robot in\nsimulation, and experimentally demonstrate our learned policies on a quadruped\n(Unitree Go1).",
            "author": [
                "Shivam Sood",
                "Ge Sun",
                "Peizhuo Li",
                "Guillaume Sartoretti"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05714v1",
                "http://arxiv.org/pdf/2310.05714v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05703v3",
            "title": "An Attribution Method for Siamese Encoders",
            "updated": "2023-11-29T15:12:00Z",
            "published": "2023-10-09T13:24:44Z",
            "summary": "Despite the success of Siamese encoder models such as sentence transformers\n(ST), little is known about the aspects of inputs they pay attention to. A\nbarrier is that their predictions cannot be attributed to individual features,\nas they compare two inputs rather than processing a single one. This paper\nderives a local attribution method for Siamese encoders by generalizing the\nprinciple of integrated gradients to models with multiple inputs. The solution\ntakes the form of feature-pair attributions, and can be reduced to a\ntoken-token matrix for STs. Our method involves the introduction of integrated\nJacobians and inherits the advantageous formal properties of integrated\ngradients: it accounts for the model's full computation graph and is guaranteed\nto converge to the actual prediction. A pilot study shows that in an ST few\ntoken-pairs can often explain large fractions of predictions, and it focuses on\nnouns and verbs. For accurate predictions, it however needs to attend to the\nmajority of tokens and parts of speech.",
            "author": [
                "Lucas M\u00f6ller",
                "Dmitry Nikolaev",
                "Sebastian Pad\u00f3"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05703v3",
                "http://arxiv.org/pdf/2310.05703v3"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05694v1",
            "title": "A Survey of Large Language Models for Healthcare: from Data, Technology,\n  and Applications to Accountability and Ethics",
            "updated": "2023-10-09T13:15:23Z",
            "published": "2023-10-09T13:15:23Z",
            "summary": "The utilization of large language models (LLMs) in the Healthcare domain has\ngenerated both excitement and concern due to their ability to effectively\nrespond to freetext queries with certain professional knowledge. This survey\noutlines the capabilities of the currently developed LLMs for Healthcare and\nexplicates their development process, with the aim of providing an overview of\nthe development roadmap from traditional Pretrained Language Models (PLMs) to\nLLMs. Specifically, we first explore the potential of LLMs to enhance the\nefficiency and effectiveness of various Healthcare applications highlighting\nboth the strengths and limitations. Secondly, we conduct a comparison between\nthe previous PLMs and the latest LLMs, as well as comparing various LLMs with\neach other. Then we summarize related Healthcare training data, training\nmethods, optimization strategies, and usage. Finally, the unique concerns\nassociated with deploying LLMs in Healthcare settings are investigated,\nparticularly regarding fairness, accountability, transparency and ethics. Our\nsurvey provide a comprehensive investigation from perspectives of both computer\nscience and Healthcare specialty. Besides the discussion about Healthcare\nconcerns, we supports the computer science community by compiling a collection\nof open source resources, such as accessible datasets, the latest\nmethodologies, code implementations, and evaluation benchmarks in the Github.\nSummarily, we contend that a significant paradigm shift is underway,\ntransitioning from PLMs to LLMs. This shift encompasses a move from\ndiscriminative AI approaches to generative AI approaches, as well as a shift\nfrom model-centered methodologies to datacentered methodologies.",
            "author": [
                "Kai He",
                "Rui Mao",
                "Qika Lin",
                "Yucheng Ruan",
                "Xiang Lan",
                "Mengling Feng",
                "Erik Cambria"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05694v1",
                "http://arxiv.org/pdf/2310.05694v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05692v1",
            "title": "Based on What We Can Control Artificial Neural Networks",
            "updated": "2023-10-09T13:09:38Z",
            "published": "2023-10-09T13:09:38Z",
            "summary": "How can the stability and efficiency of Artificial Neural Networks (ANNs) be\nensured through a systematic analysis method? This paper seeks to address that\nquery. While numerous factors can influence the learning process of ANNs,\nutilizing knowledge from control systems allows us to analyze its system\nfunction and simulate system responses. Although the complexity of most ANNs is\nextremely high, we still can analyze each factor (e.g., optimiser,\nhyperparameters) by simulating their system response. This new method also can\npotentially benefit the development of new optimiser and learning system,\nespecially when discerning which components adversely affect ANNs. Controlling\nANNs can benefit from the design of optimiser and learning system, as (1) all\noptimisers act as controllers, (2) all learning systems operate as control\nsystems with inputs and outputs, and (3) the optimiser should match the\nlearning system. Please find codes:\n\\url{https://github.com/RandomUserName2023/Control-ANNs}.",
            "author": [
                "Cheng Kang",
                "Xujing Yao"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05692v1",
                "http://arxiv.org/pdf/2310.05692v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05689v1",
            "title": "Linear Opinion Dynamics Model with Higher-Order Interactions",
            "updated": "2023-10-09T12:56:11Z",
            "published": "2023-10-09T12:56:11Z",
            "summary": "Opinion dynamics is a central subject of computational social science, and\nvarious models have been developed to understand the evolution and formulation\nof opinions. Existing models mainly focus on opinion dynamics on graphs that\nonly capture pairwise interactions between agents. In this paper, we extend the\npopular Friedkin-Johnsen model for opinion dynamics on graphs to hypergraphs,\nwhich describe higher-order interactions occurring frequently on real networks,\nespecially social networks. To achieve this, based on the fact that for linear\ndynamics the multi-way interactions can be reduced to effective pairwise node\ninteractions, we propose a method to decode the group interactions encoded in\nhyperedges by undirected edges or directed edges in graphs. We then show that\nhigher-order interactions play an important role in the opinion dynamics, since\nthe overall steady-state expressed opinion and polarization differ greatly from\nthose without group interactions. We also provide an interpretation of the\nequilibrium expressed opinion from the perspective of the spanning converging\nforest, based on which we design a fast sampling algorithm to approximately\nevaluate the overall opinion and opinion polarization on directed weighted\ngraphs. Finally, we conduct experiments on real-world hypergraph datasets,\ndemonstrating the performance of our algorithm.",
            "author": [
                "Wanyue Xu",
                "Zhongzhi Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05689v1",
                "http://arxiv.org/pdf/2310.05689v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05688v1",
            "title": "Larth: Dataset and Machine Translation for Etruscan",
            "updated": "2023-10-09T12:56:08Z",
            "published": "2023-10-09T12:56:08Z",
            "summary": "Etruscan is an ancient language spoken in Italy from the 7th century BC to\nthe 1st century AD. There are no native speakers of the language at the present\nday, and its resources are scarce, as there exist only around 12,000 known\ninscriptions. To the best of our knowledge, there are no publicly available\nEtruscan corpora for natural language processing. Therefore, we propose a\ndataset for machine translation from Etruscan to English, which contains 2891\ntranslated examples from existing academic sources. Some examples are extracted\nmanually, while others are acquired in an automatic way. Along with the\ndataset, we benchmark different machine translation models observing that it is\npossible to achieve a BLEU score of 10.1 with a small transformer model.\nReleasing the dataset can help enable future research on this language, similar\nlanguages or other languages with scarce resources.",
            "author": [
                "Gianluca Vico",
                "Gerasimos Spanakis"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05688v1",
                "http://arxiv.org/pdf/2310.05688v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05684v1",
            "title": "On the expansiveness of coarse maps between Banach spaces and geometry\n  preservation",
            "updated": "2023-10-09T12:53:47Z",
            "published": "2023-10-09T12:53:47Z",
            "summary": "We introduce a new notion of embeddability between Banach spaces. By studying\nthe classical Mazur map, we show that it is strictly weaker than the notion of\ncoarse embeddability. We use the techniques from metric cotype introduced by M.\nMendel and A. Naor to prove results about cotype preservation and complete our\nstudy of embeddability between $\\ell_p$ spaces. We confront our notion with\nnonlinear invariants introduced by N. Kalton, which are defined in terms of\nconcentration properties for Lipschitz maps defined on countably branching\nHamming or interlaced graphs. Finally, we address the problem of the\nembeddability into $\\ell_\\infty$.",
            "author": [
                "Bruno de Mendon\u00e7a Braga",
                "Gilles Lancien"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05684v1",
                "http://arxiv.org/pdf/2310.05684v1"
            ],
            "primary_category": "math.FA",
            "category": [
                "math.FA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05655v1",
            "title": "Causal structure learning with momentum: Sampling distributions over\n  Markov Equivalence Classes of DAGs",
            "updated": "2023-10-09T12:10:51Z",
            "published": "2023-10-09T12:10:51Z",
            "summary": "In the context of inferring a Bayesian network structure (directed acyclic\ngraph, DAG for short), we devise a non-reversible continuous time Markov chain,\nthe \"Causal Zig-Zag sampler\", that targets a probability distribution over\nclasses of observationally equivalent (Markov equivalent) DAGs. The classes are\nrepresented as completed partially directed acyclic graphs (CPDAGs). The\nnon-reversible Markov chain relies on the operators used in Chickering's Greedy\nEquivalence Search (GES) and is endowed with a momentum variable, which\nimproves mixing significantly as we show empirically. The possible target\ndistributions include posterior distributions based on a prior over DAGs and a\nMarkov equivalent likelihood. We offer an efficient implementation wherein we\ndevelop new algorithms for listing, counting, uniformly sampling, and applying\npossible moves of the GES operators, all of which significantly improve upon\nthe state-of-the-art.",
            "author": [
                "Moritz Schauer",
                "Marcel Wien\u00f6bst"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05655v1",
                "http://arxiv.org/pdf/2310.05655v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.AI",
                "cs.LG",
                "68T37 (primary) 60J99 (secondary)"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05654v2",
            "title": "No Token Left Behind: Efficient Vision Transformer via Dynamic Token\n  Idling",
            "updated": "2023-11-05T07:33:21Z",
            "published": "2023-10-09T12:10:41Z",
            "summary": "Vision Transformers (ViTs) have demonstrated outstanding performance in\ncomputer vision tasks, yet their high computational complexity prevents their\ndeployment in computing resource-constrained environments. Various token\npruning techniques have been introduced to alleviate the high computational\nburden of ViTs by dynamically dropping image tokens. However, some undesirable\npruning at early stages may result in permanent loss of image information in\nsubsequent layers, consequently hindering model performance. To address this\nproblem, we propose IdleViT, a dynamic token-idle-based method that achieves an\nexcellent trade-off between performance and efficiency. Specifically, in each\nlayer, IdleViT selects a subset of the image tokens to participate in\ncomputations while keeping the rest of the tokens idle and directly passing\nthem to this layer's output. By allowing the idle tokens to be re-selected in\nthe following layers, IdleViT mitigates the negative impact of improper pruning\nin the early stages. Furthermore, inspired by the normalized graph cut, we\ndevise a token cut loss on the attention map as regularization to improve\nIdleViT's token selection ability. Our method is simple yet effective and can\nbe extended to pyramid ViTs since no token is completely dropped. Extensive\nexperimental results on various ViT architectures have shown that IdleViT can\ndiminish the complexity of pretrained ViTs by up to 33\\% with no more than\n0.2\\% accuracy decrease on ImageNet, after finetuning for only 30 epochs.\nNotably, when the keep ratio is 0.5, IdleViT outperforms the state-of-the-art\nEViT on DeiT-S by 0.5\\% higher accuracy and even faster inference speed. The\nsource code is available in the supplementary material.",
            "author": [
                "Xuwei Xu",
                "Changlin Li",
                "Yudong Chen",
                "Xiaojun Chang",
                "Jiajun Liu",
                "Sen Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05654v2",
                "http://arxiv.org/pdf/2310.05654v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05651v1",
            "title": "FENCE: Fairplay Ensuring Network Chain Entity for Real-Time Multiple ID\n  Detection at Scale In Fantasy Sports",
            "updated": "2023-10-09T12:04:50Z",
            "published": "2023-10-09T12:04:50Z",
            "summary": "Dream11 takes pride in being a unique platform that enables over 190 million\nfantasy sports users to demonstrate their skills and connect deeper with their\nfavorite sports. While managing such a scale, one issue we are faced with is\nduplicate/multiple account creation in the system. This is done by some users\nwith the intent of abusing the platform, typically for bonus offers. The\nchallenge is to detect these multiple accounts before it is too late. We\npropose a graph-based solution to solve this problem in which we first predict\nedges/associations between users. Using the edge information we highlight\nclusters of colluding multiple accounts. In this paper, we talk about our\ndistributed ML system which is deployed to serve and support the inferences\nfrom our detection models. The challenge is to do this in real-time in order to\ntake corrective actions. A core part of this setup also involves\nhuman-in-the-loop components for validation, feedback, and ground-truth\nlabeling.",
            "author": [
                "Akriti Upreti",
                "Kartavya Kothari",
                "Utkarsh Thukral",
                "Vishal Verma"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05651v1",
                "http://arxiv.org/pdf/2310.05651v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "I.2.1"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05650v1",
            "title": "RAUCG: Retrieval-Augmented Unsupervised Counter Narrative Generation for\n  Hate Speech",
            "updated": "2023-10-09T12:01:26Z",
            "published": "2023-10-09T12:01:26Z",
            "summary": "The Counter Narrative (CN) is a promising approach to combat online hate\nspeech (HS) without infringing on freedom of speech. In recent years, there has\nbeen a growing interest in automatically generating CNs using natural language\ngeneration techniques. However, current automatic CN generation methods mainly\nrely on expert-authored datasets for training, which are time-consuming and\nlabor-intensive to acquire. Furthermore, these methods cannot directly obtain\nand extend counter-knowledge from external statistics, facts, or examples. To\naddress these limitations, we propose Retrieval-Augmented Unsupervised Counter\nNarrative Generation (RAUCG) to automatically expand external counter-knowledge\nand map it into CNs in an unsupervised paradigm. Specifically, we first\nintroduce an SSF retrieval method to retrieve counter-knowledge from the\nmultiple perspectives of stance consistency, semantic overlap rate, and fitness\nfor HS. Then we design an energy-based decoding mechanism by quantizing\nknowledge injection, countering and fluency constraints into differentiable\nfunctions, to enable the model to build mappings from counter-knowledge to CNs\nwithout expert-authored CN data. Lastly, we comprehensively evaluate model\nperformance in terms of language quality, toxicity, persuasiveness, relevance,\nand success rate of countering HS, etc. Experimental results show that RAUCG\noutperforms strong baselines on all metrics and exhibits stronger\ngeneralization capabilities, achieving significant improvements of +2.0% in\nrelevance and +4.5% in success rate of countering metrics. Moreover, RAUCG\nenabled GPT2 to outperform T0 in all metrics, despite the latter being\napproximately eight times larger than the former. Warning: This paper may\ncontain offensive or upsetting content!",
            "author": [
                "Shuyu Jiang",
                "Wenyi Tang",
                "Xingshu Chen",
                "Rui Tanga",
                "Haizhou Wang",
                "Wenxian Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05650v1",
                "http://arxiv.org/pdf/2310.05650v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05647v3",
            "title": "Exploiting Manifold Structured Data Priors for Improved MR\n  Fingerprinting Reconstruction",
            "updated": "2023-10-17T03:28:55Z",
            "published": "2023-10-09T11:59:11Z",
            "summary": "Estimating tissue parameter maps with high accuracy and precision from highly\nundersampled measurements presents one of the major challenges in MR\nfingerprinting (MRF). Many existing works project the recovered voxel\nfingerprints onto the Bloch manifold to improve reconstruction performance.\nHowever, little research focuses on exploiting the latent manifold structure\npriors among fingerprints. To fill this gap, we propose a novel MRF\nreconstruction framework based on manifold structured data priors. Since it is\ndifficult to directly estimate the fingerprint manifold structure, we model the\ntissue parameters as points on a low-dimensional parameter manifold. We reveal\nthat the fingerprint manifold shares the same intrinsic topology as the\nparameter manifold, although being embedded in different Euclidean spaces. To\nexploit the non-linear and non-local redundancies in MRF data, we divide the\nMRF data into spatial patches, and the similarity measurement among data\npatches can be accurately obtained using the Euclidean distance between the\ncorresponding patches in the parameter manifold. The measured similarity is\nthen used to construct the graph Laplacian operator, which represents the\nfingerprint manifold structure. Thus, the fingerprint manifold structure is\nintroduced in the reconstruction framework by using the low-dimensional\nparameter manifold. Additionally, we incorporate the locally low-rank prior in\nthe reconstruction framework to further utilize the local correlations within\neach patch for improved reconstruction performance. We also adopt a\nGPU-accelerated NUFFT library to accelerate reconstruction in non-Cartesian\nsampling scenarios. Experimental results demonstrate that our method can\nachieve significantly improved reconstruction performance with reduced\ncomputational time over the state-of-the-art methods.",
            "author": [
                "Peng Li",
                "Yuping Ji",
                "Yue Hu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05647v3",
                "http://arxiv.org/pdf/2310.05647v3"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV",
                "I.4.5; I.2.6"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05642v1",
            "title": "Plug n' Play: Channel Shuffle Module for Enhancing Tiny Vision\n  Transformers",
            "updated": "2023-10-09T11:56:35Z",
            "published": "2023-10-09T11:56:35Z",
            "summary": "Vision Transformers (ViTs) have demonstrated remarkable performance in\nvarious computer vision tasks. However, the high computational complexity\nhinders ViTs' applicability on devices with limited memory and computing\nresources. Although certain investigations have delved into the fusion of\nconvolutional layers with self-attention mechanisms to enhance the efficiency\nof ViTs, there remains a knowledge gap in constructing tiny yet effective ViTs\nsolely based on the self-attention mechanism. Furthermore, the straightforward\nstrategy of reducing the feature channels in a large but outperforming ViT\noften results in significant performance degradation despite improved\nefficiency. To address these challenges, we propose a novel channel shuffle\nmodule to improve tiny-size ViTs, showing the potential of pure self-attention\nmodels in environments with constrained computing resources. Inspired by the\nchannel shuffle design in ShuffleNetV2 \\cite{ma2018shufflenet}, our module\nexpands the feature channels of a tiny ViT and partitions the channels into two\ngroups: the \\textit{Attended} and \\textit{Idle} groups. Self-attention\ncomputations are exclusively employed on the designated \\textit{Attended}\ngroup, followed by a channel shuffle operation that facilitates information\nexchange between the two groups. By incorporating our module into a tiny ViT,\nwe can achieve superior performance while maintaining a comparable\ncomputational complexity to the vanilla model. Specifically, our proposed\nchannel shuffle module consistently improves the top-1 accuracy on the\nImageNet-1K dataset for various tiny ViT models by up to 2.8\\%, with the\nchanges in model complexity being less than 0.03 GMACs.",
            "author": [
                "Xuwei Xu",
                "Sen Wang",
                "Yudong Chen",
                "Jiajun Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05642v1",
                "http://arxiv.org/pdf/2310.05642v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05634v1",
            "title": "Towards Verifiable Generation: A Benchmark for Knowledge-aware Language\n  Model Attribution",
            "updated": "2023-10-09T11:45:59Z",
            "published": "2023-10-09T11:45:59Z",
            "summary": "Although achieving great success, Large Language Models (LLMs) usually suffer\nfrom unreliable hallucinations. In this paper, we define a new task of\nKnowledge-aware Language Model Attribution (KaLMA) that improves upon three\ncore concerns on conventional attributed LMs. First, we extend attribution\nsource from unstructured texts to Knowledge Graph (KG), whose rich structures\nbenefit both the attribution performance and working scenarios. Second, we\npropose a new ``Conscious Incompetence\" setting considering the incomplete\nknowledge repository, where the model identifies the need for supporting\nknowledge beyond the provided KG. Third, we propose a comprehensive automatic\nevaluation metric encompassing text quality, citation quality, and text\ncitation alignment. To implement the above innovations, we build a dataset in\nbiography domain BioKaLMA via a well-designed evolutionary question generation\nstrategy, to control the question complexity and necessary knowledge to the\nanswer. For evaluation, we develop a baseline solution and demonstrate the room\nfor improvement in LLMs' citation generation, emphasizing the importance of\nincorporating the \"Conscious Incompetence\" setting, and the critical role of\nretrieval accuracy.",
            "author": [
                "Xinze Li",
                "Yixin Cao2",
                "Liangming Pan",
                "Yubo Ma",
                "Aixin Sun"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05634v1",
                "http://arxiv.org/pdf/2310.05634v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05628v2",
            "title": "Glitter or Gold? Deriving Structured Insights from Sustainability\n  Reports via Large Language Models",
            "updated": "2023-11-03T10:43:06Z",
            "published": "2023-10-09T11:34:41Z",
            "summary": "Over the last decade, several regulatory bodies have started requiring the\ndisclosure of non-financial information from publicly listed companies, in\nlight of the investors' increasing attention to Environmental, Social, and\nGovernance (ESG) issues. Such information is publicly released in a variety of\nnon-structured and multi-modal documentation. Hence, it is not straightforward\nto aggregate and consolidate such data in a cohesive framework to further\nderive insights about sustainability practices across companies and markets.\nGiven these premises, it is natural to resort to Information Extraction (IE)\ntechniques to provide concise, informative, and actionable data to the\nstakeholders. Moving beyond traditional text processing techniques, in this\nwork we leverage Large Language Models (LLMs), along with the prominent\nin-context learning technique and the Retrieved Augmented Generation (RAG)\nparadigm, to extract semantically structured ESG-related information from\ncompanies' sustainability reports. We then adopt graph-based representations to\nconduct meaningful statistical, similarity and correlation analyses concerning\nthe ESG-related actions disclosed by companies in their sustainability reports.\nThese analyses unveiled that companies address ESG-related issues through\nseveral actions encompassing recognition, compliance, and partnerships;\nhighlighting the complexity and joint efforts needed to address them. Moreover,\ndisclosure similarities emerged among companies from the same region or sector.\nLastly, we investigate which factual aspects impact the most on companies' ESG\nscores using our findings and other company information. This analysis unveiled\nthat companies' disclosures affect ESG scores more than other financial or\ncompany characteristics.",
            "author": [
                "Marco Bronzini",
                "Carlo Nicolini",
                "Bruno Lepri",
                "Andrea Passerini",
                "Jacopo Staiano"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05628v2",
                "http://arxiv.org/pdf/2310.05628v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.CE",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05619v2",
            "title": "Dynamic Top-k Estimation Consolidates Disagreement between Feature\n  Attribution Methods",
            "updated": "2023-11-03T12:11:17Z",
            "published": "2023-10-09T11:19:33Z",
            "summary": "Feature attribution scores are used for explaining the prediction of a text\nclassifier to users by highlighting a k number of tokens. In this work, we\npropose a way to determine the number of optimal k tokens that should be\ndisplayed from sequential properties of the attribution scores. Our approach is\ndynamic across sentences, method-agnostic, and deals with sentence length bias.\nWe compare agreement between multiple methods and humans on an NLI task, using\nfixed k and dynamic k. We find that perturbation-based methods and Vanilla\nGradient exhibit highest agreement on most method--method and method--human\nagreement metrics with a static k. Their advantage over other methods\ndisappears with dynamic ks which mainly improve Integrated Gradient and\nGradientXInput. To our knowledge, this is the first evidence that sequential\nproperties of attribution scores are informative for consolidating attribution\nsignals for human interpretation.",
            "author": [
                "Jonathan Kamp",
                "Lisa Beinborn",
                "Antske Fokkens"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05619v2",
                "http://arxiv.org/pdf/2310.05619v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05609v1",
            "title": "Edge-Locating Coloring of Graphs",
            "updated": "2023-10-09T10:52:13Z",
            "published": "2023-10-09T10:52:13Z",
            "summary": "An edge-locating coloring of a simple connected graph $G$ is a partition of\nits edge set into matchings such that the vertices of $G$ are distinguished by\nthe distance to the matchings. The minimum number of the matchings of $G$ that\nadmits an edge-locating coloring is the edge-locating chromatic number of $G$,\nand denoted by $\\chi'_L(G)$. In this paper we initiate to introduce the concept\nof edge-locating coloring and determine the exact values $\\chi'_L(G)$ of some\ncustom graphs. The graphs $G$ with $\\chi'_L(G)\\in \\{2,m\\}$ are characterized,\nwhere $m$ is the size of $G$. We investigate the relationship between order,\ndiameter, and edge-locating chromatic number of $G$. For a complete graph\n$K_n$, we obtain the exact values of $\\chi'_L(K_n)$ and $\\chi'_L(K_n-M)$, where\n$M$ is a maximum matching; indeed this result is also extended for any graph.\nWe will determine the edge-locating chromatic number of join graph $G+H$, where\n$G$ and $H$ are some well-known graphs. In particular, for any graph $G$, we\nshow a relationship between $\\chi'_L(G+K_1)$ and $\\Delta(G)$. We investigate\nthe edge-locating chromatic number of trees and present a characterization\nbound for any tree in terms of maximum degree, number of leaves, and the\nsupport vertices of trees. Finally, we prove that any edge-locating coloring of\na graph is an edge distinguishing coloring.",
            "author": [
                "M. Korivand",
                "D. A. Mojdeh",
                "Edy Tri Baskoro",
                "A. Erfanian"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05609v1",
                "http://arxiv.org/pdf/2310.05609v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05601v1",
            "title": "On forbidding graphs as traces of hypergraphs",
            "updated": "2023-10-09T10:37:05Z",
            "published": "2023-10-09T10:37:05Z",
            "summary": "We say that a hypergraph $\\mathcal{H}$ contains a graph $H$ as a trace if\nthere exists some set $S\\subset V(\\mathcal{H})$ such that\n$\\mathcal{H}|_S=\\{h\\cap S: h\\in E(\\mathcal{H})\\}$ contains a subhypergraph\nisomorphic to $H$. We study the largest number of hyperedges in uniform\nhypergraphs avoiding some graph $F$ as trace. In particular, we determine this\nnumber in the case $F=K_3$ and any uniformity, resolving a special case of a\nconjecture of Mubayi and Zhao, and we improve a bound given by Luo and Spiro in\nthe case $F=C_4$ and uniformity 3.",
            "author": [
                "D\u00e1niel Gerbner"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05601v1",
                "http://arxiv.org/pdf/2310.05601v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05596v1",
            "title": "Stability analysis for the anisotropic curve shortening flow of planar\n  networks",
            "updated": "2023-10-09T10:32:27Z",
            "published": "2023-10-09T10:32:27Z",
            "summary": "In this article we study the anisotropic curve shortening flow for a planar\nnetwork of three curves with fixed endpoints and which meet in a triple\njunction. We show that the anisotropic curvature energy fulfills a\nLojasiewicz-Simon gradient inequality and use this knowledge to derive\nstability results for the flow. Precisely, in our main theorem we show that for\nany initial data, which are $C^{2,\\alpha}$-close to a (local) energy minimizer,\nthe flow exists globally and converges to a possibly different energy minimum.",
            "author": [
                "Michael G\u00f6\u00dfwein",
                "Matteo Novaga",
                "Paola Pozzi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05596v1",
                "http://arxiv.org/pdf/2310.05596v1"
            ],
            "primary_category": "math.AP",
            "category": [
                "math.AP",
                "math.DG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05595v1",
            "title": "Decoding the Threat Landscape : ChatGPT, FraudGPT, and WormGPT in Social\n  Engineering Attacks",
            "updated": "2023-10-09T10:31:04Z",
            "published": "2023-10-09T10:31:04Z",
            "summary": "In the ever-evolving realm of cybersecurity, the rise of generative AI models\nlike ChatGPT, FraudGPT, and WormGPT has introduced both innovative solutions\nand unprecedented challenges. This research delves into the multifaceted\napplications of generative AI in social engineering attacks, offering insights\ninto the evolving threat landscape using the blog mining technique. Generative\nAI models have revolutionized the field of cyberattacks, empowering malicious\nactors to craft convincing and personalized phishing lures, manipulate public\nopinion through deepfakes, and exploit human cognitive biases. These models,\nChatGPT, FraudGPT, and WormGPT, have augmented existing threats and ushered in\nnew dimensions of risk. From phishing campaigns that mimic trusted\norganizations to deepfake technology impersonating authoritative figures, we\nexplore how generative AI amplifies the arsenal of cybercriminals. Furthermore,\nwe shed light on the vulnerabilities that AI-driven social engineering\nexploits, including psychological manipulation, targeted phishing, and the\ncrisis of authenticity. To counter these threats, we outline a range of\nstrategies, including traditional security measures, AI-powered security\nsolutions, and collaborative approaches in cybersecurity. We emphasize the\nimportance of staying vigilant, fostering awareness, and strengthening\nregulations in the battle against AI-enhanced social engineering attacks. In an\nenvironment characterized by the rapid evolution of AI models and a lack of\ntraining data, defending against generative AI threats requires constant\nadaptation and the collective efforts of individuals, organizations, and\ngovernments. This research seeks to provide a comprehensive understanding of\nthe dynamic interplay between generative AI and social engineering attacks,\nequipping stakeholders with the knowledge to navigate this intricate\ncybersecurity landscape.",
            "author": [
                "Polra Victor Falade"
            ],
            "link": [
                "http://dx.doi.org/10.32628/CSEIT2390533",
                "http://arxiv.org/abs/2310.05595v1",
                "http://arxiv.org/pdf/2310.05595v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05589v1",
            "title": "DRIN: Dynamic Relation Interactive Network for Multimodal Entity Linking",
            "updated": "2023-10-09T10:21:42Z",
            "published": "2023-10-09T10:21:42Z",
            "summary": "Multimodal Entity Linking (MEL) is a task that aims to link ambiguous\nmentions within multimodal contexts to referential entities in a multimodal\nknowledge base. Recent methods for MEL adopt a common framework: they first\ninteract and fuse the text and image to obtain representations of the mention\nand entity respectively, and then compute the similarity between them to\npredict the correct entity. However, these methods still suffer from two\nlimitations: first, as they fuse the features of text and image before\nmatching, they cannot fully exploit the fine-grained alignment relations\nbetween the mention and entity. Second, their alignment is static, leading to\nlow performance when dealing with complex and diverse data. To address these\nissues, we propose a novel framework called Dynamic Relation Interactive\nNetwork (DRIN) for MEL tasks. DRIN explicitly models four different types of\nalignment between a mention and entity and builds a dynamic Graph Convolutional\nNetwork (GCN) to dynamically select the corresponding alignment relations for\ndifferent input samples. Experiments on two datasets show that DRIN outperforms\nstate-of-the-art methods by a large margin, demonstrating the effectiveness of\nour approach.",
            "author": [
                "Shangyu Xing",
                "Fei Zhao",
                "Zhen Wu",
                "Chunhui Li",
                "Jianbing Zhang",
                "Xinyu Dai"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05589v1",
                "http://arxiv.org/pdf/2310.05589v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.MM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05563v1",
            "title": "STREAM: Social data and knowledge collective intelligence platform for\n  TRaining Ethical AI Models",
            "updated": "2023-10-09T09:40:11Z",
            "published": "2023-10-09T09:40:11Z",
            "summary": "This paper presents Social data and knowledge collective intelligence\nplatform for TRaining Ethical AI Models (STREAM) to address the challenge of\naligning AI models with human moral values, and to provide ethics datasets and\nknowledge bases to help promote AI models \"follow good advice as naturally as a\nstream follows its course\". By creating a comprehensive and representative\nplatform that accurately mirrors the moral judgments of diverse groups\nincluding humans and AIs, we hope to effectively portray cultural and group\nvariations, and capture the dynamic evolution of moral judgments over time,\nwhich in turn will facilitate the Establishment, Evaluation, Embedding,\nEmbodiment, Ensemble, and Evolvement (6Es) of the moral capabilities of AI\nmodels. Currently, STREAM has already furnished a comprehensive collection of\nethical scenarios, and amassed substantial moral judgment data annotated by\nvolunteers and various popular Large Language Models (LLMs), collectively\nportraying the moral preferences and performances of both humans and AIs across\na range of moral contexts. This paper will outline the current structure and\nconstruction of STREAM, explore its potential applications, and discuss its\nfuture prospects.",
            "author": [
                "Yuwei Wang",
                "Enmeng Lu",
                "Zizhe Ruan",
                "Yao Liang",
                "Yi Zeng"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05563v1",
                "http://arxiv.org/pdf/2310.05563v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05560v1",
            "title": "Graph multicoloring in solving node malfunction and attacks in a\n  secret-sharing network",
            "updated": "2023-10-09T09:36:56Z",
            "published": "2023-10-09T09:36:56Z",
            "summary": "We observe a network scenario where parts of a secret are distributed among\nits nodes. Within the network, a group of attackers is actively trying to\nobtain the complete secret, while there is also the issue of some nodes\nmalfunctioning or being absent. In this paper, we address this problem by\nemploying graph multicoloring techniques, focusing on the case of a single\nattacker and varying numbers of malfunctioning nodes.",
            "author": [
                "Tanja Vojkovic",
                "Damir Vukicevic"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05560v1",
                "http://arxiv.org/pdf/2310.05560v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05556v1",
            "title": "WeatherDepth: Curriculum Contrastive Learning for Self-Supervised Depth\n  Estimation under Adverse Weather Conditions",
            "updated": "2023-10-09T09:26:27Z",
            "published": "2023-10-09T09:26:27Z",
            "summary": "Depth estimation models have shown promising performance on clear scenes but\nfail to generalize to adverse weather conditions due to illumination\nvariations, weather particles, etc. In this paper, we propose WeatherDepth, a\nself-supervised robust depth estimation model with curriculum contrastive\nlearning, to tackle performance degradation in complex weather conditions.\nConcretely, we first present a progressive curriculum learning scheme with\nthree simple-to-complex curricula to gradually adapt the model from clear to\nrelative adverse, and then to adverse weather scenes. It encourages the model\nto gradually grasp beneficial depth cues against the weather effect, yielding\nsmoother and better domain adaption. Meanwhile, to prevent the model from\nforgetting previous curricula, we integrate contrastive learning into different\ncurricula. Drawn the reference knowledge from the previous course, our strategy\nestablishes a depth consistency constraint between different courses towards\nrobust depth estimation in diverse weather. Besides, to reduce manual\nintervention and better adapt to different models, we designed an adaptive\ncurriculum scheduler to automatically search for the best timing for course\nswitching. In the experiment, the proposed solution is proven to be easily\nincorporated into various architectures and demonstrates state-of-the-art\n(SoTA) performance on both synthetic and real weather datasets.",
            "author": [
                "Jiyuan Wang",
                "Chunyu Lin",
                "Lang Nie",
                "Shujun Huang",
                "Yao Zhao",
                "Xing Pan",
                "Rui Ai"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05556v1",
                "http://arxiv.org/pdf/2310.05556v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05553v1",
            "title": "Regulation and NLP (RegNLP): Taming Large Language Models",
            "updated": "2023-10-09T09:22:40Z",
            "published": "2023-10-09T09:22:40Z",
            "summary": "The scientific innovation in Natural Language Processing (NLP) and more\nbroadly in artificial intelligence (AI) is at its fastest pace to date. As\nlarge language models (LLMs) unleash a new era of automation, important debates\nemerge regarding the benefits and risks of their development, deployment and\nuse. Currently, these debates have been dominated by often polarized narratives\nmainly led by the AI Safety and AI Ethics movements. This polarization, often\namplified by social media, is swaying political agendas on AI regulation and\ngovernance and posing issues of regulatory capture. Capture occurs when the\nregulator advances the interests of the industry it is supposed to regulate, or\nof special interest groups rather than pursuing the general public interest.\nMeanwhile in NLP research, attention has been increasingly paid to the\ndiscussion of regulating risks and harms. This often happens without systematic\nmethodologies or sufficient rooting in the disciplines that inspire an extended\nscope of NLP research, jeopardizing the scientific integrity of these\nendeavors. Regulation studies are a rich source of knowledge on how to\nsystematically deal with risk and uncertainty, as well as with scientific\nevidence, to evaluate and compare regulatory options. This resource has largely\nremained untapped so far. In this paper, we argue how NLP research on these\ntopics can benefit from proximity to regulatory studies and adjacent fields. We\ndo so by discussing basic tenets of regulation, and risk and uncertainty, and\nby highlighting the shortcomings of current NLP discussions dealing with risk\nassessment. Finally, we advocate for the development of a new multidisciplinary\nresearch space on regulation and NLP (RegNLP), focused on connecting scientific\nknowledge to regulatory processes based on systematic methodologies.",
            "author": [
                "Catalina Goanta",
                "Nikolaos Aletras",
                "Ilias Chalkidis",
                "Sofia Ranchordas",
                "Gerasimos Spanakis"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05553v1",
                "http://arxiv.org/pdf/2310.05553v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05551v1",
            "title": "Logic-guided Deep Reinforcement Learning for Stock Trading",
            "updated": "2023-10-09T09:20:13Z",
            "published": "2023-10-09T09:20:13Z",
            "summary": "Deep reinforcement learning (DRL) has revolutionized quantitative finance by\nachieving excellent performance without significant manual effort. Whereas we\nobserve that the DRL models behave unstably in a dynamic stock market due to\nthe low signal-to-noise ratio nature of the financial data. In this paper, we\npropose a novel logic-guided trading framework, termed as SYENS (Program\nSynthesis-based Ensemble Strategy). Different from the previous\nstate-of-the-art ensemble reinforcement learning strategy which arbitrarily\nselects the best-performing agent for testing based on a single measurement,\nour framework proposes regularizing the model's behavior in a hierarchical\nmanner using the program synthesis by sketching paradigm. First, we propose a\nhigh-level, domain-specific language (DSL) that is used for the depiction of\nthe market environment and action. Then based on the DSL, a novel program\nsketch is introduced, which embeds human expert knowledge in a logical manner.\nFinally, based on the program sketch, we adopt the program synthesis by\nsketching a paradigm and synthesizing a logical, hierarchical trading strategy.\nWe evaluate SYENS on the 30 Dow Jones stocks under the cash trading and the\nmargin trading settings. Experimental results demonstrate that our proposed\nframework can significantly outperform the baselines with much higher\ncumulative return and lower maximum drawdown under both settings.",
            "author": [
                "Zhiming Li",
                "Junzhe Jiang",
                "Yushi Cao",
                "Aixin Cui",
                "Bozhi Wu",
                "Bo Li",
                "Yang Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05551v1",
                "http://arxiv.org/pdf/2310.05551v1"
            ],
            "primary_category": "cs.CE",
            "category": [
                "cs.CE",
                "cs.AI",
                "cs.PL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05539v1",
            "title": "Testing High-Dimensional Mediation Effect with Arbitrary\n  Exposure-Mediator Coefficients",
            "updated": "2023-10-09T09:03:09Z",
            "published": "2023-10-09T09:03:09Z",
            "summary": "In response to the unique challenge created by high-dimensional mediators in\nmediation analysis, this paper presents a novel procedure for testing the\nnullity of the mediation effect in the presence of high-dimensional mediators.\nThe procedure incorporates two distinct features. Firstly, the test remains\nvalid under all cases of the composite null hypothesis, including the\nchallenging scenario where both exposure-mediator and mediator-outcome\ncoefficients are zero. Secondly, it does not impose structural assumptions on\nthe exposure-mediator coefficients, thereby allowing for an arbitrarily strong\nexposure-mediator relationship. To the best of our knowledge, the proposed test\nis the first of its kind to provably possess these two features in\nhigh-dimensional mediation analysis. The validity and consistency of the\nproposed test are established, and its numerical performance is showcased\nthrough simulation studies. The application of the proposed test is\ndemonstrated by examining the mediation effect of DNA methylation between\nsmoking status and lung cancer development.",
            "author": [
                "Yinan Lin",
                "Zijian Guo",
                "Baoluo Sun",
                "Zhenhua Lin"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05539v1",
                "http://arxiv.org/pdf/2310.05539v1"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME",
                "math.ST",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05527v1",
            "title": "Diagonal of Pseudoinverse of Graph Laplacian: Fast Estimation and Exact\n  Results",
            "updated": "2023-10-09T08:47:56Z",
            "published": "2023-10-09T08:47:56Z",
            "summary": "The diagonal entries of pseudoinverse of the Laplacian matrix of a graph\nappear in many important practical applications, since they contain much\ninformation of the graph and many relevant quantities can be expressed in terms\nof them, such as Kirchhoff index and current flow centrality. However, a\nna\\\"{\\i}ve approach for computing the diagonal of a matrix inverse has cubic\ncomputational complexity in terms of the matrix dimension, which is not\nacceptable for large graphs with millions of nodes. Thus, rigorous solutions to\nthe diagonal of the Laplacian matrices for general graphs, even for particluar\ngraphs are much less. In this paper, we propose a theoretically guaranteed\nestimation algorithm, which approximates all diagonal entries of the\npseudoinverse of a graph Laplacian in nearly linear time with respect to the\nnumber of edges in the graph. We execute extensive experiments on real-life\nnetworks, which indicate that our algorithm is both efficient and accurate.\nAlso, we determine exact expressions for the diagonal elements of pseudoinverse\nof the Laplacian matrices for Koch networks and uniform recursive trees, and\ncompare them with those obtained by our approximation algorithm. Finally, we\nuse our algorithm to evaluate the Kirchhoff index of three deterministic model\nnetworks, for which the Kirchhoff index can be rigorously determined. These\nresults further show the effectiveness and efficiency of our algorithm.",
            "author": [
                "Zenan Lu",
                "Wanyue Xu",
                "Zhongzhi Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05527v1",
                "http://arxiv.org/pdf/2310.05527v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05526v1",
            "title": "Projecting infinite time series graphs to finite marginal graphs using\n  number theory",
            "updated": "2023-10-09T08:45:06Z",
            "published": "2023-10-09T08:45:06Z",
            "summary": "In recent years, a growing number of method and application works have\nadapted and applied the causal-graphical-model framework to time series data.\nMany of these works employ time-resolved causal graphs that extend infinitely\ninto the past and future and whose edges are repetitive in time, thereby\nreflecting the assumption of stationary causal relationships. However, most\nresults and algorithms from the causal-graphical-model framework are not\ndesigned for infinite graphs. In this work, we develop a method for projecting\ninfinite time series graphs with repetitive edges to marginal graphical models\non a finite time window. These finite marginal graphs provide the answers to\n$m$-separation queries with respect to the infinite graph, a task that was\npreviously unresolved. Moreover, we argue that these marginal graphs are useful\nfor causal discovery and causal effect estimation in time series, effectively\nenabling to apply results developed for finite graphs to the infinite graphs.\nThe projection procedure relies on finding common ancestors in the\nto-be-projected graph and is, by itself, not new. However, the projection\nprocedure has not yet been algorithmically implemented for time series graphs\nsince in these infinite graphs there can be infinite sets of paths that might\ngive rise to common ancestors. We solve the search over these possibly infinite\nsets of paths by an intriguing combination of path-finding techniques for\nfinite directed graphs and solution theory for linear Diophantine equations. By\nproviding an algorithm that carries out the projection, our paper makes an\nimportant step towards a theoretically-grounded and method-agnostic\ngeneralization of a range of causal inference methods and results to time\nseries.",
            "author": [
                "Andreas Gerhardus",
                "Jonas Wahl",
                "Sofia Faltenbacher",
                "Urmi Ninad",
                "Jakob Runge"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05526v1",
                "http://arxiv.org/pdf/2310.05526v1"
            ],
            "primary_category": "math.ST",
            "category": [
                "math.ST",
                "cs.LG",
                "stat.ME",
                "stat.ML",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05525v1",
            "title": "Physical Layer Security in a Private 5G Network for Industrial and\n  Mobility Application",
            "updated": "2023-10-09T08:45:00Z",
            "published": "2023-10-09T08:45:00Z",
            "summary": "Cellular communication technologies such as 5G are deployed on a large scale\naround the world. Compared to other communication technologies such as WiFi,\nBluetooth, or Ultra Wideband, the 5G communication standard describes support\nfor a large variety of use cases, e.g., Internet of Things, vehicular,\nindustrial, and campus-wide communications. An organization can operate a\nPrivate 5G network to provide connectivity to devices in their manufacturing\nenvironment. Physical Layer Key Generation (PLKG) is a method to generate a\nsymmetric secret on two nodes despite the presence of a potential passive\neavesdropper. To the best of our knowledge, this work is one of the first to\nimplement PLKG in a real Private 5G network. Therefore, it highlights the\npossibility of integrating PLKG in the communication technology highly relevant\nfor industrial applications. This paper exemplifies the establishment of a\nlong-term symmetric key between an aerial vehicle and IT infrastructure both\nlocated in a manufacturing environment and communicating via the radio\ninterface of the Private 5G network.",
            "author": [
                "Shivraj Hanumant Gonde",
                "Christoph Frisch",
                "Svetoslav Duhovnikov",
                "Martin Kubisch",
                "Thomas Meyerhoff",
                "Dominic Schupke"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05525v1",
                "http://arxiv.org/pdf/2310.05525v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "eess.SP",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05517v1",
            "title": "WeatherGNN: Exploiting Complicated Relationships in Numerical Weather\n  Prediction Bias Correction",
            "updated": "2023-10-09T08:33:19Z",
            "published": "2023-10-09T08:33:19Z",
            "summary": "Numerical weather prediction (NWP) may be inaccurate or biased due to\nincomplete atmospheric physical processes, insufficient spatial-temporal\nresolution, and inherent uncertainty of weather. Previous studies have\nattempted to correct biases by using handcrafted features and domain knowledge,\nor by applying general machine learning models naively. They do not fully\nexplore the complicated meteorologic interactions and spatial dependencies in\nthe atmosphere dynamically, which limits their applicability in NWP\nbias-correction. Specifically, weather factors interact with each other in\ncomplex ways, and these interactions can vary regionally. In addition, the\ninteractions between weather factors are further complicated by the spatial\ndependencies between regions, which are influenced by varied terrain and\natmospheric motions. To address these issues, we propose WeatherGNN, an NWP\nbias-correction method that utilizes Graph Neural Networks (GNN) to learn\nmeteorologic and geographic relationships in a unified framework. Our approach\nincludes a factor-wise GNN that captures meteorological interactions within\neach grid (a specific location) adaptively, and a fast hierarchical GNN that\ncaptures spatial dependencies between grids dynamically. Notably, the fast\nhierarchical GNN achieves linear complexity with respect to the number of\ngrids, enhancing model efficiency and scalability. Our experimental results on\ntwo real-world datasets demonstrate the superiority of WeatherGNN in comparison\nwith other SOTA methods, with an average improvement of 40.50\\% on RMSE\ncompared to the original NWP.",
            "author": [
                "Binqing Wu",
                "Weiqi Chen",
                "Wengwei Wang",
                "Bingqing Peng",
                "Liang Sun",
                "Ling Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05517v1",
                "http://arxiv.org/pdf/2310.05517v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05504v1",
            "title": "Colmap-PCD: An Open-source Tool for Fine Image-to-point cloud\n  Registration",
            "updated": "2023-10-09T08:09:15Z",
            "published": "2023-10-09T08:09:15Z",
            "summary": "State-of-the-art techniques for monocular camera reconstruction predominantly\nrely on the Structure from Motion (SfM) pipeline. However, such methods often\nyield reconstruction outcomes that lack crucial scale information, and over\ntime, accumulation of images leads to inevitable drift issues. In contrast,\nmapping methods based on LiDAR scans are popular in large-scale urban scene\nreconstruction due to their precise distance measurements, a capability\nfundamentally absent in visual-based approaches. Researchers have made attempts\nto utilize concurrent LiDAR and camera measurements in pursuit of precise\nscaling and color details within mapping outcomes. However, the outcomes are\nsubject to extrinsic calibration and time synchronization precision. In this\npaper, we propose a novel cost-effective reconstruction pipeline that utilizes\na pre-established LiDAR map as a fixed constraint to effectively address the\ninherent scale challenges present in monocular camera reconstruction. To our\nknowledge, our method is the first to register images onto the point cloud map\nwithout requiring synchronous capture of camera and LiDAR data, granting us the\nflexibility to manage reconstruction detail levels across various areas of\ninterest. To facilitate further research in this domain, we have released\nColmap-PCD${^{3}}$, an open-source tool leveraging the Colmap algorithm, that\nenables precise fine-scale registration of images to the point cloud map.",
            "author": [
                "Chunge Bai",
                "Ruijie Fu",
                "Xiang Gao"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05504v1",
                "http://arxiv.org/pdf/2310.05504v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05499v1",
            "title": "Integrating Graphs with Large Language Models: Methods and Prospects",
            "updated": "2023-10-09T07:59:34Z",
            "published": "2023-10-09T07:59:34Z",
            "summary": "Large language models (LLMs) such as GPT-4 have emerged as frontrunners,\nshowcasing unparalleled prowess in diverse applications, including answering\nqueries, code generation, and more. Parallelly, graph-structured data, an\nintrinsic data type, is pervasive in real-world scenarios. Merging the\ncapabilities of LLMs with graph-structured data has been a topic of keen\ninterest. This paper bifurcates such integrations into two predominant\ncategories. The first leverages LLMs for graph learning, where LLMs can not\nonly augment existing graph algorithms but also stand as prediction models for\nvarious graph tasks. Conversely, the second category underscores the pivotal\nrole of graphs in advancing LLMs. Mirroring human cognition, we solve complex\ntasks by adopting graphs in either reasoning or collaboration. Integrating with\nsuch structures can significantly boost the performance of LLMs in various\ncomplicated tasks. We also discuss and propose open questions for integrating\nLLMs with graph-structured data for the future direction of the field.",
            "author": [
                "Shirui Pan",
                "Yizhen Zheng",
                "Yixin Liu"
            ],
            "link": [
                "http://dx.doi.org/10.1109/MIS.2023.3332242",
                "http://arxiv.org/abs/2310.05499v1",
                "http://arxiv.org/pdf/2310.05499v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05494v1",
            "title": "Finding a Minimum Spanning Tree with a Small Non-Terminal Set",
            "updated": "2023-10-09T07:56:47Z",
            "published": "2023-10-09T07:56:47Z",
            "summary": "In this paper, we study the problem of finding a minimum weight spanning tree\nthat contains each vertex in a given subset $V_{\\rm NT}$ of vertices as an\ninternal vertex. This problem, called Minimum Weight Non-Terminal Spanning\nTree, includes $s$-$t$ Hamiltonian Path as a special case, and hence it is\nNP-hard. In this paper, we first observe that Non-Terminal Spanning Tree, the\nunweighted counterpart of Minimum Weight Non-Terminal Spanning Tree, is already\nNP-hard on some special graph classes. Moreover, it is W[1]-hard when\nparameterized by clique-width. In contrast, we give a $3k$-vertex kernel and\n$O^*(2^k)$-time algorithm, where $k$ is the size of non-terminal set $V_{\\rm\nNT}$. The latter algorithm can be extended to Minimum Weight Non-Terminal\nSpanning Tree with the restriction that each edge has a polynomially bounded\nintegral weight. We also show that Minimum Weight Non-Terminal Spanning Tree is\nfixed-parameter tractable parameterized by the number of edges in the subgraph\ninduced by the non-terminal set $V_{\\rm NT}$, extending the fixed-parameter\ntractability of Minimum Weight Non-Terminal Spanning Tree to the general case.\nFinally, we give several results for structural parameterization.",
            "author": [
                "Tesshu Hanaka",
                "Yasuaki Kobayashi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05494v1",
                "http://arxiv.org/pdf/2310.05494v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "cs.CC",
                "cs.DM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05480v1",
            "title": "Collective Graph Exploration Parameterized by Vertex Cover",
            "updated": "2023-10-09T07:41:09Z",
            "published": "2023-10-09T07:41:09Z",
            "summary": "We initiate the study of the parameterized complexity of the {\\sc Collective\nGraph Exploration} ({\\sc CGE}) problem. In {\\sc CGE}, the input consists of an\nundirected connected graph $G$ and a collection of $k$ robots, initially placed\nat the same vertex $r$ of $G$, and each one of them has an energy budget of\n$B$. The objective is to decide whether $G$ can be \\emph{explored} by the $k$\nrobots in $B$ time steps, i.e., there exist $k$ closed walks in $G$, one\ncorresponding to each robot, such that every edge is covered by at least one\nwalk, every walk starts and ends at the vertex $r$, and the maximum length of\nany walk is at most $B$. Unfortunately, this problem is \\textsf{NP}-hard even\non trees [Fraigniaud {\\em et~al.}, 2006]. Further, we prove that the problem\nremains \\textsf{W[1]}-hard parameterized by $k$ even for trees of treedepth\n$3$. Due to the \\textsf{para-NP}-hardness of the problem parameterized by\ntreedepth, and motivated by real-world scenarios, we study the parameterized\ncomplexity of the problem parameterized by the vertex cover number\n($\\mathsf{vc}$) of the graph, and prove that the problem is fixed-parameter\ntractable (\\textsf{FPT}) parameterized by $\\mathsf{vc}$. Additionally, we study\nthe optimization version of {\\sc CGE}, where we want to optimize $B$, and\ndesign an approximation algorithm with an additive approximation factor of\n$O(\\mathsf{vc})$.",
            "author": [
                "Siddharth Gupta",
                "Guy Sa'ar",
                "Meirav Zehavi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05480v1",
                "http://arxiv.org/pdf/2310.05480v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05479v1",
            "title": "Deep Optimal Timing Strategies for Time Series",
            "updated": "2023-10-09T07:38:23Z",
            "published": "2023-10-09T07:38:23Z",
            "summary": "Deciding the best future execution time is a critical task in many business\nactivities while evolving time series forecasting, and optimal timing strategy\nprovides such a solution, which is driven by observed data. This solution has\nplenty of valuable applications to reduce the operation costs. In this paper,\nwe propose a mechanism that combines a probabilistic time series forecasting\ntask and an optimal timing decision task as a first systematic attempt to\ntackle these practical problems with both solid theoretical foundation and\nreal-world flexibility. Specifically, it generates the future paths of the\nunderlying time series via probabilistic forecasting algorithms, which does not\nneed a sophisticated mathematical dynamic model relying on strong prior\nknowledge as most other common practices. In order to find the optimal\nexecution time, we formulate the decision task as an optimal stopping problem,\nand employ a recurrent neural network structure (RNN) to approximate the\noptimal times. Github repository:\n\\url{github.com/ChenPopper/optimal_timing_TSF}.",
            "author": [
                "Chen Pan",
                "Fan Zhou",
                "Xuanwei Hu",
                "Xinxin Zhu",
                "Wenxin Ning",
                "Zi Zhuang",
                "Siqiao Xue",
                "James Zhang",
                "Yunhua Hu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05479v1",
                "http://arxiv.org/pdf/2310.05479v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05472v2",
            "title": "Intelligent Tutoring System: Experience of Linking Software Engineering\n  and Programming Teaching",
            "updated": "2023-10-13T06:51:17Z",
            "published": "2023-10-09T07:28:41Z",
            "summary": "The increasing number of computer science students pushes lecturers and\ntutors of first-year programming courses to their limits to provide\nhigh-quality feedback to the students. Existing systems that handle automated\ngrading primarily focus on the automation of test case executions in the\ncontext of programming assignments. However, they cannot provide customized\nfeedback about the students' errors, and hence, cannot replace the help of\ntutors. While recent research works in the area of automated grading and\nfeedback generation address this issue by using automated repair techniques, so\nfar, to the best of our knowledge, there has been no real-world deployment of\nsuch techniques. Based on the research advances in recent years, we have built\nan intelligent tutoring system that has the capability of providing automated\nfeedback and grading. Furthermore, we designed a Software Engineering course\nthat guides third-year undergraduate students in incrementally developing such\na system over the coming years. Each year, students will make contributions\nthat improve the current implementation, while at the same time, we can deploy\nthe current system for usage by first year students. This paper describes our\nteaching concept, the intelligent tutoring system architecture, and our\nexperience with the stakeholders. This software engineering project for the\nstudents has the key advantage that the users of the system are available\nin-house (i.e., students, tutors, and lecturers from the first-year programming\ncourses). This helps organize requirements engineering sessions and builds\nawareness about their contribution to a \"to be deployed\" software project. In\nthis multi-year teaching effort, we have incrementally built a tutoring system\nthat can be used in first-year programming courses. Further, it represents a\nplatform that can integrate the latest research results in APR for education.",
            "author": [
                "Zhiyu Fan",
                "Yannic Noller",
                "Ashish Dandekar",
                "Abhik Roychoudhury"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05472v2",
                "http://arxiv.org/pdf/2310.05472v2"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05471v1",
            "title": "Drawn Tree Decomposition: New Approach for Graph Drawing Problems",
            "updated": "2023-10-09T07:27:17Z",
            "published": "2023-10-09T07:27:17Z",
            "summary": "Over the past decade, we witness an increasing amount of interest in the\ndesign of exact exponential-time and parameterized algorithms for problems in\nGraph Drawing. Unfortunately, we still lack knowledge of general methods to\ndevelop such algorithms. An even more serious issue is that, here, \"standard\"\nparameters very often yield intractability. In particular, for the most common\nstructural parameter, namely, treewidth, we frequently observe NP-hardness\nalready when the input graphs are restricted to have constant (often, being\njust $1$ or $2$) treewidth.\n  Our work deals with both drawbacks simultaneously. We introduce a novel form\nof tree decomposition that, roughly speaking, does not decompose (only) a\ngraph, but an entire drawing. As such, its bags and separators are of geometric\n(rather than only combinatorial) nature. While the corresponding parameter --\nlike treewidth -- can be arbitrarily smaller than the height (and width) of the\ndrawing, we show that -- unlike treewidth -- it gives rise to efficient\nalgorithms. Specifically, we get slice-wise polynomial (XP) time algorithms\nparameterized by our parameter. We present a general scheme for the design of\nsuch algorithms, and apply it to several central problems in Graph Drawing,\nincluding the recognition of grid graphs, minimization of crossings and bends,\nand compaction. Other than for the class of problems we discussed in the paper,\nwe believe that our decomposition and scheme are of independent interest and\ncan be further extended or generalized to suit even a wider class of problems.\nAdditionally, we discuss classes of drawings where our parameter is bounded by\n$O(\\sqrt{n})$ (where $n$ is the number of vertices of the graph), yielding\nsubexponential-time algorithms. Lastly, we prove which relations exist between\ndrawn treewidth and other width measures, including treewidth, pathwidth,\n(dual) carving-width and embedded-width.",
            "author": [
                "Siddharth Gupta",
                "Guy Sa'ar",
                "Meirav Zehavi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05471v1",
                "http://arxiv.org/pdf/2310.05471v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "cs.CG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05461v1",
            "title": "Sparsistency for Inverse Optimal Transport",
            "updated": "2023-10-09T07:10:09Z",
            "published": "2023-10-09T07:10:09Z",
            "summary": "Optimal Transport is a useful metric to compare probability distributions and\nto compute a pairing given a ground cost. Its entropic regularization variant\n(eOT) is crucial to have fast algorithms and reflect fuzzy/noisy matchings.\nThis work focuses on Inverse Optimal Transport (iOT), the problem of inferring\nthe ground cost from samples drawn from a coupling that solves an eOT problem.\nIt is a relevant problem that can be used to infer unobserved/missing links,\nand to obtain meaningful information about the structure of the ground cost\nyielding the pairing. On one side, iOT benefits from convexity, but on the\nother side, being ill-posed, it requires regularization to handle the sampling\nnoise. This work presents an in-depth theoretical study of the l1\nregularization to model for instance Euclidean costs with sparse interactions\nbetween features. Specifically, we derive a sufficient condition for the robust\nrecovery of the sparsity of the ground cost that can be seen as a far reaching\ngeneralization of the Lasso's celebrated Irrepresentability Condition. To\nprovide additional insight into this condition, we work out in detail the\nGaussian case. We show that as the entropic penalty varies, the iOT problem\ninterpolates between a graphical Lasso and a classical Lasso, thereby\nestablishing a connection between iOT and graph estimation, an important\nproblem in ML.",
            "author": [
                "Francisco Andrade",
                "Gabriel Peyre",
                "Clarice Poon"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05461v1",
                "http://arxiv.org/pdf/2310.05461v1"
            ],
            "primary_category": "math.ST",
            "category": [
                "math.ST",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05427v1",
            "title": "Emergent chaotic iterations in hard sparse instances of hamiltonian path\n  problem",
            "updated": "2023-10-09T05:56:08Z",
            "published": "2023-10-09T05:56:08Z",
            "summary": "A hamiltonian path is a path walk P that can be a hamiltonian path or\nhamiltonian circuit. Determining whether such hamiltonian path exists in a\ngiven graph G = (V, E) is a NP-Complete problem. In this paper, a novel\nalgorithm with chaotic behaviour for hamiltonian path problem is proposed. We\nshow that our algorithm runs in $O(V^5(V + E))$ for hard sparse instances from\nFHCP challenge dataset.",
            "author": [
                "Cicero A. de Lima"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05427v1",
                "http://arxiv.org/pdf/2310.05427v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05421v1",
            "title": "Automating Customer Service using LangChain: Building custom open-source\n  GPT Chatbot for organizations",
            "updated": "2023-10-09T05:35:10Z",
            "published": "2023-10-09T05:35:10Z",
            "summary": "In the digital age, the dynamics of customer service are evolving, driven by\ntechnological advancements and the integration of Large Language Models (LLMs).\nThis research paper introduces a groundbreaking approach to automating customer\nservice using LangChain, a custom LLM tailored for organizations. The paper\nexplores the obsolescence of traditional customer support techniques,\nparticularly Frequently Asked Questions (FAQs), and proposes a paradigm shift\ntowards responsive, context-aware, and personalized customer interactions. The\nheart of this innovation lies in the fusion of open-source methodologies, web\nscraping, fine-tuning, and the seamless integration of LangChain into customer\nservice platforms. This open-source state-of-the-art framework, presented as\n\"Sahaay,\" demonstrates the ability to scale across industries and\norganizations, offering real-time support and query resolution. Key elements of\nthis research encompass data collection via web scraping, the role of\nembeddings, the utilization of Google's Flan T5 XXL, Base and Small language\nmodels for knowledge retrieval, and the integration of the chatbot into\ncustomer service platforms. The results section provides insights into their\nperformance and use cases, here particularly within an educational institution.\nThis research heralds a new era in customer service, where technology is\nharnessed to create efficient, personalized, and responsive interactions.\nSahaay, powered by LangChain, redefines the customer-company relationship,\nelevating customer retention, value extraction, and brand image. As\norganizations embrace LLMs, customer service becomes a dynamic and\ncustomer-centric ecosystem.",
            "author": [
                "Keivalya Pandya",
                "Mehfuza Holia"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05421v1",
                "http://arxiv.org/pdf/2310.05421v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.CY",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06872v1",
            "title": "On sparse regression, Lp-regularization, and automated model discovery",
            "updated": "2023-10-09T05:34:21Z",
            "published": "2023-10-09T05:34:21Z",
            "summary": "Sparse regression and feature extraction are the cornerstones of knowledge\ndiscovery from massive data. Their goal is to discover interpretable and\npredictive models that provide simple relationships among scientific variables.\nWhile the statistical tools for model discovery are well established in the\ncontext of linear regression, their generalization to nonlinear regression in\nmaterial modeling is highly problem-specific and insufficiently understood.\nHere we explore the potential of neural networks for automatic model discovery\nand induce sparsity by a hybrid approach that combines two strategies:\nregularization and physical constraints. We integrate the concept of Lp\nregularization for subset selection with constitutive neural networks that\nleverage our domain knowledge in kinematics and thermodynamics. We train our\nnetworks with both, synthetic and real data, and perform several thousand\ndiscovery runs to infer common guidelines and trends: L2 regularization or\nridge regression is unsuitable for model discovery; L1 regularization or lasso\npromotes sparsity, but induces strong bias; only L0 regularization allows us to\ntransparently fine-tune the trade-off between interpretability and\npredictability, simplicity and accuracy, and bias and variance. With these\ninsights, we demonstrate that Lp regularized constitutive neural networks can\nsimultaneously discover both, interpretable models and physically meaningful\nparameters. We anticipate that our findings will generalize to alternative\ndiscovery techniques such as sparse and symbolic regression, and to other\ndomains such as biology, chemistry, or medicine. Our ability to automatically\ndiscover material models from data could have tremendous applications in\ngenerative material design and open new opportunities to manipulate matter,\nalter properties of existing materials, and discover new materials with\nuser-defined properties.",
            "author": [
                "Jeremy A. McCulloch",
                "Skyler R. St. Pierre",
                "Kevin Linka",
                "Ellen Kuhl"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06872v1",
                "http://arxiv.org/pdf/2310.06872v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "65, 74",
                "I.6; J.2"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05414v1",
            "title": "Ethics of Artificial Intelligence and Robotics in the Architecture,\n  Engineering, and Construction Industry",
            "updated": "2023-10-09T05:17:14Z",
            "published": "2023-10-09T05:17:14Z",
            "summary": "Artificial intelligence (AI) and robotics research and implementation emerged\nin the architecture, engineering, and construction (AEC) industry to positively\nimpact project efficiency and effectiveness concerns such as safety,\nproductivity, and quality. This shift, however, warrants the need for ethical\nconsiderations of AI and robotics adoption due to its potential negative\nimpacts on aspects such as job security, safety, and privacy. Nevertheless,\nthis did not receive sufficient attention, particularly within the academic\ncommunity. This research systematically reviews AI and robotics research\nthrough the lens of ethics in the AEC community for the past five years. It\nidentifies nine key ethical issues namely job loss, data privacy, data\nsecurity, data transparency, decision-making conflict, acceptance and trust,\nreliability and safety, fear of surveillance, and liability, by summarizing\nexisting literature and filtering it further based on its AEC relevance.\nFurthermore, thirteen research topics along the process were identified based\non existing AEC studies that had direct relevance to the theme of ethics in\ngeneral and their parallels are further discussed. Finally, the current\nchallenges and knowledge gaps are discussed and seven specific future research\ndirections are recommended. This study not only signifies more stakeholder\nawareness of this important topic but also provides imminent steps towards\nsafer and more efficient realization.",
            "author": [
                "Ci-Jyun Liang",
                "Thai-Hoa Le",
                "Youngjib Ham",
                "Bharadwaj R. K. Mantha",
                "Marvin H. Cheng",
                "Jacob J. Lin"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05414v1",
                "http://arxiv.org/pdf/2310.05414v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05373v1",
            "title": "Quantum Bayesian Optimization",
            "updated": "2023-10-09T03:10:42Z",
            "published": "2023-10-09T03:10:42Z",
            "summary": "Kernelized bandits, also known as Bayesian optimization (BO), has been a\nprevalent method for optimizing complicated black-box reward functions. Various\nBO algorithms have been theoretically shown to enjoy upper bounds on their\ncumulative regret which are sub-linear in the number T of iterations, and a\nregret lower bound of Omega(sqrt(T)) has been derived which represents the\nunavoidable regrets for any classical BO algorithm. Recent works on quantum\nbandits have shown that with the aid of quantum computing, it is possible to\nachieve tighter regret upper bounds better than their corresponding classical\nlower bounds. However, these works are restricted to either multi-armed or\nlinear bandits, and are hence not able to solve sophisticated real-world\nproblems with non-linear reward functions. To this end, we introduce the\nquantum-Gaussian process-upper confidence bound (Q-GP-UCB) algorithm. To the\nbest of our knowledge, our Q-GP-UCB is the first BO algorithm able to achieve a\nregret upper bound of O(polylog T), which is significantly smaller than its\nregret lower bound of Omega(sqrt(T)) in the classical setting. Moreover, thanks\nto our novel analysis of the confidence ellipsoid, our Q-GP-UCB with the linear\nkernel achieves a smaller regret than the quantum linear UCB algorithm from the\nprevious work. We use simulations, as well as an experiment using a real\nquantum computer, to verify that the theoretical quantum speedup achieved by\nour Q-GP-UCB is also potentially relevant in practice.",
            "author": [
                "Zhongxiang Dai",
                "Gregory Kang Ruey Lau",
                "Arun Verma",
                "Yao Shu",
                "Bryan Kian Hsiang Low",
                "Patrick Jaillet"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05373v1",
                "http://arxiv.org/pdf/2310.05373v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05368v1",
            "title": "Measuring Acoustics with Collaborative Multiple Agents",
            "updated": "2023-10-09T02:58:27Z",
            "published": "2023-10-09T02:58:27Z",
            "summary": "As humans, we hear sound every second of our life. The sound we hear is often\naffected by the acoustics of the environment surrounding us. For example, a\nspacious hall leads to more reverberation. Room Impulse Responses (RIR) are\ncommonly used to characterize environment acoustics as a function of the scene\ngeometry, materials, and source/receiver locations. Traditionally, RIRs are\nmeasured by setting up a loudspeaker and microphone in the environment for all\nsource/receiver locations, which is time-consuming and inefficient. We propose\nto let two robots measure the environment's acoustics by actively moving and\nemitting/receiving sweep signals. We also devise a collaborative multi-agent\npolicy where these two robots are trained to explore the environment's\nacoustics while being rewarded for wide exploration and accurate prediction. We\nshow that the robots learn to collaborate and move to explore environment\nacoustics while minimizing the prediction error. To the best of our knowledge,\nwe present the very first problem formulation and solution to the task of\ncollaborative environment acoustics measurements with multiple agents.",
            "author": [
                "Yinfeng Yu",
                "Changan Chen",
                "Lele Cao",
                "Fangkai Yang",
                "Fuchun Sun"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05368v1",
                "http://arxiv.org/pdf/2310.05368v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.MA",
                "cs.SD",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05364v3",
            "title": "Universal Multi-modal Entity Alignment via Iteratively Fusing Modality\n  Similarity Paths",
            "updated": "2023-10-13T09:47:08Z",
            "published": "2023-10-09T02:50:54Z",
            "summary": "The objective of Entity Alignment (EA) is to identify equivalent entity pairs\nfrom multiple Knowledge Graphs (KGs) and create a more comprehensive and\nunified KG. The majority of EA methods have primarily focused on the structural\nmodality of KGs, lacking exploration of multi-modal information. A few\nmulti-modal EA methods have made good attempts in this field. Still, they have\ntwo shortcomings: (1) inconsistent and inefficient modality modeling that\ndesigns complex and distinct models for each modality; (2) ineffective modality\nfusion due to the heterogeneous nature of modalities in EA. To tackle these\nchallenges, we propose PathFusion, consisting of two main components: (1) MSP,\na unified modeling approach that simplifies the alignment process by\nconstructing paths connecting entities and modality nodes to represent multiple\nmodalities; (2) IRF, an iterative fusion method that effectively combines\ninformation from different modalities using the path as an information carrier.\nExperimental results on real-world datasets demonstrate the superiority of\nPathFusion over state-of-the-art methods, with 22.4%-28.9% absolute improvement\non Hits@1, and 0.194-0.245 absolute improvement on MRR.",
            "author": [
                "Bolin Zhu",
                "Xiaoze Liu",
                "Xin Mao",
                "Zhuo Chen",
                "Lingbing Guo",
                "Tao Gui",
                "Qi Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05364v3",
                "http://arxiv.org/pdf/2310.05364v3"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05343v1",
            "title": "Investigating Continuous Learning in Spiking Neural Networks",
            "updated": "2023-10-09T02:08:18Z",
            "published": "2023-10-09T02:08:18Z",
            "summary": "In this paper, the use of third-generation machine learning, also known as\nspiking neural network architecture, for continuous learning was investigated\nand compared to conventional models. The experimentation was divided into three\nseparate phases. The first phase focused on training the conventional models\nvia transfer learning. The second phase trains a Nengo model from their\nlibrary. Lastly, each conventional model is converted into a spiking neural\nnetwork and trained. Initial results from phase 1 are inline with known\nknowledge about continuous learning within current machine learning literature.\nAll models were able to correctly identify the current classes, but they would\nimmediately see a sharp performance drop in previous classes due to\ncatastrophic forgetting. However, the SNN models were able to retain some\ninformation about previous classes. Although many of the previous classes were\nstill identified as the current trained classes, the output probabilities\nshowed a higher than normal value to the actual class. This indicates that the\nSNN models do have potential to overcome catastrophic forgetting but much work\nis still needed.",
            "author": [
                "C. Tanner Fredieu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05343v1",
                "http://arxiv.org/pdf/2310.05343v1"
            ],
            "primary_category": "cs.NE",
            "category": [
                "cs.NE",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05337v1",
            "title": "What do larger image classifiers memorise?",
            "updated": "2023-10-09T01:52:07Z",
            "published": "2023-10-09T01:52:07Z",
            "summary": "The success of modern neural networks has prompted study of the connection\nbetween memorisation and generalisation: overparameterised models generalise\nwell, despite being able to perfectly fit (memorise) completely random labels.\nTo carefully study this issue, Feldman proposed a metric to quantify the degree\nof memorisation of individual training examples, and empirically computed the\ncorresponding memorisation profile of a ResNet on image classification\nbench-marks. While an exciting first glimpse into what real-world models\nmemorise, this leaves open a fundamental question: do larger neural models\nmemorise more? We present a comprehensive empirical analysis of this question\non image classification benchmarks. We find that training examples exhibit an\nunexpectedly diverse set of memorisation trajectories across model sizes: most\nsamples experience decreased memorisation under larger models, while the rest\nexhibit cap-shaped or increasing memorisation. We show that various proxies for\nthe Feldman memorization score fail to capture these fundamental trends.\nLastly, we find that knowledge distillation, an effective and popular model\ncompression technique, tends to inhibit memorisation, while also improving\ngeneralisation. Specifically, memorisation is mostly inhibited on examples with\nincreasing memorisation trajectories, thus pointing at how distillation\nimproves generalisation.",
            "author": [
                "Michal Lukasik",
                "Vaishnavh Nagarajan",
                "Ankit Singh Rawat",
                "Aditya Krishna Menon",
                "Sanjiv Kumar"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05337v1",
                "http://arxiv.org/pdf/2310.05337v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV",
                "Machine Learning (cs.LG), Artificial Intelligence (cs.AI) Machine\n  Learning (stat.ML)"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05336v1",
            "title": "GReAT: A Graph Regularized Adversarial Training Method",
            "updated": "2023-10-09T01:44:06Z",
            "published": "2023-10-09T01:44:06Z",
            "summary": "This paper proposes a regularization method called GReAT, Graph Regularized\nAdversarial Training, to improve deep learning models' classification\nperformance. Adversarial examples are a well-known challenge in machine\nlearning, where small, purposeful perturbations to input data can mislead\nmodels. Adversarial training, a powerful and one of the most effective defense\nstrategies, involves training models with both regular and adversarial\nexamples. However, it often neglects the underlying structure of the data. In\nresponse, we propose GReAT, a method that leverages data graph structure to\nenhance model robustness. GReAT deploys the graph structure of the data into\nthe adversarial training process, resulting in more robust models that better\ngeneralize its testing performance and defend against adversarial attacks.\nThrough extensive evaluation on benchmark datasets, we demonstrate GReAT's\neffectiveness compared to state-of-the-art classification methods, highlighting\nits potential in improving deep learning models' classification performance.",
            "author": [
                "Samet Bayram",
                "Kenneth Barner"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05336v1",
                "http://arxiv.org/pdf/2310.05336v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05315v1",
            "title": "Sub-quadratic (1+\\eps)-approximate Euclidean Spanners, with Applications",
            "updated": "2023-10-09T00:13:55Z",
            "published": "2023-10-09T00:13:55Z",
            "summary": "We study graph spanners for point-set in the high-dimensional Euclidean\nspace. On the one hand, we prove that spanners with stretch <\\sqrt{2} and\nsubquadratic size are not possible, even if we add Steiner points. On the other\nhand, if we add extra nodes to the graph (non-metric Steiner points), then we\ncan obtain (1+\\eps)-approximate spanners of subquadratic size. We show how to\nconstruct a spanner of size n^{2-\\Omega(\\eps^3)}, as well as a directed version\nof the spanner of size n^{2-\\Omega(\\eps^2)}.\n  We use our directed spanner to obtain an algorithm for computing\n(1+\\eps)-approximation to Earth-Mover Distance (optimal transport) between two\nsets of size n in time n^{2-\\Omega(\\eps^2)}.",
            "author": [
                "Alexandr Andoni",
                "Hengjie Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05315v1",
                "http://arxiv.org/pdf/2310.05315v1"
            ],
            "primary_category": "cs.CG",
            "category": [
                "cs.CG",
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06871v1",
            "title": "Some ideas about graphic representations of discrete fuzzy measures",
            "updated": "2023-10-08T23:08:36Z",
            "published": "2023-10-08T23:08:36Z",
            "summary": "Graphs serve as efficient tools for visualizing mathematical concepts and\ntheir interrelationships. In this paper, focusing on the discrete case with\nuniversal set with finite elements, we first introduce the rules and\ncharacteristics of graph representation of fuzzy measure and discuss graphic\nproperties of fuzzy measure's duality, symmetry, nonadditivity and\nnonmodularity. Then we show the graphic presentations of some special families\nof fuzzy measures, such as the k-additive measure, the k-maxitive and minitive\nmeasure, k-order representative measure, k-order interactive measure and the\np-symmetric fuzzy measure, as well as of three nonlinear integrals, i.e., the\nChoquet integral, the Sugeno integral and the pan integral. Finally, we provide\nvisualizations for the fuzzy measure fitting procedure and tools for comparing\nfuzzy measures.",
            "author": [
                "Jian-Zhang Wu",
                "Gleb Beliakov"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06871v1",
                "http://arxiv.org/pdf/2310.06871v1"
            ],
            "primary_category": "math.GM",
            "category": [
                "math.GM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05303v1",
            "title": "Persistent Homology of Configuration Spaces of Trees",
            "updated": "2023-10-08T22:27:19Z",
            "published": "2023-10-08T22:27:19Z",
            "summary": "Multiparameter persistence modules come up naturally in topological data\nanalysis and topological robotics. Given a metric graph $(X,\\delta)$, the\nsecond configuration space of $(X,\\delta)$ with proximity parameters (for\nexample, the minimum distance allowed between each pair of robots) can be\ninterpreted as a collection of all possible configurations of two robots moving\nin $(X,\\delta)$. In this project, we study the $2$-parameter persistence\nmodules associated with the second configuration spaces of the star graph\n($\\mathsf{Star}_k$, $k\\geq 3$) and the generalized H-graph\n($\\mathcal{H}_{m,n}$, $m,n\\geq 3$) with the edge length parameter $L_e$ and the\nrestraint parameter $r$. Moreover, we provide the indecomposable direct\nsummands for each persistence module.",
            "author": [
                "Wenwen Li"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05303v1",
                "http://arxiv.org/pdf/2310.05303v1"
            ],
            "primary_category": "math.AT",
            "category": [
                "math.AT",
                "55N31 (Primary) 55R80 (Secondary)"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05296v1",
            "title": "Tailoring Self-Attention for Graph via Rooted Subtrees",
            "updated": "2023-10-08T21:47:18Z",
            "published": "2023-10-08T21:47:18Z",
            "summary": "Attention mechanisms have made significant strides in graph learning, yet\nthey still exhibit notable limitations: local attention faces challenges in\ncapturing long-range information due to the inherent problems of the\nmessage-passing scheme, while global attention cannot reflect the hierarchical\nneighborhood structure and fails to capture fine-grained local information. In\nthis paper, we propose a novel multi-hop graph attention mechanism, named\nSubtree Attention (STA), to address the aforementioned issues. STA seamlessly\nbridges the fully-attentional structure and the rooted subtree, with\ntheoretical proof that STA approximates the global attention under extreme\nsettings. By allowing direct computation of attention weights among multi-hop\nneighbors, STA mitigates the inherent problems in existing graph attention\nmechanisms. Further we devise an efficient form for STA by employing kernelized\nsoftmax, which yields a linear time complexity. Our resulting GNN architecture,\nthe STAGNN, presents a simple yet performant STA-based graph neural network\nleveraging a hop-aware attention strategy. Comprehensive evaluations on ten\nnode classification datasets demonstrate that STA-based models outperform\nexisting graph transformers and mainstream GNNs. The code is available at\nhttps://github.com/LUMIA-Group/SubTree-Attention.",
            "author": [
                "Siyuan Huang",
                "Yunchong Song",
                "Jiayue Zhou",
                "Zhouhan Lin"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05296v1",
                "http://arxiv.org/pdf/2310.05296v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05295v2",
            "title": "Visual Storytelling with Question-Answer Plans",
            "updated": "2023-10-17T22:43:08Z",
            "published": "2023-10-08T21:45:34Z",
            "summary": "Visual storytelling aims to generate compelling narratives from image\nsequences. Existing models often focus on enhancing the representation of the\nimage sequence, e.g., with external knowledge sources or advanced graph\nstructures. Despite recent progress, the stories are often repetitive,\nillogical, and lacking in detail. To mitigate these issues, we present a novel\nframework which integrates visual representations with pretrained language\nmodels and planning. Our model translates the image sequence into a visual\nprefix, a sequence of continuous embeddings which language models can\ninterpret. It also leverages a sequence of question-answer pairs as a blueprint\nplan for selecting salient visual concepts and determining how they should be\nassembled into a narrative. Automatic and human evaluation on the VIST\nbenchmark (Huang et al., 2016) demonstrates that blueprint-based models\ngenerate stories that are more coherent, interesting, and natural compared to\ncompetitive baselines and state-of-the-art systems.",
            "author": [
                "Danyang Liu",
                "Mirella Lapata",
                "Frank Keller"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05295v2",
                "http://arxiv.org/pdf/2310.05295v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05282v1",
            "title": "Asymptotics for graphically divergent series: dense digraphs and 2-SAT\n  formulae",
            "updated": "2023-10-08T21:10:56Z",
            "published": "2023-10-08T21:10:56Z",
            "summary": "We propose a new method for obtaining complete asymptotic expansions in a\nsystematic manner, which is suitable for counting sequences of various graph\nfamilies in dense regime. The core idea is to encode the two-dimensional array\nof expansion coefficients into a special bivariate generating function, which\nwe call a coefficient generating function. We show that coefficient generating\nfunctions possess certain general properties that make it possible to express\nasymptotics in a short closed form. Also, in most scenarios, we indicate a\ncombinatorial meaning of the involved coefficients. Applications of our method\ninclude asymptotics of connected graphs, irreducible tournaments, strongly\nconnected digraphs, satisfiable 2-SAT formulae and contradictory strongly\nconnected implication digraphs. Moreover, due to its flexibility, the method\nallows to treat a wide range of structural variations, including fixing the\nnumbers of connected, irreducible, strongly connected and contradictory\ncomponents, having the variations on source-like, sink-like and isolated\ncomponents, or adding weights and marking variables.",
            "author": [
                "Sergey Dovgal",
                "Khaydar Nurligareev"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05282v1",
                "http://arxiv.org/pdf/2310.05282v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05A16 (Primary) 05A15, 05A10, 05C30, 05C80, 05C20, 60C05, 68Q87,\n  68R07 (Secondary)",
                "G.2.1; G.2.2; G.3"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05276v1",
            "title": "Enhancing Pre-Trained Language Models with Sentence Position Embeddings\n  for Rhetorical Roles Recognition in Legal Opinions",
            "updated": "2023-10-08T20:33:55Z",
            "published": "2023-10-08T20:33:55Z",
            "summary": "The legal domain is a vast and complex field that involves a considerable\namount of text analysis, including laws, legal arguments, and legal opinions.\nLegal practitioners must analyze these texts to understand legal cases,\nresearch legal precedents, and prepare legal documents. The size of legal\nopinions continues to grow, making it increasingly challenging to develop a\nmodel that can accurately predict the rhetorical roles of legal opinions given\ntheir complexity and diversity. In this research paper, we propose a novel\nmodel architecture for automatically predicting rhetorical roles using\npre-trained language models (PLMs) enhanced with knowledge of sentence position\ninformation within a document. Based on an annotated corpus from the\nLegalEval@SemEval2023 competition, we demonstrate that our approach requires\nfewer parameters, resulting in lower computational costs when compared to\ncomplex architectures employing a hierarchical model in a global-context, yet\nit achieves great performance. Moreover, we show that adding more attention to\na hierarchical model based only on BERT in the local-context, along with\nincorporating sentence position information, enhances the results.",
            "author": [
                "Anas Belfathi",
                "Nicolas Hernandez",
                "Laura Monceaux"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05276v1",
                "http://arxiv.org/pdf/2310.05276v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05269v2",
            "title": "Federated Learning: A Cutting-Edge Survey of the Latest Advancements and\n  Applications",
            "updated": "2023-10-15T05:12:38Z",
            "published": "2023-10-08T19:54:26Z",
            "summary": "In the realm of machine learning (ML) systems featuring client-host\nconnections, the enhancement of privacy security can be effectively achieved\nthrough federated learning (FL) as a secure distributed ML methodology. FL\neffectively integrates cloud infrastructure to transfer ML models onto edge\nservers using blockchain technology. Through this mechanism, it guarantees the\nstreamlined processing and data storage requirements of both centralized and\ndecentralized systems, with an emphasis on scalability, privacy considerations,\nand cost-effective communication. In current FL implementations, data owners\nlocally train their models, and subsequently upload the outcomes in the form of\nweights, gradients, and parameters to the cloud for overall model aggregation.\nThis innovation obviates the necessity of engaging Internet of Things (IoT)\nclients and participants to communicate raw and potentially confidential data\ndirectly with a cloud center. This not only reduces the costs associated with\ncommunication networks but also enhances the protection of private data. This\nsurvey conducts an analysis and comparison of recent FL applications, aiming to\nassess their efficiency, accuracy, and privacy protection. However, in light of\nthe complex and evolving nature of FL, it becomes evident that additional\nresearch is imperative to address lingering knowledge gaps and effectively\nconfront the forthcoming challenges in this field. In this study, we categorize\nrecent literature into the following clusters: privacy protection, resource\nallocation, case study analysis, and applications. Furthermore, at the end of\neach section, we tabulate the open areas and future directions presented in the\nreferenced literature, affording researchers and scholars an insightful view of\nthe evolution of the field.",
            "author": [
                "Azim Akhtarshenas",
                "Mohammad Ali Vahedifar",
                "Navid Ayoobi",
                "Behrouz Maham",
                "Tohid Alizadeh",
                "Sina Ebrahimi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05269v2",
                "http://arxiv.org/pdf/2310.05269v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CR",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05258v1",
            "title": "A Knowledge Graph-Based Search Engine for Robustly Finding Doctors and\n  Locations in the Healthcare Domain",
            "updated": "2023-10-08T18:28:17Z",
            "published": "2023-10-08T18:28:17Z",
            "summary": "Efficiently finding doctors and locations is an important search problem for\npatients in the healthcare domain, for which traditional information retrieval\nmethods tend not to work optimally. In the last ten years, knowledge graphs\n(KGs) have emerged as a powerful way to combine the benefits of gleaning\ninsights from semi-structured data using semantic modeling, natural language\nprocessing techniques like information extraction, and robust querying using\nstructured query languages like SPARQL and Cypher. In this short paper, we\npresent a KG-based search engine architecture for robustly finding doctors and\nlocations in the healthcare domain. Early results demonstrate that our approach\ncan lead to significantly higher coverage for complex queries without degrading\nquality.",
            "author": [
                "Mayank Kejriwal",
                "Hamid Haidarian",
                "Min-Hsueh Chiu",
                "Andy Xiang",
                "Deep Shrestha",
                "Faizan Javed"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05258v1",
                "http://arxiv.org/pdf/2310.05258v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.DB",
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05255v2",
            "title": "Persis: A Persian Font Recognition Pipeline Using Convolutional Neural\n  Networks",
            "updated": "2023-10-10T05:48:25Z",
            "published": "2023-10-08T18:07:15Z",
            "summary": "What happens if we encounter a suitable font for our design work but do not\nknow its name? Visual Font Recognition (VFR) systems are used to identify the\nfont typeface in an image. These systems can assist graphic designers in\nidentifying fonts used in images. A VFR system also aids in improving the speed\nand accuracy of Optical Character Recognition (OCR) systems. In this paper, we\nintroduce the first publicly available datasets in the field of Persian font\nrecognition and employ Convolutional Neural Networks (CNN) to address this\nproblem. The results show that the proposed pipeline obtained 78.0% top-1\naccuracy on our new datasets, 89.1% on the IDPL-PFOD dataset, and 94.5% on the\nKAFD dataset. Furthermore, the average time spent in the entire pipeline for\none sample of our proposed datasets is 0.54 and 0.017 seconds for CPU and GPU,\nrespectively. We conclude that CNN methods can be used to recognize Persian\nfonts without the need for additional pre-processing steps such as feature\nextraction, binarization, normalization, etc.",
            "author": [
                "Mehrdad Mohammadian",
                "Neda Maleki",
                "Tobias Olsson",
                "Fredrik Ahlgren"
            ],
            "link": [
                "http://dx.doi.org/10.1109/ICCKE57176.2022.9960037",
                "http://arxiv.org/abs/2310.05255v2",
                "http://arxiv.org/pdf/2310.05255v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05253v2",
            "title": "Explainable Claim Verification via Knowledge-Grounded Reasoning with\n  Large Language Models",
            "updated": "2023-10-20T02:31:21Z",
            "published": "2023-10-08T18:04:05Z",
            "summary": "Claim verification plays a crucial role in combating misinformation. While\nexisting works on claim verification have shown promising results, a crucial\npiece of the puzzle that remains unsolved is to understand how to verify claims\nwithout relying on human-annotated data, which is expensive to create at a\nlarge scale. Additionally, it is important for models to provide comprehensive\nexplanations that can justify their decisions and assist human fact-checkers.\nThis paper presents First-Order-Logic-Guided Knowledge-Grounded (FOLK)\nReasoning that can verify complex claims and generate explanations without the\nneed for annotated evidence using Large Language Models (LLMs). FOLK leverages\nthe in-context learning ability of LLMs to translate the claim into a\nFirst-Order-Logic (FOL) clause consisting of predicates, each corresponding to\na sub-claim that needs to be verified. Then, FOLK performs FOL-Guided reasoning\nover a set of knowledge-grounded question-and-answer pairs to make veracity\npredictions and generate explanations to justify its decision-making process.\nThis process makes our model highly explanatory, providing clear explanations\nof its reasoning process in human-readable form. Our experiment results\nindicate that FOLK outperforms strong baselines on three datasets encompassing\nvarious claim verification challenges. Our code and data are available.",
            "author": [
                "Haoran Wang",
                "Kai Shu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05253v2",
                "http://arxiv.org/pdf/2310.05253v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05251v1",
            "title": "The graphs of pyramids are determined by their spectrum",
            "updated": "2023-10-08T18:01:00Z",
            "published": "2023-10-08T18:01:00Z",
            "summary": "For natural numbers $k<n$ we study the graphs\n$T_{n,k}:=K_{k}\\lor\\overline{K_{n-k}}$. For $k=1$, $T_{n,1}$ is the star\n$S_{n-1}$. For $k>1$ we refer to $T_{n,k}$ as a \\emph{graph of pyramids}. We\nprove that the graphs of pyramids are determined by their spectrum, and that a\nstar $S_{n}$ is determined by its spectrum iff $n$ is prime. We also show that\nthe graphs $T_{n,k}$ are completely positive iff $k\\le2$.",
            "author": [
                "Noam Krupnik",
                "Abraham Berman"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05251v1",
                "http://arxiv.org/pdf/2310.05251v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C50, 15B48"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05250v1",
            "title": "Simplifying GNN Performance with Low Rank Kernel Models",
            "updated": "2023-10-08T17:56:30Z",
            "published": "2023-10-08T17:56:30Z",
            "summary": "We revisit recent spectral GNN approaches to semi-supervised node\nclassification (SSNC). We posit that many of the current GNN architectures may\nbe over-engineered. Instead, simpler, traditional methods from nonparametric\nestimation, applied in the spectral domain, could replace many deep-learning\ninspired GNN designs. These conventional techniques appear to be well suited\nfor a variety of graph types reaching state-of-the-art performance on many of\nthe common SSNC benchmarks. Additionally, we show that recent performance\nimprovements in GNN approaches may be partially attributed to shifts in\nevaluation conventions. Lastly, an ablative study is conducted on the various\nhyperparameters associated with GNN spectral filtering techniques. Code\navailable at: https://github.com/lucianoAvinas/lowrank-gnn-kernels",
            "author": [
                "Luciano Vinas",
                "Arash A. Amini"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05250v1",
                "http://arxiv.org/pdf/2310.05250v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05248v1",
            "title": "A Hall-type condition for path covers in bipartite graphs",
            "updated": "2023-10-08T17:47:42Z",
            "published": "2023-10-08T17:47:42Z",
            "summary": "Let $G$ be a bipartite graph with bipartition $(X,Y)$. Inspired by a\nhypergraph problem, we seek an upper bound on the number of disjoint paths\nneeded to cover all the vertices of $X$. We conjecture that a Hall-type\nsufficient condition holds based on the maximum value of\n$|S|-|\\mathsf\\Lambda(S)|$, where $S\\subseteq X$ and $\\mathsf\\Lambda(S)$ is the\nset of all vertices in $Y$ with at least two neighbors in $S$. This condition\nis also a necessary one for a hereditary version of the problem, where we\ndelete vertices from $X$ and try to cover the remaining vertices by disjoint\npaths. The conjecture holds when $G$ is a forest, has maximum degree $3$, or is\nregular with high girth, and we prove those results in this paper.",
            "author": [
                "Mikhail Lavrov",
                "Jennifer Vandenbussche"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05248v1",
                "http://arxiv.org/pdf/2310.05248v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C38"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05228v1",
            "title": "Recent Progress in Leptonic and Semileptonic Decays of Charmed Hadrons",
            "updated": "2023-10-08T16:51:43Z",
            "published": "2023-10-08T16:51:43Z",
            "summary": "We present a comprehensive review of purely leptonic and semileptonic decays\nof $D^{0(+)}$, $D_s^{+}$, and charmed baryons (including $\\Lambda_c^{+}$,\n$\\Xi_c$ and $\\Omega_c$). The precise studies of these decays help deepen our\nunderstanding and knowledge of quantum chromodynamics via measuring decay\nconstants and form factors, and test the Standard Model through examining the\nunitarity of Cabibbo-Kobayashi-Maskawa matrix and lepton flavor universality.\nWe give an overview of the theoretical and experimental tools before discussing\nthe recent progress. The data sets collected by BESIII near the production\nthresholds of $D\\bar{D}$, $D_s^{(*)+}D_s^{(*)-}$ and\n$\\Lambda_c^{+}\\bar{\\Lambda}_c^{-}$ offer important opportunities for studies of\ncharm physics.",
            "author": [
                "Bai-Cian Ke",
                "Jonna Koponen",
                "Hai-Bo Li",
                "Yangheng Zheng"
            ],
            "link": [
                "http://dx.doi.org/10.1146/annurev-nucl-110222-044046",
                "http://arxiv.org/abs/2310.05228v1",
                "http://arxiv.org/pdf/2310.05228v1"
            ],
            "primary_category": "hep-ex",
            "category": [
                "hep-ex",
                "hep-lat",
                "hep-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05227v2",
            "title": "Physics-aware Machine Learning Revolutionizes Scientific Paradigm for\n  Machine Learning and Process-based Hydrology",
            "updated": "2023-10-20T14:30:41Z",
            "published": "2023-10-08T16:48:29Z",
            "summary": "Accurate hydrological understanding and water cycle prediction are crucial\nfor addressing scientific and societal challenges associated with the\nmanagement of water resources, particularly under the dynamic influence of\nanthropogenic climate change. Existing reviews predominantly concentrate on the\ndevelopment of machine learning (ML) in this field, yet there is a clear\ndistinction between hydrology and ML as separate paradigms. Here, we introduce\nphysics-aware ML as a transformative approach to overcome the perceived barrier\nand revolutionize both fields. Specifically, we present a comprehensive review\nof the physics-aware ML methods, building a structured community (PaML) of\nexisting methodologies that integrate prior physical knowledge or physics-based\nmodeling into ML. We systematically analyze these PaML methodologies with\nrespect to four aspects: physical data-guided ML, physics-informed ML,\nphysics-embedded ML, and physics-aware hybrid learning. PaML facilitates\nML-aided hypotheses, accelerating insights from big data and fostering\nscientific discoveries. We first conduct a systematic review of hydrology in\nPaML, including rainfall-runoff hydrological processes and hydrodynamic\nprocesses, and highlight the most promising and challenging directions for\ndifferent objectives and PaML methods. Finally, a new PaML-based hydrology\nplatform, termed HydroPML, is released as a foundation for hydrological\napplications. HydroPML enhances the explainability and causality of ML and lays\nthe groundwork for the digital water cycle's realization. The HydroPML platform\nis publicly available at https://hydropml.github.io/.",
            "author": [
                "Qingsong Xu",
                "Yilei Shi",
                "Jonathan Bamber",
                "Ye Tuo",
                "Ralf Ludwig",
                "Xiao Xiang Zhu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05227v2",
                "http://arxiv.org/pdf/2310.05227v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "physics.flu-dyn"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05217v1",
            "title": "PointGAT: A quantum chemical property prediction model integrating graph\n  attention and 3D geometry",
            "updated": "2023-10-08T16:25:32Z",
            "published": "2023-10-08T16:25:32Z",
            "summary": "Predicting quantum chemical properties is a fundamental challenge for\ncomputational chemistry. While the development of graph neural networks has\nadvanced molecular representation learning and property prediction, their\nperformance could be further enhanced by incorporating 3D structural geometry\ninto 2D molecular graph representation. In this study, we introduce the\nPointGAT model for quantum molecular property prediction, which integrates 3D\nmolecular coordinates with graph-attention modeling. Comparison with other\ncurrent models in molecular prediction tasks showed that PointGAT could provide\nhigher predictive accuracy in various benchmark datasets from MoleculeNet,\nincluding ESOL, FreeSolv, Lipop, HIV, and 10 out of 12 tasks of the QM9\ndataset. To further examine PointGAT prediction of quantum mechanical (QM)\nenergies, we constructed a C10 dataset comprising 11,841 charged and chiral\ncarbocation intermediates with QM energies calculated at the\nDM21/6-31G*//B3LYP/6-31G* levels. Notably, PointGAT achieved an R2 value of\n0.950 and an MAE of 1.616 kcal/mol, outperforming other models. Additional\nablation studies indicated that incorporating molecular geometry into the model\nresulted in markedly higher predictive accuracy, reducing the MAE value from\n1.802 kcal/mol to 1.616 kcal/mol. Moreover, visualization of PointGAT atomic\nattention weights suggested its predictions were interpretable. Findings in\nthis study support the application of PointGAT as a powerful and versatile tool\nfor quantum chemical property prediction that can facilitate high-accuracy\nmodeling for fundamental exploration of chemical space as well as drug design\nand molecular engineering.",
            "author": [
                "Rong Zhang",
                "Rongqing Yuan",
                "Boxue Tian"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05217v1",
                "http://arxiv.org/pdf/2310.05217v1"
            ],
            "primary_category": "q-bio.QM",
            "category": [
                "q-bio.QM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05216v1",
            "title": "Probing Language Models from A Human Behavioral Perspective",
            "updated": "2023-10-08T16:16:21Z",
            "published": "2023-10-08T16:16:21Z",
            "summary": "Large Language Models (LLMs) have emerged as dominant foundational models in\nmodern NLP. However, the understanding of their prediction process and internal\nmechanisms, such as feed-forward networks and multi-head self-attention,\nremains largely unexplored. In this study, we probe LLMs from a human\nbehavioral perspective, correlating values from LLMs with eye-tracking\nmeasures, which are widely recognized as meaningful indicators of reading\npatterns. Our findings reveal that LLMs exhibit a prediction pattern distinct\nfrom that of RNN-based LMs. Moreover, with the escalation of FFN layers, the\ncapacity for memorization and linguistic knowledge encoding also surges until\nit peaks, subsequently pivoting to focus on comprehension capacity. The\nfunctions of self-attention are distributed across multiple heads. Lastly, we\nscrutinize the gate mechanisms, finding that they control the flow of\ninformation, with some gates promoting, while others eliminating information.",
            "author": [
                "Xintong Wang",
                "Xiaoyu Li",
                "Xingshan Li",
                "Chris Biemann"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05216v1",
                "http://arxiv.org/pdf/2310.05216v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05215v1",
            "title": "Motion Matching for Character Animation and Virtual Reality Avatars in\n  Unity",
            "updated": "2023-10-08T16:12:51Z",
            "published": "2023-10-08T16:12:51Z",
            "summary": "Real-time animation of virtual characters has traditionally been accomplished\nby playing short sequences of animations structured in the form of a graph.\nThese methods are time-consuming to set up and scale poorly with the number of\nmotions required in modern virtual environments. The ever-increasing need for\nhighly-realistic virtual characters in fields such as entertainment, virtual\nreality, or the metaverse has led to significant advances in the field of\ndata-driven character animation. Techniques like Motion Matching have provided\nenough versatility to conveniently animate virtual characters using a selection\nof features from an animation database. Data-driven methods retain the quality\nof the captured animations, thus delivering smoother and more natural-looking\nanimations. In this work, we researched and developed a Motion Matching\ntechnique for the Unity game engine. In this thesis, we present our findings on\nhow to implement an animation system based on Motion Matching. We also\nintroduce a novel method combining body orientation prediction with Motion\nMatching to animate avatars for consumer-grade virtual reality systems.",
            "author": [
                "Jose Luis Ponton"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05215v1",
                "http://arxiv.org/pdf/2310.05215v1"
            ],
            "primary_category": "cs.GR",
            "category": [
                "cs.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05202v3",
            "title": "Enhancing Cross-Dataset Performance of Distracted Driving Detection With\n  Score-Softmax Classifier",
            "updated": "2023-10-20T04:09:39Z",
            "published": "2023-10-08T15:28:01Z",
            "summary": "Deep neural networks enable real-time monitoring of in-vehicle driver,\nfacilitating the timely prediction of distractions, fatigue, and potential\nhazards. This technology is now integral to intelligent transportation systems.\nRecent research has exposed unreliable cross-dataset end-to-end driver behavior\nrecognition due to overfitting, often referred to as ``shortcut learning\",\nresulting from limited data samples. In this paper, we introduce the\nScore-Softmax classifier, which addresses this issue by enhancing inter-class\nindependence and Intra-class uncertainty. Motivated by human rating patterns,\nwe designed a two-dimensional supervisory matrix based on marginal Gaussian\ndistributions to train the classifier. Gaussian distributions help amplify\nintra-class uncertainty while ensuring the Score-Softmax classifier learns\naccurate knowledge. Furthermore, leveraging the summation of independent\nGaussian distributed random variables, we introduced a multi-channel\ninformation fusion method. This strategy effectively resolves the\nmulti-information fusion challenge for the Score-Softmax classifier.\nConcurrently, we substantiate the necessity of transfer learning and\nmulti-dataset combination. We conducted cross-dataset experiments using the\nSFD, AUCDD-V1, and 100-Driver datasets, demonstrating that Score-Softmax\nimproves cross-dataset performance without modifying the model architecture.\nThis provides a new approach for enhancing neural network generalization.\nAdditionally, our information fusion approach outperforms traditional methods.",
            "author": [
                "Cong Duan",
                "Zixuan Liu",
                "Jiahao Xia",
                "Minghai Zhang",
                "Jiacai Liao",
                "Libo Cao"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05202v3",
                "http://arxiv.org/pdf/2310.05202v3"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05187v1",
            "title": "Lifelong Learning for Fog Load Balancing: A Transfer Learning Approach",
            "updated": "2023-10-08T14:49:33Z",
            "published": "2023-10-08T14:49:33Z",
            "summary": "Fog computing emerged as a promising paradigm to address the challenges of\nprocessing and managing data generated by the Internet of Things (IoT). Load\nbalancing (LB) plays a crucial role in Fog computing environments to optimize\nthe overall system performance. It requires efficient resource allocation to\nimprove resource utilization, minimize latency, and enhance the quality of\nservice for end-users. In this work, we improve the performance of\nprivacy-aware Reinforcement Learning (RL) agents that optimize the execution\ndelay of IoT applications by minimizing the waiting delay. To maintain privacy,\nthese agents optimize the waiting delay by minimizing the change in the number\nof queued requests in the whole system, i.e., without explicitly observing the\nactual number of requests that are queued in each Fog node nor observing the\ncompute resource capabilities of those nodes. Besides improving the performance\nof these agents, we propose in this paper a lifelong learning framework for\nthese agents, where lightweight inference models are used during deployment to\nminimize action delay and only retrained in case of significant environmental\nchanges. To improve the performance, minimize the training cost, and adapt the\nagents to those changes, we explore the application of Transfer Learning (TL).\nTL transfers the knowledge acquired from a source domain and applies it to a\ntarget domain, enabling the reuse of learned policies and experiences. TL can\nbe also used to pre-train the agent in simulation before fine-tuning it in the\nreal environment; this significantly reduces failure probability compared to\nlearning from scratch in the real environment. To our knowledge, there are no\nexisting efforts in the literature that use TL to address lifelong learning for\nRL-based Fog LB; this is one of the main obstacles in deploying RL LB solutions\nin Fog systems.",
            "author": [
                "Maad Ebrahim",
                "Abdelhakim Senhaji Hafid",
                "Mohamed Riduan Abid"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05187v1",
                "http://arxiv.org/pdf/2310.05187v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05185v2",
            "title": "Text2NKG: Fine-Grained N-ary Relation Extraction for N-ary relational\n  Knowledge Graph Construction",
            "updated": "2023-10-12T18:26:46Z",
            "published": "2023-10-08T14:47:13Z",
            "summary": "Beyond traditional binary relational facts, n-ary relational knowledge graphs\n(NKGs) are comprised of n-ary relational facts containing more than two\nentities, which are closer to real-world facts with broader applications.\nHowever, the construction of NKGs still significantly relies on manual labor,\nand n-ary relation extraction still remains at a course-grained level, which is\nalways in a single schema and fixed arity of entities. To address these\nrestrictions, we propose Text2NKG, a novel fine-grained n-ary relation\nextraction framework for n-ary relational knowledge graph construction. We\nintroduce a span-tuple classification approach with hetero-ordered merging to\naccomplish fine-grained n-ary relation extraction in different arity.\nFurthermore, Text2NKG supports four typical NKG schemas: hyper-relational\nschema, event-based schema, role-based schema, and hypergraph-based schema,\nwith high flexibility and practicality. Experimental results demonstrate that\nText2NKG outperforms the previous state-of-the-art model by nearly 20\\% points\nin the $F_1$ scores on the fine-grained n-ary relation extraction benchmark in\nthe hyper-relational schema. Our code and datasets are publicly available.",
            "author": [
                "Haoran Luo",
                "Haihong E",
                "Yuhao Yang",
                "Tianyu Yao",
                "Yikai Guo",
                "Zichen Tang",
                "Wentai Zhang",
                "Kaiyang Wan",
                "Shiyao Peng",
                "Meina Song",
                "Wei Lin"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05185v2",
                "http://arxiv.org/pdf/2310.05185v2"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05178v1",
            "title": "Optimizing Large Language Models to Expedite the Development of Smart\n  Contracts",
            "updated": "2023-10-08T14:29:33Z",
            "published": "2023-10-08T14:29:33Z",
            "summary": "Programming has always been at the heart of technological innovation in the\n21st century. With the advent of blockchain technologies and the proliferation\nof web3 paradigms of decentralised applications, smart contracts have been very\ninstrumental in enabling developers to build applications that reside on\ndecentralised blockchains. Despite the huge interest and potential of smart\ncontracts, there is still a significant knowledge and skill gap that developers\nneed to cross in order to build web3 applications. In light of this, we\nintroduce MazzumaGPT, a large language model that has been optimised to\ngenerate smart contract code and aid developers to scaffold development and\nimprove productivity. As part of this research, we outline the optimisation and\nfine-tuning parameters, evaluate the model's performance on functional\ncorrectness and address the limitations and broader impacts of our research.",
            "author": [
                "Nii Osae Osae Dade",
                "Margaret Lartey-Quaye",
                "Emmanuel Teye-Kofi Odonkor",
                "Paul Ammah"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05178v1",
                "http://arxiv.org/pdf/2310.05178v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE",
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05177v1",
            "title": "Do Large Language Models Know about Facts?",
            "updated": "2023-10-08T14:26:55Z",
            "published": "2023-10-08T14:26:55Z",
            "summary": "Large language models (LLMs) have recently driven striking performance\nimprovements across a range of natural language processing tasks. The factual\nknowledge acquired during pretraining and instruction tuning can be useful in\nvarious downstream tasks, such as question answering, and language generation.\nUnlike conventional Knowledge Bases (KBs) that explicitly store factual\nknowledge, LLMs implicitly store facts in their parameters. Content generated\nby the LLMs can often exhibit inaccuracies or deviations from the truth, due to\nfacts that can be incorrectly induced or become obsolete over time. To this\nend, we aim to comprehensively evaluate the extent and scope of factual\nknowledge within LLMs by designing the benchmark Pinocchio. Pinocchio contains\n20K diverse factual questions that span different sources, timelines, domains,\nregions, and languages. Furthermore, we investigate whether LLMs are able to\ncompose multiple facts, update factual knowledge temporally, reason over\nmultiple pieces of facts, identify subtle factual differences, and resist\nadversarial examples. Extensive experiments on different sizes and types of\nLLMs show that existing LLMs still lack factual knowledge and suffer from\nvarious spurious correlations. We believe this is a critical bottleneck for\nrealizing trustworthy artificial intelligence. The dataset Pinocchio and our\ncodes will be publicly available.",
            "author": [
                "Xuming Hu",
                "Junzhe Chen",
                "Xiaochuan Li",
                "Yufei Guo",
                "Lijie Wen",
                "Philip S. Yu",
                "Zhijiang Guo"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05177v1",
                "http://arxiv.org/pdf/2310.05177v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05174v1",
            "title": "GSLB: The Graph Structure Learning Benchmark",
            "updated": "2023-10-08T14:13:03Z",
            "published": "2023-10-08T14:13:03Z",
            "summary": "Graph Structure Learning (GSL) has recently garnered considerable attention\ndue to its ability to optimize both the parameters of Graph Neural Networks\n(GNNs) and the computation graph structure simultaneously. Despite the\nproliferation of GSL methods developed in recent years, there is no standard\nexperimental setting or fair comparison for performance evaluation, which\ncreates a great obstacle to understanding the progress in this field. To fill\nthis gap, we systematically analyze the performance of GSL in different\nscenarios and develop a comprehensive Graph Structure Learning Benchmark (GSLB)\ncurated from 20 diverse graph datasets and 16 distinct GSL algorithms.\nSpecifically, GSLB systematically investigates the characteristics of GSL in\nterms of three dimensions: effectiveness, robustness, and complexity. We\ncomprehensively evaluate state-of-the-art GSL algorithms in node- and\ngraph-level tasks, and analyze their performance in robust learning and model\ncomplexity. Further, to facilitate reproducible research, we have developed an\neasy-to-use library for training, evaluating, and visualizing different GSL\nmethods. Empirical results of our extensive experiments demonstrate the ability\nof GSL and reveal its potential benefits on various downstream tasks, offering\ninsights and opportunities for future research. The code of GSLB is available\nat: https://github.com/GSL-Benchmark/GSLB.",
            "author": [
                "Zhixun Li",
                "Liang Wang",
                "Xin Sun",
                "Yifan Luo",
                "Yanqiao Zhu",
                "Dingshuo Chen",
                "Yingtao Luo",
                "Xiangxin Zhou",
                "Qiang Liu",
                "Shu Wu",
                "Liang Wang",
                "Jeffrey Xu Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05174v1",
                "http://arxiv.org/pdf/2310.05174v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05163v3",
            "title": "An Investigation of LLMs' Inefficacy in Understanding Converse Relations",
            "updated": "2023-11-13T07:24:14Z",
            "published": "2023-10-08T13:45:05Z",
            "summary": "Large Language Models (LLMs) have achieved remarkable success in many formal\nlanguage oriented tasks, such as structural data-to-text and semantic parsing.\nHowever current benchmarks mostly follow the data distribution of the\npre-training data of LLMs. Therefore, a natural question rises that do LLMs\nreally understand the structured semantics of formal languages. In this paper,\nwe investigate this problem on a special case, converse binary relation. We\nintroduce a new benchmark ConvRe focusing on converse relations, which contains\n17 relations and 1240 triples extracted from popular knowledge graph completion\ndatasets. Our ConvRE features two tasks, Re2Text and Text2Re, which are\nformulated as multi-choice question answering to evaluate LLMs' ability to\ndetermine the matching between relations and associated text. For the\nevaluation protocol, apart from different prompting methods, we further\nintroduce variants to the test text and few-shot example text. We conduct\nexperiments on three popular LLM families and have observed various scaling\ntrends. The results suggest that LLMs often resort to shortcut learning and\nstill face challenges on our proposed benchmark.",
            "author": [
                "Chengwen Qi",
                "Bowen Li",
                "Binyuan Hui",
                "Bailin Wang",
                "Jinyang Li",
                "Jinwang Wu",
                "Yuanjun Laili"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05163v3",
                "http://arxiv.org/pdf/2310.05163v3"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05150v1",
            "title": "From Data to Dialogue: Leveraging the Structure of Knowledge Graphs for\n  Conversational Exploratory Search",
            "updated": "2023-10-08T12:52:09Z",
            "published": "2023-10-08T12:52:09Z",
            "summary": "Exploratory search is an open-ended information retrieval process that aims\nat discovering knowledge about a topic or domain rather than searching for a\nspecific answer or piece of information. Conversational interfaces are\nparticularly suitable for supporting exploratory search, allowing users to\nrefine queries and examine search results through interactive dialogues. In\naddition to conversational search interfaces, knowledge graphs are also useful\nin supporting information exploration due to their rich semantic representation\nof data items. In this study, we demonstrate the synergistic effects of\ncombining knowledge graphs and conversational interfaces for exploratory\nsearch, bridging the gap between structured and unstructured information\nretrieval. To this end, we propose a knowledge-driven dialogue system for\nexploring news articles by asking natural language questions and using the\ngraph structure to navigate between related topics. Based on a user study with\n54 participants, we empirically evaluate the effectiveness of the graph-based\nexploratory search and discuss design implications for developing such systems.",
            "author": [
                "Phillip Schneider",
                "Nils Rehtanz",
                "Kristiina Jokinen",
                "Florian Matthes"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05150v1",
                "http://arxiv.org/pdf/2310.05150v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05149v1",
            "title": "Retrieval-Generation Synergy Augmented Large Language Models",
            "updated": "2023-10-08T12:50:57Z",
            "published": "2023-10-08T12:50:57Z",
            "summary": "Large language models augmented with task-relevant documents have\ndemonstrated impressive performance on knowledge-intensive tasks. However,\nregarding how to obtain effective documents, the existing methods are mainly\ndivided into two categories. One is to retrieve from an external knowledge\nbase, and the other is to utilize large language models to generate documents.\nWe propose an iterative retrieval-generation collaborative framework. It is not\nonly able to leverage both parametric and non-parametric knowledge, but also\nhelps to find the correct reasoning path through retrieval-generation\ninteractions, which is very important for tasks that require multi-step\nreasoning. We conduct experiments on four question answering datasets,\nincluding single-hop QA and multi-hop QA tasks. Empirical results show that our\nmethod significantly improves the reasoning ability of large language models\nand outperforms previous baselines.",
            "author": [
                "Zhangyin Feng",
                "Xiaocheng Feng",
                "Dezhi Zhao",
                "Maojin Yang",
                "Bing Qin"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05149v1",
                "http://arxiv.org/pdf/2310.05149v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05140v3",
            "title": "Harnessing the Power of Large Language Models for Empathetic Response\n  Generation: Empirical Investigations and Improvements",
            "updated": "2023-12-07T07:05:50Z",
            "published": "2023-10-08T12:21:24Z",
            "summary": "Empathetic dialogue is an indispensable part of building harmonious social\nrelationships and contributes to the development of a helpful AI. Previous\napproaches are mainly based on fine small-scale language models. With the\nadvent of ChatGPT, the application effect of large language models (LLMs) in\nthis field has attracted great attention. This work empirically investigates\nthe performance of LLMs in generating empathetic responses and proposes three\nimprovement methods of semantically similar in-context learning, two-stage\ninteractive generation, and combination with the knowledge base. Extensive\nexperiments show that LLMs can significantly benefit from our proposed methods\nand is able to achieve state-of-the-art performance in both automatic and human\nevaluations. Additionally, we explore the possibility of GPT-4 simulating human\nevaluators.",
            "author": [
                "Yushan Qian",
                "Wei-Nan Zhang",
                "Ting Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05140v3",
                "http://arxiv.org/pdf/2310.05140v3"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05139v1",
            "title": "Maximizing Utilitarian and Egalitarian Welfare of Fractional Hedonic\n  Games on Tree-like Graphs",
            "updated": "2023-10-08T12:18:08Z",
            "published": "2023-10-08T12:18:08Z",
            "summary": "Fractional hedonic games are coalition formation games where a player's\nutility is determined by the average value they assign to the members of their\ncoalition. These games are a variation of graph hedonic games, which are a\nclass of coalition formation games that can be succinctly represented. Due to\ntheir applicability in network clustering and their relationship to graph\nhedonic games, fractional hedonic games have been extensively studied from\nvarious perspectives. However, finding welfare-maximizing partitions in\nfractional hedonic games is a challenging task due to the nonlinearity of\nutilities. In fact, it has been proven to be NP-hard and can be solved in\npolynomial time only for a limited number of graph classes, such as trees. This\npaper presents (pseudo)polynomial-time algorithms to compute welfare-maximizing\npartitions in fractional hedonic games on tree-like graphs. We consider two\ntypes of social welfare measures: utilitarian and egalitarian. Tree-like graphs\nrefer to graphs with bounded treewidth and block graphs. A hardness result is\nprovided, demonstrating that the pseudopolynomial-time solvability is the best\npossible under the assumption P$\\neq$NP.",
            "author": [
                "Tesshu Hanaka",
                "Airi Ikeyama",
                "Hirotaka Ono"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05139v1",
                "http://arxiv.org/pdf/2310.05139v1"
            ],
            "primary_category": "cs.GT",
            "category": [
                "cs.GT",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05129v1",
            "title": "ed-cec: improving rare word recognition using asr postprocessing based\n  on error detection and context-aware error correction",
            "updated": "2023-10-08T11:40:30Z",
            "published": "2023-10-08T11:40:30Z",
            "summary": "Automatic speech recognition (ASR) systems often encounter difficulties in\naccurately recognizing rare words, leading to errors that can have a negative\nimpact on downstream tasks such as keyword spotting, intent detection, and text\nsummarization. To address this challenge, we present a novel ASR postprocessing\nmethod that focuses on improving the recognition of rare words through error\ndetection and context-aware error correction. Our method optimizes the decoding\nprocess by targeting only the predicted error positions, minimizing unnecessary\ncomputations. Moreover, we leverage a rare word list to provide additional\ncontextual knowledge, enabling the model to better correct rare words.\nExperimental results across five datasets demonstrate that our proposed method\nachieves significantly lower word error rates (WERs) than previous approaches\nwhile maintaining a reasonable inference speed. Furthermore, our approach\nexhibits promising robustness across different ASR systems.",
            "author": [
                "Jiajun He",
                "Zekun Yang",
                "Tomoki Toda"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05129v1",
                "http://arxiv.org/pdf/2310.05129v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05125v1",
            "title": "Bidirectional Knowledge Reconfiguration for Lightweight Point Cloud\n  Analysis",
            "updated": "2023-10-08T11:32:50Z",
            "published": "2023-10-08T11:32:50Z",
            "summary": "Point cloud analysis faces computational system overhead, limiting its\napplication on mobile or edge devices. Directly employing small models may\nresult in a significant drop in performance since it is difficult for a small\nmodel to adequately capture local structure and global shape information\nsimultaneously, which are essential clues for point cloud analysis. This paper\nexplores feature distillation for lightweight point cloud models. To mitigate\nthe semantic gap between the lightweight student and the cumbersome teacher, we\npropose bidirectional knowledge reconfiguration (BKR) to distill informative\ncontextual knowledge from the teacher to the student. Specifically, a top-down\nknowledge reconfiguration and a bottom-up knowledge reconfiguration are\ndeveloped to inherit diverse local structure information and consistent global\nshape knowledge from the teacher, respectively. However, due to the farthest\npoint sampling in most point cloud models, the intermediate features between\nteacher and student are misaligned, deteriorating the feature distillation\nperformance. To eliminate it, we propose a feature mover's distance (FMD) loss\nbased on optimal transportation, which can measure the distance between\nunordered point cloud features effectively. Extensive experiments conducted on\nshape classification, part segmentation, and semantic segmentation benchmarks\ndemonstrate the universality and superiority of our method.",
            "author": [
                "Peipei Li",
                "Xing Cui",
                "Yibo Hu",
                "Man Zhang",
                "Ting Yao",
                "Tao Mei"
            ],
            "link": [
                "http://dx.doi.org/10.1109/TMM.2023.3321535",
                "http://arxiv.org/abs/2310.05125v1",
                "http://arxiv.org/pdf/2310.05125v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05119v1",
            "title": "Dynamic Multi-Domain Knowledge Networks for Chest X-ray Report\n  Generation",
            "updated": "2023-10-08T11:20:02Z",
            "published": "2023-10-08T11:20:02Z",
            "summary": "The automated generation of radiology diagnostic reports helps radiologists\nmake timely and accurate diagnostic decisions while also enhancing clinical\ndiagnostic efficiency. However, the significant imbalance in the distribution\nof data between normal and abnormal samples (including visual and textual\nbiases) poses significant challenges for a data-driven task like automatically\ngenerating diagnostic radiology reports. Therefore, we propose a Dynamic\nMulti-Domain Knowledge(DMDK) network for radiology diagnostic report\ngeneration. The DMDK network consists of four modules: Chest Feature\nExtractor(CFE), Dynamic Knowledge Extractor(DKE), Specific Knowledge\nExtractor(SKE), and Multi-knowledge Integrator(MKI) module. Specifically, the\nCFE module is primarily responsible for extracting the unprocessed visual\nmedical features of the images. The DKE module is responsible for extracting\ndynamic disease topic labels from the retrieved radiology diagnostic reports.\nWe then fuse the dynamic disease topic labels with the original visual features\nof the images to highlight the abnormal regions in the original visual features\nto alleviate the visual data bias problem. The SKE module expands upon the\nconventional static knowledge graph to mitigate textual data biases and amplify\nthe interpretability capabilities of the model via domain-specific dynamic\nknowledge graphs. The MKI distills all the knowledge and generates the final\ndiagnostic radiology report. We performed extensive experiments on two widely\nused datasets, IU X-Ray and MIMIC-CXR. The experimental results demonstrate the\neffectiveness of our method, with all evaluation metrics outperforming previous\nstate-of-the-art models.",
            "author": [
                "Weihua Liu",
                "Youyuan Xue",
                "Chaochao Lin",
                "Said Boumaraf"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05119v1",
                "http://arxiv.org/pdf/2310.05119v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05115v1",
            "title": "Breaking Down Word Semantics from Pre-trained Language Models through\n  Layer-wise Dimension Selection",
            "updated": "2023-10-08T11:07:19Z",
            "published": "2023-10-08T11:07:19Z",
            "summary": "Contextual word embeddings obtained from pre-trained language model (PLM)\nhave proven effective for various natural language processing tasks at the word\nlevel. However, interpreting the hidden aspects within embeddings, such as\nsyntax and semantics, remains challenging. Disentangled representation learning\nhas emerged as a promising approach, which separates specific aspects into\ndistinct embeddings. Furthermore, different linguistic knowledge is believed to\nbe stored in different layers of PLM. This paper aims to disentangle semantic\nsense from BERT by applying a binary mask to middle outputs across the layers,\nwithout updating pre-trained parameters. The disentangled embeddings are\nevaluated through binary classification to determine if the target word in two\ndifferent sentences has the same meaning. Experiments with cased\nBERT$_{\\texttt{base}}$ show that leveraging layer-wise information is effective\nand disentangling semantic sense further improve performance.",
            "author": [
                "Nayoung Choi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05115v1",
                "http://arxiv.org/pdf/2310.05115v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05105v1",
            "title": "How Graph Neural Networks Learn: Lessons from Training Dynamics in\n  Function Space",
            "updated": "2023-10-08T10:19:56Z",
            "published": "2023-10-08T10:19:56Z",
            "summary": "A long-standing goal in deep learning has been to characterize the learning\nbehavior of black-box models in a more interpretable manner. For graph neural\nnetworks (GNNs), considerable advances have been made in formalizing what\nfunctions they can represent, however it remains less clear whether and how\nGNNs learn desired functions during the optimization process. To fill this\ncritical gap, we study the learning dynamics of GNNs in function space via the\nanalytic framework of overparameterization. In particular, we find that the\nseemingly complicated training process of GNNs can be re-cast into a more\nfamiliar label propagation framework, due to the graph inductive bias implicit\nin this process. From this vantage point, we provide explanations for why the\nlearned GNN functions successfully generalize and for their pathological\nbehavior on heterophilic graphs, which are consistent with observations.\nPractically, sparsifying and implementing the learning dynamics lead to a\nminimalist semi-supervised learning algorithm with the efficiency of classic\nalgorithms and the effectiveness of modern GNNs.",
            "author": [
                "Chenxiao Yang",
                "Qitian Wu",
                "David Wipf",
                "Ruoyu Sun",
                "Junchi Yan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05105v1",
                "http://arxiv.org/pdf/2310.05105v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05103v1",
            "title": "Zero-Shot Detection of Machine-Generated Codes",
            "updated": "2023-10-08T10:08:21Z",
            "published": "2023-10-08T10:08:21Z",
            "summary": "This work proposes a training-free approach for the detection of\nLLMs-generated codes, mitigating the risks associated with their indiscriminate\nusage. To the best of our knowledge, our research is the first to investigate\nzero-shot detection techniques applied to code generated by advanced black-box\nLLMs like ChatGPT. Firstly, we find that existing training-based or zero-shot\ntext detectors are ineffective in detecting code, likely due to the unique\nstatistical properties found in code structures. We then modify the previous\nzero-shot text detection method, DetectGPT (Mitchell et al., 2023) by utilizing\na surrogate white-box model to estimate the probability of the rightmost\ntokens, allowing us to identify code snippets generated by language models.\nThrough extensive experiments conducted on the python codes of the CodeContest\nand APPS dataset, our approach demonstrates its effectiveness by achieving\nstate-of-the-art detection results on text-davinci-003, GPT-3.5, and GPT-4\nmodels. Moreover, our method exhibits robustness against revision attacks and\ngeneralizes well to Java codes. We also find that the smaller code language\nmodel like PolyCoder-160M performs as a universal code detector, outperforming\nthe billion-scale counterpart. The codes will be available at\nhttps://github.com/ Xianjun-Yang/Code_detection.git",
            "author": [
                "Xianjun Yang",
                "Kexun Zhang",
                "Haifeng Chen",
                "Linda Petzold",
                "William Yang Wang",
                "Wei Cheng"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05103v1",
                "http://arxiv.org/pdf/2310.05103v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.CR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05099v1",
            "title": "Intelligent DRL-Based Adaptive Region of Interest for Delay-sensitive\n  Telemedicine Applications",
            "updated": "2023-10-08T10:01:28Z",
            "published": "2023-10-08T10:01:28Z",
            "summary": "Telemedicine applications have recently received substantial potential and\ninterest, especially after the COVID-19 pandemic. Remote experience will help\npeople get their complex surgery done or transfer knowledge to local surgeons,\nwithout the need to travel abroad. Even with breakthrough improvements in\ninternet speeds, the delay in video streaming is still a hurdle in telemedicine\napplications. This imposes using image compression and region of interest (ROI)\ntechniques to reduce the data size and transmission needs. This paper proposes\na Deep Reinforcement Learning (DRL) model that intelligently adapts the ROI\nsize and non-ROI quality depending on the estimated throughput. The delay and\nstructural similarity index measure (SSIM) comparison are used to assess the\nDRL model. The comparison findings and the practical application reveal that\nDRL is capable of reducing the delay by 13% and keeping the overall quality in\nan acceptable range. Since the latency has been significantly reduced, these\nfindings are a valuable enhancement to telemedicine applications.",
            "author": [
                "Abdulrahman Soliman",
                "Amr Mohamed",
                "Elias Yaacoub",
                "Nikhil V. Navkar",
                "Aiman Erbad"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05099v1",
                "http://arxiv.org/pdf/2310.05099v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.MM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05085v2",
            "title": "Spectral extremal results on edge blow-up of graphs",
            "updated": "2023-10-10T05:13:57Z",
            "published": "2023-10-08T09:19:32Z",
            "summary": "The edge blow-up $F^{p+1}$ of a graph $F$ for an integer $p\\geq 2$ is\nobtained by replacing each edge in $F$ with a $K_{p+1}$ containing the edge,\nwhere the new vertices of $K_{p+1}$ are all distinct. Let $ex(n,F)$ and\n$spex(n,F)$ be the maximum size and maximum spectral radius of an $F$-free\ngraph of order $n$, respectively. In this paper, we determine the range of\n$spex(n,F^{p+1})$ when $F$ is bipartite and the exact value of\n$spex(n,F^{p+1})$ when $F$ is non-bipartite for sufficiently large $n$, which\nare the spectral versions of Tur\\'{a}n's problems on $ex(n,F^{p+1})$ solved by\nYuan [J. Combin. Theory Ser. B 152 (2022) 379--398]. This generalizes several\nprevious results on $F^{p+1}$ for $F$ being a matching, or a star.\nAdditionally, we also give some other interesting results on $F^{p+1}$ for $F$\nbeing a path, a cycle, or a complete graph. To obtain the aforementioned\nspectral results, we utilize a combination of the spectral version of the\nStability Lemma and structural analyses. These approaches and tools give a new\nexploration of spectral extremal problems on non-bipartite graphs.",
            "author": [
                "Longfei Fang",
                "Huiqiu Lin"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05085v2",
                "http://arxiv.org/pdf/2310.05085v2"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05062v2",
            "title": "Local to Global: A Distributed Quantum Approximate Optimization\n  Algorithm for Pseudo-Boolean Optimization Problems",
            "updated": "2023-10-10T02:11:43Z",
            "published": "2023-10-08T08:07:11Z",
            "summary": "With the rapid advancement of quantum computing, Quantum Approximate\nOptimization Algorithm (QAOA) is considered as a promising candidate to\ndemonstrate quantum supremacy, which exponentially solves a class of Quadratic\nUnconstrained Binary Optimization (QUBO) problems. However, limited qubit\navailability and restricted coherence time challenge QAOA to solve large-scale\npseudo-Boolean problems on currently available Near-term Intermediate Scale\nQuantum (NISQ) devices. In this paper, we propose a distributed QAOA which can\nsolve a general pseudo-Boolean problem by converting it to a simplified Ising\nmodel. Different from existing distributed QAOAs' assuming that local solutions\nare part of a global one, which is not often the case, we introduce community\ndetection using Louvian algorithm to partition the graph where subgraphs are\nfurther compressed by community representation and merged into a higher level\nsubgraph. Recursively and backwards, local solutions of lower level subgraphs\nare updated by heuristics from solutions of higher level subgraphs. Compared\nwith existing methods, our algorithm incorporates global heuristics into local\nsolutions such that our algorithm is proven to achieve a higher approximation\nratio and outperforms across different graph configurations. Also, ablation\nstudies validate the effectiveness of each component in our method.",
            "author": [
                "Bo Yue",
                "Shibei Xue",
                "Yu Pan",
                "Min Jiang",
                "Daoyi Dong"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05062v2",
                "http://arxiv.org/pdf/2310.05062v2"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05041v1",
            "title": "An Anomaly Behavior Analysis Framework for Securing Autonomous Vehicle\n  Perception",
            "updated": "2023-10-08T06:51:05Z",
            "published": "2023-10-08T06:51:05Z",
            "summary": "As a rapidly growing cyber-physical platform, Autonomous Vehicles (AVs) are\nencountering more security challenges as their capabilities continue to expand.\nIn recent years, adversaries are actively targeting the perception sensors of\nautonomous vehicles with sophisticated attacks that are not easily detected by\nthe vehicles' control systems. This work proposes an Anomaly Behavior Analysis\napproach to detect a perception sensor attack against an autonomous vehicle.\nThe framework relies on temporal features extracted from a physics-based\nautonomous vehicle behavior model to capture the normal behavior of vehicular\nperception in autonomous driving. By employing a combination of model-based\ntechniques and machine learning algorithms, the proposed framework\ndistinguishes between normal and abnormal vehicular perception behavior. To\ndemonstrate the application of the framework in practice, we performed a depth\ncamera attack experiment on an autonomous vehicle testbed and generated an\nextensive dataset. We validated the effectiveness of the proposed framework\nusing this real-world data and released the dataset for public access. To our\nknowledge, this dataset is the first of its kind and will serve as a valuable\nresource for the research community in evaluating their intrusion detection\ntechniques effectively.",
            "author": [
                "Murad Mehrab Abrar",
                "Salim Hariri"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05041v1",
                "http://arxiv.org/pdf/2310.05041v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05035v2",
            "title": "Self-Convinced Prompting: Few-Shot Question Answering with Repeated\n  Introspection",
            "updated": "2023-10-10T15:03:35Z",
            "published": "2023-10-08T06:36:26Z",
            "summary": "While large language models (LLMs) such as ChatGPT and PaLM have demonstrated\nremarkable performance in various language understanding and generation tasks,\ntheir capabilities in complex reasoning and intricate knowledge utilization\nstill fall short of human-level proficiency. Recent studies have established\nthe effectiveness of prompts in steering LLMs towards generating desired\noutputs. Building on these insights, we introduce a novel framework that\nharnesses the potential of large-scale pre-trained language models, to\niteratively enhance performance of the LLMs. Our framework incorporates three\ncomponents: \\textit{Normal CoT}, a \\textit{Convincer}, and an\n\\textit{Answerer}. It processes the output of a typical few-shot\nchain-of-thought prompt, assesses the correctness of the response, scrutinizes\nthe answer, refines the reasoning, and ultimately produces a new solution.\nExperimental results on the 7 datasets of miscellaneous problems validate the\nefficacy of the Self-Convince framework, achieving substantial improvements\ncompared to the baselines. This study contributes to the burgeoning body of\nresearch focused on integrating pre-trained language models with tailored\nprompts and iterative refinement processes to augment their performance in\ncomplex tasks.",
            "author": [
                "Haodi Zhang",
                "Min Cai",
                "Xinhe Zhang",
                "Chen Jason Zhang",
                "Rui Mao",
                "Kaishun Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05035v2",
                "http://arxiv.org/pdf/2310.05035v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05034v1",
            "title": "Deep Reinforcement Learning Based Cross-Layer Design in Terahertz Mesh\n  Backhaul Networks",
            "updated": "2023-10-08T06:36:00Z",
            "published": "2023-10-08T06:36:00Z",
            "summary": "Supporting ultra-high data rates and flexible reconfigurability, Terahertz\n(THz) mesh networks are attractive for next-generation wireless backhaul\nsystems that empower the integrated access and backhaul (IAB). In THz mesh\nbackhaul networks, the efficient cross-layer routing and long-term resource\nallocation is yet an open problem due to dynamic traffic demands as well as\npossible link failures caused by the high directivity and high\nnon-line-of-sight (NLoS) path loss of THz spectrum. In addition, unpredictable\ndata traffic and the mixed integer programming property with the NP-hard nature\nfurther challenge the effective routing and long-term resource allocation\ndesign. In this paper, a deep reinforcement learning (DRL) based cross-layer\ndesign in THz mesh backhaul networks (DEFLECT) is proposed, by considering\ndynamic traffic demands and possible sudden link failures. In DEFLECT, a\nheuristic routing metric is first devised to facilitate resource efficiency\n(RE) enhancement regarding energy and sub-array usages. Furthermore, a DRL\nbased resource allocation algorithm is developed to realize long-term RE\nmaximization and fast recovery from broken links. Specifically in the DRL\nmethod, the exploited multi-task structure cooperatively benefits joint power\nand sub-array allocation. Additionally, the leveraged hierarchical architecture\nrealizes tailored resource allocation for each base station and learned\nknowledge transfer for fast recovery. Simulation results show that DEFLECT\nrouting consumes less resource, compared to the minimal hop-count metric.\nMoreover, unlike conventional DRL methods causing packet loss and second-level\nlatency, DEFLECT DRL realizes the long-term RE maximization with no packet loss\nand millisecond-level latency, and recovers resource-efficient backhaul from\nbroken links within 1s.",
            "author": [
                "Zhifeng Hu",
                "Chong Han",
                "Xudong Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05034v1",
                "http://arxiv.org/pdf/2310.05034v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05022v1",
            "title": "Fully Spiking Neural Network for Legged Robots",
            "updated": "2023-10-08T05:48:30Z",
            "published": "2023-10-08T05:48:30Z",
            "summary": "In recent years, legged robots based on deep reinforcement learning have made\nremarkable progress. Quadruped robots have demonstrated the ability to complete\nchallenging tasks in complex environments and have been deployed in real-world\nscenarios to assist humans. Simultaneously, bipedal and humanoid robots have\nachieved breakthroughs in various demanding tasks. Current reinforcement\nlearning methods can utilize diverse robot bodies and historical information to\nperform actions. However, prior research has not emphasized the speed and\nenergy consumption of network inference, as well as the biological significance\nof the neural networks themselves. Most of the networks employed are\ntraditional artificial neural networks that utilize multilayer perceptrons\n(MLP). In this paper, we successfully apply a novel Spiking Neural Network\n(SNN) to process legged robots, achieving outstanding results across a range of\nsimulated terrains. SNN holds a natural advantage over traditional neural\nnetworks in terms of inference speed and energy consumption, and their\npulse-form processing of body perception signals offers improved biological\ninterpretability. To the best of our knowledge, this is the first work to\nimplement SNN in legged robots.",
            "author": [
                "Xiaoyang Jiang",
                "Qiang Zhang",
                "Jingkai Sun",
                "Renjing Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05022v1",
                "http://arxiv.org/pdf/2310.05022v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05007v1",
            "title": "MinPrompt: Graph-based Minimal Prompt Data Augmentation for Few-shot\n  Question Answering",
            "updated": "2023-10-08T04:44:36Z",
            "published": "2023-10-08T04:44:36Z",
            "summary": "Few-shot question answering (QA) aims at achieving satisfactory results on\nmachine question answering when only a few training samples are available.\nRecent advances mostly rely on the power of pre-trained large language models\n(LLMs) and fine-tuning in specific settings. Although the pre-training stage\nhas already equipped LLMs with powerful reasoning capabilities, LLMs still need\nto be fine-tuned to adapt to specific domains to achieve the best results. In\nthis paper, we propose to select the most informative data for fine-tuning,\nthereby improving the efficiency of the fine-tuning process with comparative or\neven better accuracy on the open-domain QA task. We present MinPrompt, a\nminimal data augmentation framework for open-domain QA based on an approximate\ngraph algorithm and unsupervised question generation. We transform the raw text\ninto a graph structure to build connections between different factual\nsentences, then apply graph algorithms to identify the minimal set of sentences\nneeded to cover the most information in the raw text. We then generate QA pairs\nbased on the identified sentence subset and train the model on the selected\nsentences to obtain the final model. Empirical results on several benchmark\ndatasets and theoretical analysis show that MinPrompt is able to achieve\ncomparable or better results than baselines with a high degree of efficiency,\nbringing improvements in F-1 scores by up to 27.5%.",
            "author": [
                "Xiusi Chen",
                "Jyun-Yu Jiang",
                "Wei-Cheng Chang",
                "Cho-Jui Hsieh",
                "Hsiang-Fu Yu",
                "Wei Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05007v1",
                "http://arxiv.org/pdf/2310.05007v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05005v1",
            "title": "Rigidity of Balanced Minimal Cycle Complexes",
            "updated": "2023-10-08T04:42:34Z",
            "published": "2023-10-08T04:42:34Z",
            "summary": "A $(d-1)$-dimensional simplicial complex $\\Delta$ is balanced if its graph\n$G(\\Delta)$ is $d$-colorable. Klee and Novik obtained the balanced lower bound\ntheorem for balanced normal $(d-1)$-pseudomanifolds $\\Delta$ with $d\\geq3$ by\nshowing that the subgraph of $G(\\Delta)$ induced by the vertices colored in $T$\nis rigid in $\\mathbb{R}^3$ for any $3$ colors $T$. We show that the same\nrigidity result, and thus the balanced lower bound theorem, holds for balanced\nminimal $(d-1)$-cycle complexes with $d \\geq 3$. Motivated by the Stanley's\nwork on a colored system of parameters for the Stanley-Reisner ring of balanced\nsimplicial complexes, we further investigate the infinitesimal rigidity of\nnon-generic realization of balanced, and more broadly $\\bm{a}$-balanced,\nsimplicial complexes. Among other results, we show that for $d \\geq 4$, a\nbalanced homology $(d-1)$-manifold can be realized as an infinitesimally rigid\nframework in $\\mathbb{R}^d$ such that each vertex of color $i$ lies on the\n$i$th coordinate axis.",
            "author": [
                "Ryoshun Oba"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05005v1",
                "http://arxiv.org/pdf/2310.05005v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05002v1",
            "title": "Self-Knowledge Guided Retrieval Augmentation for Large Language Models",
            "updated": "2023-10-08T04:22:33Z",
            "published": "2023-10-08T04:22:33Z",
            "summary": "Large language models (LLMs) have shown superior performance without\ntask-specific fine-tuning. Despite the success, the knowledge stored in the\nparameters of LLMs could still be incomplete and difficult to update due to the\ncomputational costs. As complementary, retrieval-based methods can offer\nnon-parametric world knowledge and improve the performance on tasks such as\nquestion answering. However, we find that the retrieved knowledge does not\nalways help and even has a negative impact on original responses occasionally.\nTo better make use of both internal knowledge and external world knowledge, we\ninvestigate eliciting the model's ability to recognize what they know and do\nnot know (which is also called self-knowledge) and propose Self-Knowledge\nguided Retrieval augmentation (SKR), a simple yet effective method which can\nlet LLMs refer to the questions they have previously encountered and adaptively\ncall for external resources when dealing with new questions. We evaluate SKR on\nmultiple datasets and demonstrate that it outperforms chain-of-thought based\nand fully retrieval-based methods by using either InstructGPT or ChatGPT.",
            "author": [
                "Yile Wang",
                "Peng Li",
                "Maosong Sun",
                "Yang Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05002v1",
                "http://arxiv.org/pdf/2310.05002v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04999v2",
            "title": "Symmetrical Linguistic Feature Distillation with CLIP for Scene Text\n  Recognition",
            "updated": "2023-10-10T03:32:58Z",
            "published": "2023-10-08T04:00:20Z",
            "summary": "In this paper, we explore the potential of the Contrastive Language-Image\nPretraining (CLIP) model in scene text recognition (STR), and establish a novel\nSymmetrical Linguistic Feature Distillation framework (named CLIP-OCR) to\nleverage both visual and linguistic knowledge in CLIP. Different from previous\nCLIP-based methods mainly considering feature generalization on visual\nencoding, we propose a symmetrical distillation strategy (SDS) that further\ncaptures the linguistic knowledge in the CLIP text encoder. By cascading the\nCLIP image encoder with the reversed CLIP text encoder, a symmetrical structure\nis built with an image-to-text feature flow that covers not only visual but\nalso linguistic information for distillation.Benefiting from the natural\nalignment in CLIP, such guidance flow provides a progressive optimization\nobjective from vision to language, which can supervise the STR feature\nforwarding process layer-by-layer.Besides, a new Linguistic Consistency Loss\n(LCL) is proposed to enhance the linguistic capability by considering\nsecond-order statistics during the optimization. Overall, CLIP-OCR is the first\nto design a smooth transition between image and text for the STR task.Extensive\nexperiments demonstrate the effectiveness of CLIP-OCR with 93.8% average\naccuracy on six popular STR benchmarks.Code will be available at\nhttps://github.com/wzx99/CLIPOCR.",
            "author": [
                "Zixiao Wang",
                "Hongtao Xie",
                "Yuxin Wang",
                "Jianjun Xu",
                "Boqiang Zhang",
                "Yongdong Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04999v2",
                "http://arxiv.org/pdf/2310.04999v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04994v1",
            "title": "Distantly-Supervised Joint Entity and Relation Extraction with\n  Noise-Robust Learning",
            "updated": "2023-10-08T03:42:15Z",
            "published": "2023-10-08T03:42:15Z",
            "summary": "Joint entity and relation extraction is a process that identifies entity\npairs and their relations using a single model. We focus on the problem of\ntraining these models on distantly-labeled data, which is generated by aligning\nentity mentions in a text corpus with their corresponding entity and relation\ntypes in a knowledge base. One key challenge here is the presence of noisy\nlabels, which arises from both entity and relation annotations, and\nsignificantly impair the effectiveness of supervised learning applications.\nHowever, existing research primarily addresses only one type of noise, thereby\nlimiting the effectiveness of noise reduction. To fill this gap, we introduce a\nnew noise-robust approach, that 1)~incorporates a pre-trained GPT-2 into a\nsequence tagging scheme for simultaneous entity and relation detection, and\n2)~employs a noise-robust learning framework which includes a new loss function\nthat penalizes inconsistency with both significant relation patterns and\nentity-relation dependencies, as well as a self-adaptive learning step that\niteratively selects and trains on high-quality instances. Experiments on two\ndatasets show that our method outperforms the existing state-of-the-art methods\nin both joint extraction performance and noise reduction effect.",
            "author": [
                "Yufei Li",
                "Xiao Yu",
                "Yanghong Guo",
                "Yanchi Liu",
                "Haifeng Chen",
                "Cong Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04994v1",
                "http://arxiv.org/pdf/2310.04994v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04987v1",
            "title": "Data-centric Graph Learning: A Survey",
            "updated": "2023-10-08T03:17:22Z",
            "published": "2023-10-08T03:17:22Z",
            "summary": "The history of artificial intelligence (AI) has witnessed the significant\nimpact of high-quality data on various deep learning models, such as ImageNet\nfor AlexNet and ResNet. Recently, instead of designing more complex neural\narchitectures as model-centric approaches, the attention of AI community has\nshifted to data-centric ones, which focuses on better processing data to\nstrengthen the ability of neural models. Graph learning, which operates on\nubiquitous topological data, also plays an important role in the era of deep\nlearning. In this survey, we comprehensively review graph learning approaches\nfrom the data-centric perspective, and aim to answer two crucial questions: (1)\nwhen to modify graph data and (2) how to modify graph data to unlock the\npotential of various graph models. Accordingly, we propose a novel taxonomy\nbased on the stages in the graph learning pipeline, and highlight the\nprocessing methods for different data structures in the graph data, i.e.,\ntopology, feature and label. Furthermore, we analyze some potential problems\nembedded in graph data and discuss how to solve them in a data-centric manner.\nFinally, we provide some promising future directions for data-centric graph\nlearning.",
            "author": [
                "Cheng Yang",
                "Deyu Bo",
                "Jixi Liu",
                "Yufei Peng",
                "Boyu Chen",
                "Haoran Dai",
                "Ao Sun",
                "Yue Yu",
                "Yixin Xiao",
                "Qi Zhang",
                "Chunchen Wang",
                "Yuxin Guo",
                "Chuan Shi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04987v1",
                "http://arxiv.org/pdf/2310.04987v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04981v1",
            "title": "Compositional Semantics for Open Vocabulary Spatio-semantic\n  Representations",
            "updated": "2023-10-08T03:07:14Z",
            "published": "2023-10-08T03:07:14Z",
            "summary": "General-purpose mobile robots need to complete tasks without exact human\ninstructions. Large language models (LLMs) is a promising direction for\nrealizing commonsense world knowledge and reasoning-based planning.\nVision-language models (VLMs) transform environment percepts into\nvision-language semantics interpretable by LLMs. However, completing complex\ntasks often requires reasoning about information beyond what is currently\nperceived. We propose latent compositional semantic embeddings z* as a\nprincipled learning-based knowledge representation for queryable\nspatio-semantic memories. We mathematically prove that z* can always be found,\nand the optimal z* is the centroid for any set Z. We derive a probabilistic\nbound for estimating separability of related and unrelated semantics. We prove\nthat z* is discoverable by iterative optimization by gradient descent from\nvisual appearance and singular descriptions. We experimentally verify our\nfindings on four embedding spaces incl. CLIP and SBERT. Our results show that\nz* can represent up to 10 semantics encoded by SBERT, and up to 100 semantics\nfor ideal uniformly distributed high-dimensional embeddings. We demonstrate\nthat a simple dense VLM trained on the COCO-Stuff dataset can learn z* for 181\noverlapping semantics by 42.23 mIoU, while improving conventional\nnon-overlapping open-vocabulary segmentation performance by +3.48 mIoU compared\nwith a popular SOTA model.",
            "author": [
                "Robin Karlsson",
                "Francisco Lepe-Salazar",
                "Kazuya Takeda"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04981v1",
                "http://arxiv.org/pdf/2310.04981v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG",
                "I.2.10; I.2.9"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04965v1",
            "title": "MULTISCRIPT: Multimodal Script Learning for Supporting Open Domain\n  Everyday Tasks",
            "updated": "2023-10-08T01:51:17Z",
            "published": "2023-10-08T01:51:17Z",
            "summary": "Automatically generating scripts (i.e. sequences of key steps described in\ntext) from video demonstrations and reasoning about the subsequent steps are\ncrucial to the modern AI virtual assistants to guide humans to complete\neveryday tasks, especially unfamiliar ones. However, current methods for\ngenerative script learning rely heavily on well-structured preceding steps\ndescribed in text and/or images or are limited to a certain domain, resulting\nin a disparity with real-world user scenarios. To address these limitations, we\npresent a new benchmark challenge -- MultiScript, with two new tasks on\ntask-oriented multimodal script learning: (1) multimodal script generation, and\n(2) subsequent step prediction. For both tasks, the input consists of a target\ntask name and a video illustrating what has been done to complete the target\ntask, and the expected output is (1) a sequence of structured step descriptions\nin text based on the demonstration video, and (2) a single text description for\nthe subsequent step, respectively. Built from WikiHow, MultiScript covers\nmultimodal scripts in videos and text descriptions for over 6,655 human\neveryday tasks across 19 diverse domains. To establish baseline performance on\nMultiScript, we propose two knowledge-guided multimodal generative frameworks\nthat incorporate the task-related knowledge prompted from large language models\nsuch as Vicuna. Experimental results show that our proposed approaches\nsignificantly improve over the competitive baselines.",
            "author": [
                "Jingyuan Qi",
                "Minqian Liu",
                "Ying Shen",
                "Zhiyang Xu",
                "Lifu Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04965v1",
                "http://arxiv.org/pdf/2310.04965v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04962v1",
            "title": "Properly colored even cycles in edge-colored complete balanced bipartite\n  graphs",
            "updated": "2023-10-08T01:43:12Z",
            "published": "2023-10-08T01:43:12Z",
            "summary": "Consider a complete balanced bipartite graph $K_{n,n}$ and let $K^c_{n,n}$ be\nan edge-colored version of $K_{n,n}$ that is obtained from $K_{n,n}$ by having\neach edge assigned a certain color. A subgraph $H$ of $K^c_{n,n}$ is called\nproperly colored (PC) if every two adjacent edges of $H$ have distinct colors.\n$K_{n,n}^c$ is called properly vertex-even-pancyclic if for every vertex $u\\in\nV(K_{n,n}^c)$ and for every even integer $k$ with $4 \\leq k \\leq 2n$, there\nexists a PC $k$-cycle containing $u$. The minimum color degree\n$\\delta^c(K^c_{n,n})$ of $K^c_{n,n}$ is the largest integer $k$ such that for\nevery vertex $v$, there are at least $k$ distinct colors on the edges incident\nto $v$. In this paper we study the existence of PC even cycles in $K_{n,n}^c$.\nWe first show that, for every integer $t\\geq 3$, every $K^c_{n,n}$ with\n$\\delta^c(K^c_{n,n})\\geq \\frac{2n}{3}+t$ contains a PC 2-factor $H$ such that\nevery cycle of $H$ has a length of at least $t$. By using the probabilistic\nmethod and absorbing technique, we use the above result to further show that,\nfor every $\\varepsilon>0$, there exists an integer $n_0(\\varepsilon)$ such that\nevery $K^c_{n,n}$ with $n\\geq n_0(\\varepsilon)$ is properly\nvertex-even-pancyclic, provided that $\\delta^c(K^c_{n,n})\\geq\n(\\frac{2}{3}+\\varepsilon)n$.",
            "author": [
                "Shanshan Guo",
                "Fei Huang",
                "Jinjiang Yuan",
                "C. T. Ng",
                "T. C. E. Cheng"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04962v1",
                "http://arxiv.org/pdf/2310.04962v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04958v1",
            "title": "Direct Observations of X-Rays Produced by Upward Positive Lightning",
            "updated": "2023-10-08T00:48:54Z",
            "published": "2023-10-08T00:48:54Z",
            "summary": "X-rays have been observed in natural downward cloud-to-ground lightning for\nover twenty years and in rocket-triggered lightning for slightly less. In both\ncases, this energetic radiation has been detected during the stepped and dart\nleader phases of downward negative flashes. More recently, X-rays have also\nbeen reported during the dart leader phase of upward negative flashes. In this\nstudy, we present the observations of four upward positive lightning flashes\nfrom the S\\\"antis Tower (2.5 km ASL) in Switzerland. These consist of the\nsimultaneous records of electric current passing through the tower, and\nelectric field strength and X-ray flux 20 meters from the tower base. One of\nthe flashes was captured by a high-speed camera operating at 24,000 frames per\nsecond, stills from which are also presented. We detected X-rays during the\ninitial phase of upward negative leader propagation, which can be associated\nwith the leader-stepping process from electric field and current waveforms. To\nthe best of our knowledge, this is the first time that such measurements are\nreported in the literature. The obtained time-synchronised data confirm that\nthe X-ray emissions detected are associated with the initial steps of the\nupward negative leader. The frequency and energy of X-ray pulses appear to\ndecrease as a function of time, with pulses disappearing altogether within the\nfirst millisecond of the leader initiation. The X-ray pulse energy appears to\nincrease with the maximum current-derivative and the electric field change of\nits associated leader step. These observations contribute to improving the\ncurrently lackluster understanding of upward lightning, which is a primary\nsource of damage to tall structures such as telecommunications towers and wind\nturbines, as well as airplanes during take-off and landing.",
            "author": [
                "Toma Oregel-Chaumont",
                "Antonio Sunjerga",
                "Pasan Hettiarachchi",
                "Vernon Cooray",
                "Marcos Rubinstein",
                "Farhad Rachidi"
            ],
            "link": [
                "http://dx.doi.org/10.21203/rs.3.rs-3419971/v1",
                "http://arxiv.org/abs/2310.04958v1",
                "http://arxiv.org/pdf/2310.04958v1"
            ],
            "primary_category": "physics.geo-ph",
            "category": [
                "physics.geo-ph",
                "physics.ao-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04956v1",
            "title": "Towards Explainable Machine Learning: The Effectiveness of Reservoir\n  Computing in Wireless Receive Processing",
            "updated": "2023-10-08T00:44:35Z",
            "published": "2023-10-08T00:44:35Z",
            "summary": "Deep learning has seen a rapid adoption in a variety of wireless\ncommunications applications, including at the physical layer. While it has\ndelivered impressive performance in tasks such as channel equalization and\nreceive processing/symbol detection, it leaves much to be desired when it comes\nto explaining this superior performance. In this work, we investigate the\nspecific task of channel equalization by applying a popular learning-based\ntechnique known as Reservoir Computing (RC), which has shown superior\nperformance compared to conventional methods and other learning-based\napproaches. Specifically, we apply the echo state network (ESN) as a channel\nequalizer and provide a first principles-based signal processing understanding\nof its operation. With this groundwork, we incorporate the available domain\nknowledge in the form of the statistics of the wireless channel directly into\nthe weights of the ESN model. This paves the way for optimized initialization\nof the ESN model weights, which are traditionally untrained and randomly\ninitialized. Finally, we show the improvement in receive processing/symbol\ndetection performance with this optimized initialization through simulations.\nThis is a first step towards explainable machine learning (XML) and assigning\npractical model interpretability that can be utilized together with the\navailable domain knowledge to improve performance and enhance detection\nreliability.",
            "author": [
                "Shashank Jere",
                "Karim Said",
                "Lizhong Zheng",
                "Lingjia Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04956v1",
                "http://arxiv.org/pdf/2310.04956v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04949v1",
            "title": "Domain Knowledge Graph Construction Via A Simple Checker",
            "updated": "2023-10-08T00:09:31Z",
            "published": "2023-10-08T00:09:31Z",
            "summary": "With the availability of large language models, there is a growing interest\nfor semiconductor chip design companies to leverage the technologies. For those\ncompanies, deployment of a new methodology must include two important\nconsiderations: confidentiality and scalability. In this context, this work\ntackles the problem of knowledge graph construction from hardware-design domain\ntexts. We propose an oracle-checker scheme to leverage the power of GPT3.5 and\ndemonstrate that the essence of the problem is in distillation of domain\nexpert's background knowledge. Using RISC-V unprivileged ISA specification as\nan example, we explain key ideas and discuss practicality of our proposed\noracle-checker approach.",
            "author": [
                "Yueling Zeng",
                "Li-C. Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04949v1",
                "http://arxiv.org/pdf/2310.04949v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04946v1",
            "title": "Transferable Deep Clustering Model",
            "updated": "2023-10-07T23:35:17Z",
            "published": "2023-10-07T23:35:17Z",
            "summary": "Deep learning has shown remarkable success in the field of clustering\nrecently. However, how to transfer a trained clustering model on a source\ndomain to a target domain by leveraging the acquired knowledge to guide the\nclustering process remains challenging. Existing deep clustering methods often\nlack generalizability to new domains because they typically learn a group of\nfixed cluster centroids, which may not be optimal for the new domain\ndistributions. In this paper, we propose a novel transferable deep clustering\nmodel that can automatically adapt the cluster centroids according to the\ndistribution of data samples. Rather than learning a fixed set of centroids,\nour approach introduces a novel attention-based module that can adapt the\ncentroids by measuring their relationship with samples. In addition, we\ntheoretically show that our model is strictly more powerful than some classical\nclustering algorithms such as k-means or Gaussian Mixture Model (GMM).\nExperimental results on both synthetic and real-world datasets demonstrate the\neffectiveness and efficiency of our proposed transfer learning framework, which\nsignificantly improves the performance on target domain and reduces the\ncomputational cost.",
            "author": [
                "Zheng Zhang",
                "Liang Zhao"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04946v1",
                "http://arxiv.org/pdf/2310.04946v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04944v1",
            "title": "Beyond Text: A Deep Dive into Large Language Models' Ability on\n  Understanding Graph Data",
            "updated": "2023-10-07T23:25:22Z",
            "published": "2023-10-07T23:25:22Z",
            "summary": "Large language models (LLMs) have achieved impressive performance on many\nnatural language processing tasks. However, their capabilities on\ngraph-structured data remain relatively unexplored. In this paper, we conduct a\nseries of experiments benchmarking leading LLMs on diverse graph prediction\ntasks spanning node, edge, and graph levels. We aim to assess whether LLMs can\neffectively process graph data and leverage topological structures to enhance\nperformance, compared to specialized graph neural networks. Through varied\nprompt formatting and task/dataset selection, we analyze how well LLMs can\ninterpret and utilize graph structures. By comparing LLMs' performance with\nspecialized graph models, we offer insights into the strengths and limitations\nof employing LLMs for graph analytics. Our findings provide insights into LLMs'\ncapabilities and suggest avenues for further exploration in applying them to\ngraph analytics.",
            "author": [
                "Yuntong Hu",
                "Zheng Zhang",
                "Liang Zhao"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04944v1",
                "http://arxiv.org/pdf/2310.04944v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04934v1",
            "title": "UBSea: A Unified Community Detection Framework",
            "updated": "2023-10-07T22:33:55Z",
            "published": "2023-10-07T22:33:55Z",
            "summary": "Detecting communities in networks and graphs is an important task across many\ndisciplines such as statistics, social science and engineering. There are\ngenerally three different kinds of mixing patterns for the case of two\ncommunities: assortative mixing, disassortative mixing and core-periphery\nstructure. Modularity optimization is a classical way for fitting network\nmodels with communities. However, it can only deal with assortative mixing and\ndisassortative mixing when the mixing pattern is known and fails to discover\nthe core-periphery structure. In this paper, we extend modularity in a\nstrategic way and propose a new framework based on Unified Bigroups Standadized\nEdge-count Analysis (UBSea). It can address all the formerly mentioned\ncommunity mixing structures. In addition, this new framework is able to\nautomatically choose the mixing type to fit the networks. Simulation studies\nshow that the new framework has superb performance in a wide range of settings\nunder the stochastic block model and the degree-corrected stochastic block\nmodel. We show that the new approach produces consistent estimate of the\ncommunities under a suitable signal-to-noise-ratio condition, for the case of a\nblock model with two communities, for both undirected and directed networks.\nThe new method is illustrated through applications to several real-world\ndatasets.",
            "author": [
                "Xiancheng Lin",
                "Hao Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04934v1",
                "http://arxiv.org/pdf/2310.04934v1"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME",
                "math.ST",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04933v1",
            "title": "Algorithms for the Ridesharing with Profit Constraint Problem",
            "updated": "2023-10-07T22:31:19Z",
            "published": "2023-10-07T22:31:19Z",
            "summary": "Mobility-on-demand (MoD) ridesharing is a promising way to improve the\noccupancy rate of personal vehicles and reduce traffic congestion and\nemissions. Maximizing the number of passengers served and maximizing a profit\ntarget are major optimization goals in MoD ridesharing. We study the\nridesharing with profit constraint problem (labeled as RPC) which considers\nboth optimization goals altogether: maximize the total number of passengers\nsubject to an overall drivers' profit target. We give a mathematical\nformulation for the RPC problem. We present a polynomial-time exact algorithm\nframework (including two practical implementations of the algorithm) and a\n(1/2)-approximation algorithm for the case that each vehicle serves at most one\npassenger. We propose a (2/3*lambda)-approximation algorithm for the case that\neach vehicle serves at most lambda >= 2 passengers. Our algorithms revolve\naround the idea of maximum cardinality matching in bipartite graphs and\nhypergraphs (set packing) with general edge weight. Based on a real-world\nridesharing dataset in Chicago City and price schemes of Uber, we conduct an\nextensive empirical study on our model and algorithms. Experimental results\nshow that practical price schemes can be incorporated into our model, our exact\nalgorithms are efficient, and our approximation algorithms achieve about 90% of\noptimal solutions, in the number of passengers served.",
            "author": [
                "Qian-Ping Gu",
                "Jiajian Leo Liang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04933v1",
                "http://arxiv.org/pdf/2310.04933v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "F.2; I.6"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04928v2",
            "title": "Large Language Models Only Pass Primary School Exams in Indonesia: A\n  Comprehensive Test on IndoMMLU",
            "updated": "2023-10-21T17:13:05Z",
            "published": "2023-10-07T21:49:38Z",
            "summary": "Although large language models (LLMs) are often pre-trained on large-scale\nmultilingual texts, their reasoning abilities and real-world knowledge are\nmainly evaluated based on English datasets. Assessing LLM capabilities beyond\nEnglish is increasingly vital but hindered due to the lack of suitable\ndatasets. In this work, we introduce IndoMMLU, the first multi-task language\nunderstanding benchmark for Indonesian culture and languages, which consists of\nquestions from primary school to university entrance exams in Indonesia. By\nemploying professional teachers, we obtain 14,981 questions across 64 tasks and\neducation levels, with 46% of the questions focusing on assessing proficiency\nin the Indonesian language and knowledge of nine local languages and cultures\nin Indonesia. Our empirical evaluations show that GPT-3.5 only manages to pass\nthe Indonesian primary school level, with limited knowledge of local Indonesian\nlanguages and culture. Other smaller models such as BLOOMZ and Falcon perform\nat even lower levels.",
            "author": [
                "Fajri Koto",
                "Nurul Aisyah",
                "Haonan Li",
                "Timothy Baldwin"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04928v2",
                "http://arxiv.org/pdf/2310.04928v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04921v2",
            "title": "Crystal: Introspective Reasoners Reinforced with Self-Feedback",
            "updated": "2023-10-18T14:52:03Z",
            "published": "2023-10-07T21:23:58Z",
            "summary": "Extensive work has shown that the performance and interpretability of\ncommonsense reasoning can be improved via knowledge-augmented reasoning\nmethods, where the knowledge that underpins the reasoning process is explicitly\nverbalized and utilized. However, existing implementations, including\n\"chain-of-thought\" and its variants, fall short in capturing the introspective\nnature of knowledge required in commonsense reasoning, and in accounting for\nthe mutual adaptation between the generation and utilization of knowledge. We\npropose a novel method to develop an introspective commonsense reasoner,\nCrystal. To tackle commonsense problems, it first introspects for knowledge\nstatements related to the given question, and subsequently makes an informed\nprediction that is grounded in the previously introspected knowledge. The\nknowledge introspection and knowledge-grounded reasoning modes of the model are\ntuned via reinforcement learning to mutually adapt, where the reward derives\nfrom the feedback given by the model itself. Experiments show that Crystal\nsignificantly outperforms both the standard supervised finetuning and\nchain-of-thought distilled methods, and enhances the transparency of the\ncommonsense reasoning process. Our work ultimately validates the feasibility\nand potential of reinforcing a neural model with self-feedback.",
            "author": [
                "Jiacheng Liu",
                "Ramakanth Pasunuru",
                "Hannaneh Hajishirzi",
                "Yejin Choi",
                "Asli Celikyilmaz"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04921v2",
                "http://arxiv.org/pdf/2310.04921v2"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04910v1",
            "title": "Faithful Knowledge Graph Explanations for Commonsense Reasoning",
            "updated": "2023-10-07T20:29:45Z",
            "published": "2023-10-07T20:29:45Z",
            "summary": "While fusing language models (LMs) and knowledge graphs (KGs) has become\ncommon in commonsense question answering research, enabling faithful\nchain-of-thought explanations in these models remains an open problem. One\nmajor weakness of current KG-based explanation techniques is that they overlook\nthe faithfulness of generated explanations during evaluation. To address this\ngap, we make two main contributions: (1) We propose and validate two\nquantitative metrics - graph consistency and graph fidelity - to measure the\nfaithfulness of KG-based explanations. (2) We introduce Consistent GNN (CGNN),\na novel training method that adds a consistency regularization term to improve\nexplanation faithfulness. Our analysis shows that predictions from KG often\ndiverge from original model predictions. The proposed CGNN approach boosts\nconsistency and fidelity, demonstrating its potential for producing more\nfaithful explanations. Our work emphasises the importance of explicitly\nevaluating suggest a path forward for developing architectures for faithful\ngraph-based explanations.",
            "author": [
                "Weihe Zhai",
                "Arkaitz Zubiaga",
                "Bingquan Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04910v1",
                "http://arxiv.org/pdf/2310.04910v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04905v1",
            "title": "Spacelike minimal surfaces in R^4_1 through of a $\u03b8$-Family",
            "updated": "2023-10-07T19:50:48Z",
            "published": "2023-10-07T19:50:48Z",
            "summary": "In this paper we introduce a $\\theta$-family of spacelike surfaces in the\nLorentz-Minkowski space R^4_1 based in two complex valued functions $a(w),\n\\mu(w)$, which when they are holomorphic we will be dealing with a family of\nspacelike minimal surfaces. The $\\theta$-family is such that it connects\nspacelike minimal surfaces in R^3_1 to spacelike minimal surfaces in R^3. We\nstudy the family through of the curvature and we prove that the family\npreserves planar points and moreover, that the existence of planar points\ncorresponds to the existence of solutions of equation |a_w(w)|^2 =0. We also\nshow that if a pair of surfaces are associated through of a $\\theta$-family\nthen they can not be complete surfaces. As applications we focus to one type of\ngraph surfaces in R^4_1 and we prove that if the imaginary part of $a(w)$ is\nzero at least in a point then the surface cannot assume local representations\nof that type of graph. Several explicit examples are given.",
            "author": [
                "M. P. Dussan",
                "A. P. Franco Filho",
                "R. S. Santos"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04905v1",
                "http://arxiv.org/pdf/2310.04905v1"
            ],
            "primary_category": "math.DG",
            "category": [
                "math.DG",
                "53C42, 53C50, 53B30"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04893v1",
            "title": "Generalized Densest Subgraph in Multiplex Networks",
            "updated": "2023-10-07T18:32:38Z",
            "published": "2023-10-07T18:32:38Z",
            "summary": "Finding dense subgraphs of a large network is a fundamental problem in graph\nmining that has been studied extensively both for its theoretical richness and\nits many practical applications over the last five decades. However, most\nexisting studies have focused on graphs with a single type of connection. In\napplications such as biological, social, and transportation networks,\ninteractions between objects span multiple aspects, yielding multiplex graphs.\nExisting dense subgraph mining methods in multiplex graphs consider the same\nimportance for different types of connections, while in real-world\napplications, one relation type can be noisy, insignificant, or irrelevant.\nMoreover, they are limited to the edge-density measure, unable to change the\nemphasis on larger/smaller degrees depending on the application. To this end,\nwe define a new family of dense subgraph objectives, parametrized by two\nvariables $p$ and $\\beta$, that can (1) consider different importance weights\nfor each relation type, and (2) change the emphasis on the larger/smaller\ndegrees, depending on the application. Due to the NP-hardness of this problem,\nwe first extend the FirmCore, $k$-core counterpart in multiplex graphs, to\nlayer-weighted multiplex graphs, and based on it, we propose two\npolynomial-time approximation algorithms for the generalized densest subgraph\nproblem, when $p \\geq 1$ and the general case. Our experimental results show\nthe importance of considering different weights for different relation types\nand the effectiveness and efficiency of our algorithms.",
            "author": [
                "Ali Behrouz",
                "Farnoosh Hashemi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04893v1",
                "http://arxiv.org/pdf/2310.04893v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI",
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04889v1",
            "title": "GradXKG: A Universal Explain-per-use Temporal Knowledge Graph Explainer",
            "updated": "2023-10-07T18:21:35Z",
            "published": "2023-10-07T18:21:35Z",
            "summary": "Temporal knowledge graphs (TKGs) have shown promise for reasoning tasks by\nincorporating a temporal dimension to represent how facts evolve over time.\nHowever, existing TKG reasoning (TKGR) models lack explainability due to their\nblack-box nature. Recent work has attempted to address this through customized\nmodel architectures that generate reasoning paths, but these recent approaches\nhave limited generalizability and provide sparse explanatory output. To enable\ninterpretability for most TKGR models, we propose GradXKG, a novel two-stage\ngradient-based approach for explaining Relational Graph Convolution Network\n(RGCN)-based TKGR models. First, a Grad-CAM-inspired RGCN explainer tracks\ngradients to quantify each node's contribution across timesteps in an efficient\n\"explain-per-use\" fashion. Second, an integrated gradients explainer\nconsolidates importance scores for RGCN outputs, extending compatibility across\ndiverse TKGR architectures based on RGCN. Together, the two explainers\nhighlight the most critical nodes at each timestep for a given prediction. Our\nextensive experiments demonstrated that, by leveraging gradient information,\nGradXKG provides insightful explanations grounded in the model's logic in a\ntimely manner for most RGCN-based TKGR models. This helps address the lack of\ninterpretability in existing TKGR models and provides a universal explanation\napproach applicable across various models.",
            "author": [
                "Chenhan Yuan",
                "Hoda Eldardiry"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04889v1",
                "http://arxiv.org/pdf/2310.04889v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04886v1",
            "title": "A Closed-form Solution for the Strapdown Inertial Navigation Initial\n  Value Problem",
            "updated": "2023-10-07T18:01:47Z",
            "published": "2023-10-07T18:01:47Z",
            "summary": "Strapdown inertial navigation systems (SINS) are ubiquitious in robotics and\nengineering since they can estimate a rigid body pose using onboard kinematic\nmeasurements without knowledge of the dynamics of the vehicle to which they are\nattached. While recent work has focused on the closed-form evolution of the\nestimation error for SINS, which is critical for Kalman filtering, the\npropagation of the kinematics has received less attention. Runge-Kutta\nintegration approaches have been widely used to solve the initial value\nproblem; however, we show that leveraging the special structure of the SINS\nproblem and viewing it as a mixed-invariant vector field on a Lie group, yields\na closed form solution. Our closed form solution is exact given fixed gyroscope\nand accelerometer measurements over a sampling period, and it is utilizes 12\ntimes less floating point operations compared to a single integration step of a\n4th order Runge-Kutta integrator. We believe the wide applicability of this\nwork and the efficiency and accuracy gains warrant general adoption of this\nalgorithm for SINS.",
            "author": [
                "James Goppert",
                "Li-Yu Lin",
                "Kartik Pant",
                "Benjamin Perseghetti"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04886v1",
                "http://arxiv.org/pdf/2310.04886v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04878v1",
            "title": "Hybrid Recommendation System using Graph Neural Network and BERT\n  Embeddings",
            "updated": "2023-10-07T17:24:41Z",
            "published": "2023-10-07T17:24:41Z",
            "summary": "Recommender systems have emerged as a crucial component of the modern web\necosystem. The effectiveness and accuracy of such systems are critical for\nproviding users with personalized recommendations that meet their specific\ninterests and needs. In this paper, we introduce a novel model that utilizes a\nGraph Neural Network (GNN) in conjunction with sentence transformer embeddings\nto predict anime recommendations for different users. Our model employs the\ntask of link prediction to create a recommendation system that considers both\nthe features of anime and user interactions with different anime. The\nhybridization of the GNN and transformer embeddings enables us to capture both\ninter-level and intra-level features of anime data.Our model not only\nrecommends anime to users but also predicts the rating a specific user would\ngive to an anime. We utilize the GraphSAGE network for model building and\nweighted root mean square error (RMSE) to evaluate the performance of the\nmodel. Our approach has the potential to significantly enhance the accuracy and\neffectiveness of anime recommendation systems and can be extended to other\ndomains that require personalized recommendations.",
            "author": [
                "Shashidhar Reddy Javaji",
                "Krutika Sarode"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04878v1",
                "http://arxiv.org/pdf/2310.04878v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04865v1",
            "title": "ForeSeer: Product Aspect Forecasting Using Temporal Graph Embedding",
            "updated": "2023-10-07T16:21:04Z",
            "published": "2023-10-07T16:21:04Z",
            "summary": "Developing text mining approaches to mine aspects from customer reviews has\nbeen well-studied due to its importance in understanding customer needs and\nproduct attributes. In contrast, it remains unclear how to predict the future\nemerging aspects of a new product that currently has little review information.\nThis task, which we named product aspect forecasting, is critical for\nrecommending new products, but also challenging because of the missing reviews.\nHere, we propose ForeSeer, a novel textual mining and product embedding\napproach progressively trained on temporal product graphs for this novel\nproduct aspect forecasting task. ForeSeer transfers reviews from similar\nproducts on a large product graph and exploits these reviews to predict aspects\nthat might emerge in future reviews. A key novelty of our method is to jointly\nprovide review, product, and aspect embeddings that are both time-sensitive and\nless affected by extremely imbalanced aspect frequencies. We evaluated ForeSeer\non a real-world product review system containing 11,536,382 reviews and 11,000\nproducts over 3 years. We observe that ForeSeer substantially outperformed\nexisting approaches with at least 49.1\\% AUPRC improvement under the real\nsetting where aspect associations are not given. ForeSeer further improves\nfuture link prediction on the product graph and the review aspect association\nprediction. Collectively, Foreseer offers a novel framework for review\nforecasting by effectively integrating review text, product network, and\ntemporal information, opening up new avenues for online shopping recommendation\nand e-commerce applications.",
            "author": [
                "Zixuan Liu",
                "Gaurush Hiranandani",
                "Kun Qian",
                "Eddie W. Huang",
                "Yi Xu",
                "Belinda Zeng",
                "Karthik Subbian",
                "Sheng Wang"
            ],
            "link": [
                "http://dx.doi.org/10.1145/3583780.3614887",
                "http://arxiv.org/abs/2310.04865v1",
                "http://arxiv.org/pdf/2310.04865v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04859v2",
            "title": "Universal Graph Random Features",
            "updated": "2023-10-10T08:03:47Z",
            "published": "2023-10-07T15:47:31Z",
            "summary": "We propose a novel random walk-based algorithm for unbiased estimation of\narbitrary functions of a weighted adjacency matrix, coined universal graph\nrandom features (u-GRFs). This includes many of the most popular examples of\nkernels defined on the nodes of a graph. Our algorithm enjoys subquadratic time\ncomplexity with respect to the number of nodes, overcoming the notoriously\nprohibitive cubic scaling of exact graph kernel evaluation. It can also be\ntrivially distributed across machines, permitting learning on much larger\nnetworks. At the heart of the algorithm is a modulation function which\nupweights or downweights the contribution from different random walks depending\non their lengths. We show that by parameterising it with a neural network we\ncan obtain u-GRFs that give higher-quality kernel estimates or perform\nefficient, scalable kernel learning. We provide robust theoretical analysis and\nsupport our findings with experiments including pointwise estimation of fixed\ngraph kernels, solving non-homogeneous graph ordinary differential equations,\nnode clustering and kernel regression on triangular meshes.",
            "author": [
                "Isaac Reid",
                "Krzysztof Choromanski",
                "Eli Berger",
                "Adrian Weller"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04859v2",
                "http://arxiv.org/pdf/2310.04859v2"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04854v1",
            "title": "Repelling Random Walks",
            "updated": "2023-10-07T15:30:23Z",
            "published": "2023-10-07T15:30:23Z",
            "summary": "We present a novel quasi-Monte Carlo mechanism to improve graph-based\nsampling, coined repelling random walks. By inducing correlations between the\ntrajectories of an interacting ensemble such that their marginal transition\nprobabilities are unmodified, we are able to explore the graph more\nefficiently, improving the concentration of statistical estimators whilst\nleaving them unbiased. The mechanism has a trivial drop-in implementation. We\nshowcase the effectiveness of repelling random walks in a range of settings\nincluding estimation of graph kernels, the PageRank vector and graphlet\nconcentrations. We provide detailed experimental evaluation and robust\ntheoretical guarantees. To our knowledge, repelling random walks constitute the\nfirst rigorously studied quasi-Monte Carlo scheme correlating the directions of\nwalkers on a graph, inviting new research in this exciting nascent domain.",
            "author": [
                "Isaac Reid",
                "Eli Berger",
                "Krzysztof Choromanski",
                "Adrian Weller"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04854v1",
                "http://arxiv.org/pdf/2310.04854v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04851v1",
            "title": "Star Coloring of Tensor Product of Two Graphs",
            "updated": "2023-10-07T15:22:16Z",
            "published": "2023-10-07T15:22:16Z",
            "summary": "A star coloring of a graph $G$ is a proper vertex coloring such that no path\non four vertices is bicolored. The smallest integer $k$ for which $G$ admits a\nstar coloring with $k$ colors is called the star chromatic number of $G$,\ndenoted as $\\chi_s(G)$. In this paper, we study the star coloring of tensor\nproduct of two graphs and obtain the following results.\n  1. We give an upper bound on the star chromatic number of the tensor product\nof two arbitrary graphs.\n  2. We determine the exact value of the star chromatic number of tensor\nproduct two paths.\n  3. We show that the star chromatic number of tensor product of two cycles is\nfive, except for $C_3 \\times C_3$ and $C_3 \\times C_5$.\n  4. We give tight bounds for the star chromatic number of tensor product of a\ncycle and a path.",
            "author": [
                "Harshit Kumar Choudhary",
                "Swati Kumari",
                "I. Vinod Reddy"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04851v1",
                "http://arxiv.org/pdf/2310.04851v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04850v1",
            "title": "A new use of nonlocal symmetries for computing Liouvillian first\n  integrals of rational second order ordinary differential equations",
            "updated": "2023-10-07T15:18:40Z",
            "published": "2023-10-07T15:18:40Z",
            "summary": "Here we present an efficient method for finding and using a nonlocal symmetry\nadmitted by a rational second order ordinary differential equation (rational\n2ODE) in order to find a Liouvillian first integral (belonging to a vast class\nof Liouvillian functions). In a first stage, we construct an algorithm\n(improving the methodde veloped in [1]) that computes a nonlocal symmetry of a\nrational 2ODE. In ase cond stage, based on the knowledge of this symmetry, it\nis possible to construct three polynomial vector fields (in R2), which \"share\"\nthe Liouvillian first integral with the rational 2ODE. These \"plane\" polynomial\nvector fields can be used to construct a procedure (based on an idea developed\nin [2]) to determine an integrating factor for the rational 2ODE with a fast\nprobabilistic algorithm. The main advantages of the proposed method are: the\nobtaining of the nonlocal symmetry is algorithmic and very efficient and,\nfurthermore, its use to find an integrating factor is a sequence of linear or\nquasilinear processes.",
            "author": [
                "I. Deme",
                "L. G. S. Duarte",
                "L. A. C. P. da Mota"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04850v1",
                "http://arxiv.org/pdf/2310.04850v1"
            ],
            "primary_category": "nlin.CD",
            "category": [
                "nlin.CD",
                "math-ph",
                "math.MP",
                "34-XX",
                "G.1.7"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04842v1",
            "title": "Sub-linear Regret in Adaptive Model Predictive Control",
            "updated": "2023-10-07T15:07:10Z",
            "published": "2023-10-07T15:07:10Z",
            "summary": "We consider the problem of adaptive Model Predictive Control (MPC) for\nuncertain linear-systems with additive disturbances and with state and input\nconstraints. We present STT-MPC (Self-Tuning Tube-based Model Predictive\nControl), an online algorithm that combines the certainty-equivalence principle\nand polytopic tubes. Specifically, at any given step, STT-MPC infers the system\ndynamics using the Least Squares Estimator (LSE), and applies a controller\nobtained by solving an MPC problem using these estimates. The use of polytopic\ntubes is so that, despite the uncertainties, state and input constraints are\nsatisfied, and recursive-feasibility and asymptotic stability hold. In this\nwork, we analyze the regret of the algorithm, when compared to an oracle\nalgorithm initially aware of the system dynamics. We establish that the\nexpected regret of STT-MPC does not exceed $O(T^{1/2 + \\epsilon})$, where\n$\\epsilon \\in (0,1)$ is a design parameter tuning the persistent excitation\ncomponent of the algorithm. Our result relies on a recently proposed\nexponential decay of sensitivity property and, to the best of our knowledge, is\nthe first of its kind in this setting. We illustrate the performance of our\nalgorithm using a simple numerical example.",
            "author": [
                "Damianos Tranos",
                "Alexandre Proutiere"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04842v1",
                "http://arxiv.org/pdf/2310.04842v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.AI",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04835v2",
            "title": "On the Evolution of Knowledge Graphs: A Survey and Perspective",
            "updated": "2023-10-10T05:15:08Z",
            "published": "2023-10-07T14:46:51Z",
            "summary": "Knowledge graphs (KGs) are structured representations of diversified\nknowledge. They are widely used in various intelligent applications. In this\narticle, we provide a comprehensive survey on the evolution of various types of\nknowledge graphs (i.e., static KGs, dynamic KGs, temporal KGs, and event KGs)\nand techniques for knowledge extraction and reasoning. Furthermore, we\nintroduce the practical applications of different types of KGs, including a\ncase study in financial analysis. Finally, we propose our perspective on the\nfuture directions of knowledge engineering, including the potential of\ncombining the power of knowledge graphs and large language models (LLMs), and\nthe evolution of knowledge extraction, reasoning, and representation.",
            "author": [
                "Xuhui Jiang",
                "Chengjin Xu",
                "Yinghan Shen",
                "Xun Sun",
                "Lumingyuan Tang",
                "Saizhuo Wang",
                "Zhongwu Chen",
                "Yuanzhuo Wang",
                "Jian Guo"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04835v2",
                "http://arxiv.org/pdf/2310.04835v2"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04828v1",
            "title": "Guardians as You Fall: Active Mode Transition for Safe Falling",
            "updated": "2023-10-07T14:36:48Z",
            "published": "2023-10-07T14:36:48Z",
            "summary": "Recent advancements in optimal control and reinforcement learning have\nenabled quadrupedal robots to perform various agile locomotion tasks over\ndiverse terrains. During these agile motions, ensuring the stability and\nresiliency of the robot is a primary concern to prevent catastrophic falls and\nmitigate potential damages. Previous methods primarily focus on recovery\npolicies after the robot falls. There is no active safe falling solution to the\nbest of our knowledge. In this paper, we proposed Guardians as You Fall (GYF),\na safe falling/tumbling and recovery framework that can actively tumble and\nrecover to stable modes to reduce damage in highly dynamic scenarios. The key\nidea of GYF is to adaptively traverse different stable modes via active\ntumbling before the robot shifts to irrecoverable poses. Via comprehensive\nsimulation and real-world experiments, we show that GYF significantly reduces\nthe maximum acceleration and jerk of the robot base compared to the baselines.\nIn particular, GYF reduces the maximum acceleration and jerk by 20%~73% in\ndifferent scenarios in simulation and real-world experiments. GYF offers a new\nperspective on safe falling and recovery in locomotion tasks, potentially\nenabling much more aggressive explorations of existing agile locomotion skills.",
            "author": [
                "Yikai Wang",
                "Mengdi Xu",
                "Guanya Shi",
                "Ding Zhao"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04828v1",
                "http://arxiv.org/pdf/2310.04828v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04820v1",
            "title": "The BCH Family of Storage Codes on Triangle-Free Graphs is of Unit Rate",
            "updated": "2023-10-07T14:18:18Z",
            "published": "2023-10-07T14:18:18Z",
            "summary": "Let $\\Gamma$ be a simple connected graph on $n$ vertices, and let $C$ be a\ncode of length $n$ whose coordinates are indexed by the vertices of $\\Gamma$.\nWe say that $C$ is a \\textit{storage code} on $\\Gamma$ if for any codeword $c\n\\in C$, one can recover the information on each coordinate of $c$ by accessing\nits neighbors in $\\Gamma$. The main problem here is to construct high-rate\nstorage codes on triangle-free graphs. In this paper, we solve an open problem\nposed by Barg and Z\\'emor in 2022, showing that the BCH family of storage codes\nis of unit rate. Furthermore, we generalize the construction of the BCH family\nand obtain more storage codes of unit rate on triangle-free graphs.",
            "author": [
                "Haihua Deng",
                "Hexiang Huang",
                "Guobiao Weng",
                "Qing Xiang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04820v1",
                "http://arxiv.org/pdf/2310.04820v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "math.CO",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04809v2",
            "title": "Leveraging LLVM's ScalarEvolution for Symbolic Data Cache Analysis",
            "updated": "2023-10-17T09:05:30Z",
            "published": "2023-10-07T14:02:55Z",
            "summary": "While instruction cache analysis is essentially a solved problem, data cache\nanalysis is more challenging. In contrast to instruction fetches, the data\naccesses generated by a memory instruction may vary with the program's inputs\nand across dynamic occurrences of the same instruction in loops.\n  We observe that the plain control-flow graph (CFG) abstraction employed in\nclassical cache analyses is inadequate to capture the dynamic behavior of\nmemory instructions. On top of plain CFGs, accurate analysis of the underlying\nprogram's cache behavior is impossible.\n  Thus, our first contribution is the definition of a more expressive program\nabstraction coined symbolic control-flow graphs, which can be obtained from\nLLVM's ScalarEvolution analysis. To exploit this richer abstraction, our main\ncontribution is the development of symbolic data cache analysis, a smooth\ngeneralization of classical LRU must analysis from plain to symbolic\ncontrol-flow graphs.\n  The experimental evaluation demonstrates that symbolic data cache analysis\nconsistently outperforms classical LRU must analysis both in terms of accuracy\nand analysis runtime.",
            "author": [
                "Valentin Touzeau",
                "Jan Reineke"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04809v2",
                "http://arxiv.org/pdf/2310.04809v2"
            ],
            "primary_category": "cs.PL",
            "category": [
                "cs.PL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04802v1",
            "title": "Hierarchical Unsupervised Topological SLAM",
            "updated": "2023-10-07T13:40:52Z",
            "published": "2023-10-07T13:40:52Z",
            "summary": "In this paper we present a novel framework for unsupervised topological\nclustering resulting in improved loop. In this paper we present a novel\nframework for unsupervised topological clustering resulting in improved loop\ndetection and closure for SLAM. A navigating mobile robot clusters its\ntraversal into visually similar topologies where each cluster (topology)\ncontains a set of similar looking images typically observed from spatially\nadjacent locations. Each such set of spatially adjacent and visually similar\ngrouping of images constitutes a topology obtained without any supervision. We\nformulate a hierarchical loop discovery strategy that first detects loops at\nthe level of topologies and subsequently at the level of images between the\nlooped topologies. We show over a number of traversals across different Habitat\nenvironments that such a hierarchical pipeline significantly improves SOTA\nimage based loop detection and closure methods. Further, as a consequence of\nimproved loop detection, we enhance the loop closure and backend SLAM\nperformance. Such a rendering of a traversal into topological segments is\nbeneficial for downstream tasks such as navigation that can now build a\ntopological graph where spatially adjacent topological clusters are connected\nby an edge and navigate over such topological graphs.",
            "author": [
                "Ayush Sharma",
                "Yash Mehan",
                "Pradyumna Dasu",
                "Sourav Garg",
                "Madhava Krishna"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04802v1",
                "http://arxiv.org/pdf/2310.04802v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04799v1",
            "title": "Chat Vector: A Simple Approach to Equip LLMs With New Language Chat\n  Capabilities",
            "updated": "2023-10-07T13:34:21Z",
            "published": "2023-10-07T13:34:21Z",
            "summary": "With the advancements in conversational AI, such as ChatGPT, this paper\nfocuses on exploring developing Large Language Models (LLMs) for non-English\nlanguages, especially emphasizing alignment with human preferences. We\nintroduce a computationally efficient method, leveraging chat vector, to\nsynergize pre-existing knowledge and behaviors in LLMs, restructuring the\nconventional training paradigm from continual pre-train -> SFT -> RLHF to\ncontinual pre-train + chat vector. Our empirical studies, primarily focused on\nTraditional Chinese, employ LLaMA2 as the base model and acquire the chat\nvector by subtracting the pre-trained weights, LLaMA2, from the weights of\nLLaMA2-chat. Evaluating from three distinct facets, which are toxicity, ability\nof instruction following, and multi-turn dialogue demonstrates the chat\nvector's superior efficacy in chatting. To confirm the adaptability of our\napproach, we extend our experiments to include models pre-trained in both\nKorean and Simplified Chinese, illustrating the versatility of our methodology.\nOverall, we present a significant solution in aligning LLMs with human\npreferences efficiently across various languages, accomplished by the chat\nvector.",
            "author": [
                "Shih-Cheng Huang",
                "Pin-Zu Li",
                "Yu-Chi Hsu",
                "Kuang-Ming Chen",
                "Yu Tung Lin",
                "Shih-Kai Hsiao",
                "Richard Tzong-Han Tsai",
                "Hung-yi Lee"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04799v1",
                "http://arxiv.org/pdf/2310.04799v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04787v1",
            "title": "HI-SLAM: Monocular Real-time Dense Mapping with Hybrid Implicit Fields",
            "updated": "2023-10-07T12:26:56Z",
            "published": "2023-10-07T12:26:56Z",
            "summary": "In this letter, we present a neural field-based real-time monocular mapping\nframework for accurate and dense Simultaneous Localization and Mapping (SLAM).\nRecent neural mapping frameworks show promising results, but rely on RGB-D or\npose inputs, or cannot run in real-time. To address these limitations, our\napproach integrates dense-SLAM with neural implicit fields. Specifically, our\ndense SLAM approach runs parallel tracking and global optimization, while a\nneural field-based map is constructed incrementally based on the latest SLAM\nestimates. For the efficient construction of neural fields, we employ\nmulti-resolution grid encoding and signed distance function (SDF)\nrepresentation. This allows us to keep the map always up-to-date and adapt\ninstantly to global updates via loop closing. For global consistency, we\npropose an efficient Sim(3)-based pose graph bundle adjustment (PGBA) approach\nto run online loop closing and mitigate the pose and scale drift. To enhance\ndepth accuracy further, we incorporate learned monocular depth priors. We\npropose a novel joint depth and scale adjustment (JDSA) module to solve the\nscale ambiguity inherent in depth priors. Extensive evaluations across\nsynthetic and real-world datasets validate that our approach outperforms\nexisting methods in accuracy and map completeness while preserving real-time\nperformance.",
            "author": [
                "Wei Zhang",
                "Tiecheng Sun",
                "Sen Wang",
                "Qing Cheng",
                "Norbert Haala"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04787v1",
                "http://arxiv.org/pdf/2310.04787v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04782v1",
            "title": "Improving the Reliability of Large Language Models by Leveraging\n  Uncertainty-Aware In-Context Learning",
            "updated": "2023-10-07T12:06:53Z",
            "published": "2023-10-07T12:06:53Z",
            "summary": "In recent years, large-scale language models (LLMs) have gained attention for\ntheir impressive text generation capabilities. However, these models often face\nthe challenge of \"hallucination,\" which undermines their reliability. In this\nstudy, we introduce an uncertainty-aware in-context learning framework to\nempower the model to enhance or reject its output in response to uncertainty.\nHuman-defined methods for estimating uncertainty typically assume that\n\"uncertainty is lower when the model's response is correct compared to when it\nis incorrect.\" However, setting a precise threshold to distinguish correctness\nis challenging. Therefore, we introduce uncertainty information as an\nintermediary variable that implicitly influences the model's behavior. Our\ninnovative uncertainty-aware in-context learning framework involves fine-tuning\nthe LLM using a calibration dataset. Our aim is to improve the model's\nresponses by filtering out answers with high uncertainty while considering the\nmodel's knowledge limitations. We evaluate the model's knowledge by examining\nmultiple responses to the same question for the presence of a correct answer.\nWhen the model lacks relevant knowledge, the response should indicate that the\nquestion cannot be answered. Conversely, when the model has relevant knowledge,\nthe response should provide the correct answer. Extensive experiments confirm\nthe effectiveness of our framework, leading to two key findings. First, the\nlogit output values of the LLM partly reflect inherent uncertainty. Second, our\nmodel autonomously recognizes uncertainty, resulting in improved responses.",
            "author": [
                "Yuchen Yang",
                "Houqiang Li",
                "Yanfeng Wang",
                "Yu Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04782v1",
                "http://arxiv.org/pdf/2310.04782v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04764v2",
            "title": "Characterizations of Definable Context-Free Graphs",
            "updated": "2023-10-11T07:44:04Z",
            "published": "2023-10-07T09:53:52Z",
            "summary": "We give a characterization of those sets of graphs that are both definable in\nCounting Monadic Second Order Logic (CMS) and context-free, i.e., least\nsolutions of Hyperedge-Replacement (HR)-grammars introduced by Courcelle and\nEngelfriet. We give the following equivalent characterizations:\n  (a) a set of graphs is recognizable (in the algebra that consists of all\ngraphs and HR-operations) and has bounded tree-width; further, we refine this\ncondition and show equivalence with recognizability in a finite-sort subalgebra\nof the graph algebra;\n  (b) the set is parsable, i.e., there is an MS-definable transduction from\ngraphs to a set of derivation trees labelled by HR-operations, such that the\nset of graphs is the image of this set of trees under the evaluation of the\nHR-operations;\n  (c) the set of graphs is the image of unranked recognizable set of trees\nunder an MS-definable transduction whose inverse is also MS-definable.\n  The main goal of this paper is to present the above characterization, of\nwhich several directions are already known, in an accessible and unified way.\nWe rely on a novel connection between two seminal results, a logical\ncharacterization of context-free graph languages in terms of tree to graph\nMS-definable transductions, by Courcelle and Engelfriet~, and a proof that an\noptimal-width tree decomposition of a graph can be built by an MS-definable\ntransduction, by Bojanczyk and Pilipczuk.",
            "author": [
                "Radu Iosif",
                "Florian Zuleger"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04764v2",
                "http://arxiv.org/pdf/2310.04764v2"
            ],
            "primary_category": "cs.FL",
            "category": [
                "cs.FL",
                "cs.LO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04760v1",
            "title": "Multi-objective Progressive Clustering for Semi-supervised Domain\n  Adaptation in Speaker Verification",
            "updated": "2023-10-07T09:46:07Z",
            "published": "2023-10-07T09:46:07Z",
            "summary": "Utilizing the pseudo-labeling algorithm with large-scale unlabeled data\nbecomes crucial for semi-supervised domain adaptation in speaker verification\ntasks. In this paper, we propose a novel pseudo-labeling method named\nMulti-objective Progressive Clustering (MoPC), specifically designed for\nsemi-supervised domain adaptation. Firstly, we utilize limited labeled data\nfrom the target domain to derive domain-specific descriptors based on multiple\ndistinct objectives, namely within-graph denoising, intra-class denoising and\ninter-class denoising. Then, the Infomap algorithm is adopted for embedding\nclustering, and the descriptors are leveraged to further refine the target\ndomain's pseudo-labels. Moreover, to further improve the quality of pseudo\nlabels, we introduce the subcenter-purification and progressive-merging\nstrategy for label denoising. Our proposed MoPC method achieves 4.95% EER and\nranked the 1$^{st}$ place on the evaluation set of VoxSRC 2023 track 3. We also\nconduct additional experiments on the FFSVC dataset and yield promising\nresults.",
            "author": [
                "Ze Li",
                "Yuke Lin",
                "Ning Jiang",
                "Xiaoyi Qin",
                "Guoqing Zhao",
                "Haiying Wu",
                "Ming Li"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04760v1",
                "http://arxiv.org/pdf/2310.04760v1"
            ],
            "primary_category": "eess.AS",
            "category": [
                "eess.AS",
                "cs.SD"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04747v1",
            "title": "Towards Dynamic and Small Objects Refinement for Unsupervised Domain\n  Adaptative Nighttime Semantic Segmentation",
            "updated": "2023-10-07T09:05:50Z",
            "published": "2023-10-07T09:05:50Z",
            "summary": "Nighttime semantic segmentation is essential for various applications, e.g.,\nautonomous driving, which often faces challenges due to poor illumination and\nthe lack of well-annotated datasets. Unsupervised domain adaptation (UDA) has\nshown potential for addressing the challenges and achieved remarkable results\nfor nighttime semantic segmentation. However, existing methods still face\nlimitations in 1) their reliance on style transfer or relighting models, which\nstruggle to generalize to complex nighttime environments, and 2) their\nignorance of dynamic and small objects like vehicles and traffic signs, which\nare difficult to be directly learned from other domains. This paper proposes a\nnovel UDA method that refines both label and feature levels for dynamic and\nsmall objects for nighttime semantic segmentation. First, we propose a dynamic\nand small object refinement module to complement the knowledge of dynamic and\nsmall objects from the source domain to target nighttime domain. These dynamic\nand small objects are normally context-inconsistent in under-exposed\nconditions. Then, we design a feature prototype alignment module to reduce the\ndomain gap by deploying contrastive learning between features and prototypes of\nthe same class from different domains, while re-weighting the categories of\ndynamic and small objects. Extensive experiments on four benchmark datasets\ndemonstrate that our method outperforms prior arts by a large margin for\nnighttime segmentation. Project page: https://rorisis.github.io/DSRNSS/.",
            "author": [
                "Jingyi Pan",
                "Sihang Li",
                "Yucheng Chen",
                "Jinjing Zhu",
                "Lin Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04747v1",
                "http://arxiv.org/pdf/2310.04747v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04745v1",
            "title": "Incremental dynamics of prestressed viscoelastic solids and its\n  applications in shear wave elastography",
            "updated": "2023-10-07T09:03:33Z",
            "published": "2023-10-07T09:03:33Z",
            "summary": "Shear wave elastography (SWE) has emerged as a new imaging modality that\nbrings tissue mechanical properties as biomarkers potentially useful for early\nand precise diagnosis. While different SWE methods have been proposed, how to\nrelate the frequency SWE measurements to quasi-static stiffnesses of tissues\nsensed by cells when prestresses are involved remains challenging. Here we\nsuggest an incremental dynamics theory for prestressed viscoelastic solids and\ninvestigate its application in SWE across a broad frequency range. To model the\npower-law dispersion relation with minimal parameters, we introduce the\nKelvin-Voigt fractional derivation model (KVFD) in the constitutive modeling of\nmaterial viscoelasticity. To validate the usefulness of the theory, we\nperformed experiments on prestressed soft materials and biological tissues. The\nresults show that the theoretical solution fits the experimental dispersion\ncurve well over a broad frequency range and accurately captures the effect of\nprestress. The theory also reveals the correlation of phase velocities and\nattenuations of shear waves with principal stresses and leads to a method for\nprobing the prestress in a viscoelastic solid without prior knowledge of the\nconstitutive parameters as validated by our numerical experiments. Taken\ntogether, our results show that the theory presented here enables the\ndevelopment of spatially resolved SWE when high-frequency shear waves get\ninvolved, and provides insights into wave motions in soft materials subject to\nprestresses.",
            "author": [
                "Yuxuan Jiang",
                "Guo-Yang Li",
                "Zhaoyi Zhang",
                "Shiyu Ma",
                "Yanping Cao",
                "Seok-Hyun Yun"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04745v1",
                "http://arxiv.org/pdf/2310.04745v1"
            ],
            "primary_category": "cond-mat.soft",
            "category": [
                "cond-mat.soft",
                "physics.bio-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04743v1",
            "title": "Resprompt: Residual Connection Prompting Advances Multi-Step Reasoning\n  in Large Language Models",
            "updated": "2023-10-07T08:56:28Z",
            "published": "2023-10-07T08:56:28Z",
            "summary": "Chain-of-thought (CoT) prompting, which offers step-by-step problem-solving\nrationales, has impressively unlocked the reasoning potential of large language\nmodels (LLMs). Yet, the standard CoT is less effective in problems demanding\nmultiple reasoning steps. This limitation arises from the complex reasoning\nprocess in multi-step problems: later stages often depend on the results of\nseveral steps earlier, not just the results of the immediately preceding step.\nSuch complexities suggest the reasoning process is naturally represented as a\ngraph. The almost linear and straightforward structure of CoT prompting,\nhowever, struggles to capture this complex reasoning graph. To address this\nchallenge, we propose Residual Connection Prompting (RESPROMPT), a new\nprompting strategy that advances multi-step reasoning in LLMs. Our key idea is\nto reconstruct the reasoning graph within prompts. We achieve this by\nintegrating necessary connections-links present in the reasoning graph but\nmissing in the linear CoT flow-into the prompts. Termed \"residual connections\",\nthese links are pivotal in morphing the linear CoT structure into a graph\nrepresentation, effectively capturing the complex reasoning graphs inherent in\nmulti-step problems. We evaluate RESPROMPT on six benchmarks across three\ndiverse domains: math, sequential, and commonsense reasoning. For the\nopen-sourced LLaMA family of models, RESPROMPT yields a significant average\nreasoning accuracy improvement of 12.5% on LLaMA-65B and 6.8% on LLaMA2-70B.\nBreakdown analysis further highlights RESPROMPT particularly excels in complex\nmulti-step reasoning: for questions demanding at least five reasoning steps,\nRESPROMPT outperforms the best CoT based benchmarks by a remarkable average\nimprovement of 21.1% on LLaMA-65B and 14.3% on LLaMA2-70B. Through extensive\nablation studies and analyses, we pinpoint how to most effectively build\nresidual connections.",
            "author": [
                "Song Jiang",
                "Zahra Shakeri",
                "Aaron Chan",
                "Maziar Sanjabi",
                "Hamed Firooz",
                "Yinglong Xia",
                "Bugra Akyildiz",
                "Yizhou Sun",
                "Jinchao Li",
                "Qifan Wang",
                "Asli Celikyilmaz"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04743v1",
                "http://arxiv.org/pdf/2310.04743v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04741v3",
            "title": "Balancing stability and plasticity in continual learning: the\n  readout-decomposition of activation change (RDAC) framework",
            "updated": "2023-11-20T16:09:07Z",
            "published": "2023-10-07T08:54:43Z",
            "summary": "Continual learning (CL) algorithms strive to acquire new knowledge while\npreserving prior information. However, this stability-plasticity trade-off\nremains a central challenge. This paper introduces a framework that dissects\nthis trade-off, offering valuable insights into CL algorithms. The\nReadout-Decomposition of Activation Change (RDAC) framework first addresses the\nstability-plasticity dilemma and its relation to catastrophic forgetting. It\nrelates learning-induced activation changes in the range of prior readouts to\nthe degree of stability and changes in the null space to the degree of\nplasticity. In deep non-linear networks tackling split-CIFAR-110 tasks, the\nframework clarifies the stability-plasticity trade-offs of the popular\nregularization algorithms Synaptic intelligence (SI), Elastic-weight\nconsolidation (EWC), and learning without Forgetting (LwF), and replay-based\nalgorithms Gradient episodic memory (GEM), and data replay. GEM and data replay\npreserved stability and plasticity, while SI, EWC, and LwF traded off\nplasticity for stability. The inability of the regularization algorithms to\nmaintain plasticity was linked to them restricting the change of activations in\nthe null space of the prior readout. Additionally, for one-hidden-layer linear\nneural networks, we derived a gradient decomposition algorithm to restrict\nactivation change only in the range of the prior readouts, to maintain high\nstability while not further sacrificing plasticity. Results demonstrate that\nthe algorithm maintained stability without significant plasticity loss. The\nRDAC framework informs the behavior of existing CL algorithms and paves the way\nfor novel CL approaches. Finally, it sheds light on the connection between\nlearning-induced activation/representation changes and the stability-plasticity\ndilemma, also offering insights into representational drift in biological\nsystems.",
            "author": [
                "Daniel Anthes",
                "Sushrut Thorat",
                "Peter K\u00f6nig",
                "Tim C. Kietzmann"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04741v3",
                "http://arxiv.org/pdf/2310.04741v3"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV",
                "q-bio.NC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04726v1",
            "title": "Zero-shot Cross-lingual Transfer without Parallel Corpus",
            "updated": "2023-10-07T07:54:22Z",
            "published": "2023-10-07T07:54:22Z",
            "summary": "Recently, although pre-trained language models have achieved great success on\nmultilingual NLP (Natural Language Processing) tasks, the lack of training data\non many tasks in low-resource languages still limits their performance. One\neffective way of solving that problem is to transfer knowledge from\nrich-resource languages to low-resource languages. However, many previous works\non cross-lingual transfer rely heavily on the parallel corpus or translation\nmodels, which are often difficult to obtain. We propose a novel approach to\nconduct zero-shot cross-lingual transfer with a pre-trained model. It consists\nof a Bilingual Task Fitting module that applies task-related bilingual\ninformation alignment; a self-training module generates pseudo soft and hard\nlabels for unlabeled data and utilizes them to conduct self-training. We got\nthe new SOTA on different tasks without any dependencies on the parallel corpus\nor translation models.",
            "author": [
                "Yuyang Zhang",
                "Xiaofeng Han",
                "Baojun Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04726v1",
                "http://arxiv.org/pdf/2310.04726v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04723v1",
            "title": "Subspace Identification for Multi-Source Domain Adaptation",
            "updated": "2023-10-07T07:52:59Z",
            "published": "2023-10-07T07:52:59Z",
            "summary": "Multi-source domain adaptation (MSDA) methods aim to transfer knowledge from\nmultiple labeled source domains to an unlabeled target domain. Although current\nmethods achieve target joint distribution identifiability by enforcing minimal\nchanges across domains, they often necessitate stringent conditions, such as an\nadequate number of domains, monotonic transformation of latent variables, and\ninvariant label distributions. These requirements are challenging to satisfy in\nreal-world applications. To mitigate the need for these strict assumptions, we\npropose a subspace identification theory that guarantees the disentanglement of\ndomain-invariant and domain-specific variables under less restrictive\nconstraints regarding domain numbers and transformation properties, thereby\nfacilitating domain adaptation by minimizing the impact of domain shifts on\ninvariant variables. Based on this theory, we develop a Subspace Identification\nGuarantee (SIG) model that leverages variational inference. Furthermore, the\nSIG model incorporates class-aware conditional alignment to accommodate target\nshifts where label distributions change with the domains. Experimental results\ndemonstrate that our SIG model outperforms existing MSDA techniques on various\nbenchmark datasets, highlighting its effectiveness in real-world applications.",
            "author": [
                "Zijian Li",
                "Ruichu Cai",
                "Guangyi Chen",
                "Boyang Sun",
                "Zhifeng Hao",
                "Kun Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04723v1",
                "http://arxiv.org/pdf/2310.04723v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04716v1",
            "title": "Reinforced UI Instruction Grounding: Towards a Generic UI Task\n  Automation API",
            "updated": "2023-10-07T07:22:41Z",
            "published": "2023-10-07T07:22:41Z",
            "summary": "Recent popularity of Large Language Models (LLMs) has opened countless\npossibilities in automating numerous AI tasks by connecting LLMs to various\ndomain-specific models or APIs, where LLMs serve as dispatchers while\ndomain-specific models or APIs are action executors. Despite the vast numbers\nof domain-specific models/APIs, they still struggle to comprehensively cover\nsuper diverse automation demands in the interaction between human and User\nInterfaces (UIs). In this work, we build a multimodal model to ground natural\nlanguage instructions in given UI screenshots as a generic UI task automation\nexecutor. This metadata-free grounding model, consisting of a visual encoder\nand a language decoder, is first pretrained on well studied document\nunderstanding tasks and then learns to decode spatial information from UI\nscreenshots in a promptable way. To facilitate the exploitation of\nimage-to-text pretrained knowledge, we follow the pixel-to-sequence paradigm to\npredict geometric coordinates in a sequence of tokens using a language decoder.\nWe further propose an innovative Reinforcement Learning (RL) based algorithm to\nsupervise the tokens in such sequence jointly with visually semantic metrics,\nwhich effectively strengthens the spatial decoding capability of the\npixel-to-sequence paradigm. Extensive experiments demonstrate our proposed\nreinforced UI instruction grounding model outperforms the state-of-the-art\nmethods by a clear margin and shows the potential as a generic UI task\nautomation API.",
            "author": [
                "Zhizheng Zhang",
                "Wenxuan Xie",
                "Xiaoyi Zhang",
                "Yan Lu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04716v1",
                "http://arxiv.org/pdf/2310.04716v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04714v1",
            "title": "Generalized Robust Test-Time Adaptation in Continuous Dynamic Scenarios",
            "updated": "2023-10-07T07:13:49Z",
            "published": "2023-10-07T07:13:49Z",
            "summary": "Test-time adaptation (TTA) adapts the pre-trained models to test\ndistributions during the inference phase exclusively employing unlabeled test\ndata streams, which holds great value for the deployment of models in\nreal-world applications. Numerous studies have achieved promising performance\non simplistic test streams, characterized by independently and uniformly\nsampled test data originating from a fixed target data distribution. However,\nthese methods frequently prove ineffective in practical scenarios, where both\ncontinual covariate shift and continual label shift occur simultaneously, i.e.,\ndata and label distributions change concurrently and continually over time. In\nthis study, a more challenging Practical Test-Time Adaptation (PTTA) setup is\nintroduced, which takes into account the concurrent presence of continual\ncovariate shift and continual label shift, and we propose a Generalized Robust\nTest-Time Adaptation (GRoTTA) method to effectively address the difficult\nproblem. We start by steadily adapting the model through Robust Parameter\nAdaptation to make balanced predictions for test samples. To be specific,\nfirstly, the effects of continual label shift are eliminated by enforcing the\nmodel to learn from a uniform label distribution and introducing recalibration\nof batch normalization to ensure stability. Secondly, the continual covariate\nshift is alleviated by employing a source knowledge regularization with the\nteacher-student model to update parameters. Considering the potential\ninformation in the test stream, we further refine the balanced predictions by\nBias-Guided Output Adaptation, which exploits latent structure in the feature\nspace and is adaptive to the imbalanced label distribution. Extensive\nexperiments demonstrate GRoTTA outperforms the existing competitors by a large\nmargin under PTTA setting, rendering it highly conducive for adoption in\nreal-world applications.",
            "author": [
                "Shuang Li",
                "Longhui Yuan",
                "Binhui Xie",
                "Tao Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04714v1",
                "http://arxiv.org/pdf/2310.04714v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04709v1",
            "title": "Time-dependent mediators in survival analysis: Graphical representation\n  of causal assumptions",
            "updated": "2023-10-07T06:59:41Z",
            "published": "2023-10-07T06:59:41Z",
            "summary": "We study time-dependent mediators in survival analysis using a treatment\nseparation approach due to Didelez [2019] and based on earlier work by Robins\nand Richardson [2011]. This approach avoids nested counterfactuals and\ncrossworld assumptions which are otherwise common in mediation analysis. The\ncausal model of treatment, mediators, covariates, confounders and outcome is\nrepresented by causal directed acyclic graphs (DAGs). However, the DAGs tend to\nbe very complex when we have measurements at a large number of time points. We\ntherefore suggest using so-called rolled graphs in which a node represents an\nentire coordinate process instead of a single random variable, leading us to\nfar simpler graphical representations. The rolled graphs are not necessarily\nacyclic; they can be analyzed by $\\delta$-separation which is the appropriate\ngraphical separation criterion in this class of graphs and analogous to\n$d$-separation. In particular, $\\delta$-separation is a graphical tool for\nevaluating if the conditions of the mediation analysis are met or if unmeasured\nconfounders influence the estimated effects. We also state a mediational\ng-formula. This is similar to the approach in Vansteelandt et al. [2019]\nalthough that paper has a different conceptual basis. Finally, we apply this\nframework to a statistical model based on a Cox model with an added treatment\neffect.survival analysis; mediation; causal inference; graphical models; local\nindependence graphs",
            "author": [
                "S\u00f8ren Wengel Mogensen",
                "Odd O. Aalen",
                "Susanne Strohmaier"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04709v1",
                "http://arxiv.org/pdf/2310.04709v1"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME",
                "62D20, 62N02, 62H22"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04701v1",
            "title": "Twin Graph-based Anomaly Detection via Attentive Multi-Modal Learning\n  for Microservice System",
            "updated": "2023-10-07T06:28:41Z",
            "published": "2023-10-07T06:28:41Z",
            "summary": "Microservice architecture has sprung up over recent years for managing\nenterprise applications, due to its ability to independently deploy and scale\nservices. Despite its benefits, ensuring the reliability and safety of a\nmicroservice system remains highly challenging. Existing anomaly detection\nalgorithms based on a single data modality (i.e., metrics, logs, or traces)\nfail to fully account for the complex correlations and interactions between\ndifferent modalities, leading to false negatives and false alarms, whereas\nincorporating more data modalities can offer opportunities for further\nperformance gain. As a fresh attempt, we propose in this paper a\nsemi-supervised graph-based anomaly detection method, MSTGAD, which seamlessly\nintegrates all available data modalities via attentive multi-modal learning.\nFirst, we extract and normalize features from the three modalities, and further\nintegrate them using a graph, namely MST (microservice system twin) graph,\nwhere each node represents a service instance and the edge indicates the\nscheduling relationship between different service instances. The MST graph\nprovides a virtual representation of the status and scheduling relationships\namong service instances of a real-world microservice system. Second, we\nconstruct a transformer-based neural network with both spatial and temporal\nattention mechanisms to model the inter-correlations between different\nmodalities and temporal dependencies between the data points. This enables us\nto detect anomalies automatically and accurately in real-time. The source code\nof MSTGAD is publicly available at\nhttps://github.com/alipay/microservice_system_twin_graph_based_anomaly_detection.",
            "author": [
                "Jun Huang",
                "Yang Yang",
                "Hang Yu",
                "Jianguo Li",
                "Xiao Zheng"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04701v1",
                "http://arxiv.org/pdf/2310.04701v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04698v1",
            "title": "Tree-GPT: Modular Large Language Model Expert System for Forest Remote\n  Sensing Image Understanding and Interactive Analysis",
            "updated": "2023-10-07T06:12:39Z",
            "published": "2023-10-07T06:12:39Z",
            "summary": "This paper introduces a novel framework, Tree-GPT, which incorporates Large\nLanguage Models (LLMs) into the forestry remote sensing data workflow, thereby\nenhancing the efficiency of data analysis. Currently, LLMs are unable to\nextract or comprehend information from images and may generate inaccurate text\ndue to a lack of domain knowledge, limiting their use in forestry data\nanalysis. To address this issue, we propose a modular LLM expert system,\nTree-GPT, that integrates image understanding modules, domain knowledge bases,\nand toolchains. This empowers LLMs with the ability to comprehend images,\nacquire accurate knowledge, generate code, and perform data analysis in a local\nenvironment. Specifically, the image understanding module extracts structured\ninformation from forest remote sensing images by utilizing automatic or\ninteractive generation of prompts to guide the Segment Anything Model (SAM) in\ngenerating and selecting optimal tree segmentation results. The system then\ncalculates tree structural parameters based on these results and stores them in\na database. Upon receiving a specific natural language instruction, the LLM\ngenerates code based on a thought chain to accomplish the analysis task. The\ncode is then executed by an LLM agent in a local environment and . For\necological parameter calculations, the system retrieves the corresponding\nknowledge from the knowledge base and inputs it into the LLM to guide the\ngeneration of accurate code. We tested this system on several tasks, including\nSearch, Visualization, and Machine Learning Analysis. The prototype system\nperformed well, demonstrating the potential for dynamic usage of LLMs in\nforestry research and environmental sciences.",
            "author": [
                "Siqi Du",
                "Shengjun Tang",
                "Weixi Wang",
                "Xiaoming Li",
                "Renzhong Guo"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04698v1",
                "http://arxiv.org/pdf/2310.04698v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04695v1",
            "title": "Geometric model for weighted projective lines of type $(p,q)$",
            "updated": "2023-10-07T05:59:54Z",
            "published": "2023-10-07T05:59:54Z",
            "summary": "We give a geometric model for the category of coherent sheaves over the\nweighted projective line of type $(p,q)$ in terms of an annulus with marked\npoints on its boundary. We establish a bijection between indecomposable sheaves\nover the weighted projective line and certain homotopy classes of oriented\ncurves in the annulus, and prove that the dimension of extension group between\nindecomposable sheaves equals to the positive intersection number between the\ncorresponding curves.\n  By using the geometric model, we provide a combinatorial description for the\ntitling graph of tilting bundles, which is composed by quadrilaterals (or\ndegenerated to a line). Moreover, we obtain that the automorphism group of the\ncoherent sheaf category is isomorphic to the mapping class group of the marked\nannulus, and show the compatibility of their actions on the tilting graph of\ncoherent sheaves and on the triangulation of the geometric model respectively.\nA geometric description of the perpendicular category with respect to an\nexceptional sheaf is presented at the end of the paper.",
            "author": [
                "Jianmin Chen",
                "Shiquan Ruan",
                "Hongxia Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04695v1",
                "http://arxiv.org/pdf/2310.04695v1"
            ],
            "primary_category": "math.RT",
            "category": [
                "math.RT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04690v1",
            "title": "A dimension-reduced variational approach for solving physics-based\n  inverse problems using generative adversarial network priors and normalizing\n  flows",
            "updated": "2023-10-07T05:37:34Z",
            "published": "2023-10-07T05:37:34Z",
            "summary": "We propose a novel modular inference approach combining two different\ngenerative models -- generative adversarial networks (GAN) and normalizing\nflows -- to approximate the posterior distribution of physics-based Bayesian\ninverse problems framed in high-dimensional ambient spaces. We dub the proposed\nframework GAN-Flow. The proposed method leverages the intrinsic dimension\nreduction and superior sample generation capabilities of GANs to define a\nlow-dimensional data-driven prior distribution. Once a trained GAN-prior is\navailable, the inverse problem is solved entirely in the latent space of the\nGAN using variational Bayesian inference with normalizing flow-based\nvariational distribution, which approximates low-dimensional posterior\ndistribution by transforming realizations from the low-dimensional latent prior\n(Gaussian) to corresponding realizations of a low-dimensional variational\nposterior distribution. The trained GAN generator then maps realizations from\nthis approximate posterior distribution in the latent space back to the\nhigh-dimensional ambient space. We also propose a two-stage training strategy\nfor GAN-Flow wherein we train the two generative models sequentially.\nThereafter, GAN-Flow can estimate the statistics of posterior-predictive\nquantities of interest at virtually no additional computational cost. The\nsynergy between the two types of generative models allows us to overcome many\nchallenges associated with the application of Bayesian inference to large-scale\ninverse problems, chief among which are describing an informative prior and\nsampling from the high-dimensional posterior. We demonstrate the efficacy and\nflexibility of GAN-Flow on various physics-based inverse problems of varying\nambient dimensionality and prior knowledge using different types of GANs and\nnormalizing flows.",
            "author": [
                "Agnimitra Dasgupta",
                "Dhruv V Patel",
                "Deep Ray",
                "Erik A Johnson",
                "Assad A Oberai"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04690v1",
                "http://arxiv.org/pdf/2310.04690v1"
            ],
            "primary_category": "cs.CE",
            "category": [
                "cs.CE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04686v1",
            "title": "Tight Rates in Supervised Outlier Transfer Learning",
            "updated": "2023-10-07T04:14:13Z",
            "published": "2023-10-07T04:14:13Z",
            "summary": "A critical barrier to learning an accurate decision rule for outlier\ndetection is the scarcity of outlier data. As such, practitioners often turn to\nthe use of similar but imperfect outlier data from which they might transfer\ninformation to the target outlier detection task. Despite the recent empirical\nsuccess of transfer learning approaches in outlier detection, a fundamental\nunderstanding of when and how knowledge can be transferred from a source to a\ntarget outlier detection task remains elusive. In this work, we adopt the\ntraditional framework of Neyman-Pearson classification -- which formalizes\nsupervised outlier detection -- with the added assumption that one has access\nto some related but imperfect outlier data. Our main results are as follows:\n  We first determine the information-theoretic limits of the problem under a\nmeasure of discrepancy that extends some existing notions from traditional\nbalanced classification; interestingly, unlike in balanced classification,\nseemingly very dissimilar sources can provide much information about a target,\nthus resulting in fast transfer.\n  We then show that, in principle, these information-theoretic limits are\nachievable by adaptive procedures, i.e., procedures with no a priori\ninformation on the discrepancy between source and target outlier distributions.",
            "author": [
                "Mohammadreza M. Kalan",
                "Samory Kpotufe"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04686v1",
                "http://arxiv.org/pdf/2310.04686v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04677v2",
            "title": "AG-CRC: Anatomy-Guided Colorectal Cancer Segmentation in CT with\n  Imperfect Anatomical Knowledge",
            "updated": "2023-12-01T03:45:44Z",
            "published": "2023-10-07T03:22:06Z",
            "summary": "When delineating lesions from medical images, a human expert can always keep\nin mind the anatomical structure behind the voxels. However, although\nhigh-quality (though not perfect) anatomical information can be retrieved from\ncomputed tomography (CT) scans with modern deep learning algorithms, it is\nstill an open problem how these automatically generated organ masks can assist\nin addressing challenging lesion segmentation tasks, such as the segmentation\nof colorectal cancer (CRC). In this paper, we develop a novel Anatomy-Guided\nsegmentation framework to exploit the auto-generated organ masks to aid CRC\nsegmentation from CT, namely AG-CRC. First, we obtain multi-organ segmentation\n(MOS) masks with existing MOS models (e.g., TotalSegmentor) and further derive\na more robust organ of interest (OOI) mask that may cover most of the\ncolon-rectum and CRC voxels. Then, we propose an anatomy-guided training patch\nsampling strategy by optimizing a heuristic gain function that considers both\nthe proximity of important regions (e.g., the tumor or organs of interest) and\nsample diversity. Third, we design a novel self-supervised learning scheme\ninspired by the topology of tubular organs like the colon to boost the model\nperformance further. Finally, we employ a masked loss scheme to guide the model\nto focus solely on the essential learning region. We extensively evaluate the\nproposed method on two CRC segmentation datasets, where substantial performance\nimprovement (5% to 9% in Dice) is achieved over current state-of-the-art\nmedical image segmentation models, and the ablation studies further evidence\nthe efficacy of every proposed component.",
            "author": [
                "Rongzhao Zhang",
                "Zhian Bai",
                "Ruoying Yu",
                "Wenrao Pang",
                "Lingyun Wang",
                "Lifeng Zhu",
                "Xiaofan Zhang",
                "Huan Zhang",
                "Weiguo Hu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04677v2",
                "http://arxiv.org/pdf/2310.04677v2"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04676v1",
            "title": "Surgical Gym: A high-performance GPU-based platform for reinforcement\n  learning with surgical robots",
            "updated": "2023-10-07T03:21:58Z",
            "published": "2023-10-07T03:21:58Z",
            "summary": "Recent advances in robot-assisted surgery have resulted in progressively more\nprecise, efficient, and minimally invasive procedures, sparking a new era of\nrobotic surgical intervention. This enables doctors, in collaborative\ninteraction with robots, to perform traditional or minimally invasive surgeries\nwith improved outcomes through smaller incisions. Recent efforts are working\ntoward making robotic surgery more autonomous which has the potential to reduce\nvariability of surgical outcomes and reduce complication rates. Deep\nreinforcement learning methodologies offer scalable solutions for surgical\nautomation, but their effectiveness relies on extensive data acquisition due to\nthe absence of prior knowledge in successfully accomplishing tasks. Due to the\nintensive nature of simulated data collection, previous works have focused on\nmaking existing algorithms more efficient. In this work, we focus on making the\nsimulator more efficient, making training data much more accessible than\npreviously possible. We introduce Surgical Gym, an open-source high performance\nplatform for surgical robot learning where both the physics simulation and\nreinforcement learning occur directly on the GPU. We demonstrate between\n100-5000x faster training times compared with previous surgical learning\nplatforms. The code is available at:\nhttps://github.com/SamuelSchmidgall/SurgicalGym.",
            "author": [
                "Samuel Schmidgall",
                "Axel Krieger",
                "Jason Eshraghian"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04676v1",
                "http://arxiv.org/pdf/2310.04676v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04668v2",
            "title": "Label-free Node Classification on Graphs with Large Language Models\n  (LLMS)",
            "updated": "2023-10-12T18:34:08Z",
            "published": "2023-10-07T03:14:11Z",
            "summary": "In recent years, there have been remarkable advancements in node\nclassification achieved by Graph Neural Networks (GNNs). However, they\nnecessitate abundant high-quality labels to ensure promising performance. In\ncontrast, Large Language Models (LLMs) exhibit impressive zero-shot proficiency\non text-attributed graphs. Yet, they face challenges in efficiently processing\nstructural data and suffer from high inference costs. In light of these\nobservations, this work introduces a label-free node classification on graphs\nwith LLMs pipeline, LLM-GNN. It amalgamates the strengths of both GNNs and LLMs\nwhile mitigating their limitations. Specifically, LLMs are leveraged to\nannotate a small portion of nodes and then GNNs are trained on LLMs'\nannotations to make predictions for the remaining large portion of nodes. The\nimplementation of LLM-GNN faces a unique challenge: how can we actively select\nnodes for LLMs to annotate and consequently enhance the GNN training? How can\nwe leverage LLMs to obtain annotations of high quality, representativeness, and\ndiversity, thereby enhancing GNN performance with less cost? To tackle this\nchallenge, we develop an annotation quality heuristic and leverage the\nconfidence scores derived from LLMs to advanced node selection. Comprehensive\nexperimental results validate the effectiveness of LLM-GNN. In particular,\nLLM-GNN can achieve an accuracy of 74.9% on a vast-scale dataset \\products with\na cost less than 1 dollar.",
            "author": [
                "Zhikai Chen",
                "Haitao Mao",
                "Hongzhi Wen",
                "Haoyu Han",
                "Wei Jin",
                "Haiyang Zhang",
                "Hui Liu",
                "Jiliang Tang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04668v2",
                "http://arxiv.org/pdf/2310.04668v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04639v1",
            "title": "X-Transfer: A Transfer Learning-Based Framework for Robust GAN-Generated\n  Fake Image Detection",
            "updated": "2023-10-07T01:23:49Z",
            "published": "2023-10-07T01:23:49Z",
            "summary": "Generative adversarial networks (GANs) have remarkably advanced in diverse\ndomains, especially image generation and editing. However, the misuse of GANs\nfor generating deceptive images raises significant security concerns, including\nface replacement and fake accounts, which have gained widespread attention.\nConsequently, there is an urgent need for effective detection methods to\ndistinguish between real and fake images. Some of the current research centers\naround the application of transfer learning. Nevertheless, it encounters\nchallenges such as knowledge forgetting from the original dataset and\ninadequate performance when dealing with imbalanced data during training. To\nalleviate the above issues, this paper introduces a novel GAN-generated image\ndetection algorithm called X-Transfer. This model enhances transfer learning by\nutilizing two sibling neural networks that employ interleaved parallel gradient\ntransmission. This approach also effectively mitigates the problem of excessive\nknowledge forgetting. In addition, we combine AUC loss term and cross-entropy\nloss to enhance the model's performance comprehensively. The AUC loss\napproximates the AUC metric using WMW statistics, ensuring differentiability\nand improving the performance of traditional AUC evaluation. We carry out\ncomprehensive experiments on multiple facial image datasets. The results show\nthat our model outperforms the general transferring approach, and the best\naccuracy achieves 99.04%, which is increased by approximately 10%. Furthermore,\nwe demonstrate excellent performance on non-face datasets, validating its\ngenerality and broader application prospects.",
            "author": [
                "Lei Zhang",
                "Hao Chen",
                "Shu Hu",
                "Bin Zhu",
                "Xi Wu",
                "Jinrong Hu",
                "Xin Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04639v1",
                "http://arxiv.org/pdf/2310.04639v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04633v3",
            "title": "Unbiased and Robust: External Attention-enhanced Graph Contrastive\n  Learning for Cross-domain Sequential Recommendation",
            "updated": "2023-10-17T13:36:33Z",
            "published": "2023-10-07T01:00:40Z",
            "summary": "Cross-domain sequential recommenders (CSRs) are gaining considerable research\nattention as they can capture user sequential preference by leveraging side\ninformation from multiple domains. However, these works typically follow an\nideal setup, i.e., different domains obey similar data distribution, which\nignores the bias brought by asymmetric interaction densities (a.k.a. the\ninter-domain density bias). Besides, the frequently adopted mechanism (e.g.,\nthe self-attention network) in sequence encoder only focuses on the\ninteractions within a local view, which overlooks the global correlations\nbetween different training batches. To this end, we propose an External\nAttention-enhanced Graph Contrastive Learning framework, namely EA-GCL.\nSpecifically, to remove the impact of the inter-domain density bias, an\nauxiliary Self-Supervised Learning (SSL) task is attached to the traditional\ngraph encoder under a multi-task learning manner. To robustly capture users'\nbehavioral patterns, we develop an external attention-based sequence encoder\nthat contains an MLP-based memory-sharing structure. Unlike the self-attention\nmechanism, such a structure can effectively alleviate the bias interference\nfrom the batch-based training scheme. Extensive experiments on two real-world\ndatasets demonstrate that EA-GCL outperforms several state-of-the-art baselines\non CSR tasks. The source codes and relevant datasets are available at\nhttps://github.com/HoupingY/EA-GCL.",
            "author": [
                "Xinhua Wang",
                "Houping Yue",
                "Zizheng Wang",
                "Liancheng Xu",
                "Jinyu Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04633v3",
                "http://arxiv.org/pdf/2310.04633v3"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04627v1",
            "title": "Profit: Benchmarking Personalization and Robustness Trade-off in\n  Federated Prompt Tuning",
            "updated": "2023-10-06T23:46:33Z",
            "published": "2023-10-06T23:46:33Z",
            "summary": "In many applications of federated learning (FL), clients desire models that\nare personalized using their local data, yet are also robust in the sense that\nthey retain general global knowledge. However, the presence of data\nheterogeneity across clients induces a fundamental trade-off between\npersonalization (i.e., adaptation to a local distribution) and robustness\n(i.e., not forgetting previously learned general knowledge). It is critical to\nunderstand how to navigate this personalization vs robustness trade-off when\ndesigning federated systems, which are increasingly moving towards a paradigm\nof fine-tuning large foundation models. Due to limited computational and\ncommunication capabilities in most federated settings, this foundation model\nfine-tuning must be done using parameter-efficient fine-tuning (PEFT)\napproaches. While some recent work has studied federated approaches to PEFT,\nthe personalization vs robustness trade-off of federated PEFT has been largely\nunexplored. In this work, we take a step towards bridging this gap by\nbenchmarking fundamental FL algorithms -- FedAvg and FedSGD plus\npersonalization (via client local fine-tuning) -- applied to one of the most\nubiquitous PEFT approaches to large language models (LLMs) -- prompt tuning --\nin a multitude of hyperparameter settings under varying levels of data\nheterogeneity. Our results show that federated-trained prompts can be\nsurprisingly robust when using a small learning rate with many local epochs for\npersonalization, especially when using an adaptive optimizer as the client\noptimizer during federated training. We also demonstrate that simple approaches\nsuch as adding regularization and interpolating two prompts are effective in\nimproving the personalization vs robustness trade-off in computation-limited\nsettings with few local updates allowed for personalization.",
            "author": [
                "Liam Collins",
                "Shanshan Wu",
                "Sewoong Oh",
                "Khe Chai Sim"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04627v1",
                "http://arxiv.org/pdf/2310.04627v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04625v1",
            "title": "Copy Suppression: Comprehensively Understanding an Attention Head",
            "updated": "2023-10-06T23:37:24Z",
            "published": "2023-10-06T23:37:24Z",
            "summary": "We present a single attention head in GPT-2 Small that has one main role\nacross the entire training distribution. If components in earlier layers\npredict a certain token, and this token appears earlier in the context, the\nhead suppresses it: we call this copy suppression. Attention Head 10.7 (L10H7)\nsuppresses naive copying behavior which improves overall model calibration.\nThis explains why multiple prior works studying certain narrow tasks found\nnegative heads that systematically favored the wrong answer. We uncover the\nmechanism that the Negative Heads use for copy suppression with weights-based\nevidence and are able to explain 76.9% of the impact of L10H7 in GPT-2 Small.\nTo the best of our knowledge, this is the most comprehensive description of the\ncomplete role of a component in a language model to date. One major effect of\ncopy suppression is its role in self-repair. Self-repair refers to how ablating\ncrucial model components results in downstream neural network parts\ncompensating for this ablation. Copy suppression leads to self-repair: if an\ninitial overconfident copier is ablated, then there is nothing to suppress. We\nshow that self-repair is implemented by several mechanisms, one of which is\ncopy suppression, which explains 39% of the behavior in a narrow task.\nInteractive visualisations of the copy suppression phenomena may be seen at our\nweb app https://copy-suppression.streamlit.app/",
            "author": [
                "Callum McDougall",
                "Arthur Conmy",
                "Cody Rushing",
                "Thomas McGrath",
                "Neel Nanda"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04625v1",
                "http://arxiv.org/pdf/2310.04625v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04621v1",
            "title": "Model Compression in Practice: Lessons Learned from Practitioners\n  Creating On-device Machine Learning Experiences",
            "updated": "2023-10-06T23:11:26Z",
            "published": "2023-10-06T23:11:26Z",
            "summary": "On-device machine learning (ML) promises to improve the privacy,\nresponsiveness, and proliferation of new, intelligent user experiences by\nmoving ML computation onto everyday personal devices. However, today's large ML\nmodels must be drastically compressed to run efficiently on-device, a hurtle\nthat requires deep, yet currently niche expertise. To engage the broader\nhuman-centered ML community in on-device ML experiences, we present the results\nfrom an interview study with 30 experts at Apple that specialize in producing\nefficient models. We compile tacit knowledge that experts have developed\nthrough practical experience with model compression across different hardware\nplatforms. Our findings offer pragmatic considerations missing from prior work,\ncovering the design process, trade-offs, and technical strategies that go into\ncreating efficient models. Finally, we distill design recommendations for\ntooling to help ease the difficulty of this work and bring on-device ML into to\nmore widespread practice.",
            "author": [
                "Fred Hohman",
                "Mary Beth Kery",
                "Donghao Ren",
                "Dominik Moritz"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04621v1",
                "http://arxiv.org/pdf/2310.04621v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04617v1",
            "title": "SlotGNN: Unsupervised Discovery of Multi-Object Representations and\n  Visual Dynamics",
            "updated": "2023-10-06T22:37:34Z",
            "published": "2023-10-06T22:37:34Z",
            "summary": "Learning multi-object dynamics from visual data using unsupervised techniques\nis challenging due to the need for robust, object representations that can be\nlearned through robot interactions. This paper presents a novel framework with\ntwo new architectures: SlotTransport for discovering object representations\nfrom RGB images and SlotGNN for predicting their collective dynamics from RGB\nimages and robot interactions. Our SlotTransport architecture is based on slot\nattention for unsupervised object discovery and uses a feature transport\nmechanism to maintain temporal alignment in object-centric representations.\nThis enables the discovery of slots that consistently reflect the composition\nof multi-object scenes. These slots robustly bind to distinct objects, even\nunder heavy occlusion or absence. Our SlotGNN, a novel unsupervised graph-based\ndynamics model, predicts the future state of multi-object scenes. SlotGNN\nlearns a graph representation of the scene using the discovered slots from\nSlotTransport and performs relational and spatial reasoning to predict the\nfuture appearance of each slot conditioned on robot actions. We demonstrate the\neffectiveness of SlotTransport in learning object-centric features that\naccurately encode both visual and positional information. Further, we highlight\nthe accuracy of SlotGNN in downstream robotic tasks, including challenging\nmulti-object rearrangement and long-horizon prediction. Finally, our\nunsupervised approach proves effective in the real world. With only minimal\nadditional data, our framework robustly predicts slots and their corresponding\ndynamics in real-world control tasks.",
            "author": [
                "Alireza Rezazadeh",
                "Athreyi Badithela",
                "Karthik Desingh",
                "Changhyun Choi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04617v1",
                "http://arxiv.org/pdf/2310.04617v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04613v2",
            "title": "ComDMFT v.2.0: Fully Self-Consistent ab initio GW+EDMFT for the\n  Electronic Structure of Correlated Quantum Materials",
            "updated": "2023-10-10T01:14:45Z",
            "published": "2023-10-06T22:11:40Z",
            "summary": "ComDMFT is a parallel computational package designed to study the electronic\nstructure of correlated quantum materials from first principles. Our approach\nis based on the combination of first-principles methods and dynamical mean\nfield theories. In version 2.0, we implemented fully-diagrammatic GW+EDMFT from\nfirst-principles. In this approach, correlated electrons are treated within\nfull GW+EDMFT and the rest are treated within full-GW, seamlessly. This\nimplementation enables the electronic structure calculation of quantum\nmaterials with weak, intermediate, and strong electron correlation without\nprior knowledge of the degree of electron correlation.",
            "author": [
                "Byungkyun Kang",
                "Patrick Semon",
                "Corey Melnick",
                "Gabriel Kotliar",
                "Sangkook Choi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04613v2",
                "http://arxiv.org/pdf/2310.04613v2"
            ],
            "primary_category": "cond-mat.str-el",
            "category": [
                "cond-mat.str-el",
                "cond-mat.mtrl-sci"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04612v1",
            "title": "A Topological Perspective on Demystifying GNN-Based Link Prediction\n  Performance",
            "updated": "2023-10-06T22:07:49Z",
            "published": "2023-10-06T22:07:49Z",
            "summary": "Graph Neural Networks (GNNs) have shown great promise in learning node\nembeddings for link prediction (LP). While numerous studies aim to improve the\noverall LP performance of GNNs, none have explored its varying performance\nacross different nodes and its underlying reasons. To this end, we aim to\ndemystify which nodes will perform better from the perspective of their local\ntopology. Despite the widespread belief that low-degree nodes exhibit poorer LP\nperformance, our empirical findings provide nuances to this viewpoint and\nprompt us to propose a better metric, Topological Concentration (TC), based on\nthe intersection of the local subgraph of each node with the ones of its\nneighbors. We empirically demonstrate that TC has a higher correlation with LP\nperformance than other node-level topological metrics like degree and subgraph\ndensity, offering a better way to identify low-performing nodes than using\ncold-start. With TC, we discover a novel topological distribution shift issue\nin which newly joined neighbors of a node tend to become less interactive with\nthat node's existing neighbors, compromising the generalizability of node\nembeddings for LP at testing time. To make the computation of TC scalable, We\nfurther propose Approximated Topological Concentration (ATC) and\ntheoretically/empirically justify its efficacy in approximating TC and reducing\nthe computation complexity. Given the positive correlation between node TC and\nits LP performance, we explore the potential of boosting LP performance via\nenhancing TC by re-weighting edges in the message-passing and discuss its\neffectiveness with limitations. Our code is publicly available at\nhttps://github.com/YuWVandy/Topo_LP_GNN.",
            "author": [
                "Yu Wang",
                "Tong Zhao",
                "Yuying Zhao",
                "Yunchao Liu",
                "Xueqi Cheng",
                "Neil Shah",
                "Tyler Derr"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04612v1",
                "http://arxiv.org/pdf/2310.04612v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04609v1",
            "title": "Kawasaki dynamics beyond the uniqueness threshold",
            "updated": "2023-10-06T22:04:09Z",
            "published": "2023-10-06T22:04:09Z",
            "summary": "Glauber dynamics of the Ising model on a random regular graph is known to mix\nfast below the tree uniqueness threshold and exponentially slowly above it. We\nshow that Kawasaki dynamics of the canonical ferromagnetic Ising model on a\nrandom $d$-regular graph mixes fast beyond the tree uniqueness threshold when\n$d$ is large enough (and conjecture that it mixes fast up to the tree\nreconstruction threshold for all $d\\geq 3$). This result follows from a more\ngeneral spectral condition for (modified) log-Sobolev inequalities for\nconservative dynamics of Ising models. The proof of this condition in fact\nextends to perturbations of distributions with log-concave generating\npolynomial.",
            "author": [
                "Roland Bauerschmidt",
                "Thierry Bodineau",
                "Benoit Dagallier"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04609v1",
                "http://arxiv.org/pdf/2310.04609v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR",
                "cs.DS",
                "math-ph",
                "math.MP"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04606v1",
            "title": "Robust Transfer Learning with Unreliable Source Data",
            "updated": "2023-10-06T21:50:21Z",
            "published": "2023-10-06T21:50:21Z",
            "summary": "This paper addresses challenges in robust transfer learning stemming from\nambiguity in Bayes classifiers and weak transferable signals between the target\nand source distribution. We introduce a novel quantity called the ''ambiguity\nlevel'' that measures the discrepancy between the target and source regression\nfunctions, propose a simple transfer learning procedure, and establish a\ngeneral theorem that shows how this new quantity is related to the\ntransferability of learning in terms of risk improvements. Our proposed\n''Transfer Around Boundary'' (TAB) model, with a threshold balancing the\nperformance of target and source data, is shown to be both efficient and\nrobust, improving classification while avoiding negative transfer. Moreover, we\ndemonstrate the effectiveness of the TAB model on non-parametric classification\nand logistic regression tasks, achieving upper bounds which are optimal up to\nlogarithmic factors. Simulation studies lend further support to the\neffectiveness of TAB. We also provide simple approaches to bound the excess\nmisclassification error without the need for specialized knowledge in transfer\nlearning.",
            "author": [
                "Jianqing Fan",
                "Cheng Gao",
                "Jason M. Klusowski"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04606v1",
                "http://arxiv.org/pdf/2310.04606v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG",
                "math.ST",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04598v1",
            "title": "A neuro-symbolic framework for answering conjunctive queries",
            "updated": "2023-10-06T21:31:17Z",
            "published": "2023-10-06T21:31:17Z",
            "summary": "The problem of answering logical queries over incomplete knowledge graphs is\nreceiving significant attention in the machine learning community.\nNeuro-symbolic models are a promising recent approach, showing good performance\nand allowing for good interpretability properties. These models rely on trained\narchitectures to execute atomic queries, combining them with modules that\nsimulate the symbolic operators in queries. Unfortunately, most neuro-symbolic\nquery processors are limited to the so-called tree-like logical queries that\nadmit a bottom-up execution, where the leaves are constant values or anchors,\nand the root is the target variable. Tree-like queries, while expressive, fail\nshort to express properties in knowledge graphs that are important in practice,\nsuch as the existence of multiple edges between entities or the presence of\ntriangles.\n  We propose a framework for answering arbitrary conjunctive queries over\nincomplete knowledge graphs. The main idea of our method is to approximate a\ncyclic query by an infinite family of tree-like queries, and then leverage\nexisting models for the latter. Our approximations achieve strong guarantees:\nthey are complete, i.e. there are no false negatives, and optimal, i.e. they\nprovide the best possible approximation using tree-like queries. Our method\nrequires the approximations to be tree-like queries where the leaves are\nanchors or existentially quantified variables. Hence, we also show how some of\nthe existing neuro-symbolic models can handle these queries, which is of\nindependent interest. Experiments show that our approximation strategy achieves\ncompetitive results, and that including queries with existentially quantified\nvariables tends to improve the general performance of these models, both on\ntree-like queries and on our approximation strategy.",
            "author": [
                "Pablo Barcel\u00f3",
                "Tamara Cucumides",
                "Floris Geerts",
                "Juan Reutter",
                "Miguel Romero"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04598v1",
                "http://arxiv.org/pdf/2310.04598v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.DB"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04586v1",
            "title": "TrialView: An AI-powered Visual Analytics System for Temporal Event Data\n  in Clinical Trials",
            "updated": "2023-10-06T20:58:15Z",
            "published": "2023-10-06T20:58:15Z",
            "summary": "Randomized controlled trials (RCT) are the gold standards for evaluating the\nefficacy and safety of therapeutic interventions in human subjects. In addition\nto the pre-specified endpoints, trial participants' experience reveals the time\ncourse of the intervention. Few analytical tools exist to summarize and\nvisualize the individual experience of trial participants. Visual analytics\nallows integrative examination of temporal event patterns of patient\nexperience, thus generating insights for better care decisions. Towards this\nend, we introduce TrialView, an information system that combines graph\nartificial intelligence (AI) and visual analytics to enhance the dissemination\nof trial data. TrialView offers four distinct yet interconnected views:\nIndividual, Cohort, Progression, and Statistics, enabling an interactive\nexploration of individual and group-level data. The TrialView system is a\ngeneral-purpose analytical tool for a broad class of clinical trials. The\nsystem is powered by graph AI, knowledge-guided clustering, explanatory\nmodeling, and graph-based agglomeration algorithms. We demonstrate the system's\neffectiveness in analyzing temporal event data through a case study.",
            "author": [
                "Zuotian Li",
                "Xiang Liu",
                "Zelei Cheng",
                "Yingjie Chen",
                "Wanzhu Tu",
                "Jing Su"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04586v1",
                "http://arxiv.org/pdf/2310.04586v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04584v1",
            "title": "An Algorithm to Train Unrestricted Sequential Discrete Morphological\n  Neural Networks",
            "updated": "2023-10-06T20:55:05Z",
            "published": "2023-10-06T20:55:05Z",
            "summary": "With the advent of deep learning, there have been attempts to insert\nmathematical morphology (MM) operators into convolutional neural networks\n(CNN), and the most successful endeavor to date has been the morphological\nneural networks (MNN). Although MNN have performed better than CNN in solving\nsome problems, they inherit their black-box nature. Furthermore, in the case of\nbinary images, they are approximations, which loose the Boolean lattice\nstructure of MM operators and, thus, it is not possible to represent a specific\nclass of W-operators with desired properties. In a recent work, we proposed the\nDiscrete Morphological Neural Networks (DMNN) for binary image transformation\nto represent specific classes of W-operators and estimate them via machine\nlearning. We also proposed a stochastic lattice gradient descent algorithm\n(SLGDA) to learn the parameters of Canonical Discrete Morphological Neural\nNetworks (CDMNN), whose architecture is composed only of operators that can be\ndecomposed as the supremum, infimum, and complement of erosions and dilations.\nIn this paper, we propose an algorithm to learn unrestricted sequential DMNN\n(USDMNN), whose architecture is given by the composition of general\nW-operators. We consider the representation of a W-operator by its\ncharacteristic Boolean function, and then learn it via a SLGDA in the Boolean\nlattice of functions. Although both the CDMNN and USDMNN have the Boolean\nlattice structure, USDMNN are not as dependent on prior information about the\nproblem at hand, and may be more suitable in instances in which the\npractitioner does not have strong domain knowledge. We illustrate the algorithm\nin a practical example.",
            "author": [
                "Diego Marcondes",
                "Mariana Feldman",
                "Junior Barrera"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04584v1",
                "http://arxiv.org/pdf/2310.04584v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04562v1",
            "title": "Towards Foundation Models for Knowledge Graph Reasoning",
            "updated": "2023-10-06T20:00:07Z",
            "published": "2023-10-06T20:00:07Z",
            "summary": "Foundation models in language and vision have the ability to run inference on\nany textual and visual inputs thanks to the transferable representations such\nas a vocabulary of tokens in language. Knowledge graphs (KGs) have different\nentity and relation vocabularies that generally do not overlap. The key\nchallenge of designing foundation models on KGs is to learn such transferable\nrepresentations that enable inference on any graph with arbitrary entity and\nrelation vocabularies. In this work, we make a step towards such foundation\nmodels and present ULTRA, an approach for learning universal and transferable\ngraph representations. ULTRA builds relational representations as a function\nconditioned on their interactions. Such a conditioning strategy allows a\npre-trained ULTRA model to inductively generalize to any unseen KG with any\nrelation vocabulary and to be fine-tuned on any graph. Conducting link\nprediction experiments on 57 different KGs, we find that the zero-shot\ninductive inference performance of a single pre-trained ULTRA model on unseen\ngraphs of various sizes is often on par or better than strong baselines trained\non specific graphs. Fine-tuning further boosts the performance.",
            "author": [
                "Mikhail Galkin",
                "Xinyu Yuan",
                "Hesham Mostafa",
                "Jian Tang",
                "Zhaocheng Zhu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04562v1",
                "http://arxiv.org/pdf/2310.04562v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04560v1",
            "title": "Talk like a Graph: Encoding Graphs for Large Language Models",
            "updated": "2023-10-06T19:55:21Z",
            "published": "2023-10-06T19:55:21Z",
            "summary": "Graphs are a powerful tool for representing and analyzing complex\nrelationships in real-world applications such as social networks, recommender\nsystems, and computational finance. Reasoning on graphs is essential for\ndrawing inferences about the relationships between entities in a complex\nsystem, and to identify hidden patterns and trends. Despite the remarkable\nprogress in automated reasoning with natural text, reasoning on graphs with\nlarge language models (LLMs) remains an understudied problem. In this work, we\nperform the first comprehensive study of encoding graph-structured data as text\nfor consumption by LLMs. We show that LLM performance on graph reasoning tasks\nvaries on three fundamental levels: (1) the graph encoding method, (2) the\nnature of the graph task itself, and (3) interestingly, the very structure of\nthe graph considered. These novel results provide valuable insight on\nstrategies for encoding graphs as text. Using these insights we illustrate how\nthe correct choice of encoders can boost performance on graph reasoning tasks\ninside LLMs by 4.8% to 61.8%, depending on the task.",
            "author": [
                "Bahare Fatemi",
                "Jonathan Halcrow",
                "Bryan Perozzi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04560v1",
                "http://arxiv.org/pdf/2310.04560v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04540v1",
            "title": "Multi-decadal Sea Level Prediction using Neural Networks and Spectral\n  Clustering on Climate Model Large Ensembles and Satellite Altimeter Data",
            "updated": "2023-10-06T19:06:43Z",
            "published": "2023-10-06T19:06:43Z",
            "summary": "Sea surface height observations provided by satellite altimetry since 1993\nshow a rising rate (3.4 mm/year) for global mean sea level. While on average,\nsea level has risen 10 cm over the last 30 years, there is considerable\nregional variation in the sea level change. Through this work, we predict sea\nlevel trends 30 years into the future at a 2-degree spatial resolution and\ninvestigate the future patterns of the sea level change. We show the potential\nof machine learning (ML) in this challenging application of long-term sea level\nforecasting over the global ocean. Our approach incorporates sea level data\nfrom both altimeter observations and climate model simulations. We develop a\nsupervised learning framework using fully connected neural networks (FCNNs)\nthat can predict the sea level trend based on climate model projections.\nAlongside this, our method provides uncertainty estimates associated with the\nML prediction. We also show the effectiveness of partitioning our spatial\ndataset and learning a dedicated ML model for each segmented region. We compare\ntwo partitioning strategies: one achieved using domain knowledge, and the other\nemploying spectral clustering. Our results demonstrate that segmenting the\nspatial dataset with spectral clustering improves the ML predictions.",
            "author": [
                "Saumya Sinha",
                "John Fasullo",
                "R. Steven Nerem",
                "Claire Monteleoni"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04540v1",
                "http://arxiv.org/pdf/2310.04540v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04537v1",
            "title": "Melonic Radiative Correction in Four-Dimensional Spinfoam Model with\n  Cosmological Constant",
            "updated": "2023-10-06T19:03:05Z",
            "published": "2023-10-06T19:03:05Z",
            "summary": "Infrared divergence is a common feature of spinfoam models with a vanishing\ncosmological constant but is expected to disappear in presence of a\nnon-vanishing cosmological constant. In this paper, we investigate the spinfoam\namplitude with cosmological constant introduced in arXiv:2109.00034 on the\nmelon graph, which is known as the melonic radiative correction. The amplitude\nclosely relates to the state-integral model of complex Chern-Simons theory. We\nprove that the melonic radiative correction is finite in presence of a\nnon-vanishing cosmological constant, in contrast to the infrared divergence of\nspinfoam models with a vanishing cosmological constant. In addition, we also\nanalyze the scaling behavior of the radiative correction in the limit of small\ncosmological constant.",
            "author": [
                "Muxin Han",
                "Qiaoyin Pan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04537v1",
                "http://arxiv.org/pdf/2310.04537v1"
            ],
            "primary_category": "gr-qc",
            "category": [
                "gr-qc",
                "hep-th"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04535v1",
            "title": "LLM4DV: Using Large Language Models for Hardware Test Stimuli Generation",
            "updated": "2023-10-06T19:02:04Z",
            "published": "2023-10-06T19:02:04Z",
            "summary": "Test stimuli generation has been a crucial but labor-intensive task in\nhardware design verification. In this paper, we revolutionize this process by\nharnessing the power of large language models (LLMs) and present a novel\nbenchmarking framework, LLM4DV. This framework introduces a prompt template for\ninteractively eliciting test stimuli from the LLM, along with four innovative\nprompting improvements to support the pipeline execution and further enhance\nits performance. We compare LLM4DV to traditional constrained-random testing\n(CRT), using three self-designed design-under-test (DUT) modules. Experiments\ndemonstrate that LLM4DV excels in efficiently handling straightforward DUT\nscenarios, leveraging its ability to employ basic mathematical reasoning and\npre-trained knowledge. While it exhibits reduced efficiency in complex task\nsettings, it still outperforms CRT in relative terms. The proposed framework\nand the DUT modules used in our experiments will be open-sourced upon\npublication.",
            "author": [
                "Zixi Zhang",
                "Greg Chadwick",
                "Hugo McNally",
                "Yiren Zhao",
                "Robert Mullins"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04535v1",
                "http://arxiv.org/pdf/2310.04535v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04516v1",
            "title": "Vulnerability Analysis of Nonlinear Control Systems to Stealthy False\n  Data Injection Attacks",
            "updated": "2023-10-06T18:24:53Z",
            "published": "2023-10-06T18:24:53Z",
            "summary": "In this work, we focus on analyzing vulnerability of nonlinear dynamical\ncontrol systems to stealthy false data injection attacks on sensors. We start\nby defining the stealthiness notion in the most general form where an attack is\nconsidered stealthy if it would be undetected by any intrusion detector, i.e.,\nany intrusion detector could not do better than a random guess. Depending on\nthe level of attacker's knowledge about the plant model, controller, and the\nsystem states, two different attack models are considered. For each attack\nmodel, we derive the conditions for which the system will be vulnerable to\nstealthy impactful attacks, in addition to finding a methodology for designing\nsuch sequence of false data injection attacks. When the attacker has complete\nknowledge about the system, we show that if the closed loop system is\nincrementally exponentially stable while the open loop plant is incrementally\nunstable, then the system is vulnerable to stealthy yet impactful attacks on\nsensors. However, in the second attack model, with less knowledge about the\nsystem, additional conditions need to be satisfied and the level of\nstealthiness depends on the accuracy of attacker's knowledge about the system.\nWe also consider the impact of stealthy attacks on state estimation, and show\nthat if the closed loop control system including the estimator is incrementally\nstable, then the state estimation in the presence of attack converges to the\nattack free estimates. Finally, we illustrate our results on numerical case\nstudies.",
            "author": [
                "Amir Khazraei",
                "Miroslav Pajic"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04516v1",
                "http://arxiv.org/pdf/2310.04516v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04489v2",
            "title": "Extracting linear and nonlinear quasinormal modes from black hole merger\n  simulations",
            "updated": "2023-10-25T21:21:26Z",
            "published": "2023-10-06T18:00:00Z",
            "summary": "In general relativity, when two black holes merge they produce a rotating\n(Kerr) black hole remnant. According to perturbation theory, the remnant emits\n\"ringdown\" radiation: a superposition of exponentials with characteristic\ncomplex frequencies that depend only on the remnant's mass and spin. While the\ngoal of the black hole spectroscopy program is to measure the quasinormal mode\nfrequencies, a knowledge of their amplitudes and phases is equally important to\ndetermine which modes are detectable, and possibly to perform additional\nconsistency checks. Unlike the complex frequencies, the amplitudes and phases\ndepend on the properties of the binary progenitors, such as the binary mass\nratio and component spins. In this paper we develop a fitting algorithm\ndesigned to reliably identify the modes present in numerical simulations and to\nextract their amplitudes and phases. We apply the algorithm to over 500 binary\nblack hole simulations from the public SXS numerical relativity simulation\ncatalog, and we present fitting formulas for the resulting mode amplitudes and\nphases as functions of the properties of the progenitors. Crucially, our\nalgorithm allows for the extraction of not only prograde fundamental modes and\novertones, but also retrograde modes and second-order modes. We unveil\ninteresting relations for the amplitude ratios of different modes. The fitting\ncode and interactive versions of some of the plots are publicly available. The\nresults presented in this paper can be updated as more and better simulations\nbecome available.",
            "author": [
                "Mark Ho-Yeuk Cheung",
                "Emanuele Berti",
                "Vishal Baibhav",
                "Roberto Cotesta"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04489v2",
                "http://arxiv.org/pdf/2310.04489v2"
            ],
            "primary_category": "gr-qc",
            "category": [
                "gr-qc",
                "astro-ph.HE",
                "hep-th"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04407v1",
            "title": "Policy-Gradient Training of Language Models for Ranking",
            "updated": "2023-10-06T17:55:23Z",
            "published": "2023-10-06T17:55:23Z",
            "summary": "Text retrieval plays a crucial role in incorporating factual knowledge for\ndecision making into language processing pipelines, ranging from chat-based web\nsearch to question answering systems. Current state-of-the-art text retrieval\nmodels leverage pre-trained large language models (LLMs) to achieve competitive\nperformance, but training LLM-based retrievers via typical contrastive losses\nrequires intricate heuristics, including selecting hard negatives and using\nadditional supervision as learning signals. This reliance on heuristics stems\nfrom the fact that the contrastive loss itself is heuristic and does not\ndirectly optimize the downstream metrics of decision quality at the end of the\nprocessing pipeline. To address this issue, we introduce Neural PG-RANK, a\nnovel training algorithm that learns to rank by instantiating a LLM as a\nPlackett-Luce ranking policy. Neural PG-RANK provides a principled method for\nend-to-end training of retrieval models as part of larger decision systems via\npolicy gradient, with little reliance on complex heuristics, and it effectively\nunifies the training objective with downstream decision-making quality. We\nconduct extensive experiments on various text retrieval benchmarks. The results\ndemonstrate that when the training objective aligns with the evaluation setup,\nNeural PG-RANK yields remarkable in-domain performance improvement, with\nsubstantial out-of-domain generalization to some critical datasets employed in\ndownstream question answering tasks.",
            "author": [
                "Ge Gao",
                "Jonathan D. Chang",
                "Claire Cardie",
                "Kiant\u00e9 Brantley",
                "Thorsten Joachim"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04407v1",
                "http://arxiv.org/pdf/2310.04407v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.IR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04398v1",
            "title": "Analysis and Algorithmic Construction of Self-Assembled DNA Complexes",
            "updated": "2023-10-06T17:43:20Z",
            "published": "2023-10-06T17:43:20Z",
            "summary": "DNA self-assembly is an important tool that has a wide range of applications\nsuch as building nanostructures, the transport of target virotherapies, and\nnano-circuitry. Tools from graph theory can be used to encode the biological\nprocess of DNA self-assembly. The principle component of this process is to\nexamine collections of branched junction molecules, called pots, and study the\ntypes of structures that can be constructed. We restrict our attention to pots\nwhich contain one set of complementary cohesive-ends, i.e. a single bond-edge\ntype, and we identify the types and sizes of structures that can be built from\nsuch a pot. In particular, we show a dependence between the order of graphs in\nthe output of the pot and the number of arms on the corresponding tiles.\nFurthermore, we provide two algorithms which will construct complete complexes\nfor a pot with a single bond-edge type.",
            "author": [
                "Cory Johnson",
                "Andrew Lavengood-Ryan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04398v1",
                "http://arxiv.org/pdf/2310.04398v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C90, 05C85, 92E10"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04391v1",
            "title": "On a Hierarchy of Spectral Isomorphism Invariants",
            "updated": "2023-10-06T17:30:58Z",
            "published": "2023-10-06T17:30:58Z",
            "summary": "We consider a hierarchy of graph invariants that naturally extends the\nspectral invariants defined by F\\\"urer (Lin. Alg. Appl. 2010) based on angles\nbetween the projections of standard basis vectors onto an eigenspace of the\nadjacency matrix of a graph. We provide a purely combinatorial characterization\nof this hierarchy in terms of the walk counts. This allows us to give a\ncomplete answer to F\\\"urer's question about the strength of his invariants in\ndistinguishing non-isomorphic graphs in comparison to the 2-dimensional\nWeisfeiler-Leman algorithm, extending the recent work of Rattan and Seppelt\n(SODA 2023). As another application of the characterization, we prove that\nalmost all graphs are determined up to isomorphism by their eigenvalues and\nangles, which is closely related to the long-standing open problem whether\nalmost all graphs are determined by their spectrum. Finally, we describe the\nexact relationship between the hierarchy and the Weisfeiler-Leman algorithms\nfor small dimensions, as also some other important spectral characteristics of\na graph such as the generalized and the main spectra.",
            "author": [
                "V. Arvind",
                "Frank Fuhlbr\u00fcck",
                "Johannes K\u00f6bler",
                "Oleg Verbitsky"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04391v1",
                "http://arxiv.org/pdf/2310.04391v1"
            ],
            "primary_category": "cs.CC",
            "category": [
                "cs.CC",
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.05848v1",
            "title": "FMM-Head: Enhancing Autoencoder-based ECG anomaly detection with prior\n  knowledge",
            "updated": "2023-10-06T17:20:11Z",
            "published": "2023-10-06T17:20:11Z",
            "summary": "Detecting anomalies in electrocardiogram data is crucial to identifying\ndeviations from normal heartbeat patterns and providing timely intervention to\nat-risk patients. Various AutoEncoder models (AE) have been proposed to tackle\nthe anomaly detection task with ML. However, these models do not consider the\nspecific patterns of ECG leads and are unexplainable black boxes. In contrast,\nwe replace the decoding part of the AE with a reconstruction head (namely,\nFMM-Head) based on prior knowledge of the ECG shape. Our model consistently\nachieves higher anomaly detection capabilities than state-of-the-art models, up\nto 0.31 increase in area under the ROC curve (AUROC), with as little as half\nthe original model size and explainable extracted features. The processing time\nof our model is four orders of magnitude lower than solving an optimization\nproblem to obtain the same parameters, thus making it suitable for real-time\nECG parameters extraction and anomaly detection.",
            "author": [
                "Giacomo Verardo",
                "Magnus Boman",
                "Samuel Bruchfeld",
                "Marco Chiesa",
                "Sabine Koch",
                "Gerald Q. Maguire Jr.",
                "Dejan Kostic"
            ],
            "link": [
                "http://arxiv.org/abs/2310.05848v1",
                "http://arxiv.org/pdf/2310.05848v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "eess.SP",
                "I.2.0, J.3"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04376v2",
            "title": "Near-linear Time Dispersion of Mobile Agents",
            "updated": "2023-12-03T17:56:15Z",
            "published": "2023-10-06T17:07:16Z",
            "summary": "Consider that there are $k\\le n$ agents in a simple, connected, and\nundirected graph $G=(V,E)$ with $n$ nodes and $m$ edges. The goal of the\ndispersion problem is to move these $k$ agents to distinct nodes. Agents can\ncommunicate only when they are at the same node, and no other means of\ncommunication such as whiteboards are available. We assume that the agents\noperate synchronously. We consider two scenarios: when all agents are initially\nlocated at any single node (rooted setting) and when they are initially\ndistributed over any one or more nodes (general setting). Kshemkalyani and\nSharma presented a dispersion algorithm for the general setting, which uses\n$O(m_k)$ time and $\\log(k+\\delta)$ bits of memory per agent [OPODIS 2021].\nHere, $m_k$ is the maximum number of edges in any induced subgraph of $G$ with\n$k$ nodes, and $\\delta$ is the maximum degree of $G$. This algorithm is the\nfastest in the literature, as no algorithm with $o(m_k)$ time has been\ndiscovered even for the rooted setting. In this paper, we present faster\nalgorithms for both the rooted and general settings. First, we present an\nalgorithm for the rooted setting that solves the dispersion problem in $O(k\\log\n\\min(k,\\delta))=O(k\\log k)$ time using $O(\\log \\delta)$ bits of memory per\nagent. Next, we propose an algorithm for the general setting that achieves\ndispersion in $O(k (\\log k)\\cdot (\\log \\min(k,\\delta))=O(k \\log^2 k)$ time\nusing $O(\\log (k+\\delta))$ bits. Finally, for the rooted setting, we give a\ntime-optimal, i.e.,$O(k)$-time, algorithm with $O(\\delta)$ bits of space per\nagent.",
            "author": [
                "Yuichi Sudo",
                "Masahiro Shibata",
                "Junya Nakamura",
                "Yonghwan Kim",
                "Toshimitsu Masuzawa"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04376v2",
                "http://arxiv.org/pdf/2310.04376v2"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC",
                "cs.MA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04373v2",
            "title": "Confronting Reward Model Overoptimization with Constrained RLHF",
            "updated": "2023-10-10T15:01:11Z",
            "published": "2023-10-06T16:59:17Z",
            "summary": "Large language models are typically aligned with human preferences by\noptimizing $\\textit{reward models}$ (RMs) fitted to human feedback. However,\nhuman preferences are multi-faceted, and it is increasingly common to derive\nreward from a composition of simpler reward models which each capture a\ndifferent aspect of language quality. This itself presents a challenge, as it\nis difficult to appropriately weight these component RMs when combining them.\nCompounding this difficulty, because any RM is only a proxy for human\nevaluation, this process is vulnerable to $\\textit{overoptimization}$, wherein\npast a certain point, accumulating higher reward is associated with worse human\nratings. In this paper, we perform, to our knowledge, the first study on\noveroptimization in composite RMs, showing that correlation between component\nRMs has a significant effect on the locations of these points. We then\nintroduce an approach to solve this issue using constrained reinforcement\nlearning as a means of preventing the agent from exceeding each RM's threshold\nof usefulness. Our method addresses the problem of weighting component RMs by\nlearning dynamic weights, naturally expressed by Lagrange multipliers. As a\nresult, each RM stays within the range at which it is an effective proxy,\nimproving evaluation performance. Finally, we introduce an adaptive method\nusing gradient-free optimization to identify and optimize towards these points\nduring a single run.",
            "author": [
                "Ted Moskovitz",
                "Aaditya K. Singh",
                "DJ Strouse",
                "Tuomas Sandholm",
                "Ruslan Salakhutdinov",
                "Anca D. Dragan",
                "Stephen McAleer"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04373v2",
                "http://arxiv.org/pdf/2310.04373v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04363v1",
            "title": "Amortizing intractable inference in large language models",
            "updated": "2023-10-06T16:36:08Z",
            "published": "2023-10-06T16:36:08Z",
            "summary": "Autoregressive large language models (LLMs) compress knowledge from their\ntraining data through next-token conditional distributions. This limits\ntractable querying of this knowledge to start-to-end autoregressive sampling.\nHowever, many tasks of interest -- including sequence continuation, infilling,\nand other forms of constrained generation -- involve sampling from intractable\nposterior distributions. We address this limitation by using amortized Bayesian\ninference to sample from these intractable posteriors. Such amortization is\nalgorithmically achieved by fine-tuning LLMs via diversity-seeking\nreinforcement learning algorithms: generative flow networks (GFlowNets). We\nempirically demonstrate that this distribution-matching paradigm of LLM\nfine-tuning can serve as an effective alternative to maximum-likelihood\ntraining and reward-maximizing policy optimization. As an important\napplication, we interpret chain-of-thought reasoning as a latent variable\nmodeling problem and demonstrate that our approach enables data-efficient\nadaptation of LLMs to tasks that require multi-step rationalization and tool\nuse.",
            "author": [
                "Edward J. Hu",
                "Moksh Jain",
                "Eric Elmoznino",
                "Younesse Kaddar",
                "Guillaume Lajoie",
                "Yoshua Bengio",
                "Nikolay Malkin"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04363v1",
                "http://arxiv.org/pdf/2310.04363v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04358v1",
            "title": "Transferring speech-generic and depression-specific knowledge for\n  Alzheimer's disease detection",
            "updated": "2023-10-06T16:28:07Z",
            "published": "2023-10-06T16:28:07Z",
            "summary": "The detection of Alzheimer's disease (AD) from spontaneous speech has\nattracted increasing attention while the sparsity of training data remains an\nimportant issue. This paper handles the issue by knowledge transfer,\nspecifically from both speech-generic and depression-specific knowledge. The\npaper first studies sequential knowledge transfer from generic foundation\nmodels pretrained on large amounts of speech and text data. A block-wise\nanalysis is performed for AD diagnosis based on the representations extracted\nfrom different intermediate blocks of different foundation models. Apart from\nthe knowledge from speech-generic representations, this paper also proposes to\nsimultaneously transfer the knowledge from a speech depression detection task\nbased on the high comorbidity rates of depression and AD. A parallel knowledge\ntransfer framework is studied that jointly learns the information shared\nbetween these two tasks. Experimental results show that the proposed method\nimproves AD and depression detection, and produces a state-of-the-art F1 score\nof 0.928 for AD diagnosis on the commonly used ADReSSo dataset.",
            "author": [
                "Ziyun Cui",
                "Wen Wu",
                "Wei-Qiang Zhang",
                "Ji Wu",
                "Chao Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04358v1",
                "http://arxiv.org/pdf/2310.04358v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.SD",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04352v1",
            "title": "Fair Feature Importance Scores for Interpreting Tree-Based Methods and\n  Surrogates",
            "updated": "2023-10-06T16:21:21Z",
            "published": "2023-10-06T16:21:21Z",
            "summary": "Across various sectors such as healthcare, criminal justice, national\nsecurity, finance, and technology, large-scale machine learning (ML) and\nartificial intelligence (AI) systems are being deployed to make critical\ndata-driven decisions. Many have asked if we can and should trust these ML\nsystems to be making these decisions. Two critical components are prerequisites\nfor trust in ML systems: interpretability, or the ability to understand why the\nML system makes the decisions it does, and fairness, which ensures that ML\nsystems do not exhibit bias against certain individuals or groups. Both\ninterpretability and fairness are important and have separately received\nabundant attention in the ML literature, but so far, there have been very few\nmethods developed to directly interpret models with regard to their fairness.\nIn this paper, we focus on arguably the most popular type of ML interpretation:\nfeature importance scores. Inspired by the use of decision trees in knowledge\ndistillation, we propose to leverage trees as interpretable surrogates for\ncomplex black-box ML models. Specifically, we develop a novel fair feature\nimportance score for trees that can be used to interpret how each feature\ncontributes to fairness or bias in trees, tree-based ensembles, or tree-based\nsurrogates of any complex ML system. Like the popular mean decrease in impurity\nfor trees, our Fair Feature Importance Score is defined based on the mean\ndecrease (or increase) in group bias. Through simulations as well as real\nexamples on benchmark fairness datasets, we demonstrate that our Fair Feature\nImportance Score offers valid interpretations for both tree-based ensembles and\ntree-based surrogates of other ML systems.",
            "author": [
                "Camille Olivia Little",
                "Debolina Halder Lina",
                "Genevera I. Allen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04352v1",
                "http://arxiv.org/pdf/2310.04352v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04348v1",
            "title": "AccEq-DRT: Planning Demand-Responsive Transit to reduce inequality of\n  accessibility",
            "updated": "2023-10-06T16:13:28Z",
            "published": "2023-10-06T16:13:28Z",
            "summary": "Accessibility measures how well a location is connected to surrounding\nopportunities. We focus on accessibility provided by Public Transit (PT). There\nis an evident inequality in the distribution of accessibility between city\ncenters or close to main transportation corridors and suburbs. In the latter,\npoor PT service leads to a chronic car-dependency. Demand-Responsive Transit\n(DRT) is better suited for low-density areas than conventional fixed-route PT.\nHowever, its potential to tackle accessibility inequality has not yet been\nexploited. On the contrary, planning DRT without care to inequality (as in the\nmethods proposed so far) can further improve the accessibility gap in urban\nareas.\n  To the best of our knowledge this paper is the first to propose a DRT\nplanning strategy, which we call AccEq-DRT, aimed at reducing accessibility\ninequality, while ensuring overall efficiency. To this aim, we combine a graph\nrepresentation of conventional PT and a Continuous Approximation (CA) model of\nDRT. The two are combined in the same multi-layer graph, on which we compute\naccessibility. We then devise a scoring function to estimate the need of each\narea for an improvement, appropriately weighting population density and\naccessibility. Finally, we provide a bilevel optimization method, where the\nupper level is a heuristic to allocate DRT buses, guided by the scoring\nfunction, and the lower level performs traffic assignment. Numerical results in\na simplified model of Montreal show that inequality, measured with the Atkinson\nindex, is reduced by up to 34\\%.\n  Keywords: DRT Public, Transportation, Accessibility, Continuous\nApproximation, Network Design",
            "author": [
                "Duo Wang",
                "Andrea Araldo",
                "Mounim A. El Yacoubi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04348v1",
                "http://arxiv.org/pdf/2310.04348v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04332v1",
            "title": "On the Parameterized Complexity of Multiway Near-Separator",
            "updated": "2023-10-06T15:53:05Z",
            "published": "2023-10-06T15:53:05Z",
            "summary": "We study a new graph separation problem called Multiway Near-Separator. Given\nan undirected graph $G$, integer $k$, and terminal set $T \\subseteq V(G)$, it\nasks whether there is a vertex set $S \\subseteq V(G) \\setminus T$ of size at\nmost $k$ such that in graph $G-S$, no pair of distinct terminals can be\nconnected by two pairwise internally vertex-disjoint paths. Hence each terminal\npair can be separated in $G-S$ by removing at most one vertex. The problem is\ntherefore a generalization of (Node) Multiway Cut, which asks for a vertex set\nfor which each terminal is in a different component of $G-S$. We develop a\nfixed-parameter tractable algorithm for Multiway Near-Separator running in time\n$2^{O(k \\log k)} * n^{O(1)}$. Our algorithm is based on a new pushing lemma for\nsolutions with respect to important separators, along with two problem-specific\ningredients. The first is a polynomial-time subroutine to reduce the number of\nterminals in the instance to a polynomial in the solution size $k$ plus the\nsize of a given suboptimal solution. The second is a polynomial-time algorithm\nthat, given a graph $G$ and terminal set $T \\subseteq V(G)$ along with a single\nvertex $x \\in V(G)$ that forms a multiway near-separator, computes a\n14-approximation for the problem of finding a multiway near-separator not\ncontaining $x$.",
            "author": [
                "Bart M. P. Jansen",
                "Shivesh K. Roy"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04332v1",
                "http://arxiv.org/pdf/2310.04332v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04330v1",
            "title": "Lagrangian modeling of a non-homogeneous turbulent shear flow: Molding\n  homogeneous and isotropic trajectories into a jet",
            "updated": "2023-10-06T15:50:20Z",
            "published": "2023-10-06T15:50:20Z",
            "summary": "Turbulence is prevalent in nature and industry, from large-scale wave\ndynamics to small-scale combustion nozzle sprays. In addition to the\nmulti-scale nonlinear complexity and both randomness and coherent structures in\nits dynamics, practical turbulence is often non-homogeneous and anisotropic,\nleading to great modeling challenges. In this letter, an efficient model is\nproposed to predict turbulent jet statistics with high accuracy. The model\nleverages detailed knowledge of readily available velocity signals from\nidealized homogeneous turbulence and transforms them into Lagrangian\ntrajectories of a turbulent jet. The resulting spatio-temporal statistics are\ncompared against experimental jet data showing remarkable agreement at all\nscales. In particular the intermittency phenomenon is accurately mapped by the\nmodel to this inhomogeneous situation, as observed by higher-order moments and\nvelocity increment probability density functions. Crucial to the advancement of\nturbulence modeling, the transformation is simple to implement, with possible\nextensions to other inhomogeneous flows such as wind turbine wakes and canopy\nflows, to name a few.",
            "author": [
                "Bianca Viggiano",
                "Thomas Basset",
                "Micka\u00ebl Bourgoin",
                "Ra\u00fal Bayo\u00e1n Cal",
                "Laurent Chevillard",
                "Charles Meneveau",
                "Romain Volk"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04330v1",
                "http://arxiv.org/pdf/2310.04330v1"
            ],
            "primary_category": "physics.flu-dyn",
            "category": [
                "physics.flu-dyn"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04322v1",
            "title": "Three-dimensional characterization of the steel-concrete interface by\n  FIB-SEM nanotomography",
            "updated": "2023-10-06T15:34:02Z",
            "published": "2023-10-06T15:34:02Z",
            "summary": "While it is widely accepted that the steel-concrete interface (SCI) plays an\nimportant role in governing the long-term durability of reinforced concrete\nstructures, understanding about the primary features of the SCI that influence\ncorrosion degradation mechanisms has remained elusive. This lack of knowledge\ncan be attributed, on the one hand, to the complex heterogeneous nature of the\nSCI, and, on the other hand, the absence of experimental techniques suitable\nfor studying the relevant features of the SCI. Here, we use focused ion beam -\nscanning electron microscopy (FIB-SEM) nanotomography to obtain high resolution\n3D tomograms of the steel-concrete interfacial zone. Five tomograms, spanning\nvolumes ranging from 8,000 to 200,000 cubic micrometer, were acquired for\nsituations representative of both non-corroded and corroded SCIs. The achieved\nvoxel size falls within the range of 30-50 nm, thus providing a resolution\nclearly surpassing the capabilities of computed X-ray tomography. This\nresolution enables the 3D characterization of the microstructure at the\ncapillary scale, which is the scale at which relevant corrosion and related\nmass transport processes occur. Thus, FIB-SEM nanotomography is capable of\nyielding datasets of the SCI that serve as basis for the generation of digital\ntwins of the interfacial microstructure, thereby enabling future studies about\ndurability and corrosion of reinforced concrete at the pore scale.",
            "author": [
                "Nicolas Ruffray",
                "Ueli M. Angst",
                "Thilo Schmid",
                "Zhidong Zhang",
                "O. Burkan Isgor"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04322v1",
                "http://arxiv.org/pdf/2310.04322v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci",
                "physics.app-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04314v1",
            "title": "Latent Graph Inference with Limited Supervision",
            "updated": "2023-10-06T15:22:40Z",
            "published": "2023-10-06T15:22:40Z",
            "summary": "Latent graph inference (LGI) aims to jointly learn the underlying graph\nstructure and node representations from data features. However, existing LGI\nmethods commonly suffer from the issue of supervision starvation, where massive\nedge weights are learned without semantic supervision and do not contribute to\nthe training loss. Consequently, these supervision-starved weights, which may\ndetermine the predictions of testing samples, cannot be semantically optimal,\nresulting in poor generalization. In this paper, we observe that this issue is\nactually caused by the graph sparsification operation, which severely destroys\nthe important connections established between pivotal nodes and labeled ones.\nTo address this, we propose to restore the corrupted affinities and replenish\nthe missed supervision for better LGI. The key challenge then lies in\nidentifying the critical nodes and recovering the corrupted affinities. We\nbegin by defining the pivotal nodes as $k$-hop starved nodes, which can be\nidentified based on a given adjacency matrix. Considering the high\ncomputational burden, we further present a more efficient alternative inspired\nby CUR matrix decomposition. Subsequently, we eliminate the starved nodes by\nreconstructing the destroyed connections. Extensive experiments on\nrepresentative benchmarks demonstrate that reducing the starved nodes\nconsistently improves the performance of state-of-the-art LGI methods,\nespecially under extremely limited supervision (6.12% improvement on Pubmed\nwith a labeling rate of only 0.3%).",
            "author": [
                "Jianglin Lu",
                "Yi Xu",
                "Huan Wang",
                "Yue Bai",
                "Yun Fu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04314v1",
                "http://arxiv.org/pdf/2310.04314v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04303v1",
            "title": "Kernelization for Counting Problems on Graphs: Preserving the Number of\n  Minimum Solutions",
            "updated": "2023-10-06T15:04:09Z",
            "published": "2023-10-06T15:04:09Z",
            "summary": "A kernelization for a parameterized decision problem $\\mathcal{Q}$ is a\npolynomial-time preprocessing algorithm that reduces any parameterized instance\n$(x,k)$ into an instance $(x',k')$ whose size is bounded by a function of $k$\nalone and which has the same yes/no answer for $\\mathcal{Q}$. Such\npreprocessing algorithms cannot exist in the context of counting problems, when\nthe answer to be preserved is the number of solutions, since this number can be\narbitrarily large compared to $k$. However, we show that for counting minimum\nfeedback vertex sets of size at most $k$, and for counting minimum dominating\nsets of size at most $k$ in a planar graph, there is a polynomial-time\nalgorithm that either outputs the answer or reduces to an instance $(G',k')$ of\nsize polynomial in $k$ with the same number of minimum solutions. This shows\nthat a meaningful theory of kernelization for counting problems is possible and\nopens the door for future developments. Our algorithms exploit that if the\nnumber of solutions exceeds $2^{\\mathsf{poly}(k)}$, the size of the input is\nexponential in terms of $k$ so that the running time of a parameterized\ncounting algorithm can be bounded by $\\mathsf{poly}(n)$. Otherwise, we can use\ngadgets that slightly increase $k$ to represent choices among $2^{O(k)}$\noptions by only $\\mathsf{poly}(k)$ vertices.",
            "author": [
                "Bart M. P. Jansen",
                "Bart van der Steenhoven"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04303v1",
                "http://arxiv.org/pdf/2310.04303v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "68Q27, 05C69,",
                "F.2.2"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04294v1",
            "title": "Graph learning in robotics: a survey",
            "updated": "2023-10-06T14:52:25Z",
            "published": "2023-10-06T14:52:25Z",
            "summary": "Deep neural networks for graphs have emerged as a powerful tool for learning\non complex non-euclidean data, which is becoming increasingly common for a\nvariety of different applications. Yet, although their potential has been\nwidely recognised in the machine learning community, graph learning is largely\nunexplored for downstream tasks such as robotics applications. To fully unlock\ntheir potential, hence, we propose a review of graph neural architectures from\na robotics perspective. The paper covers the fundamentals of graph-based\nmodels, including their architecture, training procedures, and applications. It\nalso discusses recent advancements and challenges that arise in applied\nsettings, related for example to the integration of perception,\ndecision-making, and control. Finally, the paper provides an extensive review\nof various robotic applications that benefit from learning on graph structures,\nsuch as bodies and contacts modelling, robotic manipulation, action\nrecognition, fleet motion planning, and many more. This survey aims to provide\nreaders with a thorough understanding of the capabilities and limitations of\ngraph neural architectures in robotics, and to highlight potential avenues for\nfuture research.",
            "author": [
                "Francesca Pistilli",
                "Giuseppe Averta"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04294v1",
                "http://arxiv.org/pdf/2310.04294v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04292v3",
            "title": "Towards Foundational Models for Molecular Learning on Large-Scale\n  Multi-Task Datasets",
            "updated": "2023-10-18T11:06:43Z",
            "published": "2023-10-06T14:51:17Z",
            "summary": "Recently, pre-trained foundation models have enabled significant advancements\nin multiple fields. In molecular machine learning, however, where datasets are\noften hand-curated, and hence typically small, the lack of datasets with\nlabeled features, and codebases to manage those datasets, has hindered the\ndevelopment of foundation models. In this work, we present seven novel datasets\ncategorized by size into three distinct categories: ToyMix, LargeMix and\nUltraLarge. These datasets push the boundaries in both the scale and the\ndiversity of supervised labels for molecular learning. They cover nearly 100\nmillion molecules and over 3000 sparsely defined tasks, totaling more than 13\nbillion individual labels of both quantum and biological nature. In comparison,\nour datasets contain 300 times more data points than the widely used OGB-LSC\nPCQM4Mv2 dataset, and 13 times more than the quantum-only QM1B dataset. In\naddition, to support the development of foundational models based on our\nproposed datasets, we present the Graphium graph machine learning library which\nsimplifies the process of building and training molecular machine learning\nmodels for multi-task and multi-level molecular datasets. Finally, we present a\nrange of baseline results as a starting point of multi-task and multi-level\ntraining on these datasets. Empirically, we observe that performance on\nlow-resource biological datasets show improvement by also training on large\namounts of quantum data. This indicates that there may be potential in\nmulti-task and multi-level training of a foundation model and fine-tuning it to\nresource-constrained downstream tasks.",
            "author": [
                "Dominique Beaini",
                "Shenyang Huang",
                "Joao Alex Cunha",
                "Zhiyi Li",
                "Gabriela Moisescu-Pareja",
                "Oleksandr Dymov",
                "Samuel Maddrell-Mander",
                "Callum McLean",
                "Frederik Wenkel",
                "Luis M\u00fcller",
                "Jama Hussein Mohamud",
                "Ali Parviz",
                "Michael Craig",
                "Micha\u0142 Koziarski",
                "Jiarui Lu",
                "Zhaocheng Zhu",
                "Cristian Gabellini",
                "Kerstin Klaser",
                "Josef Dean",
                "Cas Wognum",
                "Maciej Sypetkowski",
                "Guillaume Rabusseau",
                "Reihaneh Rabbany",
                "Jian Tang",
                "Christopher Morris",
                "Ioannis Koutis",
                "Mirco Ravanelli",
                "Guy Wolf",
                "Prudencio Tossou",
                "Hadrien Mary",
                "Therence Bois",
                "Andrew Fitzgibbon",
                "B\u0142a\u017cej Banaszewski",
                "Chad Martin",
                "Dominic Masters"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04292v3",
                "http://arxiv.org/pdf/2310.04292v3"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04286v2",
            "title": "Physics-constrained symbolic model discovery for polyconvex\n  incompressible hyperelastic materials",
            "updated": "2023-10-11T14:20:36Z",
            "published": "2023-10-06T14:38:42Z",
            "summary": "We present a machine learning framework capable of consistently inferring\nmathematical expressions of hyperelastic energy functionals for incompressible\nmaterials from sparse experimental data and physical laws. To achieve this\ngoal, we propose a polyconvex neural additive model (PNAM) that enables us to\nexpress the hyperelastic model in a learnable feature space while enforcing\npolyconvexity. An upshot of this feature space obtained via the PNAM is that\n(1) it is spanned by a set of univariate basis that can be re-parametrized with\na more complex mathematical form, and (2) the resultant elasticity model is\nguaranteed to fulfill the polyconvexity, which ensures that the acoustic tensor\nremains elliptic for any deformation. To further improve the interpretability,\nwe use genetic programming to convert each univariate basis into a compact\nmathematical expression. The resultant multi-variable mathematical models\nobtained from this proposed framework are not only more interpretable but are\nalso proven to fulfill physical laws. By controlling the compactness of the\nlearned symbolic form, the machine learning-generated mathematical model also\nrequires fewer arithmetic operations than its deep neural network counterparts\nduring deployment. This latter attribute is crucial for scaling large-scale\nsimulations where the constitutive responses of every integration point must be\nupdated within each incremental time step. We compare our proposed model\ndiscovery framework against other state-of-the-art alternatives to assess the\nrobustness and efficiency of the training algorithms and examine the trade-off\nbetween interpretability, accuracy, and precision of the learned symbolic\nhyperelastic models obtained from different approaches. Our numerical results\nsuggest that our approach extrapolates well outside the training data regime\ndue to the precise incorporation of physics-based knowledge.",
            "author": [
                "Bahador Bahmani",
                "WaiChing Sun"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04286v2",
                "http://arxiv.org/pdf/2310.04286v2"
            ],
            "primary_category": "cs.CE",
            "category": [
                "cs.CE",
                "cond-mat.soft"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04283v1",
            "title": "On the Error-Propagation of Inexact Deflation for Principal Component\n  Analysis",
            "updated": "2023-10-06T14:33:21Z",
            "published": "2023-10-06T14:33:21Z",
            "summary": "Principal Component Analysis (PCA) is a popular tool in data analysis,\nespecially when the data is high-dimensional. PCA aims to find subspaces,\nspanned by the so-called \\textit{principal components}, that best explain the\nvariance in the dataset. The deflation method is a popular meta-algorithm --\nused to discover such subspaces -- that sequentially finds individual principal\ncomponents, starting from the most important one and working its way towards\nthe less important ones. However, due to its sequential nature, the numerical\nerror introduced by not estimating principal components exactly -- e.g., due to\nnumerical approximations through this process -- propagates, as deflation\nproceeds. To the best of our knowledge, this is the first work that\nmathematically characterizes the error propagation of the inexact deflation\nmethod, and this is the key contribution of this paper. We provide two main\nresults: $i)$ when the sub-routine for finding the leading eigenvector is\ngeneric, and $ii)$ when power iteration is used as the sub-routine. In the\nlatter case, the additional directional information from power iteration allows\nus to obtain a tighter error bound than the analysis of the sub-routine\nagnostic case. As an outcome, we provide explicit characterization on how the\nerror progresses and affects subsequent principal component estimations for\nthis fundamental problem.",
            "author": [
                "Fangshuo Liao",
                "Junhyung Lyle Kim",
                "Cruz Barnum",
                "Anastasios Kyrillidis"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04283v1",
                "http://arxiv.org/pdf/2310.04283v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "math.OC",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04276v1",
            "title": "From task structures to world models: What do LLMs know?",
            "updated": "2023-10-06T14:21:59Z",
            "published": "2023-10-06T14:21:59Z",
            "summary": "In what sense does a large language model have knowledge? The answer to this\nquestion extends beyond the capabilities of a particular AI system, and\nchallenges our assumptions about the nature of knowledge and intelligence. We\nanswer by granting LLMs \"instrumental knowledge\"; knowledge defined by a\ncertain set of abilities. We then ask how such knowledge is related to the more\nordinary, \"worldly\" knowledge exhibited by human agents, and explore this in\nterms of the degree to which instrumental knowledge can be said to incorporate\nthe structured world models of cognitive science. We discuss ways LLMs could\nrecover degrees of worldly knowledge, and suggest such recovery will be\ngoverned by an implicit, resource-rational tradeoff between world models and\ntask demands.",
            "author": [
                "Ilker Yildirim",
                "L. A. Paul"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04276v1",
                "http://arxiv.org/pdf/2310.04276v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "q-bio.NC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04271v1",
            "title": "Compositional Servoing by Recombining Demonstrations",
            "updated": "2023-10-06T14:16:49Z",
            "published": "2023-10-06T14:16:49Z",
            "summary": "Learning-based manipulation policies from image inputs often show weak task\ntransfer capabilities. In contrast, visual servoing methods allow efficient\ntask transfer in high-precision scenarios while requiring only a few\ndemonstrations. In this work, we present a framework that formulates the visual\nservoing task as graph traversal. Our method not only extends the robustness of\nvisual servoing, but also enables multitask capability based on a few\ntask-specific demonstrations. We construct demonstration graphs by splitting\nexisting demonstrations and recombining them. In order to traverse the\ndemonstration graph in the inference case, we utilize a similarity function\nthat helps select the best demonstration for a specific task. This enables us\nto compute the shortest path through the graph. Ultimately, we show that\nrecombining demonstrations leads to higher task-respective success. We present\nextensive simulation and real-world experimental results that demonstrate the\nefficacy of our approach.",
            "author": [
                "Max Argus",
                "Abhijeet Nayak",
                "Martin B\u00fcchner",
                "Silvio Galesso",
                "Abhinav Valada",
                "Thomas Brox"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04271v1",
                "http://arxiv.org/pdf/2310.04271v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04270v2",
            "title": "A Comprehensive Evaluation of Large Language Models on Benchmark\n  Biomedical Text Processing Tasks",
            "updated": "2023-10-10T03:26:16Z",
            "published": "2023-10-06T14:16:28Z",
            "summary": "Recently, Large Language Models (LLM) have demonstrated impressive capability\nto solve a wide range of tasks. However, despite their success across various\ntasks, no prior work has investigated their capability in the biomedical domain\nyet. To this end, this paper aims to evaluate the performance of LLMs on\nbenchmark biomedical tasks. For this purpose, we conduct a comprehensive\nevaluation of 4 popular LLMs in 6 diverse biomedical tasks across 26 datasets.\nTo the best of our knowledge, this is the first work that conducts an extensive\nevaluation and comparison of various LLMs in the biomedical domain.\nInterestingly, we find based on our evaluation that in biomedical datasets that\nhave smaller training sets, zero-shot LLMs even outperform the current\nstate-of-the-art fine-tuned biomedical models. This suggests that pretraining\non large text corpora makes LLMs quite specialized even in the biomedical\ndomain. We also find that not a single LLM can outperform other LLMs in all\ntasks, with the performance of different LLMs may vary depending on the task.\nWhile their performance is still quite poor in comparison to the biomedical\nmodels that were fine-tuned on large training sets, our findings demonstrate\nthat LLMs have the potential to be a valuable tool for various biomedical tasks\nthat lack large annotated data.",
            "author": [
                "Israt Jahan",
                "Md Tahmid Rahman Laskar",
                "Chun Peng",
                "Jimmy Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04270v2",
                "http://arxiv.org/pdf/2310.04270v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04257v1",
            "title": "On Solving Close Enough Orienteering Problem with Overlapped\n  Neighborhoods",
            "updated": "2023-10-06T14:02:34Z",
            "published": "2023-10-06T14:02:34Z",
            "summary": "The Close Enough Traveling Salesman Problem (CETSP) is a well-known variant\nof the classic Traveling Salesman Problem whereby the agent may complete its\nmission at any point within a target neighborhood. Heuristics based on\noverlapped neighborhoods, known as Steiner Zones (SZ), have gained attention in\naddressing CETSPs. While SZs offer effective approximations to the original\ngraph, their inherent overlap imposes constraints on the search space,\npotentially conflicting with global optimization objectives. Here we present\nthe Close Enough Orienteering Problem with Non-uniform Neighborhoods (CEOP-N),\nwhich extends CETSP by introducing variable prize attributes and non-uniform\ncost considerations for prize collection. To tackle CEOP-N, we develop a new\napproach featuring a Randomized Steiner Zone Discretization (RSZD) scheme\ncoupled with a hybrid algorithm based on Particle Swarm Optimization (PSO) and\nAnt Colony System (ACS) - CRaSZe-AntS. The RSZD scheme identifies sub-regions\nfor PSO exploration, and ACS determines the discrete visiting sequence. We\nevaluate the RSZD's discretization performance on CEOP instances derived from\nestablished CETSP instances, and compare CRaSZe-AntS against the most relevant\nstate-of-the-art heuristic focused on single-neighborhood optimization for\nCEOP. We also compare the performance of the interior search within SZs and the\nboundary search on individual neighborhoods in the context of CEOP-N. Our\nresults show CRaSZe-AntS can yield comparable solution quality with\nsignificantly reduced computation time compared to the single-neighborhood\nstrategy, where we observe an averaged 140.44% increase in prize collection and\n55.18% reduction of execution time. CRaSZe-AntS is thus highly effective in\nsolving emerging CEOP-N, examples of which include truck-and-drone delivery\nscenarios.",
            "author": [
                "Qiuchen Qian",
                "Yanran Wang",
                "David Boyle"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04257v1",
                "http://arxiv.org/pdf/2310.04257v1"
            ],
            "primary_category": "cs.NE",
            "category": [
                "cs.NE",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04241v2",
            "title": "Improving Reinforcement Learning Efficiency with Auxiliary Tasks in\n  Non-Visual Environments: A Comparison",
            "updated": "2023-10-09T13:02:07Z",
            "published": "2023-10-06T13:22:26Z",
            "summary": "Real-world reinforcement learning (RL) environments, whether in robotics or\nindustrial settings, often involve non-visual observations and require not only\nefficient but also reliable and thus interpretable and flexible RL approaches.\nTo improve efficiency, agents that perform state representation learning with\nauxiliary tasks have been widely studied in visual observation contexts.\nHowever, for real-world problems, dedicated representation learning modules\nthat are decoupled from RL agents are more suited to meet requirements. This\nstudy compares common auxiliary tasks based on, to the best of our knowledge,\nthe only decoupled representation learning method for low-dimensional\nnon-visual observations. We evaluate potential improvements in sample\nefficiency and returns for environments ranging from a simple pendulum to a\ncomplex simulated robotics task. Our findings show that representation learning\nwith auxiliary tasks only provides performance gains in sufficiently complex\nenvironments and that learning environment dynamics is preferable to predicting\nrewards. These insights can inform future development of interpretable\nrepresentation learning approaches for non-visual observations and advance the\nuse of RL solutions in real-world scenarios.",
            "author": [
                "Moritz Lange",
                "Noah Krystiniak",
                "Raphael C. Engelhardt",
                "Wolfgang Konen",
                "Laurenz Wiskott"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04241v2",
                "http://arxiv.org/pdf/2310.04241v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04236v1",
            "title": "Optimization with pattern-avoiding input",
            "updated": "2023-10-06T13:20:50Z",
            "published": "2023-10-06T13:20:50Z",
            "summary": "Permutation pattern-avoidance is a central concept of both enumerative and\nextremal combinatorics. In this paper we study the effect of permutation\npattern-avoidance on the complexity of optimization problems.\n  In the context of the dynamic optimality conjecture (Sleator, Tarjan, STOC\n1983), Chalermsook, Goswami, Kozma, Mehlhorn, and Saranurak (FOCS 2015)\nconjectured that the amortized access cost of an optimal binary search tree\n(BST) is $O(1)$ whenever the access sequence avoids some fixed pattern. They\nshowed a bound of $2^{\\alpha{(n)}^{O(1)}}$, which was recently improved to\n$2^{\\alpha{(n)}(1+o(1))}$ by Chalermsook, Pettie, and Yingchareonthawornchai\n(2023); here $n$ is the BST size and $\\alpha(\\cdot)$ the inverse-Ackermann\nfunction. In this paper we resolve the conjecture, showing a tight $O(1)$\nbound. This indicates a barrier to dynamic optimality: any candidate online BST\n(e.g., splay trees or greedy trees) must match this optimum, but current\nanalysis techniques only give superconstant bounds.\n  More broadly, we argue that the easiness of pattern-avoiding input is a\ngeneral phenomenon, not limited to BSTs or even to data structures. To\nillustrate this, we show that when the input avoids an arbitrary, fixed, a\npriori unknown pattern, one can efficiently compute a $k$-server solution of\n$n$ requests from a unit interval, with total cost $n^{O(1/\\log k)}$, in\ncontrast to the worst-case $\\Theta(n/k)$ bound; and a traveling salesman tour\nof $n$ points from a unit box, of length $O(\\log{n})$, in contrast to the\nworst-case $\\Theta(\\sqrt{n})$ bound; similar results hold for the euclidean\nminimum spanning tree, Steiner tree, and nearest-neighbor graphs.\n  We show both results to be tight. Our techniques build on the Marcus-Tardos\nproof of the Stanley-Wilf conjecture, and on the recently emerging concept of\ntwin-width; we believe our techniques to be more generally applicable.",
            "author": [
                "Benjamin Aram Berendsohn",
                "L\u00e1szl\u00f3 Kozma",
                "Michal Opler"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04236v1",
                "http://arxiv.org/pdf/2310.04236v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04223v1",
            "title": "Boundary rigidity of finite CAT(0) cube complexes",
            "updated": "2023-10-06T13:10:10Z",
            "published": "2023-10-06T13:10:10Z",
            "summary": "In this note, we prove that finite CAT(0) cube complexes can be reconstructed\nfrom their boundary distances (computed in their 1-skeleta). This result was\nconjectured by Haslegrave, Scott, Tamitegama, and Tan (2023). The\nreconstruction of a finite cell complex from the boundary distances is the\ndiscrete version of the boundary rigidity problem, which is a classical problem\nfrom Riemannian geometry. In the proofs, we use the bijection between CAT(0)\ncube complexes and median graphs and the corner peelings of median graphs.",
            "author": [
                "J\u00e9r\u00e9mie Chalopin",
                "Victor Chepoi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04223v1",
                "http://arxiv.org/pdf/2310.04223v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "cs.DM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04220v1",
            "title": "Spatial second-order positive and asymptotic preserving filtered $P_N$\n  schemes for nonlinear radiative transfer equations",
            "updated": "2023-10-06T13:07:44Z",
            "published": "2023-10-06T13:07:44Z",
            "summary": "A spatial second-order scheme for the nonlinear radiative transfer equations\nis introduced in this paper. The discretization scheme is based on the filtered\nspherical harmonics ($FP_N$) method for the angular variable and the unified\ngas kinetic scheme (UGKS) framework for the spatial and temporal variables\nrespectively. In order to keep the scheme positive and second-order accuracy,\nfirstly, we use the implicit Monte Carlo linearization method [6] in the\nconstruction of the UGKS numerical boundary fluxes. Then, by carefully\nanalyzing the constructed second-order fluxes involved in the macro-micro\ndecomposition, which is induced by the $FP_N$ angular discretization, we\nestablish the sufficient conditions that guarantee the positivity of the\nradiative energy density and material temperature. Finally, we employ linear\nscaling limiters for the angular variable in the $P_N$ reconstruction and for\nthe spatial variable in the piecewise linear slopes reconstruction\nrespectively, which are shown to be realizable and reasonable to enforce the\nsufficient conditions holding. Thus, the desired scheme, called the\n$PPFP_N$-based UGKS, is obtained. Furthermore, in the regime $\\epsilon\\ll 1$\nand the regime $\\epsilon=O(1)$, a simplified spatial second-order scheme,\ncalled the $PPFP_N$-based SUGKS, is presented, which possesses all the\nproperties of the non-simplified one. Inheriting the merit of UGKS, the\nproposed schemes are asymptotic preserving. By employing the $FP_N$ method for\nthe angular variable, the proposed schemes are almost free of ray effects. To\nour best knowledge, this is the first time that spatial second-order, positive,\nasymptotic preserving and almost free of ray effects schemes are constructed\nfor the nonlinear radiative transfer equations without operator splitting.\nVarious numerical experiments are included to validate the properties of the\nproposed schemes.",
            "author": [
                "Xiaojing Xu",
                "Song Jiang",
                "Wenjun Sun"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04220v1",
                "http://arxiv.org/pdf/2310.04220v1"
            ],
            "primary_category": "math.NA",
            "category": [
                "math.NA",
                "cs.NA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04218v1",
            "title": "A Fixed-Parameter Tractable Algorithm for Counting Markov Equivalence\n  Classes with the same Skeleton",
            "updated": "2023-10-06T13:05:07Z",
            "published": "2023-10-06T13:05:07Z",
            "summary": "Causal DAGs (also known as Bayesian networks) are a popular tool for encoding\nconditional dependencies between random variables. In a causal DAG, the random\nvariables are modeled as vertices in the DAG, and it is stipulated that every\nrandom variable is independent of its ancestors conditioned on its parents. It\nis possible, however, for two different causal DAGs on the same set of random\nvariables to encode exactly the same set of conditional dependencies. Such\ncausal DAGs are said to be Markov equivalent, and equivalence classes of Markov\nequivalent DAGs are known as Markov Equivalent Classes (MECs). Beautiful\ncombinatorial characterizations of MECs have been developed in the past few\ndecades, and it is known, in particular that all DAGs in the same MEC must have\nthe same ''skeleton'' (underlying undirected graph) and v-structures (induced\nsubgraph of the form $a\\rightarrow b \\leftarrow c$).\n  These combinatorial characterizations also suggest several natural\nalgorithmic questions. One of these is: given an undirected graph $G$ as input,\nhow many distinct Markov equivalence classes have the skeleton $G$? Much work\nhas been devoted in the last few years to this and other closely related\nproblems. However, to the best of our knowledge, a polynomial time algorithm\nfor the problem remains unknown.\n  In this paper, we make progress towards this goal by giving a fixed parameter\ntractable algorithm for the above problem, with the parameters being the\ntreewidth and the maximum degree of the input graph $G$. The main technical\ningredient in our work is a construction we refer to as shadow, which lets us\ncreate a \"local description'' of long-range constraints imposed by the\ncombinatorial characterizations of MECs.",
            "author": [
                "Vidya Sagar Sharma"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04218v1",
                "http://arxiv.org/pdf/2310.04218v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04217v2",
            "title": "A Stochastic Game without Approximate Equilibria",
            "updated": "2023-10-20T12:09:44Z",
            "published": "2023-10-06T13:04:42Z",
            "summary": "A game has approximate equilibria if for every $\\epsilon >0$ there is an\n$\\epsilon$-equilibrium. We show that there is a stochastic game that lacks\napproximate equilibria. This game has finitely many players and actions, their\npayoffs are Borel measurable functions on the pathways of play, and\n  all players have perfect knowledge of the past histories and the present\nstate.",
            "author": [
                "Robert Samuel Simon"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04217v2",
                "http://arxiv.org/pdf/2310.04217v2"
            ],
            "primary_category": "math.FA",
            "category": [
                "math.FA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04213v2",
            "title": "Topology-Aware Neural Networks for Fast Contingency Analysis of Power\n  Systems",
            "updated": "2023-11-10T14:37:24Z",
            "published": "2023-10-06T13:00:36Z",
            "summary": "Training Neural Networks able to capture the topology changes of the power\ngrid is one of the significant challenges towards the adoption of machine\nlearning techniques for N-k security computations and a wide range of other\noperations that involve grid reconfiguration. As the number of N-k scenarios\nincreases exponentially with increasing system size this renders such problems\nextremely time-consuming to solve with traditional solvers. In this paper, we\ncombine Physics-Informed Neural Networks with both a Guided-Dropout (GD) Neural\nNetwork (which associates dedicated neurons with specific line\nconnections/disconnections) and an edge-varrying Graph Neural Neural Network\n(GNN) architecture to learn the setpoints for a grid that considers all\nprobable single-line reconfigurations (all critical N-1 scenarios) and\nsubsequently apply the trained models to N-k scenarios.We demonstrate how\nincorporating the underlying physical equations for the network equations\nwithin the training procedure of the GD and the GNN architectures, performs\nwith N-1, N-2, and N-3 case studies. Using the AC Power Flow as a guiding\napplication, we test our methods on the 14-bus, 30-bus, 57-bus, and 118-bus\nsystems. We find that these topology-aware NNs not only achieve the task of\ncontingency screening with satisfactory accuracy but do this at up to 1000\ntimes faster than the Newton Raphson power flow solver. Moreover, our results\nprovide a comparison of the GD and GNN models in terms of accuracy and\ncomputational speed and provide recommendations on their adoption for\ncontingency analysis of power systems.",
            "author": [
                "Agnes M. Nakiganda",
                "Catherine Cheylan",
                "Spyros Chatzivasileiadis"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04213v2",
                "http://arxiv.org/pdf/2310.04213v2"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04211v1",
            "title": "Flip graph and arc complex finite rigidity",
            "updated": "2023-10-06T12:56:58Z",
            "published": "2023-10-06T12:56:58Z",
            "summary": "A subcomplex $\\mathcal{X}$ of a cell complex $\\mathcal{C}$ is called rigid\nwith respect to another cell complex $\\mathcal{C}'$ if every injective\nsimplicial map $\\lambda:\\mathcal{X} \\rightarrow \\mathcal{C}'$ has a unique\nextension to an injective simplicial map $\\phi:\\mathcal{C}\\rightarrow\n\\mathcal{C}'$. Given a surface with marked points, its flip graph and arc\ncomplex are simplicial complexes indexing the triangulations and the arcs\nbetween marked points respectively. We relate two recent results of the second\nauthor on the existence of finite rigid subcomplexes in the flip graph and in\nthe arc complex. The second author employed distinct approaches and proof\nmethods to these two complexes. In this paper, we leverage the fact that the\nflip graph can be embedded in the arc complex as its dual, relating the second\nauthor's past rigidity results. Further, this allows us to prove a new\nvariation of arc complex rigidity for surfaces with boundary.",
            "author": [
                "Chandrika Sadanand",
                "Emily Shinkle"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04211v1",
                "http://arxiv.org/pdf/2310.04211v1"
            ],
            "primary_category": "math.GT",
            "category": [
                "math.GT",
                "57K20, 57M50, 05C25, 20F65"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04205v2",
            "title": "Keyword Augmented Retrieval: Novel framework for Information Retrieval\n  integrated with speech interface",
            "updated": "2023-10-29T09:25:47Z",
            "published": "2023-10-06T12:44:04Z",
            "summary": "Retrieving answers in a quick and low cost manner without hallucinations from\na combination of structured and unstructured data using Language models is a\nmajor hurdle. This is what prevents employment of Language models in knowledge\nretrieval automation. This becomes accentuated when one wants to integrate a\nspeech interface on top of a text based knowledge retrieval system. Besides,\nfor commercial search and chat-bot applications, complete reliance on\ncommercial large language models (LLMs) like GPT 3.5 etc. can be very costly.\nIn the present study, the authors have addressed the aforementioned problem by\nfirst developing a keyword based search framework which augments discovery of\nthe context from the document to be provided to the LLM. The keywords in turn\nare generated by a relatively smaller LLM and cached for comparison with\nkeywords generated by the same smaller LLM against the query raised. This\nsignificantly reduces time and cost to find the context within documents. Once\nthe context is set, a larger LLM uses that to provide answers based on a prompt\ntailored for Q\\&A. This research work demonstrates that use of keywords in\ncontext identification reduces the overall inference time and cost of\ninformation retrieval. Given this reduction in inference time and cost with the\nkeyword augmented retrieval framework, a speech based interface for user input\nand response readout was integrated. This allowed a seamless interaction with\nthe language model.",
            "author": [
                "Anupam Purwar",
                "Rahul Sundar"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04205v2",
                "http://arxiv.org/pdf/2310.04205v2"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.AI",
                "cs.CL",
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.13001v2",
            "title": "Conversational Financial Information Retrieval Model (ConFIRM)",
            "updated": "2023-11-10T07:15:17Z",
            "published": "2023-10-06T12:31:05Z",
            "summary": "With the exponential growth in large language models (LLMs), leveraging their\nemergent properties for specialized domains like finance merits exploration.\nHowever, regulated fields such as finance pose unique constraints, requiring\ndomain-optimized frameworks. We present ConFIRM, an LLM-based conversational\nfinancial information retrieval model tailored for query intent classification\nand knowledge base labeling.\n  ConFIRM comprises two modules:\n  1) a method to synthesize finance domain-specific question-answer pairs, and\n  2) evaluation of parameter efficient fine-tuning approaches for the query\nclassification task. We generate a dataset of over 4000 samples, assessing\naccuracy on a separate test set.\n  ConFIRM achieved over 90% accuracy, essential for regulatory compliance.\nConFIRM provides a data-efficient solution to extract precise query intent for\nfinancial dialog systems.",
            "author": [
                "Stephen Choi",
                "William Gazeley",
                "Siu Ho Wong",
                "Tingting Li"
            ],
            "link": [
                "http://arxiv.org/abs/2310.13001v2",
                "http://arxiv.org/pdf/2310.13001v2"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.AI",
                "cs.CE",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04190v1",
            "title": "Non-Redundant Graph Neural Networks with Improved Expressiveness",
            "updated": "2023-10-06T12:09:09Z",
            "published": "2023-10-06T12:09:09Z",
            "summary": "Message passing graph neural networks iteratively compute node embeddings by\naggregating messages from all neighbors. This procedure can be viewed as a\nneural variant of the Weisfeiler-Leman method, which limits their expressive\npower. Moreover, oversmoothing and oversquashing restrict the number of layers\nthese networks can effectively utilize. The repeated exchange and encoding of\nidentical information in message passing amplifies oversquashing. We propose a\nnovel aggregation scheme based on neighborhood trees, which allows for\ncontrolling the redundancy by pruning branches of the unfolding trees\nunderlying standard message passing. We prove that reducing redundancy improves\nexpressivity and experimentally show that it alleviates oversquashing. We\ninvestigate the interaction between redundancy in message passing and\nredundancy in computation and propose a compact representation of neighborhood\ntrees, from which we compute node and graph embeddings via a neural tree\ncanonization technique. Our method is provably more expressive than the\nWeisfeiler-Leman method, less susceptible to oversquashing than message passing\nneural networks, and provides high classification accuracy on widely-used\nbenchmark datasets.",
            "author": [
                "Franka Bause",
                "Samir Moustafa",
                "Johannes Langguth",
                "Wilfried N. Gansterer",
                "Nils M. Kriege"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04190v1",
                "http://arxiv.org/pdf/2310.04190v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04189v2",
            "title": "Bridging the Gap between Human Motion and Action Semantics via Kinematic\n  Phrases",
            "updated": "2023-10-11T08:01:11Z",
            "published": "2023-10-06T12:08:15Z",
            "summary": "The goal of motion understanding is to establish a reliable mapping between\nmotion and action semantics, while it is a challenging many-to-many problem. An\nabstract action semantic (i.e., walk forwards) could be conveyed by\nperceptually diverse motions (walk with arms up or swinging), while a motion\ncould carry different semantics w.r.t. its context and intention. This makes an\nelegant mapping between them difficult. Previous attempts adopted\ndirect-mapping paradigms with limited reliability. Also, current automatic\nmetrics fail to provide reliable assessments of the consistency between motions\nand action semantics. We identify the source of these problems as the\nsignificant gap between the two modalities. To alleviate this gap, we propose\nKinematic Phrases (KP) that take the objective kinematic facts of human motion\nwith proper abstraction, interpretability, and generality characteristics.\nBased on KP as a mediator, we can unify a motion knowledge base and build a\nmotion understanding system. Meanwhile, KP can be automatically converted from\nmotions and to text descriptions with no subjective bias, inspiring Kinematic\nPrompt Generation (KPG) as a novel automatic motion generation benchmark. In\nextensive experiments, our approach shows superiority over other methods. Our\ncode and data would be made publicly available at https://foruck.github.io/KP.",
            "author": [
                "Xinpeng Liu",
                "Yong-Lu Li",
                "Ailing Zeng",
                "Zizheng Zhou",
                "Yang You",
                "Cewu Lu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04189v2",
                "http://arxiv.org/pdf/2310.04189v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04175v1",
            "title": "Equivariant Nica-Pimsner quotients associated with strong compactly\n  aligned product systems",
            "updated": "2023-10-06T11:46:11Z",
            "published": "2023-10-06T11:46:11Z",
            "summary": "We parametrise the gauge-invariant ideals of the Toeplitz-Nica-Pimsner\nalgebra of a strong compactly aligned product system over $\\mathbb{Z}_+^d$ by\nusing $2^d$-tuples of ideals of the coefficient algebra that are invariant,\npartially ordered, and maximal. We give an algebraic characterisation of\nmaximality that allows the iteration of a $2^d$-tuple to the maximal one\ninducing the same gauge-invariant ideal. The parametrisation respects\ninclusions and intersections, while we characterise the join operation on the\n$2^d$-tuples that renders the parametrisation a lattice isomorphism.\n  The problem of the parametrisation of the gauge-invariant ideals is\nequivalent to the study of relative Cuntz-Nica-Pimsner algebras, for which we\nprovide a generalised Gauge-Invariant Uniqueness Theorem. We focus further on\nequivariant quotients of the Cuntz-Nica-Pimsner algebra and provide\napplications to regular product systems, C*-dynamical systems, strong finitely\naligned higher-rank graphs, and product systems on finite frames. In\nparticular, we provide a description of the parametrisation for (possibly\nnon-automorphic) C*-dynamical systems and row-finite higher-rank graphs, which\nsquares with known results when restricting to crossed products and to locally\nconvex row-finite higher-rank graphs.",
            "author": [
                "Joseph A. Dessi",
                "Evgenios T. A. Kakariadis"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04175v1",
                "http://arxiv.org/pdf/2310.04175v1"
            ],
            "primary_category": "math.OA",
            "category": [
                "math.OA",
                "math.FA",
                "46L08, 47L55, 46L05"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04171v2",
            "title": "Dynamic Relation-Attentive Graph Neural Networks for Fraud Detection",
            "updated": "2023-10-09T11:30:59Z",
            "published": "2023-10-06T11:41:38Z",
            "summary": "Fraud detection aims to discover fraudsters deceiving other users by, for\nexample, leaving fake reviews or making abnormal transactions. Graph-based\nfraud detection methods consider this task as a classification problem with two\nclasses: frauds or normal. We address this problem using Graph Neural Networks\n(GNNs) by proposing a dynamic relation-attentive aggregation mechanism. Based\non the observation that many real-world graphs include different types of\nrelations, we propose to learn a node representation per relation and aggregate\nthe node representations using a learnable attention function that assigns a\ndifferent attention coefficient to each relation. Furthermore, we combine the\nnode representations from different layers to consider both the local and\nglobal structures of a target node, which is beneficial to improving the\nperformance of fraud detection on graphs with heterophily. By employing dynamic\ngraph attention in all the aggregation processes, our method adaptively\ncomputes the attention coefficients for each node. Experimental results show\nthat our method, DRAG, outperforms state-of-the-art fraud detection methods on\nreal-world benchmark datasets.",
            "author": [
                "Heehyeon Kim",
                "Jinhyeok Choi",
                "Joyce Jiyoung Whang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04171v2",
                "http://arxiv.org/pdf/2310.04171v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CR",
                "I.2"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04162v1",
            "title": "Light-LOAM: A Lightweight LiDAR Odometry and Mapping based on\n  Graph-Matching",
            "updated": "2023-10-06T11:21:31Z",
            "published": "2023-10-06T11:21:31Z",
            "summary": "Simultaneous Localization and Mapping (SLAM) plays an important role in robot\nautonomy. Reliability and efficiency are the two most valued features for\napplying SLAM in robot applications. In this paper, we consider achieving a\nreliable LiDAR-based SLAM function in computation-limited platforms, such as\nquadrotor UAVs based on graph-based point cloud association. First, contrary to\nmost works selecting salient features for point cloud registration, we propose\na non-conspicuous feature selection strategy for reliability and robustness\npurposes. Then a two-stage correspondence selection method is used to register\nthe point cloud, which includes a KD-tree-based coarse matching followed by a\ngraph-based matching method that uses geometric consistency to vote out\nincorrect correspondences. Additionally, we propose an odometry approach where\nthe weight optimizations are guided by vote results from the aforementioned\ngeometric consistency graph. In this way, the optimization of LiDAR odometry\nrapidly converges and evaluates a fairly accurate transformation resulting in\nthe back-end module efficiently finishing the mapping task. Finally, we\nevaluate our proposed framework on the KITTI odometry dataset and real-world\nenvironments. Experiments show that our SLAM system achieves a comparative\nlevel or higher level of accuracy with more balanced computation efficiency\ncompared with the mainstream LiDAR-based SLAM solutions.",
            "author": [
                "Shiquan Yi",
                "Yang Lyu",
                "Lin Hua",
                "Quan Pan",
                "Chunhui Zhao"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04162v1",
                "http://arxiv.org/pdf/2310.04162v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04161v1",
            "title": "Line graph characterization of the order supergraph of a finite group",
            "updated": "2023-10-06T11:19:21Z",
            "published": "2023-10-06T11:19:21Z",
            "summary": "The power graph $\\mathcal{P}(G)$ is the simple undirected graph with group\nelements as a vertex set and two elements are adjacent if one of them is a\npower of the other. The order supergraph $\\mathcal{S}(G)$ of the power graph\n$\\mathcal{P}(G)$ is the simple undirected graph with vertex set $G$ in which\ntwo vertices $x$ and $y$ are adjacent if $o(x)\\vert o(y)$ or $o(y)\\vert o(x)$.\nIn this paper, we classify all the finite groups $G$ such that the order\nsupergraph $\\mathcal{S}(G)$ is the line graph of some graph. Moreover, we\ncharacterize finite groups whose order supergraphs are the complement of line\ngraphs.",
            "author": [
                "Manisha",
                "Parveen",
                "Jitender Kumar"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04161v1",
                "http://arxiv.org/pdf/2310.04161v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "math.GR",
                "05C25, 20D15"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04159v1",
            "title": "Amortized Network Intervention to Steer the Excitatory Point Processes",
            "updated": "2023-10-06T11:17:28Z",
            "published": "2023-10-06T11:17:28Z",
            "summary": "We tackle the challenge of large-scale network intervention for guiding\nexcitatory point processes, such as infectious disease spread or traffic\ncongestion control. Our model-based reinforcement learning utilizes neural ODEs\nto capture how the networked excitatory point processes will evolve subject to\nthe time-varying changes in network topology. Our approach incorporates\nGradient-Descent based Model Predictive Control (GD-MPC), offering policy\nflexibility to accommodate prior knowledge and constraints. To address the\nintricacies of planning and overcome the high dimensionality inherent to such\ndecision-making problems, we design an Amortize Network Interventions (ANI)\nframework, allowing for the pooling of optimal policies from history and other\ncontexts, while ensuring a permutation equivalent property. This property\nenables efficient knowledge transfer and sharing across diverse contexts. Our\napproach has broad applications, from curbing infectious disease spread to\nreducing carbon emissions through traffic light optimization, and thus has the\npotential to address critical societal and environmental challenges.",
            "author": [
                "Zitao Song",
                "Wendi Ren",
                "Shuang Li"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04159v1",
                "http://arxiv.org/pdf/2310.04159v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04156v1",
            "title": "Postselection-free learning of measurement-induced quantum dynamics",
            "updated": "2023-10-06T11:06:06Z",
            "published": "2023-10-06T11:06:06Z",
            "summary": "We address how one can empirically infer properties of quantum states\ngenerated by dynamics involving measurements. Our focus is on many-body\nsettings where the number of measurements is extensive, making brute-force\napproaches based on postselection intractable due to their exponential sample\ncomplexity. We introduce a general-purpose inference scheme for learning\nproperties of the post-measurement ensemble of states using a scalable number\nof experimental repetitions. We first identify a general class of `estimable\nproperties' that can be directly extracted from experimental data. Then, based\non empirical observations of some such quantities, we show how one can\nindirectly infer information about a given non-estimable quantity of interest,\nsuch as the average entanglement entropy, or frame potential. We formulate our\napproach in terms of an optimization task, where one asks what are the minimum\nand maximum values that the desired quantity could possibly take, while\nensuring consistency with observations. The true value of this quantity must\nthen lie within a feasible range between these extrema, resulting in two-sided\nbounds. Narrow feasible ranges can be obtained by using a classical simulation\nof the device to determine which estimable properties one should measure. Even\nin cases where this simulation is inaccurate, unambiguous information about the\ntrue value of a given quantity realised on the quantum device can be learned.\nAs an immediate application, we show that our method can be used to verify the\nemergence of quantum state designs in experiments. We identify some fundamental\nobstructions that in some cases prevent sharp knowledge of a given quantity\nfrom being inferred, and discuss what can be learned in cases where classical\nsimulation is too computationally demanding to be feasible.",
            "author": [
                "Max McGinley"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04156v1",
                "http://arxiv.org/pdf/2310.04156v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cond-mat.quant-gas",
                "cond-mat.stat-mech"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.13000v1",
            "title": "Document-Level Relation Extraction with Relation Correlation Enhancement",
            "updated": "2023-10-06T10:59:00Z",
            "published": "2023-10-06T10:59:00Z",
            "summary": "Document-level relation extraction (DocRE) is a task that focuses on\nidentifying relations between entities within a document. However, existing\nDocRE models often overlook the correlation between relations and lack a\nquantitative analysis of relation correlations. To address this limitation and\neffectively capture relation correlations in DocRE, we propose a relation graph\nmethod, which aims to explicitly exploit the interdependency among relations.\nFirstly, we construct a relation graph that models relation correlations using\nstatistical co-occurrence information derived from prior relation knowledge.\nSecondly, we employ a re-weighting scheme to create an effective relation\ncorrelation matrix to guide the propagation of relation information.\nFurthermore, we leverage graph attention networks to aggregate relation\nembeddings. Importantly, our method can be seamlessly integrated as a\nplug-and-play module into existing models. Experimental results demonstrate\nthat our approach can enhance the performance of multi-relation extraction,\nhighlighting the effectiveness of considering relation correlations in DocRE.",
            "author": [
                "Yusheng Huang",
                "Zhouhan Lin"
            ],
            "link": [
                "http://arxiv.org/abs/2310.13000v1",
                "http://arxiv.org/pdf/2310.13000v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04149v1",
            "title": "On monoids of endomorphisms of a cycle graph",
            "updated": "2023-10-06T10:45:20Z",
            "published": "2023-10-06T10:45:20Z",
            "summary": "In this paper we consider endomorphisms of an undirected cycle graph from\nSemigroup Theory perspective. Our main aim is to present a process to determine\nsets of generators with minimal cardinality for the monoids $wEnd(C_n)$ and\n$End(C_n)$ of all weak endomorphisms and all endomorphisms of an undirected\ncycle graph $C_n$ with $n$ vertices. We also describe Green's relations and\nregularity of these monoids and calculate their cardinalities.",
            "author": [
                "Ilinka Dimitrova",
                "V\u00edtor H. Fernandes",
                "J\u00f6rg Koppitz",
                "Teresa M. Quinteiro"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04149v1",
                "http://arxiv.org/pdf/2310.04149v1"
            ],
            "primary_category": "math.RA",
            "category": [
                "math.RA",
                "05C38, 20M10, 20M20, 05C25"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04138v1",
            "title": "Universality for transversal Hamilton cycles",
            "updated": "2023-10-06T10:22:47Z",
            "published": "2023-10-06T10:22:47Z",
            "summary": "Let $\\mathbf{G}=\\{G_1, \\ldots, G_m\\}$ be a graph collection on a common\nvertex set $V$ of size $n$ such that $\\delta(G_i) \\geq (1+o(1))n/2$ for every\n$i \\in [m]$. We show that $\\mathbf{G}$ contains every Hamilton cycle pattern.\nThat is, for every map $\\chi: [n] \\to [m]$ there is a Hamilton cycle whose\n$i$-th edge lies in $G_{\\chi(i)}$.",
            "author": [
                "Candida Bowtell",
                "Patrick Morris",
                "Yanitsa Pehova",
                "Katherine Staden"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04138v1",
                "http://arxiv.org/pdf/2310.04138v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04137v1",
            "title": "Spectra and the exact number of Automorphisms of Paley-type graphs of\n  order a product of n distinct primes",
            "updated": "2023-10-06T10:21:02Z",
            "published": "2023-10-06T10:21:02Z",
            "summary": "Paley graphs are Cayley graphs which are circulant and strongly regular.\nPaley-type graph of order a product of two distinct Pythagorean primes was\nintroduced by Dr Angsuman Das. In this paper, we extend the study of Paley-type\ngraphs to the order a product of n distinct Pythagorean primes. We have\ndetermined its adjacency spectra and the exact number of the automorphisms.",
            "author": [
                "Sivaranjani A",
                "Radha S"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04137v1",
                "http://arxiv.org/pdf/2310.04137v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C25, 05C50, 20D45"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04111v1",
            "title": "Dense Random Texture Detection using Beta Distribution Statistics",
            "updated": "2023-10-06T09:20:58Z",
            "published": "2023-10-06T09:20:58Z",
            "summary": "This note describes a method for detecting dense random texture using fully\nconnected points sampled on image edges. An edge image is randomly sampled with\npoints, the standard L2 distance is calculated between all connected points in\na neighbourhood. For each point, a check is made if the point intersects with\nan image edge. If this is the case, a unity value is added to the distance,\notherwise zero. From this an edge excess index is calculated for the fully\nconnected edge graph in the range [1.0..2.0], where 1.0 indicate no edges. The\nratio can be interpreted as a sampled Bernoulli process with unknown\nprobability. The Bayesian posterior estimate of the probability can be\nassociated with its conjugate prior which is a Beta($\\alpha$, $\\beta$)\ndistribution, with hyper parameters $\\alpha$ and $\\beta$ related to the number\nof edge crossings. Low values of $\\beta$ indicate a texture rich area, higher\nvalues less rich. The method has been applied to real-time SLAM-based moving\nobject detection, where points are confined to tracked boxes (rois).",
            "author": [
                "Soeren Molander"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04111v1",
                "http://arxiv.org/pdf/2310.04111v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04103v2",
            "title": "Maker-Breaker domination game on Cartesian products of graphs",
            "updated": "2023-10-11T06:35:02Z",
            "published": "2023-10-06T09:10:26Z",
            "summary": "The Maker-Breaker domination game is played on a graph $G$ by two players,\ncalled Dominator and Staller. They alternately select an unplayed vertex in\n$G$. Dominator wins the game if he forms a dominating set while Staller wins\nthe game if she claims all vertices from a closed neighborhood of a vertex. If\nDominator is the winner in the D-game (or the S-game), then $\\gamma_{MB}(G)$\n(or $\\gamma'_{MB}(G)$) is defined by the minimum number of moves of Dominator\nto win the game under any strategy of Staller.\n  Analogously, when Staller is the winner, $\\gamma_{SMB}(G)$ and\n$\\gamma'_{SMB}(G)$ can be defined in the same way.\n  We determine the winner of the game on the Cartesian product of paths, stars,\nand complete bipartite graphs, and how fast the winner wins. We prove that\nDominator is the winner on $P_m \\square P_n$ in both the D-game and the S-game,\nand $\\gamma_{MB}(P_m \\square P_n)$ and $\\gamma'_{MB}(P_m \\square P_n)$ are\ndetermined when $m=3$ and $3 \\le n \\le 5$. Dominator also wins on $G \\square H$\nin both games if $G$ and $H$ admit nontrivial path covers. Furthermore, we\nestablish the winner in the D-game and the S-game on $K_{m,n} \\square\nK_{m',n'}$ for every positive integer $m, m',n,n'$.\n  We prove the exact formulas for $\\gamma_{MB}(G)$, $\\gamma'_{MB}(G)$,\n$\\gamma_{SMB}(G)$, and $\\gamma'_{SMB}(G)$ where $G$ is a product of stars.",
            "author": [
                "Pakanun Dokyeesun"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04103v2",
                "http://arxiv.org/pdf/2310.04103v2"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "G.2.2"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04479v1",
            "title": "Leveraging Data Geometry to Mitigate CSM in Steganalysis",
            "updated": "2023-10-06T09:08:25Z",
            "published": "2023-10-06T09:08:25Z",
            "summary": "In operational scenarios, steganographers use sets of covers from various\nsensors and processing pipelines that differ significantly from those used by\nresearchers to train steganalysis models. This leads to an inevitable\nperformance gap when dealing with out-of-distribution covers, commonly referred\nto as Cover Source Mismatch (CSM). In this study, we consider the scenario\nwhere test images are processed using the same pipeline. However, knowledge\nregarding both the labels and the balance between cover and stego is missing.\nOur objective is to identify a training dataset that allows for maximum\ngeneralization to our target. By exploring a grid of processing pipelines\nfostering CSM, we discovered a geometrical metric based on the chordal distance\nbetween subspaces spanned by DCTr features, that exhibits high correlation with\noperational regret while being not affected by the cover-stego balance. Our\ncontribution lies in the development of a strategy that enables the selection\nor derivation of customized training datasets, enhancing the overall\ngeneralization performance for a given target. Experimental validation\nhighlights that our geometry-based optimization strategy outperforms\ntraditional atomistic methods given reasonable assumptions. Additional\nresources are available at\ngithub.com/RonyAbecidan/LeveragingGeometrytoMitigateCSM.",
            "author": [
                "Rony Abecidan",
                "Vincent Itier",
                "J\u00e9r\u00e9mie Boulanger",
                "Patrick Bas",
                "Tom\u00e1\u0161 Pevn\u00fd"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04479v1",
                "http://arxiv.org/pdf/2310.04479v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CR",
                "cs.CV",
                "cs.MM",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04097v1",
            "title": "Impact of Gender on the Evaluation of Security Decisions",
            "updated": "2023-10-06T08:55:09Z",
            "published": "2023-10-06T08:55:09Z",
            "summary": "Security decisions are made by human analysts under uncertain conditions\nwhich leaves room for bias judgement. However, little is known about how\ndemographics like gender and education impact these judgments. We conducted an\nempirical study to investigate their influence on security decision\nevaluations, addressing this knowledge gap.",
            "author": [
                "Winnie Mbaka",
                "Katja Tuma"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04097v1",
                "http://arxiv.org/pdf/2310.04097v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04094v1",
            "title": "Searching COVID-19 clinical research using graphical abstracts",
            "updated": "2023-10-06T08:51:21Z",
            "published": "2023-10-06T08:51:21Z",
            "summary": "Objective. Graphical abstracts are small graphs of concepts that visually\nsummarize the main findings of scientific articles. While graphical abstracts\nare customarily used in scientific publications to anticipate and summarize\ntheir main results, we propose them as a means for expressing graph searches\nover existing literature. Materials and methods. We consider the COVID-19 Open\nResearch Dataset (CORD-19), a corpus of more than one million abstracts; each\nof them is described as a graph of co-occurring ontological terms, selected\nfrom the Unified Medical Language System (UMLS) and the Ontology of Coronavirus\nInfectious Disease (CIDO). Graphical abstracts are also expressed as graphs of\nontological terms, possibly augmented by utility terms describing their\ninteractions (e.g., \"associated with\", \"increases\", \"induces\"). We build a\nco-occurrence network of concepts mentioned in the corpus; we then identify the\nbest matches of graphical abstracts on the network. We exploit graph database\ntechnology and shortest-path queries. Results. We build a large co-occurrence\nnetwork, consisting of 128,249 entities and 47,198,965 relationships. A\nwell-designed interface allows users to explore the network by formulating or\nadapting queries in the form of an abstract; it produces a bibliography of\npublications, globally ranked; each publication is further associated with the\nspecific parts of the abstract that it explains, thereby allowing the user to\nunderstand each aspect of the matching. Discussion and Conclusion. Our approach\nsupports the process of scientific hypothesis formulation and evidence search;\nit can be reapplied to any scientific domain, although our mastering of UMLS\nmakes it most suited to clinical domains.",
            "author": [
                "Francesco Invernici",
                "Anna Bernasconi",
                "Stefano Ceri"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04094v1",
                "http://arxiv.org/pdf/2310.04094v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.DL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04080v2",
            "title": "Robust Average Networks for Monte Carlo Denoising",
            "updated": "2023-10-09T10:07:40Z",
            "published": "2023-10-06T08:18:04Z",
            "summary": "We present a method for converting denoising neural networks from spatial\ninto spatio-temporal ones by modifying the network architecture and loss\nfunction. We insert Robust Average blocks at arbitrary depths in the network\ngraph. Each block performs latent space interpolation with trainable weights\nand works on the sequence of image representations from the preceding spatial\ncomponents of the network. The temporal connections are kept live during\ntraining by forcing the network to predict a denoised frame from subsets of the\ninput sequence. Using temporal coherence for denoising improves image quality\nand reduces temporal flickering independent of scene or image complexity.",
            "author": [
                "Javor Kalojanov",
                "Kimball Thurston"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04080v2",
                "http://arxiv.org/pdf/2310.04080v2"
            ],
            "primary_category": "cs.GR",
            "category": [
                "cs.GR",
                "I.3.3"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04063v1",
            "title": "Globally Optimal Resource Allocation Design for Discrete Phase Shift\n  IRS-Assisted Multiuser Networks with Perfect and Imperfect CSI",
            "updated": "2023-10-06T07:41:02Z",
            "published": "2023-10-06T07:41:02Z",
            "summary": "Intelligent reflecting surfaces (IRSs) are a promising low-cost solution for\nachieving high spectral and energy efficiency in future communication systems\nby enabling the customization of wireless propagation environments. Despite the\nplethora of research on resource allocation design for IRS-assisted multiuser\ncommunication systems, the optimal design and the corresponding performance\nupper bound are still not fully understood. To bridge this gap in knowledge, in\nthis paper, we investigate the optimal resource allocation design for\nIRS-assisted multiuser systems employing practical discrete IRS phase shifters.\nIn particular, we jointly optimize the beamforming vector at the base station\n(BS) and the discrete IRS phase shifts to minimize the total transmit power for\nthe cases of perfect and imperfect channel state information (CSI) knowledge.\nTo this end, two novel algorithms based on the generalized Benders\ndecomposition (GBD) method are developed to obtain the globally optimal\nsolution for perfect and imperfect CSI, respectively. Moreover, to facilitate\npractical implementation, we propose two corresponding low-complexity\nsuboptimal algorithms with guaranteed convergence by capitalizing on successive\nconvex approximation (SCA). In particular, for imperfect CSI, we adopt a\nbounded error model to characterize the CSI uncertainty and propose a new\ntransformation to convexify the robust quality-of-service (QoS) constraints.\nOur numerical results confirm the optimality of the proposed GBD-based\nalgorithms for the considered system for both perfect and imperfect CSI.\nFurthermore, we unveil that both proposed SCA-based algorithms can achieve a\nclose-to-optimal performance within a few iterations. Moreover, compared with\nthe state-of-the-art solution based on the alternating optimization (AO)\nmethod, the proposed SCA-based scheme achieves a significant performance gain\nwith low complexity.",
            "author": [
                "Yifei Wu",
                "Dongfang Xu",
                "Derrick Wing Kwan Ng",
                "Robert Schober",
                "Wolfgang Gerstacker"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04063v1",
                "http://arxiv.org/pdf/2310.04063v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04056v1",
            "title": "Physics-assisted machine learning for THz spectroscopy: sensing moisture\n  on plant leaves",
            "updated": "2023-10-06T07:16:43Z",
            "published": "2023-10-06T07:16:43Z",
            "summary": "Signal processing techniques are of vital importance to bring THz\nspectroscopy to a maturity level to reach practical applications. In this work,\nwe illustrate the use of machine learning techniques for THz time-domain\nspectroscopy assisted by domain knowledge based on light-matter interactions.\nWe aim at the potential agriculture application to determine the amount of free\nwater on plant leaves, so-called leaf wetness. This quantity is important for\nunderstanding and predicting plant diseases that need leaf wetness for disease\ndevelopment. The overall transmission of a moist plant leaf for 12,000 distinct\nwater patterns was experimentally acquired using THz time-domain spectroscopy.\nWe report on key insights of applying decision trees and convolutional neural\nnetworks to the data using physics-motivated choices. Eventually, we discuss\nthe generalizability of these models to determine leaf wetness after testing\nthem on cases with increasing deviations from the training set.",
            "author": [
                "Milan Koumans",
                "Daan Meulendijks",
                "Haiko Middeljans",
                "Djero Peeters",
                "Jacob C. Douma",
                "Dook van Mechelen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04056v1",
                "http://arxiv.org/pdf/2310.04056v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP",
                "physics.app-ph",
                "physics.data-an"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04055v1",
            "title": "Kick Bad Guys Out! Zero-Knowledge-Proof-Based Anomaly Detection in\n  Federated Learning",
            "updated": "2023-10-06T07:09:05Z",
            "published": "2023-10-06T07:09:05Z",
            "summary": "Federated learning (FL) systems are vulnerable to malicious clients that\nsubmit poisoned local models to achieve their adversarial goals, such as\npreventing the convergence of the global model or inducing the global model to\nmisclassify some data. Many existing defense mechanisms are impractical in\nreal-world FL systems, as they require prior knowledge of the number of\nmalicious clients or rely on re-weighting or modifying submissions. This is\nbecause adversaries typically do not announce their intentions before\nattacking, and re-weighting might change aggregation results even in the\nabsence of attacks. To address these challenges in real FL systems, this paper\nintroduces a cutting-edge anomaly detection approach with the following\nfeatures: i) Detecting the occurrence of attacks and performing defense\noperations only when attacks happen; ii) Upon the occurrence of an attack,\nfurther detecting the malicious client models and eliminating them without\nharming the benign ones; iii) Ensuring honest execution of defense mechanisms\nat the server by leveraging a zero-knowledge proof mechanism. We validate the\nsuperior performance of the proposed approach with extensive experiments.",
            "author": [
                "Shanshan Han",
                "Wenxuan Wu",
                "Baturalp Buyukates",
                "Weizhao Jin",
                "Yuhang Yao",
                "Qifan Zhang",
                "Salman Avestimehr",
                "Chaoyang He"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04055v1",
                "http://arxiv.org/pdf/2310.04055v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04477v1",
            "title": "Higher-Order DeepTrails: Unified Approach to *Trails",
            "updated": "2023-10-06T06:54:11Z",
            "published": "2023-10-06T06:54:11Z",
            "summary": "Analyzing, understanding, and describing human behavior is advantageous in\ndifferent settings, such as web browsing or traffic navigation. Understanding\nhuman behavior naturally helps to improve and optimize the underlying\ninfrastructure or user interfaces. Typically, human navigation is represented\nby sequences of transitions between states. Previous work suggests to use\nhypotheses, representing different intuitions about the navigation to analyze\nthese transitions. To mathematically grasp this setting, first-order Markov\nchains are used to capture the behavior, consequently allowing to apply\ndifferent kinds of graph comparisons, but comes with the inherent drawback of\nlosing information about higher-order dependencies within the sequences. To\nthis end, we propose to analyze entire sequences using autoregressive language\nmodels, as they are traditionally used to model higher-order dependencies in\nsequences. We show that our approach can be easily adapted to model different\nsettings introduced in previous work, namely HypTrails, MixedTrails and even\nSubTrails, while at the same time bringing unique advantages: 1. Modeling\nhigher-order dependencies between state transitions, while 2. being able to\nidentify short comings in proposed hypotheses, and 3. naturally introducing a\nunified approach to model all settings. To show the expressiveness of our\napproach, we evaluate our approach on different synthetic datasets and conclude\nwith an exemplary analysis of a real-world dataset, examining the behavior of\nusers who interact with voice assistants.",
            "author": [
                "Tobias Koopmann",
                "Jan Pfister",
                "Andr\u00e9 Markus",
                "Astrid Carolus",
                "Carolin Wienrich",
                "Andreas Hotho"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04477v1",
                "http://arxiv.org/pdf/2310.04477v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04047v2",
            "title": "AUTOPARLLM: GNN-Guided Automatic Code Parallelization using Large\n  Language Models",
            "updated": "2023-10-09T02:35:47Z",
            "published": "2023-10-06T06:51:16Z",
            "summary": "Parallelizing sequentially written programs is a challenging task. Even\nexperienced developers need to spend considerable time finding parallelism\nopportunities and then actually writing parallel versions of sequentially\nwritten programs. To address this issue, we present AUTOPARLLM, a framework for\nautomatically discovering parallelism and generating the parallel version of\nthe sequentially written program. Our framework consists of two major\ncomponents: i) a heterogeneous Graph Neural Network (GNN) based parallelism\ndiscovery and parallel pattern detection module, and ii) an LLM-based code\ngenerator to generate the parallel counterpart of the sequential programs. We\nuse the GNN to learn the flow-aware characteristics of the programs to identify\nparallel regions in sequential programs and then construct an enhanced prompt\nusing the GNN's results for the LLM-based generator to finally produce the\nparallel counterparts of the sequential programs. We evaluate AUTOPARLLM on 11\napplications of 2 well-known benchmark suites: NAS Parallel Benchmark and\nRodinia Benchmark. Our results show that AUTOPARLLM is indeed effective in\nimproving the state-of-the-art LLM-based models for the task of parallel code\ngeneration in terms of multiple code generation metrics. AUTOPARLLM also\nimproves the average runtime of the parallel code generated by the\nstate-of-the-art LLMs by as high as 3.4% and 2.9% for the NAS Parallel\nBenchmark and Rodinia Benchmark respectively. Additionally, to overcome the\nissue that well-known metrics for translation evaluation have not been\noptimized to evaluate the quality of the generated parallel code, we propose\nOMPScore for evaluating the quality of the generated code. We show that\nOMPScore exhibits a better correlation with human judgment than existing\nmetrics, measured by up to 75% improvement of Spearman correlation.",
            "author": [
                "Quazi Ishtiaque Mahmud",
                "Ali TehraniJamsaz",
                "Hung D Phan",
                "Nesreen K. Ahmed",
                "Ali Jannesari"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04047v2",
                "http://arxiv.org/pdf/2310.04047v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04044v1",
            "title": "Graph-based 3D Collision-distance Estimation Network with Probabilistic\n  Graph Rewiring",
            "updated": "2023-10-06T06:39:58Z",
            "published": "2023-10-06T06:39:58Z",
            "summary": "We aim to solve the problem of data-driven collision-distance estimation\ngiven 3-dimensional (3D) geometries. Conventional algorithms suffer from low\naccuracy due to their reliance on limited representations, such as point\nclouds. In contrast, our previous graph-based model, GraphDistNet, achieves\nhigh accuracy using edge information but incurs higher message-passing costs\nwith growing graph size, limiting its applicability to 3D geometries. To\novercome these challenges, we propose GDN-R, a novel 3D graph-based estimation\nnetwork.GDN-R employs a layer-wise probabilistic graph-rewiring algorithm\nleveraging the differentiable Gumbel-top-K relaxation. Our method accurately\ninfers minimum distances through iterative graph rewiring and updating relevant\nembeddings. The probabilistic rewiring enables fast and robust embedding with\nrespect to unforeseen categories of geometries. Through 41,412 random benchmark\ntasks with 150 pairs of 3D objects, we show GDN-R outperforms state-of-the-art\nbaseline methods in terms of accuracy and generalizability. We also show that\nthe proposed rewiring improves the update performance reducing the size of the\nestimation model. We finally show its batch prediction and auto-differentiation\ncapabilities for trajectory optimization in both simulated and real-world\nscenarios.",
            "author": [
                "Minjae Song",
                "Yeseung Kim",
                "Daehyung Park"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04044v1",
                "http://arxiv.org/pdf/2310.04044v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04043v1",
            "title": "In the Blink of an Eye: Event-based Emotion Recognition",
            "updated": "2023-10-06T06:33:20Z",
            "published": "2023-10-06T06:33:20Z",
            "summary": "We introduce a wearable single-eye emotion recognition device and a real-time\napproach to recognizing emotions from partial observations of an emotion that\nis robust to changes in lighting conditions. At the heart of our method is a\nbio-inspired event-based camera setup and a newly designed lightweight Spiking\nEye Emotion Network (SEEN). Compared to conventional cameras, event-based\ncameras offer a higher dynamic range (up to 140 dB vs. 80 dB) and a higher\ntemporal resolution. Thus, the captured events can encode rich temporal cues\nunder challenging lighting conditions. However, these events lack texture\ninformation, posing problems in decoding temporal information effectively. SEEN\ntackles this issue from two different perspectives. First, we adopt\nconvolutional spiking layers to take advantage of the spiking neural network's\nability to decode pertinent temporal information. Second, SEEN learns to\nextract essential spatial cues from corresponding intensity frames and\nleverages a novel weight-copy scheme to convey spatial attention to the\nconvolutional spiking layers during training and inference. We extensively\nvalidate and demonstrate the effectiveness of our approach on a specially\ncollected Single-eye Event-based Emotion (SEE) dataset. To the best of our\nknowledge, our method is the first eye-based emotion recognition method that\nleverages event-based cameras and spiking neural network.",
            "author": [
                "Haiwei Zhang",
                "Jiqing Zhang",
                "Bo Dong",
                "Pieter Peers",
                "Wenwei Wu",
                "Xiaopeng Wei",
                "Felix Heide",
                "Xin Yang"
            ],
            "link": [
                "http://dx.doi.org/10.1145/3588432.3591511",
                "http://arxiv.org/abs/2310.04043v1",
                "http://arxiv.org/pdf/2310.04043v1"
            ],
            "primary_category": "cs.GR",
            "category": [
                "cs.GR",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04476v1",
            "title": "Strong transitivity of a graph",
            "updated": "2023-10-06T06:25:09Z",
            "published": "2023-10-06T06:25:09Z",
            "summary": "A vertex partition $\\pi = \\{V_1, V_2, \\ldots, V_k\\}$ of $G$ is called a\n\\emph{transitive partition} of size $k$ if $V_i$ dominates $V_j$ for all $1\\leq\ni<j\\leq k$. For two disjoint subsets $A$ and $B$ of $V$, we say $A$\n\\emph{strongly dominates} $B$ if for every vertex $y\\in B$, there exists a\nvertex $x\\in A$, such that $xy\\in E$ and $deg_G(x)\\geq deg_G(y)$. A vertex\npartition $\\pi = \\{V_1, V_2, \\ldots, V_k\\}$ of $G$ is called a \\emph{strong\ntransitive partition} of size $k$ if $V_i$ strongly dominates $V_j$ for all\n$1\\leq i<j\\leq k$. The \\textsc{Maximum Strong Transitivity Problem} is to find\na strong transitive partition of a given graph with the maximum number of\nparts. In this article, we initiate the study of this variation of transitive\npartition from algorithmic point of view. We show that the decision version of\nthis problem is NP-complete for chordal graphs. On the positive side, we prove\nthat this problem can be solved in linear time for trees and split graphs.",
            "author": [
                "Subhabrata Paul",
                "Kamal Santra"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04476v1",
                "http://arxiv.org/pdf/2310.04476v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04038v1",
            "title": "Joint Projection Learning and Tensor Decomposition Based Incomplete\n  Multi-view Clustering",
            "updated": "2023-10-06T06:19:16Z",
            "published": "2023-10-06T06:19:16Z",
            "summary": "Incomplete multi-view clustering (IMVC) has received increasing attention\nsince it is often that some views of samples are incomplete in reality. Most\nexisting methods learn similarity subgraphs from original incomplete multi-view\ndata and seek complete graphs by exploring the incomplete subgraphs of each\nview for spectral clustering. However, the graphs constructed on the original\nhigh-dimensional data may be suboptimal due to feature redundancy and noise.\nBesides, previous methods generally ignored the graph noise caused by the\ninter-class and intra-class structure variation during the transformation of\nincomplete graphs and complete graphs. To address these problems, we propose a\nnovel Joint Projection Learning and Tensor Decomposition Based method (JPLTD)\nfor IMVC. Specifically, to alleviate the influence of redundant features and\nnoise in high-dimensional data, JPLTD introduces an orthogonal projection\nmatrix to project the high-dimensional features into a lower-dimensional space\nfor compact feature learning.Meanwhile, based on the lower-dimensional space,\nthe similarity graphs corresponding to instances of different views are\nlearned, and JPLTD stacks these graphs into a third-order low-rank tensor to\nexplore the high-order correlations across different views. We further consider\nthe graph noise of projected data caused by missing samples and use a\ntensor-decomposition based graph filter for robust clustering.JPLTD decomposes\nthe original tensor into an intrinsic tensor and a sparse tensor. The intrinsic\ntensor models the true data similarities. An effective optimization algorithm\nis adopted to solve the JPLTD model. Comprehensive experiments on several\nbenchmark datasets demonstrate that JPLTD outperforms the state-of-the-art\nmethods. The code of JPLTD is available at https://github.com/weilvNJU/JPLTD.",
            "author": [
                "Wei Lv",
                "Chao Zhang",
                "Huaxiong Li",
                "Xiuyi Jia",
                "Chunlin Chen"
            ],
            "link": [
                "http://dx.doi.org/10.1109/TNNLS.2023.3306006",
                "http://arxiv.org/abs/2310.04038v1",
                "http://arxiv.org/pdf/2310.04038v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04036v1",
            "title": "Algorithmic study on $2$-transitivity of graphs",
            "updated": "2023-10-06T06:18:50Z",
            "published": "2023-10-06T06:18:50Z",
            "summary": "Let $G=(V, E)$ be a graph where $V$ and $E$ are the vertex and edge sets,\nrespectively. For two disjoint subsets $A$ and $B$ of $V$, we say $A$\n\\emph{dominates} $B$ if every vertex of $B$ is adjacent to at least one vertex\nof $A$. A vertex partition $\\pi = \\{V_1, V_2, \\ldots, V_k\\}$ of $G$ is called a\n\\emph{transitive partition} of size $k$ if $V_i$ dominates $V_j$ for all $1\\leq\ni<j\\leq k$. In this article, we study a variation of transitive partition,\nnamely \\emph{$2$-transitive partition}. For two disjoint subsets $A$ and $B$ of\n$V$, we say $A$ \\emph{$2$-dominates} $B$ if every vertex of $B$ is adjacent to\nat least two vertices of $A$. A vertex partition $\\pi = \\{V_1, V_2, \\ldots,\nV_k\\}$ of $G$ is called a \\emph{$2$-transitive partition} of size $k$ if $V_i$\n$2$-dominates $V_j$ for all $1\\leq i<j\\leq k$. The \\textsc{Maximum\n$2$-Transitivity Problem} is to find a $2$-transitive partition of a given\ngraph with the maximum number of parts. We show that the decision version of\nthis problem is NP-complete for chordal and bipartite graphs. On the positive\nside, we design three linear-time algorithms for solving \\textsc{Maximum\n$2$-Transitivity Problem} in trees, split and bipartite chain graphs.",
            "author": [
                "Subhabrata Paul",
                "Kamal Santra"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04036v1",
                "http://arxiv.org/pdf/2310.04036v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04028v1",
            "title": "Genetic prediction of quantitative traits: a machine learner's guide\n  focused on height",
            "updated": "2023-10-06T05:43:50Z",
            "published": "2023-10-06T05:43:50Z",
            "summary": "Machine learning and deep learning have been celebrating many successes in\nthe application to biological problems, especially in the domain of protein\nfolding. Another equally complex and important question has received relatively\nlittle attention by the machine learning community, namely the one of\nprediction of complex traits from genetics. Tackling this problem requires\nin-depth knowledge of the related genetics literature and awareness of various\nsubtleties associated with genetic data. In this guide, we provide an overview\nfor the machine learning community on current state of the art models and\nassociated subtleties which need to be taken into consideration when developing\nnew models for phenotype prediction. We use height as an example of a\ncontinuous-valued phenotype and provide an introduction to benchmark datasets,\nconfounders, feature selection, and common metrics.",
            "author": [
                "Lucie Bourguignon",
                "Caroline Weis",
                "Catherine R. Jutzeler",
                "Michael Adamer"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04028v1",
                "http://arxiv.org/pdf/2310.04028v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "q-bio.GN"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04018v1",
            "title": "Testing Higher-order Clusterability on graphs",
            "updated": "2023-10-06T05:07:50Z",
            "published": "2023-10-06T05:07:50Z",
            "summary": "Analysis of higher-order organizations, usually small connected subgraphs\ncalled motifs, is a fundamental task on complex networks. This paper studies a\nnew problem of testing higher-order clusterability: given query access to an\nundirected graph, can we judge whether this graph can be partitioned into a few\nclusters of highly-connected motifs? This problem is an extension of the former\nwork proposed by Czumaj et al. (STOC' 15), who recognized cluster structure on\ngraphs using the framework of property testing. In this paper, a good graph\ncluster on high dimensions is first defined for higher-order clustering. Then,\nquery lower bound is given for testing whether this kind of good cluster\nexists. Finally, an optimal sublinear-time algorithm is developed for testing\nclusterability based on triangles.",
            "author": [
                "Yifei Li",
                "Donghua Yang",
                "Jianzhong Li"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04018v1",
                "http://arxiv.org/pdf/2310.04018v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04013v1",
            "title": "A Survey of Mathematical Models on Somitogenesis",
            "updated": "2023-10-06T04:50:55Z",
            "published": "2023-10-06T04:50:55Z",
            "summary": "This paper presents a comprehensive survey of various established\nmathematical models pertaining to Somitogenesis, a biological process. The\nstudy begins by revisiting and replicating the findings from prominent research\npapers in this domain, subsequently offering a critical evaluation of the\nstrengths and weaknesses inherent in each approach. By synthesizing this\nknowledge, the paper aims to contribute to a deeper understanding of\nSomitogenesis, and pave the way for further advancements in the development of\nenhanced mathematical models for this intricate biological process. The\nconcluding section offers valuable insights and directions for prospective\nresearch in this field.",
            "author": [
                "Hanyu Song"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04013v1",
                "http://arxiv.org/pdf/2310.04013v1"
            ],
            "primary_category": "math.DS",
            "category": [
                "math.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03990v1",
            "title": "Generating scalable graph states in an atom-nanophotonic interface",
            "updated": "2023-10-06T03:33:32Z",
            "published": "2023-10-06T03:33:32Z",
            "summary": "Scalable graph states are essential for measurement-based quantum computation\nand many entanglement-assisted applications in quantum technologies. Generation\nof these multipartite entangled states requires a controllable and efficient\nquantum device with delicate design of generation protocol. Here we propose to\nprepare high-fidelity and scalable graph states in one and two dimensions,\nwhich can be tailored in an atom-nanophotonic cavity via state carving\ntechnique. We propose a systematic protocol to carve out unwanted state\ncomponents, which facilitates scalable graph states generations via adiabatic\ntransport of a definite number of atoms in optical tweezers. An analysis of\nstate fidelity is also presented, and the state preparation probability can be\noptimized via multiqubit state carvings and sequential single-photon probes.\nOur results showcase the capability of an atom-nanophotonic interface for\ncreating graph states and pave the way toward novel problem-specific\napplications using scalable high-dimensional graph states with stationary\nqubits.",
            "author": [
                "C. -H. Chien",
                "S. Goswami",
                "C. -C. Wu",
                "W. -S. Hiew",
                "Y. -C. Chen",
                "H. H. Jen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03990v1",
                "http://arxiv.org/pdf/2310.03990v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cond-mat.mes-hall",
                "physics.atom-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03988v1",
            "title": "Asymptotic distribution of degree--based topological indices",
            "updated": "2023-10-06T03:17:40Z",
            "published": "2023-10-06T03:17:40Z",
            "summary": "Topological indices play a significant role in mathematical chemistry. Given\na graph $\\mathcal{G}$ with vertex set $\\mathcal{V}=\\{1,2,\\dots,n\\}$ and edge\nset $\\mathcal{E}$, let $d_i$ be the degree of node $i$. The degree-based\ntopological index is defined as $\\mathcal{I}_n=$ $\\sum_{\\{i,j\\}\\in\n\\mathcal{E}}f(d_i,d_j)$, where $f(x,y)$ is a symmetric function. In this paper,\nwe investigate the asymptotic distribution of the degree-based topological\nindices of a heterogeneous Erd\\H{o}s-R\\'{e}nyi random graph. We show that after\nsuitably centered and scaled, the topological indices converges in distribution\nto the standard normal distribution. Interestingly, we find that the general\nRandi\\'{c} index with $f(x,y)=(xy)^{\\tau}$ for a constant $\\tau$ exhibits a\nphase change at $\\tau=-\\frac{1}{2}$.",
            "author": [
                "Mingao Yuan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03988v1",
                "http://arxiv.org/pdf/2310.03988v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "math.PR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03977v1",
            "title": "Perfect Alignment May be Poisonous to Graph Contrastive Learning",
            "updated": "2023-10-06T02:22:49Z",
            "published": "2023-10-06T02:22:49Z",
            "summary": "Graph Contrastive Learning (GCL) aims to learn node representations by\naligning positive pairs and separating negative ones. However, limited research\nhas been conducted on the inner law behind specific augmentations used in\ngraph-based learning. What kind of augmentation will help downstream\nperformance, how does contrastive learning actually influence downstream tasks,\nand why the magnitude of augmentation matters? This paper seeks to address\nthese questions by establishing a connection between augmentation and\ndownstream performance, as well as by investigating the generalization of\ncontrastive learning. Our findings reveal that GCL contributes to downstream\ntasks mainly by separating different classes rather than gathering nodes of the\nsame class. So perfect alignment and augmentation overlap which draw all\nintra-class samples the same can not explain the success of contrastive\nlearning. Then in order to comprehend how augmentation aids the contrastive\nlearning process, we conduct further investigations into its generalization,\nfinding that perfect alignment that draw positive pair the same could help\ncontrastive loss but is poisonous to generalization, on the contrary, imperfect\nalignment enhances the model's generalization ability. We analyse the result by\ninformation theory and graph spectrum theory respectively, and propose two\nsimple but effective methods to verify the theories. The two methods could be\neasily applied to various GCL algorithms and extensive experiments are\nconducted to prove its effectiveness.",
            "author": [
                "Jingyu Liu",
                "Huayi Tang",
                "Yong Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03977v1",
                "http://arxiv.org/pdf/2310.03977v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03965v2",
            "title": "Thought Propagation: An Analogical Approach to Complex Reasoning with\n  Large Language Models",
            "updated": "2023-10-09T00:35:22Z",
            "published": "2023-10-06T01:40:09Z",
            "summary": "Large Language Models (LLMs) have achieved remarkable success in reasoning\ntasks with the development of prompting methods. However, existing prompting\napproaches cannot reuse insights of solving similar problems and suffer from\naccumulated errors in multi-step reasoning, since they prompt LLMs to reason\n\\textit{from scratch}. To address these issues, we propose\n\\textbf{\\textit{Thought Propagation} (TP)}, which explores the analogous\nproblems and leverages their solutions to enhance the complex reasoning ability\nof LLMs. These analogous problems are related to the input one, with reusable\nsolutions and problem-solving strategies. Thus, it is promising to propagate\ninsights of solving previous analogous problems to inspire new problem-solving.\nTo achieve this, TP first prompts LLMs to propose and solve a set of analogous\nproblems that are related to the input one. Then, TP reuses the results of\nanalogous problems to directly yield a new solution or derive a\nknowledge-intensive plan for execution to amend the initial solution obtained\nfrom scratch. TP is compatible with existing prompting approaches, allowing\nplug-and-play generalization and enhancement in a wide range of tasks without\nmuch labor in task-specific prompt engineering. Experiments across three\nchallenging tasks demonstrate TP enjoys a substantial improvement over the\nbaselines by an average of 12\\% absolute increase in finding the optimal\nsolutions in Shortest-path Reasoning, 13\\% improvement of human preference in\nCreative Writing, and 15\\% enhancement in the task completion rate of LLM-Agent\nPlanning.",
            "author": [
                "Junchi Yu",
                "Ran He",
                "Rex Ying"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03965v2",
                "http://arxiv.org/pdf/2310.03965v2"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03961v1",
            "title": "Existence of martingale solutions to a nonlinearly coupled stochastic\n  fluid-structure interaction problem",
            "updated": "2023-10-06T01:10:28Z",
            "published": "2023-10-06T01:10:28Z",
            "summary": "In this paper we study a nonlinear stochastic fluid-structure interaction\nproblem with a multiplicative, white-in-time noise. The problem consists of the\nNavier-Stokes equations describing the flow of an incompressible, viscous fluid\nin a 2D cylinder interacting with an elastic lateral wall whose elastodynamics\nis described by a membrane/shell equation. The flow is driven by the inlet and\noutlet data, and by the stochastic forcing. The stochastic noise is applied\nboth to the fluid equations as a volumetric body force, and to the structure as\nan external forcing to the deformable fluid boundary. The fluid and the\nstructure are nonlinearly coupled via the kinematic and dynamic conditions\nassumed at the moving interface, which is a random variable not known a priori.\nThe geometric nonlinearity due to the nonlinear coupling requires the\ndevelopment of new techniques to capture martingale solutions for this class of\nstochastic fluid-structure interaction problems. We introduce a constructive\napproach based on a Lie splitting scheme and prove the existence of martingale\nsolutions to the system. To the best of our knowledge, this is the first result\nin the field of stochastic PDEs that addresses existence of solutions on moving\nfluid domains involving incompressible viscous fluids, where the displacement\nof the boundary and the fluid domain are random variables that are not known a\npriori and are parts of the solution itself.",
            "author": [
                "Krutika Tawri",
                "Suncica Canic"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03961v1",
                "http://arxiv.org/pdf/2310.03961v1"
            ],
            "primary_category": "math.AP",
            "category": [
                "math.AP",
                "math.PR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03942v1",
            "title": "Towards Distributed Quantum Computing by Qubit and Gate Graph\n  Partitioning Techniques",
            "updated": "2023-10-05T23:21:18Z",
            "published": "2023-10-05T23:21:18Z",
            "summary": "Distributed quantum computing is motivated by the difficulty in building\nlarge-scale, individual quantum computers. To solve that problem, a large\nquantum circuit is partitioned and distributed to small quantum computers for\nexecution. Partitions running on different quantum computers share quantum\ninformation using entangled Bell pairs. However, entanglement generation and\npurification introduces both a runtime and memory overhead on distributed\nquantum computing. In this paper we study that trade-off by proposing two\ntechniques for partitioning large quantum circuits and for distribution to\nsmall quantum computers. Our techniques map a quantum circuit to a graph\nrepresentation. We study two approaches: one that considers only gate\nteleportation, and another that considers both gate and state teleportation to\nachieve the distributed execution. Then we apply the METIS graph partitioning\nalgorithm to obtain the partitions and the number of entanglement requests\nbetween them. We use the SeQUeNCe quantum communication simulator to measure\nthe time required for generating all the entanglements required to execute the\ndistributed circuit. We find that the best partitioning technique will depend\non the specific circuit of interest.",
            "author": [
                "Marc Grau Davis",
                "Joaquin Chung",
                "Dirk Englund",
                "Rajkumar Kettimuthu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03942v1",
                "http://arxiv.org/pdf/2310.03942v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.ET"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03941v1",
            "title": "LaTeX: Language Pattern-aware Triggering Event Detection for Adverse\n  Experience during Pandemics",
            "updated": "2023-10-05T23:09:31Z",
            "published": "2023-10-05T23:09:31Z",
            "summary": "The COVID-19 pandemic has accentuated socioeconomic disparities across\nvarious racial and ethnic groups in the United States. While previous studies\nhave utilized traditional survey methods like the Household Pulse Survey (HPS)\nto elucidate these disparities, this paper explores the role of social media\nplatforms in both highlighting and addressing these challenges. Drawing from\nreal-time data sourced from Twitter, we analyzed language patterns related to\nfour major types of adverse experiences: loss of employment income (LI), food\nscarcity (FS), housing insecurity (HI), and unmet needs for mental health\nservices (UM). We first formulate a sparsity optimization problem that extracts\nlow-level language features from social media data sources. Second, we propose\nnovel constraints on feature similarity exploiting prior knowledge about the\nsimilarity of the language patterns among the adverse experiences. The proposed\nproblem is challenging to solve due to the non-convexity objective and\nnon-smoothness penalties. We develop an algorithm based on the alternating\ndirection method of multipliers (ADMM) framework to solve the proposed\nformulation. Extensive experiments and comparisons to other models on\nreal-world social media and the detection of adverse experiences justify the\nefficacy of our model.",
            "author": [
                "Kaiqun Fu",
                "Yangxiao Bai",
                "Weiwei Zhang",
                "Deepthi Kolady"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03941v1",
                "http://arxiv.org/pdf/2310.03941v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03936v1",
            "title": "Universal insulating-to-metallic crossover in tight-binding random\n  geometric graphs",
            "updated": "2023-10-05T22:57:09Z",
            "published": "2023-10-05T22:57:09Z",
            "summary": "Within the scattering matrix approach to electronic transport, the scattering\nand transport properties of tight-binding random graphs are analyzed. In\nparticular, we compute the scattering matrix elements, the transmission, the\nchannel-to-channel transmission distributions (including the total transmission\ndistribution), the shot noise power, and the elastic enhancement factor. Two\ngraph models are considered: random geometric graphs and bipartite random\ngeometric graphs. The results show an insulating to a metallic crossover in the\nscattering and transport properties by increasing the average degree of the\ngraphs from small to large values. Also, the scattering and transport\nproperties are shown to be invariant under a scaling parameter depending on the\naverage degree and the graph size. Furthermore, for large connectivity and in\nthe perfect coupling regime, the scattering and transport properties of both\ngraph models are well described by the random matrix theory predictions of\nelectronic transport, except for bipartite graphs in particular scattering\nsetups.",
            "author": [
                "A. M. Mart\u00ednez-Arg\u00fcello",
                "K. B. Hidalgo-Castro",
                "J. A. M\u00e9ndez-Berm\u00fadez"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03936v1",
                "http://arxiv.org/pdf/2310.03936v1"
            ],
            "primary_category": "cond-mat.dis-nn",
            "category": [
                "cond-mat.dis-nn",
                "G.1 (Primary), H.1 (Secondary)",
                "G.1; H.1"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03932v1",
            "title": "Bridging Low-level Geometry to High-level Concepts in Visual Servoing of\n  Robot Manipulation Task Using Event Knowledge Graphs and Vision-Language\n  Models",
            "updated": "2023-10-05T22:32:35Z",
            "published": "2023-10-05T22:32:35Z",
            "summary": "In this paper, we propose a framework of building knowledgeable robot control\nin the scope of smart human-robot interaction, by empowering a basic\nuncalibrated visual servoing controller with contextual knowledge through the\njoint usage of event knowledge graphs (EKGs) and large-scale pretrained\nvision-language models (VLMs). The framework is expanded in twofold: first, we\ninterpret low-level image geometry as high-level concepts, allowing us to\nprompt VLMs and to select geometric features of points and lines for motor\ncontrol skills; then, we create an event knowledge graph (EKG) to conceptualize\na robot manipulation task of interest, where the main body of the EKG is\ncharacterized by an executable behavior tree, and the leaves by semantic\nconcepts relevant to the manipulation context. We demonstrate, in an\nuncalibrated environment with real robot trials, that our method lowers the\nreliance of human annotation during task interfacing, allows the robot to\nperform activities of daily living more easily by treating low-level\ngeometric-based motor control skills as high-level concepts, and is beneficial\nin building cognitive thinking for smart robot applications.",
            "author": [
                "Chen Jiang",
                "Martin Jagersand"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03932v1",
                "http://arxiv.org/pdf/2310.03932v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03923v1",
            "title": "Open-Fusion: Real-time Open-Vocabulary 3D Mapping and Queryable Scene\n  Representation",
            "updated": "2023-10-05T21:57:36Z",
            "published": "2023-10-05T21:57:36Z",
            "summary": "Precise 3D environmental mapping is pivotal in robotics. Existing methods\noften rely on predefined concepts during training or are time-intensive when\ngenerating semantic maps. This paper presents Open-Fusion, a groundbreaking\napproach for real-time open-vocabulary 3D mapping and queryable scene\nrepresentation using RGB-D data. Open-Fusion harnesses the power of a\npre-trained vision-language foundation model (VLFM) for open-set semantic\ncomprehension and employs the Truncated Signed Distance Function (TSDF) for\nswift 3D scene reconstruction. By leveraging the VLFM, we extract region-based\nembeddings and their associated confidence maps. These are then integrated with\n3D knowledge from TSDF using an enhanced Hungarian-based feature-matching\nmechanism. Notably, Open-Fusion delivers outstanding annotation-free 3D\nsegmentation for open-vocabulary without necessitating additional 3D training.\nBenchmark tests on the ScanNet dataset against leading zero-shot methods\nhighlight Open-Fusion's superiority. Furthermore, it seamlessly combines the\nstrengths of region-based VLFM and TSDF, facilitating real-time 3D scene\ncomprehension that includes object concepts and open-world semantics. We\nencourage the readers to view the demos on our project page:\nhttps://uark-aicv.github.io/OpenFusion",
            "author": [
                "Kashu Yamazaki",
                "Taisei Hanyu",
                "Khoa Vo",
                "Thang Pham",
                "Minh Tran",
                "Gianfranco Doretto",
                "Anh Nguyen",
                "Ngan Le"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03923v1",
                "http://arxiv.org/pdf/2310.03923v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03916v1",
            "title": "Toward a Foundation Model for Time Series Data",
            "updated": "2023-10-05T21:44:50Z",
            "published": "2023-10-05T21:44:50Z",
            "summary": "A foundation model is a machine learning model trained on a large and diverse\nset of data, typically using self-supervised learning-based pre-training\ntechniques, that can be adapted to various downstream tasks. However, current\nresearch on time series pre-training has mostly focused on models pre-trained\nsolely on data from a single domain, resulting in a lack of knowledge about\nother types of time series. However, current research on time series\npre-training has predominantly focused on models trained exclusively on data\nfrom a single domain. As a result, these models possess domain-specific\nknowledge that may not be easily transferable to time series from other\ndomains. In this paper, we aim to develop an effective time series foundation\nmodel by leveraging unlabeled samples from multiple domains. To achieve this,\nwe repurposed the publicly available UCR Archive and evaluated four existing\nself-supervised learning-based pre-training methods, along with a novel method,\non the datasets. We tested these methods using four popular neural network\narchitectures for time series to understand how the pre-training methods\ninteract with different network designs. Our experimental results show that\npre-training improves downstream classification tasks by enhancing the\nconvergence of the fine-tuning process. Furthermore, we found that the proposed\npre-training method, when combined with the Transformer model, outperforms the\nalternatives.",
            "author": [
                "Chin-Chia Michael Yeh",
                "Xin Dai",
                "Huiyuan Chen",
                "Yan Zheng",
                "Yujie Fan",
                "Audrey Der",
                "Vivian Lai",
                "Zhongfang Zhuang",
                "Junpeng Wang",
                "Liang Wang",
                "Wei Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03916v1",
                "http://arxiv.org/pdf/2310.03916v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03900v1",
            "title": "A Game Approach to Multi-dimensional Opinion Dynamics in Social Networks\n  with Stubborn Strategist Agents",
            "updated": "2023-10-05T21:12:57Z",
            "published": "2023-10-05T21:12:57Z",
            "summary": "In a social network, individuals express their opinions on several\ninterdependent topics, and therefore the evolution of their opinions on these\ntopics is also mutually dependent. In this work, we propose a differential game\nmodel for the multi-dimensional opinion formation of a social network whose\npopulation of agents interacts according to a communication graph. Each\nindividual's opinion evolves according to an aggregation of disagreements\nbetween the agent's opinions and its graph neighbors on multiple interdependent\ntopics exposed to an unknown extraneous disturbance. For a social network with\nstrategist agents the opinions evolve over time with respect to the\nminimization of a quadratic cost function that solely represents each\nindividual's motives against the disturbance. We find the unique\nNash/worst-case equilibrium solution for the proposed differential game model\nof coupled multi-dimensional opinions under an open-loop information structure.\nMoreover, we propose a distributed implementation of the Nash/worst-case\nequilibrium solution. We examine the non-distributed and proposed distributed\nopen-loop Nash/worst-case strategies on a small social network with strategist\nagents in a two-dimensional opinion space. Then we compare the opinions evolved\nbased on the Nash/worst-case strategy with the opinions corresponding to social\noptimality actions for non-strategist agents.",
            "author": [
                "Hossein B. Jond",
                "Aykut Y\u0131ld\u0131z"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03900v1",
                "http://arxiv.org/pdf/2310.03900v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03899v1",
            "title": "CrysFormer: Protein Structure Prediction via 3d Patterson Maps and\n  Partial Structure Attention",
            "updated": "2023-10-05T21:10:22Z",
            "published": "2023-10-05T21:10:22Z",
            "summary": "Determining the structure of a protein has been a decades-long open question.\nA protein's three-dimensional structure often poses nontrivial computation\ncosts, when classical simulation algorithms are utilized. Advances in the\ntransformer neural network architecture -- such as AlphaFold2 -- achieve\nsignificant improvements for this problem, by learning from a large dataset of\nsequence information and corresponding protein structures. Yet, such methods\nonly focus on sequence information; other available prior knowledge, such as\nprotein crystallography and partial structure of amino acids, could be\npotentially utilized. To the best of our knowledge, we propose the first\ntransformer-based model that directly utilizes protein crystallography and\npartial structure information to predict the electron density maps of proteins.\nVia two new datasets of peptide fragments (2-residue and 15-residue) , we\ndemonstrate our method, dubbed \\texttt{CrysFormer}, can achieve accurate\npredictions, based on a much smaller dataset size and with reduced computation\ncosts.",
            "author": [
                "Chen Dun",
                "Qiutai Pan",
                "Shikai Jin",
                "Ria Stevens",
                "Mitchell D. Miller",
                "George N. Phillips, Jr.",
                "Anastasios Kyrillidis"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03899v1",
                "http://arxiv.org/pdf/2310.03899v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03891v1",
            "title": "HDNA: A graph-based change detection in HTML pages(Deface Attack\n  Detection)",
            "updated": "2023-10-05T20:49:54Z",
            "published": "2023-10-05T20:49:54Z",
            "summary": "In this paper, a new approach called HDNA (HTML DNA) is introduced for\nanalyzing and comparing Document Object Model (DOM) trees in order to detect\ndifferences in HTML pages. This method assigns an identifier to each HTML page\nbased on its structure, which proves to be particularly useful for detecting\nvariations caused by server-side updates, user interactions or potential\nsecurity risks. The process involves preprocessing the HTML content generating\na DOM tree and calculating the disparities between two or more trees. By\nassigning weights to the nodes valuable insights about their hierarchical\nimportance are obtained. The effectiveness of the HDNA approach has been\ndemonstrated in identifying changes in DOM trees even when dynamically\ngenerated content is involved. Not does this method benefit web developers,\ntesters, and security analysts by offering a deeper understanding of how web\npages evolve. It also helps ensure the functionality and performance of web\napplications. Additionally, it enables detection and response to\nvulnerabilities that may arise from modifications in DOM structures. As the web\necosystem continues to evolve HDNA proves to be a tool, for individuals engaged\nin web development, testing, or security analysis.",
            "author": [
                "Mahdi Akhi",
                "Nona Ghazizadeh"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03891v1",
                "http://arxiv.org/pdf/2310.03891v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03889v1",
            "title": "Audio Event-Relational Graph Representation Learning for Acoustic Scene\n  Classification",
            "updated": "2023-10-05T20:48:59Z",
            "published": "2023-10-05T20:48:59Z",
            "summary": "Most deep learning-based acoustic scene classification (ASC) approaches\nidentify scenes based on acoustic features converted from audio clips\ncontaining mixed information entangled by polyphonic audio events (AEs).\nHowever, these approaches have difficulties in explaining what cues they use to\nidentify scenes. This paper conducts the first study on disclosing the\nrelationship between real-life acoustic scenes and semantic embeddings from the\nmost relevant AEs. Specifically, we propose an event-relational graph\nrepresentation learning (ERGL) framework for ASC to classify scenes, and\nsimultaneously answer clearly and straightly which cues are used in\nclassifying. In the event-relational graph, embeddings of each event are\ntreated as nodes, while relationship cues derived from each pair of nodes are\ndescribed by multi-dimensional edge features. Experiments on a real-life ASC\ndataset show that the proposed ERGL achieves competitive performance on ASC by\nlearning embeddings of only a limited number of AEs. The results show the\nfeasibility of recognizing diverse acoustic scenes based on the audio\nevent-relational graph. Visualizations of graph representations learned by ERGL\nare available here (https://github.com/Yuanbo2020/ERGL).",
            "author": [
                "Yuanbo Hou",
                "Siyang Song",
                "Chuang Yu",
                "Wenwu Wang",
                "Dick Botteldooren"
            ],
            "link": [
                "http://dx.doi.org/10.1109/LSP.2023.3319233",
                "http://arxiv.org/abs/2310.03889v1",
                "http://arxiv.org/pdf/2310.03889v1"
            ],
            "primary_category": "eess.AS",
            "category": [
                "eess.AS",
                "cs.SD"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03878v1",
            "title": "Automatic and Human-AI Interactive Text Generation",
            "updated": "2023-10-05T20:26:15Z",
            "published": "2023-10-05T20:26:15Z",
            "summary": "In this tutorial, we focus on text-to-text generation, a class of natural\nlanguage generation (NLG) tasks, that takes a piece of text as input and then\ngenerates a revision that is improved according to some specific criteria\n(e.g., readability or linguistic styles), while largely retaining the original\nmeaning and the length of the text. This includes many useful applications,\nsuch as text simplification, paraphrase generation, style transfer, etc. In\ncontrast to text summarization and open-ended text completion (e.g., story),\nthe text-to-text generation tasks we discuss in this tutorial are more\nconstrained in terms of semantic consistency and targeted language styles. This\nlevel of control makes these tasks ideal testbeds for studying the ability of\nmodels to generate text that is both semantically adequate and stylistically\nappropriate. Moreover, these tasks are interesting from a technical standpoint,\nas they require complex combinations of lexical and syntactical\ntransformations, stylistic control, and adherence to factual knowledge, -- all\nat once. With a special focus on text simplification and revision, this\ntutorial aims to provide an overview of the state-of-the-art natural language\ngeneration research from four major aspects -- Data, Models, Human-AI\nCollaboration, and Evaluation -- and to discuss and showcase a few significant\nand recent advances: (1) the use of non-retrogressive approaches; (2) the shift\nfrom fine-tuning to prompting with large language models; (3) the development\nof new learnable metric and fine-grained human evaluation framework; (4) a\ngrowing body of studies and datasets on non-English languages; (5) the rise of\nHCI+NLP+Accessibility interdisciplinary research to create real-world writing\nassistant systems.",
            "author": [
                "Yao Dou",
                "Philippe Laban",
                "Claire Gardent",
                "Wei Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03878v1",
                "http://arxiv.org/pdf/2310.03878v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03877v1",
            "title": "Sturm-Hurwitz Theorem for quantum graphs",
            "updated": "2023-10-05T20:20:14Z",
            "published": "2023-10-05T20:20:14Z",
            "summary": "We prove upper and lower bounds for the number of zeroes of linear\ncombinations of Schr\\\"odinger eigenfunctions on metric (quantum) graphs. These\nbounds are distinct from both the interval and manifolds. We complement these\nbounds by giving non-trivial examples for the lower bound as well as sharp\nexamples for the upper bound. In particular, we show that even tree graphs\ndiffer from the interval with respect to the nodal count of linear combinations\nof eigenfunctions. This stands in distinction to previous results which show\nthat all tree graphs have to same eigenfunction nodal count as the interval.",
            "author": [
                "Ram Band",
                "Philippe Charron"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03877v1",
                "http://arxiv.org/pdf/2310.03877v1"
            ],
            "primary_category": "math-ph",
            "category": [
                "math-ph",
                "math.MP",
                "math.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03875v1",
            "title": "Sheaves of Measures and KMS-Weights on Topological Graph Algebras",
            "updated": "2023-10-05T20:11:56Z",
            "published": "2023-10-05T20:11:56Z",
            "summary": "We show that the collection of regular Borel measures on a second-countable\nlocally compact Hausdorff space has the structure of a sheaf. With this we give\nan alternate description of the pullback of a regular Borel measure along a\nlocal homeomorphism. We are able to use these tools to give a description of\nthe KMS-weights for the gauge-action on the graph C*-algebra of a\nsecond-countable topological graph in terms of sub-invariant measures on the\nvertex space of said topological graph.",
            "author": [
                "Jonas Eidesen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03875v1",
                "http://arxiv.org/pdf/2310.03875v1"
            ],
            "primary_category": "math.OA",
            "category": [
                "math.OA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03865v1",
            "title": "Model Complexity of Program Phases",
            "updated": "2023-10-05T19:50:15Z",
            "published": "2023-10-05T19:50:15Z",
            "summary": "In resource limited computing systems, sequence prediction models must\noperate under tight constraints. Various models are available that cater to\nprediction under these conditions that in some way focus on reducing the cost\nof implementation. These resource constrained sequence prediction models, in\npractice, exhibit a fundamental tradeoff between the cost of implementation and\nthe quality of its predictions. This fundamental tradeoff seems to be largely\nunexplored for models for different tasks. Here we formulate the necessary\ntheory and an associated empirical procedure to explore this tradeoff space for\na particular family of machine learning models such as deep neural networks. We\nanticipate that the knowledge of the behavior of this tradeoff may be\nbeneficial in understanding the theoretical and practical limits of creation\nand deployment of models for resource constrained tasks.",
            "author": [
                "Arjun Karuvally",
                "J. Eliot B. Moss"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03865v1",
                "http://arxiv.org/pdf/2310.03865v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.IT",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03863v1",
            "title": "Visualising emergent phenomena at oxide interfaces",
            "updated": "2023-10-05T19:48:06Z",
            "published": "2023-10-05T19:48:06Z",
            "summary": "Knowledge of atomic-level details of structure, chemistry, and electronic\nstates is paramount for a comprehensive understanding of emergent properties at\noxide interfaces. We utilise a novel methodology based on atomic-scale electron\nenergy loss spectroscopy (EELS) to spatially map the electronic states tied to\nthe formation of a two-dimensional electron gas (2DEG) at the prototypical\nnon-polar/polar $TiO_2$/$LaAlO_3$ interface. Combined with differential phase\ncontrast analysis we directly visualise the microscopic locations of ions and\ncharge and find that 2DEG states and $Ti^{3+}$ defect states exhibit different\nspatial distributions. Supported by density functional theory (DFT) and\ninelastic scattering simulations we examine the role of oxygen vacancies in\n2DEG formation. Our work presents a general pathway to directly image emergent\nphenomena at interfaces using this unique combination of arising microscopy\ntechniques with machine learning assisted data analysis procedures.",
            "author": [
                "Michael Oberaigner",
                "Manuel Ederer",
                "Sandeep Kumar Chaluvadi",
                "Pasquale Orgiani",
                "Regina Ciancio",
                "Stefan L\u00f6ffler",
                "Gerald Kothleitner",
                "Daniel Knez"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03863v1",
                "http://arxiv.org/pdf/2310.03863v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03849v1",
            "title": "On two conjectures about the intersection of longest paths and cycles",
            "updated": "2023-10-05T19:18:19Z",
            "published": "2023-10-05T19:18:19Z",
            "summary": "A conjecture attributed to Smith states that every pair of longest cycles in\na $k$-connected graph intersect each other in at least $k$ vertices. In this\npaper, we show that every pair of longest cycles in a~$k$-connected graph on\n$n$ vertices intersect each other in at least~$\\min\\{n,8k-n-16\\}$ vertices,\nwhich confirms Smith's conjecture when $k\\geq (n+16)/7$. An analog conjecture\nfor paths instead of cycles was stated by Hippchen. By a simple reduction, we\nrelate both conjectures, showing that Hippchen's conjecture is valid when\neither $k \\leq 6$ or $k \\geq (n+9)/7$.",
            "author": [
                "Juan Guti\u00e9rrez",
                "Christian Valqui"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03849v1",
                "http://arxiv.org/pdf/2310.03849v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "cs.DM",
                "05C38",
                "G.2.2"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03840v1",
            "title": "Contextualized Structural Self-supervised Learning for Ontology Matching",
            "updated": "2023-10-05T18:51:33Z",
            "published": "2023-10-05T18:51:33Z",
            "summary": "Ontology matching (OM) entails the identification of semantic relationships\nbetween concepts within two or more knowledge graphs (KGs) and serves as a\ncritical step in integrating KGs from various sources. Recent advancements in\ndeep OM models have harnessed the power of transformer-based language models\nand the advantages of knowledge graph embedding. Nevertheless, these OM models\nstill face persistent challenges, such as a lack of reference alignments,\nruntime latency, and unexplored different graph structures within an end-to-end\nframework. In this study, we introduce a novel self-supervised learning OM\nframework with input ontologies, called LaKERMap. This framework capitalizes on\nthe contextual and structural information of concepts by integrating implicit\nknowledge into transformers. Specifically, we aim to capture multiple\nstructural contexts, encompassing both local and global interactions, by\nemploying distinct training objectives. To assess our methods, we utilize the\nBio-ML datasets and tasks. The findings from our innovative approach reveal\nthat LaKERMap surpasses state-of-the-art systems in terms of alignment quality\nand inference time. Our models and codes are available here:\nhttps://github.com/ellenzhuwang/lakermap.",
            "author": [
                "Zhu Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03840v1",
                "http://arxiv.org/pdf/2310.03840v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03812v1",
            "title": "Fishnets: Information-Optimal, Scalable Aggregation for Sets and Graphs",
            "updated": "2023-10-05T18:01:04Z",
            "published": "2023-10-05T18:01:04Z",
            "summary": "Set-based learning is an essential component of modern deep learning and\nnetwork science. Graph Neural Networks (GNNs) and their edge-free counterparts\nDeepsets have proven remarkably useful on ragged and topologically challenging\ndatasets. The key to learning informative embeddings for set members is a\nspecified aggregation function, usually a sum, max, or mean. We propose\nFishnets, an aggregation strategy for learning information-optimal embeddings\nfor sets of data for both Bayesian inference and graph aggregation. We\ndemonstrate that i) Fishnets neural summaries can be scaled optimally to an\narbitrary number of data objects, ii) Fishnets aggregations are robust to\nchanges in data distribution, unlike standard deepsets, iii) Fishnets saturate\nBayesian information content and extend to regimes where MCMC techniques fail\nand iv) Fishnets can be used as a drop-in aggregation scheme within GNNs. We\nshow that by adopting a Fishnets aggregation scheme for message passing, GNNs\ncan achieve state-of-the-art performance versus architecture size on\nogbn-protein data over existing benchmarks with a fraction of learnable\nparameters and faster training time.",
            "author": [
                "T. Lucas Makinen",
                "Justin Alsing",
                "Benjamin D. Wandelt"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03812v1",
                "http://arxiv.org/pdf/2310.03812v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03742v2",
            "title": "A High-Performance Design, Implementation, Deployment, and Evaluation of\n  The Slim Fly Network",
            "updated": "2023-10-10T01:19:43Z",
            "published": "2023-10-05T17:59:52Z",
            "summary": "Novel low-diameter network topologies such as Slim Fly (SF) offer significant\ncost and power advantages over the established Fat Tree, Clos, or Dragonfly. To\nspearhead the adoption of low-diameter networks, we design, implement, deploy,\nand evaluate the first real-world SF installation. We focus on deployment,\nmanagement, and operational aspects of our test cluster with 200 servers and\ncarefully analyze performance. We demonstrate techniques for simple cabling and\ncabling validation as well as a novel high-performance routing architecture for\nInfiniBand-based low-diameter topologies. Our real-world benchmarks show SF's\nstrong performance for many modern workloads such as deep neural network\ntraining, graph analytics, or linear algebra kernels. SF outperforms\nnon-blocking Fat Trees in scalability while offering comparable or better\nperformance and lower cost for large network sizes. Our work can facilitate\ndeploying SF while the associated (open-source) routing architecture is fully\nportable and applicable to accelerate any low-diameter interconnect.",
            "author": [
                "Nils Blach",
                "Maciej Besta",
                "Daniele De Sensi",
                "Jens Domke",
                "Hussein Harake",
                "Shigang Li",
                "Patrick Iff",
                "Marek Konieczny",
                "Kartik Lakhotia",
                "Ales Kubicek",
                "Marcel Ferrari",
                "Fabrizio Petrini",
                "Torsten Hoefler"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03742v2",
                "http://arxiv.org/pdf/2310.03742v2"
            ],
            "primary_category": "cs.NI",
            "category": [
                "cs.NI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03714v1",
            "title": "DSPy: Compiling Declarative Language Model Calls into Self-Improving\n  Pipelines",
            "updated": "2023-10-05T17:37:25Z",
            "published": "2023-10-05T17:37:25Z",
            "summary": "The ML community is rapidly exploring techniques for prompting language\nmodels (LMs) and for stacking them into pipelines that solve complex tasks.\nUnfortunately, existing LM pipelines are typically implemented using hard-coded\n\"prompt templates\", i.e. lengthy strings discovered via trial and error. Toward\na more systematic approach for developing and optimizing LM pipelines, we\nintroduce DSPy, a programming model that abstracts LM pipelines as text\ntransformation graphs, i.e. imperative computational graphs where LMs are\ninvoked through declarative modules. DSPy modules are parameterized, meaning\nthey can learn (by creating and collecting demonstrations) how to apply\ncompositions of prompting, finetuning, augmentation, and reasoning techniques.\nWe design a compiler that will optimize any DSPy pipeline to maximize a given\nmetric. We conduct two case studies, showing that succinct DSPy programs can\nexpress and optimize sophisticated LM pipelines that reason about math word\nproblems, tackle multi-hop retrieval, answer complex questions, and control\nagent loops. Within minutes of compiling, a few lines of DSPy allow GPT-3.5 and\nllama2-13b-chat to self-bootstrap pipelines that outperform standard few-shot\nprompting (generally by over 25% and 65%, respectively) and pipelines with\nexpert-created demonstrations (by up to 5-46% and 16-40%, respectively). On top\nof that, DSPy programs compiled to open and relatively small LMs like\n770M-parameter T5 and llama2-13b-chat are competitive with approaches that rely\non expert-written prompt chains for proprietary GPT-3.5. DSPy is available at\nhttps://github.com/stanfordnlp/dspy",
            "author": [
                "Omar Khattab",
                "Arnav Singhvi",
                "Paridhi Maheshwari",
                "Zhiyuan Zhang",
                "Keshav Santhanam",
                "Sri Vardhamanan",
                "Saiful Haq",
                "Ashutosh Sharma",
                "Thomas T. Joshi",
                "Hanna Moazam",
                "Heather Miller",
                "Matei Zaharia",
                "Christopher Potts"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03714v1",
                "http://arxiv.org/pdf/2310.03714v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.IR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03694v1",
            "title": "What would it cost to connect the unconnected? Estimating global\n  universal broadband infrastructure investment",
            "updated": "2023-10-05T17:12:34Z",
            "published": "2023-10-05T17:12:34Z",
            "summary": "Roughly 3 billion citizens remain offline, equating to approximately 40\npercent of the global population. Therefore, providing Internet connectivity is\nan essential part of the Sustainable Development Goals (SDGs) (Goal 9). In this\npaper a high-resolution global model is developed to evaluate the necessary\ninvestment requirements to achieve affordable universal broadband. The results\nindicate that approximately $418 billion needs to be mobilized to connect all\nunconnected citizens globally (targeting 40-50 GB/Month per user with 95\npercent reliability). The bulk of additional investment is for emerging market\neconomies (73 percent) and low-income developing countries (24 percent). To our\nknowledge, the paper contributes the first high-resolution global assessment\nwhich quantifies universal broadband investment at the sub-national level to\nachieve SDG Goal 9.",
            "author": [
                "Edward J. Oughton",
                "David Amaglobeli",
                "Marian Moszoro"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03694v1",
                "http://arxiv.org/pdf/2310.03694v1"
            ],
            "primary_category": "econ.GN",
            "category": [
                "econ.GN",
                "q-fin.EC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03669v1",
            "title": "LumiNet: The Bright Side of Perceptual Knowledge Distillation",
            "updated": "2023-10-05T16:43:28Z",
            "published": "2023-10-05T16:43:28Z",
            "summary": "In knowledge distillation research, feature-based methods have dominated due\nto their ability to effectively tap into extensive teacher models. In contrast,\nlogit-based approaches are considered to be less adept at extracting hidden\n'dark knowledge' from teachers. To bridge this gap, we present LumiNet, a novel\nknowledge-transfer algorithm designed to enhance logit-based distillation. We\nintroduce a perception matrix that aims to recalibrate logits through\nadjustments based on the model's representation capability. By meticulously\nanalyzing intra-class dynamics, LumiNet reconstructs more granular inter-class\nrelationships, enabling the student model to learn a richer breadth of\nknowledge. Both teacher and student models are mapped onto this refined matrix,\nwith the student's goal being to minimize representational discrepancies.\nRigorous testing on benchmark datasets (CIFAR-100, ImageNet, and MSCOCO)\nattests to LumiNet's efficacy, revealing its competitive edge over leading\nfeature-based methods. Moreover, in exploring the realm of transfer learning,\nwe assess how effectively the student model, trained using our method, adapts\nto downstream tasks. Notably, when applied to Tiny ImageNet, the transferred\nfeatures exhibit remarkable performance, further underscoring LumiNet's\nversatility and robustness in diverse settings. With LumiNet, we hope to steer\nthe research discourse towards a renewed interest in the latent capabilities of\nlogit-based knowledge distillation.",
            "author": [
                "Md. Ismail Hossain",
                "M M Lutfe Elahi",
                "Sameera Ramasinghe",
                "Ali Cheraghian",
                "Fuad Rahman",
                "Nabeel Mohammed",
                "Shafin Rahman"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03669v1",
                "http://arxiv.org/pdf/2310.03669v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03637v1",
            "title": "Solving Degree Bounds For Iterated Polynomial Systems",
            "updated": "2023-10-05T16:10:14Z",
            "published": "2023-10-05T16:10:14Z",
            "summary": "For Arithmetization-Oriented ciphers and hash functions Gr\\\"obner basis\nattacks are generally considered as the most competitive attack vector.\nUnfortunately, the complexity of Gr\\\"obner basis algorithms is only understood\nfor special cases, and it is needless to say that these cases do not apply to\nmost cryptographic polynomial systems. Therefore, cryptographers have to resort\nto experiments, extrapolations and hypotheses to assess the security of their\ndesigns. One established measure to quantify the complexity of linear\nalgebra-based Gr\\\"obner basis algorithms is the so-called solving degree.\nCaminata \\& Gorla revealed that under a certain genericity condition on a\npolynomial system the solving degree is always upper bounded by the\nCastelnuovo-Mumford regularity and henceforth by the Macaulay bound, which only\ntakes the degrees and number of variables of the input polynomials into\naccount. In this paper we extend their framework to iterated polynomial\nsystems, the standard polynomial model for symmetric ciphers and hash\nfunctions. In particular, we prove solving degree bounds for various attacks on\nMiMC, Feistel-MiMC, Feistel-MiMC-Hash, Hades and GMiMC. Our bounds fall in line\nwith the hypothesized complexity of Gr\\\"obner basis attacks on these designs,\nand to the best of our knowledge this is the first time that a mathematical\nproof for these complexities is provided.\n  Moreover, by studying polynomials with degree falls we can prove lower bounds\non the Castelnuovo-Mumford regularity for attacks on MiMC, Feistel-MiMC and\nFeistel-MiMC-Hash provided that only a few solutions of the corresponding\niterated polynomial system originate from the base field. Hence,\nregularity-based solving degree estimations can never surpass a certain\nthreshold, a desirable property for cryptographic polynomial systems.",
            "author": [
                "Matthias Johann Steiner"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03637v1",
                "http://arxiv.org/pdf/2310.03637v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "math.AC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03635v1",
            "title": "CLEVRER-Humans: Describing Physical and Causal Events the Human Way",
            "updated": "2023-10-05T16:09:48Z",
            "published": "2023-10-05T16:09:48Z",
            "summary": "Building machines that can reason about physical events and their causal\nrelationships is crucial for flexible interaction with the physical world.\nHowever, most existing physical and causal reasoning benchmarks are exclusively\nbased on synthetically generated events and synthetic natural language\ndescriptions of causal relationships. This design brings up two issues. First,\nthere is a lack of diversity in both event types and natural language\ndescriptions; second, causal relationships based on manually-defined heuristics\nare different from human judgments. To address both shortcomings, we present\nthe CLEVRER-Humans benchmark, a video reasoning dataset for causal judgment of\nphysical events with human labels. We employ two techniques to improve data\ncollection efficiency: first, a novel iterative event cloze task to elicit a\nnew representation of events in videos, which we term Causal Event Graphs\n(CEGs); second, a data augmentation technique based on neural language\ngenerative models. We convert the collected CEGs into questions and answers to\nbe consistent with prior work. Finally, we study a collection of baseline\napproaches for CLEVRER-Humans question-answering, highlighting the great\nchallenges set forth by our benchmark.",
            "author": [
                "Jiayuan Mao",
                "Xuelin Yang",
                "Xikun Zhang",
                "Noah D. Goodman",
                "Jiajun Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03635v1",
                "http://arxiv.org/pdf/2310.03635v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL",
                "cs.CV",
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03624v1",
            "title": "High-Degrees-of-Freedom Dynamic Neural Fields for Robot Self-Modeling\n  and Motion Planning",
            "updated": "2023-10-05T16:01:29Z",
            "published": "2023-10-05T16:01:29Z",
            "summary": "A robot self-model is a task-agnostic representation of the robot's physical\nmorphology that can be used for motion planning tasks in absence of classical\ngeometric kinematic models. In particular, when the latter are hard to engineer\nor the robot's kinematics change unexpectedly, human-free self-modeling is a\nnecessary feature of truly autonomous agents. In this work, we leverage neural\nfields to allow a robot to self-model its kinematics as a neural-implicit query\nmodel learned only from 2D images annotated with camera poses and\nconfigurations. This enables significantly greater applicability than existing\napproaches which have been dependent on depth images or geometry knowledge. To\nthis end, alongside a curricular data sampling strategy, we propose a new\nencoder-based neural density field architecture for dynamic object-centric\nscenes conditioned on high numbers of degrees of freedom (DOFs). In a 7-DOF\nrobot test setup, the learned self-model achieves a Chamfer-L2 distance of 2%\nof the robot's workspace dimension. We demonstrate the capabilities of this\nmodel on a motion planning task as an exemplary downstream application.",
            "author": [
                "Lennart Schulze",
                "Hod Lipson"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03624v1",
                "http://arxiv.org/pdf/2310.03624v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03618v1",
            "title": "CLASSify: A Web-Based Tool for Machine Learning",
            "updated": "2023-10-05T15:51:36Z",
            "published": "2023-10-05T15:51:36Z",
            "summary": "Machine learning classification problems are widespread in bioinformatics,\nbut the technical knowledge required to perform model training, optimization,\nand inference can prevent researchers from utilizing this technology. This\narticle presents an automated tool for machine learning classification problems\nto simplify the process of training models and producing results while\nproviding informative visualizations and insights into the data. This tool\nsupports both binary and multiclass classification problems, and it provides\naccess to a variety of models and methods. Synthetic data can be generated\nwithin the interface to fill missing values, balance class labels, or generate\nentirely new datasets. It also provides support for feature evaluation and\ngenerates explainability scores to indicate which features influence the output\nthe most. We present CLASSify, an open-source tool for simplifying the user\nexperience of solving classification problems without the need for knowledge of\nmachine learning.",
            "author": [
                "Aaron D. Mullen",
                "Samuel E. Armstrong",
                "Jeff Talbert",
                "V. K. Cody Bumgardner"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03618v1",
                "http://arxiv.org/pdf/2310.03618v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.DC",
                "cs.HC",
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03615v1",
            "title": "Animatable Virtual Humans: Learning pose-dependent human representations\n  in UV space for interactive performance synthesis",
            "updated": "2023-10-05T15:49:44Z",
            "published": "2023-10-05T15:49:44Z",
            "summary": "We propose a novel representation of virtual humans for highly realistic\nreal-time animation and rendering in 3D applications. We learn pose dependent\nappearance and geometry from highly accurate dynamic mesh sequences obtained\nfrom state-of-the-art multiview-video reconstruction. Learning pose-dependent\nappearance and geometry from mesh sequences poses significant challenges, as it\nrequires the network to learn the intricate shape and articulated motion of a\nhuman body. However, statistical body models like SMPL provide valuable\na-priori knowledge which we leverage in order to constrain the dimension of the\nsearch space enabling more efficient and targeted learning and define\npose-dependency. Instead of directly learning absolute pose-dependent geometry,\nwe learn the difference between the observed geometry and the fitted SMPL\nmodel. This allows us to encode both pose-dependent appearance and geometry in\nthe consistent UV space of the SMPL model. This approach not only ensures a\nhigh level of realism but also facilitates streamlined processing and rendering\nof virtual humans in real-time scenarios.",
            "author": [
                "Wieland Morgenstern",
                "Milena T. Bagdasarian",
                "Anna Hilsmann",
                "Peter Eisert"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03615v1",
                "http://arxiv.org/pdf/2310.03615v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03605v3",
            "title": "FASER: Binary Code Similarity Search through the use of Intermediate\n  Representations",
            "updated": "2023-11-29T14:30:29Z",
            "published": "2023-10-05T15:36:35Z",
            "summary": "Being able to identify functions of interest in cross-architecture software\nis useful whether you are analysing for malware, securing the software supply\nchain or conducting vulnerability research. Cross-Architecture Binary Code\nSimilarity Search has been explored in numerous studies and has used a wide\nrange of different data sources to achieve its goals. The data sources\ntypically used draw on common structures derived from binaries such as function\ncontrol flow graphs or binary level call graphs, the output of the disassembly\nprocess or the outputs of a dynamic analysis approach. One data source which\nhas received less attention is binary intermediate representations. Binary\nIntermediate representations possess two interesting properties: they are cross\narchitecture by their very nature and encode the semantics of a function\nexplicitly to support downstream usage. Within this paper we propose Function\nas a String Encoded Representation (FASER) which combines long document\ntransformers with the use of intermediate representations to create a model\ncapable of cross architecture function search without the need for manual\nfeature engineering, pre-training or a dynamic analysis step. We compare our\napproach against a series of baseline approaches for two tasks; A general\nfunction search task and a targeted vulnerability search task. Our approach\ndemonstrates strong performance across both tasks, performing better than all\nbaseline approaches.",
            "author": [
                "Josh Collyer",
                "Tim Watson",
                "Iain Phillips"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03605v3",
                "http://arxiv.org/pdf/2310.03605v3"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.AI",
                "cs.IR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03601v1",
            "title": "The Nash-Williams orientation theorem for graphs with countably many\n  ends",
            "updated": "2023-10-05T15:29:24Z",
            "published": "2023-10-05T15:29:24Z",
            "summary": "Nash-Williams proved in 1960 that a finite graph admits a $k$-arc-connected\norientation if and only if it is $2k$-edge-connected, and conjectured that the\nsame result should hold for all infinite graphs, too.\n  Progress on Nash-Williams's problem was made by C. Thomassen, who proved in\n2016 that all $8k$-edge-connected infinite graphs admit a $k$-arc connected\norientation, and by the first author, who recently showed that\nedge-connectivity of $4k$ suffices for locally-finite, 1-ended graphs.\n  In the present article, we establish the optimal bound $2k$ in\nNash-Williams's conjecture for all locally finite graphs with countably many\nends.",
            "author": [
                "Amena Assem",
                "Marcel Koloschin",
                "Max Pitz"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03601v1",
                "http://arxiv.org/pdf/2310.03601v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C20, 05C40, 05C63"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03591v1",
            "title": "Impact of Artificial Intelligence on Electrical and Electronics\n  Engineering Productivity in the Construction Industry",
            "updated": "2023-10-05T15:14:48Z",
            "published": "2023-10-05T15:14:48Z",
            "summary": "Artificial intelligence (AI) can revolutionize the development industry,\nprimarily electrical and electronics engineering. By automating recurring\nduties, AI can grow productivity and efficiency in creating. For instance, AI\ncan research constructing designs, discover capability troubles, and generate\nanswers, reducing the effort and time required for manual analysis. AI also can\nbe used to optimize electricity consumption in buildings, which is a critical\ndifficulty in the construction enterprise. Via machines gaining knowledge of\nalgorithms to investigate electricity usage patterns, AI can discover areas\nwherein power may be stored and offer guidelines for enhancements. This can\nresult in significant value financial savings and reduced carbon emissions.\nMoreover, AI may be used to improve the protection of creation websites. By\nstudying statistics from sensors and cameras, AI can locate capacity dangers\nand alert workers to take suitable action. This could help save you from\ninjuries and accidents on production sites, lowering the chance for workers and\nenhancing overall safety in the enterprise. The impact of AI on electric and\nelectronics engineering productivity inside the creation industry is enormous.\nAI can transform how we layout, build, and function buildings by automating\nordinary duties, optimising electricity intake, and enhancing safety. However,\nensuring that AI is used ethically and responsibly and that the advantages are\nshared fairly throughout the enterprise is essential.",
            "author": [
                "Nwosu Obinnaya Chikezie Victor"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03591v1",
                "http://arxiv.org/pdf/2310.03591v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03584v1",
            "title": "Minimum number of arcs in $k$-critical digraphs with order at most\n  $2k-1$",
            "updated": "2023-10-05T15:07:21Z",
            "published": "2023-10-05T15:07:21Z",
            "summary": "The dichromatic number $\\vec{\\chi}(D)$ of a digraph $D$ is the least integer\n$k$ for which $D$ has a coloring with $k$ colors such that there is no\nmonochromatic directed cycle in $D$. The digraphs considered here are finite\nand may have antiparallel arcs, but no parallel arcs. A digraph $D$ is called\n$k$-critical if each proper subdigraph $D'$ of $D$ satisfies\n$\\vec{\\chi}(D')<\\vec{\\chi}(D)=k$. For integers $k$ and $n$, let\n$\\overrightarrow{\\mathrm{ext}}(k,n)$ denote the minimum number of arcs possible\nin a $k$-critical digraph of order $n$. It is easy to show that\n$\\overrightarrow{\\mathrm{ext}}(2,n)=n$ for all $n\\geq 2$, and\n$\\overrightarrow{\\mathrm{ext}}(3,n)\\geq 2n$ for all possible $n$, where\nequality holds if and only if $n$ is odd and $n\\geq 3$. As a main result we\nprove that if $n, k$ and $p$ are integers with $n=k+p$ and $2\\leq p \\leq k-1$,\nthen $\\overrightarrow{\\mathrm{ext}}(k,n)=2({\\binom{n}{2}} - (p^2+1))$, and we\ngive an exact characterisation of $k$-critical digraphs for which equality\nholds. This generalizes a result about critical graphs obtained in 1963 by\nTibor Gallai.",
            "author": [
                "Lucas Picasarri-Arrieta",
                "Michael Stiebitz"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03584v1",
                "http://arxiv.org/pdf/2310.03584v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "cs.DM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03548v1",
            "title": "A theoretical view on the T-web statistical description of the cosmic\n  web",
            "updated": "2023-10-05T13:58:54Z",
            "published": "2023-10-05T13:58:54Z",
            "summary": "The classification of the cosmic web into different environments is both a\ntool to study in more detail the formation of halos and galaxies via the link\nbetween their properties and the large-scale environment and as a class of\nobjects whose statistics contain cosmological information. In this paper, we\npresent an analytical framework to compute the probability of the different\nenvironments in the cosmic web based on the T-web formalism that classifies\nstructures in four different classes (voids, walls, filaments, knots) by\nstudying the eigenvalues of the tidal tensor (Hessian of the gravitational\npotential). This method relies on studying the eigenvalues of the tidal tensor\nwith respect to a given threshold and thus requires the knowledge of the JPDF\nof those eigenvalues. We perform a change of variables in terms of minimally\ncorrelated rotational invariants and we study their distribution in the linear\nregime of structure formation, and in the quasi-linear regime with the help of\na Gram-Charlier expansion and tree-order Eulerian perturbation theory. This\nexpansion allows us to predict the probability of the different environments in\nthe density field at a given smoothing scale as a function of the chosen\nthreshold and redshift. We check the validity of our predictions by comparing\nthose predictions to measurements made in the N-body Quijote simulations. We\nnotably find that scaling the threshold value with the non-linear amplitude of\nfluctuations allows us to capture almost entirely the redshift evolution of the\nprobability of the environments, even if we assume that the density field is\nGaussian (corresponding to the linear regime of structure formation). We also\nshow that adding mild non-Gaussian corrections in the form of third-order\ncumulants of the field provides even more precise predictions for cosmic web\nabundances up to scales as small as ~5 Mpc/h and redshifts down to z~0.",
            "author": [
                "Emma Ay\u00e7oberry",
                "Alexandre Barthelemy",
                "Sandrine Codis"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03548v1",
                "http://arxiv.org/pdf/2310.03548v1"
            ],
            "primary_category": "astro-ph.CO",
            "category": [
                "astro-ph.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03506v2",
            "title": "Indicated total domination game",
            "updated": "2023-10-06T14:04:18Z",
            "published": "2023-10-05T12:37:08Z",
            "summary": "A vertex $u$ in a graph $G$ totally dominates a vertex $v$ if $u$ is adjacent\nto $v$ in $G$. A total dominating set of $G$ is a set $S$ of vertices of $G$\nsuch that every vertex of $G$ is totally dominated by a vertex in $S$. The\nindicated total domination game is played on a graph $G$ by two players,\nDominator and Staller, who take turns making a move. In each of his moves,\nDominator indicates a vertex $v$ of the graph that has not been totally\ndominated in the previous moves, and Staller chooses (or selects) any vertex\nadjacent to $v$ that has not yet been played, and adds it to a set $D$ that is\nbeing built during the game. The game ends when every vertex is totally\ndominated, that is, when $D$ is a total dominating set of $G$. The goal of\nDominator is to minimize the size of $D$, while Staller wants just the\nopposite. Providing that both players are playing optimally with respect to\ntheir goals, the size of the resulting set $D$ is the indicated total\ndomination number of $G$, denoted by $\\gamma_t^{\\rm i}(G)$. In this paper we\npresent several results on indicated total domination game. Among other results\nwe prove the Indicated Total Continuation Principle, and we show that the\nindicated total domination number of a graph is bounded below by the well\nstudied upper total domination number.",
            "author": [
                "Michael A. Henning",
                "Douglas F. Rall"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03506v2",
                "http://arxiv.org/pdf/2310.03506v2"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C65, 05C69"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03777v1",
            "title": "PrIeD-KIE: Towards Privacy Preserved Document Key Information Extraction",
            "updated": "2023-10-05T12:13:00Z",
            "published": "2023-10-05T12:13:00Z",
            "summary": "In this paper, we introduce strategies for developing private Key Information\nExtraction (KIE) systems by leveraging large pretrained document foundation\nmodels in conjunction with differential privacy (DP), federated learning (FL),\nand Differentially Private Federated Learning (DP-FL). Through extensive\nexperimentation on six benchmark datasets (FUNSD, CORD, SROIE, WildReceipts,\nXFUND, and DOCILE), we demonstrate that large document foundation models can be\neffectively fine-tuned for the KIE task under private settings to achieve\nadequate performance while maintaining strong privacy guarantees. Moreover, by\nthoroughly analyzing the impact of various training and model parameters on\nmodel performance, we propose simple yet effective guidelines for achieving an\noptimal privacy-utility trade-off for the KIE task under global DP. Finally, we\nintroduce FeAm-DP, a novel DP-FL algorithm that enables efficiently upscaling\nglobal DP from a standalone context to a multi-client federated environment. We\nconduct a comprehensive evaluation of the algorithm across various client and\nprivacy settings, and demonstrate its capability to achieve comparable\nperformance and privacy guarantees to standalone DP, even when accommodating an\nincreasing number of participating clients. Overall, our study offers valuable\ninsights into the development of private KIE systems, and highlights the\npotential of document foundation models for privacy-preserved Document AI\napplications. To the best of authors' knowledge, this is the first work that\nexplores privacy preserved document KIE using document foundation models.",
            "author": [
                "Saifullah Saifullah",
                "Stefan Agne",
                "Andreas Dengel",
                "Sheraz Ahmed"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03777v1",
                "http://arxiv.org/pdf/2310.03777v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03492v1",
            "title": "Analyzing multimodal probability measures with autoencoders",
            "updated": "2023-10-05T12:03:41Z",
            "published": "2023-10-05T12:03:41Z",
            "summary": "Finding collective variables to describe some important coarse-grained\ninformation on physical systems, in particular metastable states, remains a key\nissue in molecular dynamics. Recently, machine learning techniques have been\nintensively used to complement and possibly bypass expert knowledge in order to\nconstruct collective variables. Our focus here is on neural network approaches\nbased on autoencoders. We study some relevant mathematical properties of the\nloss function considered for training autoencoders, and provide physical\ninterpretations based on conditional variances and minimum energy paths. We\nalso consider various extensions in order to better describe physical systems,\nby incorporating more information on transition states at saddle points, and/or\nallowing for multiple decoders in order to describe several transition paths.\nOur results are illustrated on toy two dimensional systems and on alanine\ndipeptide.",
            "author": [
                "Tony Leli\u00e8vre",
                "Thomas Pigeon",
                "Gabriel Stoltz",
                "Wei Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03492v1",
                "http://arxiv.org/pdf/2310.03492v1"
            ],
            "primary_category": "physics.chem-ph",
            "category": [
                "physics.chem-ph",
                "cond-mat.stat-mech"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03478v1",
            "title": "RGBManip: Monocular Image-based Robotic Manipulation through Active\n  Object Pose Estimation",
            "updated": "2023-10-05T11:46:09Z",
            "published": "2023-10-05T11:46:09Z",
            "summary": "Robotic manipulation requires accurate perception of the environment, which\nposes a significant challenge due to its inherent complexity and constantly\nchanging nature. In this context, RGB image and point-cloud observations are\ntwo commonly used modalities in visual-based robotic manipulation, but each of\nthese modalities have their own limitations. Commercial point-cloud\nobservations often suffer from issues like sparse sampling and noisy output due\nto the limits of the emission-reception imaging principle. On the other hand,\nRGB images, while rich in texture information, lack essential depth and 3D\ninformation crucial for robotic manipulation. To mitigate these challenges, we\npropose an image-only robotic manipulation framework that leverages an\neye-on-hand monocular camera installed on the robot's parallel gripper. By\nmoving with the robot gripper, this camera gains the ability to actively\nperceive object from multiple perspectives during the manipulation process.\nThis enables the estimation of 6D object poses, which can be utilized for\nmanipulation. While, obtaining images from more and diverse viewpoints\ntypically improves pose estimation, it also increases the manipulation time. To\naddress this trade-off, we employ a reinforcement learning policy to\nsynchronize the manipulation strategy with active perception, achieving a\nbalance between 6D pose accuracy and manipulation efficiency. Our experimental\nresults in both simulated and real-world environments showcase the\nstate-of-the-art effectiveness of our approach. %, which, to the best of our\nknowledge, is the first to achieve robust real-world robotic manipulation\nthrough active pose estimation. We believe that our method will inspire further\nresearch on real-world-oriented robotic manipulation.",
            "author": [
                "Boshi An",
                "Yiran Geng",
                "Kai Chen",
                "Xiaoqi Li",
                "Qi Dou",
                "Hao Dong"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03478v1",
                "http://arxiv.org/pdf/2310.03478v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03475v1",
            "title": "Fair Division with Allocator's Preference",
            "updated": "2023-10-05T11:41:39Z",
            "published": "2023-10-05T11:41:39Z",
            "summary": "We consider the fair allocation problem of indivisible items. Most previous\nwork focuses on fairness and/or efficiency among agents given agents'\npreferences. However, besides the agents, the allocator as the resource owner\nmay also be involved in many real-world scenarios, e.g., heritage division. The\nallocator has the inclination to obtain a fair or efficient allocation based on\nher own preference over the items and to whom each item is allocated. In this\npaper, we propose a new model and focus on the following two problems: 1) Is it\npossible to find an allocation that is fair for both the agents and the\nallocator? 2) What is the complexity of maximizing the allocator's social\nwelfare while satisfying the agents' fairness?\n  We consider the two fundamental fairness criteria: envy-freeness and\nproportionality. For the first problem, we study the existence of an allocation\nthat is envy-free up to $c$ goods (EF-$c$) or proportional up to $c$ goods\n(PROP-$c$) from both the agents' and the allocator's perspectives, in which\nsuch an allocation is called doubly EF-$c$ or doubly PROP-$c$ respectively.\nWhen the allocator's utility depends exclusively on the items (but not to whom\nan item is allocated), we prove that a doubly EF-$1$ allocation always exists.\nFor the general setting where the allocator has a preference over the items and\nto whom each item is allocated, we prove that a doubly EF-$1$ allocation always\nexists for two agents, a doubly PROP-$2$ allocation always exists for binary\nvaluations, and a doubly PROP-$O(\\log n)$ allocation always exists in general.\n  For the second problem, we provide various (in)approximability results in\nwhich the gaps between approximation and inapproximation ratios are\nasymptotically closed under most settings.\n  Most results are based on novel technical tools including the chromatic\nnumbers of the Kneser graphs and linear programming-based analysis.",
            "author": [
                "Xiaolin Bu",
                "Zihao Li",
                "Shengxin Liu",
                "Jiaxin Song",
                "Biaoshuai Tao"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03475v1",
                "http://arxiv.org/pdf/2310.03475v1"
            ],
            "primary_category": "cs.GT",
            "category": [
                "cs.GT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03469v1",
            "title": "FPT Approximations for Packing and Covering Problems Parameterized by\n  Elimination Distance and Even Less",
            "updated": "2023-10-05T11:24:47Z",
            "published": "2023-10-05T11:24:47Z",
            "summary": "For numerous graph problems in the realm of parameterized algorithms, using\nthe size of a smallest deletion set (called a modulator) into well-understood\ngraph families as parameterization has led to a long and successful line of\nresearch. Recently, however, there has been an extensive study of structural\nparameters that are potentially much smaller than the modulator size. In\nparticular, recent papers [Jansen et al. STOC 2021; Agrawal et al. SODA 2022]\nhave studied parameterization by the size of the modulator to a graph family\n$\\mathcal{H}$ ($\\textbf{mod}_{\\mathcal{H}}$), elimination distance to\n$\\mathcal{H}$ ($\\textbf{ed}_{\\mathcal{H}}$), and $\\mathcal{H}$-treewidth\n($\\textbf{tw}_{\\mathcal{H}}$). While these new parameters have been\nsuccessfully exploited to design fast exact algorithms their utility\n(especially that of latter two) in the context of approximation algorithms is\nmostly unexplored.\n  The conceptual contribution of this paper is to present novel algorithmic\nmeta-theorems that expand the impact of these structural parameters to the area\nof FPT Approximation, mirroring their utility in the design of exact FPT\nalgorithms. Precisely, we show that if a covering or packing problem is\ndefinable in Monadic Second Order Logic and has a property called Finite\nInteger Index, then the existence of an FPT Approximation Scheme (FPT-AS, i.e.,\n($1\\pm \\epsilon$)-approximation) parameterized these three parameters is in\nfact equivalent. As concrete exemplifications of our meta-theorems, we obtain\nFPT-ASes for well-studied graph problems such as Vertex Cover, Feedback Vertex\nSet, Cycle Packing and Dominating Set, parameterized by these three parameters.",
            "author": [
                "Tanmay Inamdar",
                "Lawqueen Kanesh",
                "Madhumita Kundu",
                "M. S. Ramanujan",
                "Saket Saurabh"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03469v1",
                "http://arxiv.org/pdf/2310.03469v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03468v1",
            "title": "State independent QKD",
            "updated": "2023-10-05T11:22:50Z",
            "published": "2023-10-05T11:22:50Z",
            "summary": "We present an adaptive procedure for aligning quantum non-locality\nexperiments without any knowledge of the two-qudit state shared by the\nparticipating parties. The quantum state produced by the source, its unitary\nevolution as well as the actual measurement bases remain unknown to both\nparties at all times. The entanglement of the quantum state helps establish\ndesired correlations between individual measurement bases of the two distant\nparties. We implement the procedure in a fiber-based quantum key distribution\n(QKD) setup with polarization-entangled photons, where we do not rely on any\nadditional alignment tools such as lasers or polarizers. In a QKD scenario the\nprocedure can be done without any additional measurements as those that are\nperformed regardless.",
            "author": [
                "Robert Kindler",
                "Johannes Handsteiner",
                "Jaroslav Kysela",
                "Kuntuo Zhu",
                "Bo Liu",
                "Anton Zeilinger"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03468v1",
                "http://arxiv.org/pdf/2310.03468v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03429v1",
            "title": "An Extended Phase Graph-based framework for DANTE-SPACE simulations\n  including physiological, temporal, and spatial variations",
            "updated": "2023-10-05T10:14:23Z",
            "published": "2023-10-05T10:14:23Z",
            "summary": "Purpose: The DANTE-SPACE sequence facilitates three-dimensional intracranial\nvessel wall imaging with simultaneous suppression of blood and cerebrospinal\nfluid (CSF). However, the achieved image contrast depends closely on the\nselected sequence parameters, and the clinical use of the sequence is limited\nin vivo by observed signal variations in the vessel wall, CSF, and blood. This\npaper introduces a comprehensive DANTE-SPACE simulation framework, with the aim\nof providing a better understanding of the underlying contrast mechanisms and\nfacilitating improved parameter selection and contrast optimization.\n  Methods: An Extended Phase Graph (EPG) formalism was developed for efficient\nspin ensemble simulation of the DANTE-SPACE sequence. Physiological processes\nsuch as pulsatile flow velocity variation, varying flow directions, intravoxel\ndephasing, diffusion, and B1+ effects were included in the framework to\nrepresent the mechanisms behind the achieved signal levels accurately.\n  Results: Intravoxel velocity averaging improved temporal stability and\nrobustness against small velocity changes. Time-varying pulsatile velocity\nvariation affected CSF simulations, introducing periods of near-zero velocity\nand partial rephasing. Inclusion of diffusion effects was found to\nsubstantially reduce the CSF signal. Blood flow trajectory variations had minor\neffects, but B1+ differences along the trajectory reduced DANTE efficiency in\nlow-B1+ areas. Introducing low-velocity pulsatility of both CSF and vessel wall\nhelped explain the in vivo observed signal heterogeneity in both tissue types.\n  Conclusion: The presented simulation framework facilitates a more\ncomprehensive optimization of DANTE-SPACE sequence parameters. Furthermore, the\nsimulation framework helps to explain observed contrasts in acquired data.",
            "author": [
                "Matthijs H. S. de Buck",
                "Peter Jezzard",
                "Aaron T. Hess"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03429v1",
                "http://arxiv.org/pdf/2310.03429v1"
            ],
            "primary_category": "physics.med-ph",
            "category": [
                "physics.med-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03424v1",
            "title": "Neural Language Model Pruning for Automatic Speech Recognition",
            "updated": "2023-10-05T10:01:32Z",
            "published": "2023-10-05T10:01:32Z",
            "summary": "We study model pruning methods applied to Transformer-based neural network\nlanguage models for automatic speech recognition. We explore three aspects of\nthe pruning frame work, namely criterion, method and scheduler, analyzing their\ncontribution in terms of accuracy and inference speed. To the best of our\nknowledge, such in-depth analyses on large-scale recognition systems has not\nbeen reported in the literature. In addition, we propose a variant of low-rank\napproximation suitable for incrementally compressing models, and delivering\nmultiple models with varied target sizes. Among other results, we show that a)\ndata-driven pruning outperforms magnitude-driven in several scenarios; b)\nincremental pruning achieves higher accuracy compared to one-shot pruning,\nespecially when targeting smaller sizes; and c) low-rank approximation presents\nthe best trade-off between size reduction and inference speed-up for moderate\ncompression.",
            "author": [
                "Leonardo Emili",
                "Thiago Fraga-Silva",
                "Ernest Pusateri",
                "Markus Nu\u00dfbaum-Thom",
                "Youssef Oualil"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03424v1",
                "http://arxiv.org/pdf/2310.03424v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03412v1",
            "title": "Localization transition in non-Hermitian systems depending on\n  reciprocity and hopping asymmetry",
            "updated": "2023-10-05T09:31:02Z",
            "published": "2023-10-05T09:31:02Z",
            "summary": "We investigated the single-particle Anderson localization problem for\nnon-Hermitian systems on directed graphs. Various undirected standard random\ngraph models were modified by controlling reciprocity and hopping asymmetry\nparameters. We found the emergence of left, biorthogonal and right localized\nstates depending on both parameters and graph structure properties such as node\ndegree $d$. For directed random graphs, the occurrence of biorthogonal\nlocalization near exceptional points is described analytically and numerically.\nThe clustering of localized states near the center of the spectrum and the\ncorresponding mobility edge for left and right states are shown numerically.\nStructural features responsible for localization, such as topologically\ninvariant nodes or drain and sources, were also described. Considering the\ndiagonal disorder, we observed the disappearance of localization dependence on\nreciprocity around $W \\sim 20$ for a random regular graph $d=4$. With a small\ndiagonal disorder, the average biorthogonal fractal dimension drastically\nreduces. Around $W \\sim 5$ localization scars occur within the spectrum,\nalternating as vertical bands of clustering of left and right localized states.",
            "author": [
                "Daniil Kochergin",
                "Vasilii Tiselko",
                "Arsenii Onuchin"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03412v1",
                "http://arxiv.org/pdf/2310.03412v1"
            ],
            "primary_category": "cond-mat.dis-nn",
            "category": [
                "cond-mat.dis-nn"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03402v1",
            "title": "A Complementary Global and Local Knowledge Network for Ultrasound\n  denoising with Fine-grained Refinement",
            "updated": "2023-10-05T09:12:34Z",
            "published": "2023-10-05T09:12:34Z",
            "summary": "Ultrasound imaging serves as an effective and non-invasive diagnostic tool\ncommonly employed in clinical examinations. However, the presence of speckle\nnoise in ultrasound images invariably degrades image quality, impeding the\nperformance of subsequent tasks, such as segmentation and classification.\nExisting methods for speckle noise reduction frequently induce excessive image\nsmoothing or fail to preserve detailed information adequately. In this paper,\nwe propose a complementary global and local knowledge network for ultrasound\ndenoising with fine-grained refinement. Initially, the proposed architecture\nemploys the L-CSwinTransformer as encoder to capture global information,\nincorporating CNN as decoder to fuse local features. We expand the resolution\nof the feature at different stages to extract more global information compared\nto the original CSwinTransformer. Subsequently, we integrate Fine-grained\nRefinement Block (FRB) within the skip-connection stage to further augment\nfeatures. We validate our model on two public datasets, HC18 and BUSI.\nExperimental results demonstrate that our model can achieve competitive\nperformance in both quantitative metrics and visual performance. Our code will\nbe available at https://github.com/AAlkaid/USDenoising.",
            "author": [
                "Zhenyu Bu",
                "Kai-Ni Wang",
                "Fuxing Zhao",
                "Shengxiao Li",
                "Guang-Quan Zhou"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03402v1",
                "http://arxiv.org/pdf/2310.03402v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03399v1",
            "title": "GRAPES: Learning to Sample Graphs for Scalable Graph Neural Networks",
            "updated": "2023-10-05T09:08:47Z",
            "published": "2023-10-05T09:08:47Z",
            "summary": "Graph neural networks (GNNs) learn the representation of nodes in a graph by\naggregating the neighborhood information in various ways. As these networks\ngrow in depth, their receptive field grows exponentially due to the increase in\nneighborhood sizes, resulting in high memory costs. Graph sampling solves\nmemory issues in GNNs by sampling a small ratio of the nodes in the graph. This\nway, GNNs can scale to much larger graphs. Most sampling methods focus on fixed\nsampling heuristics, which may not generalize to different structures or tasks.\nWe introduce GRAPES, an adaptive graph sampling method that learns to identify\nsets of influential nodes for training a GNN classifier. GRAPES uses a GFlowNet\nto learn node sampling probabilities given the classification objectives. We\nevaluate GRAPES across several small- and large-scale graph benchmarks and\ndemonstrate its effectiveness in accuracy and scalability. In contrast to\nexisting sampling methods, GRAPES maintains high accuracy even with small\nsample sizes and, therefore, can scale to very large graphs. Our code is\npublicly available at https://github.com/dfdazac/grapes.",
            "author": [
                "Taraneh Younesian",
                "Thiviyan Thanapalasingam",
                "Emile van Krieken",
                "Daniel Daza",
                "Peter Bloem"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03399v1",
                "http://arxiv.org/pdf/2310.03399v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03396v1",
            "title": "Learning to Simplify Spatial-Temporal Graphs in Gait Analysis",
            "updated": "2023-10-05T09:03:51Z",
            "published": "2023-10-05T09:03:51Z",
            "summary": "Gait analysis leverages unique walking patterns for person identification and\nassessment across multiple domains. Among the methods used for gait analysis,\nskeleton-based approaches have shown promise due to their robust and\ninterpretable features. However, these methods often rely on hand-crafted\nspatial-temporal graphs that are based on human anatomy disregarding the\nparticularities of the dataset and task. This paper proposes a novel method to\nsimplify the spatial-temporal graph representation for gait-based gender\nestimation, improving interpretability without losing performance. Our approach\nemploys two models, an upstream and a downstream model, that can adjust the\nadjacency matrix for each walking instance, thereby removing the fixed nature\nof the graph. By employing the Straight-Through Gumbel-Softmax trick, our model\nis trainable end-to-end. We demonstrate the effectiveness of our approach on\nthe CASIA-B dataset for gait-based gender estimation. The resulting graphs are\ninterpretable and differ qualitatively from fixed graphs used in existing\nmodels. Our research contributes to enhancing the explainability and\ntask-specific adaptability of gait recognition, promoting more efficient and\nreliable gait-based biometrics.",
            "author": [
                "Adrian Cosma",
                "Emilian Radoi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03396v1",
                "http://arxiv.org/pdf/2310.03396v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12838v1",
            "title": "Towards parallel intelligence: an interdisciplinary solution for complex\n  systems",
            "updated": "2023-10-05T08:51:59Z",
            "published": "2023-10-05T08:51:59Z",
            "summary": "The growing complexity of real-world systems necessitates interdisciplinary\nsolutions to confront myriad challenges in modeling, analysis, management, and\ncontrol. To meet these demands, the parallel systems method rooted in\nArtificial systems, Computational experiments, and Parallel execution (ACP)\napproach has been developed. The method cultivates a cycle, termed parallel\nintelligence, which iteratively creates data, acquires knowledge, and refines\nthe actual system. Over the past two decades, the parallel systems method has\ncontinuously woven advanced knowledge and technologies from various\ndisciplines, offering versatile interdisciplinary solutions for complex systems\nacross diverse fields. This review explores the origins and fundamental\nconcepts of the parallel systems method, showcasing its accomplishments as a\ndiverse array of parallel technologies and applications, while also\nprognosticating potential challenges. We posit that this method will\nconsiderably augment sustainable development while enhancing interdisciplinary\ncommunication and cooperation.",
            "author": [
                "Yong Zhao",
                "Zhengqiu Zhu",
                "Bin Chen",
                "Sihang Qiu",
                "Jincai Huang",
                "Xin Lu",
                "Weiyi Yang",
                "Chuan Ai",
                "Kuihua Huang",
                "Cheng He",
                "Yucheng Jin",
                "Zhong Liu",
                "Fei-Yue Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12838v1",
                "http://arxiv.org/pdf/2311.12838v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03387v1",
            "title": "Ideal structure of Nica-Toeplitz algebras",
            "updated": "2023-10-05T08:49:20Z",
            "published": "2023-10-05T08:49:20Z",
            "summary": "We study the gauge-invariant ideal structure of the Nica-Toeplitz algebra\n$\\mathcal{NT}(X)$ of a product system $(A, X)$ over $\\mathbb{N}^n$. We obtain a\nclear description of $X$-invariant ideals in $A$, that is, restrictions of\ngauge-invariant ideals in $\\mathcal{NT}(X)$ to $A$. The main result is a\nclassification of gauge-invariant ideals in $\\mathcal{NT}(X)$ for a proper\nproduct system in terms of families of ideals in $A$. We also apply our results\nto higher-rank graphs.",
            "author": [
                "Boris Bilich"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03387v1",
                "http://arxiv.org/pdf/2310.03387v1"
            ],
            "primary_category": "math.OA",
            "category": [
                "math.OA",
                "46L08, 46L55"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03376v1",
            "title": "Procedural Text Mining with Large Language Models",
            "updated": "2023-10-05T08:27:33Z",
            "published": "2023-10-05T08:27:33Z",
            "summary": "Recent advancements in the field of Natural Language Processing, particularly\nthe development of large-scale language models that are pretrained on vast\namounts of knowledge, are creating novel opportunities within the realm of\nKnowledge Engineering. In this paper, we investigate the usage of large\nlanguage models (LLMs) in both zero-shot and in-context learning settings to\ntackle the problem of extracting procedures from unstructured PDF text in an\nincremental question-answering fashion. In particular, we leverage the current\nstate-of-the-art GPT-4 (Generative Pre-trained Transformer 4) model,\naccompanied by two variations of in-context learning that involve an ontology\nwith definitions of procedures and steps and a limited number of samples of\nfew-shot learning. The findings highlight both the promise of this approach and\nthe value of the in-context learning customisations. These modifications have\nthe potential to significantly address the challenge of obtaining sufficient\ntraining data, a hurdle often encountered in deep learning-based Natural\nLanguage Processing techniques for procedure extraction.",
            "author": [
                "Anisa Rula",
                "Jennifer D'Souza"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03376v1",
                "http://arxiv.org/pdf/2310.03376v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.IT",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04461v1",
            "title": "AI-based automated active learning for discovery of hidden dynamic\n  processes: A use case in light microscopy",
            "updated": "2023-10-05T08:17:26Z",
            "published": "2023-10-05T08:17:26Z",
            "summary": "In the biomedical environment, experiments assessing dynamic processes are\nprimarily performed by a human acquisition supervisor. Contemporary\nimplementations of such experiments frequently aim to acquire a maximum number\nof relevant events from sometimes several hundred parallel, non-synchronous\nprocesses. Since in some high-throughput experiments, only one or a few\ninstances of a given process can be observed simultaneously, a strategy for\nplanning and executing an efficient acquisition paradigm is essential. To\naddress this problem, we present two new methods in this paper. The first\nmethod, Encoded Dynamic Process (EDP), is Artificial Intelligence (AI)-based\nand represents dynamic processes so as to allow prediction of pseudo-time\nvalues from single still images. Second, with Experiment Automation Pipeline\nfor Dynamic Processes (EAPDP), we present a Machine Learning Operations\n(MLOps)-based pipeline that uses the extracted knowledge from EDP to\nefficiently schedule acquisition in biomedical experiments for dynamic\nprocesses in practice. In a first experiment, we show that the pre-trained\nState-Of-The- Art (SOTA) object segmentation method Contour Proposal Networks\n(CPN) works reliably as a module of EAPDP to extract the relevant object for\nEDP from the acquired three-dimensional image stack.",
            "author": [
                "Nils Friederich",
                "Angelo Yamachui Sitcheu",
                "Oliver Neumann",
                "S\u00fcheyla Ero\u011flu-Kay\u0131k\u00e7\u0131",
                "Roshan Prizak",
                "Lennart Hilbert",
                "Ralf Mikut"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04461v1",
                "http://arxiv.org/pdf/2310.04461v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03363v1",
            "title": "Realistic Speech-to-Face Generation with Speech-Conditioned Latent\n  Diffusion Model with Face Prior",
            "updated": "2023-10-05T07:44:49Z",
            "published": "2023-10-05T07:44:49Z",
            "summary": "Speech-to-face generation is an intriguing area of research that focuses on\ngenerating realistic facial images based on a speaker's audio speech. However,\nstate-of-the-art methods employing GAN-based architectures lack stability and\ncannot generate realistic face images. To fill this gap, we propose a novel\nspeech-to-face generation framework, which leverages a Speech-Conditioned\nLatent Diffusion Model, called SCLDM. To the best of our knowledge, this is the\nfirst work to harness the exceptional modeling capabilities of diffusion models\nfor speech-to-face generation. Preserving the shared identity information\nbetween speech and face is crucial in generating realistic results. Therefore,\nwe employ contrastive pre-training for both the speech encoder and the face\nencoder. This pre-training strategy facilitates effective alignment between the\nattributes of speech, such as age and gender, and the corresponding facial\ncharacteristics in the face images. Furthermore, we tackle the challenge posed\nby excessive diversity in the synthesis process caused by the diffusion model.\nTo overcome this challenge, we introduce the concept of residuals by\nintegrating a statistical face prior to the diffusion process. This addition\nhelps to eliminate the shared component across the faces and enhances the\nsubtle variations captured by the speech condition. Extensive quantitative,\nqualitative, and user study experiments demonstrate that our method can produce\nmore realistic face images while preserving the identity of the speaker better\nthan state-of-the-art methods. Highlighting the notable enhancements, our\nmethod demonstrates significant gains in all metrics on the AVSpeech dataset\nand Voxceleb dataset, particularly noteworthy are the improvements of 32.17 and\n32.72 on the cosine distance metric for the two datasets, respectively.",
            "author": [
                "Jinting Wang",
                "Li Liu",
                "Jun Wang",
                "Hei Victor Cheng"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03363v1",
                "http://arxiv.org/pdf/2310.03363v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03356v1",
            "title": "Two product formulas for counting successive vertex orderings",
            "updated": "2023-10-05T07:23:59Z",
            "published": "2023-10-05T07:23:59Z",
            "summary": "A vertex ordering of a graph $G$ is a bijection\n$\\pi\\colon\\{1,\\dots,|V(G)|\\}\\to V(G)$. It is successive if the induced subgraph\n$G[v_{\\pi(1)},\\dots,v_{\\pi(k)}]$ is connected for each $k$. Lixing Fang, Hao\nHuang, J\\'anos Pach, G\\'abor Tardos, and Junchi Zuo [J. Comb. Theory A199\n(2023), 105776] gave formulas for counting the number of successive vertex\norderings for a class of graphs they called \"fully regular,\" and conjectured\nthat these formulas could be written as certain products involving differences\nor ratios of binomial coefficients in two cases: When the graph is the line\ngraph $L(K_n^{(3)})$ of the complete $3$-uniform hypergraph, or when it is the\nline graph $L(K_{m,n}^{(1,2)})$ of a complete \"bipartite\" $3$-uniform\nhypergraph. In this paper, we confirm both of these conjectures.",
            "author": [
                "Boon Suan Ho"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03356v1",
                "http://arxiv.org/pdf/2310.03356v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05A19 (Primary) 05C30 (Secondary)"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.12997v1",
            "title": "Parking Spot Classification based on surround view camera system",
            "updated": "2023-10-05T07:15:04Z",
            "published": "2023-10-05T07:15:04Z",
            "summary": "Surround-view fisheye cameras are commonly used for near-field sensing in\nautomated driving scenarios, including urban driving and auto valet parking.\nFour fisheye cameras, one on each side, are sufficient to cover 360{\\deg}\naround the vehicle capturing the entire near-field region. Based on surround\nview cameras, there has been much research on parking slot detection with main\nfocus on the occupancy status in recent years, but little work on whether the\nfree slot is compatible with the mission of the ego vehicle or not. For\ninstance, some spots are handicap or electric vehicles accessible only. In this\npaper, we tackle parking spot classification based on the surround view camera\nsystem. We adapt the object detection neural network YOLOv4 with a novel\npolygon bounding box model that is well-suited for various shaped parking\nspaces, such as slanted parking slots. To the best of our knowledge, we present\nthe first detailed study on parking spot detection and classification on\nfisheye cameras for auto valet parking scenarios. The results prove that our\nproposed classification approach is effective to distinguish between regular,\nelectric vehicle, and handicap parking spots.",
            "author": [
                "Andy Xiao",
                "Deep Doshi",
                "Lihao Wang",
                "Harsha Gorantla",
                "Thomas Heitzmann",
                "Peter Groth"
            ],
            "link": [
                "http://dx.doi.org/10.1117/12.2677526",
                "http://arxiv.org/abs/2310.12997v1",
                "http://arxiv.org/pdf/2310.12997v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03352v1",
            "title": "Tractable Bounding of Counterfactual Queries by Knowledge Compilation",
            "updated": "2023-10-05T07:10:40Z",
            "published": "2023-10-05T07:10:40Z",
            "summary": "We discuss the problem of bounding partially identifiable queries, such as\ncounterfactuals, in Pearlian structural causal models. A recently proposed\niterated EM scheme yields an inner approximation of those bounds by sampling\nthe initialisation parameters. Such a method requires multiple (Bayesian\nnetwork) queries over models sharing the same structural equations and\ntopology, but different exogenous probabilities. This setup makes a compilation\nof the underlying model to an arithmetic circuit advantageous, thus inducing a\nsizeable inferential speed-up. We show how a single symbolic knowledge\ncompilation allows us to obtain the circuit structure with symbolic parameters\nto be replaced by their actual values when computing the different queries. We\nalso discuss parallelisation techniques to further speed up the bound\ncomputation. Experiments against standard Bayesian network inference show clear\ncomputational advantages with up to an order of magnitude of speed-up.",
            "author": [
                "David Huber",
                "Yizuo Chen",
                "Alessandro Antonucci",
                "Adnan Darwiche",
                "Marco Zaffalon"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03352v1",
                "http://arxiv.org/pdf/2310.03352v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03347v1",
            "title": "Razumikhin-type ISS Lyapunov function and small gain theorem for\n  discrete time time-delay systems with application to a biased min-consensus\n  protocol",
            "updated": "2023-10-05T06:58:10Z",
            "published": "2023-10-05T06:58:10Z",
            "summary": "This paper considers small gain theorems for the global asymptotic and\nexponential input-to-state stability for discrete time time-delay systems using\nRazumikhin-type Lyapunov function. Among other things, unlike the existing\nliterature, it provides both necessary and sufficient conditions for\nexponential input-to-state stability in terms of the Razumikhin-type Lyapunov\nfunction and the small gain theorem. Previous necessary ad sufficient\nconditions were with the more computationally onerous, Krasovskii-type Lyapunov\nfunctions. The result finds application in the robust stability analysis of a\ngraph-based distributed algorithm, namely, the biased min-consensus protocol,\nwhich can be used to compute the length of the shortest path from each node to\nits nearest source in a graph. We consider the biased min-consensus protocol\nunder perturbations that are common in communication networks, including noise,\ndelay and asynchronous communication. By converting such a perturbed protocol\ninto a discrete time time-delay nonlinear system, we prove its exponential\ninput-to-state stability under perturbations using our Razumikhin-type\nLyapunov-based small gain theorem. Simulations are provided to verify the\ntheoretical results.",
            "author": [
                "Yuanqiu Mo",
                "Wenwu Yu",
                "Huazhou Hou",
                "Soura Dasgupta"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03347v1",
                "http://arxiv.org/pdf/2310.03347v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03774v1",
            "title": "Differential Game Strategies for Social Networks with Self-Interested\n  Individuals",
            "updated": "2023-10-05T05:59:02Z",
            "published": "2023-10-05T05:59:02Z",
            "summary": "A social network population engages in collective actions as a direct result\nof forming a particular opinion. The strategic interactions among the\nindividuals acting independently and selfishly naturally portray a\nnoncooperative game. Nash equilibrium allows for self-enforcing strategic\ninteractions between selfish and self-interested individuals. This paper\npresents a differential game approach to the opinion formation problem in\nsocial networks to investigate the evolution of opinions as a result of a Nash\nequilibrium. The opinion of each individual is described by a differential\nequation, which is the continuous-time Hegselmann-Krause model for opinion\ndynamics with a time delay in input. The objective of each individual is to\nseek optimal strategies for her own opinion evolution by minimizing an\nindividual cost function. Two differential game problems emerge, one for a\npopulation that is not stubborn and another for a population that is stubborn.\nThe open-loop Nash equilibrium actions and their associated opinion\ntrajectories are derived for both differential games using Pontryagin's\nprinciple. Additionally, the receding horizon control scheme is used to\npractice feedback strategies where the information flow is restricted by fixed\nand complete social graphs as well as the second neighborhood concept. The game\nstrategies were executed on the well-known Zachary's Karate Club social\nnetwork. The resulting opinion trajectories associated with the game strategies\nshowed consensus, polarization, and disagreement in final opinions.",
            "author": [
                "Hossein B. Jond"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03774v1",
                "http://arxiv.org/pdf/2310.03774v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03328v2",
            "title": "Reformulating Domain Adaptation of Large Language Models as\n  Adapt-Retrieve-Revise",
            "updated": "2023-10-12T22:14:52Z",
            "published": "2023-10-05T05:55:06Z",
            "summary": "While large language models (LLMs) like GPT-4 have recently demonstrated\nastonishing zero-shot capabilities in general domain tasks, they often generate\ncontent with hallucinations in specific domains such as Chinese law, hindering\ntheir application in these areas. This is typically due to the absence of\ntraining data that encompasses such a specific domain, preventing GPT-4 from\nacquiring in-domain knowledge. A pressing challenge is that it's not plausible\nto continue training LLMs of such scale on in-domain data.\n  This paper introduces a simple and effective domain adaptation framework for\nGPT-4 by reformulating generation as an \\textbf{adapt-retrieve-revise} process.\nThe initial step is to \\textbf{adapt} an affordable 7B LLM to the target domain\nby continuing learning on in-domain data. When solving a task, we leverage the\nadapted LLM to generate a draft answer given a task query. Then, the draft\nanswer will be used to \\textbf{retrieve} supporting evidence candidates from an\nexternal in-domain knowledge base. Finally, the draft answer and retrieved\nevidence are concatenated into a whole prompt to let GPT-4 assess the evidence\nand \\textbf{revise} the draft answer to generate the final answer.\n  Our proposal combines the advantages of the efficiency of adapting a smaller\n7B model with the evidence-assessing capability of GPT-4 and effectively\nprevents GPT-4 from generating hallucinatory content. In the zero-shot setting\nof four Chinese legal tasks, our method improves accuracy by 33.3\\% compared to\nthe direct generation by GPT-4. When compared to two stronger retrieval-based\nbaselines, our method outperforms them by 15.4\\% and 23.9\\%. Our code will be\nreleased",
            "author": [
                "Zhen wan",
                "Yating Zhang",
                "Yexiang Wang",
                "Fei Cheng",
                "Sadao Kurohashi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03328v2",
                "http://arxiv.org/pdf/2310.03328v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03320v3",
            "title": "BioBridge: Bridging Biomedical Foundation Models via Knowledge Graphs",
            "updated": "2023-10-30T20:43:20Z",
            "published": "2023-10-05T05:30:42Z",
            "summary": "Foundation models (FMs) are able to leverage large volumes of unlabeled data\nto demonstrate superior performance across a wide range of tasks. However, FMs\ndeveloped for biomedical domains have largely remained unimodal, i.e.,\nindependently trained and used for tasks on protein sequences alone, small\nmolecule structures alone, or clinical data alone. To overcome this limitation\nof biomedical FMs, we present BioBridge, a novel parameter-efficient learning\nframework, to bridge independently trained unimodal FMs to establish multimodal\nbehavior. BioBridge achieves it by utilizing Knowledge Graphs (KG) to learn\ntransformations between one unimodal FM and another without fine-tuning any\nunderlying unimodal FMs. Our empirical results demonstrate that BioBridge can\nbeat the best baseline KG embedding methods (on average by around 76.3%) in\ncross-modal retrieval tasks. We also identify BioBridge demonstrates\nout-of-domain generalization ability by extrapolating to unseen modalities or\nrelations. Additionally, we also show that BioBridge presents itself as a\ngeneral purpose retriever that can aid biomedical multimodal question answering\nas well as enhance the guided generation of novel drugs.",
            "author": [
                "Zifeng Wang",
                "Zichen Wang",
                "Balasubramaniam Srinivasan",
                "Vassilis N. Ioannidis",
                "Huzefa Rangwala",
                "Rishita Anubhai"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03320v3",
                "http://arxiv.org/pdf/2310.03320v3"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03312v1",
            "title": "Certifiably Robust Graph Contrastive Learning",
            "updated": "2023-10-05T05:00:11Z",
            "published": "2023-10-05T05:00:11Z",
            "summary": "Graph Contrastive Learning (GCL) has emerged as a popular unsupervised graph\nrepresentation learning method. However, it has been shown that GCL is\nvulnerable to adversarial attacks on both the graph structure and node\nattributes. Although empirical approaches have been proposed to enhance the\nrobustness of GCL, the certifiable robustness of GCL is still remain\nunexplored. In this paper, we develop the first certifiably robust framework in\nGCL. Specifically, we first propose a unified criteria to evaluate and certify\nthe robustness of GCL. We then introduce a novel technique, RES (Randomized\nEdgedrop Smoothing), to ensure certifiable robustness for any GCL model, and\nthis certified robustness can be provably preserved in downstream tasks.\nFurthermore, an effective training method is proposed for robust GCL. Extensive\nexperiments on real-world datasets demonstrate the effectiveness of our\nproposed method in providing effective certifiable robustness and enhancing the\nrobustness of any GCL model. The source code of RES is available at\nhttps://github.com/ventr1c/RES-GCL.",
            "author": [
                "Minhua Lin",
                "Teng Xiao",
                "Enyan Dai",
                "Xiang Zhang",
                "Suhang Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03312v1",
                "http://arxiv.org/pdf/2310.03312v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03311v1",
            "title": "Deep Variational Multivariate Information Bottleneck -- A Framework for\n  Variational Losses",
            "updated": "2023-10-05T04:59:58Z",
            "published": "2023-10-05T04:59:58Z",
            "summary": "Variational dimensionality reduction methods are known for their high\naccuracy, generative abilities, and robustness. These methods have many\ntheoretical justifications. Here we introduce a unifying principle rooted in\ninformation theory to rederive and generalize existing variational methods and\ndesign new ones. We base our framework on an interpretation of the multivariate\ninformation bottleneck, in which two Bayesian networks are traded off against\none another. We interpret the first network as an encoder graph, which\nspecifies what information to keep when compressing the data. We interpret the\nsecond network as a decoder graph, which specifies a generative model for the\ndata. Using this framework, we rederive existing dimensionality reduction\nmethods such as the deep variational information bottleneck (DVIB), beta\nvariational auto-encoders (beta-VAE), and deep variational canonical\ncorrelation analysis (DVCCA). The framework naturally introduces a trade-off\nparameter between compression and reconstruction in the DVCCA family of\nalgorithms, resulting in the new beta-DVCCA family. In addition, we derive a\nnew variational dimensionality reduction method, deep variational symmetric\ninformational bottleneck (DVSIB), which simultaneously compresses two variables\nto preserve information between their compressed representations. We implement\nall of these algorithms and evaluate their ability to produce shared low\ndimensional latent spaces on a modified noisy MNIST dataset. We show that\nalgorithms that are better matched to the structure of the data (beta-DVCCA and\nDVSIB) produce better latent spaces as measured by classification accuracy and\nthe dimensionality of the latent variables. We believe that this framework can\nbe used to unify other multi-view representation learning algorithms.\nAdditionally, it provides a straightforward framework for deriving\nproblem-specific loss functions.",
            "author": [
                "Eslam Abdelaleem",
                "Ilya Nemenman",
                "K. Michael Martini"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03311v1",
                "http://arxiv.org/pdf/2310.03311v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cond-mat.stat-mech",
                "cs.IT",
                "math.IT",
                "physics.data-an"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03303v1",
            "title": "A Two-stage Based Social Preference Recognition in Multi-Agent\n  Autonomous Driving System",
            "updated": "2023-10-05T04:10:56Z",
            "published": "2023-10-05T04:10:56Z",
            "summary": "Multi-Agent Reinforcement Learning (MARL) has become a promising solution for\nconstructing a multi-agent autonomous driving system (MADS) in complex and\ndense scenarios. But most methods consider agents acting selfishly, which leads\nto conflict behaviors. Some existing works incorporate the concept of social\nvalue orientation (SVO) to promote coordination, but they lack the knowledge of\nother agents' SVOs, resulting in conservative maneuvers. In this paper, we aim\nto tackle the mentioned problem by enabling the agents to understand other\nagents' SVOs. To accomplish this, we propose a two-stage system framework.\nFirstly, we train a policy by allowing the agents to share their ground truth\nSVOs to establish a coordinated traffic flow. Secondly, we develop a\nrecognition network that estimates agents' SVOs and integrates it with the\npolicy trained in the first stage. Experiments demonstrate that our developed\nmethod significantly improves the performance of the driving policy in MADS\ncompared to two state-of-the-art MARL algorithms.",
            "author": [
                "Jintao Xue",
                "Dongkun Zhang",
                "Rong Xiong",
                "Yue Wang",
                "Eryun Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03303v1",
                "http://arxiv.org/pdf/2310.03303v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03301v1",
            "title": "Learning Energy Decompositions for Partial Inference of GFlowNets",
            "updated": "2023-10-05T04:02:36Z",
            "published": "2023-10-05T04:02:36Z",
            "summary": "This paper studies generative flow networks (GFlowNets) to sample objects\nfrom the Boltzmann energy distribution via a sequence of actions. In\nparticular, we focus on improving GFlowNet with partial inference: training\nflow functions with the evaluation of the intermediate states or transitions.\nTo this end, the recently developed forward-looking GFlowNet reparameterizes\nthe flow functions based on evaluating the energy of intermediate states.\nHowever, such an evaluation of intermediate energies may (i) be too expensive\nor impossible to evaluate and (ii) even provide misleading training signals\nunder large energy fluctuations along the sequence of actions. To resolve this\nissue, we propose learning energy decompositions for GFlowNets (LED-GFN). Our\nmain idea is to (i) decompose the energy of an object into learnable potential\nfunctions defined on state transitions and (ii) reparameterize the flow\nfunctions using the potential functions. In particular, to produce informative\nlocal credits, we propose to regularize the potential to change smoothly over\nthe sequence of actions. It is also noteworthy that training GFlowNet with our\nlearned potential can preserve the optimal policy. We empirically verify the\nsuperiority of LED-GFN in five problems including the generation of\nunstructured and maximum independent sets, molecular graphs, and RNA sequences.",
            "author": [
                "Hyosoon Jang",
                "Minsu Kim",
                "Sungsoo Ahn"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03301v1",
                "http://arxiv.org/pdf/2310.03301v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03295v1",
            "title": "Can pre-trained models assist in dataset distillation?",
            "updated": "2023-10-05T03:51:21Z",
            "published": "2023-10-05T03:51:21Z",
            "summary": "Dataset Distillation (DD) is a prominent technique that encapsulates\nknowledge from a large-scale original dataset into a small synthetic dataset\nfor efficient training. Meanwhile, Pre-trained Models (PTMs) function as\nknowledge repositories, containing extensive information from the original\ndataset. This naturally raises a question: Can PTMs effectively transfer\nknowledge to synthetic datasets, guiding DD accurately? To this end, we conduct\npreliminary experiments, confirming the contribution of PTMs to DD. Afterwards,\nwe systematically study different options in PTMs, including initialization\nparameters, model architecture, training epoch and domain knowledge, revealing\nthat: 1) Increasing model diversity enhances the performance of synthetic\ndatasets; 2) Sub-optimal models can also assist in DD and outperform\nwell-trained ones in certain cases; 3) Domain-specific PTMs are not mandatory\nfor DD, but a reasonable domain match is crucial. Finally, by selecting optimal\noptions, we significantly improve the cross-architecture generalization over\nbaseline DD methods. We hope our work will facilitate researchers to develop\nbetter DD techniques. Our code is available at\nhttps://github.com/yaolu-zjut/DDInterpreter.",
            "author": [
                "Yao Lu",
                "Xuguang Chen",
                "Yuchen Zhang",
                "Jianyang Gu",
                "Tianle Zhang",
                "Yifan Zhang",
                "Xiaoniu Yang",
                "Qi Xuan",
                "Kai Wang",
                "Yang You"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03295v1",
                "http://arxiv.org/pdf/2310.03295v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03293v1",
            "title": "A New Dialogue Response Generation Agent for Large Language Models by\n  Asking Questions to Detect User's Intentions",
            "updated": "2023-10-05T03:45:54Z",
            "published": "2023-10-05T03:45:54Z",
            "summary": "Large Language Models (LLMs), such as ChatGPT, have recently been applied to\nvarious NLP tasks due to its open-domain generation capabilities. However,\nthere are two issues with applying LLMs to dialogue tasks. 1. During the\ndialogue process, users may have implicit intentions that might be overlooked\nby LLMs. Consequently, generated responses couldn't align with the user's\nintentions. 2. It is unlikely for LLMs to encompass all fields comprehensively.\nIn certain specific domains, their knowledge may be incomplete, and LLMs cannot\nupdate the latest knowledge in real-time. To tackle these issues, we propose a\nframework~\\emph{using LLM to \\textbf{E}nhance dialogue response generation by\nasking questions to \\textbf{D}etect user's \\textbf{I}mplicit\nin\\textbf{T}entions} (\\textbf{EDIT}). Firstly, EDIT generates open questions\nrelated to the dialogue context as the potential user's intention; Then, EDIT\nanswers those questions by interacting with LLMs and searching in\ndomain-specific knowledge bases respectively, and use LLMs to choose the proper\nanswers to questions as extra knowledge; Finally, EDIT enhances response\ngeneration by explicitly integrating those extra knowledge. Besides, previous\nquestion generation works only focus on asking questions with answers in\ncontext. In order to ask open questions, we construct a Context-Open-Question\n(COQ) dataset. On two task-oriented dialogue tasks (Wizard of Wikipedia and\nHoll-E), EDIT outperformed other LLMs.",
            "author": [
                "Siwei Wu",
                "Xiangqing Shen",
                "Rui Xia"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03293v1",
                "http://arxiv.org/pdf/2310.03293v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03287v1",
            "title": "PET-Enabled Dual-Energy CT Using a Kernel Method with Neural\n  Optimization Transfer",
            "updated": "2023-10-05T03:31:37Z",
            "published": "2023-10-05T03:31:37Z",
            "summary": "Integrated use of dual-energy computed tomography (DECT) with positron\nemission tomography (PET) has many potential clinical applications. However,\nthe integration would either require costly hardware upgrade or increase\nradiation dose on PET/CT scanners due to the need for an additional Xray CT\nscan. The recently proposed PET-enabled DECT method enables DECT imaging on\nPET/CT without requiring the second X-ray CT scan. It combines the\nalready-existing low-energy X-ray CT image with a 511 keV {\\gamma}-ray CT (gCT)\nimage reconstructed from time-of-flight PET emission data. A kernelized\nattenuation and activity (KAA) estimation method has been developed for\nreconstructing the gCT image from PET but the method has not fully exploited\nthe potential of image prior knowledge. In this work, we propose a neural KAA\nmethod by using neural network representation as a deep coefficient prior to\nimprove the existing KAA method. The resulting maximum-likelihood neural\nnetwork-based reconstruction problem can be efficiently solved by utilizing the\ntheory of optimization transfer. Each iteration of the algorithm consists of\nthree modular steps: PET activity image update, gCT image update, and\nneural-network learning in the image domain. This algorithm is guaranteed to\nmonotonically increase the data likelihood. The results from computer\nsimulation and real phantom data have demonstrated that the proposed neural KAA\nmethod can significantly improve gCT image quality and consequent\nmulti-material decomposition as compared to other methods.",
            "author": [
                "Siqi Li",
                "Yansong Zhu",
                "Benjamin A. Spencer",
                "Guobao Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03287v1",
                "http://arxiv.org/pdf/2310.03287v1"
            ],
            "primary_category": "physics.med-ph",
            "category": [
                "physics.med-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03285v1",
            "title": "Burning the Adversarial Bridges: Robust Windows Malware Detection\n  Against Binary-level Mutations",
            "updated": "2023-10-05T03:28:02Z",
            "published": "2023-10-05T03:28:02Z",
            "summary": "Toward robust malware detection, we explore the attack surface of existing\nmalware detection systems. We conduct root-cause analyses of the practical\nbinary-level black-box adversarial malware examples. Additionally, we uncover\nthe sensitivity of volatile features within the detection engines and exhibit\ntheir exploitability. Highlighting volatile information channels within the\nsoftware, we introduce three software pre-processing steps to eliminate the\nattack surface, namely, padding removal, software stripping, and inter-section\ninformation resetting. Further, to counter the emerging section injection\nattacks, we propose a graph-based section-dependent information extraction\nscheme for software representation. The proposed scheme leverages aggregated\ninformation within various sections in the software to enable robust malware\ndetection and mitigate adversarial settings. Our experimental results show that\ntraditional malware detection models are ineffective against adversarial\nthreats. However, the attack surface can be largely reduced by eliminating the\nvolatile information. Therefore, we propose simple-yet-effective methods to\nmitigate the impacts of binary manipulation attacks. Overall, our graph-based\nmalware detection scheme can accurately detect malware with an area under the\ncurve score of 88.32\\% and a score of 88.19% under a combination of binary\nmanipulation attacks, exhibiting the efficiency of our proposed scheme.",
            "author": [
                "Ahmed Abusnaina",
                "Yizhen Wang",
                "Sunpreet Arora",
                "Ke Wang",
                "Mihai Christodorescu",
                "David Mohaisen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03285v1",
                "http://arxiv.org/pdf/2310.03285v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03278v1",
            "title": "Mitigating Pilot Contamination and Enabling IoT Scalability in Massive\n  MIMO Systems",
            "updated": "2023-10-05T03:06:09Z",
            "published": "2023-10-05T03:06:09Z",
            "summary": "Massive MIMO is expected to play an important role in the development of 5G\nnetworks. This paper addresses the issue of pilot contamination and scalability\nin massive MIMO systems. The current practice of reusing orthogonal pilot\nsequences in adjacent cells leads to difficulty in differentiating incoming\ninter- and intra-cell pilot sequences. One possible solution is to increase the\nnumber of orthogonal pilot sequences, which results in dedicating more space of\ncoherence block to pilot transmission than data transmission. This, in turn,\nalso hinders the scalability of massive MIMO systems, particularly in\naccommodating a large number of IoT devices within a cell. To overcome these\nchallenges, this paper devises an innovative pilot allocation scheme based on\nthe data transfer patterns of IoT devices. The scheme assigns orthogonal pilot\nsequences to clusters of devices instead of individual devices, allowing\nmultiple devices to utilize the same pilot for periodically transmitting data.\nMoreover, we formulate the pilot assignment problem as a graph coloring problem\nand use the max k-cut graph partitioning approach to overcome the pilot\ncontamination in a multicell massive MIMO system. The proposed scheme\nsignificantly improves the spectral efficiency and enables the scalability of\nmassive MIMO systems; for instance, by using ten orthogonal pilot sequences, we\nare able to accommodate 200 devices with only a 12.5% omission rate.",
            "author": [
                "Muhammad Kamran Saeed",
                "Ahmed E. Kamal",
                "Ashfaq Khokhar"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03278v1",
                "http://arxiv.org/pdf/2310.03278v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "cs.LG",
                "cs.NI",
                "eess.SP",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03274v2",
            "title": "Fragment-based Pretraining and Finetuning on Molecular Graphs",
            "updated": "2023-10-28T03:22:06Z",
            "published": "2023-10-05T03:01:09Z",
            "summary": "Property prediction on molecular graphs is an important application of Graph\nNeural Networks. Recently, unlabeled molecular data has become abundant, which\nfacilitates the rapid development of self-supervised learning for GNNs in the\nchemical domain. In this work, we propose pretraining GNNs at the fragment\nlevel, a promising middle ground to overcome the limitations of node-level and\ngraph-level pretraining. Borrowing techniques from recent work on principal\nsubgraph mining, we obtain a compact vocabulary of prevalent fragments from a\nlarge pretraining dataset. From the extracted vocabulary, we introduce several\nfragment-based contrastive and predictive pretraining tasks. The contrastive\nlearning task jointly pretrains two different GNNs: one on molecular graphs and\nthe other on fragment graphs, which represents higher-order connectivity within\nmolecules. By enforcing consistency between the fragment embedding and the\naggregated embedding of the corresponding atoms from the molecular graphs, we\nensure that the embeddings capture structural information at multiple\nresolutions. The structural information of fragment graphs is further exploited\nto extract auxiliary labels for graph-level predictive pretraining. We employ\nboth the pretrained molecular-based and fragment-based GNNs for downstream\nprediction, thus utilizing the fragment information during finetuning. Our\ngraph fragment-based pretraining (GraphFP) advances the performances on 5 out\nof 8 common molecular benchmarks and improves the performances on long-range\nbiological benchmarks by at least 11.5%. Code is available at:\nhttps://github.com/lvkd84/GraphFP.",
            "author": [
                "Kha-Dinh Luong",
                "Ambuj Singh"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03274v2",
                "http://arxiv.org/pdf/2310.03274v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03272v1",
            "title": "Network Alignment with Transferable Graph Autoencoders",
            "updated": "2023-10-05T02:58:29Z",
            "published": "2023-10-05T02:58:29Z",
            "summary": "Network alignment is the task of establishing one-to-one correspondences\nbetween the nodes of different graphs and finds a plethora of applications in\nhigh-impact domains. However, this task is known to be NP-hard in its general\nform, and existing algorithms do not scale up as the size of the graphs\nincreases. To tackle both challenges we propose a novel generalized graph\nautoencoder architecture, designed to extract powerful and robust node\nembeddings, that are tailored to the alignment task. We prove that the\ngenerated embeddings are associated with the eigenvalues and eigenvectors of\nthe graphs and can achieve more accurate alignment compared to classical\nspectral methods. Our proposed framework also leverages transfer learning and\ndata augmentation to achieve efficient network alignment at a very large scale\nwithout retraining. Extensive experiments on both network and sub-network\nalignment with real-world graphs provide corroborating evidence supporting the\neffectiveness and scalability of the proposed approach.",
            "author": [
                "Jiashu He",
                "Charilaos I. Kanatsoulis",
                "Alejandro Ribeiro"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03272v1",
                "http://arxiv.org/pdf/2310.03272v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03269v1",
            "title": "InstructProtein: Aligning Human and Protein Language via Knowledge\n  Instruction",
            "updated": "2023-10-05T02:45:39Z",
            "published": "2023-10-05T02:45:39Z",
            "summary": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, but they fall short in comprehending biological sequences\nsuch as proteins. To address this challenge, we propose InstructProtein, an\ninnovative LLM that possesses bidirectional generation capabilities in both\nhuman and protein languages: (i) taking a protein sequence as input to predict\nits textual function description and (ii) using natural language to prompt\nprotein sequence generation. To achieve this, we first pre-train an LLM on both\nprotein and natural language corpora, enabling it to comprehend individual\nlanguages. Then supervised instruction tuning is employed to facilitate the\nalignment of these two distinct languages. Herein, we introduce a knowledge\ngraph-based instruction generation framework to construct a high-quality\ninstruction dataset, addressing annotation imbalance and instruction deficits\nin existing protein-text corpus. In particular, the instructions inherit the\nstructural relations between proteins and function annotations in knowledge\ngraphs, which empowers our model to engage in the causal modeling of protein\nfunctions, akin to the chain-of-thought processes in natural languages.\nExtensive experiments on bidirectional protein-text generation tasks show that\nInstructProtein outperforms state-of-the-art LLMs by large margins. Moreover,\nInstructProtein serves as a pioneering step towards text-based protein function\nprediction and sequence design, effectively bridging the gap between protein\nand human language understanding.",
            "author": [
                "Zeyuan Wang",
                "Qiang Zhang",
                "Keyan Ding",
                "Ming Qin",
                "Xiang Zhuang",
                "Xiaotong Li",
                "Huajun Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03269v1",
                "http://arxiv.org/pdf/2310.03269v1"
            ],
            "primary_category": "q-bio.BM",
            "category": [
                "q-bio.BM",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03257v2",
            "title": "On Matou\u0161ek-like Embedding Obstructions of Countably Branching\n  Graphs",
            "updated": "2023-10-08T16:41:15Z",
            "published": "2023-10-05T02:19:13Z",
            "summary": "In this paper we present new proofs of the non-embeddability of countably\nbranching trees into Banach spaces satisfying property $(\\beta_p)$ and of\ncountably branching diamonds into Banach spaces which are $p$-AMUC for $p > 1$.\nThese proofs are entirely metric in nature and are inspired by previous work of\nJi\\v{r}\\'i Matou\\v{s}ek. In addition, using this metric method, we succeed in\nextending these results to metric spaces satisfying certain curvature-like\ninequalities. Finally, we extend an embedding result of Tessera to give lower\nbounds on the compression for a class of Lipschitz embeddings of the countably\nbranching trees into Banach spaces containing $\\ell_p$-asymptotic models for $p\n\\geq 1$.",
            "author": [
                "Ryan Malthaner"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03257v2",
                "http://arxiv.org/pdf/2310.03257v2"
            ],
            "primary_category": "math.MG",
            "category": [
                "math.MG",
                "math.FA",
                "05C63, 46B06 (Primary), 46B20, 46B85, 51F30 (Secondary)"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03246v1",
            "title": "${\\tt MORALS}$: Analysis of High-Dimensional Robot Controllers via\n  Topological Tools in a Latent Space",
            "updated": "2023-10-05T01:31:45Z",
            "published": "2023-10-05T01:31:45Z",
            "summary": "Estimating the region of attraction (${\\tt RoA}$) for a robotic system's\ncontroller is essential for safe application and controller composition. Many\nexisting methods require access to a closed-form expression that limit\napplicability to data-driven controllers. Methods that operate only over\ntrajectory rollouts tend to be data-hungry. In prior work, we have demonstrated\nthat topological tools based on Morse Graphs offer data-efficient ${\\tt RoA}$\nestimation without needing an analytical model. They struggle, however, with\nhigh-dimensional systems as they operate over a discretization of the state\nspace. This paper presents ${\\it Mo}$rse Graph-aided discovery of ${\\it\nR}$egions of ${\\it A}$ttraction in a learned ${\\it L}$atent ${\\it S}$pace\n(${\\tt MORALS}$). The approach combines autoencoding neural networks with Morse\nGraphs. ${\\tt MORALS}$ shows promising predictive capabilities in estimating\nattractors and their ${\\tt RoA}$s for data-driven controllers operating over\nhigh-dimensional systems, including a 67-dim humanoid robot and a 96-dim\n3-fingered manipulator. It first projects the dynamics of the controlled system\ninto a learned latent space. Then, it constructs a reduced form of Morse Graphs\nrepresenting the bistability of the underlying dynamics, i.e., detecting when\nthe controller results in a desired versus an undesired behavior. The\nevaluation on high-dimensional robotic datasets indicates the data efficiency\nof the approach in ${\\tt RoA}$ estimation.",
            "author": [
                "Ewerton R. Vieira",
                "Aravind Sivaramakrishnan",
                "Sumanth Tangirala",
                "Edgar Granados",
                "Konstantin Mischaikow",
                "Kostas E. Bekris"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03246v1",
                "http://arxiv.org/pdf/2310.03246v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03230v1",
            "title": "The squish map and the $\\text{SL}_2$ double dimer model",
            "updated": "2023-10-05T00:54:34Z",
            "published": "2023-10-05T00:54:34Z",
            "summary": "A plane partition, whose 3D Young diagram is made of unit cubes, can be\napproximated by a ``coarser\" plane partition, made of cubes of side length 2.\nIndeed, there are two such approximations obtained by ``rounding up\" or\n``rounding down\" to the nearest cube. We relate this coarsening (or\ndownsampling) operation to the squish map introduced by the second author in\nearlier work. We exhibit a related measure-preserving map between the dimer\nmodel on the honeycomb graph, and the $\\text{SL}_2$ double dimer model on a\ncoarser honeycomb graph; we compute the most interesting special case of this\nmap, related to plane partition $q$-enumeration with 2-periodic weights. As an\napplication, we specialize the weights to be certain roots of unity, obtain\nnovel generating functions (some known, some new, and some conjectural) that\n$(-1)$-enumerate certain classes of pairs of plane partitions according to how\ntheir dimer configurations interact.",
            "author": [
                "Leigh Foster",
                "Benjamin Young"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03230v1",
                "http://arxiv.org/pdf/2310.03230v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03226v1",
            "title": "Biologically generated turbulent energy cascade in shear flow depends on\n  tensor geometry",
            "updated": "2023-10-05T00:47:15Z",
            "published": "2023-10-05T00:47:15Z",
            "summary": "It has been proposed that biologically generated turbulence plays an\nimportant role in material transport and ocean mixing. Both experimental and\nnumerical studies have reported evidence of the non-negligible mixing by\nmoderate Reynolds number swimmers in quiescent water, such as zooplankton,\nespecially at aggregation scales. However, the interaction between biologically\ngenerated agitation and the background flow as a key factor in biologically\ngenerated turbulence that could reshape our previous knowledge of biologically\ngenerated turbulence, has long been ignored. Here we show that the geometry\nbetween the biologically generated agitation and the background hydrodynamic\nshear can determine both the intensity and direction of biologically generated\nturbulent energy cascade. Measuring the migration of a centimeter-scale\nswimmer-as represented by the brine shrimp \\textit{Artemia salina}-in a shear\nflow and verifying through an analogue experiment with an artificial jet\nrevealed that different geometries between the biologically generated agitation\nand the background shear can result in spectral energy transferring toward\nlarger or smaller scales, which consequently intensifies or attenuates the\nlarge scale hydrodynamic shear. Our results suggest that the long ignored\ngeometry between the biologically generated agitation and the background flow\nfield is an important factor that should be taken into consideration in future\nstudies of biologically generated turbulence.",
            "author": [
                "Xinyu Si",
                "Lei Fang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03226v1",
                "http://arxiv.org/pdf/2310.03226v1"
            ],
            "primary_category": "physics.flu-dyn",
            "category": [
                "physics.flu-dyn"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03221v1",
            "title": "Know2BIO: A Comprehensive Dual-View Benchmark for Evolving Biomedical\n  Knowledge Graphs",
            "updated": "2023-10-05T00:34:56Z",
            "published": "2023-10-05T00:34:56Z",
            "summary": "Knowledge graphs (KGs) have emerged as a powerful framework for representing\nand integrating complex biomedical information. However, assembling KGs from\ndiverse sources remains a significant challenge in several aspects, including\nentity alignment, scalability, and the need for continuous updates to keep pace\nwith scientific advancements. Moreover, the representative power of KGs is\noften limited by the scarcity of multi-modal data integration. To overcome\nthese challenges, we propose Know2BIO, a general-purpose heterogeneous KG\nbenchmark for the biomedical domain. Know2BIO integrates data from 30 diverse\nsources, capturing intricate relationships across 11 biomedical categories. It\ncurrently consists of ~219,000 nodes and ~6,200,000 edges. Know2BIO is capable\nof user-directed automated updating to reflect the latest knowledge in\nbiomedical science. Furthermore, Know2BIO is accompanied by multi-modal data:\nnode features including text descriptions, protein and compound sequences and\nstructures, enabling the utilization of emerging natural language processing\nmethods and multi-modal data integration strategies. We evaluate KG\nrepresentation models on Know2BIO, demonstrating its effectiveness as a\nbenchmark for KG representation learning in the biomedical field. Data and\nsource code of Know2BIO are available at\nhttps://github.com/Yijia-Xiao/Know2BIO/.",
            "author": [
                "Yijia Xiao",
                "Dylan Steinecke",
                "Alexander Russell Pelletier",
                "Yushi Bai",
                "Peipei Ping",
                "Wei Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03221v1",
                "http://arxiv.org/pdf/2310.03221v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "q-bio.QM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03214v2",
            "title": "FreshLLMs: Refreshing Large Language Models with Search Engine\n  Augmentation",
            "updated": "2023-11-22T07:28:19Z",
            "published": "2023-10-05T00:04:12Z",
            "summary": "Most large language models (LLMs) are trained once and never updated; thus,\nthey lack the ability to dynamically adapt to our ever-changing world. In this\nwork, we perform a detailed study of the factuality of LLM-generated text in\nthe context of answering questions that test current world knowledge.\nSpecifically, we introduce FreshQA, a novel dynamic QA benchmark encompassing a\ndiverse range of question and answer types, including questions that require\nfast-changing world knowledge as well as questions with false premises that\nneed to be debunked. We benchmark a diverse array of both closed and\nopen-source LLMs under a two-mode evaluation procedure that allows us to\nmeasure both correctness and hallucination. Through human evaluations involving\nmore than 50K judgments, we shed light on limitations of these models and\ndemonstrate significant room for improvement: for instance, all models\n(regardless of model size) struggle on questions that involve fast-changing\nknowledge and false premises. Motivated by these results, we present\nFreshPrompt, a simple few-shot prompting method that substantially boosts the\nperformance of an LLM on FreshQA by incorporating relevant and up-to-date\ninformation retrieved from a search engine into the prompt. Our experiments\nshow that FreshPrompt outperforms both competing search engine-augmented\nprompting methods such as Self-Ask (Press et al., 2022) as well as commercial\nsystems such as Perplexity.AI. Further analysis of FreshPrompt reveals that\nboth the number of retrieved evidences and their order play a key role in\ninfluencing the correctness of LLM-generated answers. Additionally, instructing\nthe LLM to generate concise and direct answers helps reduce hallucination\ncompared to encouraging more verbose answers. To facilitate future work, we\nrelease FreshQA at github.com/freshllms/freshqa and commit to updating it at\nregular intervals.",
            "author": [
                "Tu Vu",
                "Mohit Iyyer",
                "Xuezhi Wang",
                "Noah Constant",
                "Jerry Wei",
                "Jason Wei",
                "Chris Tar",
                "Yun-Hsuan Sung",
                "Denny Zhou",
                "Quoc Le",
                "Thang Luong"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03214v2",
                "http://arxiv.org/pdf/2310.03214v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03770v1",
            "title": "Progressive reduced order modeling: empowering data-driven modeling with\n  selective knowledge transfer",
            "updated": "2023-10-04T23:50:14Z",
            "published": "2023-10-04T23:50:14Z",
            "summary": "Data-driven modeling can suffer from a constant demand for data, leading to\nreduced accuracy and impractical for engineering applications due to the high\ncost and scarcity of information. To address this challenge, we propose a\nprogressive reduced order modeling framework that minimizes data cravings and\nenhances data-driven modeling's practicality. Our approach selectively\ntransfers knowledge from previously trained models through gates, similar to\nhow humans selectively use valuable knowledge while ignoring unuseful\ninformation. By filtering relevant information from previous models, we can\ncreate a surrogate model with minimal turnaround time and a smaller training\nset that can still achieve high accuracy. We have tested our framework in\nseveral cases, including transport in porous media, gravity-driven flow, and\nfinite deformation in hyperelastic materials. Our results illustrate that\nretaining information from previous models and utilizing a valuable portion of\nthat knowledge can significantly improve the accuracy of the current model. We\nhave demonstrated the importance of progressive knowledge transfer and its\nimpact on model accuracy with reduced training samples. For instance, our\nframework with four parent models outperforms the no-parent counterpart trained\non data nine times larger. Our research unlocks data-driven modeling's\npotential for practical engineering applications by mitigating the data\nscarcity issue. Our proposed framework is a significant step toward more\nefficient and cost-effective data-driven modeling, fostering advancements\nacross various fields.",
            "author": [
                "Teeratorn Kadeethum",
                "Daniel O'Malley",
                "Youngsoo Choi",
                "Hari S. Viswanathan",
                "Hongkyu Yoon"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03770v1",
                "http://arxiv.org/pdf/2310.03770v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03207v1",
            "title": "Universal slices of the category of graphs",
            "updated": "2023-10-04T23:25:56Z",
            "published": "2023-10-04T23:25:56Z",
            "summary": "We characterise the slices of the category of graphs that are algebraically\nuniversal in terms of the structure of the slicing graph. In particular, we\nshow that algebraic universality is obtained if, and only if, the slicing graph\ncontains one of four fixed graphs as a subgraph.",
            "author": [
                "Ioannis Eleftheriadis"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03207v1",
                "http://arxiv.org/pdf/2310.03207v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "math.CT",
                "18B15, 08C05, 05C62"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03195v1",
            "title": "Deep reinforcement learning for machine scheduling: Methodology, the\n  state-of-the-art, and future directions",
            "updated": "2023-10-04T22:45:09Z",
            "published": "2023-10-04T22:45:09Z",
            "summary": "Machine scheduling aims to optimize job assignments to machines while\nadhering to manufacturing rules and job specifications. This optimization leads\nto reduced operational costs, improved customer demand fulfillment, and\nenhanced production efficiency. However, machine scheduling remains a\nchallenging combinatorial problem due to its NP-hard nature. Deep Reinforcement\nLearning (DRL), a key component of artificial general intelligence, has shown\npromise in various domains like gaming and robotics. Researchers have explored\napplying DRL to machine scheduling problems since 1995. This paper offers a\ncomprehensive review and comparison of DRL-based approaches, highlighting their\nmethodology, applications, advantages, and limitations. It categorizes these\napproaches based on computational components: conventional neural networks,\nencoder-decoder architectures, graph neural networks, and metaheuristic\nalgorithms. Our review concludes that DRL-based methods outperform exact\nsolvers, heuristics, and tabular reinforcement learning algorithms in terms of\ncomputation speed and generating near-global optimal solutions. These DRL-based\napproaches have been successfully applied to static and dynamic scheduling\nacross diverse machine environments and job characteristics. However, DRL-based\nschedulers face limitations in handling complex operational constraints,\nconfigurable multi-objective optimization, generalization, scalability,\ninterpretability, and robustness. Addressing these challenges will be a crucial\nfocus for future research in this field. This paper serves as a valuable\nresource for researchers to assess the current state of DRL-based machine\nscheduling and identify research gaps. It also aids experts and practitioners\nin selecting the appropriate DRL approach for production scheduling.",
            "author": [
                "Maziyar Khadivi",
                "Todd Charter",
                "Marjan Yaghoubi",
                "Masoud Jalayer",
                "Maryam Ahang",
                "Ardeshir Shojaeinasab",
                "Homayoun Najjaran"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03195v1",
                "http://arxiv.org/pdf/2310.03195v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.NE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03188v1",
            "title": "Talking Models: Distill Pre-trained Knowledge to Downstream Models via\n  Interactive Communication",
            "updated": "2023-10-04T22:22:21Z",
            "published": "2023-10-04T22:22:21Z",
            "summary": "Many recent breakthroughs in machine learning have been enabled by the\npre-trained foundation models. By scaling up model parameters, training data,\nand computation resources, foundation models have significantly advanced the\nstate-of-the-art in many applications. However, it is still an open question of\nhow to use these models to perform downstream tasks efficiently. Knowledge\ndistillation (KD) has been explored to tackle this challenge. KD transfers\nknowledge from a large teacher model to a smaller student model. While KD has\nbeen successful in improving student model performance, recent research has\ndiscovered that a powerful teacher does not necessarily lead to a powerful\nstudent, due to their huge capacity gap. In addition, the potential\ndistribution shifts between the pre-training data and downstream tasks can make\nknowledge transfer in KD sub-optimal for improving downstream task performance.\nIn this paper, we extend KD with an interactive communication process to help\nstudents of downstream tasks learn effectively from pre-trained foundation\nmodels. Our design is inspired by the way humans learn from teachers who can\nexplain knowledge in a way that meets the students' needs. Specifically, we let\neach model (i.e., student and teacher) train two components: (1) an encoder\nencoding the model's hidden states to a message and (2) a decoder decoding any\nmessages to its own hidden states. With encoder and decoder, not only can the\nteacher transfer rich information by encoding its hidden states, but also the\nstudent can send messages with information of downstream tasks to the teacher.\nTherefore, knowledge passing from teacher to student can be tailored to the\nstudent's capacity and downstream tasks' distributions. We conducted\nexperiments on benchmark datasets to show that our communication mechanism\noutperforms state-of-the-art distillation techniques.",
            "author": [
                "Zhe Zhao",
                "Qingyun Liu",
                "Huan Gui",
                "Bang An",
                "Lichan Hong",
                "Ed H. Chi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03188v1",
                "http://arxiv.org/pdf/2310.03188v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03186v3",
            "title": "Inferring Inference",
            "updated": "2023-10-13T22:04:12Z",
            "published": "2023-10-04T22:12:11Z",
            "summary": "Patterns of microcircuitry suggest that the brain has an array of repeated\ncanonical computational units. Yet neural representations are distributed, so\nthe relevant computations may only be related indirectly to single-neuron\ntransformations. It thus remains an open challenge how to define canonical\ndistributed computations. We integrate normative and algorithmic theories of\nneural computation into a mathematical framework for inferring canonical\ndistributed computations from large-scale neural activity patterns. At the\nnormative level, we hypothesize that the brain creates a structured internal\nmodel of its environment, positing latent causes that explain its sensory\ninputs, and uses those sensory inputs to infer the latent causes. At the\nalgorithmic level, we propose that this inference process is a nonlinear\nmessage-passing algorithm on a graph-structured model of the world. Given a\ntime series of neural activity during a perceptual inference task, our\nframework finds (i) the neural representation of relevant latent variables,\n(ii) interactions between these variables that define the brain's internal\nmodel of the world, and (iii) message-functions specifying the inference\nalgorithm. These targeted computational properties are then statistically\ndistinguishable due to the symmetries inherent in any canonical computation, up\nto a global transformation. As a demonstration, we simulate recordings for a\nmodel brain that implicitly implements an approximate inference algorithm on a\nprobabilistic graphical model. Given its external inputs and noisy neural\nactivity, we recover the latent variables, their neural representation and\ndynamics, and canonical message-functions. We highlight features of\nexperimental design needed to successfully extract canonical computations from\nneural data. Overall, this framework provides a new tool for discovering\ninterpretable structure in neural recordings.",
            "author": [
                "Rajkumar Vasudeva Raju",
                "Zhe Li",
                "Scott Linderman",
                "Xaq Pitkow"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03186v3",
                "http://arxiv.org/pdf/2310.03186v3"
            ],
            "primary_category": "q-bio.NC",
            "category": [
                "q-bio.NC",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09231v1",
            "title": "Key Factors Affecting European Reactions to AI in European Full and\n  Flawed Democracies",
            "updated": "2023-10-04T22:11:28Z",
            "published": "2023-10-04T22:11:28Z",
            "summary": "This study examines the key factors that affect European reactions to\nartificial intelligence (AI) in the context of both full and flawed democracies\nin Europe. Analysing a dataset of 4,006 respondents, categorised into full\ndemocracies and flawed democracies based on the Democracy Index developed by\nthe Economist Intelligence Unit (EIU), this research identifies crucial factors\nthat shape European attitudes toward AI in these two types of democracies. The\nanalysis reveals noteworthy findings. Firstly, it is observed that flawed\ndemocracies tend to exhibit higher levels of trust in government entities\ncompared to their counterparts in full democracies. Additionally, individuals\nresiding in flawed democracies demonstrate a more positive attitude toward AI\nwhen compared to respondents from full democracies. However, the study finds no\nsignificant difference in AI awareness between the two types of democracies,\nindicating a similar level of general knowledge about AI technologies among\nEuropean citizens. Moreover, the study reveals that trust in AI measures,\nspecifically \"Trust AI Solution\", does not significantly vary between full and\nflawed democracies. This suggests that despite the differences in democratic\nquality, both types of democracies have similar levels of confidence in AI\nsolutions.",
            "author": [
                "Long Pham",
                "Barry O'Sullivan",
                "Tai Tan Mai"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09231v1",
                "http://arxiv.org/pdf/2311.09231v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03184v2",
            "title": "Retrieval-augmented Generation to Improve Math Question-Answering:\n  Trade-offs Between Groundedness and Human Preference",
            "updated": "2023-11-11T01:26:22Z",
            "published": "2023-10-04T22:09:28Z",
            "summary": "For middle-school math students, interactive question-answering (QA) with\ntutors is an effective way to learn. The flexibility and emergent capabilities\nof generative large language models (LLMs) has led to a surge of interest in\nautomating portions of the tutoring process - including interactive QA to\nsupport conceptual discussion of mathematical concepts. However, LLM responses\nto math questions can be incorrect or mismatched to the educational context -\nsuch as being misaligned with a school's curriculum. One potential solution is\nretrieval-augmented generation (RAG), which involves incorporating a vetted\nexternal knowledge source in the LLM prompt to increase response quality. In\nthis paper, we designed prompts that retrieve and use content from a\nhigh-quality open-source math textbook to generate responses to real student\nquestions. We evaluate the efficacy of this RAG system for middle-school\nalgebra and geometry QA by administering a multi-condition survey, finding that\nhumans prefer responses generated using RAG, but not when responses are too\ngrounded in the textbook content. We argue that while RAG is able to improve\nresponse quality, designers of math QA systems must consider trade-offs between\ngenerating responses preferred by students and responses closely matched to\nspecific educational resources.",
            "author": [
                "Zachary Levonian",
                "Chenglu Li",
                "Wangda Zhu",
                "Anoushka Gade",
                "Owen Henkel",
                "Millie-Ellen Postle",
                "Wanli Xing"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03184v2",
                "http://arxiv.org/pdf/2310.03184v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03171v1",
            "title": "Online Knowledge Production in Polarized Political Memes: The Case of\n  Critical Race Theory",
            "updated": "2023-10-04T21:33:56Z",
            "published": "2023-10-04T21:33:56Z",
            "summary": "Visual culture has long been deployed by actors across the political spectrum\nas tools of political mobilization, and have recently incorporated new\ncommunication tools, such as memes, GIFs, and emojis. In this study, we analyze\nthe top-circulated Facebook memes relating to critical race theory (CRT) from\nMay 2021 - May 2022 to investigate their visual and textual appeals. Using\nimage clustering techniques and critical discourse analysis, we find that both\npro- and anti-CRT memes deploy similar rhetorical tactics to make bifurcating\narguments, most of which do not pertain to the academic formulations of CRT.\nInstead, these memes manipulate definitions of racism and antiracism to appeal\nto their respective audiences. We argue that labeling such discursive practices\nas simply a symptom of \"post-truth\" politics is a potentially unproductive\nstance. Instead, theorizing the knowledge-building practices of these memes\nthrough a lens of political epistemology allows us to understand how they\nproduce meaning.",
            "author": [
                "Alyvia Walters",
                "Tawfiq Ammari",
                "Kiran Garimella",
                "Shagun Jhaver"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03171v1",
                "http://arxiv.org/pdf/2310.03171v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03152v2",
            "title": "Towards out-of-distribution generalizable predictions of chemical\n  kinetics properties",
            "updated": "2023-12-04T20:12:42Z",
            "published": "2023-10-04T20:36:41Z",
            "summary": "Machine Learning (ML) techniques have found applications in estimating\nchemical kinetic properties. With the accumulated drug molecules identified\nthrough \"AI4drug discovery\", the next imperative lies in AI-driven design for\nhigh-throughput chemical synthesis processes, with the estimation of properties\nof unseen reactions with unexplored molecules. To this end, the existing ML\napproaches for kinetics property prediction are required to be\nOut-Of-Distribution (OOD) generalizable. In this paper, we categorize the OOD\nkinetic property prediction into three levels (structure, condition, and\nmechanism), revealing unique aspects of such problems. Under this framework, we\ncreate comprehensive datasets to benchmark (1) the state-of-the-art ML\napproaches for reaction prediction in the OOD setting and (2) the\nstate-of-the-art graph OOD methods in kinetics property prediction problems.\nOur results demonstrated the challenges and opportunities in OOD kinetics\nproperty prediction. Our datasets and benchmarks can further support research\nin this direction.",
            "author": [
                "Zihao Wang",
                "Yongqiang Chen",
                "Yang Duan",
                "Weijiang Li",
                "Bo Han",
                "James Cheng",
                "Hanghang Tong"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03152v2",
                "http://arxiv.org/pdf/2310.03152v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03151v1",
            "title": "Dynamic Changes of Brain Network during Epileptic Seizure",
            "updated": "2023-10-04T20:29:30Z",
            "published": "2023-10-04T20:29:30Z",
            "summary": "Epilepsy is a neurological disorder identified by sudden and recurrent\nseizures, which are believed to be accompanied by distinct changes in brain\ndynamics. Exploring the dynamic changes of brain network states during seizures\ncan pave the way for improving the diagnosis and treatment of patients with\nepilepsy. In this paper, the connectivity brain network is constructed using\nthe phase lag index (PLI) measurement within five frequency bands, and\ngraph-theoretic techniques are employed to extract topological features from\nthe brain network. Subsequently, an unsupervised clustering approach is used to\nexamine the state transitions of the brain network during seizures. Our\nfindings demonstrate that the level of brain synchrony during the seizure\nperiod is higher than the pre-seizure and post-seizure periods in the theta,\nalpha, and beta bands, while it decreases in the gamma bands. These changes in\nsynchronization also lead to alterations in the topological features of\nfunctional brain networks during seizures. Additionally, our results suggest\nthat the dynamics of the brain during seizures are more complex than the\ntraditional three-state model (pre-seizure, seizure, and post-seizure) and the\nbrain network state exhibits a slower rate of change during the seizure period\ncompared to the pre-seizure and post-seizure periods.",
            "author": [
                "Atefeh Khoshkhahtinat",
                "Hoda Mohammadzade"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03151v1",
                "http://arxiv.org/pdf/2310.03151v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP",
                "q-bio.NC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03115v1",
            "title": "The Necker cube surface",
            "updated": "2023-10-04T19:08:27Z",
            "published": "2023-10-04T19:08:27Z",
            "summary": "We study geodesics on the Necker cube surface, $\\mathbf N$, an infinite\nperiodic Euclidean cone surface that is homeomorphic to the plane and is tiled\nby squares meeting three or six to a vertex. We ask: When does a geodesic on\nthe surface close? When does a geodesic drift away periodically? We show that\nboth questions can be answered only using knowledge about the initial direction\nof a geodesic. Further, there is a natural projection from $\\mathbf N$ to the\nplane, and we show that regions related to simple closed geodesics tile the\nplane periodically. We also describe the full affine symmetry group of the\nhalf-translation cover and use this to study dynamical properties of the\ngeodesic flow on $\\mathbf N$. We prove results related to recurrence,\nergodicity, and divergence rates.",
            "author": [
                "W. Patrick Hooper",
                "Pavel Javornik"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03115v1",
                "http://arxiv.org/pdf/2310.03115v1"
            ],
            "primary_category": "math.DS",
            "category": [
                "math.DS",
                "37E35 (Primary) 37E15, 52B70, 52C20 (Secondary)"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03098v1",
            "title": "NOCAP: Near-Optimal Correlation-Aware Partitioning Joins",
            "updated": "2023-10-04T18:28:10Z",
            "published": "2023-10-04T18:28:10Z",
            "summary": "Storage-based joins are still commonly used today because the memory budget\ndoes not always scale with the data size. One of the many join algorithms\ndeveloped that has been widely deployed and proven to be efficient is the\nHybrid Hash Join (HHJ), which is designed to exploit any available memory to\nmaximize the data that is joined directly in memory. However, HHJ cannot fully\nexploit detailed knowledge of the join attribute correlation distribution.\n  In this paper, we show that given a correlation skew in the join attributes,\nHHJ partitions data in a suboptimal way. To do that, we derive the optimal\npartitioning using a new cost-based analysis of partitioning-based joins that\nis tailored for primary key - foreign key (PK-FK) joins, one of the most common\njoin types. This optimal partitioning strategy has a high memory cost, thus, we\nfurther derive an approximate algorithm that has tunable memory cost and leads\nto near-optimal results. Our algorithm, termed NOCAP (Near-Optimal\nCorrelation-Aware Partitioning) join, outperforms the state-of-the-art for\nskewed correlations by up to $30\\%$, and the textbook Grace Hash Join by up to\n$4\\times$. Further, for a limited memory budget, NOCAP outperforms HHJ by up to\n$10\\%$, even for uniform correlation. Overall, NOCAP dominates state-of-the-art\nalgorithms and mimics the best algorithm for a memory budget varying from below\n$\\sqrt{\\|\\text{relation}\\|}$ to more than $\\|\\text{relation}\\|$.",
            "author": [
                "Zichen Zhu",
                "Xiao Hu",
                "Manos Athanassoulis"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03098v1",
                "http://arxiv.org/pdf/2310.03098v1"
            ],
            "primary_category": "cs.DB",
            "category": [
                "cs.DB"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03095v1",
            "title": "Opinion Dynamics Optimization Through Noncooperative Differential Games",
            "updated": "2023-10-04T18:25:47Z",
            "published": "2023-10-04T18:25:47Z",
            "summary": "In this paper, I study optimizing the opinion formation of a social network\nof a population of individuals on a graph whose opinion evolves according to\nthe Hegselmann-Krause model for opinion dynamics. I propose an optimization\nproblem based on a differential game for a population of individuals who are\nnot stubborn. The objective of each individual is to seek an optimal control\npolicy for her own opinion evolution by optimizing a personal performance\nindex. The Nash equilibrium actions and the associated opinion trajectory with\nthe equilibrium actions are derived for the opinion optimization model using\nPontryagin's principle. The game strategies were executed on the well-known\nZachary's Karate Club social network. The resulting opinion trajectories\nassociated with the game strategies showed that in non-stubborn Zachary's\nnetwork, the opinions moved toward the average opinion of the network, but a\nconsensus of final opinions did not necessarily emerge.",
            "author": [
                "Hossein B. Jond"
            ],
            "link": [
                "http://dx.doi.org/10.1109/CoDIT58514.2023.10284216",
                "http://arxiv.org/abs/2310.03095v1",
                "http://arxiv.org/pdf/2310.03095v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03088v1",
            "title": "Physics-Informed Neural Networks for Accelerating Power System State\n  Estimation",
            "updated": "2023-10-04T18:14:48Z",
            "published": "2023-10-04T18:14:48Z",
            "summary": "State estimation is the cornerstone of the power system control center since\nit provides the operating condition of the system in consecutive time\nintervals. This work investigates the application of physics-informed neural\nnetworks (PINNs) for accelerating power systems state estimation in monitoring\nthe operation of power systems. Traditional state estimation techniques often\nrely on iterative algorithms that can be computationally intensive,\nparticularly for large-scale power systems. In this paper, a novel approach\nthat leverages the inherent physical knowledge of power systems through the\nintegration of PINNs is proposed. By incorporating physical laws as prior\nknowledge, the proposed method significantly reduces the computational\ncomplexity associated with state estimation while maintaining high accuracy.\nThe proposed method achieves up to 11% increase in accuracy, 75% reduction in\nstandard deviation of results, and 30% faster convergence, as demonstrated by\ncomprehensive experiments on the IEEE 14-bus system.",
            "author": [
                "Solon Falas",
                "Markos Asprou",
                "Charalambos Konstantinou",
                "Maria K. Michael"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03088v1",
                "http://arxiv.org/pdf/2310.03088v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03087v1",
            "title": "Hypertriton lifetime",
            "updated": "2023-10-04T18:11:33Z",
            "published": "2023-10-04T18:11:33Z",
            "summary": "Over the last decade, conflicting values of the hypertriton\n${}_{\\Lambda}^3\\mathrm{H}$ lifetime $\\tau({}_{\\Lambda}^3\\mathrm{H})$ were\nextracted from relativistic heavy-ion (RHI) collision experiments, ranging from\nvalues compatible with the free-$\\Lambda$ lifetime $\\tau_\\Lambda$-as expected\nnaively for a very weakly bound $\\Lambda$ in ${}_{\\Lambda}^3\\mathrm{H}$-to\nlifetimes as short as $\\tau({}_{\\Lambda}^3\\mathrm{H}) \\approx\n(0.4-0.7)\\,\\tau_\\Lambda$. In a recent work [1] we studied this\n${}_{\\Lambda}^3\\mathrm{H}$ lifetime puzzle theoretically using realistic\nthree-body ${}_{\\Lambda}^3\\mathrm{H}$ and ${}^3\\mathrm{He}$ wave functions\ncomputed within the ab initio no-core shell model approach with interactions\nderived from chiral effective field theory. In particular,\n$\\tau({}_{\\Lambda}^3\\mathrm{H})$ was found to be strongly correlated with the\n$\\Lambda$ separation energy $B_\\Lambda$ in ${}_{\\Lambda}^3\\mathrm{H}$, the\nvalue of which is rather poorly known experimentally and, in addition, is known\nto suffer from sizable theoretical uncertainties inherent in the employed\nnuclear and hypernuclear interaction models. In the present work we find that\nthese uncertainties propagate into $\\tau({}_{\\Lambda}^3\\mathrm{H})$, and thus\nlimit considerably the theoretical precision of its computed value. Although\nnone of the conflicting RHI measured $\\tau({}_{\\Lambda}^3\\mathrm{H})$ values\ncan be excluded, but rather can be attributed to a poor knowledge of\n$B_\\Lambda$, we note the good agreement between the lifetime value\n$\\tau({}_{\\Lambda}^3\\mathrm{H})=238(27)$ ps computed at the lowest value\n$B_\\Lambda=66$ keV reached by us and the very recent ALICE measured lifetime\nvalue $\\tau^{\\mathrm{ALICE}}({}_{\\Lambda}^3\\mathrm{H})=253(11)(6)$ ps\nassociated with the ALICE measured $B_\\Lambda$ value\n$B^{\\mathrm{ALICE}}_\\Lambda=102(63)(67)$ keV [2].",
            "author": [
                "D. Gazda",
                "A. P\u00e9rez-Obiol",
                "A. Gal",
                "E. Friedman"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03087v1",
                "http://arxiv.org/pdf/2310.03087v1"
            ],
            "primary_category": "nucl-th",
            "category": [
                "nucl-th",
                "hep-ph",
                "nucl-ex"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03769v1",
            "title": "What to expect from scalar-tensor space geodesy",
            "updated": "2023-10-04T18:03:09Z",
            "published": "2023-10-04T18:03:09Z",
            "summary": "Scalar-tensor theories with screening mechanisms come with non-linearities\nthat make it difficult to study setups of complex geometry without resorting to\nnumerical simulations. In this article, we use the $\\textit{femtoscope}$ code\nthat we introduced in a previous work in order to compute the fifth force\narising in the chameleon model in the Earth orbit. We go beyond published works\nby introducing a departure from spherical symmetry $\\unicode{x2014}$ embodied\nby a mountain on an otherwise spherical Earth $\\unicode{x2014}$ as well as by\nimplementing several atmospheric models, and quantify their combined effect on\nthe chameleon field. Building on the numerical results thus obtained, we\naddress the question of the detectability of a putative chameleon fifth force\nby means of space geodesy techniques and, for the first time, quantitatively\nassess the back-reaction created by the screening of a satellite itself. We\nfind that although the fifth force has a supposedly measurable effect on the\ndynamics of an orbiting spacecraft, the imprecise knowledge of the mass\ndistribution inside the Earth greatly curtails the constraining power of such\nspace missions. Finally, we show how this degeneracy can be lifted when several\nmeasurements are performed at different altitudes.",
            "author": [
                "Hugo L\u00e9vy",
                "Jo\u00ebl Berg\u00e9",
                "Jean-Philippe Uzan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03769v1",
                "http://arxiv.org/pdf/2310.03769v1"
            ],
            "primary_category": "gr-qc",
            "category": [
                "gr-qc"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03084v1",
            "title": "Discovering Knowledge-Critical Subnetworks in Pretrained Language Models",
            "updated": "2023-10-04T18:02:01Z",
            "published": "2023-10-04T18:02:01Z",
            "summary": "Pretrained language models (LMs) encode implicit representations of knowledge\nin their parameters. However, localizing these representations and\ndisentangling them from each other remains an open problem. In this work, we\ninvestigate whether pretrained language models contain various\nknowledge-critical subnetworks: particular sparse computational subgraphs\nresponsible for encoding specific knowledge the model has memorized. We propose\na multi-objective differentiable weight masking scheme to discover these\nsubnetworks and show that we can use them to precisely remove specific\nknowledge from models while minimizing adverse effects on the behavior of the\noriginal language model. We demonstrate our method on multiple GPT2 variants,\nuncovering highly sparse subnetworks (98%+) that are solely responsible for\nspecific collections of relational knowledge. When these subnetworks are\nremoved, the remaining network maintains most of its initial capacity (modeling\nlanguage and other memorized relational knowledge) but struggles to express the\nremoved knowledge, and suffers performance drops on examples needing this\nremoved knowledge on downstream tasks after finetuning.",
            "author": [
                "Deniz Bayazit",
                "Negar Foroutan",
                "Zeming Chen",
                "Gail Weiss",
                "Antoine Bosselut"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03084v1",
                "http://arxiv.org/pdf/2310.03084v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03006v2",
            "title": "COOLer: Class-Incremental Learning for Appearance-Based Multiple Object\n  Tracking",
            "updated": "2023-10-05T05:54:34Z",
            "published": "2023-10-04T17:49:48Z",
            "summary": "Continual learning allows a model to learn multiple tasks sequentially while\nretaining the old knowledge without the training data of the preceding tasks.\nThis paper extends the scope of continual learning research to\nclass-incremental learning for multiple object tracking (MOT), which is\ndesirable to accommodate the continuously evolving needs of autonomous systems.\nPrevious solutions for continual learning of object detectors do not address\nthe data association stage of appearance-based trackers, leading to\ncatastrophic forgetting of previous classes' re-identification features. We\nintroduce COOLer, a COntrastive- and cOntinual-Learning-based tracker, which\nincrementally learns to track new categories while preserving past knowledge by\ntraining on a combination of currently available ground truth labels and\npseudo-labels generated by the past tracker. To further exacerbate the\ndisentanglement of instance representations, we introduce a novel contrastive\nclass-incremental instance representation learning technique. Finally, we\npropose a practical evaluation protocol for continual learning for MOT and\nconduct experiments on the BDD100K and SHIFT datasets. Experimental results\ndemonstrate that COOLer continually learns while effectively addressing\ncatastrophic forgetting of both tracking and detection. The code is available\nat https://github.com/BoSmallEar/COOLer.",
            "author": [
                "Zhizheng Liu",
                "Mattia Segu",
                "Fisher Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03006v2",
                "http://arxiv.org/pdf/2310.03006v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03003v1",
            "title": "From Words to Watts: Benchmarking the Energy Costs of Large Language\n  Model Inference",
            "updated": "2023-10-04T17:41:59Z",
            "published": "2023-10-04T17:41:59Z",
            "summary": "Large language models (LLMs) have exploded in popularity due to their new\ngenerative capabilities that go far beyond prior state-of-the-art. These\ntechnologies are increasingly being leveraged in various domains such as law,\nfinance, and medicine. However, these models carry significant computational\nchallenges, especially the compute and energy costs required for inference.\nInference energy costs already receive less attention than the energy costs of\ntraining LLMs -- despite how often these large models are called on to conduct\ninference in reality (e.g., ChatGPT). As these state-of-the-art LLMs see\nincreasing usage and deployment in various domains, a better understanding of\ntheir resource utilization is crucial for cost-savings, scaling performance,\nefficient hardware usage, and optimal inference strategies.\n  In this paper, we describe experiments conducted to study the computational\nand energy utilization of inference with LLMs. We benchmark and conduct a\npreliminary analysis of the inference performance and inference energy costs of\ndifferent sizes of LLaMA -- a recent state-of-the-art LLM -- developed by Meta\nAI on two generations of popular GPUs (NVIDIA V100 \\& A100) and two datasets\n(Alpaca and GSM8K) to reflect the diverse set of tasks/benchmarks for LLMs in\nresearch and practice. We present the results of multi-node, multi-GPU\ninference using model sharding across up to 32 GPUs. To our knowledge, our work\nis the one of the first to study LLM inference performance from the perspective\nof computational and energy resources at this scale.",
            "author": [
                "Siddharth Samsi",
                "Dan Zhao",
                "Joseph McDonald",
                "Baolin Li",
                "Adam Michaleas",
                "Michael Jones",
                "William Bergeron",
                "Jeremy Kepner",
                "Devesh Tiwari",
                "Vijay Gadepally"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03003v1",
                "http://arxiv.org/pdf/2310.03003v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02995v3",
            "title": "IBCL: Zero-shot Model Generation for Task Trade-offs in Continual\n  Learning",
            "updated": "2023-10-09T18:40:58Z",
            "published": "2023-10-04T17:30:50Z",
            "summary": "Like generic multi-task learning, continual learning has the nature of\nmulti-objective optimization, and therefore faces a trade-off between the\nperformance of different tasks. That is, to optimize for the current task\ndistribution, it may need to compromise performance on some previous tasks.\nThis means that there exist multiple models that are Pareto-optimal at\ndifferent times, each addressing a distinct task performance trade-off.\nResearchers have discussed how to train particular models to address specific\ntrade-off preferences. However, existing algorithms require training overheads\nproportional to the number of preferences -- a large burden when there are\nmultiple, possibly infinitely many, preferences. As a response, we propose\nImprecise Bayesian Continual Learning (IBCL). Upon a new task, IBCL (1) updates\na knowledge base in the form of a convex hull of model parameter distributions\nand (2) obtains particular models to address task trade-off preferences with\nzero-shot. That is, IBCL does not require any additional training overhead to\ngenerate preference-addressing models from its knowledge base. We show that\nmodels obtained by IBCL have guarantees in identifying the Pareto optimal\nparameters. Moreover, experiments on standard image classification and NLP\ntasks support this guarantee. Statistically, IBCL improves average per-task\naccuracy by at most 23\\% and peak per-task accuracy by at most 15\\% with\nrespect to the baseline methods, with steadily near-zero or positive backward\ntransfer. Most importantly, IBCL significantly reduces the training overhead\nfrom training 1 model per preference to at most 3 models for all preferences.",
            "author": [
                "Pengyuan Lu",
                "Michele Caprio",
                "Eric Eaton",
                "Insup Lee"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02995v3",
                "http://arxiv.org/pdf/2310.02995v3"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02993v1",
            "title": "Finding coherent node groups in directed graphs",
            "updated": "2023-10-04T17:28:58Z",
            "published": "2023-10-04T17:28:58Z",
            "summary": "Summarizing a large graph by grouping the nodes into clusters is a standard\ntechnique for studying the given network. Traditionally, the order of the\ndiscovered groups does not matter. However, there are applications where, for\nexample, given a directed graph, we would like to find coherent groups while\nminimizing the backward cross edges. More formally, in this paper, we study a\nproblem where we are given a directed network and are asked to partition the\ngraph into a sequence of coherent groups while attempting to conform to the\ncross edges. We assume that nodes in the network have features, and we measure\nthe group coherence by comparing these features. Furthermore, we incorporate\nthe cross edges by penalizing the forward cross edges and backward cross edges\nwith different weights. If the weights are set to 0, then the problem is\nequivalent to clustering. However, if we penalize the backward edges\nsignificantly more, then the order of discovered groups matters, and we can\nview our problem as a generalization of a classic segmentation problem. To\nsolve the algorithm we consider a common iterative approach where we solve the\ngroups given the centroids, and then find the centroids given the groups. We\nshow that - unlike in clustering - the first subproblem is NP-hard. However, we\nshow that if the underlying graph is a tree we can solve the subproblem with\ndynamic programming. In addition, if the number of groups is 2, we can solve\nthe subproblem with a minimum cut. For the more general case, we propose a\nheuristic where we optimize each pair of groups separately while keeping the\nremaining groups intact. We also propose a greedy search where nodes are moved\nbetween the groups while optimizing the overall loss. We demonstrate with our\nexperiments that the algorithms are practical and yield interpretable results.",
            "author": [
                "Iiro Kumpulainen",
                "Nikolaj Tatti"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02993v1",
                "http://arxiv.org/pdf/2310.02993v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI",
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02986v1",
            "title": "Exploring the Impact of Disrupted Peer-to-Peer Communications on Fully\n  Decentralized Learning in Disaster Scenarios",
            "updated": "2023-10-04T17:24:38Z",
            "published": "2023-10-04T17:24:38Z",
            "summary": "Fully decentralized learning enables the distribution of learning resources\nand decision-making capabilities across multiple user devices or nodes, and is\nrapidly gaining popularity due to its privacy-preserving and decentralized\nnature. Importantly, this crowdsourcing of the learning process allows the\nsystem to continue functioning even if some nodes are affected or disconnected.\nIn a disaster scenario, communication infrastructure and centralized systems\nmay be disrupted or completely unavailable, hindering the possibility of\ncarrying out standard centralized learning tasks in these settings. Thus, fully\ndecentralized learning can help in this case. However, transitioning from\ncentralized to peer-to-peer communications introduces a dependency between the\nlearning process and the topology of the communication graph among nodes. In a\ndisaster scenario, even peer-to-peer communications are susceptible to abrupt\nchanges, such as devices running out of battery or getting disconnected from\nothers due to their position. In this study, we investigate the effects of\nvarious disruptions to peer-to-peer communications on decentralized learning in\na disaster setting. We examine the resilience of a decentralized learning\nprocess when a subset of devices drop from the process abruptly. To this end,\nwe analyze the difference between losing devices holding data, i.e., potential\nknowledge, vs. devices contributing only to the graph connectivity, i.e., with\nno data. Our findings on a Barabasi-Albert graph topology, where training data\nis distributed across nodes in an IID fashion, indicate that the accuracy of\nthe learning process is more affected by a loss of connectivity than by a loss\nof data. Nevertheless, the network remains relatively robust, and the learning\nprocess can achieve a good level of accuracy.",
            "author": [
                "Luigi Palmieri",
                "Chiara Boldrini",
                "Lorenzo Valerio",
                "Andrea Passarella",
                "Marco Conti"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02986v1",
                "http://arxiv.org/pdf/2310.02986v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02979v1",
            "title": "Flexibility of graphs with maximum average degree less than $3$",
            "updated": "2023-10-04T17:14:44Z",
            "published": "2023-10-04T17:14:44Z",
            "summary": "In the flexible list coloring problem, we receive a graph $G$ and a color\nlist assignment $L$ on $G$, as well as a set of coloring preferences at some\nvertex subset of $G$, as input. Then, our goal is to find a proper $L$-coloring\nof $G$ that satisfies some given proportion of these coloring preferences. We\nsay that $G$ is $\\epsilon$-flexibly $k$-choosable if for every $k$-size list\nassignment $L$ on $G$ and every set of coloring preferences, $G$ has a proper\n$L$-coloring that satisfies an $\\epsilon$ proportion of these coloring\npreferences. Dvo\\v{r}\\'ak, Norin, and Postle [Journal of Graph Theory, 2019]\nasked whether every $d$-degenerate graph is $\\epsilon$-flexibly\n$(d+1)$-choosable for some constant $\\epsilon = \\epsilon(d) > 0$.\n  In this paper, we prove that there exists a constant $\\epsilon > 0$ such that\nevery graph with maximum average degree less than $3$ is $\\epsilon$-flexibly\n$3$-choosable, which gives a large class of $2$-degenerate graphs which are\n$\\epsilon$-flexibly $(d+1)$-choosable. In particular, our results imply a\ntheorem of Dvo\\v{r}\\'ak, Masa\\v{r}\\'ik, Mus\\'ilek, and Pangr\\'ac [Journal of\nGraph Theory, 2020] stating that every planar graph of girth $6$ is\n$\\epsilon$-flexibly $3$-choosable for some constant $\\epsilon > 0$. To prove\nour result, we generalize the existing reducible subgraph framework\ntraditionally used for flexible list coloring to allow reducible subgraphs of\narbitrarily large order.",
            "author": [
                "Richard Bi",
                "Peter Bradshaw"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02979v1",
                "http://arxiv.org/pdf/2310.02979v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C15"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02975v1",
            "title": "Towards Fully Adaptive Regret Minimization in Heavy-Tailed Bandits",
            "updated": "2023-10-04T17:11:15Z",
            "published": "2023-10-04T17:11:15Z",
            "summary": "Heavy-tailed distributions naturally arise in many settings, from finance to\ntelecommunications. While regret minimization under sub-Gaussian or bounded\nsupport rewards has been widely studied, learning on heavy-tailed distributions\nonly gained popularity over the last decade. In the stochastic heavy-tailed\nbandit problem, an agent learns under the assumption that the distributions\nhave finite moments of maximum order $1+\\epsilon$ which are uniformly bounded\nby a constant $u$, for some $\\epsilon \\in (0,1]$. To the best of our knowledge,\nliterature only provides algorithms requiring these two quantities as an input.\nIn this paper, we study the stochastic adaptive heavy-tailed bandit, a\nvariation of the standard setting where both $\\epsilon$ and $u$ are unknown to\nthe agent. We show that adaptivity comes at a cost, introducing two lower\nbounds on the regret of any adaptive algorithm, implying a higher regret w.r.t.\nthe standard setting. Finally, we introduce a specific distributional\nassumption and provide Adaptive Robust UCB, a regret minimization strategy\nmatching the known lower bound for the heavy-tailed MAB problem.",
            "author": [
                "Gianmarco Genalti",
                "Lupo Marsigli",
                "Nicola Gatti",
                "Alberto Maria Metelli"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02975v1",
                "http://arxiv.org/pdf/2310.02975v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02964v2",
            "title": "Co-modeling the Sequential and Graphical Routes for Peptide\n  Representation Learning",
            "updated": "2023-10-05T12:42:25Z",
            "published": "2023-10-04T16:58:25Z",
            "summary": "Peptides are formed by the dehydration condensation of multiple amino acids.\nThe primary structure of a peptide can be represented either as an amino acid\nsequence or as a molecular graph consisting of atoms and chemical bonds.\nPrevious studies have indicated that deep learning routes specific to\nsequential and graphical peptide forms exhibit comparable performance on\ndownstream tasks. Despite the fact that these models learn representations of\nthe same modality of peptides, we find that they explain their predictions\ndifferently. Considering sequential and graphical models as two experts making\ninferences from different perspectives, we work on fusing expert knowledge to\nenrich the learned representations for improving the discriminative\nperformance. To achieve this, we propose a peptide co-modeling method, RepCon,\nwhich employs a contrastive learning-based framework to enhance the mutual\ninformation of representations from decoupled sequential and graphical\nend-to-end models. It considers representations from the sequential encoder and\nthe graphical encoder for the same peptide sample as a positive pair and learns\nto enhance the consistency of representations between positive sample pairs and\nto repel representations between negative pairs. Empirical studies of RepCon\nand other co-modeling methods are conducted on open-source discriminative\ndatasets, including aggregation propensity, retention time, antimicrobial\npeptide prediction, and family classification from Peptide Database. Our\nresults demonstrate the superiority of the co-modeling approach over\nindependent modeling, as well as the superiority of RepCon over other methods\nunder the co-modeling framework. In addition, the attribution on RepCon further\ncorroborates the validity of the approach at the level of model explanation.",
            "author": [
                "Zihan Liu",
                "Ge Wang",
                "Jiaqi Wang",
                "Jiangbin Zheng",
                "Stan Z. Li"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02964v2",
                "http://arxiv.org/pdf/2310.02964v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03059v4",
            "title": "Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models",
            "updated": "2023-12-06T16:16:38Z",
            "published": "2023-10-04T16:49:36Z",
            "summary": "The popularity of pre-trained large models has revolutionized downstream\ntasks across diverse fields, such as language, vision, and multi-modality. To\nminimize the adaption cost for downstream tasks, many Parameter-Efficient\nFine-Tuning (PEFT) techniques are proposed for language and 2D image\npre-trained models. However, the specialized PEFT method for 3D pre-trained\nmodels is still under-explored. To this end, we introduce Point-PEFT, a novel\nframework for adapting point cloud pre-trained models with minimal learnable\nparameters. Specifically, for a pre-trained 3D model, we freeze most of its\nparameters, and only tune the newly added PEFT modules on downstream tasks,\nwhich consist of a Point-prior Prompt and a Geometry-aware Adapter. The\nPoint-prior Prompt adopts a set of learnable prompt tokens, for which we\npropose to construct a memory bank with domain-specific knowledge, and utilize\na parameter-free attention to enhance the prompt tokens. The Geometry-aware\nAdapter aims to aggregate point cloud features within spatial neighborhoods to\ncapture fine-grained geometric information through local interactions.\nExtensive experiments indicate that our Point-PEFT can achieve better\nperformance than the full fine-tuning on various downstream tasks, while using\nonly 5% of the trainable parameters, demonstrating the efficiency and\neffectiveness of our approach. Code will be released at\nhttps://github.com/Even-JK/PEFT-3D.",
            "author": [
                "Ivan Tang",
                "Ray Zhang",
                "Zoey Guo",
                "Xianzheng Ma",
                "Dong Wang",
                "Zhigang Wang",
                "Bin Zhao",
                "Xuelong Li"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03059v4",
                "http://arxiv.org/pdf/2310.03059v4"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02954v4",
            "title": "DQ-LoRe: Dual Queries with Low Rank Approximation Re-ranking for\n  In-Context Learning",
            "updated": "2023-10-20T07:50:50Z",
            "published": "2023-10-04T16:44:37Z",
            "summary": "Recent advances in natural language processing, primarily propelled by Large\nLanguage Models (LLMs), have showcased their remarkable capabilities grounded\nin in-context learning. A promising avenue for guiding LLMs in intricate\nreasoning tasks involves the utilization of intermediate reasoning steps within\nthe Chain-of-Thought (CoT) paradigm. Nevertheless, the central challenge lies\nin the effective selection of exemplars for facilitating in-context learning.\nIn this study, we introduce a framework that leverages Dual Queries and\nLow-rank approximation Re-ranking (DQ-LoRe) to automatically select exemplars\nfor in-context learning. Dual Queries first query LLM to obtain LLM-generated\nknowledge such as CoT, then query the retriever to obtain the final exemplars\nvia both question and the knowledge. Moreover, for the second query, LoRe\nemploys dimensionality reduction techniques to refine exemplar selection,\nensuring close alignment with the input question's knowledge. Through extensive\nexperiments, we demonstrate that DQ-LoRe significantly outperforms prior\nstate-of-the-art methods in the automatic selection of exemplars for GPT-4,\nenhancing performance from 92.5% to 94.2%. Our comprehensive analysis further\nreveals that DQ-LoRe consistently outperforms retrieval-based approaches in\nterms of both performance and adaptability, especially in scenarios\ncharacterized by distribution shifts. DQ-LoRe pushes the boundaries of\nin-context learning and opens up new avenues for addressing complex reasoning\nchallenges. We will release the code soon.",
            "author": [
                "Jing Xiong",
                "Zixuan Li",
                "Chuanyang Zheng",
                "Zhijiang Guo",
                "Yichun Yin",
                "Enze Xie",
                "Zhicheng Yang",
                "Qingxing Cao",
                "Haiming Wang",
                "Xiongwei Han",
                "Jing Tang",
                "Chengming Li",
                "Xiaodan Liang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02954v4",
                "http://arxiv.org/pdf/2310.02954v4"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02946v1",
            "title": "Local Max-Entropy and Free Energy Principles, Belief Diffusions and\n  their Singularities",
            "updated": "2023-10-04T16:32:10Z",
            "published": "2023-10-04T16:32:10Z",
            "summary": "A comprehensive picture of three Bethe-Kikuchi variational principles\nincluding their relationship to belief propagation (BP) algorithms on\nhypergraphs is given. The structure of BP equations is generalized to define\ncontinuous-time diffusions, solving localized versions of the max-entropy\nprinciple (A), the variational free energy principle (B), and a less usual\nequilibrium free energy principle (C), Legendre dual to A. Both critical points\nof Bethe-Kikuchi functionals and stationary beliefs are shown to lie at the\nnon-linear intersection of two constraint surfaces, enforcing energy\nconservation and marginal consistency respectively. The hypersurface of\nsingular beliefs, accross which equilibria become unstable as the constraint\nsurfaces meet tangentially, is described by polynomial equations in the convex\npolytope of consistent beliefs. This polynomial is expressed by a loop series\nexpansion for graphs of binary variables.",
            "author": [
                "Olivier Peltre"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02946v1",
                "http://arxiv.org/pdf/2310.02946v1"
            ],
            "primary_category": "math-ph",
            "category": [
                "math-ph",
                "cond-mat.dis-nn",
                "cs.AI",
                "cs.IT",
                "math.AT",
                "math.IT",
                "math.MP",
                "05-02, 94A17, 55N25, 55U10",
                "G.2.1; G.2.2; I.2.0"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02931v1",
            "title": "Graph data modelling for outcome prediction in oropharyngeal cancer\n  patients",
            "updated": "2023-10-04T16:09:35Z",
            "published": "2023-10-04T16:09:35Z",
            "summary": "Graph neural networks (GNNs) are becoming increasingly popular in the medical\ndomain for the tasks of disease classification and outcome prediction. Since\npatient data is not readily available as a graph, most existing methods either\nmanually define a patient graph, or learn a latent graph based on pairwise\nsimilarities between the patients. There are also hypergraph neural network\n(HGNN)-based methods that were introduced recently to exploit potential higher\norder associations between the patients by representing them as a hypergraph.\nIn this work, we propose a patient hypergraph network (PHGN), which has been\ninvestigated in an inductive learning setup for binary outcome prediction in\noropharyngeal cancer (OPC) patients using computed tomography (CT)-based\nradiomic features for the first time. Additionally, the proposed model was\nextended to perform time-to-event analyses, and compared with GNN and baseline\nlinear models.",
            "author": [
                "Nithya Bhasker",
                "Stefan Leger",
                "Alexander Zwanenburg",
                "Chethan Babu Reddy",
                "Sebastian Bodenstedt",
                "Steffen L\u00f6ck",
                "Stefanie Speidel"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02931v1",
                "http://arxiv.org/pdf/2310.02931v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02917v1",
            "title": "Fusion-stable structures on triangulation categories",
            "updated": "2023-10-04T16:00:00Z",
            "published": "2023-10-04T16:00:00Z",
            "summary": "Let $\\mathcal{G}$ be a fusion category acting on a triangulated category\n$\\mathcal{D}$, in the sense that $\\mathcal{D}$ is a $\\mathcal{G}$-module\ncategory. Our motivation example is fusion-weighted species, which is\nessentially Heng's construction. We study $\\mathcal{G}$-stable tilting, cluster\nand stability structures on $\\mathcal{D}$. In particular, we prove the\ndeformation theorem for $\\mathcal{G}$-stable stability conditions.\n  A first application is that Duffield-Tumarkin's categorification of cluster\nexchange graphs of finite Coxeter-Dynkin type can be naturally realized as\nfusion-stable cluster exchange graphs. Another application is that the\nuniversal cover of the hyperplane arrangements of any finite Coxeter-Dynkin\ntype can be realized as the space of fusion-stable stability conditions for\ncertain ADE Dynkin quiver.",
            "author": [
                "Yu Qiu",
                "Xiaoting Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02917v1",
                "http://arxiv.org/pdf/2310.02917v1"
            ],
            "primary_category": "math.RT",
            "category": [
                "math.RT",
                "math.QA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02911v3",
            "title": "Deciphering the Crypto-shopper: Knowledge and Preferences of Consumers\n  Using Cryptocurrencies for Purchases",
            "updated": "2023-11-04T00:55:10Z",
            "published": "2023-10-04T15:48:28Z",
            "summary": "The fast-growing cryptocurrency sector presents both challenges and\nopportunities for businesses and consumers alike. This study investigates the\nknowledge, expertise, and buying habits of people who shop using\ncryptocurrencies. Our survey of 516 participants shows that knowledge levels\nvary from beginners to experts. Interestingly, a segment of respondents, nearly\n30%, showed high purchase frequency despite their limited knowledge. Regression\nanalyses indicated that while domain knowledge plays a role, it only accounts\nfor 11.6% of the factors affecting purchasing frequency. A K-means cluster\nanalysis further segmented the respondents into three distinct groups, each\nhaving unique knowledge levels and purchasing tendencies. These results\nchallenge the conventional idea linking extensive knowledge to increased\ncryptocurrency usage, suggesting other factors at play. Understanding this\nvarying crypto-shopper demographic is pivotal for businesses, emphasizing the\nneed for tailored strategies and user-friendly experiences. This study offers\ninsights into current crypto-shopping behaviors and discusses future research\nexploring the broader impacts and potential shifts in the crypto-consumer\nlandscape.",
            "author": [
                "Massimiliano Silenzi",
                "Umut Can Cabuk"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02911v3",
                "http://arxiv.org/pdf/2310.02911v3"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY",
                "cs.CE",
                "cs.ET"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02909v1",
            "title": "The double Hall property and cycle covers in bipartite graphs",
            "updated": "2023-10-04T15:47:57Z",
            "published": "2023-10-04T15:47:57Z",
            "summary": "In a graph $G$, the $2$-neighborhood of a vertex set $X$ consists of all\nvertices of $G$ having at least $2$ neighbors in $X$. We say that a bipartite\ngraph $G(A,B)$ satisfies the double Hall property if $|A|\\geq2$, and every\nsubset $X \\subseteq A$ of size at least $2$ has a $2$-neighborhood of size at\nleast $|X|$. Salia conjectured that any bipartite graph $G(A,B)$ satisfying the\ndouble Hall property contains a cycle covering $A$. Here, we prove the\nexistence of a $2$-factor covering $A$ in any bipartite graph $G(A,B)$\nsatisfying the double Hall property. We also show Salia's conjecture for graphs\nwith restricted degrees of vertices in $B$. Additionally, we prove a lower\nbound on the number of edges in a graph satisfying the double Hall property,\nand the bound is sharp up to a constant factor.",
            "author": [
                "J\u00e1nos Bar\u00e1t",
                "Andrzej Grzesik",
                "Attila Jung",
                "Zolt\u00e1n L\u00f3r\u00e1nt Nagy",
                "D\u00f6m\u00f6t\u00f6r P\u00e1lv\u00f6lgyi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02909v1",
                "http://arxiv.org/pdf/2310.02909v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02907v2",
            "title": "Whole-body MPC for highly redundant legged manipulators: experimental\n  evaluation with a 37 DoF dual-arm quadruped",
            "updated": "2023-10-25T10:26:33Z",
            "published": "2023-10-04T15:45:16Z",
            "summary": "Recent progress in legged locomotion has rendered quadruped manipulators a\npromising solution for performing tasks that require both mobility and\nmanipulation (loco-manipulation). In the real world, task specifications and/or\nenvironment constraints may require the quadruped manipulator to be equipped\nwith high redundancy as well as whole-body motion coordination capabilities.\nThis work presents an experimental evaluation of a whole-body Model Predictive\nControl (MPC) framework achieving real-time performance on a dual-arm quadruped\nplatform consisting of 37 actuated joints. To the best of our knowledge this is\nthe legged manipulator with the highest number of joints to be controlled with\nreal-time whole-body MPC so far. The computational efficiency of the MPC while\nconsidering the full robot kinematics and the centroidal dynamics model builds\nupon an open-source DDP-variant solver and a state-of-the-art optimal control\nproblem formulation. Differently from previous works on quadruped manipulators,\nthe MPC is directly interfaced with the low-level joint impedance controllers\nwithout the need of designing an instantaneous whole-body controller. The\nfeasibility on the real hardware is showcased using the CENTAURO platform for\nthe challenging task of picking a heavy object from the ground. Dynamic\nstepping (trotting) is also showcased for first time with this robot. The\nresults highlight the potential of replanning with whole-body information in a\npredictive control loop.",
            "author": [
                "Ioannis Dadiotis",
                "Arturo Laurenzi",
                "Nikos Tsagarakis"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02907v2",
                "http://arxiv.org/pdf/2310.02907v2"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02902v1",
            "title": "Searching for High-Value Molecules Using Reinforcement Learning and\n  Transformers",
            "updated": "2023-10-04T15:40:07Z",
            "published": "2023-10-04T15:40:07Z",
            "summary": "Reinforcement learning (RL) over text representations can be effective for\nfinding high-value policies that can search over graphs. However, RL requires\ncareful structuring of the search space and algorithm design to be effective in\nthis challenge. Through extensive experiments, we explore how different design\nchoices for text grammar and algorithmic choices for training can affect an RL\npolicy's ability to generate molecules with desired properties. We arrive at a\nnew RL-based molecular design algorithm (ChemRLformer) and perform a thorough\nanalysis using 25 molecule design tasks, including computationally complex\nprotein docking simulations. From this analysis, we discover unique insights in\nthis problem space and show that ChemRLformer achieves state-of-the-art\nperformance while being more straightforward than prior work by demystifying\nwhich design choices are actually helpful for text-based molecule design.",
            "author": [
                "Raj Ghugare",
                "Santiago Miret",
                "Adriana Hugessen",
                "Mariano Phielipp",
                "Glen Berseth"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02902v1",
                "http://arxiv.org/pdf/2310.02902v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cond-mat.mtrl-sci",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02895v1",
            "title": "CoLiDE: Concomitant Linear DAG Estimation",
            "updated": "2023-10-04T15:32:27Z",
            "published": "2023-10-04T15:32:27Z",
            "summary": "We deal with the combinatorial problem of learning directed acyclic graph\n(DAG) structure from observational data adhering to a linear structural\nequation model (SEM). Leveraging advances in differentiable, nonconvex\ncharacterizations of acyclicity, recent efforts have advocated a continuous\nconstrained optimization paradigm to efficiently explore the space of DAGs.\nMost existing methods employ lasso-type score functions to guide this search,\nwhich (i) require expensive penalty parameter retuning when the\n$\\textit{unknown}$ SEM noise variances change across problem instances; and\n(ii) implicitly rely on limiting homoscedasticity assumptions. In this work, we\npropose a new convex score function for sparsity-aware learning of linear DAGs,\nwhich incorporates concomitant estimation of scale and thus effectively\ndecouples the sparsity parameter from the exogenous noise levels.\nRegularization via a smooth, nonconvex acyclicity penalty term yields CoLiDE\n($\\textbf{Co}$ncomitant $\\textbf{Li}$near $\\textbf{D}$AG\n$\\textbf{E}$stimation), a regression-based criterion amenable to efficient\ngradient computation and closed-form estimation of noise variances in\nheteroscedastic scenarios. Our algorithm outperforms state-of-the-art methods\nwithout incurring added complexity, especially when the DAGs are larger and the\nnoise level profile is heterogeneous. We also find CoLiDE exhibits enhanced\nstability manifested via reduced standard deviations in several domain-specific\nmetrics, underscoring the robustness of our novel linear DAG estimator.",
            "author": [
                "Seyed Saman Saboksayr",
                "Gonzalo Mateos",
                "Mariano Tepper"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02895v1",
                "http://arxiv.org/pdf/2310.02895v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02887v1",
            "title": "A Grammatical Compositional Model for Video Action Detection",
            "updated": "2023-10-04T15:24:00Z",
            "published": "2023-10-04T15:24:00Z",
            "summary": "Analysis of human actions in videos demands understanding complex human\ndynamics, as well as the interaction between actors and context. However, these\ninteraction relationships usually exhibit large intra-class variations from\ndiverse human poses or object manipulations, and fine-grained inter-class\ndifferences between similar actions. Thus the performance of existing methods\nis severely limited. Motivated by the observation that interactive actions can\nbe decomposed into actor dynamics and participating objects or humans, we\npropose to investigate the composite property of them. In this paper, we\npresent a novel Grammatical Compositional Model (GCM) for action detection\nbased on typical And-Or graphs. Our model exploits the intrinsic structures and\nlatent relationships of actions in a hierarchical manner to harness both the\ncompositionality of grammar models and the capability of expressing rich\nfeatures of DNNs. The proposed model can be readily embodied into a neural\nnetwork module for efficient optimization in an end-to-end manner. Extensive\nexperiments are conducted on the AVA dataset and the Something-Else task to\ndemonstrate the superiority of our model, meanwhile the interpretability is\nenhanced through an inference parsing procedure.",
            "author": [
                "Zhijun Zhang",
                "Xu Zou",
                "Jiahuan Zhou",
                "Sheng Zhong",
                "Ying Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02887v1",
                "http://arxiv.org/pdf/2310.02887v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02886v1",
            "title": "Composition of giant planets: the roles of pebbles and planetesimals",
            "updated": "2023-10-04T15:22:01Z",
            "published": "2023-10-04T15:22:01Z",
            "summary": "One of the current challenges of planet formation theory is to explain the\nenrichment of observed exoplanetary atmospheres. Past studies have focused on\nscenarios where either pebbles or planetesimals were the heavy element\nenrichment's drivers, we combine here both approaches to understand whether the\ncomposition of a planet can constrain its formation pathway. We study three\ndifferent formation scenarios: pebble accretion, pebble accretion with\nplanetesimal formation, combined pebble and planetesimal accretion. We use the\nchemcomp code to perform semi-analytical 1D simulations of protoplanetary\ndiscs, including viscous evolution, pebble drift, and simple chemistry to\nsimulate the growth of planets from planetary embryos to gas giants as they\nmigrate through the disc, while tracking their composition. Our simulations\nconfirm that the composition of the planetary atmosphere is dominated by the\naccretion of gas enriched by inward drifting and evaporating pebbles. Including\nplanetesimal formation hinders the enrichment, because the pebbles locked into\nplanetesimals cannot evaporate and enrich the disc. This results in a big drop\nof the accreted heavy elements both in the planetesimal formation and accretion\ncase, proving that planetesimal formation needs to be inefficient in order to\nexplain planets with high heavy element content. Accretion of planetesimals\nenhances the refractory component of the atmosphere, leading to low volatile to\nrefractory ratios, contrary to the pure pebble scenario. Such low volatile to\nrefractory ratios can also be achieved by planets migrating in the inner disc\nin pure pebble scenario. Distinguishing these two scenarios requires knowledge\nabout the planet's atmospheric C/H and O/H ratios, which are higher for pure\npebble accretion. Therefore, a detailed knowledge of the composition of\nplanetary atmospheres could help to constrain the planet's formation pathway.",
            "author": [
                "Claudia Danti",
                "Bertram Bitsch",
                "Jingyi Mah"
            ],
            "link": [
                "http://dx.doi.org/10.1051/0004-6361/202347501",
                "http://arxiv.org/abs/2310.02886v1",
                "http://arxiv.org/pdf/2310.02886v1"
            ],
            "primary_category": "astro-ph.EP",
            "category": [
                "astro-ph.EP"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02875v1",
            "title": "Approximating Robot Configuration Spaces with few Convex Sets using\n  Clique Covers of Visibility Graphs",
            "updated": "2023-10-04T15:09:44Z",
            "published": "2023-10-04T15:09:44Z",
            "summary": "Many computations in robotics can be dramatically accelerated if the robot\nconfiguration space is described as a collection of simple sets. For example,\nrecently developed motion planners rely on a convex decomposition of the free\nspace to design collision-free trajectories using fast convex optimization. In\nthis work, we present an efficient method for approximately covering complex\nconfiguration spaces with a small number of polytopes. The approach constructs\na visibility graph using sampling and generates a clique cover of this graph to\nfind clusters of samples that have mutual line of sight. These clusters are\nthen inflated into large, full-dimensional, polytopes. We evaluate our method\non a variety of robotic systems and show that it consistently covers larger\nportions of free configuration space, with fewer polytopes, and in a fraction\nof the time compared to previous methods.",
            "author": [
                "Peter Werner",
                "Alexandre Amice",
                "Tobia Marcucci",
                "Daniela Rus",
                "Russ Tedrake"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02875v1",
                "http://arxiv.org/pdf/2310.02875v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.CG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02871v2",
            "title": "Cluster braid groups of Coxeter-Dynkin diagrams",
            "updated": "2023-10-20T01:41:17Z",
            "published": "2023-10-04T15:06:45Z",
            "summary": "Cluster exchange groupoids are introduced by King-Qiu as an enhancement of\ncluster exchange graphs to study stability conditions and quadratic\ndifferentials. In this paper, we introduce the exchange groupoid for any finite\nCoxeter-Dynkin diagram $\\Delta$ and show that the fundamental group of which is\nisomorphic to the corresponding braid group associated with $\\Delta$.",
            "author": [
                "Zhe Han",
                "Ping He",
                "Yu Qiu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02871v2",
                "http://arxiv.org/pdf/2310.02871v2"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "math.GT",
                "math.RT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02861v2",
            "title": "Rayleigh Quotient Graph Neural Networks for Graph-level Anomaly\n  Detection",
            "updated": "2023-10-05T03:33:59Z",
            "published": "2023-10-04T14:47:27Z",
            "summary": "Graph-level anomaly detection has gained significant attention as it finds\nmany applications in various domains, such as cancer diagnosis and enzyme\nprediction. However, existing methods fail to capture the underlying properties\nof graph anomalies, resulting in unexplainable framework design and\nunsatisfying performance. In this paper, we take a step back and re-investigate\nthe spectral differences between anomalous and normal graphs. Our main\nobservation shows a significant disparity in the accumulated spectral energy\nbetween these two classes. Moreover, we prove that the accumulated spectral\nenergy of the graph signal can be represented by its Rayleigh Quotient,\nindicating that the Rayleigh Quotient is a driving factor behind the anomalous\nproperties of graphs. Motivated by this, we propose Rayleigh Quotient Graph\nNeural Network (RQGNN), the first spectral GNN for graph-level anomaly\ndetection, providing a new perspective on exploring the inherent spectral\nfeatures of anomalous graphs. Specifically, we introduce a novel framework that\nconsists of two components: the Rayleigh Quotient learning component (RQL) and\nChebyshev Wavelet GNN with RQ-pooling (CWGNN-RQ). RQL explicitly captures the\nRayleigh Quotient of graphs and CWGNN-RQ implicitly explores the spectral space\nof graphs. Extensive experiments on 10 real-world datasets show that RQGNN\noutperforms the best rival by 6.74% in Macro-F1 score and 1.44% in AUC,\ndemonstrating the effectiveness of our framework.",
            "author": [
                "Xiangyu Dong",
                "Xingyi Zhang",
                "Sibo Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02861v2",
                "http://arxiv.org/pdf/2310.02861v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02858v1",
            "title": "Scaling limits of branching Loewner evolutions and the Dyson\n  superprocess",
            "updated": "2023-10-04T14:44:41Z",
            "published": "2023-10-04T14:44:41Z",
            "summary": "This work introduces a construction of conformal processes that combines the\ntheory of branching processes with chordal Loewner evolution. The main novelty\nlies in the choice of driving measure for the Loewner evolution: given a finite\ngenealogical tree $\\mathcal{T}$, we choose a driving measure for the Loewner\nevolution that is supported on a system of particles that evolves by Dyson\nBrownian motion at inverse temperature $\\beta \\in (0,\\infty]$ between birth and\ndeath events.\n  When $\\beta=\\infty$, the driving measure degenerates to a system of particles\nthat evolves through Coulombic repulsion between branching events. In this\nlimit, the following graph embedding theorem is established: When $\\mathcal{T}$\nis equipped with a prescribed set of angles, $\\{\\theta_v \\in (0,\\pi/2)\\}_{v \\in\n\\mathcal{T}}$ the hull of the Loewner evolution is an embedding of\n$\\mathcal{T}$ into the upper half-plane with trivalent edges that meet at\nangles $(2\\theta_v,2\\pi-4\\theta_v,2\\theta_v)$ at the image of each edge $v$.\n  We also study the scaling limit when $\\beta\\in (0,\\infty]$ is fixed and\n$\\mathcal{T}$ is a binary Galton-Watson process that converges to a continuous\nstate branching process. We treat both the unconditioned case (when the\nGalton-Watson process converges to the Feller diffusion) and the conditioned\ncase (when the Galton-Watson tree converges to the continuum random tree). In\neach case, we characterize the scaling limit of the driving measure as a\nsuperprocess. In the unconditioned case, the scaling limit is the free\nprobability analogue of the Dawson-Watanabe superprocess that we term the Dyson\nsuperprocess.",
            "author": [
                "Vivian Olsiewski Healey",
                "Govind Menon"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02858v1",
                "http://arxiv.org/pdf/2310.02858v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR",
                "60J67, 60B20"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02840v1",
            "title": "Mosaic benchmark networks: Modular link streams for testing dynamic\n  community detection algorithms",
            "updated": "2023-10-04T14:09:23Z",
            "published": "2023-10-04T14:09:23Z",
            "summary": "Community structure is a critical feature of real networks, providing\ninsights into nodes' internal organization. Nowadays, with the availability of\nhighly detailed temporal networks such as link streams, studying community\nstructures becomes more complex due to increased data precision and time\nsensitivity. Despite numerous algorithms developed in the past decade for\ndynamic community discovery, assessing their performance on link streams\nremains a challenge. Synthetic benchmark graphs are a well-accepted approach\nfor evaluating static community detection algorithms. Additionally, there have\nbeen some proposals for slowly evolving communities in low-resolution temporal\nnetworks like snapshots. Nevertheless, this approach is not yet suitable for\nlink streams. To bridge this gap, we introduce a novel framework that generates\nsynthetic modular link streams with predefined communities. Subsequently, we\nevaluate established dynamic community detection methods to uncover limitations\nthat may not be evident in snapshots with slowly evolving communities. While no\nmethod emerges as a clear winner, we observe notable differences among them.",
            "author": [
                "Yasaman Asgari",
                "Remy Cazabet",
                "Pierre Borgnat"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02840v1",
                "http://arxiv.org/pdf/2310.02840v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02829v1",
            "title": "All Sizes Matter: Improving Volumetric Brain Segmentation on Small\n  Lesions",
            "updated": "2023-10-04T13:56:32Z",
            "published": "2023-10-04T13:56:32Z",
            "summary": "Brain metastases (BMs) are the most frequently occurring brain tumors. The\ntreatment of patients having multiple BMs with stereo tactic radiosurgery\nnecessitates accurate localization of the metastases. Neural networks can\nassist in this time-consuming and costly task that is typically performed by\nhuman experts. Particularly challenging is the detection of small lesions since\nthey are often underrepresented in exist ing approaches. Yet, lesion detection\nis equally important for all sizes. In this work, we develop an ensemble of\nneural networks explicitly fo cused on detecting and segmenting small BMs. To\naccomplish this task, we trained several neural networks focusing on individual\naspects of the BM segmentation problem: We use blob loss that specifically\naddresses the imbalance of lesion instances in terms of size and texture and\nis, therefore, not biased towards larger lesions. In addition, a model using a\nsubtraction sequence between the T1 and T1 contrast-enhanced sequence focuses\non low-contrast lesions. Furthermore, we train additional models only on small\nlesions. Our experiments demonstrate the utility of the ad ditional blob loss\nand the subtraction sequence. However, including the specialized small lesion\nmodels in the ensemble deteriorates segmentation results. We also find\ndomain-knowledge-inspired postprocessing steps to drastically increase our\nperformance in most experiments. Our approach enables us to submit a\ncompetitive challenge entry to the ASNR-MICCAI BraTS Brain Metastasis Challenge\n2023.",
            "author": [
                "Ayhan Can Erdur",
                "Daniel Scholz",
                "Josef A. Buchner",
                "Stephanie E. Combs",
                "Daniel Rueckert",
                "Jan C. Peeken"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02829v1",
                "http://arxiv.org/pdf/2310.02829v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02827v1",
            "title": "Coverings by open and closed hemispheres",
            "updated": "2023-10-04T13:50:16Z",
            "published": "2023-10-04T13:50:16Z",
            "summary": "In this paper we study the nerves of two types of coverings of a sphere\n$S^{d-1}$: (1) coverings by open hemispheres; (2) antipodal coverings by closed\nhemispheres. In the first case, nerve theorem implies that the nerve is\nhomotopy equivalent to $S^{d-1}$. In the second case, we prove that the nerve\nis homotopy equivalent to a wedge of $(2d-2)$-dimensional spheres. The number\nof wedge summands equals the M\\\"{o}bius invariant of the geometric lattice (or\nhyperplane arrangement) associated with the covering. This result explains some\nobserved large-scale phenomena in topological data analysis. We review the\nparticular case, when the coverings are centered in the root system $A_d$. In\nthis case the nerve of the covering by open hemispheres is the space of\ndirected acyclic graphs (DAGs), and the nerve of the covering by closed\nhemispheres is the space of non-strongly connected directed graphs. The\nhomotopy types of these spaces were described by Bj\\\"{o}rner and Welker, and\nthe incarnation of these spaces appeared independently as \"the poset of orders\"\nand \"the poset of preorders\" respectively in the works of Bouc. We study the\nspace of DAGs in terms of Gale and combinatorial Alexander dualities, and\npropose how this space can be applied in automated machine learning.",
            "author": [
                "Anton Ayzenberg",
                "Maxim Beketov",
                "German Magai"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02827v1",
                "http://arxiv.org/pdf/2310.02827v1"
            ],
            "primary_category": "math.AT",
            "category": [
                "math.AT",
                "math.CO",
                "math.HO",
                "55P10, 55P15, 57Z25, 54A10, 52C35 (Primary) 05C20, 52B35, 14N20,\n  05C40, 06A15, 52B12, 17B22, 52A55, 52B40 (Secondary)"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02823v1",
            "title": "Learning to Scale Logits for Temperature-Conditional GFlowNets",
            "updated": "2023-10-04T13:45:56Z",
            "published": "2023-10-04T13:45:56Z",
            "summary": "GFlowNets are probabilistic models that learn a stochastic policy that\nsequentially generates compositional structures, such as molecular graphs. They\nare trained with the objective of sampling such objects with probability\nproportional to the object's reward. Among GFlowNets, the\ntemperature-conditional GFlowNets represent a family of policies indexed by\ntemperature, and each is associated with the correspondingly tempered reward\nfunction. The major benefit of temperature-conditional GFlowNets is the\ncontrollability of GFlowNets' exploration and exploitation through adjusting\ntemperature. We propose Learning to Scale Logits for temperature-conditional\nGFlowNets (LSL-GFN), a novel architectural design that greatly accelerates the\ntraining of temperature-conditional GFlowNets. It is based on the idea that\npreviously proposed temperature-conditioning approaches introduced numerical\nchallenges in the training of the deep network because different temperatures\nmay give rise to very different gradient profiles and ideal scales of the\npolicy's logits. We find that the challenge is greatly reduced if a learned\nfunction of the temperature is used to scale the policy's logits directly. We\nempirically show that our strategy dramatically improves the performances of\nGFlowNets, outperforming other baselines, including reinforcement learning and\nsampling methods, in terms of discovering diverse modes in multiple biochemical\ntasks.",
            "author": [
                "Minsu Kim",
                "Joohwan Ko",
                "Dinghuai Zhang",
                "Ling Pan",
                "Taeyoung Yun",
                "Woochang Kim",
                "Jinkyoo Park",
                "Yoshua Bengio"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02823v1",
                "http://arxiv.org/pdf/2310.02823v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02815v2",
            "title": "CoBEV: Elevating Roadside 3D Object Detection with Depth and Height\n  Complementarity",
            "updated": "2023-10-18T01:44:22Z",
            "published": "2023-10-04T13:38:53Z",
            "summary": "Roadside camera-driven 3D object detection is a crucial task in intelligent\ntransportation systems, which extends the perception range beyond the\nlimitations of vision-centric vehicles and enhances road safety. While previous\nstudies have limitations in using only depth or height information, we find\nboth depth and height matter and they are in fact complementary. The depth\nfeature encompasses precise geometric cues, whereas the height feature is\nprimarily focused on distinguishing between various categories of height\nintervals, essentially providing semantic context. This insight motivates the\ndevelopment of Complementary-BEV (CoBEV), a novel end-to-end monocular 3D\nobject detection framework that integrates depth and height to construct robust\nBEV representations. In essence, CoBEV estimates each pixel's depth and height\ndistribution and lifts the camera features into 3D space for lateral fusion\nusing the newly proposed two-stage complementary feature selection (CFS)\nmodule. A BEV feature distillation framework is also seamlessly integrated to\nfurther enhance the detection accuracy from the prior knowledge of the\nfusion-modal CoBEV teacher. We conduct extensive experiments on the public 3D\ndetection benchmarks of roadside camera-based DAIR-V2X-I and Rope3D, as well as\nthe private Supremind-Road dataset, demonstrating that CoBEV not only achieves\nthe accuracy of the new state-of-the-art, but also significantly advances the\nrobustness of previous methods in challenging long-distance scenarios and noisy\ncamera disturbance, and enhances generalization by a large margin in\nheterologous settings with drastic changes in scene and camera parameters. For\nthe first time, the vehicle AP score of a camera model reaches 80% on\nDAIR-V2X-I in terms of easy mode. The source code will be made publicly\navailable at https://github.com/MasterHow/CoBEV.",
            "author": [
                "Hao Shi",
                "Chengshan Pang",
                "Jiaming Zhang",
                "Kailun Yang",
                "Yuhao Wu",
                "Huajian Ni",
                "Yining Lin",
                "Rainer Stiefelhagen",
                "Kaiwei Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02815v2",
                "http://arxiv.org/pdf/2310.02815v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.RO",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02814v4",
            "title": "Graph-based Simultaneous Localization and Bias Tracking",
            "updated": "2023-10-15T16:34:16Z",
            "published": "2023-10-04T13:38:46Z",
            "summary": "We present a factor graph formulation and particle-based sum-product\nalgorithm for robust localization and tracking in multipath-prone environments.\nThe proposed sequential algorithm jointly estimates the mobile agent's position\ntogether with a time-varying number of multipath components (MPCs). The MPCs\nare represented by \"delay biases\" corresponding to the offset between\nline-of-sight (LOS) component delay and the respective delays of all detectable\nMPCs. The delay biases of the MPCs capture the geometric features of the\npropagation environment with respect to the mobile agent. Therefore, they can\nprovide position-related information contained in the MPCs without explicitly\nbuilding a map of the environment. We demonstrate that the position-related\ninformation enables the algorithm to provide high-accuracy position estimates\neven in fully obstructed line-of-sight (OLOS) situations. Using simulated and\nreal measurements in different scenarios we demonstrate the proposed algorithm\nto significantly outperform state-of-the-art multipath-aided tracking\nalgorithms and show that the performance of our algorithm constantly attains\nthe posterior Cramer-Rao lower bound (P-CRLB). Furthermore, we demonstrate the\nimplicit capability of the proposed method to identify unreliable measurements\nand, thus, to mitigate lost tracks.",
            "author": [
                "Alexander Venus",
                "Erik Leitinger",
                "Stefan Tertinek",
                "Florian Meyer",
                "Klaus Witrisal"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02814v4",
                "http://arxiv.org/pdf/2310.02814v4"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02807v2",
            "title": "A Deep Instance Generative Framework for MILP Solvers Under Limited Data\n  Availability",
            "updated": "2023-10-28T12:10:46Z",
            "published": "2023-10-04T13:34:34Z",
            "summary": "In the past few years, there has been an explosive surge in the use of\nmachine learning (ML) techniques to address combinatorial optimization (CO)\nproblems, especially mixed-integer linear programs (MILPs). Despite the\nachievements, the limited availability of real-world instances often leads to\nsub-optimal decisions and biased solver assessments, which motivates a suite of\nsynthetic MILP instance generation techniques. However, existing methods either\nrely heavily on expert-designed formulations or struggle to capture the rich\nfeatures of real-world instances. To tackle this problem, we propose G2MILP,\nthe first deep generative framework for MILP instances. Specifically, G2MILP\nrepresents MILP instances as bipartite graphs, and applies a masked variational\nautoencoder to iteratively corrupt and replace parts of the original graphs to\ngenerate new ones. The appealing feature of G2MILP is that it can learn to\ngenerate novel and realistic MILP instances without prior expert-designed\nformulations, while preserving the structures and computational hardness of\nreal-world datasets, simultaneously. Thus the generated instances can\nfacilitate downstream tasks for enhancing MILP solvers under limited data\navailability. We design a suite of benchmarks to evaluate the quality of the\ngenerated MILP instances. Experiments demonstrate that our method can produce\ninstances that closely resemble real-world datasets in terms of both structures\nand computational hardness. The deliverables are released at\nhttps://miralab-ustc.github.io/L2O-G2MILP.",
            "author": [
                "Zijie Geng",
                "Xijun Li",
                "Jie Wang",
                "Xiao Li",
                "Yongdong Zhang",
                "Feng Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02807v2",
                "http://arxiv.org/pdf/2310.02807v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02800v3",
            "title": "Everest: GPU-Accelerated System For Mining Temporal Motifs",
            "updated": "2023-10-11T15:24:40Z",
            "published": "2023-10-04T13:21:04Z",
            "summary": "Temporal motif mining is the task of finding the occurrences of subgraph\npatterns within a large input temporal graph that obey the specified structural\nand temporal constraints. Despite its utility in several critical application\ndomains that demand high performance (e.g., detecting fraud in financial\ntransaction graphs), the performance of existing software is limited on\ncommercial hardware platforms, in that it runs for tens of hours. This paper\npresents Everest - a system that efficiently maps the workload of mining\n(supports both enumeration and counting) temporal motifs to the highly parallel\nGPU architecture. In particular, using an input temporal graph and a more\nexpressive user-defined temporal motif query definition compared to prior\nworks, Everest generates an execution plan and runtime primitives that optimize\nthe workload execution by exploiting the high compute throughput of a GPU.\nEverest generates motif-specific mining code to reduce long-latency memory\naccesses and frequent thread divergence operations. Everest incorporates novel\nlow-cost runtime mechanisms to enable load balancing to improve GPU hardware\nutilization. To support large graphs that do not fit on GPU memory, Everest\nalso supports multi-GPU execution by intelligently partitioning the edge list\nthat prevents inter-GPU communication. Everest hides the implementation\ncomplexity of presented optimizations away from the targeted system user for\nbetter usability. Our evaluation shows that, using proposed optimizations,\nEverest improves the performance of a baseline GPU implementation by 19x, on\naverage.",
            "author": [
                "Yichao Yuan",
                "Haojie Ye",
                "Sanketh Vedula",
                "Wynn Kaza",
                "Nishil Talati"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02800v3",
                "http://arxiv.org/pdf/2310.02800v3"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02799v2",
            "title": "Dissecting Smart Contract Languages: A Survey",
            "updated": "2023-10-06T15:22:20Z",
            "published": "2023-10-04T13:18:14Z",
            "summary": "Blockchain is a distributed ledger technology that gained popularity for\nenabling the transformation of cryptocurrency among peers without mediation by\na centralized third-party authority. Smart contracts expand the applications of\nblockchain technology and have played a role in its widespread adoption. Smart\ncontracts are immutable digital programs that are deployed on blockchains to\ncodify agreements between parties. Existing smart contract implementations have\nfaced challenges, including security vulnerabilities, leading to significant\nlosses and concerns. This has stimulated a wave of attempts to improve Smart\nContract Languages (SCLs) to overcome implementation challenges and ensure code\nquality, producing many languages with diverse features. Scholars have made\nsome attempts to classify SCLs and clarify the process of selecting an SCL, but\nto the best of our knowledge, no comprehensive survey of existing SCLs has been\npublished. Our work surpasses earlier efforts by evaluating a significantly\nlarger set of SCLs, in greater depth, to ease the process of SCL selection for\nblockchain research and implementation. In this paper, we (1) propose a robust\nframework for comparing existing SCLs, (2) analyze and discuss 36 SCLs,\naddressing issues beyond those used to construct the comparison framework, and\n(3) define new parameters for future research and development of SCLs. The\nsurvey provides a guide for those who intend to select or use an SCL to\nimplement smart contracts, develop new SCLs, or add new extensions to the\nexisting SCLs.",
            "author": [
                "Majd Soud",
                "G\u00edsli Hj\u00e1lmt\u00fdsson",
                "Mohammad Hamdaqa"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02799v2",
                "http://arxiv.org/pdf/2310.02799v2"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02792v1",
            "title": "Tracking Anything in Heart All at Once",
            "updated": "2023-10-04T13:11:20Z",
            "published": "2023-10-04T13:11:20Z",
            "summary": "Myocardial motion tracking stands as an essential clinical tool in the\nprevention and detection of Cardiovascular Diseases (CVDs), the foremost cause\nof death globally. However, current techniques suffer incomplete and inaccurate\nmotion estimation of the myocardium both in spatial and temporal dimensions,\nhindering the early identification of myocardial dysfunction. In addressing\nthese challenges, this paper introduces the Neural Cardiac Motion Field\n(NeuralCMF). NeuralCMF leverages the implicit neural representation (INR) to\nmodel the 3D structure and the comprehensive 6D forward/backward motion of the\nheart. This approach offers memory-efficient storage and continuous capability\nto query the precise shape and motion of the myocardium throughout the cardiac\ncycle at any specific point. Notably, NeuralCMF operates without the need for\npaired datasets, and its optimization is self-supervised through the physics\nknowledge priors both in space and time dimensions, ensuring compatibility with\nboth 2D and 3D echocardiogram video inputs. Experimental validations across\nthree representative datasets support the robustness and innovative nature of\nthe NeuralCMF, marking significant advantages over existing state-of-the-arts\nin cardiac imaging and motion tracking.",
            "author": [
                "Chengkang Shen",
                "Hao Zhu",
                "You Zhou",
                "Yu Liu",
                "Si Yi",
                "Lili Dong",
                "Weipeng Zhao",
                "David J. Brady",
                "Xun Cao",
                "Zhan Ma",
                "Yi Lin"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02792v1",
                "http://arxiv.org/pdf/2310.02792v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02791v1",
            "title": "R-LGP: A Reachability-guided Logic-geometric Programming Framework for\n  Optimal Task and Motion Planning on Mobile Manipulators",
            "updated": "2023-10-04T13:10:47Z",
            "published": "2023-10-04T13:10:47Z",
            "summary": "This paper presents an optimization-based solution to task and motion\nplanning (TAMP) on mobile manipulators. Logic-geometric programming (LGP) has\nshown promising capabilities for optimally dealing with hybrid TAMP problems\nthat involve abstract and geometric constraints. However, LGP does not scale\nwell to high-dimensional systems (e.g. mobile manipulators) and can suffer from\nobstacle avoidance issues. In this work, we extend LGP with a sampling-based\nreachability graph to enable solving optimal TAMP on high-DoF mobile\nmanipulators. The proposed reachability graph can incorporate environmental\ninformation (obstacles) to provide the planner with sufficient geometric\nconstraints. This reachability-aware heuristic efficiently prunes infeasible\nsequences of actions in the continuous domain, hence, it reduces replanning by\nsecuring feasibility at the final full trajectory optimization. Our framework\nproves to be time-efficient in computing optimal and collision-free solutions,\nwhile outperforming the current state of the art on metrics of success rate,\nplanning time, path length and number of steps. We validate our framework on\nthe physical Toyota HSR robot and report comparisons on a series of mobile\nmanipulation tasks of increasing difficulty.",
            "author": [
                "Kim Tien Ly",
                "Valeriy Semenov",
                "Mattia Risiglione",
                "Wolfgang Merkt",
                "Ioannis Havoutis"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02791v1",
                "http://arxiv.org/pdf/2310.02791v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02778v2",
            "title": "Integrating UMLS Knowledge into Large Language Models for Medical\n  Question Answering",
            "updated": "2023-10-13T12:10:01Z",
            "published": "2023-10-04T12:50:26Z",
            "summary": "Large language models (LLMs) have demonstrated powerful text generation\ncapabilities, bringing unprecedented innovation to the healthcare field. While\nLLMs hold immense promise for applications in healthcare, applying them to real\nclinical scenarios presents significant challenges, as these models may\ngenerate content that deviates from established medical facts and even exhibit\npotential biases. In our research, we develop an augmented LLM framework based\non the Unified Medical Language System (UMLS), aiming to better serve the\nhealthcare community. We employ LLaMa2-13b-chat and ChatGPT-3.5 as our\nbenchmark models, and conduct automatic evaluations using the ROUGE Score and\nBERTScore on 104 questions from the LiveQA test set. Additionally, we establish\ncriteria for physician-evaluation based on four dimensions: Factuality,\nCompleteness, Readability and Relevancy. ChatGPT-3.5 is used for physician\nevaluation with 20 questions on the LiveQA test set. Multiple resident\nphysicians conducted blind reviews to evaluate the generated content, and the\nresults indicate that this framework effectively enhances the factuality,\ncompleteness, and relevance of generated content. Our research demonstrates the\neffectiveness of using UMLS-augmented LLMs and highlights the potential\napplication value of LLMs in in medical question-answering.",
            "author": [
                "Rui Yang",
                "Edison Marrese-Taylor",
                "Yuhe Ke",
                "Lechao Cheng",
                "Qingyu Chen",
                "Irene Li"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02778v2",
                "http://arxiv.org/pdf/2310.02778v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02774v1",
            "title": "Graph Neural Networks and Time Series as Directed Graphs for Quality\n  Recognition",
            "updated": "2023-10-04T12:43:38Z",
            "published": "2023-10-04T12:43:38Z",
            "summary": "Graph Neural Networks (GNNs) are becoming central in the study of time\nseries, coupled with existing algorithms as Temporal Convolutional Networks and\nRecurrent Neural Networks. In this paper, we see time series themselves as\ndirected graphs, so that their topology encodes time dependencies and we start\nto explore the effectiveness of GNNs architectures on them. We develop two\ndistinct Geometric Deep Learning models, a supervised classifier and an\nautoencoder-like model for signal reconstruction. We apply these models on a\nquality recognition problem.",
            "author": [
                "Angelica Simonetti",
                "Ferdinando Zanchetta"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02774v1",
                "http://arxiv.org/pdf/2310.02774v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02767v1",
            "title": "Kernel-based function learning in dynamic and non stationary\n  environments",
            "updated": "2023-10-04T12:31:31Z",
            "published": "2023-10-04T12:31:31Z",
            "summary": "One central theme in machine learning is function estimation from sparse and\nnoisy data. An example is supervised learning where the elements of the\ntraining set are couples, each containing an input location and an output\nresponse. In the last decades, a substantial amount of work has been devoted to\ndesign estimators for the unknown function and to study their convergence to\nthe optimal predictor, also characterizing the learning rate. These results\ntypically rely on stationary assumptions where input locations are drawn from a\nprobability distribution that does not change in time. In this work, we\nconsider kernel-based ridge regression and derive convergence conditions under\nnon stationary distributions, addressing also cases where stochastic adaption\nmay happen infinitely often. This includes the important\nexploration-exploitation problems where e.g. a set of agents/robots has to\nmonitor an environment to reconstruct a sensorial field and their movements\nrules are continuously updated on the basis of the acquired knowledge on the\nfield and/or the surrounding environment.",
            "author": [
                "Alberto Giaretta",
                "Mauro Bisiacco",
                "Gianluigi Pillonetto"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02767v1",
                "http://arxiv.org/pdf/2310.02767v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02762v1",
            "title": "Recurrences for values of the Hurwitz type poly-Bernoulli numbers and\n  polynomials",
            "updated": "2023-10-04T12:19:11Z",
            "published": "2023-10-04T12:19:11Z",
            "summary": "The main object of this paper is to investigate a new class of the\ngeneralized Hurwitz type poly-Bernoulli numbers and polynomials from which we\nderive some algorithms for evaluating the Hurwitz type poly-Bernoulli numbers\nand polynomials. By introducing a new generalization of the Stirling numbers of\nthe second kind, we succeed to establish some combinatorial formulas for the\ngeneralized Hurwitz type poly-Bernoulli numbers and polynomials with negative\nupper indices. Moreover, we give a connection between the generalized Stirling\nnumbers of the second kind and graph theory.",
            "author": [
                "Mohamed Amine Boutiche",
                "Mohamed Mechacha",
                "Mourad Rahmani"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02762v1",
                "http://arxiv.org/pdf/2310.02762v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "math.NT",
                "11B68, 11B73, 11M35"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02744v1",
            "title": "SALSA: Semantically-Aware Latent Space Autoencoder",
            "updated": "2023-10-04T11:34:46Z",
            "published": "2023-10-04T11:34:46Z",
            "summary": "In deep learning for drug discovery, chemical data are often represented as\nsimplified molecular-input line-entry system (SMILES) sequences which allow for\nstraightforward implementation of natural language processing methodologies,\none being the sequence-to-sequence autoencoder. However, we observe that\ntraining an autoencoder solely on SMILES is insufficient to learn molecular\nrepresentations that are semantically meaningful, where semantics are defined\nby the structural (graph-to-graph) similarities between molecules. We\ndemonstrate by example that autoencoders may map structurally similar molecules\nto distant codes, resulting in an incoherent latent space that does not respect\nthe structural similarities between molecules. To address this shortcoming we\npropose Semantically-Aware Latent Space Autoencoder (SALSA), a\ntransformer-autoencoder modified with a contrastive task, tailored specifically\nto learn graph-to-graph similarity between molecules. Formally, the contrastive\nobjective is to map structurally similar molecules (separated by a single graph\nedit) to nearby codes in the latent space. To accomplish this, we generate a\nnovel dataset comprised of sets of structurally similar molecules and opt for a\nsupervised contrastive loss that is able to incorporate full sets of positive\nsamples. We compare SALSA to its ablated counterparts, and show empirically\nthat the composed training objective (reconstruction and contrastive task)\nleads to a higher quality latent space that is more 1) structurally-aware, 2)\nsemantically continuous, and 3) property-aware.",
            "author": [
                "Kathryn E. Kirchoff",
                "Travis Maxfield",
                "Alexander Tropsha",
                "Shawn M. Gomez"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02744v1",
                "http://arxiv.org/pdf/2310.02744v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04455v1",
            "title": "Inclusive Data Representation in Federated Learning: A Novel Approach\n  Integrating Textual and Visual Prompt",
            "updated": "2023-10-04T11:20:28Z",
            "published": "2023-10-04T11:20:28Z",
            "summary": "Federated Learning (FL) is often impeded by communication overhead issues.\nPrompt tuning, as a potential solution, has been introduced to only adjust a\nfew trainable parameters rather than the whole model. However, current\nsingle-modality prompt tuning approaches fail to comprehensively portray local\nclients' data. To overcome this limitation, we present Twin Prompt Federated\nlearning (TPFL), a pioneering solution that integrates both visual and textual\nmodalities, ensuring a more holistic representation of local clients' data\ncharacteristics. Furthermore, in order to tackle the data heterogeneity issues,\nwe introduce the Augmented TPFL (ATPFL) employing the contrastive learning to\nTPFL, which not only enhances the global knowledge acquisition of client models\nbut also fosters the development of robust, compact models. The effectiveness\nof TPFL and ATPFL is substantiated by our extensive evaluations, consistently\nshowing superior performance compared to all baselines.",
            "author": [
                "Zihao Zhao",
                "Zhenpeng Shi",
                "Yang Liu",
                "Wenbo Ding"
            ],
            "link": [
                "http://dx.doi.org/10.1145/3594739.3612914",
                "http://arxiv.org/abs/2310.04455v1",
                "http://arxiv.org/pdf/2310.04455v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02732v1",
            "title": "Discriminative Training of VBx Diarization",
            "updated": "2023-10-04T11:10:25Z",
            "published": "2023-10-04T11:10:25Z",
            "summary": "Bayesian HMM clustering of x-vector sequences (VBx) has become a widely\nadopted diarization baseline model in publications and challenges. It uses an\nHMM to model speaker turns, a generatively trained probabilistic linear\ndiscriminant analysis (PLDA) for speaker distribution modeling, and Bayesian\ninference to estimate the assignment of x-vectors to speakers. This paper\npresents a new framework for updating the VBx parameters using discriminative\ntraining, which directly optimizes a predefined loss. We also propose a new\nloss that better correlates with the diarization error rate compared to binary\ncross-entropy $\\unicode{x2013}$ the default choice for diarization end-to-end\nsystems. Proof-of-concept results across three datasets (AMI, CALLHOME, and\nDIHARD II) demonstrate the method's capability of automatically finding\nhyperparameters, achieving comparable performance to those found by extensive\ngrid search, which typically requires additional hyperparameter behavior\nknowledge. Moreover, we show that discriminative fine-tuning of PLDA can\nfurther improve the model's performance. We release the source code with this\npublication.",
            "author": [
                "Dominik Klement",
                "Mireia Diez",
                "Federico Landini",
                "Luk\u00e1\u0161 Burget",
                "Anna Silnova",
                "Marc Delcroix",
                "Naohiro Tawara"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02732v1",
                "http://arxiv.org/pdf/2310.02732v1"
            ],
            "primary_category": "eess.AS",
            "category": [
                "eess.AS",
                "cs.SD"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02730v1",
            "title": "Emergence and control of synchronization in networks with directed\n  many-body interactions",
            "updated": "2023-10-04T11:08:33Z",
            "published": "2023-10-04T11:08:33Z",
            "summary": "The emergence of collective behaviors in networks of dynamical units in\npairwise interaction has been explained as the effect of diffusive coupling.\nHow does the presence of higher-order interaction impact the onset of\nspontaneous or induced synchronous behavior? Inspired by actuation and\nmeasurement constraints typical of physical and engineered systems, we propose\na diffusion mechanism over hypergraphs that explains the onset of\nsynchronization through a clarifying analogy with signed graphs. Our findings\nare mathematically backed by general conditions for convergence to the\nsynchronous state.",
            "author": [
                "Fabio Della Rossa",
                "Davide Liuzza",
                "Francesco Lo Iudice",
                "Pietro De Lellis"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02730v1",
                "http://arxiv.org/pdf/2310.02730v1"
            ],
            "primary_category": "nlin.AO",
            "category": [
                "nlin.AO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02722v1",
            "title": "Discrete-time Quantum Walk on Multilayer Networks",
            "updated": "2023-10-04T10:55:16Z",
            "published": "2023-10-04T10:55:16Z",
            "summary": "Multilayer network is a potent platform which paves a way to study the\ninteractions among entities in various networks with multiple types of\nrelationships. In this study, the dynamics of discrete-time quantum walk on a\nmultilayer network are explored in detail. We derive recurrence formulae for\nthe coefficients of the wave function of a quantum walker on an undirected\ngraph with finite number of nodes. By extending these formulae to include extra\nlayers, we develop a simulation model to describe the time-evolution of the\nquantum walker on a multilayer network. The time-averaged probability and the\nreturn probability of the quantum walker are studied in relation to Fourier and\nGrover walks on multilayer networks. Furthermore, we analyze the impact of\ndecoherence on the quantum transport, shedding light on how environmental\ninteractions may impact the behavior of quantum walkers on multilayer network\nstructures.",
            "author": [
                "M. N. Jayakody",
                "Priodyuti Pradhan",
                "Dana Ben Porath",
                "E. Cohen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02722v1",
                "http://arxiv.org/pdf/2310.02722v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02721v1",
            "title": "Leveraging Temporal Graph Networks Using Module Decoupling",
            "updated": "2023-10-04T10:52:51Z",
            "published": "2023-10-04T10:52:51Z",
            "summary": "Modern approaches for learning on dynamic graphs have adopted the use of\nbatches instead of applying updates one by one. The use of batches allows these\ntechniques to become helpful in streaming scenarios where updates to graphs are\nreceived at extreme speeds. Using batches, however, forces the models to update\ninfrequently, which results in the degradation of their performance. In this\nwork, we suggest a decoupling strategy that enables the models to update\nfrequently while using batches. By decoupling the core modules of temporal\ngraph networks and implementing them using a minimal number of learnable\nparameters, we have developed the Lightweight Decoupled Temporal Graph Network\n(LDTGN), an exceptionally efficient model for learning on dynamic graphs. LDTG\nwas validated on various dynamic graph benchmarks, providing comparable or\nstate-of-the-art results with significantly higher throughput than previous\nart. Notably, our method outperforms previous approaches by more than 20\\% on\nbenchmarks that require rapid model update rates, such as USLegis or UNTrade.\nThe code to reproduce our experiments is available at\n\\href{https://orfeld415.github.io/module-decoupling}{this http url}.",
            "author": [
                "Or Feldman",
                "Chaim Baskin"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02721v1",
                "http://arxiv.org/pdf/2310.02721v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02717v2",
            "title": "Online Clustering of Bandits with Misspecified User Models",
            "updated": "2023-10-10T08:59:25Z",
            "published": "2023-10-04T10:40:50Z",
            "summary": "The contextual linear bandit is an important online learning problem where\ngiven arm features, a learning agent selects an arm at each round to maximize\nthe cumulative rewards in the long run. A line of works, called the clustering\nof bandits (CB), utilize the collaborative effect over user preferences and\nhave shown significant improvements over classic linear bandit algorithms.\nHowever, existing CB algorithms require well-specified linear user models and\ncan fail when this critical assumption does not hold. Whether robust CB\nalgorithms can be designed for more practical scenarios with misspecified user\nmodels remains an open problem. In this paper, we are the first to present the\nimportant problem of clustering of bandits with misspecified user models\n(CBMUM), where the expected rewards in user models can be perturbed away from\nperfect linear models. We devise two robust CB algorithms, RCLUMB and RSCLUMB\n(representing the learned clustering structure with dynamic graph and sets,\nrespectively), that can accommodate the inaccurate user preference estimations\nand erroneous clustering caused by model misspecifications. We prove regret\nupper bounds of $O(\\epsilon_*T\\sqrt{md\\log T} + d\\sqrt{mT}\\log T)$ for our\nalgorithms under milder assumptions than previous CB works (notably, we move\npast a restrictive technical assumption on the distribution of the arms), which\nmatch the lower bound asymptotically in $T$ up to logarithmic factors, and also\nmatch the state-of-the-art results in several degenerate cases. The techniques\nin proving the regret caused by misclustering users are quite general and may\nbe of independent interest. Experiments on both synthetic and real-world data\nshow our outperformance over previous algorithms.",
            "author": [
                "Zhiyong Wang",
                "Jize Xie",
                "Xutong Liu",
                "Shuai Li",
                "John C. S. Lui"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02717v2",
                "http://arxiv.org/pdf/2310.02717v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02707v1",
            "title": "Geometric spectral theory of quantum graphs",
            "updated": "2023-10-04T10:20:56Z",
            "published": "2023-10-04T10:20:56Z",
            "summary": "These are lecture notes from a course given at the summer school ``Heat\nkernels and spectral geometry: from manifolds to graphs'' in Bregenz, Austria,\n2022. They are designed to be accessible to doctoral level students, and\ninclude background chapters on Laplacians on domains and quantum graphs before\nmoving on to specialised topics involving the dependence and optimisation of\noperator eigenvalues on a metric graph in function of the graph geometry, drawn\nin part from the recent literature.",
            "author": [
                "James Kennedy"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02707v1",
                "http://arxiv.org/pdf/2310.02707v1"
            ],
            "primary_category": "math.SP",
            "category": [
                "math.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02701v1",
            "title": "Cheeger cuts and Robin spectral minimal partitions of metric graphs",
            "updated": "2023-10-04T10:13:48Z",
            "published": "2023-10-04T10:13:48Z",
            "summary": "We study partition problems based on two ostensibly different kinds of energy\nfunctionals defined on $k$-partitions of metric graphs: Cheeger-type\nfunctionals whose minimisers are the $k$-Cheeger cuts of the graph, and the\ncorresponding values are the $k$-Cheeger constants of the graph; and\nfunctionals built using the first eigenvalue of the Laplacian with positive,\ni.e. absorbing, Robin (delta) vertex conditions at the boundary of the\npartition elements. We prove existence of minimising $k$-partitions, $k \\geq\n2$, for both these functionals. We also show that, for each $k \\geq 2$, as the\nRobin parameter $\\alpha \\to 0$, up to a renormalisation the spectral minimal\nRobin energy converges to the $k$-Cheeger constant. Moreover, up to a\nsubsequence, the Robin spectral minimal $k$-partitions converge in a natural\nsense to a $k$-Cheeger cut of the graph. Finally, we show that as $\\alpha \\to\n\\infty$ there is convergence in a similar sense to the corresponding Dirichlet\nminimal energy and partitions.\n  It is strongly expected that similar results hold on general (smooth,\nbounded) Euclidean domains and manifolds.",
            "author": [
                "James B. Kennedy",
                "Jo\u00e3o P. Ribeiro"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02701v1",
                "http://arxiv.org/pdf/2310.02701v1"
            ],
            "primary_category": "math.SP",
            "category": [
                "math.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02699v1",
            "title": "Continual Contrastive Spoken Language Understanding",
            "updated": "2023-10-04T10:09:12Z",
            "published": "2023-10-04T10:09:12Z",
            "summary": "Recently, neural networks have shown impressive progress across diverse\nfields, with speech processing being no exception. However, recent\nbreakthroughs in this area require extensive offline training using large\ndatasets and tremendous computing resources. Unfortunately, these models\nstruggle to retain their previously acquired knowledge when learning new tasks\ncontinually, and retraining from scratch is almost always impractical. In this\npaper, we investigate the problem of learning sequence-to-sequence models for\nspoken language understanding in a class-incremental learning (CIL) setting and\nwe propose COCONUT, a CIL method that relies on the combination of experience\nreplay and contrastive learning. Through a modified version of the standard\nsupervised contrastive loss applied only to the rehearsal samples, COCONUT\npreserves the learned representations by pulling closer samples from the same\nclass and pushing away the others. Moreover, we leverage a multimodal\ncontrastive loss that helps the model learn more discriminative representations\nof the new data by aligning audio and text features. We also investigate\ndifferent contrastive designs to combine the strengths of the contrastive loss\nwith teacher-student architectures used for distillation. Experiments on two\nestablished SLU datasets reveal the effectiveness of our proposed approach and\nsignificant improvements over the baselines. We also show that COCONUT can be\ncombined with methods that operate on the decoder side of the model, resulting\nin further metrics improvements.",
            "author": [
                "Umberto Cappellazzo",
                "Enrico Fini",
                "Muqiao Yang",
                "Daniele Falavigna",
                "Alessio Brutti",
                "Bhiksha Raj"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02699v1",
                "http://arxiv.org/pdf/2310.02699v1"
            ],
            "primary_category": "eess.AS",
            "category": [
                "eess.AS",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02692v1",
            "title": "Bridging the Domain Gap by Clustering-based Image-Text Graph Matching",
            "updated": "2023-10-04T10:03:07Z",
            "published": "2023-10-04T10:03:07Z",
            "summary": "Learning domain-invariant representations is important to train a model that\ncan generalize well to unseen target task domains. Text descriptions inherently\ncontain semantic structures of concepts and such auxiliary semantic cues can be\nused as effective pivot embedding for domain generalization problems. Here, we\nuse multimodal graph representations, fusing images and text, to get\ndomain-invariant pivot embeddings by considering the inherent semantic\nstructure between local images and text descriptors. Specifically, we aim to\nlearn domain-invariant features by (i) representing the image and text\ndescriptions with graphs, and by (ii) clustering and matching the graph-based\nimage node features into textual graphs simultaneously. We experiment with\nlarge-scale public datasets, such as CUB-DG and DomainBed, and our model\nachieves matched or better state-of-the-art performance on these datasets. Our\ncode will be publicly available upon publication.",
            "author": [
                "Nokyung Park",
                "Daewon Chae",
                "Jeongyong Shim",
                "Sangpil Kim",
                "Eun-Sol Kim",
                "Jinkyu Kim"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02692v1",
                "http://arxiv.org/pdf/2310.02692v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02655v1",
            "title": "AGIR: Automating Cyber Threat Intelligence Reporting with Natural\n  Language Generation",
            "updated": "2023-10-04T08:25:37Z",
            "published": "2023-10-04T08:25:37Z",
            "summary": "Cyber Threat Intelligence (CTI) reporting is pivotal in contemporary risk\nmanagement strategies. As the volume of CTI reports continues to surge, the\ndemand for automated tools to streamline report generation becomes increasingly\napparent. While Natural Language Processing techniques have shown potential in\nhandling text data, they often struggle to address the complexity of diverse\ndata sources and their intricate interrelationships. Moreover, established\nparadigms like STIX have emerged as de facto standards within the CTI\ncommunity, emphasizing the formal categorization of entities and relations to\nfacilitate consistent data sharing. In this paper, we introduce AGIR (Automatic\nGeneration of Intelligence Reports), a transformative Natural Language\nGeneration tool specifically designed to address the pressing challenges in the\nrealm of CTI reporting. AGIR's primary objective is to empower security\nanalysts by automating the labor-intensive task of generating comprehensive\nintelligence reports from formal representations of entity graphs. AGIR\nutilizes a two-stage pipeline by combining the advantages of template-based\napproaches and the capabilities of Large Language Models such as ChatGPT. We\nevaluate AGIR's report generation capabilities both quantitatively and\nqualitatively. The generated reports accurately convey information expressed\nthrough formal language, achieving a high recall value (0.99) without\nintroducing hallucination. Furthermore, we compare the fluency and utility of\nthe reports with state-of-the-art approaches, showing how AGIR achieves higher\nscores in terms of Syntactic Log-Odds Ratio (SLOR) and through questionnaires.\nBy using our tool, we estimate that the report writing time is reduced by more\nthan 40%, therefore streamlining the CTI production of any organization and\ncontributing to the automation of several CTI tasks.",
            "author": [
                "Filippo Perrina",
                "Francesco Marchiori",
                "Mauro Conti",
                "Nino Vincenzo Verde"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02655v1",
                "http://arxiv.org/pdf/2310.02655v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02648v1",
            "title": "Long-Term Dynamic Window Approach for Kinodynamic Local Planning in\n  Static and Crowd Environments",
            "updated": "2023-10-04T08:13:44Z",
            "published": "2023-10-04T08:13:44Z",
            "summary": "Local planning for a differential wheeled robot is designed to generate\nkinodynamic feasible actions that guide the robot to a goal position along the\nnavigation path while avoiding obstacles. Reactive, predictive, and\nlearning-based methods are widely used in local planning. However, few of them\ncan fit static and crowd environments while satisfying kinodynamic constraints\nsimultaneously. To solve this problem, we propose a novel local planning\nmethod. The method applies a long-term dynamic window approach to generate an\ninitial trajectory and then optimizes it with graph optimization. The method\ncan plan actions under the robot's kinodynamic constraints in real time while\nallowing the generated actions to be safer and more jitterless. Experimental\nresults show that the proposed method adapts well to crowd and static\nenvironments and outperforms most SOTA approaches.",
            "author": [
                "Zhiqiang Jian",
                "Songyi Zhang",
                "Lingfeng Sun",
                "Wei Zhan",
                "Nanning Zheng",
                "Masayoshi Tomizuka"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02648v1",
                "http://arxiv.org/pdf/2310.02648v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02638v1",
            "title": "P2CADNet: An End-to-End Reconstruction Network for Parametric 3D CAD\n  Model from Point Clouds",
            "updated": "2023-10-04T08:00:05Z",
            "published": "2023-10-04T08:00:05Z",
            "summary": "Computer Aided Design (CAD), especially the feature-based parametric CAD,\nplays an important role in modern industry and society. However, the\nreconstruction of featured CAD model is more challenging than the\nreconstruction of other CAD models. To this end, this paper proposes an\nend-to-end network to reconstruct featured CAD model from point cloud\n(P2CADNet). Initially, the proposed P2CADNet architecture combines a point\ncloud feature extractor, a CAD sequence reconstructor and a parameter\noptimizer. Subsequently, in order to reconstruct the featured CAD model in an\nautoregressive way, the CAD sequence reconstructor applies two transformer\ndecoders, one with target mask and the other without mask. Finally, for\npredicting parameters more precisely, we design a parameter optimizer with\ncross-attention mechanism to further refine the CAD feature parameters. We\nevaluate P2CADNet on the public dataset, and the experimental results show that\nP2CADNet has excellent reconstruction quality and accuracy. To our best\nknowledge, P2CADNet is the first end-to-end network to reconstruct featured CAD\nmodel from point cloud, and can be regarded as baseline for future works.\nTherefore, we open the source code at https://github.com/Blice0415/P2CADNet.",
            "author": [
                "Zhihao Zong",
                "Fazhi He",
                "Rubin Fan",
                "Yuxin Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02638v1",
                "http://arxiv.org/pdf/2310.02638v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.16841v1",
            "title": "Linkages among the Foreign Exchange, Stock, and Bond Markets in Japan\n  and the United States",
            "updated": "2023-10-04T07:20:45Z",
            "published": "2023-10-04T07:20:45Z",
            "summary": "While economic theory explains the linkages among the financial markets of\ndifferent countries, empirical studies mainly verify the linkages through\nGranger causality, without considering latent variables or instantaneous\neffects. Their findings are inconsistent regarding the existence of causal\nlinkages among financial markets, which might be attributed to differences in\nthe focused markets, data periods, and methods applied. Our study adopts causal\ndiscovery methods including VAR-LiNGAM and LPCMCI with domain knowledge to\nexplore the linkages among financial markets in Japan and the United States\n(US) for the post Covid-19 pandemic period under divergent monetary policy\ndirections. The VAR-LiNGAM results reveal that the previous day's US market\ninfluences the following day's Japanese market for both stocks and bonds, and\nthe bond markets of the previous day impact the following day's foreign\nexchange (FX) market directly and the following day's Japanese stock market\nindirectly. The LPCMCI results indicate the existence of potential latent\nconfounders. Our results demonstrate that VAR-LiNGAM uniquely identifies the\ndirected acyclic graph (DAG), and thus provides informative insight into the\ncausal relationship when the assumptions are considered valid. Our study\ncontributes to a better understanding of the linkages among financial markets\nin the analyzed data period by supporting the existence of linkages between\nJapan and the US for the same financial markets and among FX, stock, and bond\nmarkets, thus highlighting the importance of leveraging causal discovery\nmethods in the financial domain.",
            "author": [
                "Yi Jiang",
                "Shohei Shimizu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.16841v1",
                "http://arxiv.org/pdf/2310.16841v1"
            ],
            "primary_category": "q-fin.ST",
            "category": [
                "q-fin.ST"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02619v1",
            "title": "Generative Modeling of Regular and Irregular Time Series Data via\n  Koopman VAEs",
            "updated": "2023-10-04T07:14:43Z",
            "published": "2023-10-04T07:14:43Z",
            "summary": "Generating realistic time series data is important for many engineering and\nscientific applications. Existing work tackles this problem using generative\nadversarial networks (GANs). However, GANs are often unstable during training,\nand they can suffer from mode collapse. While variational autoencoders (VAEs)\nare known to be more robust to these issues, they are (surprisingly) less often\nconsidered for time series generation. In this work, we introduce Koopman VAE\n(KVAE), a new generative framework that is based on a novel design for the\nmodel prior, and that can be optimized for either regular and irregular\ntraining data. Inspired by Koopman theory, we represent the latent conditional\nprior dynamics using a linear map. Our approach enhances generative modeling\nwith two desired features: (i) incorporating domain knowledge can be achieved\nby leverageing spectral tools that prescribe constraints on the eigenvalues of\nthe linear map; and (ii) studying the qualitative behavior and stablity of the\nsystem can be performed using tools from dynamical systems theory. Our results\nshow that KVAE outperforms state-of-the-art GAN and VAE methods across several\nchallenging synthetic and real-world time series generation benchmarks. Whether\ntrained on regular or irregular data, KVAE generates time series that improve\nboth discriminative and predictive metrics. We also present visual evidence\nsuggesting that KVAE learns probability density functions that better\napproximate empirical ground truth distributions.",
            "author": [
                "Ilan Naiman",
                "N. Benjamin Erichson",
                "Pu Ren",
                "Michael W. Mahoney",
                "Omri Azencot"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02619v1",
                "http://arxiv.org/pdf/2310.02619v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02609v1",
            "title": "RLTrace: Synthesizing High-Quality System Call Traces for OS Fuzz\n  Testing",
            "updated": "2023-10-04T06:46:00Z",
            "published": "2023-10-04T06:46:00Z",
            "summary": "Securing operating system (OS) kernel is one central challenge in today's\ncyber security landscape. The cutting-edge testing technique of OS kernel is\nsoftware fuzz testing. By mutating the program inputs with random variations\nfor iterations, fuzz testing aims to trigger program crashes and hangs caused\nby potential bugs that can be abused by the inputs. To achieve high OS code\ncoverage, the de facto OS fuzzer typically composes system call traces as the\ninput seed to mutate and to interact with OS kernels. Hence, quality and\ndiversity of the employed system call traces become the prominent factor to\ndecide the effectiveness of OS fuzzing. However, these system call traces to\ndate are generated with hand-coded rules, or by analyzing system call logs of\nOS utility programs. Our observation shows that such system call traces can\nonly subsume common usage scenarios of OS system calls, and likely omit hidden\nbugs.\n  In this research, we propose a deep reinforcement learning-based solution,\ncalled RLTrace, to synthesize diverse and comprehensive system call traces as\nthe seed to fuzz OS kernels. During model training, the deep learning model\ninteracts with OS kernels and infers optimal system call traces w.r.t. our\nlearning goal -- maximizing kernel code coverage. Our evaluation shows that\nRLTrace outperforms other seed generators by producing more comprehensive\nsystem call traces, subsuming system call corner usage cases and subtle\ndependencies. By feeding the de facto OS fuzzer, SYZKALLER, with system call\ntraces synthesized by RLTrace, we show that SYZKALLER can achieve higher code\ncoverage for testing Linux kernels. Furthermore, RLTrace found one\nvulnerability in the Linux kernel (version 5.5-rc6), which is publicly unknown\nto the best of our knowledge by the time of writing.",
            "author": [
                "Wei Chen",
                "Huaijin Wang",
                "Weixi Gu",
                "Shuai Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02609v1",
                "http://arxiv.org/pdf/2310.02609v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02606v1",
            "title": "Learning adjacency matrix for dynamic graph neural network",
            "updated": "2023-10-04T06:42:33Z",
            "published": "2023-10-04T06:42:33Z",
            "summary": "In recent work, [1] introduced the concept of using a Block Adjacency Matrix\n(BA) for the representation of spatio-temporal data. While their method\nsuccessfully concatenated adjacency matrices to encapsulate spatio-temporal\nrelationships in a single graph, it formed a disconnected graph. This\nlimitation hampered the ability of Graph Convolutional Networks (GCNs) to\nperform message passing across nodes belonging to different time steps, as no\ntemporal links were present. To overcome this challenge, we introduce an\nencoder block specifically designed to learn these missing temporal links. The\nencoder block processes the BA and predicts connections between previously\nunconnected subgraphs, resulting in a Spatio-Temporal Block Adjacency Matrix\n(STBAM). This enriched matrix is then fed into a Graph Neural Network (GNN) to\ncapture the complex spatio-temporal topology of the network. Our evaluations on\nbenchmark datasets, surgVisDom and C2D2, demonstrate that our method, with\nslightly higher complexity, achieves superior results compared to\nstate-of-the-art results. Our approach's computational overhead remains\nsignificantly lower than conventional non-graph-based methodologies for\nspatio-temporal data.",
            "author": [
                "Osama Ahmad",
                "Omer Abdul Jalil",
                "Usman Nazir",
                "Murtaza Taj"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02606v1",
                "http://arxiv.org/pdf/2310.02606v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02604v1",
            "title": "On the Last-iterate Convergence in Time-varying Zero-sum Games: Extra\n  Gradient Succeeds where Optimism Fails",
            "updated": "2023-10-04T06:34:36Z",
            "published": "2023-10-04T06:34:36Z",
            "summary": "Last-iterate convergence has received extensive study in two player zero-sum\ngames starting from bilinear, convex-concave up to settings that satisfy the\nMVI condition. Typical methods that exhibit last-iterate convergence for the\naforementioned games include extra-gradient (EG) and optimistic gradient\ndescent ascent (OGDA). However, all the established last-iterate convergence\nresults hold for the restrictive setting where the underlying repeated game\ndoes not change over time. Recently, a line of research has focused on regret\nanalysis of OGDA in time-varying games, i.e., games where payoffs evolve with\ntime; the last-iterate behavior of OGDA and EG in time-varying environments\nremains unclear though. In this paper, we study the last-iterate behavior of\nvarious algorithms in two types of unconstrained, time-varying, bilinear\nzero-sum games: periodic and convergent perturbed games. These models expand\nupon the usual repeated game formulation and incorporate external environmental\nfactors, such as the seasonal effects on species competition and vanishing\nexternal noise. In periodic games, we prove that EG will converge while OGDA\nand momentum method will diverge. This is quite surprising, as to the best of\nour knowledge, it is the first result that indicates EG and OGDA have\nqualitatively different last-iterate behaviors and do not exhibit similar\nbehavior. In convergent perturbed games, we prove all these algorithms converge\nas long as the game itself stabilizes with a faster rate than $1/t$.",
            "author": [
                "Yi Feng",
                "Hu Fu",
                "Qun Hu",
                "Ping Li",
                "Ioannis Panageas",
                "Bo Peng",
                "Xiao Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02604v1",
                "http://arxiv.org/pdf/2310.02604v1"
            ],
            "primary_category": "cs.GT",
            "category": [
                "cs.GT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02603v1",
            "title": "Detecting a late changepoint in the preferential attachment model",
            "updated": "2023-10-04T06:19:58Z",
            "published": "2023-10-04T06:19:58Z",
            "summary": "Motivated by the problem of detecting a change in the evolution of a network,\nwe consider the preferential attachment random graph model with a\ntime-dependent attachment function. Our goal is to detect whether the\nattachment mechanism changed over time, based on a single snapshot of the\nnetwork and without directly observable information about the dynamics. We cast\nthis question as a hypothesis testing problem, where the null hypothesis is a\npreferential attachment model with a constant affine attachment parameter\n$\\delta_0$, and the alternative hypothesis is a preferential attachment model\nwhere the affine attachment parameter changes from $\\delta_0$ to $\\delta_1$ at\nan unknown changepoint time $\\tau_n$. For our analysis we focus on the regime\nwhere $\\delta_0$ and $\\delta_1$ are fixed, and the changepoint occurs close to\nthe observation time of the network (i.e., $\\tau_n = n - c n^\\gamma$ with $c>0$\nand $\\gamma \\in (0, 1)$). This corresponds to the relevant scenario where we\naim to detect the changepoint shortly after it has happened.\n  We present two tests based on the number of vertices with minimal degree, and\nshow that these are asymptotically powerful when $\\tfrac{1}{2}<\\gamma<1$. We\nconjecture that there is no powerful test based on the final network snapshot\nwhen $\\gamma < \\tfrac{1}{2}$. The first test we propose requires knowledge of\n$\\delta_0$. The second test is significantly more involved, and does not\nrequire the knowledge of $\\delta_0$ while still achieving the same performance\nguarantees. Furthermore, we prove that the test statistics for both tests are\nasymptotically normal, allowing for accurate calibration of the tests. This is\ndemonstrated by numerical experiments, that also illustrate the finite sample\ntest properties.",
            "author": [
                "Gianmarco Bet",
                "Kay Bogerd",
                "Rui M. Castro",
                "Remco van der Hofstad"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02603v1",
                "http://arxiv.org/pdf/2310.02603v1"
            ],
            "primary_category": "math.ST",
            "category": [
                "math.ST",
                "math.PR",
                "stat.TH",
                "05C80, 60F05, 62M02, 60C05"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02600v1",
            "title": "Neural Bayes Estimators for Irregular Spatial Data using Graph Neural\n  Networks",
            "updated": "2023-10-04T06:13:22Z",
            "published": "2023-10-04T06:13:22Z",
            "summary": "Neural Bayes estimators are neural networks that approximate Bayes estimators\nin a fast and likelihood-free manner. They are appealing to use with spatial\nmodels and data, where estimation is often a computational bottleneck. However,\nneural Bayes estimators in spatial applications have, to date, been restricted\nto data collected over a regular grid. These estimators are also currently\ndependent on a prescribed set of spatial locations, which means that the neural\nnetwork needs to be re-trained for new data sets; this renders them impractical\nin many applications and impedes their widespread adoption. In this work, we\nemploy graph neural networks to tackle the important problem of parameter\nestimation from data collected over arbitrary spatial locations. In addition to\nextending neural Bayes estimation to irregular spatial data, our architecture\nleads to substantial computational benefits, since the estimator can be used\nwith any arrangement or number of locations and independent replicates, thus\namortising the cost of training for a given spatial model. We also facilitate\nfast uncertainty quantification by training an accompanying neural Bayes\nestimator that approximates a set of marginal posterior quantiles. We\nillustrate our methodology on Gaussian and max-stable processes. Finally, we\nshowcase our methodology in a global sea-surface temperature application, where\nwe estimate the parameters of a Gaussian process model in 2,161 regions, each\ncontaining thousands of irregularly-spaced data points, in just a few minutes\nwith a single graphics processing unit.",
            "author": [
                "Matthew Sainsbury-Dale",
                "Jordan Richards",
                "Andrew Zammit-Mangion",
                "Rapha\u00ebl Huser"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02600v1",
                "http://arxiv.org/pdf/2310.02600v1"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02596v2",
            "title": "SweetDreamer: Aligning Geometric Priors in 2D Diffusion for Consistent\n  Text-to-3D",
            "updated": "2023-10-20T04:02:22Z",
            "published": "2023-10-04T05:59:50Z",
            "summary": "It is inherently ambiguous to lift 2D results from pre-trained diffusion\nmodels to a 3D world for text-to-3D generation. 2D diffusion models solely\nlearn view-agnostic priors and thus lack 3D knowledge during the lifting,\nleading to the multi-view inconsistency problem. We find that this problem\nprimarily stems from geometric inconsistency, and avoiding misplaced geometric\nstructures substantially mitigates the problem in the final outputs. Therefore,\nwe improve the consistency by aligning the 2D geometric priors in diffusion\nmodels with well-defined 3D shapes during the lifting, addressing the vast\nmajority of the problem. This is achieved by fine-tuning the 2D diffusion model\nto be viewpoint-aware and to produce view-specific coordinate maps of\ncanonically oriented 3D objects. In our process, only coarse 3D information is\nused for aligning. This \"coarse\" alignment not only resolves the multi-view\ninconsistency in geometries but also retains the ability in 2D diffusion models\nto generate detailed and diversified high-quality objects unseen in the 3D\ndatasets. Furthermore, our aligned geometric priors (AGP) are generic and can\nbe seamlessly integrated into various state-of-the-art pipelines, obtaining\nhigh generalizability in terms of unseen shapes and visual appearance while\ngreatly alleviating the multi-view inconsistency problem. Our method represents\na new state-of-the-art performance with an 85+% consistency rate by human\nevaluation, while many previous methods are around 30%. Our project page is\nhttps://sweetdreamer3d.github.io/",
            "author": [
                "Weiyu Li",
                "Rui Chen",
                "Xuelin Chen",
                "Ping Tan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02596v2",
                "http://arxiv.org/pdf/2310.02596v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02594v1",
            "title": "I$^2$KD-SLU: An Intra-Inter Knowledge Distillation Framework for\n  Zero-Shot Cross-Lingual Spoken Language Understanding",
            "updated": "2023-10-04T05:45:23Z",
            "published": "2023-10-04T05:45:23Z",
            "summary": "Spoken language understanding (SLU) typically includes two subtasks: intent\ndetection and slot filling. Currently, it has achieved great success in\nhigh-resource languages, but it still remains challenging in low-resource\nlanguages due to the scarcity of labeled training data. Hence, there is a\ngrowing interest in zero-shot cross-lingual SLU. Despite of the success of\nexisting zero-shot cross-lingual SLU models, most of them neglect to achieve\nthe mutual guidance between intent and slots. To address this issue, we propose\nan Intra-Inter Knowledge Distillation framework for zero-shot cross-lingual\nSpoken Language Understanding (I$^2$KD-SLU) to model the mutual guidance.\nSpecifically, we not only apply intra-knowledge distillation between intent\npredictions or slot predictions of the same utterance in different languages,\nbut also apply inter-knowledge distillation between intent predictions and slot\npredictions of the same utterance. Our experimental results demonstrate that\nour proposed framework significantly improves the performance compared with the\nstrong baselines and achieves the new state-of-the-art performance on the\nMultiATIS++ dataset, obtaining a significant improvement over the previous best\nmodel in overall accuracy.",
            "author": [
                "Tianjun Mao",
                "Chenghong Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02594v1",
                "http://arxiv.org/pdf/2310.02594v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02593v1",
            "title": "A ModelOps-based Framework for Intelligent Medical Knowledge Extraction",
            "updated": "2023-10-04T05:35:16Z",
            "published": "2023-10-04T05:35:16Z",
            "summary": "Extracting medical knowledge from healthcare texts enhances downstream tasks\nlike medical knowledge graph construction and clinical decision-making.\nHowever, the construction and application of knowledge extraction models lack\nautomation, reusability and unified management, leading to inefficiencies for\nresearchers and high barriers for non-AI experts such as doctors, to utilize\nknowledge extraction. To address these issues, we propose a ModelOps-based\nintelligent medical knowledge extraction framework that offers a low-code\nsystem for model selection, training, evaluation and optimization.\nSpecifically, the framework includes a dataset abstraction mechanism based on\nmulti-layer callback functions, a reusable model training, monitoring and\nmanagement mechanism. We also propose a model recommendation method based on\ndataset similarity, which helps users quickly find potentially suitable models\nfor a given dataset. Our framework provides convenience for researchers to\ndevelop models and simplifies model access for non-AI experts such as doctors.",
            "author": [
                "Hongxin Ding",
                "Peinie Zou",
                "Zhiyuan Wang",
                "Junfeng Zhao",
                "Yasha Wang",
                "Qiang Zhou"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02593v1",
                "http://arxiv.org/pdf/2310.02593v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02579v1",
            "title": "On the Stability of Expressive Positional Encodings for Graph Neural\n  Networks",
            "updated": "2023-10-04T04:48:55Z",
            "published": "2023-10-04T04:48:55Z",
            "summary": "Designing effective positional encodings for graphs is key to building\npowerful graph transformers and enhancing message-passing graph neural\nnetworks. Although widespread, using Laplacian eigenvectors as positional\nencodings faces two fundamental challenges: (1) \\emph{Non-uniqueness}: there\nare many different eigendecompositions of the same Laplacian, and (2)\n\\emph{Instability}: small perturbations to the Laplacian could result in\ncompletely different eigenspaces, leading to unpredictable changes in\npositional encoding.\n  Despite many attempts to address non-uniqueness, most methods overlook\nstability, leading to poor generalization on unseen graph structures. We\nidentify the cause of instability to be a \"hard partition\" of eigenspaces.\nHence, we introduce Stable and Expressive Positional Encodings (SPE), an\narchitecture for processing eigenvectors that uses eigenvalues to \"softly\npartition\" eigenspaces. SPE is the first architecture that is (1) provably\nstable, and (2) universally expressive for basis invariant functions whilst\nrespecting all symmetries of eigenvectors. Besides guaranteed stability, we\nprove that SPE is at least as expressive as existing methods, and highly\ncapable of counting graph structures. Finally, we evaluate the effectiveness of\nour method on molecular property prediction, and out-of-distribution\ngeneralization tasks, finding improved generalization compared to existing\npositional encoding methods.",
            "author": [
                "Yinan Huang",
                "William Lu",
                "Joshua Robinson",
                "Yu Yang",
                "Muhan Zhang",
                "Stefanie Jegelka",
                "Pan Li"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02579v1",
                "http://arxiv.org/pdf/2310.02579v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02572v1",
            "title": "Improving Knowledge Distillation with Teacher's Explanation",
            "updated": "2023-10-04T04:18:01Z",
            "published": "2023-10-04T04:18:01Z",
            "summary": "Knowledge distillation (KD) improves the performance of a low-complexity\nstudent model with the help of a more powerful teacher. The teacher in KD is a\nblack-box model, imparting knowledge to the student only through its\npredictions. This limits the amount of transferred knowledge. In this work, we\nintroduce a novel Knowledge Explaining Distillation (KED) framework, which\nallows the student to learn not only from the teacher's predictions but also\nfrom the teacher's explanations. We propose a class of superfeature-explaining\nteachers that provide explanation over groups of features, along with the\ncorresponding student model. We also present a method for constructing the\nsuperfeatures. We then extend KED to reduce complexity in convolutional neural\nnetworks, to allow augmentation with hidden-representation distillation\nmethods, and to work with a limited amount of training data using chimeric\nsets. Our experiments over a variety of datasets show that KED students can\nsubstantially outperform KD students of similar complexity.",
            "author": [
                "Sayantan Chowdhury",
                "Ben Liang",
                "Ali Tizghadam",
                "Ilijc Albanese"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02572v1",
                "http://arxiv.org/pdf/2310.02572v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02568v1",
            "title": "Stand for Something or Fall for Everything: Predict Misinformation\n  Spread with Stance-Aware Graph Neural Networks",
            "updated": "2023-10-04T04:02:32Z",
            "published": "2023-10-04T04:02:32Z",
            "summary": "Although pervasive spread of misinformation on social media platforms has\nbecome a pressing challenge, existing platform interventions have shown limited\nsuccess in curbing its dissemination. In this study, we propose a stance-aware\ngraph neural network (stance-aware GNN) that leverages users' stances to\nproactively predict misinformation spread. As different user stances can form\nunique echo chambers, we customize four information passing paths in\nstance-aware GNN, while the trainable attention weights provide explainability\nby highlighting each structure's importance. Evaluated on a real-world dataset,\nstance-aware GNN outperforms benchmarks by 32.65% and exceeds advanced GNNs\nwithout user stance by over 4.69%. Furthermore, the attention weights indicate\nthat users' opposition stances have a higher impact on their neighbors'\nbehaviors than supportive ones, which function as social correction to halt\nmisinformation propagation. Overall, our study provides an effective predictive\nmodel for platforms to combat misinformation, and highlights the impact of user\nstances in the misinformation propagation.",
            "author": [
                "Zihan Chen",
                "Jingyi Sun",
                "Rong Liu",
                "Feng Mai"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02568v1",
                "http://arxiv.org/pdf/2310.02568v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI",
                "cs.AI",
                "cs.CY",
                "cs.LG",
                "H.0; J.4; I.2.7"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02554v3",
            "title": "zkFL: Zero-Knowledge Proof-based Gradient Aggregation for Federated\n  Learning",
            "updated": "2023-10-19T16:21:09Z",
            "published": "2023-10-04T03:24:33Z",
            "summary": "Federated Learning (FL) is a machine learning paradigm, which enables\nmultiple and decentralized clients to collaboratively train a model under the\norchestration of a central aggregator. Traditional FL solutions rely on the\ntrust assumption of the centralized aggregator, which forms cohorts of clients\nin a fair and honest manner. However, a malicious aggregator, in reality, could\nabandon and replace the client's training models, or launch Sybil attacks to\ninsert fake clients. Such malicious behaviors give the aggregator more power to\ncontrol clients in the FL setting and determine the final training results. In\nthis work, we introduce zkFL, which leverages zero-knowledge proofs (ZKPs) to\ntackle the issue of a malicious aggregator during the training model\naggregation process. To guarantee the correct aggregation results, the\naggregator needs to provide a proof per round. The proof can demonstrate to the\nclients that the aggregator executes the intended behavior faithfully. To\nfurther reduce the verification cost of clients, we employ a blockchain to\nhandle the proof in a zero-knowledge way, where miners (i.e., the nodes\nvalidating and maintaining the blockchain data) can verify the proof without\nknowing the clients' local and aggregated models. The theoretical analysis and\nempirical results show that zkFL can achieve better security and privacy than\ntraditional FL, without modifying the underlying FL network structure or\nheavily compromising the training speed.",
            "author": [
                "Zhipeng Wang",
                "Nanqing Dong",
                "Jiahao Sun",
                "William Knottenbelt"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02554v3",
                "http://arxiv.org/pdf/2310.02554v3"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02551v1",
            "title": "Apex Graphs and Cographs",
            "updated": "2023-10-04T03:20:51Z",
            "published": "2023-10-04T03:20:51Z",
            "summary": "A class $\\mathcal{G}$ of graphs is called hereditary if it is closed under\ntaking induced subgraphs. We denote by $\\mathcal{G}^\\mathrm{apex}$ the class of\ngraphs $G$ that contain a vertex $v$ such that $G-v$ is in $\\mathcal{G}$. We\nprove that if a hereditary class $\\mathcal{G}$ has finitely many forbidden\ninduced subgraphs, then so does $\\mathcal{G}^\\mathrm{apex}$.\n  The hereditary class of cographs consists of all graphs $G$ that can be\ngenerated from $K_1$ using complementation and disjoint union. A graph is an\napex cograph if it contains a vertex whose deletion results in a cograph.\nCographs are precisely the graphs that do not have the $4$-vertex path as an\ninduced subgraph. Our main result finds all such forbidden induced subgraphs\nfor the class of apex cographs.",
            "author": [
                "Jagdeep Singh",
                "Vaidy Sivaraman",
                "Thomas Zaslavsky"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02551v1",
                "http://arxiv.org/pdf/2310.02551v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C75"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02549v1",
            "title": "Heterogeneous Federated Learning Using Knowledge Codistillation",
            "updated": "2023-10-04T03:17:26Z",
            "published": "2023-10-04T03:17:26Z",
            "summary": "Federated Averaging, and many federated learning algorithm variants which\nbuild upon it, have a limitation: all clients must share the same model\narchitecture. This results in unused modeling capacity on many clients, which\nlimits model performance. To address this issue, we propose a method that\ninvolves training a small model on the entire pool and a larger model on a\nsubset of clients with higher capacity. The models exchange information\nbidirectionally via knowledge distillation, utilizing an unlabeled dataset on a\nserver without sharing parameters. We present two variants of our method, which\nimprove upon federated averaging on image classification and language modeling\ntasks. We show this technique can be useful even if only out-of-domain or\nlimited in-domain distillation data is available. Additionally, the\nbi-directional knowledge distillation allows for domain transfer between the\nmodels when different pool populations introduce domain shift.",
            "author": [
                "Jared Lichtarge",
                "Ehsan Amid",
                "Shankar Kumar",
                "Tien-Ju Yang",
                "Rohan Anil",
                "Rajiv Mathews"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02549v1",
                "http://arxiv.org/pdf/2310.02549v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02543v1",
            "title": "Provable Tensor Completion with Graph Information",
            "updated": "2023-10-04T02:55:10Z",
            "published": "2023-10-04T02:55:10Z",
            "summary": "Graphs, depicting the interrelations between variables, has been widely used\nas effective side information for accurate data recovery in various\nmatrix/tensor recovery related applications. In this paper, we study the tensor\ncompletion problem with graph information. Current research on\ngraph-regularized tensor completion tends to be task-specific, lacking\ngenerality and systematic approaches. Furthermore, a recovery theory to ensure\nperformance remains absent. Moreover, these approaches overlook the dynamic\naspects of graphs, treating them as static akin to matrices, even though graphs\ncould exhibit dynamism in tensor-related scenarios. To confront these\nchallenges, we introduce a pioneering framework in this paper that\nsystematically formulates a novel model, theory, and algorithm for solving the\ndynamic graph regularized tensor completion problem. For the model, we\nestablish a rigorous mathematical representation of the dynamic graph, based on\nwhich we derive a new tensor-oriented graph smoothness regularization. By\nintegrating this regularization into a tensor decomposition model based on\ntransformed t-SVD, we develop a comprehensive model simultaneously capturing\nthe low-rank and similarity structure of the tensor. In terms of theory, we\nshowcase the alignment between the proposed graph smoothness regularization and\na weighted tensor nuclear norm. Subsequently, we establish assurances of\nstatistical consistency for our model, effectively bridging a gap in the\ntheoretical examination of the problem involving tensor recovery with graph\ninformation. In terms of the algorithm, we develop a solution of high\neffectiveness, accompanied by a guaranteed convergence, to address the\nresulting model. To showcase the prowess of our proposed model in contrast to\nestablished ones, we provide in-depth numerical experiments encompassing\nsynthetic data as well as real-world datasets.",
            "author": [
                "Kaidong Wang",
                "Yao Wang",
                "Xiuwu Liao",
                "Shaojie Tang",
                "Can Yang",
                "Deyu Meng"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02543v1",
                "http://arxiv.org/pdf/2310.02543v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02542v1",
            "title": "Tightly Joining Positioning and Control for Trustworthy Unmanned Aerial\n  Vehicles Based on Factor Graph Optimization in Urban Transportation",
            "updated": "2023-10-04T02:53:10Z",
            "published": "2023-10-04T02:53:10Z",
            "summary": "Unmanned aerial vehicles (UAV) showed great potential in improving the\nefficiency of parcel delivery applications in the coming smart cities era.\nUnfortunately, the trustworthy positioning and control algorithms of the UAV\nare significantly challenged in complex urban areas. For example, the\nubiquitous global navigation satellite system (GNSS) positioning can be\ndegraded by the signal reflections from surrounding high-rising buildings,\nleading to significantly increased positioning uncertainty. An additional\nchallenge is introduced to the control algorithm due to the complex wind\ndisturbances in urban canyons. Given the fact that the system positioning and\ncontrol are highly correlated with each other, for example, the system dynamics\nof the control can largely help with the positioning, this paper proposed a\njoint positioning and control method (JPCM) based on factor graph optimization\n(FGO), which combines sensors' measurements and control intention. In\nparticular, the positioning measurements are formulated as the factors in the\nfactor graph model, such as the positioning from the GNSS. The model predictive\ncontrol (MPC) is also formulated as the additional factors in the factor graph\nmodel. By solving the factor graph contributed by both the positioning factor\nand the MPC-based factors, the complementariness of positioning and control can\nbe fully explored. To guarantee reliable system dynamic parameters, we validate\nthe effectiveness of the proposed method using a simulated quadrotor system\nwhich showed significantly improved trajectory following performance. To\nbenefit the research community, we open-source our code and make it available\nat https://github.com/RoboticsPolyu/IPN_MPC.",
            "author": [
                "Peiwen Yang",
                "Weisong Wen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02542v1",
                "http://arxiv.org/pdf/2310.02542v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02540v1",
            "title": "Auto-FP: An Experimental Study of Automated Feature Preprocessing for\n  Tabular Data",
            "updated": "2023-10-04T02:46:44Z",
            "published": "2023-10-04T02:46:44Z",
            "summary": "Classical machine learning models, such as linear models and tree-based\nmodels, are widely used in industry. These models are sensitive to data\ndistribution, thus feature preprocessing, which transforms features from one\ndistribution to another, is a crucial step to ensure good model quality.\nManually constructing a feature preprocessing pipeline is challenging because\ndata scientists need to make difficult decisions about which preprocessors to\nselect and in which order to compose them. In this paper, we study how to\nautomate feature preprocessing (Auto-FP) for tabular data. Due to the large\nsearch space, a brute-force solution is prohibitively expensive. To address\nthis challenge, we interestingly observe that Auto-FP can be modelled as either\na hyperparameter optimization (HPO) or a neural architecture search (NAS)\nproblem. This observation enables us to extend a variety of HPO and NAS\nalgorithms to solve the Auto-FP problem. We conduct a comprehensive evaluation\nand analysis of 15 algorithms on 45 public ML datasets. Overall,\nevolution-based algorithms show the leading average ranking. Surprisingly, the\nrandom search turns out to be a strong baseline. Many surrogate-model-based and\nbandit-based search algorithms, which achieve good performance for HPO and NAS,\ndo not outperform random search for Auto-FP. We analyze the reasons for our\nfindings and conduct a bottleneck analysis to identify the opportunities to\nimprove these algorithms. Furthermore, we explore how to extend Auto-FP to\nsupport parameter search and compare two ways to achieve this goal. In the end,\nwe evaluate Auto-FP in an AutoML context and discuss the limitations of popular\nAutoML tools. To the best of our knowledge, this is the first study on\nautomated feature preprocessing. We hope our work can inspire researchers to\ndevelop new algorithms tailored for Auto-FP.",
            "author": [
                "Danrui Qi",
                "Jinglin Peng",
                "Yongjun He",
                "Jiannan Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02540v1",
                "http://arxiv.org/pdf/2310.02540v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.DB",
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02538v1",
            "title": "Nash Equilibrium Seeking in Networked Games with Intermittent\n  Communication",
            "updated": "2023-10-04T02:45:04Z",
            "published": "2023-10-04T02:45:04Z",
            "summary": "This paper investigates the Nash equilibrium seeking problems for networked\ngames with intermittent communication, where each player is capable of\ncommunicating with other players intermittently over a strongly connected and\ndirected graph. Noticing that the players are not directly and continuously\navailable for the actions of other players, this paper proposed an intermittent\ncommunication strategy. Compared with previous literature on intermittent\ncommunication, the players considered in this paper communicate with other\nplayers without quasi-periodic constraint. Instead, the players are supposed to\nestimate the actions of the other players with completely aperiodically\nintermittent communication. The distributions of communication time and silent\ntime are characterized newly according to the concept of average communication\nratio. And each player estimates other players' actions only during\ncommunication time. Finally, the validity of the theoretical results is\ndemonstrated by two simulation examples.",
            "author": [
                "Ying Zhai",
                "Rui Yuan",
                "Huan Su"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02538v1",
                "http://arxiv.org/pdf/2310.02538v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03766v1",
            "title": "Literature Based Discovery (LBD): Towards Hypothesis Generation and\n  Knowledge Discovery in Biomedical Text Mining",
            "updated": "2023-10-04T02:13:11Z",
            "published": "2023-10-04T02:13:11Z",
            "summary": "Biomedical knowledge is growing in an astounding pace with a majority of this\nknowledge is represented as scientific publications. Text mining tools and\nmethods represents automatic approaches for extracting hidden patterns and\ntrends from this semi structured and unstructured data. In Biomedical Text\nmining, Literature Based Discovery (LBD) is the process of automatically\ndiscovering novel associations between medical terms otherwise mentioned in\ndisjoint literature sets. LBD approaches proven to be successfully reducing the\ndiscovery time of potential associations that are hidden in the vast amount of\nscientific literature. The process focuses on creating concept profiles for\nmedical terms such as a disease or symptom and connecting it with a drug and\ntreatment based on the statistical significance of the shared profiles. This\nknowledge discovery approach introduced in 1989 still remains as a core task in\ntext mining. Currently the ABC principle based two approaches namely open\ndiscovery and closed discovery are mostly explored in LBD process. This review\nstarts with general introduction about text mining followed by biomedical text\nmining and introduces various literature resources such as MEDLINE, UMLS, MESH,\nand SemMedDB. This is followed by brief introduction of the core ABC principle\nand its associated two approaches open discovery and closed discovery in LBD\nprocess. This review also discusses the deep learning applications in LBD by\nreviewing the role of transformer models and neural networks based LBD models\nand its future aspects. Finally, reviews the key biomedical discoveries\ngenerated through LBD approaches in biomedicine and conclude with the current\nlimitations and future directions of LBD.",
            "author": [
                "Balu Bhasuran",
                "Gurusamy Murugesan",
                "Jeyakumar Natarajan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03766v1",
                "http://arxiv.org/pdf/2310.03766v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02521v1",
            "title": "Who Audits the Auditors? Recommendations from a field scan of the\n  algorithmic auditing ecosystem",
            "updated": "2023-10-04T01:40:03Z",
            "published": "2023-10-04T01:40:03Z",
            "summary": "AI audits are an increasingly popular mechanism for algorithmic\naccountability; however, they remain poorly defined. Without a clear\nunderstanding of audit practices, let alone widely used standards or regulatory\nguidance, claims that an AI product or system has been audited, whether by\nfirst-, second-, or third-party auditors, are difficult to verify and may\nexacerbate, rather than mitigate, bias and harm. To address this knowledge gap,\nwe provide the first comprehensive field scan of the AI audit ecosystem. We\nshare a catalog of individuals (N=438) and organizations (N=189) who engage in\nalgorithmic audits or whose work is directly relevant to algorithmic audits;\nconduct an anonymous survey of the group (N=152); and interview industry\nleaders (N=10). We identify emerging best practices as well as methods and\ntools that are becoming commonplace, and enumerate common barriers to\nleveraging algorithmic audits as effective accountability mechanisms. We\noutline policy recommendations to improve the quality and impact of these\naudits, and highlight proposals with wide support from algorithmic auditors as\nwell as areas of debate. Our recommendations have implications for lawmakers,\nregulators, internal company policymakers, and standards-setting bodies, as\nwell as for auditors. They are: 1) require the owners and operators of AI\nsystems to engage in independent algorithmic audits against clearly defined\nstandards; 2) notify individuals when they are subject to algorithmic\ndecision-making systems; 3) mandate disclosure of key components of audit\nfindings for peer review; 4) consider real-world harm in the audit process,\nincluding through standardized harm incident reporting and response mechanisms;\n5) directly involve the stakeholders most likely to be harmed by AI systems in\nthe algorithmic audit process; and 6) formalize evaluation and, potentially,\naccreditation of algorithmic auditors.",
            "author": [
                "Sasha Costanza-Chock",
                "Emma Harvey",
                "Inioluwa Deborah Raji",
                "Martha Czernuszenko",
                "Joy Buolamwini"
            ],
            "link": [
                "http://dx.doi.org/10.1145/3531146.3533213",
                "http://arxiv.org/abs/2310.02521v1",
                "http://arxiv.org/pdf/2310.02521v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02517v2",
            "title": "The unresolved stochastic background from compact binary mergers\n  detectable by next-generation ground-based gravitational-wave observatories",
            "updated": "2023-10-12T19:21:36Z",
            "published": "2023-10-04T01:32:31Z",
            "summary": "The next generation of ground-based gravitational-wave detectors will look\nmuch deeper into the Universe and have unprecedented sensitivities and\nlow-frequency capabilities. Especially alluring is the possibility of detecting\nan early-Universe cosmological stochastic background that could provide\nimportant insights into the beginnings of our Universe and fundamental physics\nat extremely high energies. However, even if next-generation detectors are\nsensitive to cosmological stochastic backgrounds, they will be masked by more\ndominant astrophysical backgrounds, namely the residual background from the\nimperfect subtraction of resolvable compact binary coalescences (CBCs) as well\nas the CBC background from individually unresolvable CBCs. Using our latest\nknowledge of masses, rates, and delay time distributions, we present a\ndata-driven estimate of the unresolvable CBC background that will be seen by\nnext-generation detectors. Accounting for statistical and systematic errors,\nthis estimate quantifies an important piece in the CBC noise budget for\nnext-generation detectors and can help inform detector design and subtraction\nalgorithms. We compare our results with predictions for backgrounds from\nseveral cosmological sources in the literature, finding that the unresolvable\nbackground will likely be a significant impediment for many models. This\nmotivates the need for simultaneous inference methods or other statistical\ntechniques to detect early-Universe cosmological backgrounds.",
            "author": [
                "Darsan S. Bellie",
                "Sharan Banagiri",
                "Zoheyr Doctor",
                "Vicky Kalogera"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02517v2",
                "http://arxiv.org/pdf/2310.02517v2"
            ],
            "primary_category": "gr-qc",
            "category": [
                "gr-qc",
                "astro-ph.HE",
                "hep-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02508v1",
            "title": "Ophiuchus: Scalable Modeling of Protein Structures through Hierarchical\n  Coarse-graining SO(3)-Equivariant Autoencoders",
            "updated": "2023-10-04T01:01:11Z",
            "published": "2023-10-04T01:01:11Z",
            "summary": "Three-dimensional native states of natural proteins display recurring and\nhierarchical patterns. Yet, traditional graph-based modeling of protein\nstructures is often limited to operate within a single fine-grained resolution,\nand lacks hourglass neural architectures to learn those high-level building\nblocks. We narrow this gap by introducing Ophiuchus, an SO(3)-equivariant\ncoarse-graining model that efficiently operates on all heavy atoms of standard\nprotein residues, while respecting their relevant symmetries. Our model departs\nfrom current approaches that employ graph modeling, instead focusing on local\nconvolutional coarsening to model sequence-motif interactions in log-linear\nlength complexity. We train Ophiuchus on contiguous fragments of PDB monomers,\ninvestigating its reconstruction capabilities across different compression\nrates. We examine the learned latent space and demonstrate its prompt usage in\nconformational interpolation, comparing interpolated trajectories to structure\nsnapshots from the PDBFlex dataset. Finally, we leverage denoising diffusion\nprobabilistic models (DDPM) to efficiently sample readily-decodable latent\nembeddings of diverse miniproteins. Our experiments demonstrate Ophiuchus to be\na scalable basis for efficient protein modeling and generation.",
            "author": [
                "Allan dos Santos Costa",
                "Ilan Mitnikov",
                "Mario Geiger",
                "Manvitha Ponnapati",
                "Tess Smidt",
                "Joseph Jacobson"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02508v1",
                "http://arxiv.org/pdf/2310.02508v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02506v1",
            "title": "Proactive Human-Robot Interaction using Visuo-Lingual Transformers",
            "updated": "2023-10-04T00:50:21Z",
            "published": "2023-10-04T00:50:21Z",
            "summary": "Humans possess the innate ability to extract latent visuo-lingual cues to\ninfer context through human interaction. During collaboration, this enables\nproactive prediction of the underlying intention of a series of tasks. In\ncontrast, robotic agents collaborating with humans naively follow elementary\ninstructions to complete tasks or use specific hand-crafted triggers to\ninitiate proactive collaboration when working towards the completion of a goal.\nEndowing such robots with the ability to reason about the end goal and\nproactively suggest intermediate tasks will engender a much more intuitive\nmethod for human-robot collaboration. To this end, we propose a learning-based\nmethod that uses visual cues from the scene, lingual commands from a user and\nknowledge of prior object-object interaction to identify and proactively\npredict the underlying goal the user intends to achieve. Specifically, we\npropose ViLing-MMT, a vision-language multimodal transformer-based architecture\nthat captures inter and intra-modal dependencies to provide accurate scene\ndescriptions and proactively suggest tasks where applicable. We evaluate our\nproposed model in simulation and real-world scenarios.",
            "author": [
                "Pranay Mathur"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02506v1",
                "http://arxiv.org/pdf/2310.02506v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02469v1",
            "title": "Large Language Models Can Be Good Privacy Protection Learners",
            "updated": "2023-10-03T22:37:01Z",
            "published": "2023-10-03T22:37:01Z",
            "summary": "The proliferation of Large Language Models (LLMs) has driven considerable\ninterest in fine-tuning them with domain-specific data to create specialized\nlanguage models. Nevertheless, such domain-specific fine-tuning data often\ncontains sensitive personally identifiable information (PII). Direct\nfine-tuning LLMs on this data without privacy protection poses a risk of\nleakage. To address this challenge, we introduce Privacy Protection Language\nModels (PPLM), a novel paradigm for fine-tuning LLMs that effectively injects\ndomain-specific knowledge while safeguarding data privacy. Our work offers a\ntheoretical analysis for model design and delves into various techniques such\nas corpus curation, penalty-based unlikelihood in training loss, and\ninstruction-based tuning, etc. Extensive experiments across diverse datasets\nand scenarios demonstrate the effectiveness of our approaches. In particular,\ninstruction tuning with both positive and negative examples, stands out as a\npromising method, effectively protecting private data while enhancing the\nmodel's knowledge. Our work underscores the potential for Large Language Models\nas robust privacy protection learners.",
            "author": [
                "Yijia Xiao",
                "Yiqiao Jin",
                "Yushi Bai",
                "Yue Wu",
                "Xianjun Yang",
                "Xiao Luo",
                "Wenchao Yu",
                "Xujiang Zhao",
                "Yanchi Liu",
                "Haifeng Chen",
                "Wei Wang",
                "Wei Cheng"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02469v1",
                "http://arxiv.org/pdf/2310.02469v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03046v1",
            "title": "EcoAssistant: Using LLM Assistant More Affordably and Accurately",
            "updated": "2023-10-03T22:16:13Z",
            "published": "2023-10-03T22:16:13Z",
            "summary": "Today, users ask Large language models (LLMs) as assistants to answer queries\nthat require external knowledge; they ask about the weather in a specific city,\nabout stock prices, and even about where specific locations are within their\nneighborhood. These queries require the LLM to produce code that invokes\nexternal APIs to answer the user's question, yet LLMs rarely produce correct\ncode on the first try, requiring iterative code refinement upon execution\nresults. In addition, using LLM assistants to support high query volumes can be\nexpensive. In this work, we contribute a framework, EcoAssistant, that enables\nLLMs to answer code-driven queries more affordably and accurately. EcoAssistant\ncontains three components. First, it allows the LLM assistants to converse with\nan automatic code executor to iteratively refine code or to produce answers\nbased on the execution results. Second, we use a hierarchy of LLM assistants,\nwhich attempts to answer the query with weaker, cheaper LLMs before backing off\nto stronger, expensive ones. Third, we retrieve solutions from past successful\nqueries as in-context demonstrations to help subsequent queries. Empirically,\nwe show that EcoAssistant offers distinct advantages for affordability and\naccuracy, surpassing GPT-4 by 10 points of success rate with less than 50% of\nGPT-4's cost.",
            "author": [
                "Jieyu Zhang",
                "Ranjay Krishna",
                "Ahmed H. Awadallah",
                "Chi Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03046v1",
                "http://arxiv.org/pdf/2310.03046v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02459v1",
            "title": "Distributionally Safe Reinforcement Learning under Model Uncertainty: A\n  Single-Level Approach by Differentiable Convex Programming",
            "updated": "2023-10-03T22:05:05Z",
            "published": "2023-10-03T22:05:05Z",
            "summary": "Safety assurance is uncompromisable for safety-critical environments with the\npresence of drastic model uncertainties (e.g., distributional shift),\nespecially with humans in the loop. However, incorporating uncertainty in safe\nlearning will naturally lead to a bi-level problem, where at the lower level\nthe (worst-case) safety constraint is evaluated within the uncertainty\nambiguity set. In this paper, we present a tractable distributionally safe\nreinforcement learning framework to enforce safety under a distributional shift\nmeasured by a Wasserstein metric. To improve the tractability, we first use\nduality theory to transform the lower-level optimization from\ninfinite-dimensional probability space where distributional shift is measured,\nto a finite-dimensional parametric space. Moreover, by differentiable convex\nprogramming, the bi-level safe learning problem is further reduced to a\nsingle-level one with two sequential computationally efficient modules: a\nconvex quadratic program to guarantee safety followed by a projected gradient\nascent to simultaneously find the worst-case uncertainty. This end-to-end\ndifferentiable framework with safety constraints, to the best of our knowledge,\nis the first tractable single-level solution to address distributional safety.\nWe test our approach on first and second-order systems with varying\ncomplexities and compare our results with the uncertainty-agnostic policies,\nwhere our approach demonstrates a significant improvement on safety guarantees.",
            "author": [
                "Alaa Eddine Chriat",
                "Chuangchuang Sun"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02459v1",
                "http://arxiv.org/pdf/2310.02459v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.RO",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02439v1",
            "title": "Novice Learner and Expert Tutor: Evaluating Math Reasoning Abilities of\n  Large Language Models with Misconceptions",
            "updated": "2023-10-03T21:19:50Z",
            "published": "2023-10-03T21:19:50Z",
            "summary": "We propose novel evaluations for mathematical reasoning capabilities of Large\nLanguage Models (LLMs) based on mathematical misconceptions. Our primary\napproach is to simulate LLMs as a novice learner and an expert tutor, aiming to\nidentify the incorrect answer to math question resulted from a specific\nmisconception and to recognize the misconception(s) behind an incorrect answer,\nrespectively. Contrary to traditional LLMs-based mathematical evaluations that\nfocus on answering math questions correctly, our approach takes inspirations\nfrom principles in educational learning sciences. We explicitly ask LLMs to\nmimic a novice learner by answering questions in a specific incorrect manner\nbased on incomplete knowledge; and to mimic an expert tutor by identifying\nmisconception(s) corresponding to an incorrect answer to a question. Using\nsimple grade-school math problems, our experiments reveal that, while LLMs can\neasily answer these questions correctly, they struggle to identify 1) the\nincorrect answer corresponding to specific incomplete knowledge\n(misconceptions); 2) the misconceptions that explain particular incorrect\nanswers. Our study indicates new opportunities for enhancing LLMs' math\nreasoning capabilities, especially on developing robust student simulation and\nexpert tutoring models in the educational applications such as intelligent\ntutoring systems.",
            "author": [
                "Naiming Liu",
                "Shashank Sonkar",
                "Zichao Wang",
                "Simon Woodhead",
                "Richard G. Baraniuk"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02439v1",
                "http://arxiv.org/pdf/2310.02439v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02428v2",
            "title": "EGraFFBench: Evaluation of Equivariant Graph Neural Network Force Fields\n  for Atomistic Simulations",
            "updated": "2023-11-24T17:26:56Z",
            "published": "2023-10-03T20:49:00Z",
            "summary": "Equivariant graph neural networks force fields (EGraFFs) have shown great\npromise in modelling complex interactions in atomic systems by exploiting the\ngraphs' inherent symmetries. Recent works have led to a surge in the\ndevelopment of novel architectures that incorporate equivariance-based\ninductive biases alongside architectural innovations like graph transformers\nand message passing to model atomic interactions. However, thorough evaluations\nof these deploying EGraFFs for the downstream task of real-world atomistic\nsimulations, is lacking. To this end, here we perform a systematic benchmarking\nof 6 EGraFF algorithms (NequIP, Allegro, BOTNet, MACE, Equiformer, TorchMDNet),\nwith the aim of understanding their capabilities and limitations for realistic\natomistic simulations. In addition to our thorough evaluation and analysis on\neight existing datasets based on the benchmarking literature, we release two\nnew benchmark datasets, propose four new metrics, and three challenging tasks.\nThe new datasets and tasks evaluate the performance of EGraFF to\nout-of-distribution data, in terms of different crystal structures,\ntemperatures, and new molecules. Interestingly, evaluation of the EGraFF models\nbased on dynamic simulations reveals that having a lower error on energy or\nforce does not guarantee stable or reliable simulation or faithful replication\nof the atomic structures. Moreover, we find that no model clearly outperforms\nother models on all datasets and tasks. Importantly, we show that the\nperformance of all the models on out-of-distribution datasets is unreliable,\npointing to the need for the development of a foundation model for force fields\nthat can be used in real-world simulations. In summary, this work establishes a\nrigorous framework for evaluating machine learning force fields in the context\nof atomic simulations and points to open research challenges within this\ndomain.",
            "author": [
                "Vaibhav Bihani",
                "Utkarsh Pratiush",
                "Sajid Mannan",
                "Tao Du",
                "Zhimin Chen",
                "Santiago Miret",
                "Matthieu Micoulaut",
                "Morten M Smedskjaer",
                "Sayan Ranu",
                "N M Anoop Krishnan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02428v2",
                "http://arxiv.org/pdf/2310.02428v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cond-mat.mtrl-sci"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02424v2",
            "title": "AXNav: Replaying Accessibility Tests from Natural Language",
            "updated": "2023-10-13T18:05:06Z",
            "published": "2023-10-03T20:37:58Z",
            "summary": "Developers and quality assurance testers often rely on manual testing to test\naccessibility features throughout the product lifecycle. Unfortunately, manual\ntesting can be tedious, often has an overwhelming scope, and can be difficult\nto schedule amongst other development milestones. Recently, Large Language\nModels (LLMs) have been used for a variety of tasks including automation of\nUIs, however to our knowledge no one has yet explored their use in controlling\nassistive technologies for the purposes of supporting accessibility testing. In\nthis paper, we explore the requirements of a natural language based\naccessibility testing workflow, starting with a formative study. From this we\nbuild a system that takes as input a manual accessibility test (e.g., ``Search\nfor a show in VoiceOver'') and uses an LLM combined with pixel-based UI\nUnderstanding models to execute the test and produce a chaptered, navigable\nvideo. In each video, to help QA testers we apply heuristics to detect and flag\naccessibility issues (e.g., Text size not increasing with Large Text enabled,\nVoiceOver navigation loops). We evaluate this system through a 10 participant\nuser study with accessibility QA professionals who indicated that the tool\nwould be very useful in their current work and performed tests similarly to how\nthey would manually test the features. The study also reveals insights for\nfuture work on using LLMs for accessibility testing.",
            "author": [
                "Maryam Taeb",
                "Amanda Swearngin",
                "Eldon Schoop",
                "Ruijia Cheng",
                "Yue Jiang",
                "Jeffrey Nichols"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02424v2",
                "http://arxiv.org/pdf/2310.02424v2"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02421v1",
            "title": "Can a student Large Language Model perform as well as it's teacher?",
            "updated": "2023-10-03T20:34:59Z",
            "published": "2023-10-03T20:34:59Z",
            "summary": "The burgeoning complexity of contemporary deep learning models, while\nachieving unparalleled accuracy, has inadvertently introduced deployment\nchallenges in resource-constrained environments. Knowledge distillation, a\ntechnique aiming to transfer knowledge from a high-capacity \"teacher\" model to\na streamlined \"student\" model, emerges as a promising solution to this dilemma.\nThis paper provides a comprehensive overview of the knowledge distillation\nparadigm, emphasizing its foundational principles such as the utility of soft\nlabels and the significance of temperature scaling. Through meticulous\nexamination, we elucidate the critical determinants of successful distillation,\nincluding the architecture of the student model, the caliber of the teacher,\nand the delicate balance of hyperparameters. While acknowledging its profound\nadvantages, we also delve into the complexities and challenges inherent in the\nprocess. Our exploration underscores knowledge distillation's potential as a\npivotal technique in optimizing the trade-off between model performance and\ndeployment efficiency.",
            "author": [
                "Sia Gholami",
                "Marwan Omar"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02421v1",
                "http://arxiv.org/pdf/2310.02421v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02416v2",
            "title": "Bag of Tricks for Fully Test-Time Adaptation",
            "updated": "2023-11-09T15:00:37Z",
            "published": "2023-10-03T20:28:09Z",
            "summary": "Fully Test-Time Adaptation (TTA), which aims at adapting models to data\ndrifts, has recently attracted wide interest. Numerous tricks and techniques\nhave been proposed to ensure robust learning on arbitrary streams of unlabeled\ndata. However, assessing the true impact of each individual technique and\nobtaining a fair comparison still constitutes a significant challenge. To help\nconsolidate the community's knowledge, we present a categorization of selected\northogonal TTA techniques, including small batch normalization, stream\nrebalancing, reliable sample selection, and network confidence calibration. We\nmeticulously dissect the effect of each approach on different scenarios of\ninterest. Through our analysis, we shed light on trade-offs induced by those\ntechniques between accuracy, the computational power required, and model\ncomplexity. We also uncover the synergy that arises when combining techniques\nand are able to establish new state-of-the-art results.",
            "author": [
                "Saypraseuth Mounsaveng",
                "Florent Chiaroni",
                "Malik Boudiaf",
                "Marco Pedersoli",
                "Ismail Ben Ayed"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02416v2",
                "http://arxiv.org/pdf/2310.02416v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02408v1",
            "title": "MindTheDApp: A Toolchain for Complex Network-Driven Structural Analysis\n  of Ethereum-based Decentralised Applications",
            "updated": "2023-10-03T20:03:08Z",
            "published": "2023-10-03T20:03:08Z",
            "summary": "This paper presents MindTheDApp, a toolchain designed specifically for the\nstructural analysis of Ethereum-based Decentralized Applications (DApps), with\na distinct focus on a complex network-driven approach. Unlike existing tools,\nour toolchain combines the power of ANTLR4 and Abstract Syntax Tree (AST)\ntraversal techniques to transform the architecture and interactions within\nsmart contracts into a specialized bipartite graph. This enables advanced\nnetwork analytics to highlight operational efficiencies within the DApp's\narchitecture.\n  The bipartite graph generated by the proposed tool comprises two sets of\nnodes: one representing smart contracts, interfaces, and libraries, and the\nother including functions, events, and modifiers. Edges in the graph connect\nfunctions to smart contracts they interact with, offering a granular view of\ninterdependencies and execution flow within the DApp. This network-centric\napproach allows researchers and practitioners to apply complex network theory\nin understanding the robustness, adaptability, and intricacies of decentralized\nsystems.\n  Our work contributes to the enhancement of security in smart contracts by\nallowing the visualisation of the network, and it provides a deep understanding\nof the architecture and operational logic within DApps. Given the growing\nimportance of smart contracts in the blockchain ecosystem and the emerging\napplication of complex network theory in technology, our toolchain offers a\ntimely contribution to both academic research and practical applications in the\nfield of blockchain technology.",
            "author": [
                "Giacomo Ibba",
                "Sabrina Aufiero",
                "Silvia Bartolucci",
                "Rumyana Neykova",
                "Marco Ortu",
                "Roberto Tonelli",
                "Giuseppe Destefanis"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02408v1",
                "http://arxiv.org/pdf/2310.02408v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "cs.CL",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02394v1",
            "title": "Estimating systemic importance with missing data in input-output graphs",
            "updated": "2023-10-03T19:28:29Z",
            "published": "2023-10-03T19:28:29Z",
            "summary": "In the context of the Cobb-Douglas productivity model we consider the $N\n\\times N$ input-output linkage matrix $W$ for a network of $N$ firms $f_1, f_2,\n\\cdots, f_N$. The associated influence vector $v_w$ of $W$ is defined in terms\nof the Leontief inverse $L_W$ of $W$ as $v_W = \\frac{\\alpha}{N} L_W\n\\vec{\\mathbf{1}}$ where $L_W = (I - (1-\\alpha) W')^{-1}$, $W'$ denotes the\ntranspose of $W$ and $I$ is the identity matrix. Here $\\vec{\\mathbf{1}}$ is the\n$N \\times 1$ vector whose entries are all one. The influence vector is a metric\nof the importance for the firms in the production network. Under the realistic\nassumption that the data to compute the influence vector is incomplete, we\nprove bounds on the worst-case error for the influence vector that are sharp up\nto a constant factor. We also consider the situation where the missing data is\nbinomially distributed and contextualize the bound on the influence vector\naccordingly. We also investigate how far off the influence vector can be when\nwe only have data on nodes and connections that are within distance $k$ of some\nsource node. A comparison of our results is juxtaposed against PageRank\nanalogues. We close with a discussion on a possible extension beyond\nCobb-Douglas to the Constant Elasticity of Substitution model, as well as the\npossibility of considering other probability distributions for missing data.",
            "author": [
                "Jesse Geneson",
                "Alvin Moon",
                "Nicolas Robles",
                "Aaron Strong",
                "Jonathan Welburn"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02394v1",
                "http://arxiv.org/pdf/2310.02394v1"
            ],
            "primary_category": "cs.DM",
            "category": [
                "cs.DM",
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02389v1",
            "title": "Impact of vector mesons polarization on its interaction with matter",
            "updated": "2023-10-03T19:16:19Z",
            "published": "2023-10-03T19:16:19Z",
            "summary": "The production of unstable particles on different nuclei provides the\npossibility to determine the total cross section of the interaction of vector\nmesons $V=\\rho,\\omega,\\varphi, K^{*0}(892),J/\\psi$ etc. with nucleons. This\ninteraction is defined by a set of amplitudes that correspond to the transverse\n(helicity $\\lambda=\\pm 1$) or longitudinal ($\\lambda=0$) polarization of the\nvector meson. The total cross section for the interaction of transversely\npolarized vector mesons with nucleon $\\sigma_T=\\sigma(V_T N)$ has been\nextracted from the coherent photoproduction of vector mesons off nuclei, while\nthe vector meson production in charge exchange reactions as\n$\\pi^\\pm(K^\\pm)+A\\to V^0(K^{*0})+A'$ provides the unique opportunity to obtain\nthe not yet measured total cross section for longitudinally polarized vector\nmeson interacting with nucleon $\\sigma_L=\\sigma(V_L N)$. We shortly discuss the\nimportance of the knowledge of $\\sigma_L$ and possibility to extract its value\nfrom experiments on nuclear target.",
            "author": [
                "S. R. Gevorkyan",
                "A. V. Guskov"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02389v1",
                "http://arxiv.org/pdf/2310.02389v1"
            ],
            "primary_category": "hep-ph",
            "category": [
                "hep-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02383v1",
            "title": "Automatic Multi-Path Web Story Creation from a Structural Article",
            "updated": "2023-10-03T19:08:22Z",
            "published": "2023-10-03T19:08:22Z",
            "summary": "Web articles such as Wikipedia serve as one of the major sources of knowledge\ndissemination and online learning. However, their in-depth information--often\nin a dense text format--may not be suitable for mobile browsing, even in a\nresponsive UI. We propose an automatic approach that converts a structural\narticle of any length into a set of interactive Web Stories that are ideal for\nmobile experiences. We focused on Wikipedia articles and developed Wiki2Story,\na pipeline based on language and layout models, to demonstrate the concept.\nWiki2Story dynamically slices an article and plans one to multiple Story paths\naccording to the document hierarchy. For each slice, it generates a multi-page\nsummary Story composed of text and image pairs in visually-appealing layouts.\nWe derived design principles from an analysis of manually-created Story\npractices. We executed our pipeline on 500 Wikipedia documents and conducted\nuser studies to review selected outputs. Results showed that Wiki2Story\neffectively captured and presented salient content from the original articles\nand sparked interest in viewers.",
            "author": [
                "Daniel Nkemelu",
                "Peggy Chi",
                "Daniel Castro Chin",
                "Krishna Srinivasan",
                "Irfan Essa"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02383v1",
                "http://arxiv.org/pdf/2310.02383v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03044v1",
            "title": "scg-cli -- a Tool Supporting Software Comprehension via Extraction and\n  Analysis of Semantic Code Graph",
            "updated": "2023-10-03T19:04:51Z",
            "published": "2023-10-03T19:04:51Z",
            "summary": "We present scg-cli, a~command line tool facilitating software comprehension.\nThe tool extracts semantic information about code structure and dependencies\nfrom the Java and Scala projects, and structures it as a~Semantic Code Graph\n(SCG), an information model underlying scg-cli. The SCG data, once written into\na~portable, open protobuf-based format, can be used by the scg-cli command line\ntool to obtain project metrics, find the most critical code entities, and\ncompute project partitionings. The results of this analysis and the SCG data\ncan be exported for further investigation by external tools such as Gephi\nsoftware (visualization) and, notably, as a Jupyter Notebook environment with\nhelper APIs to enable advanced analysis of the project using data analytics\nmethods. We explain functionalities of the scg-cli tool and demonstrate its\ncapabilities by showing an example analysis of an open-source Java project\ncommons-io.",
            "author": [
                "Krzysztof Borowski",
                "Bartosz Bali\u015b"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03044v1",
                "http://arxiv.org/pdf/2310.03044v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02380v1",
            "title": "Non-blocking Dynamic Unbounded Graphs with Wait-Free Snapshot",
            "updated": "2023-10-03T19:03:14Z",
            "published": "2023-10-03T19:03:14Z",
            "summary": "Graphs are arguably one of the most fundamental data-structure used in many\ndomains such as block-chain, networks etc. Theoretically and practically,\nimproving Graph performance is one of the most studied and omnipresent research\nproblems. In this paper, we have implemented a dynamic unbounded concurrent\ngraph which can perform the add, delete or lookup operations on vertices and\nedges concurrently. All these methods are lock-free and linearizable. On top of\nthis, we have also implemented the wait-free graph snapshot algorithm. To the\nbest of knowledge this is first wait-free implementation of snapshot on\nconcurrent graphs. We have used the snapshot of the algorithm to calculate the\ndiameter and between centrality. We have compared our implementation with its\ncounterparts and outperformed them by a good margin. This illustrates the\nefficiency of our snapshot method which is a generic method and can be used to\nperform other useful graph analytics operations.",
            "author": [
                "Gaurav Bhardwaj",
                "Sathya Peri",
                "Pratik Shetty"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02380v1",
                "http://arxiv.org/pdf/2310.02380v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02374v2",
            "title": "Conversational Health Agents: A Personalized LLM-Powered Agent Framework",
            "updated": "2023-10-21T01:23:26Z",
            "published": "2023-10-03T18:54:10Z",
            "summary": "Conversational Health Agents (CHAs) are interactive systems designed to\nenhance personal healthcare services by engaging in empathetic conversations\nand processing multimodal data. While current CHAs, especially those utilizing\nLarge Language Models (LLMs), primarily focus on conversation, they often need\nmore comprehensive agent capabilities. This limitation includes accessing\npersonal user health data from wearables, ubiquitous data collection sources,\nand electronic health records, integrating the latest published health\ninsights, and connecting with established multimodal data analysis tools. In\nthis paper, we propose an LLM-powered framework to empower CHAs to generate a\npersonalized response for users' healthcare queries. This framework provides\ncritical thinking, knowledge acquisition, and problem-solving abilities by\nintegrating healthcare data sources, enabling multilingual and multimodal\nconversations, and interacting with various user data analysis tools. We\nillustrate the framework's proficiency in handling complex healthcare tasks via\na case study on stress level estimation, showcasing the agent's cognitive and\noperational capabilities.",
            "author": [
                "Mahyar Abbasian",
                "Iman Azimi",
                "Amir M. Rahmani",
                "Ramesh Jain"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02374v2",
                "http://arxiv.org/pdf/2310.02374v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02372v1",
            "title": "ProtoNER: Few shot Incremental Learning for Named Entity Recognition\n  using Prototypical Networks",
            "updated": "2023-10-03T18:52:19Z",
            "published": "2023-10-03T18:52:19Z",
            "summary": "Key value pair (KVP) extraction or Named Entity Recognition(NER) from\nvisually rich documents has been an active area of research in document\nunderstanding and data extraction domain. Several transformer based models such\nas LayoutLMv2, LayoutLMv3, and LiLT have emerged achieving state of the art\nresults. However, addition of even a single new class to the existing model\nrequires (a) re-annotation of entire training dataset to include this new class\nand (b) retraining the model again. Both of these issues really slow down the\ndeployment of updated model. \\\\ We present \\textbf{ProtoNER}: Prototypical\nNetwork based end-to-end KVP extraction model that allows addition of new\nclasses to an existing model while requiring minimal number of newly annotated\ntraining samples. The key contributions of our model are: (1) No dependency on\ndataset used for initial training of the model, which alleviates the need to\nretain original training dataset for longer duration as well as data\nre-annotation which is very time consuming task, (2) No intermediate synthetic\ndata generation which tends to add noise and results in model's performance\ndegradation, and (3) Hybrid loss function which allows model to retain\nknowledge about older classes as well as learn about newly added classes.\\\\\nExperimental results show that ProtoNER finetuned with just 30 samples is able\nto achieve similar results for the newly added classes as that of regular model\nfinetuned with 2600 samples.",
            "author": [
                "Ritesh Kumar",
                "Saurabh Goyal",
                "Ashish Verma",
                "Vatche Isahagian"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02372v1",
                "http://arxiv.org/pdf/2310.02372v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02350v1",
            "title": "Neuromimetic Dynamic Networks with Hebbian Learning",
            "updated": "2023-10-03T18:27:16Z",
            "published": "2023-10-03T18:27:16Z",
            "summary": "Leveraging recent advances in neuroscience and control theory, this paper\npresents a neuromimetic network model with dynamic symmetric connections\ngoverned by Hebbian learning rules. Formal analysis grounded in graph theory\nand classical control establishes that this biologically plausible model\nexhibits boundedness, stability, and structural controllability given a\ngeneralized sym-cactus structure with multiple control nodes. We prove the\nnecessity of this topology when there are distributed control inputs.\nSimulations using a 14-node generalized sym-cactus network with two input types\nvalidate the model's effectiveness in capturing key neural dynamics.",
            "author": [
                "Zexin Sun",
                "John Baillieul"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02350v1",
                "http://arxiv.org/pdf/2310.02350v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02336v1",
            "title": "Hereditary Nordhaus-Gaddum Graphs",
            "updated": "2023-10-03T18:15:22Z",
            "published": "2023-10-03T18:15:22Z",
            "summary": "Nordhaus and Gaddum proved in 1956 that the sum of the chromatic number\n$\\chi$ of a graph $G$ and its complement is at most $|G|+1$. The\nNordhaus-Gaddum graphs are the class of graphs satisfying this inequality with\nequality, and are well-understood. In this paper we consider a hereditary\ngeneralization: graphs $G$ for which all induced subgraphs $H$ of $G$ satisfy\n$\\chi(H) + \\chi(\\overline{H}) \\le |H|$. We characterize the forbidden induced\nsubgraphs of this class and find its intersection with a number of common\nclasses, including line graphs. We also discuss $\\chi$-boundedness and\nalgorithmic results.",
            "author": [
                "Vaidy Sivaraman",
                "Rebecca Whitman"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02336v1",
                "http://arxiv.org/pdf/2310.02336v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C75, 05C17"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02323v1",
            "title": "Approximately Equivariant Quantum Neural Network for $p4m$ Group\n  Symmetries in Images",
            "updated": "2023-10-03T18:01:02Z",
            "published": "2023-10-03T18:01:02Z",
            "summary": "Quantum Neural Networks (QNNs) are suggested as one of the quantum algorithms\nwhich can be efficiently simulated with a low depth on near-term quantum\nhardware in the presence of noises. However, their performance highly relies on\nchoosing the most suitable architecture of Variational Quantum Algorithms\n(VQAs), and the problem-agnostic models often suffer issues regarding\ntrainability and generalization power. As a solution, the most recent works\nexplore Geometric Quantum Machine Learning (GQML) using QNNs equivariant with\nrespect to the underlying symmetry of the dataset. GQML adds an inductive bias\nto the model by incorporating the prior knowledge on the given dataset and\nleads to enhancing the optimization performance while constraining the search\nspace. This work proposes equivariant Quantum Convolutional Neural Networks\n(EquivQCNNs) for image classification under planar $p4m$ symmetry, including\nreflectional and $90^\\circ$ rotational symmetry. We present the results tested\nin different use cases, such as phase detection of the 2D Ising model and\nclassification of the extended MNIST dataset, and compare them with those\nobtained with the non-equivariant model, proving that the equivariance fosters\nbetter generalization of the model.",
            "author": [
                "Su Yeon Chang",
                "Michele Grossi",
                "Bertrand Le Saux",
                "Sofia Vallecorsa"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02323v1",
                "http://arxiv.org/pdf/2310.02323v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02265v1",
            "title": "DREAM: Visual Decoding from Reversing Human Visual System",
            "updated": "2023-10-03T17:59:58Z",
            "published": "2023-10-03T17:59:58Z",
            "summary": "In this work we present DREAM, an fMRI-to-image method for reconstructing\nviewed images from brain activities, grounded on fundamental knowledge of the\nhuman visual system. We craft reverse pathways that emulate the hierarchical\nand parallel nature of how humans perceive the visual world. These tailored\npathways are specialized to decipher semantics, color, and depth cues from fMRI\ndata, mirroring the forward pathways from visual stimuli to fMRI recordings. To\ndo so, two components mimic the inverse processes within the human visual\nsystem: the Reverse Visual Association Cortex (R-VAC) which reverses pathways\nof this brain region, extracting semantics from fMRI data; the Reverse Parallel\nPKM (R-PKM) component simultaneously predicting color and depth from fMRI\nsignals. The experiments indicate that our method outperforms the current\nstate-of-the-art models in terms of the consistency of appearance, structure,\nand semantics. Code will be made publicly available to facilitate further\nresearch in this field.",
            "author": [
                "Weihao Xia",
                "Raoul de Charette",
                "Cengiz \u00d6ztireli",
                "Jing-Hao Xue"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02265v1",
                "http://arxiv.org/pdf/2310.02265v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG",
                "eess.IV",
                "q-bio.NC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02247v1",
            "title": "Efficient Enumeration of Drawings and Combinatorial Structures for\n  Maximal Planar Graphs",
            "updated": "2023-10-03T17:51:55Z",
            "published": "2023-10-03T17:51:55Z",
            "summary": "We propose efficient algorithms for enumerating the notorious combinatorial\nstructures of maximal planar graphs, called canonical orderings and Schnyder\nwoods, and the related classical graph drawings by de Fraysseix, Pach, and\nPollack [Combinatorica, 1990] and by Schnyder [SODA, 1990], called canonical\ndrawings and Schnyder drawings, respectively. To this aim (i) we devise an\nalgorithm for enumerating special $e$-bipolar orientations of maximal planar\ngraphs, called canonical orientations; (ii) we establish bijections between\ncanonical orientations and canonical drawings, and between canonical\norientations and Schnyder drawings; and (iii) we exploit the known\ncorrespondence between canonical orientations and canonical orderings, and the\nknown bijection between canonical orientations and Schnyder woods. All our\nenumeration algorithms have $O(n)$ setup time, space usage, and delay between\nany two consecutively listed outputs, for an $n$-vertex maximal planar graph.",
            "author": [
                "Giordano Da Lozzo",
                "Giuseppe Di Battista",
                "Fabrizio Frati",
                "Fabrizio Grosso",
                "Maurizio Patrignani"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02247v1",
                "http://arxiv.org/pdf/2310.02247v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "cs.CG",
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02238v2",
            "title": "Who's Harry Potter? Approximate Unlearning in LLMs",
            "updated": "2023-10-04T05:20:19Z",
            "published": "2023-10-03T17:48:14Z",
            "summary": "Large language models (LLMs) are trained on massive internet corpora that\noften contain copyrighted content. This poses legal and ethical challenges for\nthe developers and users of these models, as well as the original authors and\npublishers. In this paper, we propose a novel technique for unlearning a subset\nof the training data from a LLM, without having to retrain it from scratch.\n  We evaluate our technique on the task of unlearning the Harry Potter books\nfrom the Llama2-7b model (a generative language model recently open-sourced by\nMeta). While the model took over 184K GPU-hours to pretrain, we show that in\nabout 1 GPU hour of finetuning, we effectively erase the model's ability to\ngenerate or recall Harry Potter-related content, while its performance on\ncommon benchmarks (such as Winogrande, Hellaswag, arc, boolq and piqa) remains\nalmost unaffected. We make our fine-tuned model publicly available on\nHuggingFace for community evaluation. To the best of our knowledge, this is the\nfirst paper to present an effective technique for unlearning in generative\nlanguage models.\n  Our technique consists of three main components: First, we use a reinforced\nmodel that is further trained on the target data to identify the tokens that\nare most related to the unlearning target, by comparing its logits with those\nof a baseline model. Second, we replace idiosyncratic expressions in the target\ndata with generic counterparts, and leverage the model's own predictions to\ngenerate alternative labels for every token. These labels aim to approximate\nthe next-token predictions of a model that has not been trained on the target\ndata. Third, we finetune the model on these alternative labels, which\neffectively erases the original text from the model's memory whenever it is\nprompted with its context.",
            "author": [
                "Ronen Eldan",
                "Mark Russinovich"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02238v2",
                "http://arxiv.org/pdf/2310.02238v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02232v2",
            "title": "HoloNets: Spectral Convolutions do extend to Directed Graphs",
            "updated": "2023-11-10T15:34:22Z",
            "published": "2023-10-03T17:42:09Z",
            "summary": "Within the graph learning community, conventional wisdom dictates that\nspectral convolutional networks may only be deployed on undirected graphs: Only\nthere could the existence of a well-defined graph Fourier transform be\nguaranteed, so that information may be translated between spatial- and spectral\ndomains. Here we show this traditional reliance on the graph Fourier transform\nto be superfluous and -- making use of certain advanced tools from complex\nanalysis and spectral theory -- extend spectral convolutions to directed\ngraphs. We provide a frequency-response interpretation of newly developed\nfilters, investigate the influence of the basis used to express filters and\ndiscuss the interplay with characteristic operators on which networks are\nbased. In order to thoroughly test the developed theory, we conduct experiments\nin real world settings, showcasing that directed spectral convolutional\nnetworks provide new state of the art results for heterophilic node\nclassification on many datasets and -- as opposed to baselines -- may be\nrendered stable to resolution-scale varying topological perturbations.",
            "author": [
                "Christian Koke",
                "Daniel Cremers"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02232v2",
                "http://arxiv.org/pdf/2310.02232v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02922v1",
            "title": "Public verifiable measurement-only blind quantum computation based on\n  entanglement witnesses",
            "updated": "2023-10-03T17:16:15Z",
            "published": "2023-10-03T17:16:15Z",
            "summary": "Recently, Sato et al. proposed an public verifiable blind quantum computation\n(BQC) protocol by inserting a third-party arbiter. However, it is not true\npublic verifiable in a sense, because the arbiter is determined in advance and\nparticipates in the whole process. In this paper, a public verifiable protocol\nfor measurement-only BQC is proposed. The fidelity between arbitrary states and\nthe graph states of 2-colorable graphs is estimated by measuring the\nentanglement witnesses of the graph states,so as to verify the correctness of\nthe prepared graph states. Compared with the previous protocol, our protocol is\npublic verifiable in the true sense by allowing other random clients to execute\nthe public verification. It also has greater advantages in the efficiency,\nwhere the number of local measurements is O(n^3*log {n}) and graph states'\ncopies is O(n^2*log{n}).",
            "author": [
                "Wen-Jie Liu",
                "Zi-Xian Li",
                "Wen-Bo Li",
                "Qi Yang"
            ],
            "link": [
                "http://dx.doi.org/10.1007/s11128-023-03859-9",
                "http://arxiv.org/abs/2310.02922v1",
                "http://arxiv.org/pdf/2310.02922v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.CR",
                "cs.ET"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02211v1",
            "title": "Fast Localization and Tracking in City-Scale UWB Networks",
            "updated": "2023-10-03T17:08:45Z",
            "published": "2023-10-03T17:08:45Z",
            "summary": "Localization of networked nodes is an essential problem in emerging\napplications, including first-responder navigation, automated manufacturing\nlines, vehicular and drone navigation, asset navigation and tracking, Internet\nof Things and 5G communication networks. In this paper, we present Locate3D, a\nnovel system for peer-to-peer node localization and orientation estimation in\nlarge networks. Unlike traditional range-only methods, Locate3D introduces\nangle-of-arrival (AoA) data as an added network topology constraint. The system\nsolves three key challenges: it uses angles to reduce the number of\nmeasurements required by 4x and jointly use range and angle data for location\nestimation. We develop a spanning-tree approach for fast location updates, and\nto ensure the output graphs are rigid and uniquely realizable, even in occluded\nor weakly connected areas. Locate3D cuts down latency by up to 75% without\ncompromising accuracy, surpassing standard range-only solutions. It has a 10.2\nmeters median localization error for large-scale networks (30,000 nodes, 15\nanchors spread across 14km square) and 0.5 meters for small-scale networks (10\nnodes).",
            "author": [
                "Nakul Garg",
                "Irtaza Shahid",
                "Ramanujan K Sheshadri",
                "Karthikeyan Sundaresan",
                "Nirupam Roy"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02211v1",
                "http://arxiv.org/pdf/2310.02211v1"
            ],
            "primary_category": "cs.NI",
            "category": [
                "cs.NI",
                "cs.RO",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02207v1",
            "title": "Language Models Represent Space and Time",
            "updated": "2023-10-03T17:06:52Z",
            "published": "2023-10-03T17:06:52Z",
            "summary": "The capabilities of large language models (LLMs) have sparked debate over\nwhether such systems just learn an enormous collection of superficial\nstatistics or a coherent model of the data generating process -- a world model.\nWe find evidence for the latter by analyzing the learned representations of\nthree spatial datasets (world, US, NYC places) and three temporal datasets\n(historical figures, artworks, news headlines) in the Llama-2 family of models.\nWe discover that LLMs learn linear representations of space and time across\nmultiple scales. These representations are robust to prompting variations and\nunified across different entity types (e.g. cities and landmarks). In addition,\nwe identify individual ``space neurons'' and ``time neurons'' that reliably\nencode spatial and temporal coordinates. Our analysis demonstrates that modern\nLLMs acquire structured knowledge about fundamental dimensions such as space\nand time, supporting the view that they learn not merely superficial\nstatistics, but literal world models.",
            "author": [
                "Wes Gurnee",
                "Max Tegmark"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02207v1",
                "http://arxiv.org/pdf/2310.02207v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02203v1",
            "title": "Stochastic Quantum Power Flow for Risk Assessment in Power Systems",
            "updated": "2023-10-03T16:59:26Z",
            "published": "2023-10-03T16:59:26Z",
            "summary": "This paper introduces, to the best of our knowledge, the first quantum\ncomputing methodology for stochastic power flow. Stochastic power flow is\nwidely used in power system operation and planning to study the impact of\nstochastic factors, such as uncertain generation, load, or contingencies, on\npower systems. Most standard approaches use Monte-Carlo simulations. In this\npaper, we focus on how the uncertainty of wind infeed affects the probability\nof line overloadings in a power grid. Quantum Monte Carlo approaches and the\nsolution of linear systems have both been theoretically proven to be more\ncomputationally efficient than classical computing approaches. For example,\nQuantum Monte Carlo requires substantially fewer samples than Classical Monte\nCarlo to achieve the same accuracy. This paper presents the first formulation\nthat exploits this quantum advantage to formulate a Quantum Stochastic Power\nFlow method. The developed method is tested for two small power systems and\ncompared to classical simulations.",
            "author": [
                "Brynjar S\u00e6varsson",
                "Hj\u00f6rtur J\u00f3hannsson",
                "Spyros Chatzivasileiadis"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02203v1",
                "http://arxiv.org/pdf/2310.02203v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.06862v2",
            "title": "Imaginary Cities of the Diophantine equation $X^3+Y^3+Z^3=K$",
            "updated": "2023-11-18T02:50:36Z",
            "published": "2023-10-03T16:57:14Z",
            "summary": "The purpose of this article is to make a graph representation of the\nDiophantine equation $X^3+Y^3+X^3=K$ using the theory of \"imaginary cities\" for\nits construction and to determine the modular combinations on its edges with\nthe De Bruijn cycles.",
            "author": [
                "Eduardo J. Acu\u00f1a Tarazona"
            ],
            "link": [
                "http://arxiv.org/abs/2310.06862v2",
                "http://arxiv.org/pdf/2310.06862v2"
            ],
            "primary_category": "math.GM",
            "category": [
                "math.GM",
                "05C10 11D25 11J25"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02198v1",
            "title": "Strong Faithfulness for ELH Ontology Embeddings",
            "updated": "2023-10-03T16:47:35Z",
            "published": "2023-10-03T16:47:35Z",
            "summary": "Ontology embedding methods are powerful approaches to represent and reason\nover structured knowledge in various domains. One advantage of ontology\nembeddings over knowledge graph embeddings is their ability to capture and\nimpose an underlying schema to which the model must conform. Despite advances,\nmost current approaches do not guarantee that the resulting embedding respects\nthe axioms the ontology entails. In this work, we formally prove that\nnormalized ${\\cal ELH}$ has the strong faithfulness property on convex\ngeometric models, which means that there is an embedding that precisely\ncaptures the original ontology. We present a region-based geometric model for\nembedding normalized ${\\cal ELH}$ ontologies into a continuous vector space. To\nprove strong faithfulness, our construction takes advantage of the fact that\nnormalized ${\\cal ELH}$ has a finite canonical model. We first prove the\nstatement assuming (possibly) non-convex regions, allowing us to keep the\nrequired dimensions low. Then, we impose convexity on the regions and show the\nproperty still holds. Finally, we consider reasoning tasks on geometric models\nand analyze the complexity in the class of convex geometric models used for\nproving strong faithfulness.",
            "author": [
                "Victor Lacerda",
                "Ana Ozaki",
                "Ricardo Guimar\u00e3es"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02198v1",
                "http://arxiv.org/pdf/2310.02198v1"
            ],
            "primary_category": "cs.LO",
            "category": [
                "cs.LO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02193v1",
            "title": "Uncertainty Quantification in Inverse Models in Hydrology",
            "updated": "2023-10-03T16:39:21Z",
            "published": "2023-10-03T16:39:21Z",
            "summary": "In hydrology, modeling streamflow remains a challenging task due to the\nlimited availability of basin characteristics information such as soil geology\nand geomorphology. These characteristics may be noisy due to measurement errors\nor may be missing altogether. To overcome this challenge, we propose a\nknowledge-guided, probabilistic inverse modeling method for recovering physical\ncharacteristics from streamflow and weather data, which are more readily\navailable. We compare our framework with state-of-the-art inverse models for\nestimating river basin characteristics. We also show that these estimates offer\nimprovement in streamflow modeling as opposed to using the original basin\ncharacteristic values. Our inverse model offers 3\\% improvement in R$^2$ for\nthe inverse model (basin characteristic estimation) and 6\\% for the forward\nmodel (streamflow prediction). Our framework also offers improved\nexplainability since it can quantify uncertainty in both the inverse and the\nforward model. Uncertainty quantification plays a pivotal role in improving the\nexplainability of machine learning models by providing additional insights into\nthe reliability and limitations of model predictions. In our analysis, we\nassess the quality of the uncertainty estimates. Compared to baseline\nuncertainty quantification methods, our framework offers 10\\% improvement in\nthe dispersion of epistemic uncertainty and 13\\% improvement in coverage rate.\nThis information can help stakeholders understand the level of uncertainty\nassociated with the predictions and provide a more comprehensive view of the\npotential outcomes.",
            "author": [
                "Somya Sharma Chatterjee",
                "Rahul Ghosh",
                "Arvind Renganathan",
                "Xiang Li",
                "Snigdhansu Chatterjee",
                "John Nieber",
                "Christopher Duffy",
                "Vipin Kumar"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02193v1",
                "http://arxiv.org/pdf/2310.02193v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "stat.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02167v1",
            "title": "Towards a Unified Framework for Sequential Decision Making",
            "updated": "2023-10-03T16:01:06Z",
            "published": "2023-10-03T16:01:06Z",
            "summary": "In recent years, the integration of Automated Planning (AP) and Reinforcement\nLearning (RL) has seen a surge of interest. To perform this integration, a\ngeneral framework for Sequential Decision Making (SDM) would prove immensely\nuseful, as it would help us understand how AP and RL fit together. In this\npreliminary work, we attempt to provide such a framework, suitable for any\nmethod ranging from Classical Planning to Deep RL, by drawing on concepts from\nProbability Theory and Bayesian inference. We formulate an SDM task as a set of\ntraining and test Markov Decision Processes (MDPs), to account for\ngeneralization. We provide a general algorithm for SDM which we hypothesize\nevery SDM method is based on. According to it, every SDM algorithm can be seen\nas a procedure that iteratively improves its solution estimate by leveraging\nthe task knowledge available. Finally, we derive a set of formulas and\nalgorithms for calculating interesting properties of SDM tasks and methods,\nwhich make possible their empirical evaluation and comparison.",
            "author": [
                "Carlos N\u00fa\u00f1ez-Molina",
                "Pablo Mesejo",
                "Juan Fern\u00e1ndez-Olivares"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02167v1",
                "http://arxiv.org/pdf/2310.02167v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "I.2.8"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02166v1",
            "title": "Large Language Models Meet Knowledge Graphs to Answer Factoid Questions",
            "updated": "2023-10-03T15:57:00Z",
            "published": "2023-10-03T15:57:00Z",
            "summary": "Recently, it has been shown that the incorporation of structured knowledge\ninto Large Language Models significantly improves the results for a variety of\nNLP tasks. In this paper, we propose a method for exploring pre-trained\nText-to-Text Language Models enriched with additional information from\nKnowledge Graphs for answering factoid questions. More specifically, we propose\nan algorithm for subgraphs extraction from a Knowledge Graph based on question\nentities and answer candidates. Then, we procure easily interpreted information\nwith Transformer-based models through the linearization of the extracted\nsubgraphs. Final re-ranking of the answer candidates with the extracted\ninformation boosts Hits@1 scores of the pre-trained text-to-text language\nmodels by 4-6%.",
            "author": [
                "Mikhail Salnikov",
                "Hai Le",
                "Prateek Rajput",
                "Irina Nikishina",
                "Pavel Braslavski",
                "Valentin Malykh",
                "Alexander Panchenko"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02166v1",
                "http://arxiv.org/pdf/2310.02166v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02161v1",
            "title": "Selenite: Scaffolding Decision Making with Comprehensive Overviews\n  Elicited from Large Language Models",
            "updated": "2023-10-03T15:48:22Z",
            "published": "2023-10-03T15:48:22Z",
            "summary": "Decision-making in unfamiliar domains can be challenging, demanding\nconsiderable user effort to compare different options with respect to various\ncriteria. Prior research and our formative study found that people would\nbenefit from seeing an overview of the information space upfront, such as the\ncriteria that others have previously found useful. However, existing\nsensemaking tools struggle with the \"cold-start\" problem -- it not only\nrequires significant input from previous users to generate and share these\noverviews, but such overviews may also be biased and incomplete. In this work,\nwe introduce a novel system, Selenite, which leverages LLMs as reasoning\nmachines and knowledge retrievers to automatically produce a comprehensive\noverview of options and criteria to jumpstart users' sensemaking processes.\nSubsequently, Selenite also adapts as people use it, helping users find, read,\nand navigate unfamiliar information in a systematic yet personalized manner.\nThrough three studies, we found that Selenite produced accurate and\nhigh-quality overviews reliably, significantly accelerated users' information\nprocessing, and effectively improved their overall comprehension and\nsensemaking experience.",
            "author": [
                "Michael Xieyang Liu",
                "Tongshuang Wu",
                "Tianying Chen",
                "Franklin Mingzhe Li",
                "Aniket Kittur",
                "Brad A. Myers"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02161v1",
                "http://arxiv.org/pdf/2310.02161v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02156v3",
            "title": "Probabilistically Rewired Message-Passing Neural Networks",
            "updated": "2023-10-15T17:56:16Z",
            "published": "2023-10-03T15:43:59Z",
            "summary": "Message-passing graph neural networks (MPNNs) emerged as powerful tools for\nprocessing graph-structured input. However, they operate on a fixed input graph\nstructure, ignoring potential noise and missing information. Furthermore, their\nlocal aggregation mechanism can lead to problems such as over-squashing and\nlimited expressive power in capturing relevant graph structures. Existing\nsolutions to these challenges have primarily relied on heuristic methods, often\ndisregarding the underlying data distribution. Hence, devising principled\napproaches for learning to infer graph structures relevant to the given\nprediction task remains an open challenge. In this work, leveraging recent\nprogress in exact and differentiable $k$-subset sampling, we devise\nprobabilistically rewired MPNNs (PR-MPNNs), which learn to add relevant edges\nwhile omitting less beneficial ones. For the first time, our theoretical\nanalysis explores how PR-MPNNs enhance expressive power, and we identify\nprecise conditions under which they outperform purely randomized approaches.\nEmpirically, we demonstrate that our approach effectively mitigates issues like\nover-squashing and under-reaching. In addition, on established real-world\ndatasets, our method exhibits competitive or superior predictive performance\ncompared to traditional MPNN models and recent graph transformer architectures.",
            "author": [
                "Chendi Qian",
                "Andrei Manolache",
                "Kareem Ahmed",
                "Zhe Zeng",
                "Guy Van den Broeck",
                "Mathias Niepert",
                "Christopher Morris"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02156v3",
                "http://arxiv.org/pdf/2310.02156v3"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.NE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02152v1",
            "title": "Graph Neural Network-based EEG Classification: A Survey",
            "updated": "2023-10-03T15:40:03Z",
            "published": "2023-10-03T15:40:03Z",
            "summary": "Graph neural networks (GNN) are increasingly used to classify EEG for tasks\nsuch as emotion recognition, motor imagery and neurological diseases and\ndisorders. A wide range of methods have been proposed to design GNN-based\nclassifiers. Therefore, there is a need for a systematic review and\ncategorisation of these approaches. We exhaustively search the published\nliterature on this topic and derive several categories for comparison. These\ncategories highlight the similarities and differences among the methods. The\nresults suggest a prevalence of spectral graph convolutional layers over\nspatial. Additionally, we identify standard forms of node features, with the\nmost popular being the raw EEG signal and differential entropy. Our results\nsummarise the emerging trends in GNN-based approaches for EEG classification.\nFinally, we discuss several promising research directions, such as exploring\nthe potential of transfer learning methods and appropriate modelling of\ncross-frequency interactions.",
            "author": [
                "Dominik Klepl",
                "Min Wu",
                "Fei He"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02152v1",
                "http://arxiv.org/pdf/2310.02152v1"
            ],
            "primary_category": "q-bio.NC",
            "category": [
                "q-bio.NC",
                "cs.LG",
                "q-bio.QM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02142v1",
            "title": "Arena-independent Memory Bounds for Nash Equilibria in Reachability\n  Games",
            "updated": "2023-10-03T15:26:41Z",
            "published": "2023-10-03T15:26:41Z",
            "summary": "We study the memory requirements of Nash equilibria in turn-based multiplayer\ngames on possibly infinite graphs with reachability, shortest path and B\\\"uchi\nobjectives.\n  We present constructions for finite-memory Nash equilibria in these games\nthat apply to arbitrary game graphs, bypassing the finite-arena requirement\nthat is central in existing approaches. We show that, for these three types of\ngames, from any Nash equilibrium, we can derive another Nash equilibrium where\nall strategies are finite-memory such that the same players accomplish their\nobjective, without increasing their cost for shortest path games.\n  Furthermore, we provide memory bounds that are independent of the size of the\ngame graph for reachability and shortest path games. These bounds depend only\non the number of players.\n  To the best of our knowledge, we provide the first results pertaining to\nfinite-memory constrained Nash equilibria in infinite arenas and the first\narena-independent memory bounds for Nash equilibria.",
            "author": [
                "James C. A. Main"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02142v1",
                "http://arxiv.org/pdf/2310.02142v1"
            ],
            "primary_category": "cs.GT",
            "category": [
                "cs.GT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02130v2",
            "title": "Clustering Graphs of Bounded Treewidth to Minimize the Sum of\n  Radius-Dependent Costs",
            "updated": "2023-10-04T10:34:43Z",
            "published": "2023-10-03T15:12:47Z",
            "summary": "We consider the following natural problem that generalizes min-sum-radii\nclustering: Given is $k\\in\\mathbb{N}$ as well as some metric space $(V,d)$\nwhere $V=F\\cup C$ for facilities $F$ and clients $C$. The goal is to find a\nclustering given by $k$ facility-radius pairs $(f_1,r_1),\\dots,(f_k,r_k)\\in\nF\\times\\mathbb{R}_{\\geq 0}$ such that $C\\subseteq B(f_1,r_1)\\cup\\dots\\cup\nB(f_k,r_k)$ and $\\sum_{i=1,\\dots,k} g(r_i)$ is minimized for some increasing\nfunction $g:\\mathbb{R}_{\\geq 0}\\rightarrow\\mathbb{R}_{\\geq 0}$. Here, $B(x,r)$\nis the radius-$r$ ball centered at $x$. For the case that $(V,d)$ is the\nshortest-path metric of some edge-weighted graph of bounded treewidth, we\npresent a dynamic program that is tailored to this class of problems and\nachieves a polynomial running time, establishing that the problem is in\n$\\mathsf{XP}$ with parameter treewidth.",
            "author": [
                "Lukas Drexler",
                "Jan H\u00f6ckendorff",
                "Joshua K\u00f6nen",
                "Kevin Schewior"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02130v2",
                "http://arxiv.org/pdf/2310.02130v2"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02129v2",
            "title": "Unveiling the Pitfalls of Knowledge Editing for Large Language Models",
            "updated": "2023-11-21T17:59:04Z",
            "published": "2023-10-03T15:10:46Z",
            "summary": "As the cost associated with fine-tuning Large Language Models (LLMs)\ncontinues to rise, recent research efforts have pivoted towards developing\nmethodologies to edit implicit knowledge embedded within LLMs. Yet, there's\nstill a dark cloud lingering overhead -- will knowledge editing trigger\nbutterfly effect? since it is still unclear whether knowledge editing might\nintroduce side effects that pose potential risks or not. This paper pioneers\nthe investigation into the potential pitfalls associated with knowledge editing\nfor LLMs. To achieve this, we introduce new benchmark datasets and propose\ninnovative evaluation metrics. Our results underline two pivotal concerns: (1)\nKnowledge Conflict: Editing groups of facts that logically clash can magnify\nthe inherent inconsistencies in LLMs-a facet neglected by previous methods. (2)\nKnowledge Distortion: Altering parameters with the aim of editing factual\nknowledge can irrevocably warp the innate knowledge structure of LLMs.\nExperimental results vividly demonstrate that knowledge editing might\ninadvertently cast a shadow of unintended consequences on LLMs, which warrant\nattention and efforts for future works. Code is available at\nhttps://github.com/zjunlp/PitfallsKnowledgeEditing.",
            "author": [
                "Zhoubo Li",
                "Ningyu Zhang",
                "Yunzhi Yao",
                "Mengru Wang",
                "Xi Chen",
                "Huajun Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02129v2",
                "http://arxiv.org/pdf/2310.02129v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.CV",
                "cs.DB",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02128v1",
            "title": "Semantic Code Graph -- an information model to facilitate software\n  comprehension",
            "updated": "2023-10-03T15:09:49Z",
            "published": "2023-10-03T15:09:49Z",
            "summary": "Software comprehension can be extremely time-consuming due to the\never-growing size of codebases. Consequently, there is an increasing need to\naccelerate the code comprehension process to facilitate maintenance and reduce\nassociated costs. A crucial aspect of this process is understanding and\npreserving the high quality of the code dependency structure. While a variety\nof code structure models already exist, there is a surprising lack of models\nthat closely represent the source code and focus on software comprehension. As\na result, there are no readily available and easy-to-use tools to assist with\ndependency comprehension, refactoring, and quality monitoring of code. To\naddress this gap, we propose the Semantic Code Graph (SCG), an information\nmodel that offers a detailed abstract representation of code dependencies with\na close relationship to the source code. To validate the SCG model's usefulness\nin software comprehension, we compare it to nine other source code\nrepresentation models. Additionally, we select 11 well-known and widely-used\nopen-source projects developed in Java and Scala and perform a range of\nsoftware comprehension activities on them using three different code\nrepresentation models: the proposed SCG, the Call Graph (CG), and the Class\nCollaboration Network (CCN). We then qualitatively analyze the results to\ncompare the performance of these models in terms of software comprehension\ncapabilities. These activities encompass project structure comprehension,\nidentifying critical project entities, interactive visualization of code\ndependencies, and uncovering code similarities through software mining. Our\nfindings demonstrate that the SCG enhances software comprehension capabilities\ncompared to the prevailing CCN and CG models. We believe that the work\ndescribed is a step towards the next generation of tools that streamline code\ndependency comprehension and management.",
            "author": [
                "Krzysztof Borowski",
                "Bartosz Bali\u015b",
                "Tomasz Orzechowski"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02128v1",
                "http://arxiv.org/pdf/2310.02128v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02118v1",
            "title": "TWIZ: The Wizard of Multimodal Conversational-Stimulus",
            "updated": "2023-10-03T14:59:35Z",
            "published": "2023-10-03T14:59:35Z",
            "summary": "In this report, we describe the vision, challenges, and scientific\ncontributions of the Task Wizard team, TWIZ, in the Alexa Prize TaskBot\nChallenge 2022. Our vision, is to build TWIZ bot as an helpful, multimodal,\nknowledgeable, and engaging assistant that can guide users towards the\nsuccessful completion of complex manual tasks. To achieve this, we focus our\nefforts on three main research questions: (1) Humanly-Shaped Conversations, by\nproviding information in a knowledgeable way; (2) Multimodal Stimulus, making\nuse of various modalities including voice, images, and videos; and (3)\nZero-shot Conversational Flows, to improve the robustness of the interaction to\nunseen scenarios. TWIZ is an assistant capable of supporting a wide range of\ntasks, with several innovative features such as creative cooking, video\nnavigation through voice, and the robust TWIZ-LLM, a Large Language Model\ntrained for dialoguing about complex manual tasks. Given ratings and feedback\nprovided by users, we observed that TWIZ bot is an effective and robust system,\ncapable of guiding users through tasks while providing several multimodal\nstimuli.",
            "author": [
                "Rafael Ferreira",
                "Diogo Tavares",
                "Diogo Silva",
                "Rodrigo Val\u00e9rio",
                "Jo\u00e3o Bordalo",
                "In\u00eas Sim\u00f5es",
                "Vasco Ramos",
                "David Semedo",
                "Jo\u00e3o Magalh\u00e3es"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02118v1",
                "http://arxiv.org/pdf/2310.02118v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "I.2.7"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02104v1",
            "title": "An empirical study of ChatGPT-3.5 on question answering and code\n  maintenance",
            "updated": "2023-10-03T14:48:32Z",
            "published": "2023-10-03T14:48:32Z",
            "summary": "Ever since the launch of ChatGPT in 2022, a rising concern is whether ChatGPT\nwill replace programmers and kill jobs. Motivated by this widespread concern,\nwe conducted an empirical study to systematically compare ChatGPT against\nprogrammers in question-answering and software-maintaining. We reused a dataset\nintroduced by prior work, which includes 130 StackOverflow (SO) discussion\nthreads referred to by the Java developers of 357 GitHub projects. We mainly\ninvestigated three research questions (RQs). First, how does ChatGPT compare\nwith programmers when answering technical questions? Second, how do developers\nperceive the differences between ChatGPT's answers and SO answers? Third, how\ndoes ChatGPT compare with humans when revising code for maintenance requests?\n  For RQ1, we provided the 130 SO questions to ChatGPT, and manually compared\nChatGPT answers with the accepted/most popular SO answers in terms of\nrelevance, readability, informativeness, comprehensiveness, and reusability.\nFor RQ2, we conducted a user study with 30 developers, asking each developer to\nassess and compare 10 pairs of answers, without knowing the information source\n(i.e., ChatGPT or SO). For RQ3, we distilled 48 software maintenance tasks from\n48 GitHub projects citing the studied SO threads. We queried ChatGPT to revise\na given Java file, and to incorporate the code implementation for any\nprescribed maintenance requirement. Our study reveals interesting phenomena:\nFor the majority of SO questions (97/130), ChatGPT provided better answers; in\n203 of 300 ratings, developers preferred ChatGPT answers to SO answers; ChatGPT\nrevised code correctly for 22 of the 48 tasks. Our research will expand\npeople's knowledge of ChatGPT capabilities, and shed light on future adoption\nof ChatGPT by the software industry.",
            "author": [
                "Md Mahir Asef Kabir",
                "Sk Adnan Hassan",
                "Xiaoyin Wang",
                "Ying Wang",
                "Hai Yu",
                "Na Meng"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02104v1",
                "http://arxiv.org/pdf/2310.02104v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02087v1",
            "title": "Scaling limit of the cluster size distribution for the random current\n  measure on the complete graph",
            "updated": "2023-10-03T14:30:20Z",
            "published": "2023-10-03T14:30:20Z",
            "summary": "We study the percolation configuration arising from the random current\nrepresentation of the near-critical Ising model on the complete graph. We\ncompute the scaling limit of the cluster size distribution for an arbitrary set\nof sources in the single and the double current measures. As a byproduct, we\ncompute the tangling probabilities recently introduced by Gunaratnam,\nPanagiotis, Panis, and Severo in [GPPS22]. This provides a new perspective on\nthe switching lemma for the $\\varphi^4$ model introduced in the same paper: in\nthe Gaussian limit we recover Wick's law, while in the Ising limit we recover\nthe corresponding tool for the Ising model.",
            "author": [
                "Dmitrii Krachun",
                "Christoforos Panagiotis",
                "Romain Panis"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02087v1",
                "http://arxiv.org/pdf/2310.02087v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR",
                "math-ph",
                "math.CO",
                "math.MP",
                "60K35, 82B27, 82B43, 05C80"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02071v4",
            "title": "Towards End-to-End Embodied Decision Making via Multi-modal Large\n  Language Model: Explorations with GPT4-Vision and Beyond",
            "updated": "2023-11-28T11:23:14Z",
            "published": "2023-10-03T14:13:36Z",
            "summary": "In this study, we explore the potential of Multimodal Large Language Models\n(MLLMs) in improving embodied decision-making processes for agents. While Large\nLanguage Models (LLMs) have been widely used due to their advanced reasoning\nskills and vast world knowledge, MLLMs like GPT4-Vision offer enhanced visual\nunderstanding and reasoning capabilities. We investigate whether\nstate-of-the-art MLLMs can handle embodied decision-making in an end-to-end\nmanner and whether collaborations between LLMs and MLLMs can enhance\ndecision-making. To address these questions, we introduce a new benchmark\ncalled PCA-EVAL, which evaluates embodied decision-making from the perspectives\nof Perception, Cognition, and Action. Additionally, we propose HOLMES, a\nmulti-agent cooperation framework that allows LLMs to leverage MLLMs and APIs\nto gather multimodal information for informed decision-making. We compare\nend-to-end embodied decision-making and HOLMES on our benchmark and find that\nthe GPT4-Vision model demonstrates strong end-to-end embodied decision-making\nabilities, outperforming GPT4-HOLMES in terms of average decision accuracy\n(+3%). However, this performance is exclusive to the latest GPT4-Vision model,\nsurpassing the open-source state-of-the-art MLLM by 26%. Our results indicate\nthat powerful MLLMs like GPT4-Vision hold promise for decision-making in\nembodied agents, offering new avenues for MLLM research. Code and data are open\nat https://github.com/pkunlp-icler/PCA-EVAL/.",
            "author": [
                "Liang Chen",
                "Yichi Zhang",
                "Shuhuai Ren",
                "Haozhe Zhao",
                "Zefan Cai",
                "Yuchi Wang",
                "Peiyi Wang",
                "Tianyu Liu",
                "Baobao Chang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02071v4",
                "http://arxiv.org/pdf/2310.02071v4"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL",
                "cs.CV",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02056v1",
            "title": "Leveraging Data-Driven Models for Accurate Analysis of Grid-Tied Smart\n  Inverters Dynamics",
            "updated": "2023-10-03T13:56:24Z",
            "published": "2023-10-03T13:56:24Z",
            "summary": "The integration of power electronic converters (PECs) and distributed energy\nresources (DERs) in modern power systems has introduced dynamism and\ncomplexity. Accurate simulation becomes essential to comprehend the influence\nof converter domination on the power grid. This study addresses the\nfast-switching and stochastic behaviors exhibited by inverter-based resources\nin converter-dominated power systems, highlighting the necessity for precise\nanalytical models. In the realm of modeling real-world systems, multiple\nmethodologies exist. Notably, black-box and data-driven system identification\ntechniques are employed to construct PEC models using experimental data,\nwithout relying on a priori knowledge of the internal system physics. This\napproach entails a systematic process of model class selection, parameter\nestimation, and model validation. While a range of linear and nonlinear model\nstructures and estimation algorithms are at our disposal, it remains imperative\nto harness creativity and a profound understanding of the physical system to\ncraft data-driven models that align seamlessly with their intended\napplications. These applications may encompass simulation, prediction, control,\nor fault detection. This report offers valuable insights into the collection of\ndatasets from commercial off-the-shelf inverters, along with the presentation\nof intricate simulation models.",
            "author": [
                "Sunil Subedi",
                "Nischal Guruwacharya",
                "Bidur Poudel",
                "Jesus D. Vasquez-Plaza",
                "Fabio Andrade",
                "Robert Fourney",
                "Hossein Moradi Rekabdarkolaee",
                "Timothy M. Hansen",
                "Reinaldo Tonkoski"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02056v1",
                "http://arxiv.org/pdf/2310.02056v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02053v1",
            "title": "Controlling Topic-Focus Articulation in Meaning-to-Text Generation using\n  Graph Neural Networks",
            "updated": "2023-10-03T13:51:01Z",
            "published": "2023-10-03T13:51:01Z",
            "summary": "A bare meaning representation can be expressed in various ways using natural\nlanguage, depending on how the information is structured on the surface level.\nWe are interested in finding ways to control topic-focus articulation when\ngenerating text from meaning. We focus on distinguishing active and passive\nvoice for sentences with transitive verbs. The idea is to add pragmatic\ninformation such as topic to the meaning representation, thereby forcing either\nactive or passive voice when given to a natural language generation system. We\nuse graph neural models because there is no explicit information about word\norder in a meaning represented by a graph. We try three different methods for\ntopic-focus articulation (TFA) employing graph neural models for a\nmeaning-to-text generation task. We propose a novel encoding strategy about\nnode aggregation in graph neural models, which instead of traditional encoding\nby aggregating adjacent node information, learns node representations by using\ndepth-first search. The results show our approach can get competitive\nperformance with state-of-art graph models on general text generation, and lead\nto significant improvements on the task of active-passive conversion compared\nto traditional adjacency-based aggregation strategies. Different types of TFA\ncan have a huge impact on the performance of the graph models.",
            "author": [
                "Chunliu Wang",
                "Rik van Noord",
                "Johan Bos"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02053v1",
                "http://arxiv.org/pdf/2310.02053v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02035v2",
            "title": "Caracterizaci\u00f3n sem\u00e1ntico-deductiva de la l\u00f3gica doble LD y los\n  gr\u00e1ficos existenciales Gamma-LD",
            "updated": "2023-10-12T14:04:48Z",
            "published": "2023-10-03T13:25:02Z",
            "summary": "This work presents the deductive system Double Propositional Logic, LD, along\nwith the semantics of possible worlds that characterize it. LD includes an\nalternate affirmation operator and an alternate negation operator and is not\nvalid for the principle of the excluded third when one of the disjunctions is\none of the alternate operators. The LD system includes as theorems, the\ntheorems of the Classic Propositional Logic LC. In LD, the intuitionist\nconnec-tives are recovered from the classics and the alternates, which implies\nthat, the LD system includes as theorems, the theorems of the Propositional\nIntuitionist Logic LI. It is rigorously and detailed to prove that theorems of\ncon-sistency, validity and completeness. It illustrates LD's ability to fix a\nversion of the liar's paradox, where LC and LI fail. Finally, the location of\nthe LD system is precisely determined in relation to the systems that made it\npossi-ble to intuit the structure of LD, such as, Basic Logic for the\nAristotelian Truth LBVA, the Logic of Tautologies LT and the Basic Logic for\nTruth and Falsehood LBVF. LD is characterized by Gamma-LD Existential Graphs,\nin the style of Peirce, so the semantics of possible worlds presented in this\nwork also characterizes Gamma-LD.",
            "author": [
                "Manuel Sierra Aristiz\u00e1bal"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02035v2",
                "http://arxiv.org/pdf/2310.02035v2"
            ],
            "primary_category": "math.LO",
            "category": [
                "math.LO",
                "03B60"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02031v4",
            "title": "OceanGPT: A Large Language Model for Ocean Science Tasks",
            "updated": "2023-10-25T10:55:38Z",
            "published": "2023-10-03T13:17:35Z",
            "summary": "Ocean science, which delves into the oceans that are reservoirs of life and\nbiodiversity, is of great significance given that oceans cover over 70% of our\nplanet's surface. Recently, advances in Large Language Models (LLMs) have\ntransformed the paradigm in science. Despite the success in other domains,\ncurrent LLMs often fall short in catering to the needs of domain experts like\noceanographers, and the potential of LLMs for ocean science is under-explored.\nThe intrinsic reason may be the immense and intricate nature of ocean data as\nwell as the necessity for higher granularity and richness in knowledge. To\nalleviate these issues, we introduce OceanGPT, the first-ever LLM in the ocean\ndomain, which is expert in various ocean science tasks. We propose DoInstruct,\na novel framework to automatically obtain a large volume of ocean domain\ninstruction data, which generates instructions based on multi-agent\ncollaboration. Additionally, we construct the first oceanography benchmark,\nOceanBench, to evaluate the capabilities of LLMs in the ocean domain. Though\ncomprehensive experiments, OceanGPT not only shows a higher level of knowledge\nexpertise for oceans science tasks but also gains preliminary embodied\nintelligence capabilities in ocean technology. Codes, data and checkpoints will\nsoon be available at https://github.com/zjunlp/KnowLM.",
            "author": [
                "Zhen Bi",
                "Ningyu Zhang",
                "Yida Xue",
                "Yixin Ou",
                "Daxiong Ji",
                "Guozhou Zheng",
                "Huajun Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02031v4",
                "http://arxiv.org/pdf/2310.02031v4"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.CE",
                "cs.LG",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02029v1",
            "title": "Between accurate prediction and poor decision making: the AI/ML gap",
            "updated": "2023-10-03T13:15:02Z",
            "published": "2023-10-03T13:15:02Z",
            "summary": "Intelligent agents rely on AI/ML functionalities to predict the consequence\nof possible actions and optimise the policy. However, the effort of the\nresearch community in addressing prediction accuracy has been so intense (and\nsuccessful) that it created the illusion that the more accurate the learner\nprediction (or classification) the better would have been the final decision.\nNow, such an assumption is valid only if the (human or artificial) decision\nmaker has complete knowledge of the utility of the possible actions. This paper\nargues that AI/ML community has taken so far a too unbalanced approach by\ndevoting excessive attention to the estimation of the state (or target)\nprobability to the detriment of accurate and reliable estimations of the\nutility. In particular, few evidence exists about the impact of a wrong utility\nassessment on the resulting expected utility of the decision strategy. This\nsituation is creating a substantial gap between the expectations and the\neffective impact of AI solutions, as witnessed by recent criticisms and\nemphasised by the regulatory legislative efforts. This paper aims to study this\ngap by quantifying the sensitivity of the expected utility to the utility\nuncertainty and comparing it to the one due to probability estimation.\nTheoretical and simulated results show that an inaccurate utility assessment\nmay as (and sometimes) more harmful than a poor probability estimation. The\nfinal recommendation to the community is then to undertake a focus shift from a\npure accuracy-driven (or obsessed) approach to a more utility-aware\nmethodology.",
            "author": [
                "Gianluca Bontempi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02029v1",
                "http://arxiv.org/pdf/2310.02029v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02027v2",
            "title": "DeepHGCN: Toward Deeper Hyperbolic Graph Convolutional Networks",
            "updated": "2023-10-04T18:39:26Z",
            "published": "2023-10-03T13:10:14Z",
            "summary": "Hyperbolic graph convolutional networks (HGCN) have demonstrated significant\npotential in extracting information from hierarchical graphs. However, existing\nHGCNs are limited to shallow architectures, due to the expensive hyperbolic\noperations and the over-smoothing issue as depth increases. Although in GCNs,\ntreatments have been applied to alleviate over-smoothing, developing a\nhyperbolic therapy presents distinct challenges since operations should be\ncarefully designed to fit the hyperbolic nature. Addressing the above\nchallenges, in this work, we propose DeepHGCN, the first deep multi-layer HGCN\narchitecture with dramatically improved computational efficiency and\nsubstantially alleviated over-smoothing effect. DeepHGCN presents two key\nenablers of deep HGCNs: (1) a novel hyperbolic feature transformation layer\nthat enables fast and accurate linear maps; and (2) Techniques such as\nhyperbolic residual connections and regularization for both weights and\nfeatures facilitated by an efficient hyperbolic midpoint method. Extensive\nexperiments demonstrate that DeepHGCN obtains significant improvements in link\nprediction and node classification tasks compared to both Euclidean and shallow\nhyperbolic GCN variants.",
            "author": [
                "Jiaxu Liu",
                "Xinping Yi",
                "Xiaowei Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02027v2",
                "http://arxiv.org/pdf/2310.02027v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02025v2",
            "title": "DeepZero: Scaling up Zeroth-Order Optimization for Deep Model Training",
            "updated": "2023-11-05T04:15:39Z",
            "published": "2023-10-03T13:05:36Z",
            "summary": "Zeroth-order (ZO) optimization has become a popular technique for solving\nmachine learning (ML) problems when first-order (FO) information is difficult\nor impossible to obtain. However, the scalability of ZO optimization remains an\nopen problem: Its use has primarily been limited to relatively small-scale ML\nproblems, such as sample-wise adversarial attack generation. To our best\nknowledge, no prior work has demonstrated the effectiveness of ZO optimization\nin training deep neural networks (DNNs) without a significant decrease in\nperformance. To overcome this roadblock, we develop DeepZero, a principled ZO\ndeep learning (DL) framework that can scale ZO optimization to DNN training\nfrom scratch through three primary innovations. First, we demonstrate the\nadvantages of coordinate-wise gradient estimation (CGE) over randomized\nvector-wise gradient estimation in training accuracy and computational\nefficiency. Second, we propose a sparsity-induced ZO training protocol that\nextends the model pruning methodology using only finite differences to explore\nand exploit the sparse DL prior in CGE. Third, we develop the methods of\nfeature reuse and forward parallelization to advance the practical\nimplementations of ZO training. Our extensive experiments show that DeepZero\nachieves state-of-the-art (SOTA) accuracy on ResNet-20 trained on CIFAR-10,\napproaching FO training performance for the first time. Furthermore, we show\nthe practical utility of DeepZero in applications of certified adversarial\ndefense and DL-based partial differential equation error correction, achieving\n10-20% improvement over SOTA. We believe our results will inspire future\nresearch on scalable ZO optimization and contribute to advancing DL with black\nbox.",
            "author": [
                "Aochuan Chen",
                "Yimeng Zhang",
                "Jinghan Jia",
                "James Diffenderfer",
                "Jiancheng Liu",
                "Konstantinos Parasyris",
                "Yihua Zhang",
                "Zheng Zhang",
                "Bhavya Kailkhura",
                "Sijia Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02025v2",
                "http://arxiv.org/pdf/2310.02025v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02017v1",
            "title": "DAGnabbit! Ensuring Consistency between Noise and Detection in\n  Hierarchical Bayesian Inference",
            "updated": "2023-10-03T12:46:07Z",
            "published": "2023-10-03T12:46:07Z",
            "summary": "Hierarchical Bayesian inference can simultaneously account for both\nmeasurement uncertainty and selection effects within astronomical catalogs. In\nparticular, the hierarchy imposed encodes beliefs about the interdependence of\nthe physical processes that generate the observed data. We show that several\nproposed approximations within the literature actually correspond to inferences\nthat are incompatible with any physical detection process, which can be\ndescribed by a directed acyclic graph (DAG). This generically leads to biases\nand is associated with the assumption that detectability is independent of the\nobserved data given the true source parameters. We show several examples of how\nthis error can affect astrophysical inferences based on catalogs of coalescing\nbinaries observed through gravitational waves, including misestimating the\nredshift evolution of the merger rate as well as incorrectly inferring that\nGeneral Relativity is the correct theory of gravity when it is not. In general,\none cannot directly fit for the ``detected distribution'' and ``divide out''\nthe selection effects in post-processing. Similarly, when comparing theoretical\npredictions to observations, it is better to simulate detected data (including\nboth measurement noise and selection effects) rather than comparing estimates\nof the detected distributions of event parameters (which include only selection\neffects). While the biases introduced by model misspecification from incorrect\nassumptions may be smaller than statistical uncertainty for moderate catalog\nsizes (O(100) events), they will nevertheless pose a significant barrier to\nprecision measurements of astrophysical populations.",
            "author": [
                "Reed Essick",
                "Maya Fishbach"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02017v1",
                "http://arxiv.org/pdf/2310.02017v1"
            ],
            "primary_category": "gr-qc",
            "category": [
                "gr-qc",
                "astro-ph.HE",
                "astro-ph.IM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02010v1",
            "title": "Rings of functions which are discontinuous on a finite set with\n  countable range",
            "updated": "2023-10-03T12:31:41Z",
            "published": "2023-10-03T12:31:41Z",
            "summary": "Consider the ring $C_c(X)_F$ of real valued functions which are discontinuous\non a finite set with countable range. We discuss $(\\mathcal{Z}_c)_F$-filters on\n$X$ and $(\\mathcal{Z}_c)_F$-ideals of $C_c(X)_F$. We establish an analogous\nversion of Gelfand-Kolmogoroff theorem in our setting. We prove some equivalent\nconditions when $C_c(X)_F$ is a Baer-ring and a regular ring. Lastly, we talk\nabout the zero divisor graph on $C_c(X)_F$.",
            "author": [
                "Achintya Singha",
                "D. Mandal",
                "Samir Ch Manda",
                "Sagarmoy Bag"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02010v1",
                "http://arxiv.org/pdf/2310.02010v1"
            ],
            "primary_category": "math.GN",
            "category": [
                "math.GN",
                "54C30 (Primary), 54C40 (Secondary)"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01960v1",
            "title": "Language Models as Knowledge Bases for Visual Word Sense Disambiguation",
            "updated": "2023-10-03T11:11:55Z",
            "published": "2023-10-03T11:11:55Z",
            "summary": "Visual Word Sense Disambiguation (VWSD) is a novel challenging task that lies\nbetween linguistic sense disambiguation and fine-grained multimodal retrieval.\nThe recent advancements in the development of visiolinguistic (VL) transformers\nsuggest some off-the-self implementations with encouraging results, which\nhowever we argue that can be further improved. To this end, we propose some\nknowledge-enhancement techniques towards improving the retrieval performance of\nVL transformers via the usage of Large Language Models (LLMs) as Knowledge\nBases. More specifically, knowledge stored in LLMs is retrieved with the help\nof appropriate prompts in a zero-shot manner, achieving performance\nadvancements. Moreover, we convert VWSD to a purely textual question-answering\n(QA) problem by considering generated image captions as multiple-choice\ncandidate answers. Zero-shot and few-shot prompting strategies are leveraged to\nexplore the potential of such a transformation, while Chain-of-Thought (CoT)\nprompting in the zero-shot setting is able to reveal the internal reasoning\nsteps an LLM follows to select the appropriate candidate. In total, our\npresented approach is the first one to analyze the merits of exploiting\nknowledge stored in LLMs in different ways to solve WVSD.",
            "author": [
                "Anastasia Kritharoula",
                "Maria Lymperaiou",
                "Giorgos Stamou"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01960v1",
                "http://arxiv.org/pdf/2310.01960v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01959v2",
            "title": "Beyond Labeling Oracles: What does it mean to steal ML models?",
            "updated": "2023-11-21T09:01:17Z",
            "published": "2023-10-03T11:10:21Z",
            "summary": "Model extraction attacks are designed to steal trained models with only query\naccess, as is often provided through APIs that ML-as-a-Service providers offer.\nML models are expensive to train, in part because data is hard to obtain, and a\nprimary incentive for model extraction is to acquire a model while incurring\nless cost than training from scratch. Literature on model extraction commonly\nclaims or presumes that the attacker is able to save on both data acquisition\nand labeling costs. We show that the attacker often does not. This is because\ncurrent attacks implicitly rely on the adversary being able to sample from the\nvictim model's data distribution. We thoroughly evaluate factors influencing\nthe success of model extraction. We discover that prior knowledge of the\nattacker, i.e. access to in-distribution data, dominates other factors like the\nattack policy the adversary follows to choose which queries to make to the\nvictim model API. Thus, an adversary looking to develop an equally capable\nmodel with a fixed budget has little practical incentive to perform model\nextraction, since for the attack to work they need to collect in-distribution\ndata, saving only on the cost of labeling. With low labeling costs in the\ncurrent market, the usefulness of such attacks is questionable. Ultimately, we\ndemonstrate that the effect of prior knowledge needs to be explicitly decoupled\nfrom the attack policy. To this end, we propose a benchmark to evaluate attack\npolicy directly.",
            "author": [
                "Avital Shafran",
                "Ilia Shumailov",
                "Murat A. Erdogdu",
                "Nicolas Papernot"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01959v2",
                "http://arxiv.org/pdf/2310.01959v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01946v1",
            "title": "CoralVOS: Dataset and Benchmark for Coral Video Segmentation",
            "updated": "2023-10-03T10:45:37Z",
            "published": "2023-10-03T10:45:37Z",
            "summary": "Coral reefs formulate the most valuable and productive marine ecosystems,\nproviding habitat for many marine species. Coral reef surveying and analysis\nare currently confined to coral experts who invest substantial effort in\ngenerating comprehensive and dependable reports (\\emph{e.g.}, coral coverage,\npopulation, spatial distribution, \\textit{etc}), from the collected survey\ndata. However, performing dense coral analysis based on manual efforts is\nsignificantly time-consuming, the existing coral analysis algorithms compromise\nand opt for performing down-sampling and only conducting sparse point-based\ncoral analysis within selected frames. However, such down-sampling will\n\\textbf{inevitable} introduce the estimation bias or even lead to wrong\nresults. To address this issue, we propose to perform \\textbf{dense coral video\nsegmentation}, with no down-sampling involved. Through video object\nsegmentation, we could generate more \\textit{reliable} and \\textit{in-depth}\ncoral analysis than the existing coral reef analysis algorithms. To boost such\ndense coral analysis, we propose a large-scale coral video segmentation\ndataset: \\textbf{CoralVOS} as demonstrated in Fig. 1. To the best of our\nknowledge, our CoralVOS is the first dataset and benchmark supporting dense\ncoral video segmentation. We perform experiments on our CoralVOS dataset,\nincluding 6 recent state-of-the-art video object segmentation (VOS) algorithms.\nWe fine-tuned these VOS algorithms on our CoralVOS dataset and achieved\nobservable performance improvement. The results show that there is still great\npotential for further promoting the segmentation accuracy. The dataset and\ntrained models will be released with the acceptance of this work to foster the\ncoral reef research community.",
            "author": [
                "Zheng Ziqiang",
                "Xie Yaofeng",
                "Liang Haixin",
                "Yu Zhibin",
                "Sai-Kit Yeung"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01946v1",
                "http://arxiv.org/pdf/2310.01946v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.03617v1",
            "title": "RouteKG: A knowledge graph-based framework for route prediction on road\n  networks",
            "updated": "2023-10-03T10:40:35Z",
            "published": "2023-10-03T10:40:35Z",
            "summary": "Short-term route prediction on road networks allows us to anticipate the\nfuture trajectories of road users, enabling a plethora of intelligent\ntransportation applications such as dynamic traffic control or personalized\nroute recommendation. Despite the recent advances in this area, existing\nmethods focus primarily on learning sequential patterns, neglecting the\ninherent spatial structure in road networks that can affect human routing\ndecisions. To fill the gap, this paper introduces RouteKG, a novel Knowledge\nGraph-based framework for route prediction. Specifically, we construct a\nKnowledge Graph on the road network, thereby learning and leveraging spatial\nrelations, especially moving directions, which are crucial for human\nnavigation. Moreover, an n-ary tree-based algorithm is introduced to\nefficiently generate top-K routes in a batch mode, enhancing scalability and\ncomputational efficiency. To further optimize the prediction performance, a\nrank refinement module is incorporated to fine-tune the candidate route\nrankings. The model performance is evaluated using two real-world vehicle\ntrajectory datasets from two Chinese cities, Chengdu and Shanghai, under\nvarious practical scenarios. The results demonstrate a significant improvement\nin accuracy over baseline methods, with an average increase of 6.2%, 7.8%, and\n6.1% in top-1, 5, and 10 routes predictions, respectively. We further validate\nour model through a case study that utilizes the pretrained model as a\nsimulator for real-time traffic flow estimation at the link level. The proposed\nRouteKG promises wide-ranging applications in vehicle navigation, traffic\nmanagement, and other intelligent transportation tasks.",
            "author": [
                "Yihong Tang",
                "Weipeng Deng",
                "Shuyu Lei",
                "Yuebing Liang",
                "Zhenliang Ma",
                "Zhan Zhao"
            ],
            "link": [
                "http://arxiv.org/abs/2310.03617v1",
                "http://arxiv.org/pdf/2310.03617v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01941v1",
            "title": "Bandwidth of Timed Automata: 3 Classes",
            "updated": "2023-10-03T10:37:59Z",
            "published": "2023-10-03T10:37:59Z",
            "summary": "Timed languages contain sequences of discrete events (\"letters'') separated\nby real-valued delays, they can be recognized by timed automata, and represent\nbehaviors of various real-time systems. The notion of bandwidth of a timed\nlanguage defined in a previous paper characterizes the amount of information\nper time unit, encoded in words of the language observed with some precision\n{\\epsilon}.\n  In this paper, we identify three classes of timed automata according to the\nasymptotics of the bandwidth of their languages with respect to this precision\n{\\epsilon}: automata are either meager, with an O(1) bandwidth, normal, with a\n{\\Theta}(log (1/{\\epsilon})) bandwidth, or obese, with {\\Theta}(1/{\\epsilon})\nbandwidth. We define two structural criteria and prove that they partition\ntimed automata into these three classes of bandwidth, implying that there are\nno intermediate asymptotic classes. The classification problem of a timed\nautomaton is PSPACE-complete.\n  Both criteria are formulated using morphisms from paths of the timed\nautomaton to some finite monoids extending Puri's orbit graphs; the proofs are\nbased on Simon's factorization forest theorem.",
            "author": [
                "Eugene Asarin",
                "Aldric Degorre",
                "Catalin Dima",
                "Bernardo Jacobo Inclan"
            ],
            "link": [
                "http://dx.doi.org/10.4230/LIPIcs.FSTTCS.2023",
                "http://arxiv.org/abs/2310.01941v1",
                "http://arxiv.org/pdf/2310.01941v1"
            ],
            "primary_category": "cs.FL",
            "category": [
                "cs.FL",
                "cs.IT",
                "math.IT",
                "68Q70, 68Q45, 68P30",
                "F.4.3; E.4; F.1.1"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01939v1",
            "title": "Optimizing microlens arrays for incoherent HiLo microscopy",
            "updated": "2023-10-03T10:35:24Z",
            "published": "2023-10-03T10:35:24Z",
            "summary": "HiLo microscopy is a powerful, low-cost, and easily configurable technique\nfor acquiring high-contrast optically-sectioned images. However, traditional\nHiLo microscopes are based on coherent light sources with diffusive glass\nplates or incoherent light sources with digital mirror devices (DMD) and\nspatial light modulators (SLM), which are more expensive. Here, we propose a\nnew low-cost HiLo microscopy technique using MLAs and incoherent LED light\nsources. We simulated structured illumination (SI) patterns and HiLo image\ngeneration based on Fresnel diffraction and incoherent imaging. To observe how\nMLAs affect HiLo images, we used three common MLAs with specific microlens\npitch and numerical aperture (NA) to generate periodic illumination patterns.\nAccording to our simulations, using MLAs and incoherent light sources can\nenhance the image contrast compared with a traditional widefield fluorescence\nmicroscope. We found that the MLA NA does not significantly affect HiLo images.\nA larger lens pitch can bring a higher image contrast. However, there is an\noptimized lens pitch. If the lens pitch is too high, artefacts are observed in\nHiLo images. To our knowledge, this is the first numerical study about\nMLA-based HiLo microscopy. This study can benefit researchers using MLAs and\nincoherent light sources to configure their low-cost HiLo microscopes.",
            "author": [
                "Ziao Jiao",
                "Xi Chen",
                "David Day Uei Li"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01939v1",
                "http://arxiv.org/pdf/2310.01939v1"
            ],
            "primary_category": "physics.optics",
            "category": [
                "physics.optics"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01936v1",
            "title": "Constructing Image-Text Pair Dataset from Books",
            "updated": "2023-10-03T10:23:28Z",
            "published": "2023-10-03T10:23:28Z",
            "summary": "Digital archiving is becoming widespread owing to its effectiveness in\nprotecting valuable books and providing knowledge to many people\nelectronically. In this paper, we propose a novel approach to leverage digital\narchives for machine learning. If we can fully utilize such digitized data,\nmachine learning has the potential to uncover unknown insights and ultimately\nacquire knowledge autonomously, just like humans read books. As a first step,\nwe design a dataset construction pipeline comprising an optical character\nreader (OCR), an object detector, and a layout analyzer for the autonomous\nextraction of image-text pairs. In our experiments, we apply our pipeline on\nold photo books to construct an image-text pair dataset, showing its\neffectiveness in image-text retrieval and insight extraction.",
            "author": [
                "Yamato Okamoto",
                "Haruto Toyonaga",
                "Yoshihisa Ijiri",
                "Hirokatsu Kataoka"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01936v1",
                "http://arxiv.org/pdf/2310.01936v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01935v1",
            "title": "Who is the Audience? Designing Casual Data Visualizations for the\n  'General Public'",
            "updated": "2023-10-03T10:18:02Z",
            "published": "2023-10-03T10:18:02Z",
            "summary": "Casual data visualizations play a vital role in communicating data to lay\naudiences. Despite this, little is known about how data visualization\npractitioners make design decisions based on their envisioned target audiences\nusing different media channels. We draw on the findings of a semi-structured\ninterview study to explore how data visualization practitioners working in\nvarious settings conceptualize and design for lay audiences and how they\nevaluate their visualization designs. Our findings suggest that practitioners\noften use broad definitions of their target audience, yet they stress the\nimportance of 'knowing the readers' for their design decisions. At the same\ntime, commonly used evaluation and feedback mechanisms do not allow a deep\nknowledge of their readers but rely instead on tacit knowledge, simple usage\nmetrics, or testing with colleagues. We conclude by calling for different forms\nof visualization evaluation that are feasible for practitioners to implement in\ntheir daily workflows.",
            "author": [
                "Regina Schuster",
                "Laura Koesten",
                "Torsten M\u00f6ller",
                "Kathleen Gregory"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01935v1",
                "http://arxiv.org/pdf/2310.01935v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01931v1",
            "title": "MarineDet: Towards Open-Marine Object Detection",
            "updated": "2023-10-03T10:13:42Z",
            "published": "2023-10-03T10:13:42Z",
            "summary": "Marine object detection has gained prominence in marine research, driven by\nthe pressing need to unravel oceanic mysteries and enhance our understanding of\ninvaluable marine ecosystems. There is a profound requirement to efficiently\nand accurately identify and localize diverse and unseen marine entities within\nunderwater imagery. The open-marine object detection (OMOD for short) is\nrequired to detect diverse and unseen marine objects, performing categorization\nand localization simultaneously. To achieve OMOD, we present\n\\textbf{MarineDet}. We formulate a joint visual-text semantic space through\npre-training and then perform marine-specific training to achieve\nin-air-to-marine knowledge transfer. Considering there is no specific dataset\ndesigned for OMOD, we construct a \\textbf{MarineDet dataset} consisting of 821\nmarine-relative object categories to promote and measure OMOD performance. The\nexperimental results demonstrate the superior performance of MarineDet over\nexisting generalist and specialist object detection algorithms. To the best of\nour knowledge, we are the first to present OMOD, which holds a more valuable\nand practical setting for marine ecosystem monitoring and management. Our\nresearch not only pushes the boundaries of marine understanding but also offers\na standard pipeline for OMOD.",
            "author": [
                "Liang Haixin",
                "Zheng Ziqiang",
                "Ma Zeyu",
                "Sai-Kit Yeung"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01931v1",
                "http://arxiv.org/pdf/2310.01931v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01929v2",
            "title": "Navigating Cultural Chasms: Exploring and Unlocking the Cultural POV of\n  Text-To-Image Models",
            "updated": "2023-11-29T15:11:02Z",
            "published": "2023-10-03T10:13:36Z",
            "summary": "Text-To-Image (TTI) models, such as DALL-E and StableDiffusion, have\ndemonstrated remarkable prompt-based image generation capabilities.\nMultilingual encoders may have a substantial impact on the cultural agency of\nthese models, as language is a conduit of culture. In this study, we explore\nthe cultural perception embedded in TTI models by characterizing culture across\nthree hierarchical tiers: cultural dimensions, cultural domains, and cultural\nconcepts. Based on this ontology, we derive prompt templates to unlock the\ncultural knowledge in TTI models, and propose a comprehensive suite of\nevaluation techniques, including intrinsic evaluations using the CLIP space,\nextrinsic evaluations with a Visual-Question-Answer (VQA) model and human\nassessments, to evaluate the cultural content of TTI-generated images. To\nbolster our research, we introduce the CulText2I dataset, derived from four\ndiverse TTI models and spanning ten languages. Our experiments provide insights\nregarding Do, What, Which and How research questions about the nature of\ncultural encoding in TTI models, paving the way for cross-cultural applications\nof these models.",
            "author": [
                "Mor Ventura",
                "Eyal Ben-David",
                "Anna Korhonen",
                "Roi Reichart"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01929v2",
                "http://arxiv.org/pdf/2310.01929v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01930v1",
            "title": "A Distributed Multi-Robot Framework for Exploration, Information\n  Acquisition and Consensus",
            "updated": "2023-10-03T10:13:36Z",
            "published": "2023-10-03T10:13:36Z",
            "summary": "The distributed coordination of robot teams performing complex tasks is\nchallenging to formulate. The different aspects of a complete task such as\nlocal planning for obstacle avoidance, global goal coordination and\ncollaborative mapping are often solved separately, when clearly each of these\nshould influence the others for the most efficient behaviour. In this paper we\nuse the example application of distributed information acquisition as a robot\nteam explores a large space to show that we can formulate the whole problem as\na single factor graph with multiple connected layers representing each aspect.\nWe use Gaussian Belief Propagation (GBP) as the inference mechanism, which\npermits parallel, on-demand or asynchronous computation for efficiency when\ndifferent aspects are more or less important. This is the first time that a\ndistributed GBP multi-robot solver has been proven to enable intelligent\ncollaborative behaviour rather than just guiding robots to individual, selfish\ngoals. We encourage the reader to view our demos at\nhttps://aalpatya.github.io/gbpstack",
            "author": [
                "Aalok Patwardhan",
                "Andrew J. Davison"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01930v1",
                "http://arxiv.org/pdf/2310.01930v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01928v1",
            "title": "Smartphone-based Optical Sectioning (SOS) Microscopy with A Telecentric\n  Design for Fluorescence Imaging",
            "updated": "2023-10-03T10:11:56Z",
            "published": "2023-10-03T10:11:56Z",
            "summary": "We proposed a Smartphone-based Optical Sectioning (SOS) microscope based on\nthe HiLo technique, with a single smartphone replacing a high-cost illumination\nsource and a camera sensor.We built our SOS with off-the-shelf optical\nmechanical cage systems with 3D-printed adapters to integrate the smartphone\nwith the SOS main body seamlessly.The liquid light guide can be integrated with\nthe adapter, guiding the smartphone LED light to the digital mirror device with\nneglectable loss.We used an electrically tunable lens (ETL) instead of a\nmechanical translation stage to realize low-cost axial scanning. The ETL was\nconjugated to the objective lens back pupil plane (BPP) to construct a\ntelecentric design by a 4f configuration. This can exempt images of different\nlayers from the variation in magnification. SOS has a 571.5 {\\mu}m telecentric\nscanning range and an 11.7 {\\mu}m axial resolution. The broadband smartphone\nLED torch can effectively excite fluorescent polystyrene (PS) beads. We\nsuccessfully used SOS for high contrast fluorescent PS beads imaging with\ndifferent wavelengths and optical sectioning imaging of accumulated fluorescent\nPS beads. To our knowledge, the proposed SOS is the first smartphone-based HiLo\noptical sectioning microscopy. It is a powerful, low-cost tool for biomedical\nresearch in resource-limited areas.",
            "author": [
                "Ziao Jiao",
                "Mingliang Pan",
                "Khadija Yousaf",
                "Daniel Doveiko",
                "Michelle Maclean",
                "David Griffin",
                "Yu Chen",
                "David Day Uei Lia"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01928v1",
                "http://arxiv.org/pdf/2310.01928v1"
            ],
            "primary_category": "physics.optics",
            "category": [
                "physics.optics",
                "physics.bio-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02295v1",
            "title": "Unsupervised Complex Semi-Binary Matrix Factorization for Activation\n  Sequence Recovery of Quasi-Stationary Sources",
            "updated": "2023-10-03T09:29:16Z",
            "published": "2023-10-03T09:29:16Z",
            "summary": "Advocating for a sustainable, resilient and human-centric industry, the three\npillars of Industry 5.0 call for an increased understanding of industrial\nprocesses and manufacturing systems, as well as their energy sustainability.\nOne of the most fundamental elements of comprehension is knowing when the\nsystems are operated, as this is key to locating energy intensive subsystems\nand operations. Such knowledge is often lacking in practice. Activation\nstatuses can be recovered from sensor data though. Some non-intrusive sensors\n(accelerometers, current sensors, etc.) acquire mixed signals containing\ninformation about multiple actuators at once. Despite their low cost as regards\nthe fleet of systems they monitor, additional signal processing is required to\nextract the individual activation sequences. To that end, sparse regression\ntechniques can extract leading dynamics in sequential data. Notorious\ndictionary learning algorithms have proven effective in this regard. This paper\nconsiders different industrial settings in which the identification of binary\nsubsystem activation sequences is sought. In this context, it is assumed that\neach sensor measures an extensive physical property, source signals are\nperiodic, quasi-stationary and independent, albeit these signals may be\ncorrelated and their noise distribution is arbitrary. Existing methods either\nrestrict these assumptions, e.g., by imposing orthogonality or noise\ncharacteristics, or lift them using additional assumptions, typically using\nnonlinear transforms.",
            "author": [
                "Romain Delabeye",
                "Martin Ghienne",
                "Olivia Penas",
                "Jean-Luc Dion"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02295v1",
                "http://arxiv.org/pdf/2310.02295v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02294v1",
            "title": "Beyond-Accuracy: A Review on Diversity, Serendipity and Fairness in\n  Recommender Systems Based on Graph Neural Networks",
            "updated": "2023-10-03T09:25:01Z",
            "published": "2023-10-03T09:25:01Z",
            "summary": "By providing personalized suggestions to users, recommender systems have\nbecome essential to numerous online platforms. Collaborative filtering,\nparticularly graph-based approaches using Graph Neural Networks (GNNs), have\ndemonstrated great results in terms of recommendation accuracy. However,\naccuracy may not always be the most important criterion for evaluating\nrecommender systems' performance, since beyond-accuracy aspects such as\nrecommendation diversity, serendipity, and fairness can strongly influence user\nengagement and satisfaction. This review paper focuses on addressing these\ndimensions in GNN-based recommender systems, going beyond the conventional\naccuracy-centric perspective. We begin by reviewing recent developments in\napproaches that improve not only the accuracy-diversity trade-off but also\npromote serendipity and fairness in GNN-based recommender systems. We discuss\ndifferent stages of model development including data preprocessing, graph\nconstruction, embedding initialization, propagation layers, embedding fusion,\nscore computation, and training methodologies. Furthermore, we present a look\ninto the practical difficulties encountered in assuring diversity, serendipity,\nand fairness, while retaining high accuracy. Finally, we discuss potential\nfuture research directions for developing more robust GNN-based recommender\nsystems that go beyond the unidimensional perspective of focusing solely on\naccuracy. This review aims to provide researchers and practitioners with an\nin-depth understanding of the multifaceted issues that arise when designing\nGNN-based recommender systems, setting our work apart by offering a\ncomprehensive exploration of beyond-accuracy dimensions.",
            "author": [
                "Tomislav Duricic",
                "Dominik Kowald",
                "Emanuel Lacic",
                "Elisabeth Lex"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02294v1",
                "http://arxiv.org/pdf/2310.02294v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01905v3",
            "title": "Domain-Driven Design in Software Development: A Systematic Literature\n  Review on Implementation, Challenges, and Effectiveness",
            "updated": "2023-11-09T10:58:46Z",
            "published": "2023-10-03T09:22:53Z",
            "summary": "Context: Domain-Driven Design (DDD) addresses software challenges, gaining\nattention for refactoring, reimplementation, and adoption. It centers on domain\nknowledge to solve complex business problems. Objective: This Systematic\nLiterature Review (SLR) analyzes DDD research in software development to assess\nits effectiveness in solving architecture problems, identify challenges, and\nexplore outcomes. Method: We selected 36 peer-reviewed studies and conducted\nquantitative and qualitative analysis. Results: DDD effectively improved\nsoftware systems, emphasizing Ubiquitous Language, Bounded Context, and Domain\nEvents. DDD in microservices gained prominence for system decomposition. Some\nstudies lacked empirical evaluations, identifying challenges in onboarding and\nexpertise. Conclusion: Adopting DDD benefits software development, involving\nstakeholders like engineers, architects, managers, and domain experts. More\nempirical evaluations and open discussions on challenges are needed.\nCollaboration between academia and industry advances DDD adoption and knowledge\ntransfer in projects.",
            "author": [
                "Ozan \u00d6zkan",
                "\u00d6nder Babur",
                "Mark van den Brand"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01905v3",
                "http://arxiv.org/pdf/2310.01905v3"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01902v2",
            "title": "On the cardinality and dimension of the slices of Okamoto's functions",
            "updated": "2023-11-03T14:54:15Z",
            "published": "2023-10-03T09:19:25Z",
            "summary": "The graphs of Okamoto's functions, denoted by $K_q$, are self-affine fractal\ncurves contained in $[0,1]^2$, parameterised by $q \\in (1,2)$. In this paper we\nconsider the cardinality and dimension of the intersection of these curves with\nhorizontal lines. Our first theorem proves that if $q$ is sufficiently close to\n$2$, then $K_q$ admits a horizontal slice with exactly three elements. Our\nsecond theorem proves that if a horizontal slice of $K_q$ contains an\nuncountable number of elements then it has positive Hausdorff dimension\nprovided $q$ is in a certain subset of $(1,2)$. Finally, we prove that if $q$\nis a $k$-Bonacci number for some $k \\in \\mathbb{N}_{\\geq 3}$, then the set of\n$y \\in [0,1]$ such that the horizontal slice at height $y$ has $(2m+1)$\nelements has positive Hausdorff dimension for any $m \\in \\mathbb{N}$. We also\nshow that, under the same assumption on $q$, there is some horizontal slice\nwhose cardinality is countably infinite.",
            "author": [
                "Simon Baker",
                "George Bender"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01902v2",
                "http://arxiv.org/pdf/2310.01902v2"
            ],
            "primary_category": "math.DS",
            "category": [
                "math.DS",
                "math.CA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04448v1",
            "title": "Fault Trees, Decision Trees, And Binary Decision Diagrams: A Systematic\n  Comparison",
            "updated": "2023-10-03T09:12:49Z",
            "published": "2023-10-03T09:12:49Z",
            "summary": "In reliability engineering, we need to understand system dependencies,\ncause-effect relations, identify critical components, and analyze how they\ntrigger failures. Three prominent graph models commonly used for these purposes\nare fault trees (FTs), decision trees (DTs), and binary decision diagrams\n(BDDs). These models are popular because they are easy to interpret, serve as a\ncommunication tool between stakeholders of various backgrounds, and support\ndecision-making processes. Moreover, these models help to understand real-world\nproblems by computing reliability metrics, minimum cut sets, logic rules, and\ndisplaying dependencies. Nevertheless, it is unclear how these graph models\ncompare. Thus, the goal of this paper is to understand the similarities and\ndifferences through a systematic comparison based on their (i) purpose and\napplication, (ii) structural representation, (iii) analysis methods, (iv)\nconstruction, and (v) benefits & limitations. Furthermore, we use a running\nexample based on a Container Seal Design to showcase the models in practice.\nOur results show that, given that FTs, DTs and BDDs have different purposes and\napplication domains, they adopt different structural representations and\nanalysis methodologies that entail a variety of benefits and limitations, the\nlatter can be addressed via conversion methods or extensions. Specific remarks\nare that BDDs can be considered as a compact representation of binary DTs,\nsince the former allows sub-node sharing, which makes BDDs more efficient at\nrepresenting logical rules than binary DTs. It is possible to obtain cut sets\nfrom BDDs and DTs and construct a FT using the (con/dis)junctive normal form,\nalthough this may result in a sub-optimal FT structure.",
            "author": [
                "L. A. Jimenez-Roa",
                "T. Heskes",
                "M. Stoelinga"
            ],
            "link": [
                "http://dx.doi.org/10.3850/978-981-18-2016-8_241-cd",
                "http://arxiv.org/abs/2310.04448v1",
                "http://arxiv.org/pdf/2310.04448v1"
            ],
            "primary_category": "cs.OH",
            "category": [
                "cs.OH",
                "cs.LO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01883v1",
            "title": "Reduction and efficient solution of MILP models of mixed Hamming\n  packings yielding improved upper bounds",
            "updated": "2023-10-03T08:37:18Z",
            "published": "2023-10-03T08:37:18Z",
            "summary": "Mixed Hamming packings are considered: the maximal cardinality given a\nminimum codeword Hamming distance of mixed codes is addressed via mixed integer\nprogramming models. Adopting the concept of contact graph from classical\ncontinuous sphere packing problems, a reduction technique for the models is\nintroduced, which enables their efficient solution. Several best known upper\nbounds are improved and some of them are found to be sharp.",
            "author": [
                "P\u00e9ter Naszvadi",
                "M\u00e1ty\u00e1s Koniorczyk"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01883v1",
                "http://arxiv.org/pdf/2310.01883v1"
            ],
            "primary_category": "cs.DM",
            "category": [
                "cs.DM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01846v1",
            "title": "Benchmarking and Improving Generator-Validator Consistency of Language\n  Models",
            "updated": "2023-10-03T07:23:22Z",
            "published": "2023-10-03T07:23:22Z",
            "summary": "As of September 2023, ChatGPT correctly answers \"what is 7+8\" with 15, but\nwhen asked \"7+8=15, True or False\" it responds with \"False\". This inconsistency\nbetween generating and validating an answer is prevalent in language models\n(LMs) and erodes trust. In this paper, we propose a framework for measuring the\nconsistency between generation and validation (which we call\ngenerator-validator consistency, or GV-consistency), finding that even GPT-4, a\nstate-of-the-art LM, is GV-consistent only 76% of the time. To improve the\nconsistency of LMs, we propose to finetune on the filtered generator and\nvalidator responses that are GV-consistent, and call this approach consistency\nfine-tuning. We find that this approach improves GV-consistency of Alpaca-30B\nfrom 60% to 93%, and the improvement extrapolates to unseen tasks and domains\n(e.g., GV-consistency for positive style transfers extrapolates to unseen\nstyles like humor). In addition to improving consistency, consistency\nfine-tuning improves both generator quality and validator accuracy without\nusing any labeled data. Evaluated across 6 tasks, including math questions,\nknowledge-intensive QA, and instruction following, our method improves the\ngenerator quality by 16% and the validator accuracy by 6.3% across all tasks.",
            "author": [
                "Xiang Lisa Li",
                "Vaishnavi Shrivastava",
                "Siyan Li",
                "Tatsunori Hashimoto",
                "Percy Liang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01846v1",
                "http://arxiv.org/pdf/2310.01846v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01844v1",
            "title": "Semi-Aerodynamic Model Aided Invariant Kalman Filtering for UAV\n  Full-State Estimation",
            "updated": "2023-10-03T07:19:12Z",
            "published": "2023-10-03T07:19:12Z",
            "summary": "Due to the state trajectory-independent features of invariant Kalman\nfiltering (InEKF), it has attracted widespread attention in the research\ncommunity for its significantly improved state estimation accuracy and\nconvergence under disturbance. In this paper, we formulate the full-source data\nfusion navigation problem for fixed-wing unmanned aerial vehicle (UAV) within a\nframework based on error state right-invariant extended Kalman filtering\n(ES-RIEKF) on Lie groups. We merge measurements from a multi-rate onboard\nsensor network on UAVs to achieve real-time estimation of pose, air flow\nangles, and wind speed. Detailed derivations are provided, and the algorithm's\nconvergence and accuracy improvements over established methods like Error State\nEKF (ES-EKF) and Nonlinear Complementary Filter (NCF) are demonstrated using\nreal-flight data from UAVs. Additionally, we introduce a semi-aerodynamic model\nfusion framework that relies solely on ground-measurable parameters. We design\nand train an Long Short Term Memory (LSTM) deep network to achieve drift-free\nprediction of the UAV's angle of attack (AOA) and side-slip angle (SA) using\neasily obtainable onboard data like control surface deflections, thereby\nsignificantly reducing dependency on GNSS or complicated aerodynamic model\nparameters. Further, we validate the algorithm's robust advantages under GNSS\ndenied, where flight data shows that the maximum positioning error stays within\n30 meters over a 130-second denial period. To the best of our knowledge, this\nstudy is the first to apply ES-RIEKF to full-source navigation applications for\nfixed-wing UAVs, aiming to provide engineering references for designers. Our\nimplementations using MATLAB/Simulink will open source.",
            "author": [
                "Xiaoyu Ye",
                "Fujun Song",
                "Zongyu Zhang",
                "Rui Zhang",
                "Qinghua Zeng"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01844v1",
                "http://arxiv.org/pdf/2310.01844v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01842v1",
            "title": "SelfGraphVQA: A Self-Supervised Graph Neural Network for Scene-based\n  Question Answering",
            "updated": "2023-10-03T07:14:53Z",
            "published": "2023-10-03T07:14:53Z",
            "summary": "The intersection of vision and language is of major interest due to the\nincreased focus on seamless integration between recognition and reasoning.\nScene graphs (SGs) have emerged as a useful tool for multimodal image analysis,\nshowing impressive performance in tasks such as Visual Question Answering\n(VQA). In this work, we demonstrate that despite the effectiveness of scene\ngraphs in VQA tasks, current methods that utilize idealized annotated scene\ngraphs struggle to generalize when using predicted scene graphs extracted from\nimages. To address this issue, we introduce the SelfGraphVQA framework. Our\napproach extracts a scene graph from an input image using a pre-trained scene\ngraph generator and employs semantically-preserving augmentation with\nself-supervised techniques. This method improves the utilization of graph\nrepresentations in VQA tasks by circumventing the need for costly and\npotentially biased annotated data. By creating alternative views of the\nextracted graphs through image augmentations, we can learn joint embeddings by\noptimizing the informational content in their representations using an\nun-normalized contrastive approach. As we work with SGs, we experiment with\nthree distinct maximization strategies: node-wise, graph-wise, and\npermutation-equivariant regularization. We empirically showcase the\neffectiveness of the extracted scene graph for VQA and demonstrate that these\napproaches enhance overall performance by highlighting the significance of\nvisual information. This offers a more practical solution for VQA tasks that\nrely on SGs for complex reasoning questions.",
            "author": [
                "Bruno Souza",
                "Marius Aasan",
                "Helio Pedrini",
                "Ad\u00edn Ram\u00edrez Rivera"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01842v1",
                "http://arxiv.org/pdf/2310.01842v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01820v1",
            "title": "Towards Robust Fidelity for Evaluating Explainability of Graph Neural\n  Networks",
            "updated": "2023-10-03T06:25:14Z",
            "published": "2023-10-03T06:25:14Z",
            "summary": "Graph Neural Networks (GNNs) are neural models that leverage the dependency\nstructure in graphical data via message passing among the graph nodes. GNNs\nhave emerged as pivotal architectures in analyzing graph-structured data, and\ntheir expansive application in sensitive domains requires a comprehensive\nunderstanding of their decision-making processes -- necessitating a framework\nfor GNN explainability. An explanation function for GNNs takes a pre-trained\nGNN along with a graph as input, to produce a `sufficient statistic' subgraph\nwith respect to the graph label. A main challenge in studying GNN\nexplainability is to provide fidelity measures that evaluate the performance of\nthese explanation functions. This paper studies this foundational challenge,\nspotlighting the inherent limitations of prevailing fidelity metrics, including\n$Fid_+$, $Fid_-$, and $Fid_\\Delta$. Specifically, a formal,\ninformation-theoretic definition of explainability is introduced and it is\nshown that existing metrics often fail to align with this definition across\nvarious statistical scenarios. The reason is due to potential distribution\nshifts when subgraphs are removed in computing these fidelity measures.\nSubsequently, a robust class of fidelity measures are introduced, and it is\nshown analytically that they are resilient to distribution shift issues and are\napplicable in a wide range of scenarios. Extensive empirical analysis on both\nsynthetic and real datasets are provided to illustrate that the proposed\nmetrics are more coherent with gold standard metrics.",
            "author": [
                "Xu Zheng",
                "Farhad Shirani",
                "Tianchun Wang",
                "Wei Cheng",
                "Zhuomin Chen",
                "Haifeng Chen",
                "Hua Wei",
                "Dongsheng Luo"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01820v1",
                "http://arxiv.org/pdf/2310.01820v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01811v1",
            "title": "The Laplacian spectral moments of power hypergraphs",
            "updated": "2023-10-03T05:54:53Z",
            "published": "2023-10-03T05:54:53Z",
            "summary": "The $d$-th order Laplacian spectral moment of a $k$-uniform hypergraph is the\nsum of the $d$-th powers of all eigenvalues of its Laplacian tensor. In this\npaper, we obtain some expressions of the Laplacian spectral moments for\n$k$-uniform power hypergraphs, and these expressions can be represented by some\nparameters of graphs. And we show that some graphs can be determined by their\nhigh-order Laplacian spectrum by using the Laplacian spectral moments of power\nhypergraphs.",
            "author": [
                "Jueru Liu",
                "Lixiang Chen",
                "Changjiang Bu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01811v1",
                "http://arxiv.org/pdf/2310.01811v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01799v2",
            "title": "SMRD: SURE-based Robust MRI Reconstruction with Diffusion Models",
            "updated": "2023-10-18T20:19:53Z",
            "published": "2023-10-03T05:05:35Z",
            "summary": "Diffusion models have recently gained popularity for accelerated MRI\nreconstruction due to their high sample quality. They can effectively serve as\nrich data priors while incorporating the forward model flexibly at inference\ntime, and they have been shown to be more robust than unrolled methods under\ndistribution shifts. However, diffusion models require careful tuning of\ninference hyperparameters on a validation set and are still sensitive to\ndistribution shifts during testing. To address these challenges, we introduce\nSURE-based MRI Reconstruction with Diffusion models (SMRD), a method that\nperforms test-time hyperparameter tuning to enhance robustness during testing.\nSMRD uses Stein's Unbiased Risk Estimator (SURE) to estimate the mean squared\nerror of the reconstruction during testing. SURE is then used to automatically\ntune the inference hyperparameters and to set an early stopping criterion\nwithout the need for validation tuning. To the best of our knowledge, SMRD is\nthe first to incorporate SURE into the sampling stage of diffusion models for\nautomatic hyperparameter selection. SMRD outperforms diffusion model baselines\non various measurement noise levels, acceleration factors, and anatomies,\nachieving a PSNR improvement of up to 6 dB under measurement noise. The code is\npublicly available at https://github.com/NVlabs/SMRD .",
            "author": [
                "Batu Ozturkler",
                "Chao Liu",
                "Benjamin Eckart",
                "Morteza Mardani",
                "Jiaming Song",
                "Jan Kautz"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01799v2",
                "http://arxiv.org/pdf/2310.01799v2"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01796v1",
            "title": "LLMParser: A LLM-based Log Parsing Framework",
            "updated": "2023-10-03T04:46:59Z",
            "published": "2023-10-03T04:46:59Z",
            "summary": "The process of log parsing, which converts log messages into structured\nformats, is a crucial step for various log analysis tasks. Although numerous\nlog parsers have been proposed, their effectiveness on complex log data is\noften hindered due to reliance on human-made rules or learning-based models\nwith limited training data. The recent rise of powerful large language models\n(LLMs) shows potential for log parsing due to their extensive pre-trained\nknowledge related to code and logging. However, their accuracy is currently\nlimited due to the lack of specialized log parsing capabilities. Additionally,\nthe inconsistency of their answers and significant overhead obstruct the\npractical implementation of LLM-based log parsing.\n  To tackle these challenges, we introduce LLMParser, the first practical\nLLM-based log parsing framework. LLMParser enables accurate and robust log\nparsing by leveraging the in-context learning (ICL) capability of the LLM,\nemploying a hierarchical candidate sampling algorithm, and selecting\nhigh-quality demonstrations. LLMParser also includes a novel adaptive parsing\ncache component to store and refine the templates generated by the LLM. This\ndesign aids in addressing the inefficiency of LLMs by rapid matching to\npreviously parsed log templates. LLMParser also adaptively updates the\ntemplates in the parsing cache to ensure consistent parsed results. Extensive\nevaluation on large-scale public datasets demonstrates that LLMParser surpasses\nthe state-of-the-art methods. Furthermore, LLMParser significantly reduces the\nquery times to LLMs, achieving efficiency comparable to the most efficient\nbaseline, Drain.",
            "author": [
                "Zhihan Jiang",
                "Jinyang Liu",
                "Zhuangbin Chen",
                "Yichen Li",
                "Junjie Huang",
                "Yintong Huo",
                "Pinjia He",
                "Jiazhen Gu",
                "Michael R. Lyu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01796v1",
                "http://arxiv.org/pdf/2310.01796v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01793v1",
            "title": "On regular sets in Cayley graphs",
            "updated": "2023-10-03T04:42:41Z",
            "published": "2023-10-03T04:42:41Z",
            "summary": "Let $\\Ga = (V, E)$ be a graph and $a, b$ nonnegative integers. An $(a,\nb)$-regular set in $\\Ga$ is a nonempty proper subset $D$ of $V$ such that every\nvertex in $D$ has exactly $a$ neighbours in $D$ and every vertex in $V\n\\setminus D$ has exactly $b$ neighbours in $D$. A $(0,1)$-regular set is called\na perfect code, an efficient dominating set, or an independent perfect\ndominating set. A subset $D$ of a group $G$ is called an $(a,b)$-regular set of\n$G$ if it is an $(a, b)$-regular set in some Cayley graph of $G$, and an $(a,\nb)$-regular set in a Cayley graph of $G$ is called a subgroup $(a, b)$-regular\nset if it is also a subgroup of $G$. In this paper we study $(a, b)$-regular\nsets in Cayley graphs with a focus on $(0, k)$-regular sets, where $k \\ge 1$ is\nan integer. Among other things we determine when a non-trivial proper normal\nsubgroup of a group is a $(0, k)$-regular set of the group. We also determine\nall subgroup $(0, k)$-regular sets of dihedral groups and generalized\nquaternion groups. We obtain necessary and sufficient conditions for a\nhypercube or the Cartesian product of $n$ copies of the cycle of length $p$ to\nadmit $(0, k)$-regular sets, where $p$ is an odd prime. Our results generalize\nseveral known results from perfect codes to $(0, k)$-regular sets.",
            "author": [
                "Xiaomeng Wang",
                "Shou-Jun Xu",
                "Sanming Zhou"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01793v1",
                "http://arxiv.org/pdf/2310.01793v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C25, 05C69, 94B25"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01789v1",
            "title": "Charge Transport Through DNA with Energy-Dependent Decoherence",
            "updated": "2023-10-03T04:30:31Z",
            "published": "2023-10-03T04:30:31Z",
            "summary": "Modeling charge transport in DNA is essential to understand and control the\nelectrical properties and develop DNA-based nanoelectronics. DNA is a\nfluctuating molecule that exists in a solvent environment, which makes the\nelectron susceptible to decoherence. While knowledge of the Hamiltonian\nresponsible for decoherence will provide a microscopic description, the\ninteractions are complex and methods to calculate decoherence are unclear. One\nprominent phenomenological model to include decoherence is through fictitious\nprobes that depend on spatially variant scattering rates. However, the built-in\nenergy-independence of the decoherence (E-indep) model overestimates the\ntransmission in the bandgap and washes out distinct features inside the valence\nor conduction bands. In this study, we introduce a related model where the\ndecoherence rate is energy-dependent (E-dep). This decoherence rate is maximum\nat energy levels and decays away from these energies. Our results show that the\nE-dep model allows for exponential transmission decay with the DNA length and\nmaintains features within the bands' transmission spectra. We further\ndemonstrate that we can obtain DNA conductance values within the experimental\nrange. The new model can help study and design nanoelectronics devices that\nutilize weakly-coupled molecular structures such as DNA.",
            "author": [
                "Hashem Mohammad",
                "M. P. Anantram"
            ],
            "link": [
                "http://dx.doi.org/10.1103/PhysRevE.108.044403",
                "http://arxiv.org/abs/2310.01789v1",
                "http://arxiv.org/pdf/2310.01789v1"
            ],
            "primary_category": "physics.bio-ph",
            "category": [
                "physics.bio-ph",
                "cond-mat.soft"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01783v1",
            "title": "Can large language models provide useful feedback on research papers? A\n  large-scale empirical analysis",
            "updated": "2023-10-03T04:14:17Z",
            "published": "2023-10-03T04:14:17Z",
            "summary": "Expert feedback lays the foundation of rigorous research. However, the rapid\ngrowth of scholarly production and intricate knowledge specialization challenge\nthe conventional scientific feedback mechanisms. High-quality peer reviews are\nincreasingly difficult to obtain. Researchers who are more junior or from\nunder-resourced settings have especially hard times getting timely feedback.\nWith the breakthrough of large language models (LLM) such as GPT-4, there is\ngrowing interest in using LLMs to generate scientific feedback on research\nmanuscripts. However, the utility of LLM-generated feedback has not been\nsystematically studied. To address this gap, we created an automated pipeline\nusing GPT-4 to provide comments on the full PDFs of scientific papers. We\nevaluated the quality of GPT-4's feedback through two large-scale studies. We\nfirst quantitatively compared GPT-4's generated feedback with human peer\nreviewer feedback in 15 Nature family journals (3,096 papers in total) and the\nICLR machine learning conference (1,709 papers). The overlap in the points\nraised by GPT-4 and by human reviewers (average overlap 30.85% for Nature\njournals, 39.23% for ICLR) is comparable to the overlap between two human\nreviewers (average overlap 28.58% for Nature journals, 35.25% for ICLR). The\noverlap between GPT-4 and human reviewers is larger for the weaker papers. We\nthen conducted a prospective user study with 308 researchers from 110 US\ninstitutions in the field of AI and computational biology to understand how\nresearchers perceive feedback generated by our GPT-4 system on their own\npapers. Overall, more than half (57.4%) of the users found GPT-4 generated\nfeedback helpful/very helpful and 82.4% found it more beneficial than feedback\nfrom at least some human reviewers. While our findings show that LLM-generated\nfeedback can help researchers, we also identify several limitations.",
            "author": [
                "Weixin Liang",
                "Yuhui Zhang",
                "Hancheng Cao",
                "Binglu Wang",
                "Daisy Ding",
                "Xinyu Yang",
                "Kailas Vodrahalli",
                "Siyu He",
                "Daniel Smith",
                "Yian Yin",
                "Daniel McFarland",
                "James Zou"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01783v1",
                "http://arxiv.org/pdf/2310.01783v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CL",
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01779v2",
            "title": "HallE-Switch: Controlling Object Hallucination in Large Vision Language\n  Models",
            "updated": "2023-12-03T12:06:56Z",
            "published": "2023-10-03T04:01:27Z",
            "summary": "Current large vision-language models (LVLMs) achieve remarkable progress, yet\nthere remains significant uncertainty regarding their ability to accurately\napprehend visual details, that is, in performing detailed captioning. To\naddress this, we introduce $\\textit{CCEval}$, a GPT-4 assisted evaluation\nmethod tailored for detailed captioning. Interestingly, while LVLMs demonstrate\nminimal object existence hallucination in existing VQA benchmarks, our proposed\nevaluation reveals continued susceptibility to such hallucinations. In this\npaper, we make the first attempt to investigate such hallucination from\ndifferent aspects, including image resolution, the language decoder size, and\ninstruction data amount, quality, granularity. Our findings underscore the\nunwarranted inference when the language description includes details at a finer\nobject granularity than what the vision module can ground or verify, thus\ninducing hallucination. To control such hallucinations, we further attribute\nthe reliability of captioning to contextual knowledge (involving only\ncontextually grounded objects) and parametric knowledge (containing inferred\nobjects by the model). Thus, we introduce $\\textit{HallE-Switch}$, a\ncontrollable LVLM in terms of $\\textbf{Hall}$ucination in object\n$\\textbf{E}$xistence. HallE-Switch can condition the captioning to shift\nbetween (i) exclusively depicting contextual knowledge for grounded objects and\n(ii) blending it with parametric knowledge to imagine inferred objects. Our\nmethod reduces hallucination by 44% compared to LLaVA$_{7B}$ and maintains the\nsame object coverage.",
            "author": [
                "Bohan Zhai",
                "Shijia Yang",
                "Chenfeng Xu",
                "Sheng Shen",
                "Kurt Keutzer",
                "Manling Li"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01779v2",
                "http://arxiv.org/pdf/2310.01779v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01777v1",
            "title": "SEA: Sparse Linear Attention with Estimated Attention Mask",
            "updated": "2023-10-03T03:56:26Z",
            "published": "2023-10-03T03:56:26Z",
            "summary": "The transformer architecture has made breakthroughs in recent years on tasks\nwhich require modeling pairwise relationships between sequential elements, as\nis the case in natural language understanding. However, transformers struggle\nwith long sequences due to the quadratic complexity of the attention operation,\nand previous research has aimed to lower the complexity by sparsifying or\nlinearly approximating the attention matrix. Yet, these approaches cannot\nstraightforwardly distill knowledge from a teacher's attention matrix, and\noften require complete retraining from scratch. Furthermore, previous sparse\nand linear approaches may also lose interpretability if they do not produce\nfull quadratic attention matrices. To address these challenges, we propose SEA:\nSparse linear attention with an Estimated Attention mask. SEA estimates the\nattention matrix with linear complexity via kernel-based linear attention, then\ncreates a sparse approximation to the full attention matrix with a top-k\nselection to perform a sparse attention operation. For language modeling tasks\n(Wikitext2), previous linear and sparse attention methods show a roughly\ntwo-fold worse perplexity scores over the quadratic OPT-125M baseline, while\nSEA achieves an even better perplexity than OPT-125M, using roughly half as\nmuch memory as OPT-125M. Moreover, SEA maintains an interpretable attention\nmatrix and can utilize knowledge distillation to lower the complexity of\nexisting pretrained transformers. We believe that our work will have a large\npractical impact, as it opens the possibility of running large transformers on\nresource-limited devices with less memory.",
            "author": [
                "Heejun Lee",
                "Jina Kim",
                "Jeffrey Willette",
                "Sung Ju Hwang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01777v1",
                "http://arxiv.org/pdf/2310.01777v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01765v1",
            "title": "Data Cleaning and Machine Learning: A Systematic Literature Review",
            "updated": "2023-10-03T03:13:23Z",
            "published": "2023-10-03T03:13:23Z",
            "summary": "Context: Machine Learning (ML) is integrated into a growing number of systems\nfor various applications. Because the performance of an ML model is highly\ndependent on the quality of the data it has been trained on, there is a growing\ninterest in approaches to detect and repair data errors (i.e., data cleaning).\nResearchers are also exploring how ML can be used for data cleaning; hence\ncreating a dual relationship between ML and data cleaning. To the best of our\nknowledge, there is no study that comprehensively reviews this relationship.\nObjective: This paper's objectives are twofold. First, it aims to summarize the\nlatest approaches for data cleaning for ML and ML for data cleaning. Second, it\nprovides future work recommendations. Method: We conduct a systematic\nliterature review of the papers published between 2016 and 2022 inclusively. We\nidentify different types of data cleaning activities with and for ML: feature\ncleaning, label cleaning, entity matching, outlier detection, imputation, and\nholistic data cleaning. Results: We summarize the content of 101 papers\ncovering various data cleaning activities and provide 24 future work\nrecommendations. Our review highlights many promising data cleaning techniques\nthat can be further extended. Conclusion: We believe that our review of the\nliterature will help the community develop better approaches to clean data.",
            "author": [
                "Pierre-Olivier C\u00f4t\u00e9",
                "Amin Nikanjam",
                "Nafisa Ahmed",
                "Dmytro Humeniuk",
                "Foutse Khomh"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01765v1",
                "http://arxiv.org/pdf/2310.01765v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.DB"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01756v1",
            "title": "Improved Algorithms for Adversarial Bandits with Unbounded Losses",
            "updated": "2023-10-03T02:44:31Z",
            "published": "2023-10-03T02:44:31Z",
            "summary": "We consider the Adversarial Multi-Armed Bandits (MAB) problem with unbounded\nlosses, where the algorithms have no prior knowledge on the sizes of the\nlosses. We present UMAB-NN and UMAB-G, two algorithms for non-negative and\ngeneral unbounded loss respectively. For non-negative unbounded loss, UMAB-NN\nachieves the first adaptive and scale free regret bound without uniform\nexploration. Built up on that, we further develop UMAB-G that can learn from\narbitrary unbounded loss. Our analysis reveals the asymmetry between positive\nand negative losses in the MAB problem and provide additional insights. We also\naccompany our theoretical findings with extensive empirical evaluations,\nshowing that our algorithms consistently out-performs all existing algorithms\nthat handles unbounded losses.",
            "author": [
                "Mingyu Chen",
                "Xuezhou Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01756v1",
                "http://arxiv.org/pdf/2310.01756v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01753v1",
            "title": "CausalTime: Realistically Generated Time-series for Benchmarking of\n  Causal Discovery",
            "updated": "2023-10-03T02:29:19Z",
            "published": "2023-10-03T02:29:19Z",
            "summary": "Time-series causal discovery (TSCD) is a fundamental problem of machine\nlearning. However, existing synthetic datasets cannot properly evaluate or\npredict the algorithms' performance on real data. This study introduces the\nCausalTime pipeline to generate time-series that highly resemble the real data\nand with ground truth causal graphs for quantitative performance evaluation.\nThe pipeline starts from real observations in a specific scenario and produces\na matching benchmark dataset. Firstly, we harness deep neural networks along\nwith normalizing flow to accurately capture realistic dynamics. Secondly, we\nextract hypothesized causal graphs by performing importance analysis on the\nneural network or leveraging prior knowledge. Thirdly, we derive the ground\ntruth causal graphs by splitting the causal model into causal term, residual\nterm, and noise term. Lastly, using the fitted network and the derived causal\ngraph, we generate corresponding versatile time-series proper for algorithm\nassessment. In the experiments, we validate the fidelity of the generated data\nthrough qualitative and quantitative experiments, followed by a benchmarking of\nexisting TSCD algorithms using these generated datasets. CausalTime offers a\nfeasible solution to evaluating TSCD algorithms in real applications and can be\ngeneralized to a wide range of fields. For easy use of the proposed approach,\nwe also provide a user-friendly website, hosted on www.causaltime.cc.",
            "author": [
                "Yuxiao Cheng",
                "Ziqian Wang",
                "Tingxiong Xiao",
                "Qin Zhong",
                "Jinli Suo",
                "Kunlun He"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01753v1",
                "http://arxiv.org/pdf/2310.01753v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01746v1",
            "title": "An approach to study interactions of antineutrons with CsI at a $J/\u03c8$\n  factory",
            "updated": "2023-10-03T02:14:52Z",
            "published": "2023-10-03T02:14:52Z",
            "summary": "Cesium Iodide (CsI) crystals are widely used in high-energy physics for their\nscintillation properties that enable detection of charged and neutral particles\nvia direct and indirect ionization and form the basis of electromagnetic\ncalorimeters. However, knowledge of antineutron interactions with CsI is\nlimited due to the difficulty of obtaining sources of antineutron of sufficient\nintensity and energy definition. As antineutron are abundantly produced by many\nprocesses it would be particularly useful to improve understanding of the\ninteractions of antineutrons with CsI crystals.\n  We propose to use the decay $J/\\psi\\to p\\pi^-\\bar{n}$ at the BEPCII $J/\\psi$\nfactory as a source of antineutrons using the BESIII detector with a CsI target\nadded between the beam pipe and the detector. The BESIII Monte Carlo simulation\nwith varying thicknesses of CsI target is used to validate the approach and\noptimize the target thickness. Selecting $p\\pi^-$ charged particle tracks from\nthe Monte Carlo we obtain clean antineutron samples with well defined momentum\nand direction. The selection efficiency, momentum and angular resolutions, as\nwell as the interaction probability between antineutron and the CsI target are\nestimated.\n  As the CsI thickness is increased more antineutron CsI interactions are\nobtained,however the quality of the $p\\pi^-$ selection is degraded. The Monte\nCarlo study yields an optimum thickness that balances these effects. This\napproach can be applied to similar experiments with other types of target\nmaterials to measure baryons such as liquid hydrogen/deuterium and\n$\\Lambda/\\Xi$ hyperons.",
            "author": [
                "Si-Cheng Yuan",
                "Liang-Liang Wang",
                "Wei-Dong Li"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01746v1",
                "http://arxiv.org/pdf/2310.01746v1"
            ],
            "primary_category": "hep-ex",
            "category": [
                "hep-ex",
                "physics.ins-det"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01736v1",
            "title": "Exact results for some extremal problems on expansions I",
            "updated": "2023-10-03T01:52:06Z",
            "published": "2023-10-03T01:52:06Z",
            "summary": "The expansion of a graph $F$, denoted by $F^3$, is the $3$-graph obtained\nfrom $F$ by adding a new vertex to each edge such that different edges receive\ndifferent vertices. For large $n$, we establish tight upper bounds for:\n  The maximum number of edges in an $n$-vertex $3$-graph that does not contain\n$T^3$ for certain class $\\mathcal{T}$ of trees, sharpening (partially) a result\nof Kostochka--Mubayi--Verstra\\\"{e}te.\n  The minimum number of colors needed to color the complete $n$-vertex\n$3$-graph to ensure the existence of a rainbow copy of $F^3$ when $F$ is a\ngraph obtained from some tree $T\\in \\mathcal{T}$ by adding a new edge,\nextending anti-Ramsey results on $P_{2t}^3$ by Gu--Li--Shi and $C_{2t}^3$ by\nTang--Li--Yan.\n  The maximum number of edges in an $n$-vertex $3$-graph whose shadow does not\ncontain the shadow of $C_{k}^3$ or $T^3$ for $T\\in \\mathcal{T}$, answering a\nquestion of Lv \\etal on generalized Tur\\'{a}n problems.",
            "author": [
                "Xizhi Liu",
                "Jialei Song"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01736v1",
                "http://arxiv.org/pdf/2310.01736v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01731v1",
            "title": "Destructive social noise effects on homogeneous and heterogeneous\n  networks: Induced-phases in the majority-rule model",
            "updated": "2023-10-03T01:47:15Z",
            "published": "2023-10-03T01:47:15Z",
            "summary": "This paper explores the effects of destructive social noises, characterized\nby independence and anticonformity, on the occurrence of order-disorder phase\ntransitions within the framework of the majority-rule model. The study\nencompasses various network topologies, including the complete graph,\ntwo-dimensional (2-D) square lattice, three-dimensional (3-D) square lattice,\nas well as heterogeneous networks like Watts-Strogatz, Barab\\'asi-Albert, and\nErd\\H{o}s-R\\'enyi networks. These social behaviors are quantified using the\nparameter $p$, representing the probability of agents exhibiting independent or\nanticonformist tendencies. Our results reveal that the occurrence and nature of\nphase transitions depend on the fundamental characteristics of the model and\nthe underlying network structure. Specifically, the model exhibits continuous\nphase transitions on the complete graph and the 2-D square lattice, with\ncritical points that vary based on the model's attributes. However, on the 3-D\nlattice, the independence model notably lacks a phase transition, while the\nanticonformity model still exhibits a continuous phase transition. By employing\nfinite-size scaling analysis and evaluating critical exponents, we confirm that\nthe model falls within the Ising model universality class, except in the 3-D\nmodel. This study provides insights into the dynamic interplay between social\ndynamics and network topology, especially based on the majority-rule model.",
            "author": [
                "Didi Ahmad Mulya",
                "Roni Muslim"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01731v1",
                "http://arxiv.org/pdf/2310.01731v1"
            ],
            "primary_category": "physics.soc-ph",
            "category": [
                "physics.soc-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01727v1",
            "title": "Can GPT-4 Replicate Empirical Software Engineering Research?",
            "updated": "2023-10-03T01:27:23Z",
            "published": "2023-10-03T01:27:23Z",
            "summary": "Empirical software engineering research on production systems has brought\nforth a better understanding of the software engineering process for\npractitioners and researchers alike. However, only a small subset of production\nsystems is studied, limiting the impact of this research. While software\nengineering practitioners benefit from replicating research on their own data,\nthis poses its own set of challenges, since performing replications requires a\ndeep understanding of research methodologies and subtle nuances in software\nengineering data. Given that large language models (LLMs), such as GPT-4, show\npromise in tackling both software engineering- and science-related tasks, these\nmodels could help democratize empirical software engineering research.\n  In this paper, we examine LLMs' abilities to perform replications of\nempirical software engineering research on new data. We specifically study\ntheir ability to surface assumptions made in empirical software engineering\nresearch methodologies, as well as their ability to plan and generate code for\nanalysis pipelines on seven empirical software engineering papers. We perform a\nuser study with 14 participants with software engineering research expertise,\nwho evaluate GPT-4-generated assumptions and analysis plans (i.e., a list of\nmodule specifications) from the papers. We find that GPT-4 is able to surface\ncorrect assumptions, but struggle to generate ones that reflect common\nknowledge about software engineering data. In a manual analysis of the\ngenerated code, we find that the GPT-4-generated code contains the correct\nhigh-level logic, given a subset of the methodology. However, the code contains\nmany small implementation-level errors, reflecting a lack of software\nengineering knowledge. Our findings have implications for leveraging LLMs for\nsoftware engineering research as well as practitioner data scientists in\nsoftware teams.",
            "author": [
                "Jenny T. Liang",
                "Carmen Badea",
                "Christian Bird",
                "Robert DeLine",
                "Denae Ford",
                "Nicole Forsgren",
                "Thomas Zimmermann"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01727v1",
                "http://arxiv.org/pdf/2310.01727v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01717v1",
            "title": "Ensemble Distillation for Unsupervised Constituency Parsing",
            "updated": "2023-10-03T01:02:44Z",
            "published": "2023-10-03T01:02:44Z",
            "summary": "We investigate the unsupervised constituency parsing task, which organizes\nwords and phrases of a sentence into a hierarchical structure without using\nlinguistically annotated data. We observe that existing unsupervised parsers\ncapture differing aspects of parsing structures, which can be leveraged to\nenhance unsupervised parsing performance. To this end, we propose a notion of\n\"tree averaging,\" based on which we further propose a novel ensemble method for\nunsupervised parsing. To improve inference efficiency, we further distill the\nensemble knowledge into a student model; such an ensemble-then-distill process\nis an effective approach to mitigate the over-smoothing problem existing in\ncommon multi-teacher distilling methods. Experiments show that our method\nsurpasses all previous approaches, consistently demonstrating its effectiveness\nand robustness across various runs, with different ensemble components, and\nunder domain-shift conditions.",
            "author": [
                "Behzad Shayegh",
                "Yanshuai Cao",
                "Xiaodan Zhu",
                "Jackie C. K. Cheung",
                "Lili Mou"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01717v1",
                "http://arxiv.org/pdf/2310.01717v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01715v2",
            "title": "Broadband terahertz near-field excitation and detection of silicon\n  photonic crystal modes",
            "updated": "2023-10-27T01:20:39Z",
            "published": "2023-10-03T00:59:07Z",
            "summary": "Chip-based terahertz (THz) devices are emerging as versatile tools for\nmanipulating mm-wave frequencies in the context of integrated high-speed\ncommunication technologies for potential sixth-generation (6G) wireless\napplications. The characterization of THz devices is typically performed using\nfar-field techniques that provide limited information about the underlying\nphysical mechanisms producing them. As the library of chip-based\nfunctionalities expands, e.g., for tailoring the emission and directional\npropagation properties of THz antennas and waveguides, novel characterization\ntechniques will likely be beneficial for observing subtle effects that are\nsensitive to a device's structural parameters. Here we present near-field\nmeasurements showing the emission properties of a broadband THz emitter placed\nin the vicinity of a photonic crystal (PHC) slab. These experiments reveal\nlong-predicted emission properties, but which to our knowledge have yet to be\nexperimentally observed at THz frequencies. We demonstrate three distinct\neffects between 0.3-0.5 THz: (i) field suppression at frequencies corresponding\nto quasi-TE bandgaps (ii) a frequency-dependent directed emission along two\ndistinct pathways for two neighboring frequencies, resulting in a local field\nconcentration; (iii) a re-direction of the emission, achieved by rotating the\nPHC with respect to the dipole orientation. Simulations reveal that the\nobserved behavior can be predicted from the underlying band structure. Our\nresults highlight the opportunities that PHCs can potentially provide for\nalignment-free, chip-based 6G technologies. Our experimental technique extends\nthe applicability realms of THz spectroscopy and will find use for\ncharacterizing the THz modes supported by true samples, whose inherent\nimperfections cannot realistically be accounted for by simulations,\nparticularly in highly dispersive frequency bands.",
            "author": [
                "Kseniia Lezhennikova",
                "Sahand Mahmoodian",
                "Boris T. Kuhlmey",
                "Redha Abdeddaim",
                "Stefan Enoch",
                "C. Martijn de Sterke",
                "Alessandro Tuniz"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01715v2",
                "http://arxiv.org/pdf/2310.01715v2"
            ],
            "primary_category": "physics.optics",
            "category": [
                "physics.optics"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01714v2",
            "title": "Large Language Models as Analogical Reasoners",
            "updated": "2023-10-07T06:05:46Z",
            "published": "2023-10-03T00:57:26Z",
            "summary": "Chain-of-thought (CoT) prompting for language models demonstrates impressive\nperformance across reasoning tasks, but typically needs labeled exemplars of\nthe reasoning process. In this work, we introduce a new prompting approach,\nAnalogical Prompting, designed to automatically guide the reasoning process of\nlarge language models. Inspired by analogical reasoning, a cognitive process in\nwhich humans draw from relevant past experiences to tackle new problems, our\napproach prompts language models to self-generate relevant exemplars or\nknowledge in the context, before proceeding to solve the given problem. This\nmethod presents several advantages: it obviates the need for labeling or\nretrieving exemplars, offering generality and convenience; it can also tailor\nthe generated exemplars and knowledge to each problem, offering adaptability.\nExperimental results show that our approach outperforms 0-shot CoT and manual\nfew-shot CoT in a variety of reasoning tasks, including math problem solving in\nGSM8K and MATH, code generation in Codeforces, and other reasoning tasks in\nBIG-Bench.",
            "author": [
                "Michihiro Yasunaga",
                "Xinyun Chen",
                "Yujia Li",
                "Panupong Pasupat",
                "Jure Leskovec",
                "Percy Liang",
                "Ed H. Chi",
                "Denny Zhou"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01714v2",
                "http://arxiv.org/pdf/2310.01714v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01711v1",
            "title": "Learning Class-Specific Spectral Patterns to Improve Deep Learning Based\n  Scene-Level Fire Smoke Detection from Multi-Spectral Satellite Imagery",
            "updated": "2023-10-03T00:47:17Z",
            "published": "2023-10-03T00:47:17Z",
            "summary": "Detecting fire smoke is crucial for the timely identification of early\nwildfires using satellite imagery. However, the spatial and spectral similarity\nof fire smoke to other confounding aerosols, such as clouds and haze, often\nconfuse even the most advanced deep-learning (DL) models. Nonetheless, these\naerosols also present distinct spectral characteristics in some specific bands,\nand such spectral patterns are useful for distinguishing the aerosols more\naccurately. Early research tried to derive various threshold values from the\nreflectance and brightness temperature in specific spectral bands to\ndifferentiate smoke and cloud pixels. However, such threshold values were\ndetermined based on domain knowledge and are hard to generalise. In addition,\nsuch threshold values were manually derived from specific combinations of bands\nto infer spectral patterns, making them difficult to employ in deep-learning\nmodels. In this paper, we introduce a DL module called input amplification\n(InAmp) which is designed to enable DL models to learn class-specific spectral\npatterns automatically from multi-spectral satellite imagery and improve the\nfire smoke detection accuracy. InAmp can be conveniently integrated with\ndifferent DL architectures. We evaluate the effectiveness of the InAmp module\non different Convolutional neural network (CNN) architectures using two\nsatellite imagery datasets: USTC_SmokeRS, derived from Moderate Resolution\nImaging Spectroradiometer (MODIS) with three spectral bands; and Landsat_Smk,\nderived from Landsat 5/8 with six spectral bands. Our experimental results\ndemonstrate that the InAmp module improves the fire smoke detection accuracy of\nthe CNN models. Additionally, we visualise the spectral patterns extracted by\nthe InAmp module using test imagery and demonstrate that the InAmp module can\neffectively extract class-specific spectral patterns.",
            "author": [
                "Liang Zhao",
                "Jixue Liu",
                "Stefan Peters",
                "Jiuyong Li",
                "Norman Mueller",
                "Simon Oliver"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01711v1",
                "http://arxiv.org/pdf/2310.01711v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01708v1",
            "title": "Deciphering Diagnoses: How Large Language Models Explanations Influence\n  Clinical Decision Making",
            "updated": "2023-10-03T00:08:23Z",
            "published": "2023-10-03T00:08:23Z",
            "summary": "Clinical Decision Support Systems (CDSS) utilize evidence-based knowledge and\npatient data to offer real-time recommendations, with Large Language Models\n(LLMs) emerging as a promising tool to generate plain-text explanations for\nmedical decisions. This study explores the effectiveness and reliability of\nLLMs in generating explanations for diagnoses based on patient complaints.\nThree experienced doctors evaluated LLM-generated explanations of the\nconnection between patient complaints and doctor and model-assigned diagnoses\nacross several stages. Experimental results demonstrated that LLM explanations\nsignificantly increased doctors' agreement rates with given diagnoses and\nhighlighted potential errors in LLM outputs, ranging from 5% to 30%. The study\nunderscores the potential and challenges of LLMs in healthcare and emphasizes\nthe need for careful integration and evaluation to ensure patient safety and\noptimal clinical utility.",
            "author": [
                "D. Umerenkov",
                "G. Zubkova",
                "A. Nesterov"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01708v1",
                "http://arxiv.org/pdf/2310.01708v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01706v1",
            "title": "On Representation Complexity of Model-based and Model-free Reinforcement\n  Learning",
            "updated": "2023-10-03T00:01:58Z",
            "published": "2023-10-03T00:01:58Z",
            "summary": "We study the representation complexity of model-based and model-free\nreinforcement learning (RL) in the context of circuit complexity. We prove\ntheoretically that there exists a broad class of MDPs such that their\nunderlying transition and reward functions can be represented by constant depth\ncircuits with polynomial size, while the optimal $Q$-function suffers an\nexponential circuit complexity in constant-depth circuits. By drawing attention\nto the approximation errors and building connections to complexity theory, our\ntheory provides unique insights into why model-based algorithms usually enjoy\nbetter sample complexity than model-free algorithms from a novel representation\ncomplexity perspective: in some cases, the ground-truth rule (model) of the\nenvironment is simple to represent, while other quantities, such as\n$Q$-function, appear complex. We empirically corroborate our theory by\ncomparing the approximation error of the transition kernel, reward function,\nand optimal $Q$-function in various Mujoco environments, which demonstrates\nthat the approximation errors of the transition kernel and reward function are\nconsistently lower than those of the optimal $Q$-function. To the best of our\nknowledge, this work is the first to study the circuit complexity of RL, which\nalso provides a rigorous framework for future research.",
            "author": [
                "Hanlin Zhu",
                "Baihe Huang",
                "Stuart Russell"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01706v1",
                "http://arxiv.org/pdf/2310.01706v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01704v1",
            "title": "Transformers are efficient hierarchical chemical graph learners",
            "updated": "2023-10-02T23:57:04Z",
            "published": "2023-10-02T23:57:04Z",
            "summary": "Transformers, adapted from natural language processing, are emerging as a\nleading approach for graph representation learning. Contemporary graph\ntransformers often treat nodes or edges as separate tokens. This approach leads\nto computational challenges for even moderately-sized graphs due to the\nquadratic scaling of self-attention complexity with token count. In this paper,\nwe introduce SubFormer, a graph transformer that operates on subgraphs that\naggregate information by a message-passing mechanism. This approach reduces the\nnumber of tokens and enhances learning long-range interactions. We demonstrate\nSubFormer on benchmarks for predicting molecular properties from chemical\nstructures and show that it is competitive with state-of-the-art graph\ntransformers at a fraction of the computational cost, with training times on\nthe order of minutes on a consumer-grade graphics card. We interpret the\nattention weights in terms of chemical structures. We show that SubFormer\nexhibits limited over-smoothing and avoids over-squashing, which is prevalent\nin traditional graph neural networks.",
            "author": [
                "Zihan Pengmei",
                "Zimu Li",
                "Chih-chan Tien",
                "Risi Kondor",
                "Aaron R. Dinner"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01704v1",
                "http://arxiv.org/pdf/2310.01704v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01689v1",
            "title": "Threat Modelling in Internet of Things (IoT) Environment Using Dynamic\n  Attack Graphs",
            "updated": "2023-10-02T23:07:19Z",
            "published": "2023-10-02T23:07:19Z",
            "summary": "We present a threat modelling approach to represent changes to the attack\npaths through an Internet of Things (IoT) environment when the environment\nchanges dynamically, i.e., when new devices are added or removed from the\nsystem or when whole sub-systems join or leave. The proposed approach\ninvestigates the propagation of threats using attack graphs. However,\ntraditional attack graph approaches have been applied in static environments\nthat do not continuously change such as the Enterprise networks, leading to\nstatic and usually very large attack graphs. In contrast, IoT environments are\noften characterised by dynamic change and interconnections; different\ntopologies for different systems may interconnect with each other dynamically\nand outside the operator control. Such new interconnections lead to changes in\nthe reachability amongst devices according to which their corresponding attack\ngraphs change. This requires dynamic topology and attack graphs for threat and\nrisk analysis. In this paper, we develop a threat modelling approach that cope\nwith dynamic system changes that may occur in IoT environments and enables\nidentifying attack paths whilst allowing for system dynamics. We develop\ndynamic topology and attack graphs that are able to cope with the changes in\nthe IoT environment rapidly by maintaining their associated graphs. To motivate\nthe work and illustrate our approach we introduce an example scenario based on\nhealthcare systems. Our approach is implemented using a Graph Database\nManagement Tool (GDBM) -- Neo4j -- which is a popular tool for mapping,\nvisualising and querying the graphs of highly connected data, and is efficient\nin providing a rapid threat modelling mechanism, which makes it suitable for\ncapturing security changes in the dynamic IoT environment.",
            "author": [
                "Marwa Salayma",
                "Emil C Lupu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01689v1",
                "http://arxiv.org/pdf/2310.01689v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.DC",
                "cs.NI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01684v1",
            "title": "Designing User-Centric Behavioral Interventions to Prevent Dysglycemia\n  with Novel Counterfactual Explanations",
            "updated": "2023-10-02T22:42:52Z",
            "published": "2023-10-02T22:42:52Z",
            "summary": "Maintaining normal blood glucose levels through lifestyle behaviors is\ncentral to maintaining health and preventing disease. Frequent exposure to\ndysglycemia (i.e., abnormal glucose events such as hyperlycemia and\nhypoglycemia) leads to chronic complications including diabetes, kidney disease\nand need for dialysis, myocardial infarction, stroke, amputation, and death.\nTherefore, a tool capable of predicting dysglycemia and offering users\nactionable feedback about how to make changes in their diet, exercise, and\nmedication to prevent abnormal glycemic events could have significant societal\nimpacts. Counterfactual explanations can provide insights into why a model made\na particular prediction by generating hypothetical instances that are similar\nto the original input but lead to a different prediction outcome. Therefore,\ncounterfactuals can be viewed as a means to design AI-driven health\ninterventions to prevent adverse health outcomes such as dysglycemia. In this\npaper, we design GlyCoach, a framework for generating counterfactual\nexplanations for glucose control. Leveraging insights from adversarial\nlearning, GlyCoach characterizes the decision boundary for high-dimensional\nhealth data and performs a grid search to generate actionable interventions.\nGlyCoach is unique in integrating prior knowledge about user preferences of\nplausible explanations into the process of counterfactual generation. We\nevaluate GlyCoach extensively using two real-world datasets and external\nsimulators from prior studies that predict glucose response. GlyCoach achieves\n87\\% sensitivity in the simulation-aided validation, surpassing the\nstate-of-the-art techniques for generating counterfactual explanations by at\nleast $10\\%$. Besides, counterfactuals from GlyCoach exhibit a $32\\%$ improved\nnormalized distance compared to previous research.",
            "author": [
                "Asiful Arefeen",
                "Hassan Ghasemzadeh"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01684v1",
                "http://arxiv.org/pdf/2310.01684v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.HC",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01678v1",
            "title": "Score dynamics: scaling molecular dynamics with picosecond timesteps via\n  conditional diffusion model",
            "updated": "2023-10-02T22:29:45Z",
            "published": "2023-10-02T22:29:45Z",
            "summary": "We propose score dynamics (SD), a general framework for learning effective\nevolution operators for atomistic as well as coarse-grained dynamics from\nmolecular-dynamics (MD) simulations. SD is centered around scores, or\nderivatives of the transition log-probability with respect to the dynamical\ndegrees of freedom. The latter play the same role as force fields in MD but are\nused in denoising diffusion probability models to generate discrete transitions\nof the dynamical variables in an SD timestep, which can be orders of magnitude\nlarger than a typical MD timestep. In this work, we construct graph neural\nnetwork based score dynamics models of realistic molecular systems that are\nevolved with 1~ps timesteps. We demonstrate the efficacy of score dynamics with\ncase studies of alanine dipeptide and short alkanes in aqueous solution. Both\nequilibrium predictions derived from the stationary distributions of the\nconditional probability and kinetic predictions for the transition rates and\ntransition paths are in good agreement with MD at about 8-18 fold wall-clock\nspeedup. Open challenges and possible future remedies to improve score dynamics\nare also discussed.",
            "author": [
                "Tim Hsu",
                "Babak Sadigh",
                "Vasily Bulatov",
                "Fei Zhou"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01678v1",
                "http://arxiv.org/pdf/2310.01678v1"
            ],
            "primary_category": "physics.comp-ph",
            "category": [
                "physics.comp-ph",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01671v1",
            "title": "A directional regularization method for the limited-angle Helsinki\n  Tomography Challenge using the Core Imaging Library (CIL)",
            "updated": "2023-10-02T22:02:29Z",
            "published": "2023-10-02T22:02:29Z",
            "summary": "This article presents the algorithms developed by the Core Imaging Library\n(CIL) developer team for the Helsinki Tomography Challenge 2022. The challenge\nfocused on reconstructing 2D phantom shapes from limited-angle computed\ntomography (CT) data. The CIL team designed and implemented five reconstruction\nmethods using CIL (https://ccpi.ac.uk/cil/), an open-source Python package for\ntomographic imaging. The CIL team adopted a model-based reconstruction\nstrategy, unique to this challenge with all other teams relying on\ndeep-learning techniques. The CIL algorithms showcased exceptional performance,\nwith one algorithm securing the third place in the competition. The\nbest-performing algorithm employed careful CT data pre-processing and an\noptimization problem with single-sided directional total variation\nregularization combined with isotropic total variation and tailored lower and\nupper bounds. The reconstructions and segmentations achieved high quality for\ndata with angular ranges down to 50 degrees, and in some cases acceptable\nperformance even at 40 and 30 degrees. This study highlights the effectiveness\nof model-based approaches in limited-angle tomography and emphasizes the\nimportance of proper algorithmic design leveraging on available prior knowledge\nto overcome data limitations. Finally, this study highlights the flexibility of\nCIL for prototyping and comparison of different optimization methods.",
            "author": [
                "Jakob Sauer J\u00f8rgensen",
                "Evangelos Papoutsellis",
                "Laura Murgatroyd",
                "Gemma Fardell",
                "Edoardo Pasca"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01671v1",
                "http://arxiv.org/pdf/2310.01671v1"
            ],
            "primary_category": "physics.med-ph",
            "category": [
                "physics.med-ph",
                "cs.MS",
                "cs.NA",
                "math.NA",
                "math.OC",
                "65R32, 94A08, 65K10",
                "G.1.6; G.1.10; G.4"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01668v1",
            "title": "Locality-Aware Graph-Rewiring in GNNs",
            "updated": "2023-10-02T21:59:44Z",
            "published": "2023-10-02T21:59:44Z",
            "summary": "Graph Neural Networks (GNNs) are popular models for machine learning on\ngraphs that typically follow the message-passing paradigm, whereby the feature\nof a node is updated recursively upon aggregating information over its\nneighbors. While exchanging messages over the input graph endows GNNs with a\nstrong inductive bias, it can also make GNNs susceptible to over-squashing,\nthereby preventing them from capturing long-range interactions in the given\ngraph. To rectify this issue, graph rewiring techniques have been proposed as a\nmeans of improving information flow by altering the graph connectivity. In this\nwork, we identify three desiderata for graph-rewiring: (i) reduce\nover-squashing, (ii) respect the locality of the graph, and (iii) preserve the\nsparsity of the graph. We highlight fundamental trade-offs that occur between\nspatial and spectral rewiring techniques; while the former often satisfy (i)\nand (ii) but not (iii), the latter generally satisfy (i) and (iii) at the\nexpense of (ii). We propose a novel rewiring framework that satisfies all of\n(i)--(iii) through a locality-aware sequence of rewiring operations. We then\ndiscuss a specific instance of such rewiring framework and validate its\neffectiveness on several real-world benchmarks, showing that it either matches\nor significantly outperforms existing rewiring approaches.",
            "author": [
                "Federico Barbero",
                "Ameya Velingker",
                "Amin Saberi",
                "Michael Bronstein",
                "Francesco Di Giovanni"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01668v1",
                "http://arxiv.org/pdf/2310.01668v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01658v1",
            "title": "Some Equations Involving the Gamma Function",
            "updated": "2023-10-02T21:43:13Z",
            "published": "2023-10-02T21:43:13Z",
            "summary": "Let $V\\subseteq\\mathbb{C}^{2n}$ be an algebraic variety with no constant\ncoordinates and with a dominant projection onto the first $n$ coordinates. We\nshow that the intersection of $V$ with the graph of the $\\Gamma$ function is\nZariski dense in $V$.",
            "author": [
                "Sebastian Eterovi\u0107",
                "Adele Padgett"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01658v1",
                "http://arxiv.org/pdf/2310.01658v1"
            ],
            "primary_category": "math.NT",
            "category": [
                "math.NT",
                "math.CV",
                "math.LO",
                "30C15 33B15 30D35 32A60"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.04443v2",
            "title": "Human Mobility Question Answering (Vision Paper)",
            "updated": "2023-10-13T05:07:49Z",
            "published": "2023-10-02T21:24:26Z",
            "summary": "Question answering (QA) systems have attracted much attention from the\nartificial intelligence community as they can learn to answer questions based\non the given knowledge source (e.g., images in visual question answering).\nHowever, the research into question answering systems with human mobility data\nremains unexplored. Mining human mobility data is crucial for various\napplications such as smart city planning, pandemic management, and personalised\nrecommendation system. In this paper, we aim to tackle this gap and introduce a\nnovel task, that is, human mobility question answering (MobQA). The aim of the\ntask is to let the intelligent system learn from mobility data and answer\nrelated questions. This task presents a new paradigm change in mobility\nprediction research and further facilitates the research of human mobility\nrecommendation systems. To better support this novel research topic, this\nvision paper also proposes an initial design of the dataset and a potential\ndeep learning model framework for the introduced MobQA task. We hope that this\npaper will provide novel insights and open new directions in human mobility\nresearch and question answering research.",
            "author": [
                "Hao Xue",
                "Flora D. Salim"
            ],
            "link": [
                "http://arxiv.org/abs/2310.04443v2",
                "http://arxiv.org/pdf/2310.04443v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01642v1",
            "title": "Exploring Naming Conventions (and Defects) of Pre-trained Deep Learning\n  Models in Hugging Face and Other Model Hubs",
            "updated": "2023-10-02T21:13:32Z",
            "published": "2023-10-02T21:13:32Z",
            "summary": "As innovation in deep learning continues, many engineers want to adopt\nPre-Trained deep learning Models (PTMs) as components in computer systems. PTMs\nare part of a research-to-practice pipeline: researchers publish PTMs, which\nengineers adapt for quality or performance and then deploy. If PTM authors\nchoose appropriate names for their PTMs, it could facilitate model discovery\nand reuse. However, prior research has reported that model names are not always\nwell chosen, and are sometimes erroneous. The naming conventions and naming\ndefects for PTM packages have not been systematically studied - understanding\nthem will add to our knowledge of how the research-to-practice process works\nfor PTM packages\n  In this paper, we report the first study of PTM naming conventions and the\nassociated PTM naming defects. We define the components of a PTM package name,\ncomprising the package name and claimed architecture from the metadata. We\npresent the first study focused on characterizing the nature of naming in PTM\necosystem. To this end, we developed a novel automated naming assessment\ntechnique that can automatically extract the semantic and syntactic patterns.\nTo identify potential naming defects, we developed a novel algorithm, automated\nDNN ARchitecture Assessment pipeline (DARA), to cluster PTMs based on\narchitectural differences. Our study suggests the naming conventions for PTMs,\nand frames the naming conventions as signal of the research-to-practice\nrelationships in the PTM ecosystem. We envision future works on further\nempirical study on leveraging meta-features of PTMs to support model search and\nreuse.",
            "author": [
                "Wenxin Jiang",
                "Chingwo Cheung",
                "George K. Thiruvathukal",
                "James C. Davis"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01642v1",
                "http://arxiv.org/pdf/2310.01642v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01636v2",
            "title": "Adaptive Visual Scene Understanding: Incremental Scene Graph Generation",
            "updated": "2023-10-11T02:02:48Z",
            "published": "2023-10-02T21:02:23Z",
            "summary": "Scene graph generation (SGG) involves analyzing images to extract meaningful\ninformation about objects and their relationships. Given the dynamic nature of\nthe visual world, it becomes crucial for AI systems to detect new objects and\nestablish their new relationships with existing objects. To address the lack of\ncontinual learning methodologies in SGG, we introduce the comprehensive\nContinual ScenE Graph Generation (CSEGG) dataset along with 3 learning\nscenarios and 8 evaluation metrics. Our research investigates the continual\nlearning performances of existing SGG methods on the retention of previous\nobject entities and relationships as they learn new ones. Moreover, we also\nexplore how continual object detection enhances generalization in classifying\nknown relationships on unknown objects. We conduct extensive experiments\nbenchmarking and analyzing the classical two-stage SGG methods and the most\nrecent transformer-based SGG methods in continual learning settings, and gain\nvaluable insights into the CSEGG problem. We invite the research community to\nexplore this emerging field of study.",
            "author": [
                "Naitik Khandelwal",
                "Xiao Liu",
                "Mengmi Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01636v2",
                "http://arxiv.org/pdf/2310.01636v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01634v1",
            "title": "Deep Insights into Noisy Pseudo Labeling on Graph Data",
            "updated": "2023-10-02T20:57:11Z",
            "published": "2023-10-02T20:57:11Z",
            "summary": "Pseudo labeling (PL) is a wide-applied strategy to enlarge the labeled\ndataset by self-annotating the potential samples during the training process.\nSeveral works have shown that it can improve the graph learning model\nperformance in general. However, we notice that the incorrect labels can be\nfatal to the graph training process. Inappropriate PL may result in the\nperformance degrading, especially on graph data where the noise can propagate.\nSurprisingly, the corresponding error is seldom theoretically analyzed in the\nliterature. In this paper, we aim to give deep insights of PL on graph learning\nmodels. We first present the error analysis of PL strategy by showing that the\nerror is bounded by the confidence of PL threshold and consistency of\nmulti-view prediction. Then, we theoretically illustrate the effect of PL on\nconvergence property. Based on the analysis, we propose a cautious pseudo\nlabeling methodology in which we pseudo label the samples with highest\nconfidence and multi-view consistency. Finally, extensive experiments\ndemonstrate that the proposed strategy improves graph learning process and\noutperforms other PL strategies on link prediction and node classification\ntasks.",
            "author": [
                "Botao Wang",
                "Jia Li",
                "Yang Liu",
                "Jiashun Cheng",
                "Yu Rong",
                "Wenjia Wang",
                "Fugee Tsung"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01634v1",
                "http://arxiv.org/pdf/2310.01634v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01633v1",
            "title": "Distributionally Robust Path Integral Control",
            "updated": "2023-10-02T20:56:31Z",
            "published": "2023-10-02T20:56:31Z",
            "summary": "We consider a continuous-time continuous-space stochastic optimal control\nproblem, where the controller lacks exact knowledge of the underlying diffusion\nprocess, relying instead on a finite set of historical disturbance\ntrajectories. In situations where data collection is limited, the controller\nsynthesized from empirical data may exhibit poor performance. To address this\nissue, we introduce a novel approach named Distributionally Robust Path\nIntegral (DRPI). The proposed method employs distributionally robust\noptimization (DRO) to robustify the resulting policy against the unknown\ndiffusion process. Notably, the DRPI scheme shows similarities with\nrisk-sensitive control, which enables us to utilize the path integral control\n(PIC) framework as an efficient solution scheme. We derive theoretical\nperformance guarantees for the DRPI scheme, which closely aligns with selecting\na risk parameter in risk-sensitive control. We validate the efficacy of our\nscheme and showcase its superiority when compared to risk-neutral PIC policies\nin the absence of the true diffusion process.",
            "author": [
                "Hyuk Park",
                "Duo Zhou",
                "Grani A. Hanasusanto",
                "Takashi Tanaka"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01633v1",
                "http://arxiv.org/pdf/2310.01633v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01627v1",
            "title": "VAL: Interactive Task Learning with GPT Dialog Parsing",
            "updated": "2023-10-02T20:45:41Z",
            "published": "2023-10-02T20:45:41Z",
            "summary": "Reinforcement learning often requires millions of examples to produce static,\nblack-box models. In contrast, interactive task learning (ITL) emphasizes\nincremental knowledge acquisition from limited instruction provided by humans\nin modalities such as natural language. However, in practice, ITL systems often\nsuffers from brittle, error-prone language parsing. Large language models\n(LLMs) are resistant to brittleness but are not interpretable and cannot learn\nincrementally. We present VAL, an ITL system with a new philosophy for\nLLM/symbolic integration. By using LLMs only for specific tasks -- such as\npredicate and argument selection -- within an algorithmic framework, VAL reaps\nthe benefits of LLMs to support interactive learning of hierarchical task\nknowledge from natural language. Acquired knowledge is human interpretable and\ngeneralizes to support execution of novel tasks without additional training. We\nstudied users' interactions with VAL in a video game setting, finding that most\nusers could successfully teach VAL using language they felt was natural.",
            "author": [
                "Lane Lawley",
                "Christopher J. MacLellan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01627v1",
                "http://arxiv.org/pdf/2310.01627v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC",
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01626v1",
            "title": "Model Explanation via Support Graphs",
            "updated": "2023-10-02T20:40:26Z",
            "published": "2023-10-02T20:40:26Z",
            "summary": "In this note, we introduce the notion of support graph to define explanations\nfor any model of a logic program. An explanation is an acyclic support graph\nthat, for each true atom in the model, induces a proof in terms of program\nrules represented by labels. A classical model may have zero, one or several\nexplanations: when it has at least one, it is called a justified model. We\nprove that all stable models are justified whereas, in general, the opposite\ndoes not hold, at least for disjunctive programs. We also provide a\nmeta-programming encoding in Answer Set Programming that generates the\nexplanations for a given stable model of some program. We prove that the\nencoding is sound and complete, that is, there is a one-to-one correspondence\nbetween each answer set of the encoding and each explanation for the original\nstable model.",
            "author": [
                "Pedro Cabalar",
                "Brais Mu\u00f1iz"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01626v1",
                "http://arxiv.org/pdf/2310.01626v1"
            ],
            "primary_category": "cs.LO",
            "category": [
                "cs.LO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01618v1",
            "title": "Operator Learning Meets Numerical Analysis: Improving Neural Networks\n  through Iterative Methods",
            "updated": "2023-10-02T20:25:36Z",
            "published": "2023-10-02T20:25:36Z",
            "summary": "Deep neural networks, despite their success in numerous applications, often\nfunction without established theoretical foundations. In this paper, we bridge\nthis gap by drawing parallels between deep learning and classical numerical\nanalysis. By framing neural networks as operators with fixed points\nrepresenting desired solutions, we develop a theoretical framework grounded in\niterative methods for operator equations. Under defined conditions, we present\nconvergence proofs based on fixed point theory. We demonstrate that popular\narchitectures, such as diffusion models and AlphaFold, inherently employ\niterative operator learning. Empirical assessments highlight that performing\niterations through network operators improves performance. We also introduce an\niterative graph neural network, PIGN, that further demonstrates benefits of\niterations. Our work aims to enhance the understanding of deep learning by\nmerging insights from numerical analysis, potentially guiding the design of\nfuture networks with clearer theoretical underpinnings and improved\nperformance.",
            "author": [
                "Emanuele Zappala",
                "Daniel Levine",
                "Sizhuang He",
                "Syed Rizvi",
                "Sacha Levy",
                "David van Dijk"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01618v1",
                "http://arxiv.org/pdf/2310.01618v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.NA",
                "math.NA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01599v1",
            "title": "Synthesis technique and electron beam damage study of nanometer-thin\n  single-crystalline Thymine",
            "updated": "2023-10-02T19:45:10Z",
            "published": "2023-10-02T19:45:10Z",
            "summary": "Samples suitable for electron diffraction studies must satisfy certain\ncharacteristics such as having a thickness in the range of 10 - 100 nm. We\nreport, to our knowledge, the first successful synthesis technique of\nnanometer-thin sheets of single-crystalline thymine suitable for electron\ndiffraction and spectroscopy studies. This development provides a well defined\nsystem to explore issues related to UV photochemistry of DNA and high intrinsic\nstability essential to maintaining integrity of genetic information. The\ncrystals are grown using the evaporation technique and the nanometer-thin\nsheets are obtained via microtoming. The sample is characterized via x-ray\ndiffraction (XRD) and is subsequently studied using electron diffraction via a\ntransmission electron microscope (TEM). The electron damage threshold is\ndetermined to be a factor of 10 higher than similar molecular moieties, which\nfurther highlights the extremely fast relaxation processes of electron\nscattering-induced excited states, extending the concept of radiation hardening\nbeyond photoexcited states. The high stability of thymine in particular opens\nthe door for further studies of these ultrafast relaxation processes giving\nrise to the high stability of DNA to UV radiation.",
            "author": [
                "Hazem Daoud",
                "Sreelaja Pulleri Vadhyar",
                "Ehsan Nikbin",
                "Cheng Lu",
                "R. J. Dwayne Miller"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01599v1",
                "http://arxiv.org/pdf/2310.01599v1"
            ],
            "primary_category": "physics.chem-ph",
            "category": [
                "physics.chem-ph",
                "physics.app-ph",
                "physics.bio-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01593v1",
            "title": "Prescribed Fire Modeling using Knowledge-Guided Machine Learning for\n  Land Management",
            "updated": "2023-10-02T19:38:04Z",
            "published": "2023-10-02T19:38:04Z",
            "summary": "In recent years, the increasing threat of devastating wildfires has\nunderscored the need for effective prescribed fire management. Process-based\ncomputer simulations have traditionally been employed to plan prescribed fires\nfor wildfire prevention. However, even simplified process models like QUIC-Fire\nare too compute-intensive to be used for real-time decision-making, especially\nwhen weather conditions change rapidly. Traditional ML methods used for fire\nmodeling offer computational speedup but struggle with physically inconsistent\npredictions, biased predictions due to class imbalance, biased estimates for\nfire spread metrics (e.g., burned area, rate of spread), and generalizability\nin out-of-distribution wind conditions. This paper introduces a novel machine\nlearning (ML) framework that enables rapid emulation of prescribed fires while\naddressing these concerns. By incorporating domain knowledge, the proposed\nmethod helps reduce physical inconsistencies in fuel density estimates in\ndata-scarce scenarios. To overcome the majority class bias in predictions, we\nleverage pre-existing source domain data to augment training data and learn the\nspread of fire more effectively. Finally, we overcome the problem of biased\nestimation of fire spread metrics by incorporating a hierarchical modeling\nstructure to capture the interdependence in fuel density and burned area.\nNotably, improvement in fire metric (e.g., burned area) estimates offered by\nour framework makes it useful for fire managers, who often rely on these fire\nmetric estimates to make decisions about prescribed burn management.\nFurthermore, our framework exhibits better generalization capabilities than the\nother ML-based fire modeling methods across diverse wind conditions and\nignition patterns.",
            "author": [
                "Somya Sharma Chatterjee",
                "Kelly Lindsay",
                "Neel Chatterjee",
                "Rohan Patil",
                "Ilkay Altintas De Callafon",
                "Michael Steinbach",
                "Daniel Giron",
                "Mai H. Nguyen",
                "Vipin Kumar"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01593v1",
                "http://arxiv.org/pdf/2310.01593v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "stat.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01571v1",
            "title": "Contraction Properties of the Global Workspace Primitive",
            "updated": "2023-10-02T19:04:41Z",
            "published": "2023-10-02T19:04:41Z",
            "summary": "To push forward the important emerging research field surrounding multi-area\nrecurrent neural networks (RNNs), we expand theoretically and empirically on\nthe provably stable RNNs of RNNs introduced by Kozachkov et al. in \"RNNs of\nRNNs: Recursive Construction of Stable Assemblies of Recurrent Neural\nNetworks\". We prove relaxed stability conditions for salient special cases of\nthis architecture, most notably for a global workspace modular structure. We\nthen demonstrate empirical success for Global Workspace Sparse Combo Nets with\na small number of trainable parameters, not only through strong overall test\nperformance but also greater resilience to removal of individual subnetworks.\nThese empirical results for the global workspace inter-area topology are\ncontingent on stability preservation, highlighting the relevance of our\ntheoretical work for enabling modular RNN success. Further, by exploring\nsparsity in the connectivity structure between different subnetwork modules\nmore broadly, we improve the state of the art performance for stable RNNs on\nbenchmark sequence processing tasks, thus underscoring the general utility of\nspecialized graph structures for multi-area RNNs.",
            "author": [
                "Michaela Ennis",
                "Leo Kozachkov",
                "Jean-Jacques Slotine"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01571v1",
                "http://arxiv.org/pdf/2310.01571v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.SY",
                "eess.SY",
                "q-bio.NC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01570v1",
            "title": "Magnetization of Antiferromagnetic Cactus Graph Model with Exact Dimer\n  Ground State",
            "updated": "2023-10-02T19:04:21Z",
            "published": "2023-10-02T19:04:21Z",
            "summary": "We introduce and explore the magnetization behavior in a quantum spin system\non a cactus graph, deemed the Cactus Graph Model (CGM), featuring an exact\ndimer singlet ground state. We analyze the singlet-triplet gap, interactions\namong excited triplets, and correlated hopping under an external magnetic field\nusing a strong coupling expansion. Employing an effective hard-core boson\nrepresentation, we unveil density wave magnetization plateaus and supersolid\nphases, revealing the intricate interplay between interactions and hopping\ndynamics. Furthermore, we conjecture that correlated hopping gives rise to\nmulti-triplet bound states, and potentially engendering to the stabilization of\nadditional low-lying magnetization plateaus.",
            "author": [
                "Pratyay Ghosh"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01570v1",
                "http://arxiv.org/pdf/2310.01570v1"
            ],
            "primary_category": "cond-mat.str-el",
            "category": [
                "cond-mat.str-el"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01542v1",
            "title": "Fusing Models with Complementary Expertise",
            "updated": "2023-10-02T18:31:35Z",
            "published": "2023-10-02T18:31:35Z",
            "summary": "Training AI models that generalize across tasks and domains has long been\namong the open problems driving AI research. The emergence of Foundation Models\nmade it easier to obtain expert models for a given task, but the heterogeneity\nof data that may be encountered at test time often means that any single expert\nis insufficient. We consider the Fusion of Experts (FoE) problem of fusing\noutputs of expert models with complementary knowledge of the data distribution\nand formulate it as an instance of supervised learning. Our method is\napplicable to both discriminative and generative tasks and leads to significant\nperformance improvements in image and text classification, text summarization,\nmultiple-choice QA, and automatic evaluation of generated text. We also extend\nour method to the \"frugal\" setting where it is desired to reduce the number of\nexpert model evaluations at test time.",
            "author": [
                "Hongyi Wang",
                "Felipe Maia Polo",
                "Yuekai Sun",
                "Souvik Kundu",
                "Eric Xing",
                "Mikhail Yurochkin"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01542v1",
                "http://arxiv.org/pdf/2310.01542v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01540v1",
            "title": "On the power of geometrically-local classical and quantum circuits",
            "updated": "2023-10-02T18:27:53Z",
            "published": "2023-10-02T18:27:53Z",
            "summary": "We show a relation, based on parallel repetition of the Magic Square game,\nthat can be solved, with probability exponentially close to $1$ (worst-case\ninput), by $1D$ (uniform) depth $2$, geometrically-local, noisy (noise below a\nthreshold), fan-in $4$, quantum circuits. We show that the same relation cannot\nbe solved, with an exponentially small success probability (averaged over\ninputs drawn uniformly), by $1D$ (non-uniform) geometrically-local, sub-linear\ndepth, classical circuits consisting of fan-in $2$ NAND gates. Quantum and\nclassical circuits are allowed to use input-independent\n(geometrically-non-local) resource states, that is entanglement and randomness\nrespectively. To the best of our knowledge, previous best (analogous) depth\nseparation for a task between quantum and classical circuits was constant v/s\nsub-logarithmic, although for general (geometrically non-local) circuits. Our\nhardness result for classical circuits is based on a direct product theorem\nabout classical communication protocols from Jain and Kundu [JK22]. As an\napplication, we propose a protocol that can potentially demonstrate verifiable\nquantum advantage in the NISQ era. We also provide generalizations of our\nresult for higher dimensional circuits as well as a wider class of Bell games.",
            "author": [
                "Kishor Bharti",
                "Rahul Jain"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01540v1",
                "http://arxiv.org/pdf/2310.01540v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.CC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01539v1",
            "title": "Effect of Triangular Pre-Cracks on the Mechanical Behavior of 2D\n  MoTe$_2$: A Molecular Dynamics Study",
            "updated": "2023-10-02T18:26:47Z",
            "published": "2023-10-02T18:26:47Z",
            "summary": "Among two-dimensional (2D) materials, transition metal dichalcogenides (TMDs)\nstand out for their remarkable electronic, optical, and chemical properties. In\naddition to being variable bandgap semiconductor materials, the atomic thinness\nprovides flexibility to TMDs. Therefore, understanding the physical properties\nof TMDs for applications in flexible and wearable devices is crucial. Despite\nthe growing enthusiasm surrounding two-dimensional transition metal\ndichalcogenides (TMDs), our understanding of the mechanical characteristics of\nmolybdenum ditelluride (MoTe$_2$) remains limited. The mechanical properties of\nMoTe$_2$ deteriorate in the presence of pre-existing cracks or vacancy defects,\nwhich are very common in grown TMDs. In this study, the fracture properties and\ncrack propagation of monolayer molybdenum ditelluride (MoTe$_2$) sheets\ncontaining pre-existing triangular cracks with various vertex angles are\ninvestigated by performing molecular dynamics (MD) simulations of uniaxial and\nbiaxial tensile loading. Due to pre-crack length, angle, and perimeter\nvariations, monolayer MoTe$_2$ with pre-existing cracks underwent considerable\nchanges in Young's modulus, tensile strength, fracture toughness, and fracture\nstrain values. We have found that the pre-cracked MoTe$_2$ is more brittle than\nits pristine counterpart. Regulated alteration of pre-crack angle under\nconstant simulation conditions improved the uniaxial mechanical properties.\nSimilarly, regulated alteration of the perimeter of the pre-crack resulted in\nimproved biaxial mechanical properties. This study contributes to the\nfoundational knowledge for advanced design strategies involving strain\nengineering in MoTe$_2$ and other similar transition metal dichalcogenides.",
            "author": [
                "Md. Jobayer Aziz",
                "Md Akibul Islam",
                "Md. Rezwanul Karim",
                "Arafat Ahmed Bhuiyan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01539v1",
                "http://arxiv.org/pdf/2310.01539v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01537v1",
            "title": "Adversarial Client Detection via Non-parametric Subspace Monitoring in\n  the Internet of Federated Things",
            "updated": "2023-10-02T18:25:02Z",
            "published": "2023-10-02T18:25:02Z",
            "summary": "The Internet of Federated Things (IoFT) represents a network of\ninterconnected systems with federated learning as the backbone, facilitating\ncollaborative knowledge acquisition while ensuring data privacy for individual\nsystems. The wide adoption of IoFT, however, is hindered by security concerns,\nparticularly the susceptibility of federated learning networks to adversarial\nattacks. In this paper, we propose an effective non-parametric approach FedRR,\nwhich leverages the low-rank features of the transmitted parameter updates\ngenerated by federated learning to address the adversarial attack problem.\nBesides, our proposed method is capable of accurately detecting adversarial\nclients and controlling the false alarm rate under the scenario with no attack\noccurring. Experiments based on digit recognition using the MNIST datasets\nvalidated the advantages of our approach.",
            "author": [
                "Xianjian Xie",
                "Xiaochen Xian",
                "Dan Li",
                "Andi Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01537v1",
                "http://arxiv.org/pdf/2310.01537v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01412v2",
            "title": "DriveGPT4: Interpretable End-to-end Autonomous Driving via Large\n  Language Model",
            "updated": "2023-10-08T13:47:23Z",
            "published": "2023-10-02T17:59:52Z",
            "summary": "In the past decade, autonomous driving has experienced rapid development in\nboth academia and industry. However, its limited interpretability remains a\nsignificant unsolved problem, severely hindering autonomous vehicle\ncommercialization and further development. Previous approaches utilizing small\nlanguage models have failed to address this issue due to their lack of\nflexibility, generalization ability, and robustness. Recently, multimodal large\nlanguage models (LLMs) have gained considerable attention from the research\ncommunity for their capability to process and reason non-text data (e.g.,\nimages and videos) by text. In this paper, we present DriveGPT4, an\ninterpretable end-to-end autonomous driving system utilizing LLMs. DriveGPT4 is\ncapable of interpreting vehicle actions and providing corresponding reasoning,\nas well as answering diverse questions posed by human users for enhanced\ninteraction. Additionally, DriveGPT4 predicts vehicle low-level control signals\nin an end-to-end fashion. These capabilities stem from a customized visual\ninstruction tuning dataset specifically designed for autonomous driving. To the\nbest of our knowledge, DriveGPT4 is the first work focusing on interpretable\nend-to-end autonomous driving. When evaluated on multiple tasks alongside\nconventional methods and video understanding LLMs, DriveGPT4 demonstrates\nsuperior qualitative and quantitative performance. Additionally, DriveGPT4 can\nbe generalized in a zero-shot fashion to accommodate more unseen scenarios. The\nproject page is available at https://tonyxuqaq.github.io/projects/DriveGPT4/ .",
            "author": [
                "Zhenhua Xu",
                "Yujia Zhang",
                "Enze Xie",
                "Zhen Zhao",
                "Yong Guo",
                "Kwan-Yee. K. Wong",
                "Zhenguo Li",
                "Hengshuang Zhao"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01412v2",
                "http://arxiv.org/pdf/2310.01412v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01410v1",
            "title": "LEAP: Liberate Sparse-view 3D Modeling from Camera Poses",
            "updated": "2023-10-02T17:59:37Z",
            "published": "2023-10-02T17:59:37Z",
            "summary": "Are camera poses necessary for multi-view 3D modeling? Existing approaches\npredominantly assume access to accurate camera poses. While this assumption\nmight hold for dense views, accurately estimating camera poses for sparse views\nis often elusive. Our analysis reveals that noisy estimated poses lead to\ndegraded performance for existing sparse-view 3D modeling methods. To address\nthis issue, we present LEAP, a novel pose-free approach, therefore challenging\nthe prevailing notion that camera poses are indispensable. LEAP discards\npose-based operations and learns geometric knowledge from data. LEAP is\nequipped with a neural volume, which is shared across scenes and is\nparameterized to encode geometry and texture priors. For each incoming scene,\nwe update the neural volume by aggregating 2D image features in a\nfeature-similarity-driven manner. The updated neural volume is decoded into the\nradiance field, enabling novel view synthesis from any viewpoint. On both\nobject-centric and scene-level datasets, we show that LEAP significantly\noutperforms prior methods when they employ predicted poses from\nstate-of-the-art pose estimators. Notably, LEAP performs on par with prior\napproaches that use ground-truth poses while running $400\\times$ faster than\nPixelNeRF. We show LEAP generalizes to novel object categories and scenes, and\nlearns knowledge closely resembles epipolar geometry. Project page:\nhttps://hwjiang1510.github.io/LEAP/",
            "author": [
                "Hanwen Jiang",
                "Zhenyu Jiang",
                "Yue Zhao",
                "Qixing Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01410v1",
                "http://arxiv.org/pdf/2310.01410v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01404v2",
            "title": "H-InDex: Visual Reinforcement Learning with Hand-Informed\n  Representations for Dexterous Manipulation",
            "updated": "2023-10-13T03:14:16Z",
            "published": "2023-10-02T17:59:03Z",
            "summary": "Human hands possess remarkable dexterity and have long served as a source of\ninspiration for robotic manipulation. In this work, we propose a human\n$\\textbf{H}$and$\\textbf{-In}$formed visual representation learning framework to\nsolve difficult $\\textbf{Dex}$terous manipulation tasks ($\\textbf{H-InDex}$)\nwith reinforcement learning. Our framework consists of three stages: (i)\npre-training representations with 3D human hand pose estimation, (ii) offline\nadapting representations with self-supervised keypoint detection, and (iii)\nreinforcement learning with exponential moving average BatchNorm. The last two\nstages only modify $0.36\\%$ parameters of the pre-trained representation in\ntotal, ensuring the knowledge from pre-training is maintained to the full\nextent. We empirically study 12 challenging dexterous manipulation tasks and\nfind that H-InDex largely surpasses strong baseline methods and the recent\nvisual foundation models for motor control. Code is available at\nhttps://yanjieze.com/H-InDex .",
            "author": [
                "Yanjie Ze",
                "Yuyao Liu",
                "Ruizhe Shi",
                "Jiaxin Qin",
                "Zhecheng Yuan",
                "Jiashun Wang",
                "Huazhe Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01404v2",
                "http://arxiv.org/pdf/2310.01404v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01402v1",
            "title": "Evaluating the Decency and Consistency of Data Validation Tests\n  Generated by LLMs",
            "updated": "2023-10-02T17:58:52Z",
            "published": "2023-10-02T17:58:52Z",
            "summary": "We investigated the potential of large language models (LLMs) in developing\ndataset validation tests. We carried out 96 experiments each for both GPT-3.5\nand GPT-4, examining different prompt scenarios, learning modes, temperature\nsettings, and roles. The prompt scenarios were: 1) Asking for expectations, 2)\nAsking for expectations with a given context, 3) Asking for expectations after\nrequesting a simulation, and 4) Asking for expectations with a provided data\nsample. For learning modes, we tested: 1) zero-shot, 2) one-shot, and 3)\nfew-shot learning. We also tested four temperature settings: 0, 0.4, 0.6, and\n1. Furthermore, two distinct roles were considered: 1) \"helpful assistant\", 2)\n\"expert data scientist\". To gauge consistency, every setup was tested five\ntimes. The LLM-generated responses were benchmarked against a gold standard\nsuite, created by an experienced data scientist knowledgeable about the data in\nquestion. We find there are considerable returns to the use of few-shot\nlearning, and that the more explicit the data setting can be the better. The\nbest LLM configurations complement, rather than substitute, the gold standard\nresults. This study underscores the value LLMs can bring to the data cleaning\nand preparation stages of the data science workflow.",
            "author": [
                "Rohan Alexander",
                "Lindsay Katz",
                "Callandra Moore",
                "Zane Schwartz"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01402v1",
                "http://arxiv.org/pdf/2310.01402v1"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01397v2",
            "title": "Posterior Uncertainty Estimation via a Monte Carlo Procedure Specialized\n  for Data Assimilation",
            "updated": "2023-11-15T15:18:12Z",
            "published": "2023-10-02T17:55:39Z",
            "summary": "Through the Bayesian lens of data assimilation, uncertainty on model\nparameters is traditionally quantified through the posterior covariance matrix.\nHowever, in modern settings involving high-dimensional and computationally\nexpensive forward models, posterior covariance knowledge must be relaxed to\ndeterministic or stochastic approximations. In the carbon flux inversion\nliterature, Chevallier et al. proposed a stochastic method capable of\napproximating posterior variances of linear functionals of the model parameters\nthat is particularly well-suited for large-scale Earth-system data assimilation\ntasks. This note formalizes this algorithm and clarifies its properties. We\nprovide a formal statement of the algorithm, demonstrate why it converges to\nthe desired posterior variance quantity of interest, and provide additional\nuncertainty quantification allowing incorporation of the Monte Carlo sampling\nuncertainty into the method's Bayesian credible intervals. The methodology is\ndemonstrated using toy simulations and a realistic carbon flux inversion\nobserving system simulation experiment.",
            "author": [
                "Michael Stanley",
                "Mikael Kuusela",
                "Brendan Byrne",
                "Junjie Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01397v2",
                "http://arxiv.org/pdf/2310.01397v2"
            ],
            "primary_category": "stat.CO",
            "category": [
                "stat.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01387v1",
            "title": "It's MBR All the Way Down: Modern Generation Techniques Through the Lens\n  of Minimum Bayes Risk",
            "updated": "2023-10-02T17:47:10Z",
            "published": "2023-10-02T17:47:10Z",
            "summary": "Minimum Bayes Risk (MBR) decoding is a method for choosing the outputs of a\nmachine learning system based not on the output with the highest probability,\nbut the output with the lowest risk (expected error) among multiple candidates.\nIt is a simple but powerful method: for an additional cost at inference time,\nMBR provides reliable several-point improvements across metrics for a wide\nvariety of tasks without any additional data or training. Despite this, MBR is\nnot frequently applied in NLP works, and knowledge of the method itself is\nlimited. We first provide an introduction to the method and the recent\nliterature. We show that several recent methods that do not reference MBR can\nbe written as special cases of MBR; this reformulation provides additional\ntheoretical justification for the performance of these methods, explaining some\nresults that were previously only empirical. We provide theoretical and\nempirical results about the effectiveness of various MBR variants and make\nconcrete recommendations for the application of MBR in NLP models, including\nfuture directions in this area.",
            "author": [
                "Amanda Bertsch",
                "Alex Xie",
                "Graham Neubig",
                "Matthew R. Gormley"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01387v1",
                "http://arxiv.org/pdf/2310.01387v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01382v1",
            "title": "Compressing LLMs: The Truth is Rarely Pure and Never Simple",
            "updated": "2023-10-02T17:42:37Z",
            "published": "2023-10-02T17:42:37Z",
            "summary": "Despite their remarkable achievements, modern Large Language Models (LLMs)\nencounter exorbitant computational and memory footprints. Recently, several\nworks have shown significant success in training-free and data-free compression\n(pruning and quantization) of LLMs achieving 50-60% sparsity and reducing the\nbit-width down to 3 or 4 bits per weight, with negligible perplexity\ndegradation over the uncompressed baseline. As recent research efforts are\nfocused on developing increasingly sophisticated compression methods, our work\ntakes a step back, and re-evaluates the effectiveness of existing SoTA\ncompression methods, which rely on a fairly simple and widely questioned\nmetric, perplexity (even for dense LLMs). We introduce Knowledge-Intensive\nCompressed LLM BenchmarK (LLM-KICK), a collection of carefully-curated tasks to\nre-define the evaluation protocol for compressed LLMs, which have significant\nalignment with their dense counterparts, and perplexity fail to capture subtle\nchange in their true capabilities. LLM-KICK unveils many favorable merits and\nunfortunate plights of current SoTA compression methods: all pruning methods\nsuffer significant performance degradation, sometimes at trivial sparsity\nratios (e.g., 25-30%), and fail for N:M sparsity on knowledge-intensive tasks;\ncurrent quantization methods are more successful than pruning; yet, pruned LLMs\neven at $\\geq 50$% sparsity are robust in-context retrieval and summarization\nsystems; among others. LLM-KICK is designed to holistically access compressed\nLLMs' ability for language understanding, reasoning, generation, in-context\nretrieval, in-context summarization, etc. We hope our study can foster the\ndevelopment of better LLM compression methods. All our related codes are planed\nto be open-sourced.",
            "author": [
                "Ajay Jaiswal",
                "Zhe Gan",
                "Xianzhi Du",
                "Bowen Zhang",
                "Zhangyang Wang",
                "Yinfei Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01382v1",
                "http://arxiv.org/pdf/2310.01382v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01378v1",
            "title": "On Grid Graph Reachability and Puzzle Games",
            "updated": "2023-10-02T17:41:35Z",
            "published": "2023-10-02T17:41:35Z",
            "summary": "Many puzzle video games, like Sokoban, involve moving some agent in a maze.\nThe reachable locations are usually apparent for a human player, and the\ndifficulty of the game is mainly related to performing actions on objects, such\nas pushing (reachable) boxes. For this reason, the difficulty of a particular\nlevel is often measured as the number of actions on objects, other than agent\nwalking, needed to find a solution. In this paper we study CP and SAT\napproaches for solving these kind of problems. We review some reachability\nencodings and propose a new one. We empirically show that the new encoding is\nwell-suited for solving puzzle problems in the planning as SAT paradigm,\nespecially when considering the execution of several actions in parallel.",
            "author": [
                "Miquel Bofill",
                "Cristina Borralleras",
                "Joan Espasa",
                "Mateu Villaret"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01378v1",
                "http://arxiv.org/pdf/2310.01378v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01376v3",
            "title": "Towards Distribution-Agnostic Generalized Category Discovery",
            "updated": "2023-11-01T14:01:04Z",
            "published": "2023-10-02T17:39:58Z",
            "summary": "Data imbalance and open-ended distribution are two intrinsic characteristics\nof the real visual world. Though encouraging progress has been made in tackling\neach challenge separately, few works dedicated to combining them towards\nreal-world scenarios. While several previous works have focused on classifying\nclose-set samples and detecting open-set samples during testing, it's still\nessential to be able to classify unknown subjects as human beings. In this\npaper, we formally define a more realistic task as distribution-agnostic\ngeneralized category discovery (DA-GCD): generating fine-grained predictions\nfor both close- and open-set classes in a long-tailed open-world setting. To\ntackle the challenging problem, we propose a Self-Balanced Co-Advice\ncontrastive framework (BaCon), which consists of a contrastive-learning branch\nand a pseudo-labeling branch, working collaboratively to provide interactive\nsupervision to resolve the DA-GCD task. In particular, the contrastive-learning\nbranch provides reliable distribution estimation to regularize the predictions\nof the pseudo-labeling branch, which in turn guides contrastive learning\nthrough self-balanced knowledge transfer and a proposed novel contrastive loss.\nWe compare BaCon with state-of-the-art methods from two closely related fields:\nimbalanced semi-supervised learning and generalized category discovery. The\neffectiveness of BaCon is demonstrated with superior performance over all\nbaselines and comprehensive analysis across various datasets. Our code is\npublicly available.",
            "author": [
                "Jianhong Bai",
                "Zuozhu Liu",
                "Hualiang Wang",
                "Ruizhe Chen",
                "Lianrui Mu",
                "Xiaomeng Li",
                "Joey Tianyi Zhou",
                "Yang Feng",
                "Jian Wu",
                "Haoji Hu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01376v3",
                "http://arxiv.org/pdf/2310.01376v3"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01366v2",
            "title": "Window-based Model Averaging Improves Generalization in Heterogeneous\n  Federated Learning",
            "updated": "2023-10-10T08:39:31Z",
            "published": "2023-10-02T17:30:14Z",
            "summary": "Federated Learning (FL) aims to learn a global model from distributed users\nwhile protecting their privacy. However, when data are distributed\nheterogeneously the learning process becomes noisy, unstable, and biased\ntowards the last seen clients' data, slowing down convergence. To address these\nissues and improve the robustness and generalization capabilities of the global\nmodel, we propose WIMA (Window-based Model Averaging). WIMA aggregates global\nmodels from different rounds using a window-based approach, effectively\ncapturing knowledge from multiple users and reducing the bias from the last\nones. By adopting a windowed view on the rounds, WIMA can be applied from the\ninitial stages of training. Importantly, our method introduces no additional\ncommunication or client-side computation overhead. Our experiments demonstrate\nthe robustness of WIMA against distribution shifts and bad client sampling,\nresulting in smoother and more stable learning trends. Additionally, WIMA can\nbe easily integrated with state-of-the-art algorithms. We extensively evaluate\nour approach on standard FL benchmarks, demonstrating its effectiveness.",
            "author": [
                "Debora Caldarola",
                "Barbara Caputo",
                "Marco Ciccone"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01366v2",
                "http://arxiv.org/pdf/2310.01366v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01356v1",
            "title": "Less is More: Toward Zero-Shot Local Scene Graph Generation via\n  Foundation Models",
            "updated": "2023-10-02T17:19:04Z",
            "published": "2023-10-02T17:19:04Z",
            "summary": "Humans inherently recognize objects via selective visual perception,\ntransform specific regions from the visual field into structured symbolic\nknowledge, and reason their relationships among regions based on the allocation\nof limited attention resources in line with humans' goals. While it is\nintuitive for humans, contemporary perception systems falter in extracting\nstructural information due to the intricate cognitive abilities and commonsense\nknowledge required. To fill this gap, we present a new task called Local Scene\nGraph Generation. Distinct from the conventional scene graph generation task,\nwhich encompasses generating all objects and relationships in an image, our\nproposed task aims to abstract pertinent structural information with partial\nobjects and their relationships for boosting downstream tasks that demand\nadvanced comprehension and reasoning capabilities. Correspondingly, we\nintroduce zEro-shot Local scEne GrAph geNeraTion (ELEGANT), a framework\nharnessing foundation models renowned for their powerful perception and\ncommonsense reasoning, where collaboration and information communication among\nfoundation models yield superior outcomes and realize zero-shot local scene\ngraph generation without requiring labeled supervision. Furthermore, we propose\na novel open-ended evaluation metric, Entity-level CLIPScorE (ECLIPSE),\nsurpassing previous closed-set evaluation metrics by transcending their limited\nlabel space, offering a broader assessment. Experiment results show that our\napproach markedly outperforms baselines in the open-ended evaluation setting,\nand it also achieves a significant performance boost of up to 24.58% over prior\nmethods in the close-set setting, demonstrating the effectiveness and powerful\nreasoning ability of our proposed framework.",
            "author": [
                "Shu Zhao",
                "Huijuan Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01356v1",
                "http://arxiv.org/pdf/2310.01356v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01353v1",
            "title": "Scaling Up Music Information Retrieval Training with Semi-Supervised\n  Learning",
            "updated": "2023-10-02T17:16:47Z",
            "published": "2023-10-02T17:16:47Z",
            "summary": "In the era of data-driven Music Information Retrieval (MIR), the scarcity of\nlabeled data has been one of the major concerns to the success of an MIR task.\nIn this work, we leverage the semi-supervised teacher-student training approach\nto improve MIR tasks. For training, we scale up the unlabeled music data to\n240k hours, which is much larger than any public MIR datasets. We iteratively\ncreate and refine the pseudo-labels in the noisy teacher-student training\nprocess. Knowledge expansion is also explored to iteratively scale up the model\nsizes from as small as less than 3M to almost 100M parameters. We study the\nperformance correlation between data size and model size in the experiments. By\nscaling up both model size and training data, our models achieve\nstate-of-the-art results on several MIR tasks compared to models that are\neither trained in a supervised manner or based on a self-supervised pretrained\nmodel. To our knowledge, this is the first attempt to study the effects of\nscaling up both model and training data for a variety of MIR tasks.",
            "author": [
                "Yun-Ning Hung",
                "Ju-Chiang Wang",
                "Minz Won",
                "Duc Le"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01353v1",
                "http://arxiv.org/pdf/2310.01353v1"
            ],
            "primary_category": "eess.AS",
            "category": [
                "eess.AS",
                "cs.SD"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01352v3",
            "title": "RA-DIT: Retrieval-Augmented Dual Instruction Tuning",
            "updated": "2023-11-05T06:25:55Z",
            "published": "2023-10-02T17:16:26Z",
            "summary": "Retrieval-augmented language models (RALMs) improve performance by accessing\nlong-tail and up-to-date knowledge from external data stores, but are\nchallenging to build. Existing approaches require either expensive\nretrieval-specific modifications to LM pre-training or use post-hoc integration\nof the data store that leads to suboptimal performance. We introduce\nRetrieval-Augmented Dual Instruction Tuning (RA-DIT), a lightweight fine-tuning\nmethodology that provides a third option by retrofitting any LLM with retrieval\ncapabilities. Our approach operates in two distinct fine-tuning steps: (1) one\nupdates a pre-trained LM to better use retrieved information, while (2) the\nother updates the retriever to return more relevant results, as preferred by\nthe LM. By fine-tuning over tasks that require both knowledge utilization and\ncontextual awareness, we demonstrate that each stage yields significant\nperformance improvements, and using both leads to additional gains. Our best\nmodel, RA-DIT 65B, achieves state-of-the-art performance across a range of\nknowledge-intensive zero- and few-shot learning benchmarks, significantly\noutperforming existing in-context RALM approaches by up to +8.9% in 0-shot\nsetting and +1.4% in 5-shot setting on average.",
            "author": [
                "Xi Victoria Lin",
                "Xilun Chen",
                "Mingda Chen",
                "Weijia Shi",
                "Maria Lomeli",
                "Rich James",
                "Pedro Rodriguez",
                "Jacob Kahn",
                "Gergely Szilvasy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Scott Yih"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01352v3",
                "http://arxiv.org/pdf/2310.01352v3"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01340v1",
            "title": "Extensions of Schoen--Simon--Yau and Schoen--Simon theorems via\n  iteration \u00e0 la De Giorgi",
            "updated": "2023-10-02T17:04:18Z",
            "published": "2023-10-02T17:04:18Z",
            "summary": "We give an alternative proof of the Schoen--Simon--Yau curvature estimates\nand associated Bernstein-type theorems (1975), and extend the original result\nby including the case of $6$-dimensional (stable minimal) immersions. The key\nstep is an $\\epsilon$-regularity theorem, that assumes smallness of the\nscale-invariant $L^2$ norm of the second fundamental form.\n  Further, we obtain a graph description, in the Lipschitz multi-valued sense,\nfor any stable minimal immersion of dimension $n\\geq 2$, that may have a\nsingular set $\\Sigma$ of locally finite $\\mathcal{H}^{n-2}$-measure, and that\nis weakly close to a hyperplane. (In fact, if $\\mathcal{H}^{n-2}(\\Sigma)=0$,\nthe conclusion is strengthened to a union of smooth graphs.) This follows\ndirectly from an $\\epsilon$-regularity theorem, that assumes smallness of the\nscale-invariant $L^2$ tilt-excess (verified when the hypersurface is weakly\nclose to a hyperplane). A further direct consequence is that (for an immersed\nhypersurface of the type considered) any tangent cone supported on a hyperplane\nis the unique tangent. Specialising the multi-valued decomposition to the case\nof embeddings, we recover the Schoen--Simon theorem (1981).\n  In both $\\epsilon$-regularity theorems the relevant quantity (respectively,\nlength of the second fundamental form and tilt function) solves a non-linear\nPDE on the immersed minimal hypersurface. The proof is carried out\nintrinsically (without linearising the PDE) by implementing an iteration method\n\\`{a} la De Giorgi (from the linear De Giorgi--Nash--Moser theory). Stability\nimplies estimates (intrinsic weak Caccioppoli inequalities) that make the\niteration effective despite the non-linear framework. (In both\n$\\epsilon$-regularity theorems the method gives explicit constants that\nquantify the required smallness.)",
            "author": [
                "Costante Bellettini"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01340v1",
                "http://arxiv.org/pdf/2310.01340v1"
            ],
            "primary_category": "math.AP",
            "category": [
                "math.AP",
                "math.DG",
                "49Q05, 35J15, 53C42, 35J60, 53A10, 49Q20"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01469v2",
            "title": "LLM Lies: Hallucinations are not Bugs, but Features as Adversarial\n  Examples",
            "updated": "2023-10-04T17:53:49Z",
            "published": "2023-10-02T17:01:56Z",
            "summary": "Large Language Models (LLMs), including GPT-3.5, LLaMA, and PaLM, seem to be\nknowledgeable and able to adapt to many tasks. However, we still can not\ncompletely trust their answer, since LLMs suffer from\nhallucination--fabricating non-existent facts to cheat users without\nperception. And the reasons for their existence and pervasiveness remain\nunclear. In this paper, we demonstrate that non-sense prompts composed of\nrandom tokens can also elicit the LLMs to respond with hallucinations. This\nphenomenon forces us to revisit that hallucination may be another view of\nadversarial examples, and it shares similar features with conventional\nadversarial examples as the basic feature of LLMs. Therefore, we formalize an\nautomatic hallucination triggering method as the hallucination attack in an\nadversarial way. Finally, we explore basic feature of attacked adversarial\nprompts and propose a simple yet effective defense strategy. Our code is\nreleased on GitHub.",
            "author": [
                "Jia-Yu Yao",
                "Kun-Peng Ning",
                "Zhen-Hui Liu",
                "Mu-Nan Ning",
                "Li Yuan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01469v2",
                "http://arxiv.org/pdf/2310.01469v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01334v1",
            "title": "Merge, Then Compress: Demystify Efficient SMoE with Hints from Its\n  Routing Policy",
            "updated": "2023-10-02T16:51:32Z",
            "published": "2023-10-02T16:51:32Z",
            "summary": "Sparsely activated Mixture-of-Experts (SMoE) has shown promise to scale up\nthe learning capacity of neural networks, however, they have issues like (a)\nHigh Memory Usage, due to duplication of the network layers into multiple\ncopies as experts; and (b) Redundancy in Experts, as common learning-based\nrouting policies suffer from representational collapse. Therefore, vanilla SMoE\nmodels are memory inefficient and non-scalable, especially for\nresource-constrained downstream scenarios. In this paper, we ask: Can we craft\na compact SMoE model by consolidating expert information? What is the best\nrecipe to merge multiple experts into fewer but more knowledgeable experts? Our\npilot investigation reveals that conventional model merging methods fail to be\neffective in such expert merging for SMoE. The potential reasons are: (1)\nredundant information overshadows critical experts; (2) appropriate neuron\npermutation for each expert is missing to bring all of them in alignment. To\naddress this, we propose M-SMoE, which leverages routing statistics to guide\nexpert merging. Specifically, it starts with neuron permutation alignment for\nexperts; then, dominant experts and their \"group members\" are formed; lastly,\nevery expert group is merged into a single expert by utilizing each expert's\nactivation frequency as their weight for merging, thus diminishing the impact\nof insignificant experts. Moreover, we observed that our proposed merging\npromotes a low dimensionality in the merged expert's weight space, naturally\npaving the way for additional compression. Hence, our final method, MC-SMoE\n(i.e., Merge, then Compress SMoE), further decomposes the merged experts into\nlow-rank and structural sparse alternatives. Extensive experiments across 8\nbenchmarks validate the effectiveness of MC-SMoE. For instance, our MC-SMoE\nachieves up to 80% memory and a 20% FLOPs reduction, with virtually no loss in\nperformance.",
            "author": [
                "Pingzhi Li",
                "Zhenyu Zhang",
                "Prateek Yadav",
                "Yi-Lin Sung",
                "Yu Cheng",
                "Mohit Bansal",
                "Tianlong Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01334v1",
                "http://arxiv.org/pdf/2310.01334v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01331v2",
            "title": "ChoiceMates: Supporting Unfamiliar Online Decision-Making with\n  Multi-Agent Conversational Interactions",
            "updated": "2023-11-14T06:02:15Z",
            "published": "2023-10-02T16:49:39Z",
            "summary": "Unfamiliar decisions -- decisions where people lack adequate domain knowledge\nor expertise -- specifically increase the complexity and uncertainty of the\nprocess of searching for, understanding, and making decisions with online\ninformation. Through our formative study (n=14), we observed users' challenges\nin accessing diverse perspectives, identifying relevant information, and\ndeciding the right moment to make the final decision. We present ChoiceMates, a\nsystem that enables conversations with a dynamic set of LLM-powered agents for\na holistic domain understanding and efficient discovery and management of\ninformation to make decisions. Agents, as opinionated personas, flexibly join\nthe conversation, not only providing responses but also conversing among\nthemselves to elicit each agent's preferences. Our between-subjects study\n(n=36) comparing ChoiceMates to conventional web search and single-agent showed\nthat ChoiceMates was more helpful in discovering, diving deeper, and managing\ninformation compared to Web with higher confidence. We also describe how\nparticipants utilized multi-agent conversations in their decision-making\nprocess.",
            "author": [
                "Jeongeon Park",
                "Bryan Min",
                "Xiaojuan Ma",
                "Juho Kim"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01331v2",
                "http://arxiv.org/pdf/2310.01331v2"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01330v1",
            "title": "Towards reporting bias in visual-language datasets: bimodal augmentation\n  by decoupling object-attribute association",
            "updated": "2023-10-02T16:48:50Z",
            "published": "2023-10-02T16:48:50Z",
            "summary": "Reporting bias arises when people assume that some knowledge is universally\nunderstood and hence, do not necessitate explicit elaboration. In this paper,\nwe focus on the wide existence of reporting bias in visual-language datasets,\nembodied as the object-attribute association, which can subsequentially degrade\nmodels trained on them. To mitigate this bias, we propose a bimodal\naugmentation (BiAug) approach through object-attribute decoupling to flexibly\nsynthesize visual-language examples with a rich array of object-attribute\npairing and construct cross-modal hard negatives. We employ large language\nmodels (LLMs) in conjunction with a grounding object detector to extract target\nobjects. Subsequently, the LLM generates a detailed attribute description for\neach object and produces a corresponding hard negative counterpart. An\ninpainting model is then used to create images based on these detailed object\ndescriptions. By doing so, the synthesized examples explicitly complement\nomitted objects and attributes to learn, and the hard negative pairs steer the\nmodel to distinguish object attributes. Our experiments demonstrated that BiAug\nis superior in object-attribute understanding. In addition, BiAug also improves\nthe performance on zero-shot retrieval tasks on general benchmarks like MSCOCO\nand Flickr30K. BiAug refines the way of collecting text-image datasets.\nMitigating the reporting bias helps models achieve a deeper understanding of\nvisual-language phenomena, expanding beyond mere frequent patterns to encompass\nthe richness and diversity of real-world scenarios.",
            "author": [
                "Qiyu Wu",
                "Mengjie Zhao",
                "Yutong He",
                "Lang Huang",
                "Junya Ono",
                "Hiromi Wakaki",
                "Yuki Mitsufuji"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01330v1",
                "http://arxiv.org/pdf/2310.01330v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01329v1",
            "title": "BTR: Binary Token Representations for Efficient Retrieval Augmented\n  Language Models",
            "updated": "2023-10-02T16:48:47Z",
            "published": "2023-10-02T16:48:47Z",
            "summary": "Retrieval augmentation addresses many critical problems in large language\nmodels such as hallucination, staleness, and privacy leaks. However, running\nretrieval-augmented language models (LMs) is slow and difficult to scale due to\nprocessing large amounts of retrieved text. We introduce binary token\nrepresentations (BTR), which use 1-bit vectors to precompute every token in\npassages, significantly reducing computation during inference. Despite the\npotential loss of accuracy, our new calibration techniques and training\nobjectives restore performance. Combined with offline and runtime compression,\nthis only requires 127GB of disk space for encoding 3 billion tokens in\nWikipedia. Our experiments show that on five knowledge-intensive NLP tasks, BTR\naccelerates state-of-the-art inference by up to 4x and reduces storage by over\n100x while maintaining over 95% task performance.",
            "author": [
                "Qingqing Cao",
                "Sewon Min",
                "Yizhong Wang",
                "Hannaneh Hajishirzi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01329v1",
                "http://arxiv.org/pdf/2310.01329v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01322v1",
            "title": "A combinatorial model for the moduli of bordered Riemann surfaces and a\n  compactification",
            "updated": "2023-10-02T16:31:42Z",
            "published": "2023-10-02T16:31:42Z",
            "summary": "We construct a combinatorial moduli space closely related to the\nKSV-compactification of the moduli space of bordered marked Riemann surfaces.\nThe open part arises from symmetric metric ribbon graphs. The compactification\nis obtained by considering sequences of non contractible subgraphs. This leads\nto a partial real blow-up of rational cells that together form a compact\norbi-cell space. For genus zero the constructed space gives an orbi-cell\ndecomposition of the corresponding analytic moduli space decorated by real\nnumbers and a compactification of this space. In higher genus the relation is\nmore involved, as we briefly explain. The spaces we construct are of interest\nin their own right as they are constructed directly from an interesting class\nof graphs.",
            "author": [
                "Ralph Kaufmann",
                "Javier Z\u00fa\u00f1iga"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01322v1",
                "http://arxiv.org/pdf/2310.01322v1"
            ],
            "primary_category": "math.GT",
            "category": [
                "math.GT",
                "32G15, 57Q15, 57R18"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01319v1",
            "title": "CAD: Clustering And Deep Reinforcement Learning Based Multi-Period\n  Portfolio Management Strategy",
            "updated": "2023-10-02T16:27:04Z",
            "published": "2023-10-02T16:27:04Z",
            "summary": "In this paper, we present a novel trading strategy that integrates\nreinforcement learning methods with clustering techniques for portfolio\nmanagement in multi-period trading. Specifically, we leverage the clustering\nmethod to categorize stocks into various clusters based on their financial\nindices. Subsequently, we utilize the algorithm Asynchronous Advantage\nActor-Critic to determine the trading actions for stocks within each cluster.\nFinally, we employ the algorithm DDPG to generate the portfolio weight vector,\nwhich decides the amount of stocks to buy, sell, or hold according to the\ntrading actions of different clusters. To the best of our knowledge, our\napproach is the first to combine clustering methods and reinforcement learning\nmethods for portfolio management in the context of multi-period trading.\n  Our proposed strategy is evaluated using a series of back-tests on four\ndatasets, comprising a of 800 stocks, obtained from the Shanghai Stock Exchange\nand National Association of Securities Deal Automated Quotations sources. Our\nresults demonstrate that our approach outperforms conventional portfolio\nmanagement techniques, such as the Robust Median Reversion strategy, Passive\nAggressive Median Reversion Strategy, and several machine learning methods,\nacross various metrics. In our back-test experiments, our proposed strategy\nyields an average return of 151% over 360 trading periods with 800 stocks,\ncompared to the highest return of 124% achieved by other techniques over\nidentical trading periods and stocks.",
            "author": [
                "Zhengyong Jiang",
                "Jeyan Thiayagalingam",
                "Jionglong Su",
                "Jinjun Liang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01319v1",
                "http://arxiv.org/pdf/2310.01319v1"
            ],
            "primary_category": "q-fin.PM",
            "category": [
                "q-fin.PM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01318v2",
            "title": "Subgraph densities and scaling limits of random graphs with a prescribed\n  modular decomposition",
            "updated": "2023-10-23T13:59:01Z",
            "published": "2023-10-02T16:26:44Z",
            "summary": "We consider large uniform labeled random graphs in different classes with\nprescribed decorations in their modular decomposition. Our main result is the\nestimation of the number of copies of every graph as an induced subgraph. As a\nconsequence, we obtain the convergence of a uniform random graph in such\nclasses to a Brownian limit object in the space of graphons.\n  Our proofs rely on combinatorial arguments, computing generating series using\nthe symbolic method and deriving asymptotics using singularity analysis.",
            "author": [
                "Th\u00e9o Lenoir"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01318v2",
                "http://arxiv.org/pdf/2310.01318v2"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "math.PR",
                "05C80, 60C05, 05A15, 05A16"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01310v1",
            "title": "Parameterized Complexity of Incomplete Connected Fair Division",
            "updated": "2023-10-02T16:16:46Z",
            "published": "2023-10-02T16:16:46Z",
            "summary": "\\textit{Fair division} of resources among competing agents is a fundamental\nproblem in computational social choice and economic game theory. It has been\nintensively studied on various kinds of items (\\textit{divisible} and\n\\textit{indivisible}) and under various notions of \\textit{fairness}. We focus\non Connected Fair Division (\\CFDO), the variant of fair division on graphs,\nwhere the \\textit{resources} are modeled as an \\textit{item graph}. Here, each\nagent has to be assigned a connected subgraph of the item graph, and each item\nhas to be assigned to some agent.\n  We introduce a generalization of \\CFDO, termed Incomplete Connected Fair\nDivision (\\CFD), where exactly $p$ vertices of the item graph should be\nassigned to the agents. This might be useful, in particular when the\nallocations are intended to be ``economical'' as well as fair. We consider four\nwell-known notions of fairness: \\PROP, \\EF, \\EFO, \\EFX. First, we prove that\n\\EF-\\CFD, \\EFO-\\CFD, and \\EFX-\\CFD are W[1]-hard parameterized by $p$ plus the\nnumber of agents, even for graphs having constant \\textit{vertex cover number}\n($\\mathsf{vcn}$). In contrast, we present a randomized \\FPT algorithm for\n\\PROP-\\CFD parameterized only by $p$. Additionally, we prove both positive and\nnegative results concerning the kernelization complexity of \\CFD under all four\nfairness notions, parameterized by $p$, $\\mathsf{vcn}$, and the total number of\ndifferent valuations in the item graph ($\\mathsf{val}$).",
            "author": [
                "Harmender Gahlawat",
                "Meirav Zehavi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01310v1",
                "http://arxiv.org/pdf/2310.01310v1"
            ],
            "primary_category": "cs.GT",
            "category": [
                "cs.GT",
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01299v1",
            "title": "Generating Explanations in Medical Question-Answering by Expectation\n  Maximization Inference over Evidence",
            "updated": "2023-10-02T16:00:37Z",
            "published": "2023-10-02T16:00:37Z",
            "summary": "Medical Question Answering~(medical QA) systems play an essential role in\nassisting healthcare workers in finding answers to their questions. However, it\nis not sufficient to merely provide answers by medical QA systems because users\nmight want explanations, that is, more analytic statements in natural language\nthat describe the elements and context that support the answer. To do so, we\npropose a novel approach for generating natural language explanations for\nanswers predicted by medical QA systems. As high-quality medical explanations\nrequire additional medical knowledge, so that our system extract knowledge from\nmedical textbooks to enhance the quality of explanations during the explanation\ngeneration process. Concretely, we designed an expectation-maximization\napproach that makes inferences about the evidence found in these texts,\noffering an efficient way to focus attention on lengthy evidence passages.\nExperimental results, conducted on two datasets MQAE-diag and MQAE, demonstrate\nthe effectiveness of our framework for reasoning with textual evidence. Our\napproach outperforms state-of-the-art models, achieving a significant\nimprovement of \\textbf{6.86} and \\textbf{9.43} percentage points on the Rouge-1\nscore; \\textbf{8.23} and \\textbf{7.82} percentage points on the Bleu-4 score on\nthe respective datasets.",
            "author": [
                "Wei Sun",
                "Mingxiao Li",
                "Damien Sileo",
                "Jesse Davis",
                "Marie-Francine Moens"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01299v1",
                "http://arxiv.org/pdf/2310.01299v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01290v1",
            "title": "Knowledge Crosswords: Geometric Reasoning over Structured Knowledge with\n  Large Language Models",
            "updated": "2023-10-02T15:43:53Z",
            "published": "2023-10-02T15:43:53Z",
            "summary": "Large language models (LLMs) are widely adopted in knowledge-intensive tasks\nand have achieved impressive performance thanks to their knowledge abilities.\nWhile LLMs have demonstrated outstanding performance on atomic or linear\n(multi-hop) QA tasks, whether they can reason in knowledge-rich scenarios with\ninterweaving constraints remains an underexplored problem. In this work, we\npropose geometric reasoning over structured knowledge, where pieces of\nknowledge are connected in a graph structure and models need to fill in the\nmissing information. Such geometric knowledge reasoning would require the\nability to handle structured knowledge, reason with uncertainty, verify facts,\nand backtrack when an error occurs. We propose Knowledge Crosswords, a\nmulti-blank QA dataset where each problem consists of a natural language\nquestion representing the geometric constraints of an incomplete entity\nnetwork, where LLMs are tasked with working out the missing entities while\nmeeting all factual constraints. Knowledge Crosswords contains 2,101 individual\nproblems, covering various knowledge domains and further divided into three\ndifficulty levels. We conduct extensive experiments to evaluate existing LLM\nprompting approaches on the Knowledge Crosswords benchmark. We additionally\npropose two new approaches, Staged Prompting and Verify-All, to augment LLMs'\nability to backtrack and verify structured constraints. Our results demonstrate\nthat while baseline approaches perform well on easier problems but struggle\nwith hard ones, our proposed Verify-All outperforms other methods by a large\nmargin and is more robust with hard problems. Further analysis reveals that\nLLMs' ability of geometric reasoning over structured knowledge is still far\nfrom robust or perfect, susceptible to confounders such as the order of\noptions, certain structural patterns, assumption of existence of correct\nanswer, and more.",
            "author": [
                "Wenxuan Ding",
                "Shangbin Feng",
                "Yuhan Liu",
                "Zhaoxuan Tan",
                "Vidhisha Balachandran",
                "Tianxing He",
                "Yulia Tsvetkov"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01290v1",
                "http://arxiv.org/pdf/2310.01290v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01272v2",
            "title": "A Unified View on Neural Message Passing with Opinion Dynamics for\n  Social Networks",
            "updated": "2023-10-03T11:42:18Z",
            "published": "2023-10-02T15:18:19Z",
            "summary": "Social networks represent a common form of interconnected data frequently\ndepicted as graphs within the domain of deep learning-based inference. These\ncommunities inherently form dynamic systems, achieving stability through\ncontinuous internal communications and opinion exchanges among social actors\nalong their social ties. In contrast, neural message passing in deep learning\nprovides a clear and intuitive mathematical framework for understanding\ninformation propagation and aggregation among connected nodes in graphs. Node\nrepresentations are dynamically updated by considering both the connectivity\nand status of neighboring nodes. This research harmonizes concepts from\nsociometry and neural message passing to analyze and infer the behavior of\ndynamic systems. Drawing inspiration from opinion dynamics in sociology, we\npropose ODNet, a novel message passing scheme incorporating bounded confidence,\nto refine the influence weight of local nodes for message propagation. We\nadjust the similarity cutoffs of bounded confidence and influence weights of\nODNet and define opinion exchange rules that align with the characteristics of\nsocial network graphs. We show that ODNet enhances prediction performance\nacross various graph types and alleviates oversmoothing issues. Furthermore,\nour approach surpasses conventional baselines in graph representation learning\nand proves its practical significance in analyzing real-world co-occurrence\nnetworks of metabolic genes. Remarkably, our method simplifies complex social\nnetwork graphs solely by leveraging knowledge of interaction frequencies among\nentities within the system. It accurately identifies internal communities and\nthe roles of genes in different metabolic pathways, including opinion leaders,\nbridge communicators, and isolators.",
            "author": [
                "Outongyi Lv",
                "Bingxin Zhou",
                "Jing Wang",
                "Xiang Xiao",
                "Weishu Zhao",
                "Lirong Zheng"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01272v2",
                "http://arxiv.org/pdf/2310.01272v2"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI",
                "cs.AI",
                "q-bio.QM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01271v2",
            "title": "LEEC: A Legal Element Extraction Dataset with an Extensive\n  Domain-Specific Label System",
            "updated": "2023-10-10T04:34:56Z",
            "published": "2023-10-02T15:16:31Z",
            "summary": "As a pivotal task in natural language processing, element extraction has\ngained significance in the legal domain. Extracting legal elements from\njudicial documents helps enhance interpretative and analytical capacities of\nlegal cases, and thereby facilitating a wide array of downstream applications\nin various domains of law. Yet existing element extraction datasets are limited\nby their restricted access to legal knowledge and insufficient coverage of\nlabels. To address this shortfall, we introduce a more comprehensive,\nlarge-scale criminal element extraction dataset, comprising 15,831 judicial\ndocuments and 159 labels. This dataset was constructed through two main steps:\nfirst, designing the label system by our team of legal experts based on prior\nlegal research which identified critical factors driving and processes\ngenerating sentencing outcomes in criminal cases; second, employing the legal\nknowledge to annotate judicial documents according to the label system and\nannotation guideline. The Legal Element ExtraCtion dataset (LEEC) represents\nthe most extensive and domain-specific legal element extraction dataset for the\nChinese legal system. Leveraging the annotated data, we employed various SOTA\nmodels that validates the applicability of LEEC for Document Event Extraction\n(DEE) task. The LEEC dataset is available on https://github.com/THUlawtech/LEEC .",
            "author": [
                "Xue Zongyue",
                "Liu Huanghai",
                "Hu Yiran",
                "Kong Kangle",
                "Wang Chenlu",
                "Liu Yun",
                "Shen Weixing"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01271v2",
                "http://arxiv.org/pdf/2310.01271v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01267v1",
            "title": "Cooperative Graph Neural Networks",
            "updated": "2023-10-02T15:08:52Z",
            "published": "2023-10-02T15:08:52Z",
            "summary": "Graph neural networks are popular architectures for graph machine learning,\nbased on iterative computation of node representations of an input graph\nthrough a series of invariant transformations. A large class of graph neural\nnetworks follow a standard message-passing paradigm: at every layer, each node\nstate is updated based on an aggregate of messages from its neighborhood. In\nthis work, we propose a novel framework for training graph neural networks,\nwhere every node is viewed as a player that can choose to either 'listen',\n'broadcast', 'listen and broadcast', or to 'isolate'. The standard message\npropagation scheme can then be viewed as a special case of this framework where\nevery node 'listens and broadcasts' to all neighbors. Our approach offers a\nmore flexible and dynamic message-passing paradigm, where each node can\ndetermine its own strategy based on their state, effectively exploring the\ngraph topology while learning. We provide a theoretical analysis of the new\nmessage-passing scheme which is further supported by an extensive empirical\nanalysis on a synthetic dataset and on real-world datasets.",
            "author": [
                "Ben Finkelshtein",
                "Xingyue Huang",
                "Michael Bronstein",
                "\u0130smail \u0130lkan Ceylan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01267v1",
                "http://arxiv.org/pdf/2310.01267v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01252v1",
            "title": "Pre-training Contextual Location Embeddings in Personal Trajectories via\n  Efficient Hierarchical Location Representations",
            "updated": "2023-10-02T14:40:24Z",
            "published": "2023-10-02T14:40:24Z",
            "summary": "Pre-training the embedding of a location generated from human mobility data\nhas become a popular method for location based services. In practice, modeling\nthe location embedding is too expensive, due to the large number of locations\nto be trained in situations with fine-grained resolution or extensive target\nregions. Previous studies have handled less than ten thousand distinct\nlocations, which is insufficient in the real-world applications. To tackle this\nproblem, we propose a Geo-Tokenizer, designed to efficiently reduce the number\nof locations to be trained by representing a location as a combination of\nseveral grids at different scales. In the Geo-Tokenizer, a grid at a larger\nscale shares the common set of grids at smaller scales, which is a key factor\nin reducing the size of the location vocabulary. The sequences of locations\npreprocessed with the Geo-Tokenizer are utilized by a causal location embedding\nmodel to capture the temporal dependencies of locations. This model dynamically\ncalculates the embedding vector of a target location, which varies depending on\nits trajectory. In addition, to efficiently pre-train the location embedding\nmodel, we propose the Hierarchical Auto-regressive Location Model objective to\neffectively train decomposed locations in the Geo-Tokenizer. We conducted\nexperiments on two real-world user trajectory datasets using our pre-trained\nlocation model. The experimental results show that our model significantly\nimproves the performance of downstream tasks with fewer model parameters\ncompared to existing location embedding methods.",
            "author": [
                "Chung Park",
                "Taesan Kim",
                "Junui Hong",
                "Minsung Choi",
                "Jaegul Choo"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01252v1",
                "http://arxiv.org/pdf/2310.01252v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01247v1",
            "title": "Self-supervised Learning for Anomaly Detection in Computational\n  Workflows",
            "updated": "2023-10-02T14:31:56Z",
            "published": "2023-10-02T14:31:56Z",
            "summary": "Anomaly detection is the task of identifying abnormal behavior of a system.\nAnomaly detection in computational workflows is of special interest because of\nits wide implications in various domains such as cybersecurity, finance, and\nsocial networks. However, anomaly detection in computational workflows~(often\nmodeled as graphs) is a relatively unexplored problem and poses distinct\nchallenges. For instance, when anomaly detection is performed on graph data,\nthe complex interdependency of nodes and edges, the heterogeneity of node\nattributes, and edge types must be accounted for. Although the use of graph\nneural networks can help capture complex inter-dependencies, the scarcity of\nlabeled anomalous examples from workflow executions is still a significant\nchallenge. To address this problem, we introduce an autoencoder-driven\nself-supervised learning~(SSL) approach that learns a summary statistic from\nunlabeled workflow data and estimates the normal behavior of the computational\nworkflow in the latent space. In this approach, we combine generative and\ncontrastive learning objectives to detect outliers in the summary statistics.\nWe demonstrate that by estimating the distribution of normal behavior in the\nlatent space, we can outperform state-of-the-art anomaly detection methods on\nour benchmark datasets.",
            "author": [
                "Hongwei Jin",
                "Krishnan Raghavan",
                "George Papadimitriou",
                "Cong Wang",
                "Anirban Mandal",
                "Ewa Deelman",
                "Prasanna Balaprakash"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01247v1",
                "http://arxiv.org/pdf/2310.01247v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01228v2",
            "title": "Reconstructing 3D Human Pose from RGB-D Data with Occlusions",
            "updated": "2023-10-15T14:48:58Z",
            "published": "2023-10-02T14:16:13Z",
            "summary": "We propose a new method to reconstruct the 3D human body from RGB-D images\nwith occlusions. The foremost challenge is the incompleteness of the RGB-D data\ndue to occlusions between the body and the environment, leading to implausible\nreconstructions that suffer from severe human-scene penetration. To reconstruct\na semantically and physically plausible human body, we propose to reduce the\nsolution space based on scene information and prior knowledge. Our key idea is\nto constrain the solution space of the human body by considering the occluded\nbody parts and visible body parts separately: modeling all plausible poses\nwhere the occluded body parts do not penetrate the scene, and constraining the\nvisible body parts using depth data. Specifically, the first component is\nrealized by a neural network that estimates the candidate region named the\n\"free zone\", a region carved out of the open space within which it is safe to\nsearch for poses of the invisible body parts without concern for penetration.\nThe second component constrains the visible body parts using the \"truncated\nshadow volume\" of the scanned body point cloud. Furthermore, we propose to use\na volume matching strategy, which yields better performance than surface\nmatching, to match the human body with the confined region. We conducted\nexperiments on the PROX dataset, and the results demonstrate that our method\nproduces more accurate and plausible results compared with other methods.",
            "author": [
                "Bowen Dang",
                "Xi Zhao",
                "Bowen Zhang",
                "He Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01228v2",
                "http://arxiv.org/pdf/2310.01228v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01224v1",
            "title": "Revisiting Mobility Modeling with Graph: A Graph Transformer Model for\n  Next Point-of-Interest Recommendation",
            "updated": "2023-10-02T14:11:16Z",
            "published": "2023-10-02T14:11:16Z",
            "summary": "Next Point-of-Interest (POI) recommendation plays a crucial role in urban\nmobility applications. Recently, POI recommendation models based on Graph\nNeural Networks (GNN) have been extensively studied and achieved, however, the\neffective incorporation of both spatial and temporal information into such\nGNN-based models remains challenging. Extracting distinct fine-grained features\nunique to each piece of information is difficult since temporal information\noften includes spatial information, as users tend to visit nearby POIs. To\naddress the challenge, we propose \\textbf{\\underline{Mob}}ility\n\\textbf{\\underline{G}}raph \\textbf{\\underline{T}}ransformer (MobGT) that\nenables us to fully leverage graphs to capture both the spatial and temporal\nfeatures in users' mobility patterns. MobGT combines individual spatial and\ntemporal graph encoders to capture unique features and global user-location\nrelations. Additionally, it incorporates a mobility encoder based on Graph\nTransformer to extract higher-order information between POIs. To address the\nlong-tailed problem in spatial-temporal data, MobGT introduces a novel loss\nfunction, Tail Loss. Experimental results demonstrate that MobGT outperforms\nstate-of-the-art models on various datasets and metrics, achieving 24\\%\nimprovement on average. Our codes are available at\n\\url{https://github.com/Yukayo/MobGT}.",
            "author": [
                "Xiaohang Xu",
                "Toyotaro Suzumura",
                "Jiawei Yong",
                "Masatoshi Hanai",
                "Chuang Yang",
                "Hiroki Kanezashi",
                "Renhe Jiang",
                "Shintaro Fukushima"
            ],
            "link": [
                "http://dx.doi.org/10.1145/3589132.3625644",
                "http://arxiv.org/abs/2310.01224v1",
                "http://arxiv.org/pdf/2310.01224v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.02284v1",
            "title": "PASTA: PArallel Spatio-Temporal Attention with spatial auto-correlation\n  gating for fine-grained crowd flow prediction",
            "updated": "2023-10-02T14:10:42Z",
            "published": "2023-10-02T14:10:42Z",
            "summary": "Understanding the movement patterns of objects (e.g., humans and vehicles) in\na city is essential for many applications, including city planning and\nmanagement. This paper proposes a method for predicting future city-wide crowd\nflows by modeling the spatio-temporal patterns of historical crowd flows in\nfine-grained city-wide maps. We introduce a novel neural network named PArallel\nSpatio-Temporal Attention with spatial auto-correlation gating (PASTA) that\neffectively captures the irregular spatio-temporal patterns of fine-grained\nmaps. The novel components in our approach include spatial auto-correlation\ngating, multi-scale residual block, and temporal attention gating module. The\nspatial auto-correlation gating employs the concept of spatial statistics to\nidentify irregular spatial regions. The multi-scale residual block is\nresponsible for handling multiple range spatial dependencies in the\nfine-grained map, and the temporal attention gating filters out irrelevant\ntemporal information for the prediction. The experimental results demonstrate\nthat our model outperforms other competing baselines, especially under\nchallenging conditions that contain irregular spatial regions. We also provide\na qualitative analysis to derive the critical time information where our model\nassigns high attention scores in prediction.",
            "author": [
                "Chung Park",
                "Junui Hong",
                "Cheonbok Park",
                "Taesan Kim",
                "Minsung Choi",
                "Jaegul Choo"
            ],
            "link": [
                "http://arxiv.org/abs/2310.02284v1",
                "http://arxiv.org/pdf/2310.02284v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01462v1",
            "title": "$m$-Magic Labeling on Anti Fuzzy Path Graph",
            "updated": "2023-10-02T14:10:29Z",
            "published": "2023-10-02T14:10:29Z",
            "summary": "The topic of $m$-magic labeling is discussed in this article. The application\nof these problem is applied to path anti fuzzy and path anti fuzzy bipolar.\nAnti fuzzy and anti fuzzy bipolar graphs are new concepts of fuzzy graphs. This\narticle to discover m magic anti fuzzy and anti fuzzy bipolar graphs through\nthe adaptation of some previous studies on labeling magic, bimagic, trimagic\nanti fuzzy graphs, and anti fuzzy bipolar graphs. The graph path is the subject\nof $m$-magic anti fuzzy and anti fuzzy bipolar graph study. The results of this\nstudy show that, for every natural number $n$ dan $n \\ge (2m+1+ma)$ with $a =\n\\{0,1,2,3,\\ldots,1\\}$, there is an anti fuzzy $m$-magic graph path labeling.\nWhile in $m > 2$ in terms of 2 cases. Case 1 when for odd $n$ with $n \\equiv\n2m+1$ mod$m$ and for even $n$ with $n \\equiv 2m+1$ mod$2m$, case 2 when $n\n\\equiv m+1$ mod $2m$ for $m$ odd and $n \\ne m+1$ there is $m$-magic graph path\nanti fuzzy bipolar labeling.",
            "author": [
                "Yeni Rahma Oktaviani",
                "Toto Nusantara",
                "Santi Irawati"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01462v1",
                "http://arxiv.org/pdf/2310.01462v1"
            ],
            "primary_category": "math.GM",
            "category": [
                "math.GM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01217v1",
            "title": "ScaLearn: Simple and Highly Parameter-Efficient Task Transfer by\n  Learning to Scale",
            "updated": "2023-10-02T14:01:36Z",
            "published": "2023-10-02T14:01:36Z",
            "summary": "Multi-task learning (MTL) has shown considerable practical benefits,\nparticularly when using pre-trained language models (PLMs). While this is\ncommonly achieved by simultaneously learning $n$ tasks under a joint\noptimization procedure, recent methods such as AdapterFusion structure the\nproblem into two distinct stages: (i) task learning, where knowledge specific\nto a task is encapsulated within sets of parameters (\\eg adapters), and (ii)\ntransfer, where this already learned knowledge is leveraged for a target task.\nThis separation of concerns provides numerous benefits, such as promoting\nreusability, and addressing cases involving data privacy and societal concerns;\non the flip side, current two-stage MTL methods come with the cost of\nintroducing a substantial number of additional parameters. In this work, we\naddress this issue by leveraging the usefulness of linearly scaling the output\nrepresentations of source adapters for transfer learning. We introduce\nScaLearn, a simple and highly parameter-efficient two-stage MTL method that\ncapitalizes on the knowledge of the source tasks by learning a minimal set of\nscaling parameters that enable effective knowledge transfer to a target task.\nOur experiments on three benchmarks (GLUE, SuperGLUE, and HumSet) show that our\nScaLearn, in addition to facilitating the benefits of two-stage MTL,\nconsistently outperforms strong baselines with only a small number of transfer\nparameters - roughly 0.35% of those of AdapterFusion. Remarkably, we observe\nthat ScaLearn maintains its strong abilities even when further reducing\nparameters through uniform scaling and layer-sharing, achieving similarly\ncompetitive results with only $8$ transfer parameters for each target task. Our\nproposed approach thus demonstrates the power of simple scaling as a promise\nfor more efficient task transfer.",
            "author": [
                "Markus Frohmann",
                "Carolin Holtermann",
                "Shahed Masoudian",
                "Anne Lauscher",
                "Navid Rekabsaz"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01217v1",
                "http://arxiv.org/pdf/2310.01217v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01211v1",
            "title": "From Bricks to Bridges: Product of Invariances to Enhance Latent Space\n  Communication",
            "updated": "2023-10-02T13:55:38Z",
            "published": "2023-10-02T13:55:38Z",
            "summary": "It has been observed that representations learned by distinct neural networks\nconceal structural similarities when the models are trained under similar\ninductive biases. From a geometric perspective, identifying the classes of\ntransformations and the related invariances that connect these representations\nis fundamental to unlocking applications, such as merging, stitching, and\nreusing different neural modules. However, estimating task-specific\ntransformations a priori can be challenging and expensive due to several\nfactors (e.g., weights initialization, training hyperparameters, or data\nmodality). To this end, we introduce a versatile method to directly incorporate\na set of invariances into the representations, constructing a product space of\ninvariant components on top of the latent representations without requiring\nprior knowledge about the optimal invariance to infuse. We validate our\nsolution on classification and reconstruction tasks, observing consistent\nlatent similarity and downstream performance improvements in a zero-shot\nstitching setting. The experimental analysis comprises three modalities\n(vision, text, and graphs), twelve pretrained foundational models, eight\nbenchmarks, and several architectures trained from scratch.",
            "author": [
                "Irene Cannistraci",
                "Luca Moschella",
                "Marco Fumero",
                "Valentino Maiorca",
                "Emanuele Rodol\u00e0"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01211v1",
                "http://arxiv.org/pdf/2310.01211v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01210v4",
            "title": "Towards Robust Cardiac Segmentation using Graph Convolutional Networks",
            "updated": "2023-11-06T11:35:43Z",
            "published": "2023-10-02T13:55:06Z",
            "summary": "Fully automatic cardiac segmentation can be a fast and reproducible method to\nextract clinical measurements from an echocardiography examination. The U-Net\narchitecture is the current state-of-the-art deep learning architecture for\nmedical segmentation and can segment cardiac structures in real-time with\naverage errors comparable to inter-observer variability. However, this\narchitecture still generates large outliers that are often anatomically\nincorrect. This work uses the concept of graph convolutional neural networks\nthat predict the contour points of the structures of interest instead of\nlabeling each pixel. We propose a graph architecture that uses two\nconvolutional rings based on cardiac anatomy and show that this eliminates\nanatomical incorrect multi-structure segmentations on the publicly available\nCAMUS dataset. Additionally, this work contributes with an ablation study on\nthe graph convolutional architecture and an evaluation of clinical measurements\non the clinical HUNT4 dataset. Finally, we propose to use the inter-model\nagreement of the U-Net and the graph network as a predictor of both the input\nand segmentation quality. We show this predictor can detect out-of-distribution\nand unsuitable input images in real-time. Source code is available online:\nhttps://github.com/gillesvntnu/GCN_multistructure",
            "author": [
                "Gilles Van De Vyver",
                "Sarina Thomas",
                "Guy Ben-Yosef",
                "Sindre Hellum Olaisen",
                "H\u00e5vard Dalen",
                "Lasse L\u00f8vstakken",
                "Erik Smistad"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01210v4",
                "http://arxiv.org/pdf/2310.01210v4"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01208v1",
            "title": "Label Supervised LLaMA Finetuning",
            "updated": "2023-10-02T13:53:03Z",
            "published": "2023-10-02T13:53:03Z",
            "summary": "The recent success of Large Language Models (LLMs) has gained significant\nattention in both academia and industry. Substantial efforts have been made to\nenhance the zero- and few-shot generalization capabilities of open-source LLMs\nthrough finetuning. Currently, the prevailing approach is instruction-tuning,\nwhich trains LLMs to complete real-world tasks by generating responses guided\nby natural language instructions. It is worth noticing that such an approach\nmay underperform in sequence and token classification tasks. Unlike text\ngeneration tasks, classification tasks have a limited label space, where\nprecise label prediction is more appreciated than generating diverse and\nhuman-like responses. Prior research has unveiled that instruction-tuned LLMs\ncannot outperform BERT, prompting us to explore the potential of leveraging\nlatent representations from LLMs for supervised label prediction. In this\npaper, we introduce a label-supervised adaptation for LLMs, which aims to\nfinetuning the model with discriminant labels. We evaluate this approach with\nLabel Supervised LLaMA (LS-LLaMA), based on LLaMA-2-7B, a relatively\nsmall-scale LLM, and can be finetuned on a single GeForce RTX4090 GPU. We\nextract latent representations from the final LLaMA layer and project them into\nthe label space to compute the cross-entropy loss. The model is finetuned by\nLow-Rank Adaptation (LoRA) to minimize this loss. Remarkably, without intricate\nprompt engineering or external knowledge, LS-LLaMA substantially outperforms\nLLMs ten times its size in scale and demonstrates consistent improvements\ncompared to robust baselines like BERT-Large and RoBERTa-Large in text\nclassification. Moreover, by removing the causal mask from decoders, LS-unLLaMA\nachieves the state-of-the-art performance in named entity recognition (NER).\nOur work will shed light on a novel approach to adapting LLMs for various\ndownstream tasks.",
            "author": [
                "Zongxi Li",
                "Xianming Li",
                "Yuzhang Liu",
                "Haoran Xie",
                "Jing Li",
                "Fu-lee Wang",
                "Qing Li",
                "Xiaoqin Zhong"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01208v1",
                "http://arxiv.org/pdf/2310.01208v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01207v1",
            "title": "Learn to Follow: Decentralized Lifelong Multi-agent Pathfinding via\n  Planning and Learning",
            "updated": "2023-10-02T13:51:32Z",
            "published": "2023-10-02T13:51:32Z",
            "summary": "Multi-agent Pathfinding (MAPF) problem generally asks to find a set of\nconflict-free paths for a set of agents confined to a graph and is typically\nsolved in a centralized fashion.\n  Conversely, in this work, we investigate the decentralized MAPF setting, when\nthe central controller that posses all the information on the agents' locations\nand goals is absent and the agents have to sequientially decide the actions on\ntheir own without having access to a full state of the environment. We focus on\nthe practically important lifelong variant of MAPF, which involves continuously\nassigning new goals to the agents upon arrival to the previous ones. To address\nthis complex problem, we propose a method that integrates two complementary\napproaches: planning with heuristic search and reinforcement learning through\npolicy optimization. Planning is utilized to construct and re-plan individual\npaths. We enhance our planning algorithm with a dedicated technique tailored to\navoid congestion and increase the throughput of the system. We employ\nreinforcement learning to discover the collision avoidance policies that\neffectively guide the agents along the paths. The policy is implemented as a\nneural network and is effectively trained without any reward-shaping or\nexternal guidance.\n  We evaluate our method on a wide range of setups comparing it to the\nstate-of-the-art solvers. The results show that our method consistently\noutperforms the learnable competitors, showing higher throughput and better\nability to generalize to the maps that were unseen at the training stage.\nMoreover our solver outperforms a rule-based one in terms of throughput and is\nan order of magnitude faster than a state-of-the-art search-based solver.",
            "author": [
                "Alexey Skrynnik",
                "Anton Andreychuk",
                "Maria Nesterova",
                "Konstantin Yakovlev",
                "Aleksandr Panov"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01207v1",
                "http://arxiv.org/pdf/2310.01207v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.MA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01190v1",
            "title": "Graph-Theoretic B\u00e9zier Curve Optimization over Safe Corridors for Safe\n  and Smooth Motion Planning",
            "updated": "2023-10-02T13:28:16Z",
            "published": "2023-10-02T13:28:16Z",
            "summary": "As a parametric motion representation, B\\'ezier curves have significant\napplications in polynomial trajectory optimization for safe and smooth motion\nplanning of various robotic systems, including flying drones, autonomous\nvehicles, and robotic manipulators. An essential component of B\\'ezier curve\noptimization is the optimization objective, as it significantly influences the\nresulting robot motion. Standard physical optimization objectives, such as\nminimizing total velocity, acceleration, jerk, and snap, are known to yield\nquadratic optimization of B\\'ezier curve control points. In this paper, we\npresent a unifying graph-theoretic perspective for defining and understanding\nB\\'ezier curve optimization objectives using a consensus distance of B\\'ezier\ncontrol points derived based on their interaction graph Laplacian. In addition\nto demonstrating how standard physical optimization objectives define a\nconsensus distance between B\\'ezier control points, we also introduce geometric\nand statistical optimization objectives as alternative consensus distances,\nconstructed using finite differencing and differential variance. To compare\nthese optimization objectives, we apply B\\'ezier curve optimization over convex\npolygonal safe corridors that are automatically constructed around a\nmaximal-clearance minimal-length reference path. We provide an explicit\nanalytical formulation for quadratic optimization of B\\'ezier curves using\nB\\'ezier matrix operations. We conclude that the norm and variance of the\nfinite differences of B\\'ezier control points lead to simpler and more\nintuitive interaction graphs and optimization objectives compared to B\\'ezier\nderivative norms, despite having similar robot motion profiles.",
            "author": [
                "Soufyan Zayou",
                "\u00d6m\u00fcr Arslan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01190v1",
                "http://arxiv.org/pdf/2310.01190v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.CG",
                "cs.SY",
                "eess.SY",
                "68T40, 90C23, 65D17",
                "I.2.9; I.3; J.6"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01186v2",
            "title": "Hypergraph anti-Ramsey theorems",
            "updated": "2023-10-03T15:10:47Z",
            "published": "2023-10-02T13:26:14Z",
            "summary": "The anti-Ramsey number $\\mathrm{ar}(n,F)$ of an $r$-graph $F$ is the minimum\nnumber of colors needed to color the complete $n$-vertex $r$-graph to ensure\nthe existence of a rainbow copy of $F$. We prove a general upper bound for\n$\\mathrm{ar}(n,F)$ when $F$ is the expansion of a hypergraph with smaller\nuniformality, which refines the general bound $\\mathrm{ar}(n,F) =\n\\mathrm{ex}(n,F_{-}) + o(n^r)$ by Erd{\\H o}s--Simonovits--S{\\' o}s. Here\n$F_{-}$ is the family of $r$-graphs obtained from $F$ by removing one edge. We\nalso determine the exact value of $\\mathrm{ar}(n,F)$ for large $n$ when $F$ is\nthe expansion of a complete graph, extending a result of Erd{\\H\no}s--Simonovits--S{\\' o}s from graphs to hypergraphs.",
            "author": [
                "Xizhi Liu",
                "Jialei Song"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01186v2",
                "http://arxiv.org/pdf/2310.01186v2"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01181v2",
            "title": "Graph Isomorphic Networks for Assessing Reliability of the\n  Medium-Voltage Grid",
            "updated": "2023-10-03T09:42:23Z",
            "published": "2023-10-02T13:19:35Z",
            "summary": "Ensuring electricity grid reliability becomes increasingly challenging with\nthe shift towards renewable energy and declining conventional capacities.\nDistribution System Operators (DSOs) aim to achieve grid reliability by\nverifying the n-1 principle, ensuring continuous operation in case of component\nfailure. Electricity networks' complex graph-based data holds crucial\ninformation for n-1 assessment: graph structure and data about stations/cables.\nUnlike traditional machine learning methods, Graph Neural Networks (GNNs)\ndirectly handle graph-structured data. This paper proposes using Graph\nIsomorphic Networks (GINs) for n-1 assessments in medium voltage grids. The GIN\nframework is designed to generalise to unseen grids and utilise graph structure\nand data about stations/cables. The proposed GIN approach demonstrates faster\nand more reliable grid assessments than a traditional mathematical optimisation\napproach, reducing prediction times by approximately a factor of 1000. The\nfindings offer a promising approach to address computational challenges and\nenhance the reliability and efficiency of energy grid assessments.",
            "author": [
                "Charlotte Cambier van Nooten",
                "Tom van de Poll",
                "Sonja F\u00fcllhase",
                "Jacco Heres",
                "Tom Heskes",
                "Yuliya Shapovalova"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01181v2",
                "http://arxiv.org/pdf/2310.01181v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01180v1",
            "title": "Evolutionary Neural Architecture Search for Transformer in Knowledge\n  Tracing",
            "updated": "2023-10-02T13:19:33Z",
            "published": "2023-10-02T13:19:33Z",
            "summary": "Knowledge tracing (KT) aims to trace students' knowledge states by predicting\nwhether students answer correctly on exercises. Despite the excellent\nperformance of existing Transformer-based KT approaches, they are criticized\nfor the manually selected input features for fusion and the defect of single\nglobal context modelling to directly capture students' forgetting behavior in\nKT, when the related records are distant from the current record in terms of\ntime. To address the issues, this paper first considers adding convolution\noperations to the Transformer to enhance its local context modelling ability\nused for students' forgetting behavior, then proposes an evolutionary neural\narchitecture search approach to automate the input feature selection and\nautomatically determine where to apply which operation for achieving the\nbalancing of the local/global context modelling. In the search space, the\noriginal global path containing the attention module in Transformer is replaced\nwith the sum of a global path and a local path that could contain different\nconvolutions, and the selection of input features is also considered. To search\nthe best architecture, we employ an effective evolutionary algorithm to explore\nthe search space and also suggest a search space reduction strategy to\naccelerate the convergence of the algorithm. Experimental results on the two\nlargest and most challenging education datasets demonstrate the effectiveness\nof the architecture found by the proposed approach.",
            "author": [
                "Shangshang Yang",
                "Xiaoshan Yu",
                "Ye Tian",
                "Xueming Yan",
                "Haiping Ma",
                "Xingyi Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01180v1",
                "http://arxiv.org/pdf/2310.01180v1"
            ],
            "primary_category": "cs.NE",
            "category": [
                "cs.NE",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01166v1",
            "title": "Gotcha! This Model Uses My Code! Evaluating Membership Leakage Risks in\n  Code Models",
            "updated": "2023-10-02T12:50:43Z",
            "published": "2023-10-02T12:50:43Z",
            "summary": "Given large-scale source code datasets available in open-source projects and\nadvanced large language models, recent code models have been proposed to\naddress a series of critical software engineering tasks, such as program repair\nand code completion. The training data of the code models come from various\nsources, not only the publicly available source code, e.g., open-source\nprojects on GitHub but also the private data such as the confidential source\ncode from companies, which may contain sensitive information (for example, SSH\nkeys and personal information). As a result, the use of these code models may\nraise new privacy concerns.\n  In this paper, we focus on a critical yet not well-explored question on using\ncode models: what is the risk of membership information leakage in code models?\nMembership information leakage refers to the risk that an attacker can infer\nwhether a given data point is included in (i.e., a member of) the training\ndata. To answer this question, we propose Gotcha, a novel membership inference\nattack method specifically for code models. We investigate the membership\nleakage risk of code models. Our results reveal a worrying fact that the risk\nof membership leakage is high: although the previous attack methods are close\nto random guessing, Gotcha can predict the data membership with a high true\npositive rate of 0.95 and a low false positive rate of 0.10. We also show that\nthe attacker's knowledge of the victim model (e.g., the model architecture and\nthe pre-training data) impacts the success rate of attacks. Further analysis\ndemonstrates that changing the decoding strategy can mitigate the risk of\nmembership leakage. This study calls for more attention to understanding the\nprivacy of code models and developing more effective countermeasures against\nsuch attacks.",
            "author": [
                "Zhou Yang",
                "Zhipeng Zhao",
                "Chenyu Wang",
                "Jieke Shi",
                "Dongsum Kim",
                "Donggyun Han",
                "David Lo"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01166v1",
                "http://arxiv.org/pdf/2310.01166v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01162v1",
            "title": "DINE: Dimensional Interpretability of Node Embeddings",
            "updated": "2023-10-02T12:47:42Z",
            "published": "2023-10-02T12:47:42Z",
            "summary": "Graphs are ubiquitous due to their flexibility in representing social and\ntechnological systems as networks of interacting elements. Graph representation\nlearning methods, such as node embeddings, are powerful approaches to map nodes\ninto a latent vector space, allowing their use for various graph tasks. Despite\ntheir success, only few studies have focused on explaining node embeddings\nlocally. Moreover, global explanations of node embeddings remain unexplored,\nlimiting interpretability and debugging potentials. We address this gap by\ndeveloping human-understandable explanations for dimensions in node embeddings.\nTowards that, we first develop new metrics that measure the global\ninterpretability of embedding vectors based on the marginal contribution of the\nembedding dimensions to predicting graph structure. We say that an embedding\ndimension is more interpretable if it can faithfully map to an understandable\nsub-structure in the input graph - like community structure. Having observed\nthat standard node embeddings have low interpretability, we then introduce DINE\n(Dimension-based Interpretable Node Embedding), a novel approach that can\nretrofit existing node embeddings by making them more interpretable without\nsacrificing their task performance. We conduct extensive experiments on\nsynthetic and real-world graphs and show that we can simultaneously learn\nhighly interpretable node embeddings with effective performance in link\nprediction.",
            "author": [
                "Simone Piaggesi",
                "Megha Khosla",
                "Andr\u00e9 Panisson",
                "Avishek Anand"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01162v1",
                "http://arxiv.org/pdf/2310.01162v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01151v1",
            "title": "Computing Functions by Teams of Deterministic Finite Automata",
            "updated": "2023-10-02T12:36:58Z",
            "published": "2023-10-02T12:36:58Z",
            "summary": "We consider the task of computing functions $f: \\mathbb{N}^k\\to \\mathbb{N}$,\nwhere $ \\mathbb{N}$ is the set of natural numbers, by finite teams of agents\nmodelled as deterministic finite automata. The computation is carried out in a\ndistributed way, using the {\\em discrete half-line}, which is the infinite\ngraph with one node of degree 1 (called the root) and infinitely many nodes of\ndegree 2. The node at distance $j$ from the root represents the integer $j$. We\nsay that a team $\\mathcal{A}^f$ of automata computes a function $f$, if in the\nbeginning of the computation all automata from $\\mathcal{A}^f$ are located at\nthe arguments $x_1,\\dots,x_k$ of the function $f$, in groups $\\mathcal{A}^f _j$\nat $x_j$, and at the end, all automata of the team gather at $f(x_1,\\dots,x_k)$\nand transit to a special state $STOP$. At each step of the computation, an\nautomaton $a$ can ``see'' states of all automata colocated at the same node:\nthe set of these states forms an input of $a$.\n  Our main result shows that, for every primitive recursive function, there\nexists a finite team of automata that computes this function. We prove this by\nshowing that basic primitive recursive functions can be computed by teams of\nautomata, and that functions resulting from the operations of composition and\nof primitive recursion can be computed by teams of automata, provided that the\ningredient functions of these operations can be computed by teams of automata.\nWe also observe that cooperation between automata is necessary: even some very\nsimple functions $f: \\mathbb{N}\\to \\mathbb{N}$ cannot be computed by a single\nautomaton.",
            "author": [
                "Debasish Pattanayak",
                "Andrzej Pelc"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01151v1",
                "http://arxiv.org/pdf/2310.01151v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01149v1",
            "title": "On $b$-Matching and Fully-Dynamic Maximum $k$-Edge Coloring",
            "updated": "2023-10-02T12:35:37Z",
            "published": "2023-10-02T12:35:37Z",
            "summary": "Given a graph $G$ that is modified by a sequence of edge insertions and\ndeletions, we study the Maximum $k$-Edge Coloring problem Having access to $k$\ncolors, how can we color as many edges of $G$ as possible such that no two\nadjacent edges share the same color? While this problem is different from\nsimply maintaining a $b$-matching with $b=k$, the two problems are closely\nrelated: a maximum $k$-matching always contains a $\\frac{k+1}k$-approximate\nmaximum $k$-edge coloring. However, maximum $b$-matching can be solved\nefficiently in the static setting, whereas the Maximum $k$-Edge Coloring\nproblem is NP-hard and even APX-hard for $k \\ge 2$.\n  We present new results on both problems: For $b$-matching, we show a new\nintegrality gap result and for the case where $b$ is a constant, we adapt\nWajc's matching sparsification scheme~[STOC20].\n  Using these as basis, we give three new algorithms for the dynamic Maximum\n$k$-Edge Coloring problem: Our MatchO algorithm builds on the dynamic\n$(2+\\epsilon)$-approximation algorithm of Bhattacharya, Gupta, and\nMohan~[ESA17] for $b$-matching and achieves a $(2+\\epsilon)\\frac{k+1}\nk$-approximation in $O(poly(\\log n, \\epsilon^{-1}))$ update time against an\noblivious adversary. Our MatchA algorithm builds on the dynamic\n$8$-approximation algorithm by Bhattacharya, Henzinger, and Italiano~[SODA15]\nfor fractional $b$-matching and achieves a\n$(8+\\epsilon)\\frac{3k+3}{3k-1}$-approximation in $O(poly(\\log n,\n\\epsilon^{-1}))$ update time against an adaptive adversary. Moreover, our\nreductions use the dynamic $b$-matching algorithm as a black box, so any future\nimprovement in the approximation ratio for dynamic $b$-matching will\nautomatically translate into a better approximation ratio for our algorithms.\nFinally, we present a greedy algorithm that runs in $O(\\Delta+k)$ update time,\nwhile guaranteeing a $2.16$~approximation factor.",
            "author": [
                "Antoine El-Hayek",
                "Kathrin Hanauer",
                "Monika Henzinger"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01149v1",
                "http://arxiv.org/pdf/2310.01149v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01144v2",
            "title": "The Map Equation Goes Neural",
            "updated": "2023-11-27T11:54:55Z",
            "published": "2023-10-02T12:32:18Z",
            "summary": "Community detection and graph clustering are essential for unsupervised data\nexploration and understanding the high-level organisation of networked systems.\nRecently, graph clustering has received attention as a primary task for graph\nneural networks. Although hierarchical graph pooling has been shown to improve\nperformance in graph and node classification tasks, it performs poorly in\nidentifying meaningful clusters. Community detection has a long history in\nnetwork science, but typically relies on optimising objective functions with\ncustom-tailored search algorithms, not leveraging recent advances in deep\nlearning, particularly from graph neural networks. In this paper, we narrow\nthis gap between the deep learning and network science communities. We consider\nthe map equation, an information-theoretic objective function for unsupervised\ncommunity detection. Expressing it in a fully differentiable tensor form that\nproduces soft cluster assignments, we optimise the map equation with deep\nlearning through gradient descent. More specifically, the reformulated map\nequation is a loss function compatible with any graph neural network\narchitecture, enabling flexible clustering and graph pooling that clusters both\ngraph structure and data features in an end-to-end way, automatically finding\nan optimum number of clusters without explicit regularisation by following the\nminimum description length principle. We evaluate our approach experimentally\nusing different neural network architectures for unsupervised clustering in\nsynthetic and real data. Our results show that our approach achieves\ncompetitive performance against baselines, naturally detects overlapping\ncommunities, and avoids over-partitioning sparse graphs.",
            "author": [
                "Christopher Bl\u00f6cker",
                "Chester Tan",
                "Ingo Scholtes"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01144v2",
                "http://arxiv.org/pdf/2310.01144v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01136v1",
            "title": "Deterministic Treasure Hunt and Rendezvous in Arbitrary Connected Graphs",
            "updated": "2023-10-02T12:22:06Z",
            "published": "2023-10-02T12:22:06Z",
            "summary": "Treasure hunt and rendezvous are fundamental tasks performed by mobile agents\nin graphs. In treasure hunt, an agent has to find an inert target (called\ntreasure) situated at an unknown node of the graph. In rendezvous, two agents,\ninitially located at distinct nodes of the graph, traverse its edges in\nsynchronous rounds and have to meet at some node. We assume that the graph is\nconnected (otherwise none of these tasks is feasible) and consider\ndeterministic treasure hunt and rendezvous algorithms. The time of a treasure\nhunt algorithm is the worst-case number of edge traversals performed by the\nagent until the treasure is found. The time of a rendezvous algorithm is the\nworst-case number of rounds since the wakeup of the earlier agent until the\nmeeting.\n  To the best of our knowledge, all known treasure hunt and rendezvous\nalgorithms rely on the assumption that degrees of all nodes are finite, even\nwhen the graph itself may be infinite. In the present paper we remove this\nassumption for the first time, and consider both above tasks in arbitrary\nconnected graphs whose nodes can have either finite or countably infinite\ndegrees. Our main result is the first universal treasure hunt algorithm working\nfor arbitrary connected graphs. We prove that the time of this algorithm has\noptimal order of magnitude among all possible treasure hunt algorithms working\nfor arbitrary connected graphs.\n  As a consequence of this result we obtain the first universal rendezvous\nalgorithm working for arbitrary connected graphs. The time of this algorithm is\npolynomial in a lower bound holding in many graphs, in particular in the tree\nall of whose degrees are infinite.",
            "author": [
                "Debasish Pattanayak",
                "Andrzej Pelc"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01136v1",
                "http://arxiv.org/pdf/2310.01136v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01134v1",
            "title": "Existential Second-Order Logic Over Graphs: Parameterized Complexity",
            "updated": "2023-10-02T12:20:28Z",
            "published": "2023-10-02T12:20:28Z",
            "summary": "By Fagin's Theorem, NP contains precisely those problems that can be\ndescribed by formulas starting with an existential second-order quantifier,\nfollowed by only first-order quantifiers (ESO formulas). Subsequent research\nrefined this result, culminating in powerful theorems that characterize for\neach possible sequence of first-order quantifiers how difficult the described\nproblem can be. We transfer this line of inquiry to the parameterized setting,\nwhere the size of the set quantified by the second-order quantifier is the\nparameter. Many natural parameterized problems can be described in this way\nusing simple sequences of first-order quantifiers: For the clique or vertex\ncover problems, two universal first-order quantifiers suffice (\"for all pairs\nof vertices ... must hold\"); for the dominating set problem, a universal\nfollowed by an existential quantifier suffice (\"for all vertices, there is a\nvertex such that ...\"); and so on. We present a complete characterization that\nstates for each possible sequence of first-order quantifiers how high the\nparameterized complexity of the described problems can be. The uncovered\ndividing line between quantifier sequences that lead to tractable versus\nintractable problems is distinct from that known from the classical setting,\nand it depends on whether the parameter is a lower bound on, an upper bound on,\nor equal to the size of the quantified set.",
            "author": [
                "Max Bannach",
                "Florian Chudigiewitsch",
                "Till Tantau"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01134v1",
                "http://arxiv.org/pdf/2310.01134v1"
            ],
            "primary_category": "cs.LO",
            "category": [
                "cs.LO",
                "cs.CC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01098v1",
            "title": "NP$^2$L: Negative Pseudo Partial Labels Extraction for Graph Neural\n  Networks",
            "updated": "2023-10-02T11:13:59Z",
            "published": "2023-10-02T11:13:59Z",
            "summary": "How to utilize the pseudo labels has always been a research hotspot in\nmachine learning. However, most methods use pseudo labels as supervised\ntraining, and lack of valid assessing for their accuracy. Moreover,\napplications of pseudo labels in graph neural networks (GNNs) oversee the\ndifference between graph learning and other machine learning tasks such as\nmessage passing mechanism. Aiming to address the first issue, we found through\na large number of experiments that the pseudo labels are more accurate if they\nare selected by not overlapping partial labels and defined as negative node\npairs relations. Therefore, considering the extraction based on pseudo and\npartial labels, negative edges are constructed between two nodes by the\nnegative pseudo partial labels extraction (NP$^2$E) module. With that, a signed\ngraph are built containing highly accurate pseudo labels information from the\noriginal graph, which effectively assists GNN in learning at the\nmessage-passing level, provide one solution to the second issue. Empirical\nresults about link prediction and node classification tasks on several\nbenchmark datasets demonstrate the effectiveness of our method.\nState-of-the-art performance is achieved on the both tasks.",
            "author": [
                "Xinjie Shen",
                "Danyang Wu",
                "Jitao Lu",
                "Junjie Liang",
                "Jin Xu",
                "Feiping Nie"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01098v1",
                "http://arxiv.org/pdf/2310.01098v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01089v1",
            "title": "GraphText: Graph Reasoning in Text Space",
            "updated": "2023-10-02T11:03:57Z",
            "published": "2023-10-02T11:03:57Z",
            "summary": "Large Language Models (LLMs) have gained the ability to assimilate human\nknowledge and facilitate natural language interactions with both humans and\nother LLMs. However, despite their impressive achievements, LLMs have not made\nsignificant advancements in the realm of graph machine learning. This\nlimitation arises because graphs encapsulate distinct relational data, making\nit challenging to transform them into natural language that LLMs understand. In\nthis paper, we bridge this gap with a novel framework, GraphText, that\ntranslates graphs into natural language. GraphText derives a graph-syntax tree\nfor each graph that encapsulates both the node attributes and inter-node\nrelationships. Traversal of the tree yields a graph text sequence, which is\nthen processed by an LLM to treat graph tasks as text generation tasks.\nNotably, GraphText offers multiple advantages. It introduces training-free\ngraph reasoning: even without training on graph data, GraphText with ChatGPT\ncan achieve on par with, or even surpassing, the performance of\nsupervised-trained graph neural networks through in-context learning (ICL).\nFurthermore, GraphText paves the way for interactive graph reasoning, allowing\nboth humans and LLMs to communicate with the model seamlessly using natural\nlanguage. These capabilities underscore the vast, yet-to-be-explored potential\nof LLMs in the domain of graph machine learning.",
            "author": [
                "Jianan Zhao",
                "Le Zhuo",
                "Yikang Shen",
                "Meng Qu",
                "Kai Liu",
                "Michael Bronstein",
                "Zhaocheng Zhu",
                "Jian Tang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01089v1",
                "http://arxiv.org/pdf/2310.01089v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01080v2",
            "title": "Rel2Graph: Automated Mapping From Relational Databases to a Unified\n  Property Knowledge Graph",
            "updated": "2023-10-26T08:35:07Z",
            "published": "2023-10-02T10:46:51Z",
            "summary": "Although a few approaches are proposed to convert relational databases to\ngraphs, there is a genuine lack of systematic evaluation across a wider\nspectrum of databases. Recognising the important issue of query mapping, this\npaper proposes an approach Rel2Graph, an automatic knowledge graph construction\n(KGC) approach from an arbitrary number of relational databases. Our approach\nalso supports the mapping of conjunctive SQL queries into pattern-based NoSQL\nqueries. We evaluate our proposed approach on two widely used relational\ndatabase-oriented datasets: Spider and KaggleDBQA benchmarks for semantic\nparsing. We employ the execution accuracy (EA) metric to quantify the\nproportion of results by executing the NoSQL queries on the property knowledge\ngraph we construct that aligns with the results of SQL queries performed on\nrelational databases. Consequently, the counterpart property knowledge graph of\nbenchmarks with high accuracy and integrity can be ensured. The code and data\nwill be publicly available. The code and data are available at\ngithub\\footnote{https://github.com/nlp-tlp/Rel2Graph}.",
            "author": [
                "Ziyu Zhao",
                "Wei Liu",
                "Tim French",
                "Michael Stewart"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01080v2",
                "http://arxiv.org/pdf/2310.01080v2"
            ],
            "primary_category": "cs.DB",
            "category": [
                "cs.DB"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01074v2",
            "title": "Back to the Future: Towards Explainable Temporal Reasoning with Large\n  Language Models",
            "updated": "2023-10-08T12:45:18Z",
            "published": "2023-10-02T10:35:23Z",
            "summary": "Temporal reasoning is a crucial NLP task, providing a nuanced understanding\nof time-sensitive contexts within textual data. Although recent advancements in\nLLMs have demonstrated their potential in temporal reasoning, the predominant\nfocus has been on tasks such as temporal expression and temporal relation\nextraction. These tasks are primarily designed for the extraction of direct and\npast temporal cues and to engage in simple reasoning processes. A significant\ngap remains when considering complex reasoning tasks such as event forecasting,\nwhich requires multi-step temporal reasoning on events and prediction on the\nfuture timestamp. Another notable limitation of existing methods is their\nincapability to provide an illustration of their reasoning process, hindering\nexplainability. In this paper, we introduce the first task of explainable\ntemporal reasoning, to predict an event's occurrence at a future timestamp\nbased on context which requires multiple reasoning over multiple events, and\nsubsequently provide a clear explanation for their prediction. Our task offers\na comprehensive evaluation of both the LLMs' complex temporal reasoning\nability, the future event prediction ability, and explainability-a critical\nattribute for AI applications. To support this task, we present the first\nmulti-source instruction-tuning dataset of explainable temporal reasoning\n(ExpTime) with 26k derived from the temporal knowledge graph datasets and their\ntemporal reasoning paths, using a novel knowledge-graph-instructed-generation\nstrategy. Based on the dataset, we propose the first open-source LLM series\nTimeLlaMA based on the foundation LlaMA2, with the ability of instruction\nfollowing for explainable temporal reasoning. We compare the performance of our\nmethod and a variety of LLMs, where our method achieves the state-of-the-art\nperformance of temporal prediction and explanation.",
            "author": [
                "Chenhan Yuan",
                "Qianqian Xie",
                "Jimin Huang",
                "Sophia Ananiadou"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01074v2",
                "http://arxiv.org/pdf/2310.01074v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01066v1",
            "title": "Finding a reconfiguration sequence between longest increasing\n  subsequences",
            "updated": "2023-10-02T10:22:29Z",
            "published": "2023-10-02T10:22:29Z",
            "summary": "In this note, we consider the problem of finding a step-by-step\ntransformation between two longest increasing subsequences in a sequence,\nnamely Longest Increasing Subsequence Reconfiguration. We give a\npolynomial-time algorithm for deciding whether there is a reconfiguration\nsequence between two longest increasing subsequences in a sequence. This\nimplies that Independent Set Reconfiguration and Token Sliding are\npolynomial-time solvable on permutation graphs, provided that the input two\nindependent sets are largest among all independent sets in the input graph. We\nalso consider a special case, where the underlying permutation graph of an\ninput sequence is bipartite. In this case, we give a polynomial-time algorithm\nfor finding a shortest reconfiguration sequence (if it exists).",
            "author": [
                "Yuuki Aoike",
                "Masashi Kiyomi",
                "Yasuaki Kobayashi",
                "Yota Otachi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01066v1",
                "http://arxiv.org/pdf/2310.01066v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01065v1",
            "title": "KGEx: Explaining Knowledge Graph Embeddings via Subgraph Sampling and\n  Knowledge Distillation",
            "updated": "2023-10-02T10:20:24Z",
            "published": "2023-10-02T10:20:24Z",
            "summary": "Despite being the go-to choice for link prediction on knowledge graphs,\nresearch on interpretability of knowledge graph embeddings (KGE) has been\nrelatively unexplored. We present KGEx, a novel post-hoc method that explains\nindividual link predictions by drawing inspiration from surrogate models\nresearch. Given a target triple to predict, KGEx trains surrogate KGE models\nthat we use to identify important training triples. To gauge the impact of a\ntraining triple, we sample random portions of the target triple neighborhood\nand we train multiple surrogate KGE models on each of them. To ensure\nfaithfulness, each surrogate is trained by distilling knowledge from the\noriginal KGE model. We then assess how well surrogates predict the target\ntriple being explained, the intuition being that those leading to faithful\npredictions have been trained on impactful neighborhood samples. Under this\nassumption, we then harvest triples that appear frequently across impactful\nneighborhoods. We conduct extensive experiments on two publicly available\ndatasets, to demonstrate that KGEx is capable of providing explanations\nfaithful to the black-box model.",
            "author": [
                "Vasileios Baltatzis",
                "Luca Costabello"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01065v1",
                "http://arxiv.org/pdf/2310.01065v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01061v1",
            "title": "Reasoning on Graphs: Faithful and Interpretable Large Language Model\n  Reasoning",
            "updated": "2023-10-02T10:14:43Z",
            "published": "2023-10-02T10:14:43Z",
            "summary": "Large language models (LLMs) have demonstrated impressive reasoning abilities\nin complex tasks. However, they lack up-to-date knowledge and experience\nhallucinations during reasoning, which can lead to incorrect reasoning\nprocesses and diminish their performance and trustworthiness. Knowledge graphs\n(KGs), which capture vast amounts of facts in a structured format, offer a\nreliable source of knowledge for reasoning. Nevertheless, existing KG-based LLM\nreasoning methods only treat KGs as factual knowledge bases and overlook the\nimportance of their structural information for reasoning. In this paper, we\npropose a novel method called reasoning on graphs (RoG) that synergizes LLMs\nwith KGs to enable faithful and interpretable reasoning. Specifically, we\npresent a planning-retrieval-reasoning framework, where RoG first generates\nrelation paths grounded by KGs as faithful plans. These plans are then used to\nretrieve valid reasoning paths from the KGs for LLMs to conduct faithful\nreasoning. Furthermore, RoG not only distills knowledge from KGs to improve the\nreasoning ability of LLMs through training but also allows seamless integration\nwith any arbitrary LLMs during inference. Extensive experiments on two\nbenchmark KGQA datasets demonstrate that RoG achieves state-of-the-art\nperformance on KG reasoning tasks and generates faithful and interpretable\nreasoning results.",
            "author": [
                "Linhao Luo",
                "Yuan-Fang Li",
                "Gholamreza Haffari",
                "Shirui Pan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01061v1",
                "http://arxiv.org/pdf/2310.01061v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01035v1",
            "title": "Learnable Cross-modal Knowledge Distillation for Multi-modal Learning\n  with Missing Modality",
            "updated": "2023-10-02T09:24:54Z",
            "published": "2023-10-02T09:24:54Z",
            "summary": "The problem of missing modalities is both critical and non-trivial to be\nhandled in multi-modal models. It is common for multi-modal tasks that certain\nmodalities contribute more compared to other modalities, and if those important\nmodalities are missing, the model performance drops significantly. Such fact\nremains unexplored by current multi-modal approaches that recover the\nrepresentation from missing modalities by feature reconstruction or blind\nfeature aggregation from other modalities, instead of extracting useful\ninformation from the best performing modalities. In this paper, we propose a\nLearnable Cross-modal Knowledge Distillation (LCKD) model to adaptively\nidentify important modalities and distil knowledge from them to help other\nmodalities from the cross-modal perspective for solving the missing modality\nissue. Our approach introduces a teacher election procedure to select the most\n``qualified'' teachers based on their single modality performance on certain\ntasks. Then, cross-modal knowledge distillation is performed between teacher\nand student modalities for each task to push the model parameters to a point\nthat is beneficial for all tasks. Hence, even if the teacher modalities for\ncertain tasks are missing during testing, the available student modalities can\naccomplish the task well enough based on the learned knowledge from their\nautomatically elected teacher modalities. Experiments on the Brain Tumour\nSegmentation Dataset 2018 (BraTS2018) shows that LCKD outperforms other methods\nby a considerable margin, improving the state-of-the-art performance by 3.61%\nfor enhancing tumour, 5.99% for tumour core, and 3.76% for whole tumour in\nterms of segmentation Dice score.",
            "author": [
                "Hu Wang",
                "Yuanhong Chen",
                "Congbo Ma",
                "Jodie Avery",
                "Louise Hull",
                "Gustavo Carneiro"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01035v1",
                "http://arxiv.org/pdf/2310.01035v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01031v1",
            "title": "Hydrodynamical modelling of tidal dissipation in gas giant planets at\n  the time of space missions",
            "updated": "2023-10-02T09:22:32Z",
            "published": "2023-10-02T09:22:32Z",
            "summary": "Gas giant planets are differentially rotating magnetic objects that have\nstrong and complex interactions with their environment. In our Solar system,\nthey interact with their numerous moons while exoplanets with very short\norbital periods (hot Jupiters), interact with their host star. The dissipation\nof waves excited by tidal forces in their interiors shapes the orbital\narchitecture and the rotational dynamics of these systems. Recently,\nastrometric observations of Jupiter and Saturn systems have challenged our\nunderstanding of their formation and evolution, with stronger tidal dissipation\nin these planets than previously predicted, in contrast to what appears to be\nweaker in gas giant exoplanets. These new constraints are motivating the\ndevelopment of realistic models of tidal dissipation inside these planets. At\nthe same time, the Juno and Cassini space missions have revolutionised our\nknowledge of the interiors of Jupiter and Saturn, whose structure is a\ncombination of stably stratified zones and convective regions. In this work, we\npresent results of hydrodynamical calculations modelling tidal waves and their\ndissipation in Jupiter, taking for the first time the latest, state-of-the-art\ninterior model of the planet. We performed 2D numerical simulations of linear\ntidal gravito-inertial waves that propagate and dissipate within Jupiter\ninterior by taking into account viscous, thermal and chemical diffusions. This\nnew model allows us to explore the properties of the dissipation and the\nassociated tidal torque as a function of all the key hydrodynamical and\nstructural parameters.",
            "author": [
                "Hachem Dhouib",
                "Cl\u00e9ment Baruteau",
                "St\u00e9phane Mathis",
                "Florian Debras",
                "Aur\u00e9lie Astoul",
                "Michel Rieutord"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01031v1",
                "http://arxiv.org/pdf/2310.01031v1"
            ],
            "primary_category": "astro-ph.EP",
            "category": [
                "astro-ph.EP",
                "physics.flu-dyn"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01011v2",
            "title": "Towards Fixing Clever-Hans Predictors with Counterfactual Knowledge\n  Distillation",
            "updated": "2023-10-03T18:10:48Z",
            "published": "2023-10-02T09:02:51Z",
            "summary": "This paper introduces a novel technique called counterfactual knowledge\ndistillation (CFKD) to detect and remove reliance on confounders in deep\nlearning models with the help of human expert feedback. Confounders are\nspurious features that models tend to rely on, which can result in unexpected\nerrors in regulated or safety-critical domains. The paper highlights the\nbenefit of CFKD in such domains and shows some advantages of counterfactual\nexplanations over other types of explanations. We propose an experiment scheme\nto quantitatively evaluate the success of CFKD and different teachers that can\ngive feedback to the model. We also introduce a new metric that is better\ncorrelated with true test performance than validation accuracy. The paper\ndemonstrates the effectiveness of CFKD on synthetically augmented datasets and\non real-world histopathological datasets.",
            "author": [
                "Sidney Bender",
                "Christopher J. Anders",
                "Pattarawatt Chormai",
                "Heike Marxfeld",
                "Jan Herrmann",
                "Gr\u00e9goire Montavon"
            ],
            "link": [
                "http://arxiv.org/abs/2310.01011v2",
                "http://arxiv.org/pdf/2310.01011v2"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01007v1",
            "title": "On the Descriptive Complexity of Groups without Abelian Normal Subgroups\n  (Extended Abstract)",
            "updated": "2023-10-02T09:01:35Z",
            "published": "2023-10-02T09:01:35Z",
            "summary": "In this paper, we explore the descriptive complexity theory of finite groups\nby examining the power of the second Ehrenfeucht-Fraisse bijective pebble game\nin Hella's (Ann. Pure Appl. Log., 1989) hierarchy. This is a Spoiler-Duplicator\ngame in which Spoiler can place up to two pebbles each round. While it\ntrivially solves graph isomorphism, it may be nontrivial for finite groups, and\nother ternary relational structures. We first provide a novel generalization of\nWeisfeiler-Leman (WL) coloring, which we call 2-ary WL. We then show that the\n2-ary WL is equivalent to the second Ehrenfeucht-Fraisse bijective pebble game\nin Hella's hierarchy.\n  Our main result is that, in the pebble game characterization, only O(1)\npebbles and O(1) rounds are sufficient to identify all groups without Abelian\nnormal subgroups (a class of groups for which isomorphism testing is known to\nbe in P; Babai, Codenotti, & Qiao, ICALP 2012). In particular, we show that\nwithin the first few rounds, Spoiler can force Duplicator to select an\nisomorphism between two such groups at each subsequent round. By Hella's\nresults (ibid.), this is equivalent to saying that these groups are identified\nby formulas in first-order logic with generalized 2-ary quantifiers, using only\nO(1) variables and O(1) quantifier depth.",
            "author": [
                "Joshua A. Grochow",
                "Michael Levet"
            ],
            "link": [
                "http://dx.doi.org/10.4204/EPTCS.390.12",
                "http://arxiv.org/abs/2310.01007v1",
                "http://arxiv.org/pdf/2310.01007v1"
            ],
            "primary_category": "cs.LO",
            "category": [
                "cs.LO",
                "cs.CC",
                "cs.DS",
                "F.4.1; F.1.3; F.2.2; G.2.2"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.01004v1",
            "title": "The Recursive Arrival Problem",
            "updated": "2023-10-02T09:01:14Z",
            "published": "2023-10-02T09:01:14Z",
            "summary": "We study an extension of the Arrival problem, called Recursive Arrival,\ninspired by Recursive State Machines, which allows for a family of switching\ngraphs that can call each other in a recursive way. We study the computational\ncomplexity of deciding whether a Recursive Arrival instance terminates at a\ngiven target vertex. We show this problem is contained in NP \\cap coNP, and we\nshow that a search version of the problem lies in UEOPL, and hence in EOPL =\nPLS \\cap PPAD. Furthermore, we show P-hardness of the Recursive Arrival\ndecision problem. By contrast, the current best-known hardness result for\nArrival is PL-hardness.",
            "author": [
                "Thomas Webster"
            ],
            "link": [
                "http://dx.doi.org/10.4204/EPTCS.390.11",
                "http://arxiv.org/abs/2310.01004v1",
                "http://arxiv.org/pdf/2310.01004v1"
            ],
            "primary_category": "cs.CC",
            "category": [
                "cs.CC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.00999v1",
            "title": "CGAAL: Distributed On-The-Fly ATL Model Checker with Heuristics",
            "updated": "2023-10-02T08:59:13Z",
            "published": "2023-10-02T08:59:13Z",
            "summary": "We present CGAAL, our efficient on-the-fly model checker for alternating-time\ntemporal logic (ATL) on concurrent game structures (CGS). We present how our\ntool encodes ATL as extended dependency graphs with negation edges and employs\nthe distributed on-the-fly algorithm by Dalsgaard et al. Our tool offers\nmultiple novel search strategies for the algorithm, including DHS which is\ninspired by PageRank and uses the in-degree of configurations as a heuristic,\nIHS which estimates instability of assignment values, and LPS which estimates\nthe distance to a state satisfying the constituent property using linear\nprogramming. CGS are input using our modelling language LCGS, where composition\nand synchronisation are easily described. We prove the correctness of our\nencoding, and our experiments show that our tool CGAAL is often one to three\norders of magnitude faster than the popular tool PRISM-games on case studies\nfrom PRISM's documentation and among case studies we have developed. In our\nevaluation, we also compare and evaluate our search strategies, and find that\nour custom search strategies are often significantly faster than the usual\nbreadth-first and depth-first search strategies.",
            "author": [
                "Falke B. \u00d8. Carlsen",
                "Lars Bo P. Frydenskov",
                "Nicolaj \u00d8. Jensen",
                "Jener Rasmussen",
                "Mathias M. S\u00f8rensen",
                "Asger G. Weirs\u00f8e",
                "Mathias C. Jensen",
                "Kim G. Larsen"
            ],
            "link": [
                "http://dx.doi.org/10.4204/EPTCS.390.7",
                "http://arxiv.org/abs/2310.00999v1",
                "http://arxiv.org/pdf/2310.00999v1"
            ],
            "primary_category": "cs.LO",
            "category": [
                "cs.LO",
                "D.2.1; D.2.4"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.00989v1",
            "title": "On Two- and Three-valued Semantics for Impure Simplicial Complexes",
            "updated": "2023-10-02T08:56:26Z",
            "published": "2023-10-02T08:56:26Z",
            "summary": "Simplicial complexes are a convenient semantic primitive to reason about\nprocesses (agents) communicating with each other in synchronous and\nasynchronous computation. Impure simplicial complexes distinguish active\nprocesses from crashed ones, in other words, agents that are alive from agents\nthat are dead. In order to rule out that dead agents reason about themselves\nand about other agents, three-valued epistemic semantics have been proposed\nwhere, in addition to the usual values true and false, the third value stands\nfor undefined: the knowledge of dead agents is undefined and so are the\npropositional variables describing their local state. Other semantics for\nimpure complexes are two-valued where a dead agent knows everything. Different\nchoices in designing a semantics produce different three-valued semantics, and\nalso different two-valued semantics. In this work, we categorize the available\nchoices by discounting the bad ones, identifying the equivalent ones, and\nconnecting the non-equivalent ones via a translation. The main result of the\npaper is identifying the main relevant distinction to be the number of truth\nvalues and bridging this difference by means of a novel embedding from three-\ninto two-valued semantics. This translation also enables us to highlight quite\nfundamental modeling differences underpinning various two- and three-valued\napproaches in this area of combinatorial topology. In particular, pure\ncomplexes can be defined as those invariant under the translation.",
            "author": [
                "Hans van Ditmarsch",
                "Roman Kuznets",
                "Rojo Randrianomentsoa"
            ],
            "link": [
                "http://dx.doi.org/10.4204/EPTCS.390.4",
                "http://arxiv.org/abs/2310.00989v1",
                "http://arxiv.org/pdf/2310.00989v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC",
                "cs.LO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.00952v1",
            "title": "LS-VOS: Identifying Outliers in 3D Object Detections Using Latent Space\n  Virtual Outlier Synthesis",
            "updated": "2023-10-02T07:44:26Z",
            "published": "2023-10-02T07:44:26Z",
            "summary": "LiDAR-based 3D object detectors have achieved unprecedented speed and\naccuracy in autonomous driving applications. However, similar to other neural\nnetworks, they are often biased toward high-confidence predictions or return\ndetections where no real object is present. These types of detections can lead\nto a less reliable environment perception, severely affecting the functionality\nand safety of autonomous vehicles. We address this problem by proposing LS-VOS,\na framework for identifying outliers in 3D object detections. Our approach\nbuilds on the idea of Virtual Outlier Synthesis (VOS), which incorporates\noutlier knowledge during training, enabling the model to learn more compact\ndecision boundaries. In particular, we propose a new synthesis approach that\nrelies on the latent space of an auto-encoder network to generate outlier\nfeatures with a parametrizable degree of similarity to in-distribution\nfeatures. In extensive experiments, we show that our approach improves the\noutlier detection capabilities of a state-of-the-art object detector while\nmaintaining high 3D object detection performance.",
            "author": [
                "Aldi Piroli",
                "Vinzenz Dallabetta",
                "Johannes Kopp",
                "Marc Walessa",
                "Daniel Meissner",
                "Klaus Dietmayer"
            ],
            "link": [
                "http://arxiv.org/abs/2310.00952v1",
                "http://arxiv.org/pdf/2310.00952v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.00946v1",
            "title": "Distilling Influences to Mitigate Prediction Churn in Graph Neural\n  Networks",
            "updated": "2023-10-02T07:37:28Z",
            "published": "2023-10-02T07:37:28Z",
            "summary": "Models with similar performances exhibit significant disagreement in the\npredictions of individual samples, referred to as prediction churn. Our work\nexplores this phenomenon in graph neural networks by investigating differences\nbetween models differing only in their initializations in their utilized\nfeatures for predictions. We propose a novel metric called Influence Difference\n(ID) to quantify the variation in reasons used by nodes across models by\ncomparing their influence distribution. Additionally, we consider the\ndifferences between nodes with a stable and an unstable prediction, positing\nthat both equally utilize different reasons and thus provide a meaningful\ngradient signal to closely match two models even when the predictions for nodes\nare similar. Based on our analysis, we propose to minimize this ID in Knowledge\nDistillation, a domain where a new model should closely match an established\none. As an efficient approximation, we introduce DropDistillation (DD) that\nmatches the output for a graph perturbed by edge deletions. Our empirical\nevaluation of six benchmark datasets for node classification validates the\ndifferences in utilized features. DD outperforms previous methods regarding\nprediction stability and overall performance in all considered Knowledge\nDistillation experiments.",
            "author": [
                "Andreas Roth",
                "Thomas Liebig"
            ],
            "link": [
                "http://arxiv.org/abs/2310.00946v1",
                "http://arxiv.org/pdf/2310.00946v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.00941v1",
            "title": "Improved Variational Bayesian Phylogenetic Inference using Mixtures",
            "updated": "2023-10-02T07:18:48Z",
            "published": "2023-10-02T07:18:48Z",
            "summary": "We present VBPI-Mixtures, an algorithm designed to enhance the accuracy of\nphylogenetic posterior distributions, particularly for tree-topology and\nbranch-length approximations. Despite the Variational Bayesian Phylogenetic\nInference (VBPI), a leading-edge black-box variational inference (BBVI)\nframework, achieving remarkable approximations of these distributions, the\nmultimodality of the tree-topology posterior presents a formidable challenge to\nsampling-based learning techniques such as BBVI. Advanced deep learning\nmethodologies such as normalizing flows and graph neural networks have been\nexplored to refine the branch-length posterior approximation, yet efforts to\nameliorate the posterior approximation over tree topologies have been lacking.\nOur novel VBPI-Mixtures algorithm bridges this gap by harnessing the latest\nbreakthroughs in mixture learning within the BBVI domain. As a result,\nVBPI-Mixtures is capable of capturing distributions over tree-topologies that\nVBPI fails to model. We deliver state-of-the-art performance on difficult\ndensity estimation tasks across numerous real phylogenetic datasets.",
            "author": [
                "Oskar Kviman",
                "Ricky Mol\u00e9n",
                "Jens Lagergren"
            ],
            "link": [
                "http://arxiv.org/abs/2310.00941v1",
                "http://arxiv.org/pdf/2310.00941v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.00940v1",
            "title": "1-planar unit distance graphs",
            "updated": "2023-10-02T07:17:09Z",
            "published": "2023-10-02T07:17:09Z",
            "summary": "A matchstick graph is a plane graph with edges drawn as unit distance line\nsegments. This class of graphs was introduced by Harborth who conjectured that\na matchstick graph on $n$ vertices can have at most $\\lfloor 3n - \\sqrt{12n -\n3}\\rfloor$ edges. Recently his conjecture was settled by Lavoll\\'ee and\nSwanepoel. In this paper we consider $1$-planar unit distance graphs. We say\nthat a graph is a $1$-planar unit distance graph if it can be drawn in the\nplane such that all edges are drawn as unit distance line segments while each\nof them are involved in at most one crossing. We show that such graphs on $n$\nvertices can have at most $3n-\\sqrt[4]{n}/10$ edges.",
            "author": [
                "Panna Geh\u00e9r",
                "G\u00e9za T\u00f3th"
            ],
            "link": [
                "http://arxiv.org/abs/2310.00940v1",
                "http://arxiv.org/pdf/2310.00940v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C10, 05C12, 05C62"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.00938v2",
            "title": "An FPRAS for two terminal reliability in directed acyclic graphs",
            "updated": "2023-11-22T10:21:19Z",
            "published": "2023-10-02T07:06:37Z",
            "summary": "We give a fully polynomial-time randomized approximation scheme (FPRAS) for\ntwo terminal reliability in directed acyclic graphs.",
            "author": [
                "Weiming Feng",
                "Heng Guo"
            ],
            "link": [
                "http://arxiv.org/abs/2310.00938v2",
                "http://arxiv.org/pdf/2310.00938v2"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.00936v1",
            "title": "Trained Latent Space Navigation to Prevent Lack of Photorealism in\n  Generated Images on Style-based Models",
            "updated": "2023-10-02T07:02:32Z",
            "published": "2023-10-02T07:02:32Z",
            "summary": "Recent studies on StyleGAN variants show promising performances for various\ngeneration tasks. In these models, latent codes have traditionally been\nmanipulated and searched for the desired images. However, this approach\nsometimes suffers from a lack of photorealism in generated images due to a lack\nof knowledge about the geometry of the trained latent space. In this paper, we\nshow a simple unsupervised method that provides well-trained local latent\nsubspace, enabling latent code navigation while preserving the photorealism of\nthe generated images. Specifically, the method identifies densely mapped latent\nspaces and restricts latent manipulations within the local latent subspace.\nExperimental results demonstrate that images generated within the local latent\nsubspace maintain photorealism even when the latent codes are significantly and\nrepeatedly manipulated. Moreover, experiments show that the method can be\napplied to latent code optimization for various types of style-based models.\nOur empirical evidence of the method will benefit applications in style-based\nmodels.",
            "author": [
                "Takumi Harada",
                "Kazuyuki Aihara",
                "Hiroyuki Sakai"
            ],
            "link": [
                "http://arxiv.org/abs/2310.00936v1",
                "http://arxiv.org/pdf/2310.00936v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.00935v1",
            "title": "Resolving Knowledge Conflicts in Large Language Models",
            "updated": "2023-10-02T06:57:45Z",
            "published": "2023-10-02T06:57:45Z",
            "summary": "Large language models (LLMs) often encounter knowledge conflicts, scenarios\nwhere discrepancy arises between the internal parametric knowledge of LLMs and\nnon-parametric information provided in the prompt context. In this work we ask\nwhat are the desiderata for LLMs when a knowledge conflict arises and whether\nexisting LLMs fulfill them. We posit that LLMs should 1) identify knowledge\nconflicts, 2) pinpoint conflicting information segments, and 3) provide\ndistinct answers or viewpoints in conflicting scenarios. To this end, we\nintroduce KNOWLEDGE CONFLICT, an evaluation framework for simulating contextual\nknowledge conflicts and quantitatively evaluating to what extent LLMs achieve\nthese goals. KNOWLEDGE CONFLICT includes diverse and complex situations of\nknowledge conflict, knowledge from diverse entities and domains, two synthetic\nconflict creation methods, and settings with progressively increasing\ndifficulty to reflect realistic knowledge conflicts. Extensive experiments with\nthe KNOWLEDGE CONFLICT framework reveal that while LLMs perform well in\nidentifying the existence of knowledge conflicts, they struggle to determine\nthe specific conflicting knowledge and produce a response with distinct answers\namidst conflicting information. To address these challenges, we propose new\ninstruction-based approaches that augment LLMs to better achieve the three\ngoals. Further analysis shows that abilities to tackle knowledge conflicts are\ngreatly impacted by factors such as knowledge domain and prompt text, while\ngenerating robust responses to knowledge conflict scenarios remains an open\nresearch question.",
            "author": [
                "Yike Wang",
                "Shangbin Feng",
                "Heng Wang",
                "Weijia Shi",
                "Vidhisha Balachandran",
                "Tianxing He",
                "Yulia Tsvetkov"
            ],
            "link": [
                "http://arxiv.org/abs/2310.00935v1",
                "http://arxiv.org/pdf/2310.00935v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.00931v1",
            "title": "Beyond the Pseudoforest Strong Nine Dragon Tree Theorem",
            "updated": "2023-10-02T06:51:53Z",
            "published": "2023-10-02T06:51:53Z",
            "summary": "The pseudoforest version of the Strong Nine Dragon Tree Conjecture states\nthat if a graph $G$ has maximum average degree $\\text{mad}(G) = 2 \\max_{H\n\\subseteq G} \\frac{e(G)}{v(G)}$ at most $2(k + \\frac{d}{k+d+1})$, then it has a\ndecomposition into $k+1$ pseudoforests where in one pseudoforest $F$ the\ncomponents of $F$ have at most $d$ edges. This was proven in 2020. We\nstrengthen this theorem by showing that we can find such a decomposition where\nadditionally $F$ is acyclic, the diameter of the components of $F$ is at most\n$2\\ell + 2$, where $\\ell = \\lfloor\\frac{d-1}{k+1} \\rfloor$, and at most $2\\ell\n+ 1$ if $d \\equiv 1 \\bmod k+1$. Furthermore, for any component $K$ of $F$ and\nany $z \\in \\mathbb N$, we have $diam(K) \\leq 2z$ if $e(K) \\geq d - z(k-1) + 1$.\nWe also show that both diameter bounds are best possible as an extension for\nboth the Strong Nine Dragon Tree Conjecture for pseudoforests and its original\nconjecture for forests. In fact, they are still optimal even if we only enforce\n$F$ to have any constant maximum degree, instead of enforcing every component\nof $F$ to have at most $d$ edges.",
            "author": [
                "Sebastian Mies",
                "Benjamin Moore",
                "Evelyne Smith Roberge"
            ],
            "link": [
                "http://arxiv.org/abs/2310.00931v1",
                "http://arxiv.org/pdf/2310.00931v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.00926v1",
            "title": "Integration of Graph Neural Network and Neural-ODEs for Tumor Dynamic\n  Prediction",
            "updated": "2023-10-02T06:39:08Z",
            "published": "2023-10-02T06:39:08Z",
            "summary": "In anti-cancer drug development, a major scientific challenge is\ndisentangling the complex relationships between high-dimensional genomics data\nfrom patient tumor samples, the corresponding tumor's organ of origin, the drug\ntargets associated with given treatments and the resulting treatment response.\nFurthermore, to realize the aspirations of precision medicine in identifying\nand adjusting treatments for patients depending on the therapeutic response,\nthere is a need for building tumor dynamic models that can integrate both\nlongitudinal tumor size as well as multimodal, high-content data. In this work,\nwe take a step towards enhancing personalized tumor dynamic predictions by\nproposing a heterogeneous graph encoder that utilizes a bipartite Graph\nConvolutional Neural network (GCN) combined with Neural Ordinary Differential\nEquations (Neural-ODEs). We applied the methodology to a large collection of\npatient-derived xenograft (PDX) data, spanning a wide variety of treatments (as\nwell as their combinations) on tumors that originated from a number of\ndifferent organs. We first show that the methodology is able to discover a\ntumor dynamic model that significantly improves upon an empirical model which\nis in current use. Additionally, we show that the graph encoder is able to\neffectively utilize multimodal data to enhance tumor predictions. Our findings\nindicate that the methodology holds significant promise and offers potential\napplications in pre-clinical settings.",
            "author": [
                "Omid Bazgir",
                "Zichen Wang",
                "Marc Hafner",
                "James Lu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.00926v1",
                "http://arxiv.org/pdf/2310.00926v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.14895v1",
            "title": "Astronomical Anomalies: Their Role in the Quest for Extraterrestrial\n  Life",
            "updated": "2023-10-02T05:58:26Z",
            "published": "2023-10-02T05:58:26Z",
            "summary": "Astronomers occasionally detect an object having unexpected shape,\nunexplainable photometry, or unprecedented spectra that are inconsistent with\nour contemporary knowledge of the universe. Upon careful assessment, many of\nthese anomalies are discarded as mere noise, contamination, or faulty analysis.\nBut some anomalies survive scrutiny to yield new astronomical objects and\nphysical processes. Examples of validated anomalies include quasars, pulsars,\nand periodic Doppler shifts of Sun-like stars caused by the gravitational pull\nof orbiting planets. Other anomalies persist as mysteries, including Fast Radio\nBursts, dark energy, 'Oumuamua as an alien spaceship, and simultaneously\nvanishing stars. Advanced technological life may present astronomers with\nanomalies that require carefully designed observations from multiple vantage\npoints simultaneously and with real-time spectroscopy.",
            "author": [
                "Beatriz Villarroel",
                "Geoffrey W. Marcy"
            ],
            "link": [
                "http://arxiv.org/abs/2310.14895v1",
                "http://arxiv.org/pdf/2310.14895v1"
            ],
            "primary_category": "physics.pop-ph",
            "category": [
                "physics.pop-ph"
            ]
        }
    }
]